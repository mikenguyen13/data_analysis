# Nonparametric Regression

## Introduction

### What Is Nonparametric Regression?

**Nonparametric regression** refers to a class of regression techniques that do not assume a specific functional form (e.g., linear, polynomial of fixed degree) for the relationship between a predictor $x\in\mathbb{R}$ (or $\mathbf{x}\in\mathbb{R}^p$) and a response variable $y\in\mathbb{R}$. Instead, nonparametric methods aim to estimate this relationship directly from the data, allowing the data to "speak for themselves."

In a standard regression framework, we have a response $Y$ and one or more predictors$\mathbf{X} = (X_1, X_2, \ldots, X_p)$. Let us start with a univariate setting for simplicity. We assume:

$$
Y = m(x) + \varepsilon,
$$

where:

-   $m(x) = \mathbb{E}[Y \mid X = x]$ is the regression function we want to estimate,

-   $\varepsilon$ is a noise term with $\mathbb{E}[\varepsilon \mid X = x] = 0$ and variance $\sigma^2$.

In **parametric** regression (e.g., linear regression), we might assume $m(x)$ has the form $m(x) = \beta_0 + \beta_1 x + \cdots + \beta_d x^d$. In contrast, **nonparametric** regression relaxes this assumption and uses procedures that can adapt to potentially complex shapes in $m(x)$.

### Why Nonparametric?

1.  **Flexibility**: Nonparametric methods can capture nonlinear and more complex trends that would be missed by simple parametric models.

2.  **Fewer assumptions**: We do not assume a specific functional form; fewer assumptions may reduce misspecification errors.

3.  **Interpretability**: While more flexible, many nonparametric approaches still allow for meaningful interpretation of how the predictor $x$ influences $m(x)$.

### Drawbacks and Challenges

1.  **Curse of Dimensionality**: As the dimension $p$ (number of predictors) grows, nonparametric methods often require exponentially larger sample sizes to maintain accuracy.

2.  **Choice of Hyperparameters**: Methods such as kernel smoothing and splines depend on bandwidth or smoothing parameters, which must be chosen carefully.

3.  **Computational Complexity**: Some nonparametric methods can be more computationally intensive than parametric counterparts.

## Basic Concepts in Nonparametric Estimation

### Bias-Variance Trade-Off

For a given method of estimating $m(x)$, we define the estimator as $\hat{m}(x)$. The **mean squared error** (MSE) at a point $x$ is:

$$
\mathbb{E}\bigl[\{\hat{m}(x) - m(x)\}^2\bigr].
$$

The MSE can be decomposed into **bias** and **variance** terms:

$$
\mathbb{E}\bigl[\{\hat{m}(x) - m(x)\}^2\bigr] = \bigl[\mathbb{E}[\hat{m}(x)] - m(x)\bigr]^2 + \mathbb{E}\bigl[\{\hat{m}(x) - \mathbb{E}[\hat{m}(x)]\}^2\bigr].
$$

-   **Bias** = $\bigl[\mathbb{E}[\hat{m}(x)] - m(x)\bigr]^2$.

-   **Variance** = $\mathbb{E}\bigl[\{\hat{m}(x) - \mathbb{E}[\hat{m}(x)]\}^2\bigr]$.

Nonparametric methods often have **low bias** (because they can approximate a wide range of possible functions) but may suffer from **high variance** (because flexible models can adapt too closely to the noise in the data). The **bandwidth** or **smoothing parameter** in nonparametric methods typically controls this trade-off:

-   A large bandwidth =\> smoother function =\> higher bias but lower variance.

-   A small bandwidth =\> more wiggly function =\> lower bias but higher variance.

### Kernel Smoothing and Local Averages

Many nonparametric regression estimators can be viewed as a weighted local average of the response values $\{Y_i\}$, where the weights typically depend on the distance of $X_i$ from $x$. In the univariate case, if $x_i$ are observations of the predictor and $y_i$ are corresponding response values, the nonparametric estimator at a point $x$ often takes a generic form:

$$
\hat{m}(x) = \sum_{i=1}^n w_i(x) \, y_i,
$$

where the weights $w_i(x)$ depend on $\lvert x_i - x\rvert$∣ and also on a smoothing parameter (bandwidth). We will see how this arises more concretely in kernel regression below.

## Kernel Regression

### Basic Setup

A **kernel function** $K(\cdot)$ is a non-negative function that integrates (or sums, in the discrete case) to 1 and is symmetric around zero:

1.  $K(u) \ge 0$.

2.  $\int_{-\infty}^{\infty} K(u) \,du = 1$.

3.  $K(u) = K(-u)$.

Common kernel functions:

-   **Gaussian kernel**: $K(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}$.

-   **Epanechnikov kernel**: $K(u) = \frac{3}{4}(1 - u^2)$ for \$\lvert u\rvert \le 1\$1, and 0 otherwise.

-   **Uniform kernel**: $K(u) = \tfrac{1}{2}$ for $\lvert u\rvert \le 1$, and 0 otherwise.

A kernel function essentially measures "proximity," assigning higher weights to observations $x_i$ close to $x$ and smaller weights to observations far away.

### Nadaraya-Watson Kernel Estimator

The most common kernel-based regression estimator is the **Nadaraya-Watson** estimator, defined as:

$$
\hat{m}_h(x) = \frac{\sum_{i=1}^n K\!\Bigl(\frac{x - x_i}{h}\Bigr) \, y_i}{\sum_{i=1}^n K\!\Bigl(\frac{x - x_i}{h}\Bigr)}.
$$

Here, $h > 0$ is the **bandwidth**. Intuitively, we are computing a weighted average of the $y_i$'s, where each weight is given by $K(\frac{x - x_i}{h})$.

**Interpretation**:

-   If $\lvert x - x_i\rvert$ is small (i.e., $x_i$ is close to $x$), then $K(\frac{x - x_i}{h})$ will be relatively large.

-   If $\lvert x - x_i\rvert$∣ is large, the kernel value will be small (or even zero, if the kernel is compactly supported).

Thus, observations near $x$ have a larger impact on the estimated value $\hat{m}_h(x)$ than those far from $x$.

#### Weights Representation

We can write:

$$
w_i(x) = \frac{K\!\Bigl(\frac{x - x_i}{h}\Bigr)}{\sum_{j=1}^n K\!\Bigl(\frac{x - x_j}{h}\Bigr)}.
$$ Hence,

$$
\hat{m}_h(x) = \sum_{i=1}^n w_i(x) y_i
$$

with $\sum_{i=1}^n w_i(x) = 1$ for any $x$. Notice that $w_i(x)$ is always between 0 and 1.

### Bandwidth Selection

The choice of **bandwidth** $h$ is crucial.

-   If $h$ is **too large**, the kernel includes too many data points around $x$, leading to a smoother (high-bias, low-variance) estimate.

-   If $h$ is **too small**, the estimate becomes wiggly (low-bias, high-variance).

#### Mean Squared Error and Optimal Bandwidth

A standard approach to analyze bandwidth is to look at the **asymptotic** behavior of the **mean integrated squared error** (MISE):

$$
\text{MISE}(\hat{m}_h) = \mathbb{E}\left[\int \{\hat{m}_h(x) - m(x)\}^2 \, dx \right].
$$

As $n \to \infty$ (the number of samples goes large), under smoothness assumptions on $m(x)$ and some regularity conditions on the kernel $K$, we have an asymptotic expansion of MISE:

$$
\text{MISE}(\hat{m}_h) \approx \frac{R(K)}{n h} \, \sigma^2 + \frac{1}{4}\mu_2^2(K) \, h^4 \int \{m''(x)\}^2 dx,
$$

where:

-   $R(K) = \int_{-\infty}^{\infty} K(u)^2 du$.

-   $\mu_2(K) = \int_{-\infty}^{\infty} u^2 K(u) du$.

-   $\sigma^2$ is the noise variance, assuming $\mathrm{Var}(\varepsilon \mid X=x) = \sigma^2$.

-   $m''(x)$ denotes the second derivative of $m(x)$ (assuming $m$ is twice differentiable).

To minimize this approximate expression in $h$, we set the derivative w.r.t. $h$ to zero. This yields the **optimal bandwidth** (in the asymptotic sense):

$$
h_{\mathrm{opt}} = \left(\frac{R(K) \, \sigma^2 \, n^{-1}}{\mu_2^2(K) \int \{m''(x)\}^2 dx}\right)^{\!\!1/5}.
$$

In practice, we do not know $\sigma^2$ or $\int \{m''(x)\}^2 dx$, so we must estimate them from data. An often-used approach is **cross-validation**.

#### Cross-Validation

The **leave-one-out cross-validation** (CV) score for bandwidth selection is commonly used:

1.  For each $i$ from 1 to $n$, fit the kernel estimator $\hat{m}_{h,-i}(x)$ using all the data **except** $(x_i, y_i)$.

2.  Evaluate the squared prediction error at each left-out point: $(y_i - \hat{m}_{h,-i}(x_i))^2$.

3.  Sum (or average) those squared errors over $i$:

$$
\mathrm{CV}(h) = \frac{1}{n} \sum_{i=1}^n \bigl\{y_i - \hat{m}_{h,-i}(x_i)\bigr\}^2.
$$

We then pick $h$ that minimizes$\mathrm{CV}(h)$.

### Asymptotic Properties

For the Nadaraya-Watson estimator, under smoothness assumptions and a suitable choice of $h \to 0$ as $n \to \infty$:

-   $\hat{m}_h(x)$ is **consistent** for $m(x)$. That is, $\hat{m}_h(x) \overset{p}{\longrightarrow} m(x)$

-   The **rate of convergence** in the mean squared sense (pointwise) is roughly $n^{-2/5}$ for one-dimensional $x$. This arises from balancing the terms $1/(nh)$ and $h^4$ in the MISE.

### Derivation of the Nadaraya-Watson Estimator (Optional Deeper Look)

Let us briefly outline a derivation that arrives at the form of $\hat{m}_h(x)$ from a density-based view:

1.  By Bayes' rule and conditional density relationships: $$
    m(x) = \mathbb{E}[Y \mid X=x] = \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{\int y \, f_{X,Y}(x,y)\,dy}{f_X(x)},
    $$ where $f_{X,Y}$ is the joint density of $(X,Y)$ and $f_X$ is the marginal density of $X$.

2.  An empirical density estimator for $f_X(x)$ is $$
    \hat{f}_X(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h}K\!\Bigl(\frac{x - x_i}{h}\Bigr).
    $$

3.  Likewise, we can estimate $f_{X,Y}(x,y)$ by $$
    \hat{f}_{X,Y}(x,y) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h}K\!\Bigl(\frac{x - x_i}{h}\Bigr)\,\delta_{y_i}(y),
    $$ where $\delta_{y_i}(y)$ is a point mass at $y_i$.

4.  Then, $$
    \hat{m}_h(x) = \frac{\int y \,\hat{f}_{X,Y}(x,y) \, dy}{\hat{f}_X(x)} = \frac{\sum_{i=1}^n \frac{1}{h}K\!\Bigl(\frac{x - x_i}{h}\Bigr)\, y_i}{\sum_{i=1}^n \frac{1}{h}K\!\Bigl(\frac{x - x_i}{h}\Bigr)} = \frac{\sum_{i=1}^n K\!\Bigl(\frac{x - x_i}{h}\Bigr)\, y_i}{\sum_{i=1}^n K\!\Bigl(\frac{x - x_i}{h}\Bigr)},
    $$ which is exactly the Nadaraya-Watson form.

## Local Polynomial Regression

### Motivation

While the Nadaraya-Watson estimator is effectively a **local constant** estimator (it approximates $m(x)$ by a constant in a small neighborhood of $x$), **local polynomial regression** extends this idea by fitting a **local polynomial** around each point xxx. The advantage of local polynomials is that they can better handle boundary bias and can capture local curvature more effectively.

### Local Polynomial Fitting

A **local polynomial regression** of degree $p$ at point $x$ estimates a polynomial function $$
m_x(t) = \beta_0 + \beta_1 (t - x) + \beta_2 (t - x)^2 + \cdots + \beta_p (t - x)^p
$$ that best fits the data $\{(x_i, y_i)\}$ within a neighborhood of $x$, **weighted by a kernel**. Specifically, we solve: $$
(\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p) = \underset{\beta_0, \ldots, \beta_p}{\arg\min} \sum_{i=1}^n \Bigl[y_i - \bigl\{\beta_0 + \beta_1 (x_i - x) + \cdots + \beta_p (x_i - x)^p\bigr\}\Bigr]^2 \, K\!\Bigl(\frac{x_i - x}{h}\Bigr).
$$ We then estimate: $$
\hat{m}(x) = \hat{\beta}_0.
$$ Because $\beta_0$ is the constant term of the local polynomial expansion around $x$.

**Why center the polynomial at** $x$ rather than 0?\
Centering at $x$ helps ensure the fitted polynomial approximates the function near $x$. We are essentially using a Taylor approximation concept locally.

### Mathematical Form of the Solution

Let $\mathbf{X}_x$ be the design matrix for the local polynomial expansion at point $x$. For degree $p$, each row $i$ of $\mathbf{X}_x$ is: $$
\bigl(1,\; x_i - x,\; (x_i - x)^2,\; \ldots,\; (x_i - x)^p \bigr).
$$ Let $\mathbf{W}_x$ be the diagonal matrix whose $i$-th diagonal entry is $K\!\Bigl(\tfrac{x_i - x}{h}\Bigr)$. Then the parameter vector $\boldsymbol{\beta} = (\beta_0,\beta_1,\ldots,\beta_p)^T$ is estimated by weighted least squares: $$
\hat{\boldsymbol{\beta}}(x) = \bigl(\mathbf{X}_x^T \mathbf{W}_x \mathbf{X}_x\bigr)^{-1} \mathbf{X}_x^T \mathbf{W}_x \mathbf{y},
$$ where $\mathbf{y} = (y_1, y_2, \ldots, y_n)^T$. The local polynomial estimator of $m(x)$ is the first component of $\hat{\boldsymbol{\beta}}(x)$: $$
\hat{m}(x) = \hat{\beta}_0(x).
$$ A more explicit expression (though somewhat cumbersome) is: $$
\hat{m}(x) = \mathbf{e}_1^T \bigl(\mathbf{X}_x^T \mathbf{W}_x \mathbf{X}_x\bigr)^{-1} \mathbf{X}_x^T \mathbf{W}_x \mathbf{y},
$$ where $\mathbf{e}_1 = (1,0,0,\ldots,0)^T$.

### Bias, Variance, and Asymptotics

Local polynomial estimators have well-known expansions for bias and variance, similar to kernel regression but with additional terms due to polynomial fitting. Key highlights:

1.  **Bias**: For local polynomial of degree $p$, typically the leading bias term involves the $(p+1)$-th derivative of $m$.

2.  **Variance**: The variance will still involve a term on the order of $\frac{1}{n h}$.

3.  **Boundary Issues**: Local linear (and higher-order) polynomials can significantly reduce boundary bias compared to local constant fits.

**Local Linear Regression (degree** $p=1$) is popular because:

-   It often performs well in practice,

-   It helps mitigate boundary bias,

-   It is not overly complicated.

**Higher-degree** polynomials (e.g., local quadratic) may give better approximations if $m$ is smooth, but also increase computational and interpretational complexity.

## Smoothing Splines

### Overview

A **spline** is a piecewise polynomial function that is smooth at the junction points (called **knots**). **Smoothing splines** provide a nonparametric approach by penalizing excessive curvature in the function.

In the univariate case, assume we have data $\{(x_i, y_i)\}_{i=1}^n$ with $0 \le x_1 < x_2 < \cdots < x_n \le 1$ (we can always rescale). The **smoothing spline** estimate $\hat{m}(x)$ is the function that minimizes: $$
\sum_{i=1}^n \{y_i - f(x_i)\}^2 + \lambda \int_{0}^{1} \{f''(t)\}^2 \, dt,
$$ over all twice-differentiable functions $f$. The first term is a **lack-of-fit** measure; the second term is a **roughness penalty** controlled by $\lambda \ge 0$.

-   If $\lambda = 0$, we fit an interpolating spline (i.e., the function passes exactly through the data points).

-   If $\lambda \to \infty$, we heavily penalize curvature, and the solution approaches a straight line.

### Properties and Form of the Smoothing Spline

**Key Theorem** (from spline theory): The function that minimizes $$
\sum_{i=1}^n \{y_i - f(x_i)\}^2 + \lambda \int \{f''(t)\}^2 \, dt
$$ is a **cubic spline** with knots at $\{x_1, \ldots, x_n\}$. In other words, we can write: $$
\hat{m}(x) = a_0 + a_1 x + \sum_{j=1}^n b_j \, (x - x_j)_+^3,
$$ where $(u)_+ = \max(u,0)$. The coefficients $\{a_0,a_1,b_j\}$ are determined by solving a linear system that comes from the minimization problem.

### Choice of $\lambda$

The parameter $\lambda$ controls the **smoothness**:

-   Large $\lambda$ =\> stronger penalty on curvature =\> smoother $\hat{m}(x)$.

-   Small $\lambda$ =\> less penalty =\> $\hat{m}(x)$ can wiggle more to fit the data.

**Selection of** $\lambda$ can be done via methods like **cross-validation** (e.g., generalized cross-validation, GCV).

### Connection to Reproducing Kernel Hilbert Spaces (RKHS)

Smoothing splines have an elegant interpretation in terms of **RKHS**. The roughness penalty $\int (f''(t))^2 \, dt$ defines a norm in an appropriate Hilbert space. The solution $\hat{m}(x)$ that minimizes the penalized sum of squares is the orthogonal projection of the data onto that space, subject to the penalty. This viewpoint generalizes to higher-dimensional spline methods and other kernel-based methods.

## Confidence Intervals in Nonparametric Regression

Constructing pointwise confidence intervals for $\hat{m}(x)$ can be more complicated in nonparametric settings. However, asymptotic approximations are often used:

1.  **Asymptotic Normality**: Under regularity conditions, $\hat{m}(x)$ is asymptotically normal with mean m(x)m(x)m(x) and variance that depends on $n$, $h$, the kernel function, or the spline penalty, etc. Symbolically, $$
    sqrt{n h} \bigl\{\hat{m}(x) - m(x)\bigr\} \overset{\mathcal{D}}{\longrightarrow} \mathcal{N}(0, \sigma^2 \,\nu(x)),
    $$ for some $\nu(x)$ that depends on the kernel and $x$. For local polynomial regression, a similar normal limit holds but the form of $\nu(x)$ is more complex.

2.  **Bootstrap Methods**: Alternatively, we can use **bootstrap** procedures (e.g., residual bootstrap, wild bootstrap) to construct confidence bands for the estimated function.

## Other Nonparametric Regression Methods

### Generalized Additive Models (GAMs)

A **generalized additive model** extends generalized linear models by allowing **additive smooth terms**: $$
g(\mathbb{E}[Y]) = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p),
$$ where each $f_j$ is a smooth, nonparametric function (e.g., a spline). This is important when $p\geq 2$ (multiple predictors).

### Regression Trees and Random Forests

Though not typically framed as "kernel" or "spline," **tree-based methods** (e.g., CART, random forests) are also **nonparametric** in that they do not assume a predetermined functional form. They adaptively partition the space of predictors to make piecewise constant (or piecewise linear) fits.

### Wavelet Regression

**Wavelet regression** uses basis expansions in wavelet functions, which have nice localization properties in both the time (or space) and frequency domains. It can effectively capture local features like discontinuities, though it is less common in basic applied statistics courses.

## Practical Implementation Details

### Computational Complexity

1.  **Kernel Regression**: For each $x$ where we want $\hat{m}(x)$, we typically compute weights for all $n$ data points. This naively costs $O(n)$ per query, or $O(n^2)$ to compute the entire function for $n$ points if we do a naive approach.

2.  **Local Polynomial**: Similar in complexity to kernel regression, except we also invert a $(p+1)\times(p+1)$ matrix for each $x$. Typically, $p$ is small and this cost is negligible.

3.  **Smoothing Splines**: We solve a linear system of size $n\times n$ (or use specialized spline algorithms). This can cost $O(n^3)$ in naive implementations, but there are more efficient algorithms exploiting spline structure.

### Boundary Effects

-   **Local Averages** (Nadaraya-Watson) can have higher bias near the boundary because the "effective" set of neighbors is truncated.

-   **Local Polynomial** partially alleviates boundary bias by using local linear (or higher) approximations.

-   **Splines**: The boundary knots can also lead to different behavior near the extremes of the data range.

### Data-driven Parameter Selection

1.  **Cross-validation** remains the most common approach to picking smoothing parameters (bandwidth $h$, penalty $\lambda$, polynomial degree, etc.).

2.  **Information Criteria**: For larger models, one might use AIC, BIC, or GCV-like criteria.

## Detailed Mathematical Walk-through: Bias and Variance for Local Polynomial

To give a more concrete sense of how one might derive bias-variance expressions for local linear regression (degree 1), let's consider the approximation around a point $x$.

### Local Linear Approximation

We approximate $m$ near $x$ by: $$
m(t) \approx m(x) + m'(x)\,(t-x).
$$ When we do local linear regression, we solve: $$
\min_{\beta_0, \beta_1} \sum_{i=1}^n \Bigl[y_i - \{\beta_0 + \beta_1 (x_i - x)\}\Bigr]^2 K\!\Bigl(\frac{x_i - x}{h}\Bigr).
$$ If we imagine $y_i = m(x_i) + \varepsilon_i$, we can expand $m(x_i)$ in a Taylor series around $x$: $$
m(x_i) = m(x) + m'(x)(x_i - x) + \tfrac{1}{2}m''(x)(x_i - x)^2 + \cdots
$$ For small $\lvert x_i - x \rvert$, the higher-order terms might be small, but they contribute to the bias.

### Normal Equations

Let's define: $$
h),S_0(x) = \sum_{i=1}^n K\!\Bigl(\frac{x_i - x}{h}\Bigr), \quad S_1(x) = \sum_{i=1}^n (x_i - x) K\!\Bigl(\frac{x_i - x}{h}\Bigr), \quad S_2(x) = \sum_{i=1}^n (x_i - x)^2 K\!\Bigl(\frac{x_i - x}{h}\Bigr),
$$ and so on. Then the solutions $\hat{\beta}_0,\hat{\beta}_1$ to the weighted least squares problem can be found by solving the system: $$
\begin{pmatrix} S_0 & S_1 \\ S_1 & S_2 \end{pmatrix} \begin{pmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{pmatrix} = \begin{pmatrix} \sum_{i=1}^n y_i K\!\bigl(\frac{x_i - x}{h}\bigr) \\ \sum_{i=1}^n y_i (x_i - x) K\!\bigl(\frac{x_i - x}{h}\bigr) \end{pmatrix}.
$$ We then identify $\hat{m}(x) = \hat{\beta}_0$. Expanding these sums with the Taylor series for $y_i$ and taking expectations leads to bias terms involving $\frac{m''(x)}{2}$ and integrals over the kernel function, reminiscent of the expansions in kernel regression.

### Leading Order Terms

After algebraic manipulations, the leading bias term for local linear is typically on the order of $h^2$ (rather than $h^2$ times a derivative of $m$, more precisely $\tfrac{1}{2} m''(x) \kappa$ for some constant $\kappa$ depending on the kernel's moments). The leading variance term is of order $\frac{1}{n h}$. Balancing $h^2$ and $\frac{1}{n h}$ yields $h \sim n^{-1/3}$ for local linear, giving a rate of convergence $\hat{m}(x) - m(x) = O_p(n^{-2/3})$. (There are some differences between local constant and local linear rates; local linear can have better boundary performance.)

## Multivariate Nonparametric Regression (Brief Overview)

Nonparametric regression in higher dimensions (p\>1p\>1p\>1) is considerably more difficult because of the **curse of dimensionality**:

-   Kernel-based methods require large sample sizes for accurate estimation as $p$ grows.

-   Splines in multiple dimensions become more complicated (e.g., thin-plate splines, radial basis functions).

-   Additive models (GAMs) reduce the complexity by modeling $m(\mathbf{x})$ as a sum of functions each in one dimension.
