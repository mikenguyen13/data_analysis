<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 36 Endogeneity | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Refresher A general model framework \[ \mathbf{Y = X \beta + \epsilon} \] where \(\mathbf{Y} = n \times 1\) \(\mathbf{X} = n \times k\) \(\beta = k \times 1\) \(\epsilon = n \times 1\) Then, OLS...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 36 Endogeneity | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/endogeneity.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Refresher A general model framework \[ \mathbf{Y = X \beta + \epsilon} \] where \(\mathbf{Y} = n \times 1\) \(\mathbf{X} = n \times k\) \(\beta = k \times 1\) \(\epsilon = n \times 1\) Then, OLS...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 36 Endogeneity | A Guide on Data Analysis">
<meta name="twitter:description" content="Refresher A general model framework \[ \mathbf{Y = X \beta + \epsilon} \] where \(\mathbf{Y} = n \times 1\) \(\mathbf{X} = n \times k\) \(\beta = k \times 1\) \(\epsilon = n \times 1\) Then, OLS...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="active" href="endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">39</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="endogeneity" class="section level1" number="36">
<h1>
<span class="header-section-number">36</span> Endogeneity<a class="anchor" aria-label="anchor" href="#endogeneity"><i class="fas fa-link"></i></a>
</h1>
<p>Refresher</p>
<p>A general model framework</p>
<p><span class="math display">\[
\mathbf{Y = X \beta + \epsilon}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\mathbf{Y} = n \times 1\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{X} = n \times k\)</span></p></li>
<li><p><span class="math inline">\(\beta = k \times 1\)</span></p></li>
<li><p><span class="math inline">\(\epsilon = n \times 1\)</span></p></li>
</ul>
<p>Then, OLS estimates of coefficients are</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta}_{OLS} &amp;= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{Y}) \\
&amp;= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'(\mathbf{X \beta + \epsilon})) \\
&amp;= (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{X}) \beta + (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon}) \\
\hat{\beta}_{OLS} &amp; \to \beta + (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon})
\end{aligned}
\]</span></p>
<p>To have unbiased estimates, we have to get rid of the second part <span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon})\)</span></p>
<p>There are 2 conditions to achieve unbiased estimates:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(E(\epsilon |X) = 0\)</span> (This is easy, putting an intercept can solve this issue)</li>
<li>
<span class="math inline">\(Cov(\mathbf{X}, \epsilon) = 0\)</span> (This is the hard part)</li>
</ol>
<p>We only care about omitted variable</p>
<p>Usually, the problem will stem Omitted Variables Bias, but we only care about omitted variable bias when</p>
<ol style="list-style-type: decimal">
<li>Omitted variables correlate with the variables we care about (<span class="math inline">\(X\)</span>). If OMV does not correlate with <span class="math inline">\(X\)</span>, we don’t care, and random assignment makes this correlation goes to 0)</li>
<li>Omitted variables correlates with outcome/ dependent variable</li>
</ol>
<p>There are more types of endogeneity listed below.</p>
<p>Types of endogeneity (See <span class="citation">Hill et al. (<a href="references.html#ref-hill2021endogeneity">2021</a>)</span> for a review in management):</p>
<ol style="list-style-type: decimal">
<li><a href="endogeneity.html#endogenous-treatment">Endogenous Treatment</a></li>
</ol>
<ul>
<li>
<p>Omitted Variables Bias</p>
<ul>
<li>Motivation</li>
<li>Ability/talent</li>
<li>Self-selection</li>
</ul>
</li>
<li><p>Feedback Effect (<a href="endogeneity.html#simultaneity">Simultaneity</a>): also known as bidirectionality</p></li>
<li><p>Reverse Causality: Subtle difference from <a href="endogeneity.html#simultaneity">Simultaneity</a>: Technically, two variables affect each other sequentially, but in a big enough time frame, (e.g., monthly, or yearly), our coefficient will be biased just like simultaneity.</p></li>
<li><p><a href="endogeneity.html#measurement-error">Measurement Error</a></p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><a href="endogeneity.html#endogenous-sample-selection">Endogenous Sample Selection</a></li>
</ol>
<p>To deal with this problem, we have a toolbox (that has been mentioned in previous chapter <a href="#causal-inference"><strong>??</strong></a>)</p>
<p>Using control variables in regression is a “selection on observables” identification strategy.</p>
<p>In other words, if you believe you have an omitted variable, and you can measure it, including it in the regression model solves your problem. These uninterested variables are called control variables in your model.</p>
<p>However, this is rarely the case (because the problem is we don’t have their measurements). Hence, we need more elaborate methods:</p>
<ul>
<li><p><a href="endogeneity.html#endogenous-treatment">Endogenous Treatment</a></p></li>
<li><p><a href="endogeneity.html#endogenous-sample-selection">Endogenous Sample Selection</a></p></li>
</ul>
<p>Before we get to methods that deal with bias arises from omitted variables, we consider cases where we do have measurements of a variable, but there is measurement error (bias).</p>
<div id="endogenous-treatment" class="section level2" number="36.1">
<h2>
<span class="header-section-number">36.1</span> Endogenous Treatment<a class="anchor" aria-label="anchor" href="#endogenous-treatment"><i class="fas fa-link"></i></a>
</h2>
<div id="measurement-error" class="section level3" number="36.1.1">
<h3>
<span class="header-section-number">36.1.1</span> Measurement Error<a class="anchor" aria-label="anchor" href="#measurement-error"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<p>Data error can stem from</p>
<ul>
<li><p>Coding errors</p></li>
<li><p>Reporting errors</p></li>
</ul>
</li>
</ul>
<p>Two forms of measurement error:</p>
<ol style="list-style-type: decimal">
<li>Random (stochastic) (indeterminate error) (<a href="endogeneity.html#classical-measurement-errors">Classical Measurement Errors</a>): noise or measurement errors do not show up in a consistent or predictable way.</li>
<li>Systematic (determinate error) (<a href="endogeneity.html#non-classical-measurement-errors">Non-classical Measurement Errors</a>): When measurement error is consistent and predictable across observations.
<ol style="list-style-type: decimal">
<li>Instrument errors (e.g., faulty scale) -&gt; calibration or adjustment</li>
<li>Method errors (e.g., sampling errors) -&gt; better method development + study design</li>
<li>Human errors (e.g., judgement)</li>
</ol>
</li>
</ol>
<p>Usually the systematic measurement error is a bigger issue because it introduces “bias” into our estimates, while random error introduces noise into our estimates</p>
<ul>
<li>Noise -&gt; regression estimate to 0</li>
<li>Bias -&gt; can pull estimate to upward or downward.</li>
</ul>
<div id="classical-measurement-errors" class="section level4" number="36.1.1.1">
<h4>
<span class="header-section-number">36.1.1.1</span> Classical Measurement Errors<a class="anchor" aria-label="anchor" href="#classical-measurement-errors"><i class="fas fa-link"></i></a>
</h4>
<div id="right-hand-side" class="section level5" number="36.1.1.1.1">
<h5>
<span class="header-section-number">36.1.1.1.1</span> Right-hand side<a class="anchor" aria-label="anchor" href="#right-hand-side"><i class="fas fa-link"></i></a>
</h5>
<ul>
<li>Right-hand side measurement error: When the measurement is in the covariates, then we have the endogeneity problem.</li>
</ul>
<p>Say you know the true model is</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + u_i
\]</span></p>
<p>But you don’t observe <span class="math inline">\(X_i\)</span>, but you observe</p>
<p><span class="math display">\[
\tilde{X}_i = X_i + e_i
\]</span></p>
<p>which is known as classical measurement errors where we <strong>assume</strong> <span class="math inline">\(e_i\)</span> is uncorrelated with <span class="math inline">\(X_i\)</span> (i.e., <span class="math inline">\(E(X_i e_i) = 0\)</span>)</p>
<p>Then, when you estimate your observed variables, you have (substitute <span class="math inline">\(X_i\)</span> with <span class="math inline">\(\tilde{X}_i - e_i\)</span> ):</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 (\tilde{X}_i - e_i)+ u_i \\
&amp;= \beta_0 + \beta_1 \tilde{X}_i + u_i - \beta_1 e_i \\
&amp;= \beta_0 + \beta_1 \tilde{X}_i + v_i
\end{aligned}
\]</span></p>
<p>In words, the measurement error in <span class="math inline">\(X_i\)</span> is now a part of the error term in the regression equation <span class="math inline">\(v_i\)</span>. Hence, we have an endogeneity bias.</p>
<p>Endogeneity arises when</p>
<p><span class="math display">\[
\begin{aligned}
E(\tilde{X}_i v_i) &amp;= E((X_i + e_i )(u_i - \beta_1 e_i)) \\
&amp;= -\beta_1 Var(e_i) \neq 0
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\tilde{X}_i\)</span> and <span class="math inline">\(e_i\)</span> are positively correlated, then it leads to</p>
<ul>
<li><p>a negative bias in <span class="math inline">\(\hat{\beta}_1\)</span> if the true <span class="math inline">\(\beta_1\)</span> is positive</p></li>
<li><p>a positive bias if <span class="math inline">\(\beta_1\)</span> is negative</p></li>
</ul>
<p>In other words, measurement errors cause <strong>attenuation bias</strong>, which inter turn pushes the coefficient towards 0</p>
<p>As <span class="math inline">\(Var(e_i)\)</span> increases or <span class="math inline">\(\frac{Var(e_i)}{Var(\tilde{X})} \to 1\)</span> then <span class="math inline">\(e_i\)</span> is a random (noise) and <span class="math inline">\(\beta_1 \to 0\)</span> (random variable <span class="math inline">\(\tilde{X}\)</span> should not have any relation to <span class="math inline">\(Y_i\)</span>)</p>
<p>Technical note:</p>
<p>The size of the bias in the OLS-estimator is</p>
<p><span class="math display">\[
\hat{\beta}_{OLS} = \frac{ cov(\tilde{X}, Y)}{var(\tilde{X})} = \frac{cov(X + e, \beta X + u)}{var(X + e)}
\]</span></p>
<p>then</p>
<p><span class="math display">\[
plim \hat{\beta}_{OLS} = \beta \frac{\sigma^2_X}{\sigma^2_X + \sigma^2_e} = \beta \lambda
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is <strong>reliability</strong> or signal-to-total variance ratio or attenuation factor</p>
<p>Reliability affect the extent to which measurement error attenuates <span class="math inline">\(\hat{\beta}\)</span>. The attenuation bias is</p>
<p><span class="math display">\[
\hat{\beta}_{OLS} - \beta = -(1-\lambda)\beta
\]</span></p>
<p>Thus, <span class="math inline">\(\hat{\beta}_{OLS} &lt; \beta\)</span> (unless <span class="math inline">\(\lambda = 1\)</span>, in which case we don’t even have measurement error).</p>
<p>Note:</p>
<p><strong>Data transformation worsen (magnify) the measurement error</strong></p>
<p><span class="math display">\[
y= \beta x + \gamma x^2 + \epsilon
\]</span></p>
<p>then, the attenuation factor for <span class="math inline">\(\hat{\gamma}\)</span> is the square of the attenuation factor for <span class="math inline">\(\hat{\beta}\)</span> (i.e., <span class="math inline">\(\lambda_{\hat{\gamma}} = \lambda_{\hat{\beta}}^2\)</span>)</p>
<p><strong>Adding covariates increases attenuation bias</strong></p>
<p>To fix classical measurement error problem, we can</p>
<ol style="list-style-type: decimal">
<li>Find estimates of either <span class="math inline">\(\sigma^2_X, \sigma^2_\epsilon\)</span> or <span class="math inline">\(\lambda\)</span> from validation studies, or survey data.</li>
<li>
<a href="endogeneity.html#endogenous-treatment">Endogenous Treatment</a> Use instrument <span class="math inline">\(Z\)</span> correlated with <span class="math inline">\(X\)</span> but uncorrelated with <span class="math inline">\(\epsilon\)</span>
</li>
<li>Abandon your project</li>
</ol>
</div>
<div id="left-hand-side" class="section level5" number="36.1.1.1.2">
<h5>
<span class="header-section-number">36.1.1.1.2</span> Left-hand side<a class="anchor" aria-label="anchor" href="#left-hand-side"><i class="fas fa-link"></i></a>
</h5>
<p>When the measurement is in the outcome variable, econometricians or causal scientists do not care because they still have an unbiased estimate of the coefficients (the zero conditional mean assumption is not violated, hence we don’t have endogeneity). However, statisticians might care because it might inflate our uncertainty in the coefficient estimates (i.e., higher standard errors).</p>
<p><span class="math display">\[
\tilde{Y} = Y + v
\]</span></p>
<p>then the model you estimate is</p>
<p><span class="math display">\[
\tilde{Y} = \beta X + u + v
\]</span></p>
<p>Since <span class="math inline">\(v\)</span> is uncorrelated with <span class="math inline">\(X\)</span>, then <span class="math inline">\(\hat{\beta}\)</span> is consistently estimated by OLS</p>
<p>If we have measurement error in <span class="math inline">\(Y_i\)</span>, it will pass through <span class="math inline">\(\beta_1\)</span> and go to <span class="math inline">\(u_i\)</span></p>
</div>
</div>
<div id="non-classical-measurement-errors" class="section level4" number="36.1.1.2">
<h4>
<span class="header-section-number">36.1.1.2</span> Non-classical Measurement Errors<a class="anchor" aria-label="anchor" href="#non-classical-measurement-errors"><i class="fas fa-link"></i></a>
</h4>
<p>Relaxing the assumption that <span class="math inline">\(X\)</span> and <span class="math inline">\(\epsilon\)</span> are uncorrelated</p>
<p>Recall the true model we have true estimate is</p>
<p><span class="math display">\[
\hat{\beta} = \frac{cov(X + \epsilon, \beta X + u)}{var(X + \epsilon)}
\]</span></p>
<p>then without the above assumption, we have</p>
<p><span class="math display">\[
\begin{aligned}
plim \hat{\beta} &amp;= \frac{\beta (\sigma^2_X + \sigma_{X \epsilon})}{\sigma^2_X + \sigma^2_\epsilon + 2 \sigma_{X \epsilon}} \\
&amp;= (1 - \frac{\sigma^2_{\epsilon} + \sigma_{X \epsilon}}{\sigma^2_X + \sigma^2_\epsilon + 2 \sigma_{X \epsilon}}) \beta \\
&amp;= (1 - b_{\epsilon \tilde{X}}) \beta
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(b_{\epsilon \tilde{X}}\)</span> is the covariance between <span class="math inline">\(\tilde{X}\)</span> and <span class="math inline">\(\epsilon\)</span> (also the regression coefficient of a regression of <span class="math inline">\(\epsilon\)</span> on <span class="math inline">\(\tilde{X}\)</span>)</p>
<p>Hence, the <a href="endogeneity.html#classical-measurement-errors">Classical Measurement Errors</a> is just a special case of <a href="endogeneity.html#non-classical-measurement-errors">Non-classical Measurement Errors</a> where <span class="math inline">\(b_{\epsilon \tilde{X}} = 1 - \lambda\)</span></p>
<p>So when <span class="math inline">\(\sigma_{X \epsilon} = 0\)</span> (<a href="endogeneity.html#classical-measurement-errors">Classical Measurement Errors</a>), increasing this covariance <span class="math inline">\(b_{\epsilon \tilde{X}}\)</span> increases the covariance increases the attenuation factor if more than half of the variance in <span class="math inline">\(\tilde{X}\)</span> is measurement error, and decreases the attenuation factor otherwise. This is also known as <strong>mean reverting measurement error</strong> <span class="citation">Bound, Brown, and Mathiowetz (<a href="references.html#ref-bound2001measurement">2001</a>)</span></p>
<p>A general framework for both right-hand side and left-hand side measurement error is <span class="citation">(<a href="references.html#ref-bound2001measurement">Bound, Brown, and Mathiowetz 2001</a>)</span>:</p>
<p>consider the true model</p>
<p><span class="math display">\[
\mathbf{Y = X \beta + \epsilon}
\]</span></p>
<p>then</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta} &amp;= \mathbf{(\tilde{X}' \tilde{X})^{-1}\tilde{X} \tilde{Y}} \\
&amp;= \mathbf{(\tilde{X}' \tilde{X})^{-1} \tilde{X}' (\tilde{X} \beta - U \beta + v + \epsilon )} \\
&amp;= \mathbf{\beta + (\tilde{X}' \tilde{X})^{-1} \tilde{X}' (-U \beta + v + \epsilon)} \\
plim \hat{\beta} &amp;= \beta + plim (\tilde{X}' \tilde{X})^{-1} \tilde{X}' ( -U\beta + v) \\
&amp;= \beta + plim (\tilde{X}' \tilde{X})^{-1} \tilde{X}' W
\left[
\begin{array}
{c}
- \beta \\
1
\end{array}
\right]
\end{aligned}
\]</span></p>
<p>Since we collect the measurement errors in a matrix <span class="math inline">\(W = [U|v]\)</span>, then</p>
<p><span class="math display">\[
( -U\beta + v) = W
\left[
\begin{array}
{c}
- \beta \\
1
\end{array}
\right]
\]</span></p>
<p>Hence, in general, biases in the coefficients <span class="math inline">\(\beta\)</span> are regression coefficients from regressing the measurement errors on the mis-measured <span class="math inline">\(\tilde{X}\)</span></p>
<p>Notes:</p>
<ul>
<li><p><a href="endogeneity.html#instrumental-variable">Instrumental Variable</a> can help fix this problem</p></li>
<li><p>There can also be measurement error in dummy variables and you can still use <a href="endogeneity.html#instrumental-variable">Instrumental Variable</a> to fix it.</p></li>
</ul>
</div>
<div id="solution-to-measurement-errors" class="section level4" number="36.1.1.3">
<h4>
<span class="header-section-number">36.1.1.3</span> Solution to Measurement Errors<a class="anchor" aria-label="anchor" href="#solution-to-measurement-errors"><i class="fas fa-link"></i></a>
</h4>
<div id="correlation" class="section level5" number="36.1.1.3.1">
<h5>
<span class="header-section-number">36.1.1.3.1</span> Correlation<a class="anchor" aria-label="anchor" href="#correlation"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
\begin{aligned}
P(\rho | data) &amp;= \frac{P(data|\rho)P(\rho)}{P(data)} \\
\text{Posterior Probability} &amp;\propto \text{Likelihood} \times \text{Prior Probability}
\end{aligned}
\]</span> where</p>
<ul>
<li>
<span class="math inline">\(\rho\)</span> is a correlation coefficient</li>
<li>
<span class="math inline">\(P(data|\rho)\)</span> is the likelihood function evaluated at <span class="math inline">\(\rho\)</span>
</li>
<li>
<span class="math inline">\(P(\rho)\)</span> prior probability</li>
<li>
<span class="math inline">\(P(data)\)</span> is the normalizing constant</li>
</ul>
<p>With sample correlation coefficient <span class="math inline">\(r\)</span>:</p>
<p><span class="math display">\[
r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
\]</span> Then the posterior density approximation of <span class="math inline">\(\rho\)</span> is <span class="citation">(<a href="references.html#ref-schisterman2003estimation">Schisterman et al. 2003, 3</a>)</span></p>
<p><span class="math display">\[
P(\rho| x, y)  \propto P(\rho) \frac{(1- \rho^2)^{(n-1)/2}}{(1- \rho \times r)^{n - (3/2)}}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\rho = \tanh \xi\)</span> where <span class="math inline">\(\xi \sim N(z, 1/n)\)</span>
</li>
<li><span class="math inline">\(r = \tanh z\)</span></li>
</ul>
<p>Then the posterior density follow a normal distribution where</p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu_{posterior} = \sigma^2_{posterior} \times (n_{prior} \times \tanh^{-1} r_{prior}+ n_{likelihood} \times \tanh^{-1} r_{likelihood})
\]</span></p>
<p><strong>variance</strong></p>
<p><span class="math display">\[
\sigma^2_{posterior} = \frac{1}{n_{prior} + n_{Likelihood}}
\]</span></p>
<p>To simplify the integration process, we choose prior that is</p>
<p><span class="math display">\[
P(\rho) \propto (1 - \rho^2)^c
\]</span> where</p>
<ul>
<li>
<span class="math inline">\(c\)</span> is the weight the prior will have in estimation (i.e., <span class="math inline">\(c = 0\)</span> if no prior info, hence <span class="math inline">\(P(\rho) \propto 1\)</span>)</li>
</ul>
<p>Example:</p>
<p>Current study: <span class="math inline">\(r_{xy} = 0.5, n = 200\)</span></p>
<p>Previous study: <span class="math inline">\(r_{xy} = 0.2765, (n=50205)\)</span></p>
<p>Combining two, we have the posterior following a normal distribution with the <strong>variance</strong> of</p>
<p><span class="math display">\[
\sigma^2_{posterior} =  \frac{1}{n_{prior} + n_{Likelihood}} = \frac{1}{200 + 50205} = 0.0000198393
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mu_{Posterior} &amp;= \sigma^2_{Posterior}  \times (n_{prior} \times \tanh^{-1} r_{prior}+ n_{likelihood} \times \tanh^{-1} r_{likelihood}) \\
&amp;= 0.0000198393 \times (50205 \times \tanh^{-1} 0.2765 + 200 \times \tanh^{-1}0.5 )\\
&amp;= 0.2849415
\end{aligned}
\]</span></p>
<p>Hence, <span class="math inline">\(Posterior \sim N(0.691, 0.0009)\)</span>, which means the correlation coefficient is <span class="math inline">\(\tanh(0.691) = 0.598\)</span> and 95% CI is</p>
<p><span class="math display">\[
\mu_{posterior} \pm 1.96 \times \sqrt{\sigma^2_{Posterior}} = 0.2849415 \pm 1.96 \times (0.0000198393)^{1/2} = (0.2762115, 0.2936714)
\]</span></p>
<p>Hence, the interval for posterior <span class="math inline">\(\rho\)</span> is <span class="math inline">\((0.2693952, 0.2855105)\)</span></p>
<p>If future authors suspect that they have</p>
<ol style="list-style-type: decimal">
<li>Large sampling variation</li>
<li>Measurement error in either measures in the correlation, which attenuates the relationship between the two variables</li>
</ol>
<p>Applying this Bayesian correction can give them a better estimate of the correlation between the two.</p>
<p>To implement this calculation in R, see below</p>
<div class="sourceCode" id="cb947"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n_new</span>              <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">r_new</span>              <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">alpha</span>              <span class="op">&lt;-</span> <span class="fl">0.05</span></span>
<span></span>
<span><span class="va">update_correlation</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n_new</span>, <span class="va">r_new</span>, <span class="va">alpha</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">n_meta</span>             <span class="op">&lt;-</span> <span class="fl">50205</span></span>
<span>    <span class="va">r_meta</span>             <span class="op">&lt;-</span> <span class="fl">0.2765</span></span>
<span>    </span>
<span>    <span class="co"># Variance</span></span>
<span>    <span class="va">var_xi</span>         <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="va">n_new</span> <span class="op">+</span> <span class="va">n_meta</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/format.html">format</a></span><span class="op">(</span><span class="va">var_xi</span>, scientific <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># mean</span></span>
<span>    <span class="va">mu_xi</span>          <span class="op">&lt;-</span> <span class="va">var_xi</span> <span class="op">*</span> <span class="op">(</span><span class="va">n_meta</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">atanh</a></span><span class="op">(</span><span class="va">r_meta</span><span class="op">)</span> <span class="op">+</span> <span class="va">n_new</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">atanh</a></span><span class="op">(</span><span class="va">r_new</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/format.html">format</a></span><span class="op">(</span><span class="va">mu_xi</span>, scientific  <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># confidence interval</span></span>
<span>    <span class="va">upper_xi</span>       <span class="op">&lt;-</span> <span class="va">mu_xi</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_xi</span><span class="op">)</span></span>
<span>    <span class="va">lower_xi</span>       <span class="op">&lt;-</span> <span class="va">mu_xi</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_xi</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># rho</span></span>
<span>    <span class="va">mean_rho</span>       <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">tanh</a></span><span class="op">(</span><span class="va">mu_xi</span><span class="op">)</span></span>
<span>    <span class="va">upper_rho</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">tanh</a></span><span class="op">(</span><span class="va">upper_xi</span><span class="op">)</span></span>
<span>    <span class="va">lower_rho</span>      <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">tanh</a></span><span class="op">(</span><span class="va">lower_xi</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># return a list</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>            <span class="st">"mu_xi"</span> <span class="op">=</span> <span class="va">mu_xi</span>,</span>
<span>            <span class="st">"var_xi"</span> <span class="op">=</span> <span class="va">var_xi</span>,</span>
<span>            <span class="st">"upper_xi"</span> <span class="op">=</span> <span class="va">upper_xi</span>,</span>
<span>            <span class="st">"lower_xi"</span> <span class="op">=</span> <span class="va">lower_xi</span>,</span>
<span>            <span class="st">"mean_rho"</span> <span class="op">=</span> <span class="va">mean_rho</span>,</span>
<span>            <span class="st">"upper_rho"</span> <span class="op">=</span> <span class="va">upper_rho</span>,</span>
<span>            <span class="st">"lower_rho"</span> <span class="op">=</span> <span class="va">lower_rho</span></span>
<span>        <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span></span>
<span></span>
<span></span>
<span><span class="co"># Old confidence interval</span></span>
<span><span class="va">r_new</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="va">n_new</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.6385904</span></span>
<span><span class="va">r_new</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="va">n_new</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.3614096</span></span>
<span></span>
<span><span class="va">testing</span> <span class="op">=</span> <span class="fu">update_correlation</span><span class="op">(</span>n_new <span class="op">=</span> <span class="va">n_new</span>, r_new <span class="op">=</span> <span class="va">r_new</span>, alpha <span class="op">=</span> <span class="va">alpha</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Updated rho</span></span>
<span><span class="va">testing</span><span class="op">$</span><span class="va">mean_rho</span></span>
<span><span class="co">#&gt; [1] 0.2774723</span></span>
<span></span>
<span><span class="co"># Updated confidence interval</span></span>
<span><span class="va">testing</span><span class="op">$</span><span class="va">upper_rho</span></span>
<span><span class="co">#&gt; [1] 0.2855105</span></span>
<span><span class="va">testing</span><span class="op">$</span><span class="va">lower_rho</span></span>
<span><span class="co">#&gt; [1] 0.2693952</span></span></code></pre></div>
</div>
</div>
</div>
<div id="simultaneity" class="section level3" number="36.1.2">
<h3>
<span class="header-section-number">36.1.2</span> Simultaneity<a class="anchor" aria-label="anchor" href="#simultaneity"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>When independent variables (<span class="math inline">\(X\)</span>’s) are jointly determined with the dependent variable <span class="math inline">\(Y\)</span>, typically through an equilibrium mechanism, violates the second condition for causality (i.e., temporal order).</p></li>
<li><p>Examples: quantity and price by demand and supply, investment and productivity, sales and advertisement</p></li>
</ul>
<p>General Simultaneous (Structural) Equations</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 X_i + u_i \\
X_i &amp;= \alpha_0 + \alpha_1 Y_i + v_i
\end{aligned}
\]</span></p>
<p>Hence, the solutions are</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp;= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 v_i + u_i}{1 - \alpha_1 \beta_1} \\
X_i &amp;= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}
\end{aligned}
\]</span></p>
<p>If we run only one regression, we will have biased estimators (because of <strong>simultaneity bias</strong>):</p>
<p><span class="math display">\[
\begin{aligned}
Cov(X_i, u_i) &amp;= Cov(\frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}, u_i) \\
&amp;= \frac{\alpha_1}{1- \alpha_1 \beta_1} Var(u_i)
\end{aligned}
\]</span></p>
<p>In an even more general model</p>
<p><span class="math display">\[
\begin{cases}
Y_i = \beta_0 + \beta_1 X_i + \beta_2 T_i + u_i \\
X_i = \alpha_0 + \alpha_1 Y_i + \alpha_2 Z_i + v_i
\end{cases}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(X_i, Y_i\)</span> are <strong>endogenous</strong> variables determined within the system</p></li>
<li><p><span class="math inline">\(T_i, Z_i\)</span> are <strong>exogenous</strong> variables</p></li>
</ul>
<p>Then, the reduced form of the model is</p>
<p><span class="math display">\[
\begin{cases}
\begin{aligned}
Y_i &amp;= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 \alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{u}_i \\
&amp;= B_0 + B_1 Z_i + B_2 T_i + \tilde{u}_i
\end{aligned}
\\
\begin{aligned}
X_i &amp;= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{\alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\alpha_1\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{v}_i \\
&amp;= A_0 + A_1 Z_i + A_2 T_i + \tilde{v}_i
\end{aligned}
\end{cases}
\]</span></p>
<p>Then, now we can get consistent estimates of the reduced form parameters</p>
<p>And to get the original parameter estimates</p>
<p><span class="math display">\[
\begin{aligned}
\frac{B_1}{A_1} &amp;= \beta_1 \\
B_2 (1 - \frac{B_1 A_2}{A_1B_2}) &amp;= \beta_2 \\
\frac{A_2}{B_2} &amp;= \alpha_1 \\
A_1 (1 - \frac{B_1 A_2}{A_1 B_2}) &amp;= \alpha_2
\end{aligned}
\]</span></p>
<p>Rules for Identification</p>
<p><strong>Order Condition</strong> (necessary but not sufficient)</p>
<p><span class="math display">\[
K - k \ge m - 1
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(M\)</span> = number of endogenous variables in the model</p></li>
<li><p>K = number of exogenous variables int he model</p></li>
<li><p><span class="math inline">\(m\)</span> = number of endogenous variables in a given</p></li>
<li><p><span class="math inline">\(k\)</span> = is the number of exogenous variables in a given equation</p></li>
</ul>
<p>This is actually the general framework for instrumental variables</p>
</div>
<div id="endogenous-treatment-solutions" class="section level3" number="36.1.3">
<h3>
<span class="header-section-number">36.1.3</span> Endogenous Treatment Solutions<a class="anchor" aria-label="anchor" href="#endogenous-treatment-solutions"><i class="fas fa-link"></i></a>
</h3>
<p>Using the OLS estimates as a reference point</p>
<div class="sourceCode" id="cb948"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/mmeierer/REndo">REndo</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">421</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"CASchools"</span><span class="op">)</span></span>
<span><span class="va">school</span> <span class="op">&lt;-</span> <span class="va">CASchools</span></span>
<span><span class="va">school</span><span class="op">$</span><span class="va">stratio</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">CASchools</span>, <span class="va">students</span> <span class="op">/</span> <span class="va">teachers</span><span class="op">)</span></span>
<span><span class="va">m1.ols</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">read</span> <span class="op">~</span> <span class="va">stratio</span> <span class="op">+</span> <span class="va">english</span> <span class="op">+</span> <span class="va">lunch</span> </span>
<span>       <span class="op">+</span> <span class="va">grades</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">calworks</span> <span class="op">+</span> <span class="va">county</span>,</span>
<span>       data <span class="op">=</span> <span class="va">school</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m1.ols</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>,<span class="op">]</span></span>
<span><span class="co">#&gt;                 Estimate Std. Error     t value      Pr(&gt;|t|)</span></span>
<span><span class="co">#&gt; (Intercept) 683.45305948 9.56214469  71.4748711 3.011667e-218</span></span>
<span><span class="co">#&gt; stratio      -0.30035544 0.25797023  -1.1643027  2.450536e-01</span></span>
<span><span class="co">#&gt; english      -0.20550107 0.03765408  -5.4576041  8.871666e-08</span></span>
<span><span class="co">#&gt; lunch        -0.38684059 0.03700982 -10.4523759  1.427370e-22</span></span>
<span><span class="co">#&gt; gradesKK-08  -1.91291321 1.35865394  -1.4079474  1.599886e-01</span></span>
<span><span class="co">#&gt; income        0.71615378 0.09832843   7.2832829  1.986712e-12</span></span>
<span><span class="co">#&gt; calworks     -0.05273312 0.06154758  -0.8567863  3.921191e-01</span></span></code></pre></div>
<div id="instrumental-variable" class="section level4" number="36.1.3.1">
<h4>
<span class="header-section-number">36.1.3.1</span> Instrumental Variable<a class="anchor" aria-label="anchor" href="#instrumental-variable"><i class="fas fa-link"></i></a>
</h4>
<p>[A3a] requires <span class="math inline">\(\epsilon_i\)</span> to be uncorrelated with <span class="math inline">\(\mathbf{x}_i\)</span></p>
<p>Assume <a href="linear-regression.html#a1-linearity">A1</a> , <a href="linear-regression.html#a2-full-rank">A2</a>, <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a></p>
<p><span class="math display">\[
plim(\hat{\beta}_{OLS}) = \beta + [E(\mathbf{x_i'x_i})]^{-1}E(\mathbf{x_i'}\epsilon_i)
\]</span></p>
<p>[A3a] is the weakest assumption needed for OLS to be <strong>consistent</strong></p>
<p><a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> fails when <span class="math inline">\(x_{ik}\)</span> is correlated with <span class="math inline">\(\epsilon_i\)</span></p>
<ul>
<li>Omitted Variables Bias: <span class="math inline">\(\epsilon_i\)</span> includes any other factors that may influence the dependent variable (linearly)</li>
<li>
<a href="endogeneity.html#simultaneity">Simultaneity</a> Demand and prices are simultaneously determined.</li>
<li>
<a href="endogeneity.html#endogenous-sample-selection">Endogenous Sample Selection</a> we did not have iid sample</li>
<li><a href="endogeneity.html#measurement-error">Measurement Error</a></li>
</ul>
<p><strong>Note</strong></p>
<ul>
<li>Omitted Variable: an omitted variable is a variable, omitted from the model (but is in the <span class="math inline">\(\epsilon_i\)</span>) and unobserved has predictive power towards the outcome.</li>
<li>Omitted Variable Bias: is the bias (and inconsistency when looking at large sample properties) of the OLS estimator when the omitted variable.</li>
<li>We cam have both positive and negative selection bias (it depends on what our story is)</li>
</ul>
<p>The <strong>structural equation</strong> is used to emphasize that we are interested understanding a <strong>causal relationship</strong></p>
<p><span class="math display">\[
y_{i1} = \beta_0 + \mathbf{z}_i1 \beta_1 + y_{i2}\beta_2 +  \epsilon_i
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(y_{it}\)</span> is the outcome variable (inherently correlated with <span class="math inline">\(\epsilon_i\)</span>)</li>
<li>
<span class="math inline">\(y_{i2}\)</span> is the endogenous covariate (presumed to be correlated with <span class="math inline">\(\epsilon_i\)</span>)</li>
<li>
<span class="math inline">\(\beta_1\)</span> represents the causal effect of <span class="math inline">\(y_{i2}\)</span> on <span class="math inline">\(y_{i1}\)</span>
</li>
<li>
<span class="math inline">\(\mathbf{z}_{i1}\)</span> is exogenous controls (uncorrelated with <span class="math inline">\(\epsilon_i\)</span>) (<span class="math inline">\(E(z_{1i}'\epsilon_i) = 0\)</span>)</li>
</ul>
<p>OLS is an inconsistent estimator of the causal effect <span class="math inline">\(\beta_2\)</span></p>
<p>If there was no endogeneity</p>
<ul>
<li><span class="math inline">\(E(y_{i2}'\epsilon_i) = 0\)</span></li>
<li>the exogenous variation in <span class="math inline">\(y_{i2}\)</span> is what identifies the causal effect</li>
</ul>
<p>If there is endogeneity</p>
<ul>
<li>Any wiggle in <span class="math inline">\(y_{i2}\)</span> will shift simultaneously with <span class="math inline">\(\epsilon_i\)</span>
</li>
</ul>
<p><span class="math display">\[
plim(\hat{\beta}_{OLS}) = \beta + [E(\mathbf{x'_ix_i})]^{-1}E(\mathbf{x'_i}\epsilon_i)
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\beta\)</span> is the causal effect</li>
<li>
<span class="math inline">\([E(\mathbf{x'_ix_i})]^{-1}E(\mathbf{x'_i}\epsilon_i)\)</span> is the endogenous effect</li>
</ul>
<p>Hence <span class="math inline">\(\hat{\beta}_{OLS}\)</span> can be either more positive and negative than the true causal effect.</p>
<p>Motivation for <strong>Two Stage Least Squares (2SLS)</strong></p>
<p><span class="math display">\[
y_{i1}=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i
\]</span></p>
<p>We want to understand how movement in <span class="math inline">\(y_{i2}\)</span> effects movement in <span class="math inline">\(y_{i1}\)</span>, but whenever we move <span class="math inline">\(y_{i2}\)</span>, <span class="math inline">\(\epsilon_i\)</span> also moves.</p>
<p><strong>Solution</strong><br>
We need a way to move <span class="math inline">\(y_{i2}\)</span> independently of <span class="math inline">\(\epsilon_i\)</span>, then we can analyze the response in <span class="math inline">\(y_{i1}\)</span> as a causal effect</p>
<ul>
<li>
<p>Find an <strong>instrumental variable(s)</strong> <span class="math inline">\(z_{i2}\)</span></p>
<ul>
<li>Instrument <strong>Relevance</strong>: when** <span class="math inline">\(z_{i2}\)</span> moves then <span class="math inline">\(y_{i2}\)</span> also moves</li>
<li>Instrument <strong>Exogeneity</strong>: when <span class="math inline">\(z_{i2}\)</span> moves then <span class="math inline">\(\epsilon_i\)</span> does not move.</li>
</ul>
</li>
<li><p><span class="math inline">\(z_{i2}\)</span> is the <strong>exogenous variation that identifies</strong> the causal effect <span class="math inline">\(\beta_2\)</span></p></li>
</ul>
<p>Finding an Instrumental variable:</p>
<ul>
<li>Random Assignment: + Effect of class size on educational outcomes: instrument is initial random</li>
<li>Relation’s Choice + Effect of Education on Fertility: instrument is parent’s educational level</li>
<li>Eligibility + Trade-off between IRA and 401K retirement savings: instrument is 401k eligibility</li>
</ul>
<p><strong>Example</strong></p>
<p>Return to College</p>
<ul>
<li><p>education is correlated with ability - endogenous</p></li>
<li>
<p><strong>Near 4year</strong> as an instrument</p>
<ul>
<li>Instrument Relevance: when <strong>near</strong> moves then education also moves</li>
<li>Instrument Exogeneity: when <strong>near</strong> moves then <span class="math inline">\(\epsilon_i\)</span> does not move.</li>
</ul>
</li>
<li><p>Other potential instruments; near a 2-year college. Parent’s Education. Owning Library Card</p></li>
</ul>
<p><span class="math display">\[
y_{i1}=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i
\]</span></p>
<p>First Stage (Reduced Form) Equation:</p>
<p><span class="math display">\[
y_{i2} = \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2}\)</span> is exogenous variation <span class="math inline">\(v_i\)</span> is endogenous variation</li>
</ul>
<p>This is called a <strong>reduced form equation</strong></p>
<ul>
<li><p>Not interested in the causal interpretation of <span class="math inline">\(\pi_1\)</span> or <span class="math inline">\(\pi_2\)</span></p></li>
<li><p>A linear projection of <span class="math inline">\(z_{i1}\)</span> and <span class="math inline">\(z_{i2}\)</span> on <span class="math inline">\(y_{i2}\)</span> (simple correlations)</p></li>
<li><p>The projections <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> guarantee that <span class="math inline">\(E(z_{i1}'v_i)=0\)</span> and <span class="math inline">\(E(z_{i2}'v_i)=0\)</span></p></li>
</ul>
<p>Instrumental variable <span class="math inline">\(z_{i2}\)</span></p>
<ul>
<li>
<strong>Instrument Relevance</strong>: <span class="math inline">\(\pi_2 \neq 0\)</span>
</li>
<li>
<strong>Instrument Exogeneity</strong>: <span class="math inline">\(E(\mathbf{z_{i2}\epsilon_i})=0\)</span>
</li>
</ul>
<p>Moving only the exogenous part of <span class="math inline">\(y_i2\)</span> is moving</p>
<p><span class="math display">\[
\tilde{y}_{i2} = \pi_0 + \mathbf{z_{i1}\pi_1 + z_{i2}\pi_2}
\]</span></p>
<p><strong>two Stage Least Squares (2SLS)</strong></p>
<p><span class="math display">\[
y_{i1} = \beta_0 +\mathbf{z_{i1}\beta_1}+ y_{i2}\beta_2 + \epsilon_i
\]</span></p>
<p><span class="math display">\[
y_{i2} = \pi_0 + \mathbf{z_{i2}\pi_2} + \mathbf{v_i}
\]</span></p>
<p>Equivalently,</p>
<span class="math display" id="eq:2SLS">\[\begin{equation}
\begin{split}
y_{i1} = \beta_0 + \mathbf{z_{i1}}\beta_1 + \tilde{y}_{i2}\beta_2 + u_i
\end{split}
\tag{36.1}
\end{equation}\]</span>
<p>where</p>
<ul>
<li><span class="math inline">\(\tilde{y}_{i2} =\pi_0 + \mathbf{z_{i2}\pi_2}\)</span></li>
<li><span class="math inline">\(u_i = v_i \beta_2+ \epsilon_i\)</span></li>
</ul>
<p>The <a href="endogeneity.html#eq:2SLS">(36.1)</a> holds for <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a></p>
<ul>
<li>
<a href="linear-regression.html#a2-full-rank">A2</a> holds if the instrument is relevant <span class="math inline">\(\pi_2 \neq 0\)</span> + <span class="math inline">\(y_{i1} = \beta_0 + \mathbf{z_{i1}\beta_1 + (\pi_0 + z_{i1}\pi_1 + z_{i2}\pi_2)}\beta_2 + u_i\)</span>
</li>
<li>[A3a] holds if the instrument is exogenous <span class="math inline">\(E(\mathbf{z}_{i2}\epsilon_i)=0\)</span>
</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
E(\tilde{y}_{i2}'u_i) &amp;= E((\pi_0 + \mathbf{z_{i1}\pi_1+z_{i2}})(v_i\beta_2 + \epsilon_i)) \\
&amp;= E((\pi_0 + \mathbf{z_{i1}\pi_1+z_{i2}})( \epsilon_i)) \\
&amp;= E(\epsilon_i)\pi_0 + E(\epsilon_iz_{i1})\pi_1 + E(\epsilon_iz_{i2}) \\
&amp;=0
\end{aligned}
\]</span></p>
<p>Hence, <a href="endogeneity.html#eq:2SLS">(36.1)</a> is consistent</p>
<p>The 2SLS Estimator<br>
1. Estimate the first stage using <a href="linear-regression.html#ordinary-least-squares">OLS</a></p>
<p><span class="math display">\[
y_{i2} = \pi_0 + \mathbf{z_{i2}\pi_2} + \mathbf{v_i}
\]</span></p>
<p>and obtained estimated value <span class="math inline">\(\hat{y}_{i2}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Estimate the altered equation using <a href="linear-regression.html#ordinary-least-squares">OLS</a>
</li>
</ol>
<p><span class="math display">\[
y_{i1} = \beta_0 +\mathbf{z_{i1}\beta_1}+ \hat{y}_{i2}\beta_2 + \epsilon_i
\]</span></p>
<p><strong>Properties of the 2SLS Estimator</strong></p>
<ul>
<li>Under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, [A3a] (for <span class="math inline">\(z_{i1}\)</span>), <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a> and if the instrument satisfies the following two conditions, + <strong>Instrument Relevance</strong>: <span class="math inline">\(\pi_2 \neq 0\)</span> + <strong>Instrument Exogeneity</strong>: <span class="math inline">\(E(\mathbf{z}_{i2}'\epsilon_i) = 0\)</span> then the 2SLS estimator is consistent</li>
<li>Can handle more than one endogenous variable and more than one instrumental variable</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
y_{i1} &amp;= \beta_0 + z_{i1}\beta_1 + y_{i2}\beta_2 + y_{i3}\beta_3 + \epsilon_i \\
y_{i2} &amp;= \pi_0 + z_{i1}\pi_1 + z_{i2}\pi_2 + z_{i3}\pi_3 + z_{i4}\pi_4 + v_{i2} \\
y_{i3} &amp;= \gamma_0 + z_{i1}\gamma_1 + z_{i2}\gamma_2 + z_{i3}\gamma_3 + z_{i4}\gamma_4 + v_{i3}
\end{aligned}
\]</span></p>
<pre><code>    + **IV estimator**: one endogenous variable with a single instrument 
    + **2SLS estimator**: one endogenous variable with multiple instruments 
    + **GMM estimator**: multiple endogenous variables with multiple instruments
    </code></pre>
<ul>
<li>
<p>Standard errors produced in the second step are not correct</p>
<ul>
<li>Because we do not know <span class="math inline">\(\tilde{y}\)</span> perfectly and need to estimate it in the firs step, we are introducing additional variation</li>
<li>We did not have this problem with <a href="linear-regression.html#feasible-generalized-least-squares">FGLS</a> because “the first stage was orthogonal to the second stage.” This is generally not true for most multi-step procedure.<br>
</li>
<li>If <a href="linear-regression.html#a4-homoskedasticity">A4</a> does not hold, need to report robust standard errors.</li>
</ul>
</li>
<li>
<p>2SLS is less efficient than OLS and will always have larger standard errors.<br></p>
<ul>
<li>First, <span class="math inline">\(Var(u_i) = Var(v_i\beta_2 + \epsilon_i) &gt; Var(\epsilon_i)\)</span><br>
</li>
<li>Second, <span class="math inline">\(\hat{y}_{i2}\)</span> is generally highly collinear with <span class="math inline">\(\mathbf{z}_{i1}\)</span>
</li>
</ul>
</li>
<li><p>The number of instruments need to be at least as many or more the number of endogenous variables.</p></li>
</ul>
<p><strong>Note</strong></p>
<ul>
<li>2SLS can be combined with <a href="linear-regression.html#feasible-generalized-least-squares">FGLS</a> to make the estimator more efficient: You have the same first-stage, and in the second-stage, instead of using OLS, you can use FLGS with the weight matrix <span class="math inline">\(\hat{w}\)</span>
</li>
<li>Generalized Method of Moments can be more efficient than 2SLS.</li>
<li>In the second-stage of 2SLS, you can also use <a href="linear-regression.html#maximum-likelihood-estimator">MLE</a>, but then you are making assumption on the distribution of the outcome variable, the endogenous variable, and their relationship (joint distribution).</li>
</ul>
<div id="testing-assumptions-1" class="section level5" number="36.1.3.1.1">
<h5>
<span class="header-section-number">36.1.3.1.1</span> Testing Assumptions<a class="anchor" aria-label="anchor" href="#testing-assumptions-1"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li><p><a href="endogeneity.html#endogeneity-test">Endogeneity Test</a>: Is <span class="math inline">\(y_{i2}\)</span> truly endogenous (i.e., can we just use OLS instead of 2SLS)?</p></li>
<li><p><a href="endogeneity.html#exogeneity">Exogeneity</a> (Cannot always test and when you can it might not be informative)</p></li>
<li><p><a href="endogeneity.html#relevancy">Relevancy</a> (need to avoid “weak instruments”)</p></li>
</ol>
<div id="endogeneity-test" class="section level6" number="36.1.3.1.1.1">
<h6>
<span class="header-section-number">36.1.3.1.1.1</span> Endogeneity Test<a class="anchor" aria-label="anchor" href="#endogeneity-test"><i class="fas fa-link"></i></a>
</h6>
<ul>
<li><p>2SLS is generally so inefficient that we may prefer OLS if there is not much endogeneity</p></li>
<li><p>Biased but inefficient vs efficient but biased</p></li>
<li>
<p>Want a sense of “how endogenous” <span class="math inline">\(y_{i2}\)</span> is</p>
<ul>
<li>if “very” endogenous - should use 2SLS</li>
<li>if not “very” endogenous - perhaps prefer OLS</li>
</ul>
</li>
</ul>
<p><strong>Invalid</strong> Test of Endogeneity: <span class="math inline">\(y_{i2}\)</span> is endogenous if it is correlated with <span class="math inline">\(\epsilon_i\)</span>,</p>
<p><span class="math display">\[
\epsilon_i = \gamma_0 + y_{i2}\gamma_1 + error_i
\]</span></p>
<p>where <span class="math inline">\(\gamma_1 \neq 0\)</span> implies that there is endogeneity</p>
<ul>
<li>
<span class="math inline">\(\epsilon_i\)</span> is not observed, but using the residuals</li>
</ul>
<p><span class="math display">\[
e_i = \gamma_0 + y_{i2}\gamma_1 + error_i
\]</span></p>
<p>is <strong>NOT</strong> a valid test of endogeneity + The OLS residual, e is mechanically uncorrelated with <span class="math inline">\(y_{i2}\)</span> (by FOC for OLS) + In every situation, <span class="math inline">\(\gamma_1\)</span> will be essentially 0 and you will never be able to reject the null of no endogeneity</p>
<p><strong>Valid</strong> test of endogeneity</p>
<ul>
<li>If <span class="math inline">\(y_{i2}\)</span> is not endogenous then <span class="math inline">\(\epsilon_i\)</span> and v are uncorrelated</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
y_{i1} &amp;= \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &amp;= \pi_0 + \mathbf{z}_{i1}\pi_1 + z_{i2}\pi_2 + v_i
\end{aligned}
\]</span></p>
<p><strong>Variable Addition test</strong>: include the first stage residuals as an additional variable,</p>
<p><span class="math display">\[
y_{i1} = \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \hat{v}_i \theta + error_i
\]</span></p>
<p>Then the usual <span class="math inline">\(t\)</span>-test of significance is a valid test to evaluate the following hypothesis. <strong>note</strong> this test requires your instrument to be valid instrument.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \theta = 0 &amp; \text{  (not endogenous)} \\
&amp;H_1: \theta \neq 0 &amp; \text{  (endogenous)}
\end{aligned}
\]</span></p>
</div>
<div id="exogeneity" class="section level6" number="36.1.3.1.1.2">
<h6>
<span class="header-section-number">36.1.3.1.1.2</span> Exogeneity<a class="anchor" aria-label="anchor" href="#exogeneity"><i class="fas fa-link"></i></a>
</h6>
<p>Why exogeneity matter?</p>
<p><span class="math display">\[
E(\mathbf{z}_{i2}'\epsilon_i) = 0
\]</span></p>
<ul>
<li>If [A3a] fails - 2SLS is also inconsistent</li>
<li>If instrument is not exogenous, then we need to find a new one.</li>
<li>Similar to <a href="endogeneity.html#endogeneity-test">Endogeneity Test</a>, when there is a single instrument</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
e_i &amp;= \gamma_0 + \mathbf{z}_{i2}\gamma_1 + error_i \\
H_0: \gamma_1 &amp;= 0
\end{aligned}
\]</span></p>
<p>is <strong>NOT</strong> a valid test of endogeneity</p>
<ul>
<li>the OLS residual, e is mechanically uncorrelated with <span class="math inline">\(z_{i2}\)</span>: <span class="math inline">\(\hat{\gamma}_1\)</span> will be essentially 0 and you will never be able to determine if the instrument is endogenous.</li>
</ul>
<p><strong>Solution</strong></p>
<p>Testing Instrumental Exogeneity in an Over-identified Model</p>
<ul>
<li>
<p>When there is more than one exogenous instrument (per endogenous variable), we can test for instrument exogeneity.</p>
<ul>
<li><p>When we have multiple instruments, the model is said to be over-identified.</p></li>
<li><p>Could estimate the same model several ways (i.e., can identify/ estimate <span class="math inline">\(\beta_1\)</span> more than one way)</p></li>
</ul>
</li>
<li><p>Idea behind the test: if the controls and instruments are truly exogenous then OLS estimation of the following regression,</p></li>
</ul>
<p><span class="math display">\[
\epsilon_i = \gamma_0 + \mathbf{z}_{i1}\gamma_1 + \mathbf{z}_{i2}\gamma_2 + error_i
\]</span></p>
<p>should have a very low <span class="math inline">\(R^2\)</span></p>
<ul>
<li>if the model is <strong>just identified</strong> (one instrument per endogenous variable) then the <span class="math inline">\(R^2 = 0\)</span>
</li>
</ul>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the structural equation by 2SLS (using all available instruments) and obtain the residuals e</p></li>
<li><p>Regress e on all controls and instruments and obtain the <span class="math inline">\(R^2\)</span></p></li>
<li>
<p>Under the null hypothesis (all IV’s are uncorrelated), <span class="math inline">\(nR^2 \sim \chi^2(q)\)</span>, where q is the number of instrumental variables minus the number of endogenous variables</p>
<ul>
<li>if the model is just identified (one instrument per endogenous variable) then q = 0, and the distribution under the null collapses.</li>
</ul>
</li>
</ol>
<p>low p-value means you reject the null of exogenous instruments. Hence you would like to have high p-value in this test.</p>
<p><strong>Pitfalls for the Overid test</strong></p>
<ul>
<li>
<p>the overid test is essentially compiling the following information.</p>
<ul>
<li>Conditional on first instrument being exogenous is the other instrument exogenous?</li>
<li>Conditional on the other instrument being exogenous, is the first instrument exogenous?</li>
</ul>
</li>
<li><p>If all instruments are endogenous than neither test will be valid</p></li>
<li><p>really only useful if one instrument is thought to be truly exogenous (randomly assigned). even f you do reject the null, the test does not tell you which instrument is exogenous and which is endogenous.</p></li>
</ul>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="22%">
<col width="77%">
</colgroup>
<thead><tr class="header">
<th>Result</th>
<th>Implication</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>reject the null</td>
<td>you can be pretty sure there is an endogenous instrument, but don’t know which one.</td>
</tr>
<tr class="even">
<td>fail to reject</td>
<td>could be either (1) they are both exogenous, (2) they are both endogenous.</td>
</tr>
</tbody>
</table></div>
</div>
<div id="relevancy" class="section level6" number="36.1.3.1.1.3">
<h6>
<span class="header-section-number">36.1.3.1.1.3</span> Relevancy<a class="anchor" aria-label="anchor" href="#relevancy"><i class="fas fa-link"></i></a>
</h6>
<p>Why Relevance matter?</p>
<p><span class="math display">\[
\pi_2 \neq 0
\]</span></p>
<ul>
<li>
<p>used to show <a href="linear-regression.html#a2-full-rank">A2</a> holds</p>
<ul>
<li><p>If <span class="math inline">\(\pi_2 = 0\)</span> (instrument is not relevant) then <a href="linear-regression.html#a2-full-rank">A2</a> fails - perfect multicollinearity</p></li>
<li><p>If <span class="math inline">\(\pi_2\)</span> is close to 0 (<strong>weak instrument</strong>) then there is near perfect multicollinearity - 2SLS is highly inefficient (Large standard errors).</p></li>
</ul>
</li>
<li>
<p>A weak instrument will exacerbate any inconsistency due to an instrument being (even slightly) endogenous.</p>
<ul>
<li>In the simple case with no controls and a single endogenous variable and single instrumental variable,</li>
</ul>
</li>
</ul>
<p><span class="math display">\[
plim(\hat{\beta}_{2_{2SLS}}) = \beta_2 + \frac{E(z_{i2}\epsilon_i)}{E(z_{i2}y_{i2})}
\]</span></p>
<p><strong>Testing Weak Instruments</strong></p>
<ul>
<li><p>can use <span class="math inline">\(t\)</span>-test (or <span class="math inline">\(F\)</span>-test for over-identified models) in the first stage to determine if there is a weak instrument problem.</p></li>
<li>
<p><span class="citation">J. Stock and Yogo (<a href="references.html#ref-stock2005asymptotic">2005</a>)</span>: a statistical rejection of the null hypothesis in the first stage at the 5% (or even 1%) level is not enough to insure the instrument is not weak</p>
<ul>
<li>Rule of Thumb: need a <span class="math inline">\(F\)</span>-stat of at least 10 (or a <span class="math inline">\(t\)</span>-stat of at least 3.2) to reject the null hypothesis that the instrument is weak.</li>
</ul>
</li>
</ul>
<p><strong>Summary of the 2SLS Estimator</strong></p>
<p><span class="math display">\[
\begin{aligned}
y_{i1} &amp;=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &amp;= \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\end{aligned}
\]</span></p>
<ul>
<li>when [A3a] does not hold</li>
</ul>
<p><span class="math display">\[
E(y_{i2}'\epsilon_i) \neq 0
\]</span></p>
<ul>
<li><p>Then the OLS estimator is no longer unbiased or consistent.</p></li>
<li><p>If we have valid instruments <span class="math inline">\(\mathbf{z}_{i2}\)</span></p></li>
<li>
<p><a href="endogeneity.html#relevancy">Relevancy</a> (need to avoid “weak instruments”): <span class="math inline">\(\pi_2 \neq 0\)</span> Then the 2SLS estimator is consistent under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, [A5a], and the above two conditions.</p>
<ul>
<li><p>If <a href="linear-regression.html#a4-homoskedasticity">A4</a> also holds, then the usual standard errors are valid.</p></li>
<li><p>If <a href="linear-regression.html#a4-homoskedasticity">A4</a> does not hold then use the robust standard errors.</p></li>
</ul>
</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
y_{i1} &amp;= \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &amp;= \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\end{aligned}
\]</span></p>
<ul>
<li>When [A3a] does hold</li>
</ul>
<p><span class="math display">\[
E(y_{i2}'\epsilon_i) = 0
\]</span></p>
<p>and we have valid instruments, then both the OLS and 2SLS estimators are consistent.</p>
<ul>
<li>The OLS estimator is always more efficient</li>
<li>can use the variable addition test to determine if 2SLS is need (A3a does hold) or if OLS is valid (A3a does not hold)</li>
</ul>
<p>Sometimes we can test the assumption for instrument to be valid:</p>
<ul>
<li>
<a href="endogeneity.html#exogeneity">Exogeneity</a> : Only table when there are more instruments than endogenous variables.</li>
<li>
<a href="endogeneity.html#relevancy">Relevancy</a> (need to avoid “weak instruments”): Always testable, need the F-stat to be greater than 10 to rule out a weak instrument</li>
</ul>
<p>Application</p>
<p>Expenditure as observed instrument</p>
<div class="sourceCode" id="cb950"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m2.2sls</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span></span>
<span>        <span class="va">read</span> <span class="op">~</span> <span class="va">stratio</span> <span class="op">+</span> <span class="va">english</span> <span class="op">+</span> <span class="va">lunch</span> </span>
<span>        <span class="op">+</span> <span class="va">grades</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">calworks</span> <span class="op">+</span> <span class="va">county</span> <span class="op">|</span></span>
<span>            </span>
<span>            <span class="va">expenditure</span> <span class="op">+</span> <span class="va">english</span> <span class="op">+</span> <span class="va">lunch</span> </span>
<span>        <span class="op">+</span> <span class="va">grades</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">calworks</span> <span class="op">+</span> <span class="va">county</span> ,</span>
<span>        data <span class="op">=</span> <span class="va">school</span></span>
<span>    <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m2.2sls</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>,<span class="op">]</span></span>
<span><span class="co">#&gt;                 Estimate  Std. Error     t value      Pr(&gt;|t|)</span></span>
<span><span class="co">#&gt; (Intercept) 700.47891593 13.58064436  51.5792106 8.950497e-171</span></span>
<span><span class="co">#&gt; stratio      -1.13674002  0.53533638  -2.1234126  3.438427e-02</span></span>
<span><span class="co">#&gt; english      -0.21396934  0.03847833  -5.5607753  5.162571e-08</span></span>
<span><span class="co">#&gt; lunch        -0.39384225  0.03773637 -10.4366757  1.621794e-22</span></span>
<span><span class="co">#&gt; gradesKK-08  -1.89227865  1.37791820  -1.3732881  1.704966e-01</span></span>
<span><span class="co">#&gt; income        0.62487986  0.11199008   5.5797785  4.668490e-08</span></span>
<span><span class="co">#&gt; calworks     -0.04950501  0.06244410  -0.7927892  4.284101e-01</span></span></code></pre></div>
</div>
</div>
<div id="checklist" class="section level5" number="36.1.3.1.2">
<h5>
<span class="header-section-number">36.1.3.1.2</span> Checklist<a class="anchor" aria-label="anchor" href="#checklist"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li>Regress the dependent variable on the instrument (reduced form). Since under OLS, we have unbiased estimate, the coefficient estimate should be significant (make sure the sign makes sense)</li>
<li>Report F-stat on the excluded instruments. F-stat &lt; 10 means you have a weak instrument <span class="citation">(<a href="references.html#ref-stock2002survey">J. H. Stock, Wright, and Yogo 2002</a>)</span>.</li>
<li>Present <span class="math inline">\(R^2\)</span> before and after including the instrument <span class="citation">(<a href="references.html#ref-rossi2014even">Rossi 2014</a>)</span>
</li>
<li>For models with multiple instrument, present firs-t and second-stage result for each instrument separately. Overid test should be conducted (e.g., Sargan-Hansen J)</li>
<li>Hausman test between OLS and 2SLS (don’t confuse this test for evidence that endogeneity is irrelevant - under invalid IV, the test is useless)</li>
<li>Compare the 2SLS with the limited information ML. If they are different, you have evidence for weak instruments.</li>
</ol>
</div>
</div>
<div id="good-instruments" class="section level4" number="36.1.3.2">
<h4>
<span class="header-section-number">36.1.3.2</span> Good Instruments<a class="anchor" aria-label="anchor" href="#good-instruments"><i class="fas fa-link"></i></a>
</h4>
<p><a href="endogeneity.html#exogeneity">Exogeneity</a> and <a href="endogeneity.html#relevancy">Relevancy</a> are necessary but not sufficient for IV to produce consistent estimates.</p>
<p>Without theory or possible explanation, you can always create a new variable that is correlated with <span class="math inline">\(X\)</span> and uncorrelated with <span class="math inline">\(\epsilon\)</span></p>
<p>For example, we want to estimate the effect of price on quantity <span class="citation">(<a href="references.html#ref-reiss2011structural">Reiss 2011, 960</a>)</span></p>
<p><span class="math display">\[
\begin{aligned}
Q &amp;= \beta_1 P + \beta_2 X + \epsilon \\
P &amp;= \pi_1 X + \eta
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\eta\)</span> are jointly determined, <span class="math inline">\(X \perp \epsilon, \eta\)</span></p>
<p>Without theory, we can just create a new variable <span class="math inline">\(Z = X + u\)</span> where <span class="math inline">\(E(u) = 0; u \perp X, \epsilon, \eta\)</span></p>
<p>Then, <span class="math inline">\(Z\)</span> satisfied both conditions:</p>
<ul>
<li><p>Relevancy: <span class="math inline">\(X\)</span> correlates <span class="math inline">\(P\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(Z\)</span> correlates <span class="math inline">\(P\)</span></p></li>
<li><p>Exogeneity: <span class="math inline">\(u \perp \epsilon\)</span> (random noise)</p></li>
</ul>
<p>But obviously, it’s not a valid instrument (intuitively). But theoretically, relevance and exogeneity are not sufficient to identify <span class="math inline">\(\beta\)</span> because of unsatisfied rank condition for identification.</p>
<p>Moreover, the functional form of the instrument also plays a role when choosing a good instrument. Hence, we always need to check for the robustness of our instrument.</p>
<p>IV methods even with valid instruments can still have poor sampling properties (finite sample bias, large sampling errors) <span class="citation">(<a href="references.html#ref-rossi2014even">Rossi 2014</a>)</span></p>
<p>When you have a weak instrument, it’s important to report it appropriately. This problem will be exacerbated if you have multiple instruments <span class="citation">(<a href="references.html#ref-larcker2010use">Larcker and Rusticus 2010</a>)</span>.</p>
<div id="lagged-dependent-variable" class="section level5" number="36.1.3.2.1">
<h5>
<span class="header-section-number">36.1.3.2.1</span> Lagged dependent variable<a class="anchor" aria-label="anchor" href="#lagged-dependent-variable"><i class="fas fa-link"></i></a>
</h5>
<p>In time series data sets, we can use lagged dependent variable as an instrument because it is not influenced by current shocks. For example, <span class="citation">Chetty, Friedman, and Rockoff (<a href="references.html#ref-chetty2014measuring">2014</a>)</span> used lagged dependent variable in econ.</p>
</div>
<div id="lagged-explanatory-variable" class="section level5" number="36.1.3.2.2">
<h5>
<span class="header-section-number">36.1.3.2.2</span> Lagged explanatory variable<a class="anchor" aria-label="anchor" href="#lagged-explanatory-variable"><i class="fas fa-link"></i></a>
</h5>
<ul>
<li>
<p>Common practice in applied economics: Replace suspected simultaneously determined explanatory variable with its lagged value <span class="citation">Bellemare, Masaki, and Pepinsky (<a href="references.html#ref-bellemare2017lagged">2017</a>)</span>.</p>
<ul>
<li><p>This practice does not avoid simultaneity bias.</p></li>
<li><p>Estimates using this method are still inconsistent.</p></li>
<li><p>Hypothesis testing becomes invalid under this approach.</p></li>
<li><p>Lagging variables changes how endogeneity bias operates, adding a “no dynamics among unobservables” assumption to the “selection on observables” assumption.</p></li>
</ul>
</li>
<li>
<p>Key conditions for appropriate use <span class="citation">(<a href="references.html#ref-bellemare2017lagged">Bellemare, Masaki, and Pepinsky 2017</a>)</span>:</p>
<ul>
<li>
<strong>Under unobserved confounding:</strong>
<ul>
<li>No dynamics among unobservables.</li>
<li>The lagged variable <span class="math inline">\(X\)</span> is a stationary autoregressive process.</li>
</ul>
</li>
<li>
<strong>Under no unobserved confounding:</strong>
<ul>
<li>No reverse causality; the causal effect operates with a one-period lag (<span class="math inline">\(X_{t-1} \to Y\)</span>, <span class="math inline">\(X_t \not\to Y_t\)</span>).</li>
<li>Reverse causality is contemporaneous, with a one-period lag effect.</li>
<li>Reverse causality is contemporaneous; no dynamics in <span class="math inline">\(Y\)</span>, but dynamics exist in <span class="math inline">\(X\)</span> (<span class="math inline">\(X_{t-1} \to X\)</span>).</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Alternative approach</strong>: Use lagged values of the endogenous variable in IV estimation. However, IV estimation is only effective if <span class="citation">(<a href="references.html#ref-reed2015practice">Reed 2015</a>)</span>:</p>
<ul>
<li><p>Lagged values do not belong in the estimating equation.</p></li>
<li><p>Lagged values are sufficiently correlated with the simultaneously determined explanatory variable.</p></li>
<li><p>Lagged IVs help mitigate endogeneity if they only violate the independence assumption. However, if lagged IVs violate both the independence assumption and exclusion restriction, they may aggravate endogeneity <span class="citation">(<a href="references.html#ref-wang2019lagged">Yu Wang and Bellemare 2019</a>)</span>.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="internal-instrumental-variable" class="section level4" number="36.1.3.3">
<h4>
<span class="header-section-number">36.1.3.3</span> Internal instrumental variable<a class="anchor" aria-label="anchor" href="#internal-instrumental-variable"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p>(also known as <strong>instrument free methods</strong>). This section is based on Raluca Gui’s <a href="https://cran.r-project.org/web/packages/REndo/vignettes/REndo-introduction.pdf">guide</a></p></li>
<li><p>alternative to external instrumental variable approaches</p></li>
<li><p>All approaches here assume a <strong>continuous dependent variable</strong></p></li>
</ul>
<div id="non-hierarchical-data-cross-classified" class="section level5" number="36.1.3.3.1">
<h5>
<span class="header-section-number">36.1.3.3.1</span> Non-hierarchical Data (Cross-classified)<a class="anchor" aria-label="anchor" href="#non-hierarchical-data-cross-classified"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
Y_t = \beta_0 + \beta_1 P_t + \beta_2 X_t + \epsilon_t
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(t = 1, .., T\)</span> (indexes either time or cross-sectional units)</li>
<li>
<span class="math inline">\(Y_t\)</span> is a <span class="math inline">\(k \times 1\)</span> response variable</li>
<li>
<span class="math inline">\(X_t\)</span> is a <span class="math inline">\(k \times n\)</span> exogenous regressor</li>
<li>
<span class="math inline">\(P_t\)</span> is a <span class="math inline">\(k \times 1\)</span> continuous endogenous regressor</li>
<li>
<span class="math inline">\(\epsilon_t\)</span> is a structural error term with <span class="math inline">\(\mu_\epsilon =0\)</span> and <span class="math inline">\(E(\epsilon^2) = \sigma^2\)</span>
</li>
<li>
<span class="math inline">\(\beta\)</span> are model parameters</li>
</ul>
<p>The endogeneity problem arises from the correlation of <span class="math inline">\(P_t\)</span> and <span class="math inline">\(\epsilon_t\)</span>:</p>
<p><span class="math display">\[
P_t = \gamma Z_t + v_t
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(Z_t\)</span> is a <span class="math inline">\(l \times 1\)</span> vector of internal instrumental variables</li>
<li>
<span class="math inline">\(ν_t\)</span> is a random error with <span class="math inline">\(\mu_{v_t}, E(v^2) = \sigma^2_v, E(\epsilon v) = \sigma_{\epsilon v}\)</span>
</li>
<li>
<span class="math inline">\(Z_t\)</span> is assumed to be stochastic with distribution <span class="math inline">\(G\)</span>
</li>
<li>
<span class="math inline">\(ν_t\)</span> is assumed to have density <span class="math inline">\(h(·)\)</span>
</li>
</ul>
<div id="latent-instrumental-variable" class="section level6" number="36.1.3.3.1.1">
<h6>
<span class="header-section-number">36.1.3.3.1.1</span> Latent Instrumental Variable<a class="anchor" aria-label="anchor" href="#latent-instrumental-variable"><i class="fas fa-link"></i></a>
</h6>
<p><span class="citation">(<a href="references.html#ref-ebbes2005solving">Ebbes et al. 2005</a>)</span></p>
<p>assume <span class="math inline">\(Z_t\)</span> (unobserved) to be uncorrelated with <span class="math inline">\(\epsilon_t\)</span>, which is similar to <a href="endogeneity.html#instrumental-variable">Instrumental Variable</a>. Hence, <span class="math inline">\(Z_t\)</span> and <span class="math inline">\(ν_t\)</span> can’t be identified without distributional assumptions</p>
<p>The distributions of <span class="math inline">\(Z_t\)</span> and <span class="math inline">\(ν_t\)</span> need to be specified such that:</p>
<ol style="list-style-type: decimal">
<li>endogeneity of <span class="math inline">\(P_t\)</span> is corrected</li>
<li>the distribution of <span class="math inline">\(P_t\)</span> is empirically close to the integral that expresses the amount of overlap of Z as it is shifted over ν (= the convolution between <span class="math inline">\(Z_t\)</span> and <span class="math inline">\(ν_t\)</span>).</li>
</ol>
<p>When the density h(·) = Normal, then G cannot be normal because the parameters would not be identified <span class="citation">(<a href="references.html#ref-ebbes2005solving">Ebbes et al. 2005</a>)</span> .</p>
<p>Hence,</p>
<ul>
<li>in the <a href="endogeneity.html#latent-instrumental-variable">LIV</a> model the distribution of <span class="math inline">\(Z_t\)</span> is discrete</li>
<li>in the <a href="endogeneity.html#higher-moments-method">Higher Moments Method</a> and <a href="endogeneity.html#joint-estimation-using-copula">Joint Estimation Using Copula</a> methods, the distribution of <span class="math inline">\(Z_t\)</span> is taken to be skewed.</li>
</ul>
<p><span class="math inline">\(Z_t\)</span> are assumed <strong>unobserved, discrete and exogenous</strong>, with</p>
<ul>
<li>an unknown number of groups m</li>
<li>
<span class="math inline">\(\gamma\)</span> is a vector of group means.</li>
</ul>
<p>Identification of the parameters relies on the distributional assumptions of</p>
<ul>
<li>
<span class="math inline">\(P_t\)</span>: a non-Gaussian distribution</li>
<li>
<span class="math inline">\(Z_t\)</span> discrete with <span class="math inline">\(m \ge 2\)</span>
</li>
</ul>
<p>Note:</p>
<ul>
<li>If <span class="math inline">\(Z_t\)</span> is continuous, the model is unidentified</li>
<li>If <span class="math inline">\(P_t \sim N\)</span>, you have inefficient estimates.</li>
</ul>
<div class="sourceCode" id="cb951"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m3.liv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/REndo/man/latentIV.html">latentIV</a></span><span class="op">(</span><span class="va">read</span> <span class="op">~</span> <span class="va">stratio</span>, data <span class="op">=</span> <span class="va">school</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m3.liv</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>, <span class="op">]</span></span>
<span><span class="co">#&gt;                   Estimate    Std. Error       z-score     Pr(&gt;|z|)</span></span>
<span><span class="co">#&gt; (Intercept)   6.996014e+02  2.686165e+02  2.604462e+00 9.529035e-03</span></span>
<span><span class="co">#&gt; stratio      -2.272673e+00  1.367747e+01 -1.661618e-01 8.681097e-01</span></span>
<span><span class="co">#&gt; pi1          -4.896363e+01           NaN           NaN          NaN</span></span>
<span><span class="co">#&gt; pi2           1.963920e+01  9.225351e-02  2.128830e+02 0.000000e+00</span></span>
<span><span class="co">#&gt; theta5       6.939432e-152 3.143456e-161  2.207581e+09 0.000000e+00</span></span>
<span><span class="co">#&gt; theta6        3.787512e+02  4.249436e+01  8.912976e+00 1.541010e-17</span></span>
<span><span class="co">#&gt; theta7       -1.227543e+00  4.885237e+01 -2.512761e-02 9.799651e-01</span></span></code></pre></div>
<p>it will return a coefficient very different from the other methods since there is only one endogenous variable.</p>
</div>
<div id="joint-estimation-using-copula" class="section level6" number="36.1.3.3.1.2">
<h6>
<span class="header-section-number">36.1.3.3.1.2</span> Joint Estimation Using Copula<a class="anchor" aria-label="anchor" href="#joint-estimation-using-copula"><i class="fas fa-link"></i></a>
</h6>
<p>assume <span class="math inline">\(Z_t\)</span> (unobserved) to be uncorrelated with <span class="math inline">\(\epsilon_t\)</span>, which is similar to <a href="endogeneity.html#instrumental-variable">Instrumental Variable</a>. Hence, <span class="math inline">\(Z_t\)</span> and <span class="math inline">\(ν_t\)</span> can’t be identified without distributional assumptions</p>
<p><span class="citation">(<a href="references.html#ref-park2012handling">S. Park and Gupta 2012</a>)</span> allows joint estimation of the continuous <span class="math inline">\(P_t\)</span> and <span class="math inline">\(\epsilon_t\)</span> using Gaussian copulas, where a copula is a function that maps several conditional distribution functions (CDF) into their joint CDF).</p>
<p>The underlying idea is that using information contained in the observed data, one selects marginal distributions for <span class="math inline">\(P_t\)</span> and <span class="math inline">\(\epsilon_t\)</span>. Then, the copula model constructs a flexible multivariate joint distribution that allows a wide range of correlations between the two marginals.</p>
<p>The method allows both continuous and discrete <span class="math inline">\(P_t\)</span>.</p>
<p>In the special case of <strong>one continuous</strong> <span class="math inline">\(P_t\)</span>, estimation is based on MLE<br>
Otherwise, based on Gaussian copulas, augmented OLS estimation is used.</p>
<p><strong>Assumptions</strong>:</p>
<ul>
<li><p>skewed <span class="math inline">\(P_t\)</span></p></li>
<li><p>the recovery of the correct parameter estimates</p></li>
<li><p><span class="math inline">\(\epsilon_t \sim\)</span> normal marginal distribution. The marginal distribution of <span class="math inline">\(P_t\)</span> is obtained using the <strong>Epanechnikov kernel density estimator</strong><br><span class="math display">\[
\hat{h}_p = \frac{1}{T . b} \sum_{t=1}^TK(\frac{p - P_t}{b})
\]</span> where</p></li>
<li><p><span class="math inline">\(P_t\)</span> = endogenous variables</p></li>
<li><p><span class="math inline">\(K(x) = 0.75(1-x^2)I(||x||\le 1)\)</span></p></li>
<li>
<p><span class="math inline">\(b=0.9T^{-1/5}\times min(s, IQR/1.34)\)</span></p>
<ul>
<li>IQR = interquartile range</li>
<li>
<span class="math inline">\(s\)</span> = sample standard deviation</li>
<li>
<span class="math inline">\(T\)</span> = n of time periods observed in the data</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb952"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 1.34 comes from this</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.25</span>, <span class="fl">0.75</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.34898</span></span></code></pre></div>
<p>In augmented OLS and MLE, the inference procedure occurs in two stages:</p>
<p>(1): the empirical distribution of <span class="math inline">\(P_t\)</span> is computed<br>
(2) used in it constructing the likelihood function)<br>
Hence, the standard errors would not be correct.</p>
<p>So we use the sampling distributions (from bootstrapping) to get standard errors and the variance-covariance matrix. Since the distribution of the bootstrapped parameters is highly skewed, we report the percentile confidence intervals is preferable.</p>
<div class="sourceCode" id="cb953"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">110</span><span class="op">)</span></span>
<span><span class="va">m4.cc</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/REndo/man/copulaCorrection.html">copulaCorrection</a></span><span class="op">(</span></span>
<span>        <span class="va">read</span> <span class="op">~</span> <span class="va">stratio</span> <span class="op">+</span> <span class="va">english</span> <span class="op">+</span> <span class="va">lunch</span> <span class="op">+</span> <span class="va">calworks</span> <span class="op">+</span></span>
<span>            <span class="va">grades</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">county</span> <span class="op">|</span></span>
<span>            <span class="fu">continuous</span><span class="op">(</span><span class="va">stratio</span><span class="op">)</span>,</span>
<span>        data <span class="op">=</span> <span class="va">school</span>,</span>
<span>        optimx.args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span>method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Nelder-Mead"</span><span class="op">)</span>, </span>
<span>                           itnmax <span class="op">=</span> <span class="fl">60000</span><span class="op">)</span>,</span>
<span>        num.boots <span class="op">=</span> <span class="fl">2</span>,</span>
<span>        verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>    <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m4.cc</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>,<span class="op">]</span></span>
<span><span class="co">#&gt;             Point Estimate    Boots SE Lower Boots CI (95%)</span></span>
<span><span class="co">#&gt; (Intercept)   682.38500189 2.438874461                   NA</span></span>
<span><span class="co">#&gt; stratio        -0.31091374 0.022332810                   NA</span></span>
<span><span class="co">#&gt; english        -0.19648765 0.026136685                   NA</span></span>
<span><span class="co">#&gt; lunch          -0.38439247 0.032293205                   NA</span></span>
<span><span class="co">#&gt; calworks       -0.04363619 0.009027372                   NA</span></span>
<span><span class="co">#&gt; gradesKK-08    -1.97957885 0.138420392                   NA</span></span>
<span><span class="co">#&gt; income          0.77449995 0.033266598                   NA</span></span>
<span><span class="co">#&gt;             Upper Boots CI (95%)</span></span>
<span><span class="co">#&gt; (Intercept)                   NA</span></span>
<span><span class="co">#&gt; stratio                       NA</span></span>
<span><span class="co">#&gt; english                       NA</span></span>
<span><span class="co">#&gt; lunch                         NA</span></span>
<span><span class="co">#&gt; calworks                      NA</span></span>
<span><span class="co">#&gt; gradesKK-08                   NA</span></span>
<span><span class="co">#&gt; income                        NA</span></span></code></pre></div>
<p>we run this model with only one endogenous continuous regressor (<code>stratio</code>). Sometimes, the code will not converge, in which case you can use different</p>
<ul>
<li>optimization algorithm</li>
<li>starting values</li>
<li>maximum number of iterations</li>
</ul>
</div>
<div id="higher-moments-method" class="section level6" number="36.1.3.3.1.3">
<h6>
<span class="header-section-number">36.1.3.3.1.3</span> Higher Moments Method<a class="anchor" aria-label="anchor" href="#higher-moments-method"><i class="fas fa-link"></i></a>
</h6>
<p>suggested by <span class="citation">(<a href="references.html#ref-lewbel1997constructing">Lewbel 1997</a>)</span> to identify <span class="math inline">\(\epsilon_t\)</span> caused by <strong>measurement error</strong>.</p>
<p>Identification is achieved by using third moments of the data, with no restrictions on the distribution of <span class="math inline">\(\epsilon_t\)</span><br>
The following instruments can be used with 2SLS estimation to obtain consistent estimates:</p>
<p><span class="math display">\[
\begin{aligned}
q_{1t} &amp;=  (G_t - \bar{G}) \\
q_{2t} &amp;=  (G_t - \bar{G})(P_t - \bar{P}) \\
q_{3t} &amp;=   (G_t - \bar{G})(Y_t - \bar{Y})\\
q_{4t} &amp;=  (Y_t - \bar{Y})(P_t - \bar{P}) \\
q_{5t} &amp;=  (P_t - \bar{P})^2 \\
q_{6t} &amp;=  (Y_t - \bar{Y})^2 \\
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(G_t = G(X_t)\)</span> for any given function G that has finite third own and cross moments</li>
<li>
<span class="math inline">\(X\)</span> = exogenous variable</li>
</ul>
<p><span class="math inline">\(q_{5t}, q_{6t}\)</span> can be used only when the measurement and <span class="math inline">\(\epsilon_t\)</span> are symmetrically distributed. The rest of the instruments does not require any distributional assumptions for <span class="math inline">\(\epsilon_t\)</span>.</p>
<p>Since the regressors <span class="math inline">\(G(X) = X\)</span> are included as instruments, <span class="math inline">\(G(X)\)</span> can’t be a linear function of X in <span class="math inline">\(q_{1t}\)</span></p>
<p>Since this method has very strong assumptions, <a href="endogeneity.html#higher-moments-method">Higher Moments Method</a> should only be used in case of overidentification</p>
<div class="sourceCode" id="cb954"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">111</span><span class="op">)</span></span>
<span><span class="va">m5.hetEr</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/REndo/man/hetErrorsIV.html">hetErrorsIV</a></span><span class="op">(</span></span>
<span>        <span class="va">read</span> <span class="op">~</span> <span class="va">stratio</span> <span class="op">+</span> <span class="va">english</span> <span class="op">+</span> <span class="va">lunch</span> <span class="op">+</span> <span class="va">calworks</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span></span>
<span>            <span class="va">grades</span> <span class="op">+</span> <span class="va">county</span> <span class="op">|</span></span>
<span>            <span class="va">stratio</span> <span class="op">|</span> <span class="fu">IIV</span><span class="op">(</span><span class="va">income</span>, <span class="va">english</span><span class="op">)</span>,</span>
<span>        data <span class="op">=</span> <span class="va">school</span></span>
<span>    <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m5.hetEr</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>,<span class="op">]</span></span>
<span><span class="co">#&gt;                 Estimate  Std. Error    t value     Pr(&gt;|t|)</span></span>
<span><span class="co">#&gt; (Intercept) 662.78791557 27.90173069 23.7543657 2.380436e-76</span></span>
<span><span class="co">#&gt; stratio       0.71480686  1.31077325  0.5453322 5.858545e-01</span></span>
<span><span class="co">#&gt; english      -0.19522271  0.04057527 -4.8113717 2.188618e-06</span></span>
<span><span class="co">#&gt; lunch        -0.37834232  0.03927793 -9.6324402 9.760809e-20</span></span>
<span><span class="co">#&gt; calworks     -0.05665126  0.06302095 -0.8989273 3.692776e-01</span></span>
<span><span class="co">#&gt; income        0.82693755  0.17236557  4.7975797 2.335271e-06</span></span>
<span><span class="co">#&gt; gradesKK-08  -1.93795843  1.38723186 -1.3969968 1.632541e-01</span></span></code></pre></div>
<p>recommend using this approach to create additional instruments to use with external ones for better efficiency.</p>
</div>
<div id="heteroskedastic-error-approach" class="section level6" number="36.1.3.3.1.4">
<h6>
<span class="header-section-number">36.1.3.3.1.4</span> Heteroskedastic Error Approach<a class="anchor" aria-label="anchor" href="#heteroskedastic-error-approach"><i class="fas fa-link"></i></a>
</h6>
<ul>
<li>using means of variables that are uncorrelated with the product of heteroskedastic errors to identify structural parameters.</li>
<li>This method can be use either when you don’t have external instruments or you want to use additional instruments to improve the efficiency of the IV estimator <span class="citation">(<a href="references.html#ref-lewbel2012using">Lewbel 2012</a>)</span>
</li>
<li>The instruments are constructed as simple functions of data</li>
<li>Model’s assumptions:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
E(X \epsilon) &amp;= 0 \\
E(X v ) &amp;= 0 \\
cov(Z, \epsilon v) &amp;= 0  \\
cov(Z, v^2) &amp;\neq 0 \text{  (for identification)}
\end{aligned}
\]</span></p>
<p>Structural parameters are identified by 2SLS regression of Y on X and P, using X and [Z − E(Z)]ν as instruments.</p>
<p><span class="math display">\[
\text{instrument's strength} \propto cov((Z-\bar{Z})v,v)
\]</span></p>
<p>where <span class="math inline">\(cov((Z-\bar{Z})v,v)\)</span> is the degree of heteroskedasticity of ν with respect to Z <span class="citation">(<a href="references.html#ref-lewbel2012using">Lewbel 2012</a>)</span>, which can be empirically tested.</p>
<p>If it is zero or close to zero (i.e.,the instrument is weak), you might have imprecise estimates, with large standard errors.</p>
<ul>
<li>Under homoskedasticity, the parameters of the model are unidentified.</li>
<li>Under heteroskedasticity related to at least some elements of X, the parameters of the model are identified.</li>
</ul>
</div>
</div>
<div id="hierarchical-data" class="section level5" number="36.1.3.3.2">
<h5>
<span class="header-section-number">36.1.3.3.2</span> Hierarchical Data<a class="anchor" aria-label="anchor" href="#hierarchical-data"><i class="fas fa-link"></i></a>
</h5>
<p>Multiple independent assumptions involving various random components at different levels mean that any moderate correlation between some predictors and a random component or error term can result in a significant bias of the coefficients and of the variance components. <span class="citation">(<a href="references.html#ref-kim2007multilevel">J.-S. Kim and Frees 2007</a>)</span> proposed a generalized method of moments which uses both, the between and within variations of the exogenous variables, but only assumes the within variation of the variables to be endogenous.</p>
<p><strong>Assumptions</strong></p>
<ul>
<li>the errors at each level <span class="math inline">\(\sim iid N\)</span>
</li>
<li>the slope variables are exogenous</li>
<li>the level-1 <span class="math inline">\(\epsilon \perp X, P\)</span>. If this is not the case, additional, external instruments are necessary</li>
</ul>
<p><strong>Hierarchical Model</strong></p>
<p><span class="math display">\[
\begin{aligned}
Y_{cst} &amp;= Z_{cst}^1 \beta_{cs}^1 + X_{cst}^1 \beta_1 + \epsilon_{cst}^1 \\
\beta^1_{cs} &amp;= Z_{cs}^2 \beta_{c}^2 + X_{cst}^2 \beta_2 + \epsilon_{cst}^2 \\
\beta^2_{c} &amp;= X^3_c \beta_3 + \epsilon_c^3
\end{aligned}
\]</span></p>
<p>Bias could stem from:</p>
<ul>
<li>errors at the higher two levels (<span class="math inline">\(\epsilon_c^3,\epsilon_{cst}^2\)</span>) are correlated with some of the regressors</li>
<li>only third level errors (<span class="math inline">\(\epsilon_c^3\)</span>) are correlated with some of the regressors</li>
</ul>
<p><span class="citation">(<a href="references.html#ref-kim2007multilevel">J.-S. Kim and Frees 2007</a>)</span> proposed</p>
<ul>
<li>When all variables are assumed exogenous, the proposed estimator equals the random effects estimator</li>
<li>When all variables are assumed endogenous, it equals the fixed effects estimator</li>
<li>also use omitted variable test (based on the Hausman-test <span class="citation">(<a href="references.html#ref-hausman1978specification">J. A. Hausman 1978</a>)</span> for panel data), which allows the comparison of a robust estimator and an estimator that is efficient under the null hypothesis of no omitted variables or the comparison of two robust estimators at different levels.</li>
</ul>
<div class="sourceCode" id="cb955"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># function 'cholmod_factor_ldetA' not provided by package 'Matrix'</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">113</span><span class="op">)</span></span>
<span><span class="va">school</span><span class="op">$</span><span class="va">gr08</span> <span class="op">&lt;-</span> <span class="va">school</span><span class="op">$</span><span class="va">grades</span> <span class="op">==</span> <span class="st">"KK-06"</span></span>
<span><span class="va">m7.multilevel</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/REndo/man/multilevelIV.html">multilevelIV</a></span><span class="op">(</span><span class="va">read</span> <span class="op">~</span> <span class="va">stratio</span> <span class="op">+</span> <span class="va">english</span> <span class="op">+</span> <span class="va">lunch</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">gr08</span> <span class="op">+</span></span>
<span>                     <span class="va">calworks</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">county</span><span class="op">)</span> <span class="op">|</span> <span class="fu">endo</span><span class="op">(</span><span class="va">stratio</span><span class="op">)</span>,</span>
<span>                 data <span class="op">=</span> <span class="va">school</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m7.multilevel</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">7</span>,<span class="op">]</span></span></code></pre></div>
<p>Another example using simulated data</p>
<ul>
<li>level-1 regressors: <span class="math inline">\(X_{11}, X_{12}, X_{13}, X_{14}, X_{15}\)</span>, where <span class="math inline">\(X_{15}\)</span> is correlated with the level-2 error (i.e., endogenous).<br>
</li>
<li>level-2 regressors: <span class="math inline">\(X_{21}, X_{22}, X_{23}, X_{24}\)</span><br>
</li>
<li>level-3 regressors: <span class="math inline">\(X_{31}, X_{32}, X_{33}\)</span>
</li>
</ul>
<p>We estimate a three-level model with X15 assumed endogenous. Having a three-level hierarchy, <code><a href="https://rdrr.io/pkg/REndo/man/multilevelIV.html">multilevelIV()</a></code> returns five estimators, from the most robust to omitted variables (FE_L2), to the most efficient (REF) (i.e. lowest mean squared error).</p>
<ul>
<li>The random effects estimator (REF) is efficient assuming no omitted variables</li>
<li>The fixed effects estimator (FE) is unbiased and asymptotically normal even in the presence of omitted variables.</li>
<li>Because of the efficiency, the random effects estimator is preferable if you think there is no omitted. variables</li>
<li>The robust estimator would be preferable if you think there is omitted variables.</li>
</ul>
<div class="sourceCode" id="cb956"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># function 'cholmod_factor_ldetA' not provided by package 'Matrix'’</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">dataMultilevelIV</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">114</span><span class="op">)</span></span>
<span><span class="va">formula1</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">y</span> <span class="op">~</span> <span class="va">X11</span> <span class="op">+</span> <span class="va">X12</span> <span class="op">+</span> <span class="va">X13</span> <span class="op">+</span> <span class="va">X14</span> <span class="op">+</span> <span class="va">X15</span> <span class="op">+</span> <span class="va">X21</span> <span class="op">+</span> <span class="va">X22</span> <span class="op">+</span> <span class="va">X23</span> <span class="op">+</span> <span class="va">X24</span> <span class="op">+</span></span>
<span>    <span class="va">X31</span> <span class="op">+</span> <span class="va">X32</span> <span class="op">+</span> <span class="va">X33</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">CID</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">SID</span><span class="op">)</span> <span class="op">|</span> <span class="fu">endo</span><span class="op">(</span><span class="va">X15</span><span class="op">)</span></span>
<span><span class="va">m8.multilevel</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/REndo/man/multilevelIV.html">multilevelIV</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">formula1</span>, data <span class="op">=</span> <span class="va">dataMultilevelIV</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">m8.multilevel</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m8.multilevel</span>, <span class="st">"REF"</span><span class="op">)</span></span></code></pre></div>
<p>True <span class="math inline">\(\beta_{X_{15}} =-1\)</span>. We can see that some estimators are bias because <span class="math inline">\(X_{15}\)</span> is correlated with the level-two error, to which only FE_L2 and GMM_L2 are robust</p>
<p>To select the appropriate estimator, we use the omitted variable test.</p>
<p>In a three-level setting, we can have different estimator comparisons:</p>
<ul>
<li>Fixed effects vs. random effects estimators: Test for omitted level-two and level-three omitted effects, simultaneously, one compares FE_L2 to REF. But we will not know at which omitted variables exist.<br>
</li>
<li>Fixed effects vs. GMM estimators: Once the existence of omitted effects is established but not sure at which level, we test for level-2 omitted effects by comparing FE_L2 vs GMM_L3. If you reject the null, the omitted variables are at level-2 The same is accomplished by testing FE_L2 vs. GMM_L2, since the latter is consistent only if there are no omitted effects at level-2.<br>
</li>
<li>Fixed effects vs. fixed effects estimators: We can test for omitted level-2 effects, while allowing for omitted level-3 effects by comparing FE_L2 vs. FE_L3 since FE_L2 is robust against both level-2 and level-3 omitted effects while FE_L3 is only robust to level-3 omitted variables.</li>
</ul>
<p>Summary, use the omitted variable test comparing <code>REF vs. FE_L2</code> first.</p>
<ul>
<li><p>If the null hypothesis is rejected, then there are omitted variables either at level-2 or level-3</p></li>
<li>
<p>Next, test whether there are level-2 omitted effects, since testing for omitted level three effects relies on the assumption there are no level-two omitted effects. You can use any of these pair of comparisons:</p>
<ul>
<li><code>FE_L2 vs. FE_L3</code></li>
<li><code>FE_L2 vs. GMM_L2</code></li>
</ul>
</li>
<li>
<p>If no omitted variables at level-2 are found, test for omitted level-3 effects by comparing either</p>
<ul>
<li>
<code>FE_L3</code> vs. <code>GMM_L3</code>
</li>
<li>
<code>GMM_L2</code> vs. <code>GMM_L3</code>
</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb957"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m8.multilevel</span>, <span class="st">"REF"</span><span class="op">)</span></span>
<span><span class="co"># compare REF with all the other estimators. Testing REF (the most efficient estimator) against FE_L2 (the most robust estimator), equivalently we are testing simultaneously for level-2 and level-3 omitted effects. </span></span></code></pre></div>
<p>Since the null hypothesis is rejected (p = 0.000139), there is bias in the random effects estimator.</p>
<p>To test for level-2 omitted effects (regardless of level-3 omitted effects), we compare FE_L2 versus FE_L3</p>
<div class="sourceCode" id="cb958"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m8.multilevel</span>,<span class="st">"FE_L2"</span><span class="op">)</span></span></code></pre></div>
<p>The null hypothesis of no omitted level-2 effects is rejected (<span class="math inline">\(p = 3.92e − 05\)</span>). Hence, there are omitted effects at level-two. We should use FE_L2 which is consistent with the underlying data that we generated (level-2 error correlated with <span class="math inline">\(X_15\)</span>, which leads to biased FE_L3 coefficients.</p>
<p>The omitted variable test between FE_L2 and GMM_L2 should reject the null hypothesis of no omitted level-2 effects (p-value is 0).</p>
<p>If we assume an endogenous variable as exogenous, the RE and GMM estimators will be biased because of the wrong set of internal instrumental variables. To increase our confidence, we should compare the omitted variable tests when the variable is considered endogenous vs. exogenous to get a sense whether the variable is truly endogenous.</p>
</div>
</div>
<div id="proxy-variables" class="section level4" number="36.1.3.4">
<h4>
<span class="header-section-number">36.1.3.4</span> Proxy Variables<a class="anchor" aria-label="anchor" href="#proxy-variables"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p>Can be in place of the omitted variable</p></li>
<li><p>will not be able to estimate the effect of the omitted variable</p></li>
<li><p>will be able to reduce some endogeneity caused bye the omitted variable</p></li>
<li><p>but it can have <a href="endogeneity.html#measurement-error">Measurement Error</a>. Hence, you have to be extremely careful when using proxies.</p></li>
</ul>
<p>Criteria for a proxy variable:</p>
<ol style="list-style-type: decimal">
<li>The proxy is correlated with the omitted variable.</li>
<li>Having the omitted variable in the regression will solve the problem of endogeneity</li>
<li>The variation of the omitted variable unexplained by the proxy is uncorrelated with all independent variables, including the proxy.</li>
</ol>
<p>IQ test can be a proxy for ability in the regression between wage explained education.</p>
<p>For the third requirement</p>
<p><span class="math display">\[
ability = \gamma_0 + \gamma_1 IQ + \epsilon
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is uncorrelated with education and IQ test.</p>
<hr>
</div>
</div>
</div>
<div id="endogenous-sample-selection" class="section level2" number="36.2">
<h2>
<span class="header-section-number">36.2</span> Endogenous Sample Selection<a class="anchor" aria-label="anchor" href="#endogenous-sample-selection"><i class="fas fa-link"></i></a>
</h2>
<p>Endogenous sample selection arises in <strong>observational</strong> or <strong>non-experimental</strong> research whenever the inclusion of observations (or assignment to treatment) is <strong>not random</strong>, and the same unobservable factors influencing selection also affect the outcome of interest. This scenario leads to <strong>biased and inconsistent</strong> estimates of causal parameters (e.g., [Average Treatment Effects]) if not properly addressed.</p>
<p>This problem was first formalized in the econometric literature by <span class="citation">J. Heckman (<a href="references.html#ref-heckman1974shadow">1974</a>)</span> <span class="citation">J. J. Heckman (<a href="references.html#ref-heckman1976common">1976b</a>)</span> <span class="citation">J. J. Heckman (<a href="references.html#ref-heckman1979sample">1979</a>)</span>, whose work addressed the issue in the context of labor force participation among women. Later, <span class="citation">Amemiya (<a href="references.html#ref-amemiya1984tobit">1984</a>)</span> generalize the method. Now, it has since been applied widely across social sciences, epidemiology, marketing, and finance.</p>
<p>Endogenous sample selection is often conflated with general selection bias, but it is important to understand that sample selection refers specifically to the inclusion of observations into the estimation sample, not just to assignment into treatment (i.e., selection bias).</p>
<p>This problem comes in many names such as self-selection problem, incidental truncation, or omitted variable (i.e., the omitted variable is how people were selected into the sample). Some disciplines consider nonresponse/selection bias as sample selection:</p>
<ul>
<li>When unobservable factors that affect who is in the sample are independent of unobservable factors that affect the outcome, the sample selection is not endogenous. Hence, the sample selection is ignorable and estimator that ignores sample selection is still consistent.</li>
<li>When the unobservable factors that affect who is included in the sample are correlated with the unobservable factors that affect the outcome, the sample selection is endogenous and not ignorable, because estimators that ignore endogenous sample selection are not consistent (we don’t know which part of the observable outcome is related to the causal relationship and which part is due to different people were selected for the treatment and control groups).</li>
</ul>
<p>Many evaluation studies use observational data, and in such data:</p>
<ul>
<li>Participants are not randomly assigned.</li>
<li>Treatment or exposure is determined by individual or institutional choices.</li>
<li>Counterfactual outcomes are not observed.</li>
<li>The treatment indicator is often endogenous.</li>
</ul>
<p>Some notable terminologies include:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Truncation</strong>: Occurs when data are collected only from a restricted subpopulation based on the value of a variable.
<ul>
<li>
<strong>Left truncation</strong>: Values below a threshold are excluded (e.g., only high-income individuals are surveyed).</li>
<li>
<strong>Right truncation</strong>: Values above a threshold are excluded.</li>
</ul>
</li>
<li>
<strong>Censoring</strong>: Occurs when the variable is <strong>observed but coarsened</strong> beyond a threshold.
<ul>
<li>E.g., incomes below a certain level are coded as zero; arrest counts above a threshold are top-coded.</li>
</ul>
</li>
<li>
<strong>Incidental Truncation</strong>: Refers to selection based on a latent variable (e.g., employment decisions), <strong>not directly observed</strong>. This is what makes Heckman’s model distinct.
<ul>
<li>Also called <strong>non-random sample selection</strong>.</li>
<li>The error in the outcome equation is correlated with the selection indicator.</li>
</ul>
</li>
</ol>
<p>Researchers often categorize self-selection into:</p>
<ul>
<li>
<strong>Negative (Mitigation-Based) Selection:</strong> Individuals select into a treatment or sample to address an existing problem, so they start off with worse potential outcomes.
<ul>
<li>Bias direction: Underestimates true treatment effects (makes the treatment look less effective than it is).</li>
<li>Individuals select into treatment to combat a problem they already face.</li>
<li>
<strong>Examples</strong>:
<ul>
<li>People at high risk of severe illness (e.g., elderly or immunocompromised individuals) are more likely to get vaccinated. If we compare vaccinated vs. unvaccinated individuals without adjusting for risk factors, we might mistakenly conclude that vaccines are ineffective simply because vaccinated individuals had worse initial health conditions.</li>
<li>Evaluating the effect of job training programs—unemployed individuals with the greatest difficulty finding jobs are most likely to enroll, leading to underestimated program benefits.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Positive (Preference-Based) Selection:</strong> Individuals select into a treatment or sample because they have advantageous traits, preferences, or resources. Hence, those who take treatment are systematically better off compared to those who do not.
<ul>
<li>Bias direction: Overestimates true treatment effects (makes the treatment look more effective than it is).</li>
<li>Individuals select into treatment because they inherently prefer it, rather than because of an underlying problem.</li>
<li>
<strong>Examples:</strong>
<ul>
<li>People who are health-conscious and physically active are more likely to join a fitness program. If we compare fitness program participants to non-participants, we might falsely attribute their better health outcomes to the program, when in reality, their pre-existing lifestyle contributed to their improved health.</li>
<li>Evaluating the effect of private school education—students who attend private schools often come from wealthier families with greater academic support, making it difficult to isolate the true impact of the school itself.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Both forms of selection reflect correlation between <strong>unobservables</strong> (driving selection) and <strong>potential outcomes</strong>—the hallmark of <strong>endogenous selection bias</strong>.</p>
<hr>
<p>Some seminal applied works in this area include:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Labor Force Participation</strong> <span class="citation">(<a href="references.html#ref-heckman1974shadow">J. Heckman 1974</a>)</span>
</li>
</ol>
<ul>
<li>Wages are observed <strong>only</strong> for women who choose to work.</li>
<li>Unobservable preferences (reservation wages) drive participation.</li>
<li>Ignoring this leads to <strong>biased estimates of the returns to education</strong>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Union Membership</strong> <span class="citation">(<a href="references.html#ref-lewis1986union">Lewis 1986</a>)</span>
</li>
</ol>
<ul>
<li>Wages differ between union and non-union workers.</li>
<li>But union membership is <strong>not exogenous</strong>—workers choose to join based on anticipated benefits.</li>
<li>Naïve OLS yields biased estimates of union premium.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>College Attendance</strong> <span class="citation">(<a href="references.html#ref-card1999causal">Card 1999</a>, <a href="references.html#ref-card2001estimating">2001</a>)</span>
</li>
</ol>
<ul>
<li>Comparing income of college graduates vs. non-graduates.</li>
<li>Attending college is a choice based on expected gains, ability, or family background.</li>
<li>A treatment effect model (described next) is more appropriate here.</li>
</ul>
<hr>
<div id="unifying-model-frameworks" class="section level3" number="36.2.1">
<h3>
<span class="header-section-number">36.2.1</span> Unifying Model Frameworks<a class="anchor" aria-label="anchor" href="#unifying-model-frameworks"><i class="fas fa-link"></i></a>
</h3>
<p>Though often conflated, there are several overlapping models to address endogenous selection:</p>
<ol style="list-style-type: decimal">
<li>
<a href="endogeneity.html#sec-sample-selection-model">Sample Selection Model</a> <span class="citation">(<a href="references.html#ref-heckman1979sample">J. J. Heckman 1979</a>)</span>: Outcome is <em>unobserved</em> if an agent is not selected into the sample.</li>
<li>
<a href="endogeneity.html#sec-treatment-effect-switching-model">Treatment Effect Model</a>: Outcome is observed for both groups (treated vs. untreated), but treatment assignment is endogenous.</li>
<li>
<a href="endogeneity.html#sec-heckman-type-control-function">Heckman-Type / Control Function Approaches</a>: Decompose the endogenous regressor or incorporate a correction term (Inverse Mills Ratio or residual) to control for endogeneity.</li>
</ol>
<p>All revolve around the challenge: unobserved factors affect both who is included (or treated) and outcomes.</p>
<p>To formalize the problem, we consider the outcome and selection equations. Let:</p>
<ul>
<li>
<span class="math inline">\(y_i\)</span>: observed outcome (e.g., wage)</li>
<li>
<span class="math inline">\(x_i\)</span>: covariates affecting outcome</li>
<li>
<span class="math inline">\(z_i\)</span>: covariates affecting selection</li>
<li>
<span class="math inline">\(w_i\)</span>: binary indicator for selection into the sample (e.g., employment)</li>
</ul>
<div id="sec-sample-selection-model" class="section level4" number="36.2.1.1">
<h4>
<span class="header-section-number">36.2.1.1</span> Sample Selection Model<a class="anchor" aria-label="anchor" href="#sec-sample-selection-model"><i class="fas fa-link"></i></a>
</h4>
<p>We begin with an <strong>outcome equation</strong>, which describes the variable of interest <span class="math inline">\(y_i\)</span>. However, we only observe <span class="math inline">\(y_i\)</span> if a certain <strong>selection mechanism</strong> indicates it is part of the sample. That mechanism is captured by a binary indicator <span class="math inline">\(w_i = 1\)</span>. Formally, the observed outcome equation is:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;= x_i' \beta + \varepsilon_i, \quad &amp;\text{(Observed only if } w_i = 1\text{)}, \\
\varepsilon_i &amp;\sim N(0, \sigma_\varepsilon^2).
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(x_i\)</span> is a vector of explanatory variables (or covariates) that explain <span class="math inline">\(y_i\)</span>. The noise term <span class="math inline">\(\varepsilon_i\)</span> is assumed to be normally distributed with mean zero and variance <span class="math inline">\(\sigma_\varepsilon^2\)</span>. However, because we only see <span class="math inline">\(y_i\)</span> for those cases in which <span class="math inline">\(w_i = 1\)</span>, we must account for how the selection occurs.</p>
<p>Next, we specify the <strong>selection equation</strong> via a <strong>latent index model</strong>. Let <span class="math inline">\(w_i^*\)</span> be an unobserved latent variable:</p>
<p><span class="math display">\[
\begin{aligned}
w_i^* &amp;= z_i' \gamma + u_i, \\
w_i &amp;= \begin{cases}
1 &amp; \text{if } w_i^* &gt; 0, \\
0 &amp; \text{otherwise}.
\end{cases}
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(z_i\)</span> is a vector of variables that influence whether or not <span class="math inline">\(y_i\)</span> is observed. In practice, <span class="math inline">\(z_i\)</span> may overlap with <span class="math inline">\(x_i\)</span>, but can also include variables not in the outcome equation. For <strong>identification</strong>, we normalize <span class="math inline">\(\mathrm{Var}(u_i) = 1\)</span>. This is analogous to the probit model’s standard normalization.</p>
<p>Because <span class="math inline">\(w_i = 1\)</span> exactly when <span class="math inline">\(w_i^* &gt; 0\)</span>, this event occurs if <span class="math inline">\(u_i \ge -\,z_i' \gamma\)</span>. Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
P(w_i = 1)
&amp;= P\bigl(u_i \ge -z_i' \gamma\bigr),\\
&amp;= 1 - \Phi\bigl(-z_i'\gamma\bigr), \\
&amp;= \Phi\bigl(z_i'\gamma\bigr),
\end{aligned}
\]</span></p>
<p>where we use the symmetry of the standard normal distribution.</p>
<p>We assume <span class="math inline">\((\varepsilon_i, u_i)\)</span> are <strong>jointly normally distributed</strong> with correlation <span class="math inline">\(\rho\)</span>. In other words,</p>
<p><span class="math display">\[
\begin{pmatrix}
\varepsilon_i \\
u_i
\end{pmatrix}
\;\sim\; \mathcal{N} \!\Biggl(
\begin{pmatrix} 0 \\ 0 \end{pmatrix},
\begin{pmatrix} \sigma^2_\varepsilon &amp; \rho \,\sigma_\varepsilon \\
\rho \,\sigma_\varepsilon &amp; 1 \end{pmatrix}
\Biggr).
\]</span></p>
<ul>
<li>If <span class="math inline">\(\rho = 0\)</span>, the selection is exogenous: whether <span class="math inline">\(y_i\)</span> is observed is unrelated to unobserved determinants of <span class="math inline">\(y_i\)</span>.</li>
<li>If <span class="math inline">\(\rho \neq 0\)</span>, <strong>sample selection is endogenous</strong>. Failing to model this selection mechanism leads to biased estimates of <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p>Interpreting <span class="math inline">\(\rho\)</span></p>
<ul>
<li>
<span class="math inline">\(\rho &gt; 0\)</span>: Individuals with higher unobserved components in <span class="math inline">\(\varepsilon_i\)</span> (and thus typically larger <span class="math inline">\(y_i\)</span>) are <strong>more likely</strong> to appear in the sample. (Positive selection)</li>
<li>
<span class="math inline">\(\rho &lt; 0\)</span>: Individuals with higher unobserved components in <span class="math inline">\(\varepsilon_i\)</span> are <strong>less likely</strong> to appear. (Negative selection)</li>
<li>
<span class="math inline">\(\rho = 0\)</span>: No endogenous selection. Observed outcomes are effectively random with respect to the unobserved part of <span class="math inline">\(y_i\)</span>.</li>
</ul>
<p>In empirical practice, <span class="math inline">\(\rho\)</span> signals the direction of bias one might expect if the selection process is ignored.</p>
<p>Often, it is helpful to visualize how part of the distribution of <span class="math inline">\(u_i\)</span> (the error in the selection equation) is truncated based on the threshold <span class="math inline">\(w_i^*&gt;0\)</span>. Below is a notional R code snippet that draws a normal density and shades the region where <span class="math inline">\(u_i &gt; -z_i'\gamma\)</span>.</p>
<div class="sourceCode" id="cb959"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, <span class="fl">3</span>, length <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,</span>
<span>     <span class="va">y</span>,</span>
<span>     type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>     main <span class="op">=</span>  <span class="fu"><a href="https://rdrr.io/r/base/bquote.html">bquote</a></span><span class="op">(</span><span class="st">"Probabibility distribution of"</span> <span class="op">~</span> <span class="va">u</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x_shaded</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">3</span>, length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">y_shaded</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x_shaded</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/polygon.html">polygon</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="va">x_shaded</span>, <span class="fl">3</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">y_shaded</span>, <span class="fl">0</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.1</span>, <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://andrisignorell.github.io/DescTools/reference/CramerV.html">Phi</a></span><span class="op">(</span><span class="op">-</span><span class="va">z</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">*</span> <span class="va">gamma</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/arrows.html">arrows</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0</span>, length <span class="op">=</span> <span class="fl">0.15</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.12</span>, <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="op">-</span><span class="va">z</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">*</span> <span class="va">gamma</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topright"</span>,</span>
<span>       <span class="st">"Gray = Prob of Observed"</span>,</span>
<span>       pch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>       inset <span class="op">=</span> <span class="fl">0.02</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="36-endogeneity_files/figure-html/unnamed-chunk-12-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>In this figure, the gray‐shaded area represents <span class="math inline">\(u_i &gt; -z_i'\gamma\)</span>. Observations in that range are included in the sample. If <span class="math inline">\(\rho\neq 0\)</span>, then the unobserved factors that drive <span class="math inline">\(u_i\)</span> also affect <span class="math inline">\(\varepsilon_i\)</span>, causing a non‐representative sample of <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p>A core insight of the Heckman model is the conditional expectation of <span class="math inline">\(y_i\)</span> given <span class="math inline">\(w_i=1\)</span>:</p>
<p><span class="math display">\[
E\bigl(y_i \mid w_i = 1\bigr)
\;=\;
E\bigl(y_i \mid w_i^*&gt;0\bigr)
\;=\;
E\bigl(x_i'\beta + \varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr).
\]</span></p>
<p>Since <span class="math inline">\(x_i'\beta\)</span> is nonrandom (conditional on <span class="math inline">\(x_i\)</span>), we get</p>
<p><span class="math display">\[
E\bigl(y_i \mid w_i=1\bigr)
= x_i'\beta + E\bigl(\varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr).
\]</span></p>
<p>From bivariate normal properties:</p>
<p><span class="math display">\[
\varepsilon_i \,\bigl\lvert\, u_i=a
\;\sim\;
N\!\Bigl(\rho\,\sigma_{\varepsilon}\cdot a,\; (1-\rho^2)\,\sigma_{\varepsilon}^2\Bigr).
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
E\bigl(\varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr)
=\;
\rho\,\sigma_{\varepsilon}\,
E\bigl(u_i \mid u_i &gt; -z_i'\gamma\bigr).
\]</span></p>
<p>If <span class="math inline">\(U\sim N(0,1)\)</span>, then</p>
<p><span class="math display">\[
E(U \mid U&gt;a)
= \frac{\phi(a)}{1-\Phi(a)}
= \frac{\phi(a)}{\Phi(-a)},
\]</span> where <span class="math inline">\(\phi\)</span> is the standard normal pdf, <span class="math inline">\(\Phi\)</span> is the cdf. By symmetry, <span class="math inline">\(\phi(-a)=\phi(a)\)</span> and <span class="math inline">\(\Phi(-a)=1-\Phi(a)\)</span>. Letting <span class="math inline">\(a = -\,z_i'\gamma\)</span> yields</p>
<p><span class="math display">\[
E\bigl(U \mid U &gt; -z_i'\gamma\bigr)
= \frac{\phi(-z_i'\gamma)}{1-\Phi(-z_i'\gamma)}
= \frac{\phi(z_i'\gamma)}{\Phi(z_i'\gamma)}.
\]</span></p>
<p>Define the <strong>Inverse Mills Ratio</strong> (IMR) as</p>
<p><span class="math display">\[
\lambda(x)
= \frac{\phi(x)}{\Phi(x)}.
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
E\bigl(\varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr)
= \rho\,\sigma_{\varepsilon}\,\lambda\bigl(z_i'\gamma\bigr),
\]</span> and therefore</p>
<p><span class="math display">\[
\boxed{
E\bigl(y_i \mid w_i=1\bigr)
= x_i'\beta
\;+\;
\rho\,\sigma_{\varepsilon}\,
\frac{\phi\bigl(z_i'\gamma\bigr)}{\Phi\bigl(z_i'\gamma\bigr)}.
}
\]</span></p>
<p>This extra term is the so‐called <strong>Heckman correction</strong>.</p>
<p>The IMR appears in the two‐step procedure as a regressor for bias correction. It has useful derivatives:</p>
<p><span class="math display">\[
\frac{d}{dx}\Bigl[\text{IMR}(x)\Bigr]
= \frac{d}{dx}\Bigl[\frac{\phi(x)}{\Phi(x)}\Bigr]
= -x\,\text{IMR}(x)\;-\;\bigl[\text{IMR}(x)\bigr]^2.
\]</span></p>
<p>This arises from the quotient rule and the fact that <span class="math inline">\(\phi'(x)=-x\phi(x)\)</span>, <span class="math inline">\(\Phi'(x)=\phi(x)\)</span>. The derivative property also helps in interpreting marginal effects in selection models.</p>
<hr>
</div>
<div id="sec-treatment-effect-switching-model" class="section level4" number="36.2.1.2">
<h4>
<span class="header-section-number">36.2.1.2</span> Treatment Effect (Switching) Model<a class="anchor" aria-label="anchor" href="#sec-treatment-effect-switching-model"><i class="fas fa-link"></i></a>
</h4>
<p>While the sample selection model is used when outcome is only observed for one group (e.g., <span class="math inline">\(D = 1\)</span>), the treatment effect model is used when outcomes are observed for both groups, but treatment assignment is endogenous.</p>
<p>Treatment Effect Model Equations:</p>
<ul>
<li>Outcome: <span class="math display">\[ y_i = x_i' \beta + D_i \delta + \varepsilon_i \]</span>
</li>
<li>Selection: <span class="math display">\[ D_i^* = z_i' \gamma + u_i \\ D_i = 1 \text{ if } D_i^* &gt; 0 \]</span>
</li>
</ul>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(D_i\)</span> is the treatment indicator.</p></li>
<li><p><span class="math inline">\((\varepsilon_i, u_i)\)</span> are again bivariate normal with correlation <span class="math inline">\(\rho\)</span>.</p></li>
</ul>
<p>The treatment effect model is sometimes called a <strong>switching regression</strong>.</p>
</div>
<div id="sec-heckman-type-control-function" class="section level4" number="36.2.1.3">
<h4>
<span class="header-section-number">36.2.1.3</span> Heckman-Type vs. Control Function<a class="anchor" aria-label="anchor" href="#sec-heckman-type-control-function"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<strong>Heckman Sample Selection</strong>: Insert an Inverse Mills Ratio (IMR) to adjust the outcome equation for non-random truncation.</li>
<li>
<strong>Control Function</strong>: Residual-based or predicted-endogenous-variable approach that mirrors IV logic, but typically <em>still</em> requires an instrument or parametric assumption.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<caption>Differences between Heckman Sample Selection vs. Heckman-type correction</caption>
<colgroup>
<col width="19%">
<col width="30%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td><strong>Heckman Sample Selection Model</strong></td>
<td><strong>Heckman-Type Corrections</strong></td>
</tr>
<tr class="even">
<td>When</td>
<td>Only observes one sample (treated), addressing selection bias directly.</td>
<td>Two samples are observed (treated and untreated), known as the control function approach.</td>
</tr>
<tr class="odd">
<td>Model</td>
<td>Probit</td>
<td>OLS (even for dummy endogenous variable)</td>
</tr>
<tr class="even">
<td>Integration of 1st stage</td>
<td>Also include a term (called Inverse Mills ratio) besides the endogenous variable.</td>
<td>Decompose the endogenous variable to get the part that is uncorrelated with the error terms of the outcome equation. Either use the predicted endogenous variable directly or include the residual from the first-stage equation.</td>
</tr>
<tr class="odd">
<td>Advantages and Assumptions</td>
<td>Provides a direct test for endogeneity via the coefficient of the inverse Mills ratio but requires the assumption of joint normality of errors.</td>
<td>Does not require the assumption of joint normality, but can’t test for endogeneity directly.</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="estimation-methods-2" class="section level3" number="36.2.2">
<h3>
<span class="header-section-number">36.2.2</span> Estimation Methods<a class="anchor" aria-label="anchor" href="#estimation-methods-2"><i class="fas fa-link"></i></a>
</h3>
<div id="heckmans-two-step-estimator-heckit" class="section level4" number="36.2.2.1">
<h4>
<span class="header-section-number">36.2.2.1</span> Heckman’s Two-Step Estimator (Heckit)<a class="anchor" aria-label="anchor" href="#heckmans-two-step-estimator-heckit"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Step 1: Estimate Selection Equation with Probit</strong></p>
<p>We estimate the probability of being included in the sample: <span class="math display">\[ P(w_i = 1 \mid z_i) = \Phi(z_i' \gamma) \]</span></p>
<p>From the estimated model, we compute the <strong>Inverse Mills Ratio (IMR)</strong>: <span class="math display">\[ \lambda_i = \frac{\phi(z_i' \hat{\gamma})}{\Phi(z_i' \hat{\gamma})} \]</span></p>
<p>This term captures the expected value of the error in the outcome equation, conditional on selection.</p>
<p><strong>Step 2: Include IMR in Outcome Equation</strong></p>
<p>We then estimate the regression: <span class="math display">\[ y_i = x_i' \beta + \delta \lambda_i + \nu_i \]</span></p>
<ul>
<li>If <span class="math inline">\(\delta\)</span> is significantly different from 0, selection bias is present.</li>
<li>
<span class="math inline">\(\lambda_i\)</span> corrects for the non-random selection.</li>
<li>OLS on this augmented model yields consistent estimates of <span class="math inline">\(\beta\)</span> under the joint normality assumption.</li>
<li>Pros: Conceptually simple; widely used.</li>
<li>Cons: Relies heavily on the bivariate normal assumption for <span class="math inline">\((\varepsilon_i, u_i)\)</span>. If no good exclusion variable is available, identification rests on the functional form.</li>
</ul>
<p>Specifically, the model can be identified <strong>without an exclusion restriction</strong>, but in such cases, identification is driven purely by the <strong>non-linearity</strong> of the probit function and the normality assumption (through the IMR). This is <strong>fragile</strong>.</p>
<ul>
<li>With strong exclusion restriction for the covariate in the correction equation, the variation in this variable can help identify the control for selection.</li>
<li>With weak exclusion restriction, and the variable exists in both steps, it’s the assumed error structure that identifies the control for selection <span class="citation">(<a href="references.html#ref-heckman2004using">J. Heckman and Navarro-Lozano 2004</a>)</span>.</li>
<li>In management, <span class="citation">Wolfolds and Siegel (<a href="references.html#ref-wolfolds2019misaccounting">2019</a>)</span> found that papers should have valid exclusion conditions, because without these, simulations show that results using the Heckman method are less reliable than those obtained with OLS.</li>
</ul>
<p>For robust identification, we prefer an <strong>exclusion restriction</strong>:</p>
<ul>
<li>A variable that affects selection (through <span class="math inline">\(z_i\)</span>) but not the outcome.</li>
<li>Example: Distance to a training center might affect the probability of enrollment, but not post-training income.</li>
</ul>
<p>Without such a variable, the model relies solely on functional form.</p>
<p>The Heckman two-step estimation procedure is less efficient than FIML. One key limitation is that the two-step estimator does not fully exploit the joint distribution of the error terms across equations, leading to a loss of efficiency. Moreover, the two-step approach may introduce <strong>measurement error</strong> in the second stage. This arises because the inverse Mills ratio used in the second stage is itself an estimated regressor, which can lead to biased standard errors and inference.</p>
<div class="sourceCode" id="cb960"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">###########################</span></span>
<span><span class="co">#   SIM 1: Heckman 2-step #</span></span>
<span><span class="co">###########################</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">rho</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">beta_true</span> <span class="op">&lt;-</span> <span class="fl">2</span></span>
<span></span>
<span><span class="va">gamma_true</span> <span class="op">&lt;-</span> <span class="fl">1.0</span></span>
<span><span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">rho</span>, <span class="va">rho</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">errors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>, <span class="va">Sigma</span><span class="op">)</span></span>
<span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="va">errors</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">u</span>       <span class="op">&lt;-</span> <span class="va">errors</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">w</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Selection</span></span>
<span><span class="va">z_star</span> <span class="op">&lt;-</span> <span class="va">w</span><span class="op">*</span><span class="va">gamma_true</span> <span class="op">+</span> <span class="va">u</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">z_star</span><span class="op">&gt;</span><span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Outcome</span></span>
<span></span>
<span><span class="va">y_star</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">*</span> <span class="va">beta_true</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span><span class="co"># Observed only if z=1</span></span>
<span><span class="va">y_obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">z</span> <span class="op">==</span> <span class="fl">1</span>, <span class="va">y_star</span>, <span class="cn">NA</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 1: Probit</span></span>
<span><span class="va">sel_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">z</span> <span class="op">~</span> <span class="va">w</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"probit"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">z_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">sel_mod</span>, type <span class="op">=</span> <span class="st">"link"</span><span class="op">)</span></span>
<span><span class="va">lambda_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">z_hat</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">z_hat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: OLS on observed + IMR</span></span>
<span><span class="va">data_heck</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">y_obs</span>,</span>
<span>                        x <span class="op">=</span> <span class="va">x</span>,</span>
<span>                        imr <span class="op">=</span> <span class="va">lambda_vals</span>,</span>
<span>                        z <span class="op">=</span> <span class="va">z</span><span class="op">)</span></span>
<span><span class="va">observed_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/subset.html">subset</a></span><span class="op">(</span><span class="va">data_heck</span>, <span class="va">z</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">heck_lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span> <span class="op">+</span> <span class="va">imr</span>, data <span class="op">=</span> <span class="va">observed_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck_lm</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ x + imr, data = observed_data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -2.76657 -0.60099 -0.02776  0.56317  2.74797 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.01715    0.07068   0.243    0.808    </span></span>
<span><span class="co">#&gt; x            1.95925    0.03934  49.800  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; imr          0.41900    0.10063   4.164 3.69e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.8942 on 501 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8332, Adjusted R-squared:  0.8325 </span></span>
<span><span class="co">#&gt; F-statistic:  1251 on 2 and 501 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"True beta="</span>, <span class="va">beta_true</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; True beta= 2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Heckman 2-step estimated beta="</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">heck_lm</span><span class="op">)</span><span class="op">[</span><span class="st">"x"</span><span class="op">]</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Heckman 2-step estimated beta= 1.959249</span></span></code></pre></div>
</div>
<div id="full-information-maximum-likelihood" class="section level4" number="36.2.2.2">
<h4>
<span class="header-section-number">36.2.2.2</span> Full Information Maximum Likelihood<a class="anchor" aria-label="anchor" href="#full-information-maximum-likelihood"><i class="fas fa-link"></i></a>
</h4>
<p>Jointly estimates the selection and outcome equations via ML, assuming:</p>
<p><span class="math display">\[
\biggl(\varepsilon_i, u_i\biggr) \sim \mathcal{N}\biggl(\begin{pmatrix}0\\0\end{pmatrix},\begin{pmatrix}\sigma_{\varepsilon}^2 &amp; \rho\,\sigma_{\varepsilon}\\\rho\,\sigma_{\varepsilon} &amp; 1\end{pmatrix}\biggr).
\]</span></p>
<ul>
<li>Pros: More efficient if the distributional assumption is correct. Allows a direct test of <span class="math inline">\(\rho=0\)</span> (LR test).</li>
<li>Cons: More sensitive to specification errors (i.e., requires stronger distributional assumptions); potentially complex to implement.</li>
</ul>
<p>We can use the <strong><code>sampleSelection</code></strong> package in R to perform full maximum likelihood estimation for the same data:</p>
<div class="sourceCode" id="cb961"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#############################</span></span>
<span><span class="co"># SIM 2: 2-step vs. FIML    #</span></span>
<span><span class="co">#############################</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Using same data (z, x, y_obs) from above</span></span>
<span></span>
<span><span class="co"># 1) Heckman 2-step (built-in)</span></span>
<span><span class="va">heck2</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">z</span> <span class="op">~</span> <span class="va">w</span>, <span class="va">y_obs</span> <span class="op">~</span> <span class="va">x</span>, method <span class="op">=</span> <span class="st">"2step"</span>, data <span class="op">=</span> <span class="va">data_heck</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck2</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; 2-step Heckman / heckit estimation</span></span>
<span><span class="co">#&gt; 1000 observations (496 censored and 504 observed)</span></span>
<span><span class="co">#&gt; 7 free parameters (df = 994)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02053    0.04494   0.457    0.648    </span></span>
<span><span class="co">#&gt; w            0.94063    0.05911  15.913   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.01715    0.07289   0.235    0.814    </span></span>
<span><span class="co">#&gt; x            1.95925    0.03924  49.932   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Multiple R-Squared:0.8332,   Adjusted R-Squared:0.8325</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; invMillsRatio   0.4190     0.1018   4.116 4.18e-05 ***</span></span>
<span><span class="co">#&gt; sigma           0.9388         NA      NA       NA    </span></span>
<span><span class="co">#&gt; rho             0.4463         NA      NA       NA    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span></span>
<span><span class="co"># 2) FIML</span></span>
<span><span class="va">heckFIML</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">z</span> <span class="op">~</span> <span class="va">w</span>, <span class="va">y_obs</span> <span class="op">~</span> <span class="va">x</span>, method <span class="op">=</span> <span class="st">"ml"</span>, data <span class="op">=</span> <span class="va">data_heck</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heckFIML</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 2 iterations</span></span>
<span><span class="co">#&gt; Return code 8: successive function values within relative tolerance limit (reltol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -1174.233 </span></span>
<span><span class="co">#&gt; 1000 observations (496 censored and 504 observed)</span></span>
<span><span class="co">#&gt; 6 free parameters (df = 994)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02169    0.04488   0.483    0.629    </span></span>
<span><span class="co">#&gt; w            0.94203    0.05908  15.945   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 0.008601   0.071315   0.121    0.904    </span></span>
<span><span class="co">#&gt; x           1.959195   0.039124  50.077   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma  0.94118    0.03503  26.867  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; rho    0.46051    0.09411   4.893 1.16e-06 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>You can compare the coefficient estimates on <code>x</code> from <code>heck2</code> vs. <code>heckFIML</code>. In large samples, both should converge to the true <span class="math inline">\(\beta\)</span>. FIML is typically more efficient, but if the normality assumption is violated, both can be biased.</p>
</div>
<div id="cf-and-iv-approaches" class="section level4" number="36.2.2.3">
<h4>
<span class="header-section-number">36.2.2.3</span> CF and IV Approaches<a class="anchor" aria-label="anchor" href="#cf-and-iv-approaches"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><strong>Control Function</strong></li>
</ol>
<ul>
<li>Residual-based approach: Regress the selection (or treatment) variable on excluded instruments and included controls. Obtain the predicted residual. Include that residual in the main outcome regression.</li>
<li>If correlated residual is significant, that indicates endogeneity; adjusting for it can correct bias.</li>
<li>Often used in the context of treatment effect models or simultaneously with IV logic.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Instrumental Variables</strong></li>
</ol>
<ul>
<li>In the pure treatment effect context, an IV must affect treatment assignment but not the outcome directly.</li>
<li>For sample selection, an exclusion restriction (“instrument”) must shift selection but not outcomes.</li>
<li>Example: Distance to a training center influences participation in a job program but not post-training earnings.</li>
</ul>
<hr>
</div>
</div>
<div id="theoretical-connections" class="section level3" number="36.2.3">
<h3>
<span class="header-section-number">36.2.3</span> Theoretical Connections<a class="anchor" aria-label="anchor" href="#theoretical-connections"><i class="fas fa-link"></i></a>
</h3>
<div id="conditional-expectation-from-truncated-distributions" class="section level4" number="36.2.3.1">
<h4>
<span class="header-section-number">36.2.3.1</span> Conditional Expectation from Truncated Distributions<a class="anchor" aria-label="anchor" href="#conditional-expectation-from-truncated-distributions"><i class="fas fa-link"></i></a>
</h4>
<p>In the sample selection scenario:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[y_i\mid w_i=1] &amp;= x_i\beta + \mathbb{E}[\varepsilon_i\mid w_i^*&gt;0],\\
&amp;= x_i\beta + \rho\,\sigma_{\varepsilon}\,\frac{\phi(z_i'\gamma)}{\Phi(z_i'\gamma)},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\rho\,\sigma_{\varepsilon}\)</span> is the covariance term and <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are the standard normal PDF and CDF, respectively. This formula underpins the inverse Mills ratio correction.</p>
<ul>
<li>If <span class="math inline">\(\rho&gt;0\)</span>, then the same unobservables that increase the likelihood of selection also increase outcomes, implying positive selection.</li>
<li>If <span class="math inline">\(\rho&lt;0\)</span>, selection is negatively correlated with outcomes.</li>
<li>
<span class="math inline">\(\hat{\rho}\)</span>: Estimated correlation of error terms. If significantly different from 0, endogenous selection is present.</li>
<li>Wald or Likelihood Ratio Test: Used to test <span class="math inline">\(H_0: \rho = 0\)</span>.</li>
<li>Lambda (<span class="math inline">\(\hat{\lambda}\)</span>): Product of <span class="math inline">\(\hat{\rho} \hat{\sigma}_\varepsilon\)</span>—measures strength of selection bias.</li>
<li>Inverse Mills Ratio: Can be saved and inspected to understand sample inclusion probabilities.</li>
</ul>
</div>
<div id="relationship-among-models" class="section level4" number="36.2.3.2">
<h4>
<span class="header-section-number">36.2.3.2</span> Relationship Among Models<a class="anchor" aria-label="anchor" href="#relationship-among-models"><i class="fas fa-link"></i></a>
</h4>
<p>All the models in <a href="endogeneity.html#unifying-model-frameworks">Unifying Model Frameworks</a> can be seen as special or generalized cases:</p>
<ul>
<li><p>If one only has data for the selected group, it’s a sample selection setup.</p></li>
<li><p>If data for both groups exist, but assignment is endogenous, it’s a treatment effect problem.</p></li>
<li><p>If there’s a valid instrument, one can do a control function or IV approach.</p></li>
<li><p>If the normality assumption holds and selection is truly parametric, Heckman or FIML correct for the bias.</p></li>
</ul>
<p>Summary Table of Methods</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="16%">
<col width="17%">
<col width="16%">
<col width="16%">
<col width="16%">
<col width="16%">
</colgroup>
<thead><tr class="header">
<th><strong>Method</strong></th>
<th><strong>Data Observed</strong></th>
<th><strong>Key Assumption</strong></th>
<th><strong>Exclusion?</strong></th>
<th><strong>Pros</strong></th>
<th><strong>Cons</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>OLS (Naive)</td>
<td>Full or partial, ignoring selection</td>
<td>No endogeneity in errors</td>
<td>Not required</td>
<td>Simple to implement</td>
<td>Biased if endogeneity is present</td>
</tr>
<tr class="even">
<td>Heckman 2-Step (Heckit)</td>
<td>Outcome only for selected group</td>
<td>Joint normal errors; linear functional</td>
<td>Strongly recommended</td>
<td>Intuitive, widely used</td>
<td>Sensitive to normality/functional form.</td>
</tr>
<tr class="odd">
<td>FIML (Full ML)</td>
<td>Same as Heckman (subset observed)</td>
<td>Joint normal errors</td>
<td>Strongly recommended</td>
<td>More efficient, direct test of <span class="math inline">\(\rho=0\)</span>
</td>
<td>Complex, more sensitive to misspecification</td>
</tr>
<tr class="even">
<td>Control Function</td>
<td>Observed data for both or one group (depending on setup)</td>
<td>Some form of valid instrument or exog.</td>
<td>Yes (instrument)</td>
<td>Extends easily to many models</td>
<td>Must find valid instrument, no direct test for endogeneity</td>
</tr>
<tr class="odd">
<td>Instrumental Variables</td>
<td>Observed data for both groups, or entire sample (for selection)</td>
<td>IV must affect selection but not outcome</td>
<td>Yes (instrument)</td>
<td>Common approach in program evaluation</td>
<td>Exclusion restriction validity is critical</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="concerns" class="section level4" number="36.2.3.3">
<h4>
<span class="header-section-number">36.2.3.3</span> Concerns<a class="anchor" aria-label="anchor" href="#concerns"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Small Samples</strong>: Two-step procedures can be unstable in smaller datasets.</li>
<li>
<strong>Exclusion Restrictions</strong>: Without a credible variable that predicts selection but not outcomes, identification depends purely on functional form (bivariate normal + nonlinearity of probit).</li>
<li>
<strong>Distributional Assumption</strong>: If normality is seriously violated, neither 2-step nor FIML may reliably remove bias.</li>
<li>
<strong>Measurement Error in IMR</strong>: The second-stage includes an <em>estimated</em> regressor <span class="math inline">\(\hat{\lambda}_i\)</span>, which can add noise.</li>
<li>
<strong>Connection to IV</strong>: If a strong instrument exists, one could proceed with a control function or standard IV in a treatment effect setup. But for sample selection (lack of data on unselected), the Heckman approach is more common.</li>
<li>
<strong>Presence of Correlation between the Error Terms</strong>: The Heckman treatment effect model outperforms OLS when <span class="math inline">\(\rho \neq 0\)</span> because it corrects for selection bias due to unobserved factors. However, when <span class="math inline">\(\rho = 0\)</span>, the correction is unnecessary and can introduce inefficiency, making simpler methods more accurate.</li>
</ol>
<hr>
</div>
</div>
<div id="tobit-2-heckmans-sample-selection-model" class="section level3" number="36.2.4">
<h3>
<span class="header-section-number">36.2.4</span> Tobit-2: Heckman’s Sample Selection Model<a class="anchor" aria-label="anchor" href="#tobit-2-heckmans-sample-selection-model"><i class="fas fa-link"></i></a>
</h3>
<p>The Tobit-2 model, also known as <strong>Heckman’s standard sample selection model</strong>, is designed to correct for sample selection bias. This arises when the outcome variable is only observed for a non-random subset of the population, and the selection process is correlated with the outcome of interest.</p>
<p>A key assumption of the model is the <strong>joint normality of the error terms</strong> in the selection and outcome equations.</p>
<div id="panel-study-of-income-dynamics" class="section level4" number="36.2.4.1">
<h4>
<span class="header-section-number">36.2.4.1</span> Panel Study of Income Dynamics<a class="anchor" aria-label="anchor" href="#panel-study-of-income-dynamics"><i class="fas fa-link"></i></a>
</h4>
<p>We demonstrate the model using the classic dataset from <span class="citation">Mroz (<a href="references.html#ref-mroz1984sensitivity">1984</a>)</span>, which provides data from the 1975 Panel Study of Income Dynamics on married women’s labor-force participation and wages.</p>
<p>We aim to estimate the log of hourly wages for married women, using:</p>
<ul>
<li>
<code>educ</code>: Years of education</li>
<li>
<code>exper</code>: Years of work experience</li>
<li>
<code>exper^2</code>: Experience squared (to capture non-linear effects)</li>
<li>
<code>city</code>: A dummy for residence in a big city</li>
</ul>
<p>However, wages are only observed for those who participated in the labor force, meaning an OLS regression using only this subsample would suffer from selection bias.</p>
<p>Because we also have data on non-participants, we can use Heckman’s two-step method to correct for this bias.</p>
<hr>
<ol style="list-style-type: decimal">
<li><strong>Load and Prepare Data</strong></li>
</ol>
<div class="sourceCode" id="cb962"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/hadley/reshape">reshape2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Mroz87"</span><span class="op">)</span>  <span class="co"># PSID data on married women in 1975</span></span>
<span><span class="va">Mroz87</span> <span class="op">=</span> <span class="va">Mroz87</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>kids <span class="op">=</span> <span class="va">kids5</span> <span class="op">+</span> <span class="va">kids618</span><span class="op">)</span>  <span class="co"># total number of children</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Mroz87</span><span class="op">)</span></span>
<span><span class="co">#&gt;   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage</span></span>
<span><span class="co">#&gt; 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288</span></span>
<span><span class="co">#&gt; 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416</span></span>
<span><span class="co">#&gt; 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807</span></span>
<span><span class="co">#&gt; 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417</span></span>
<span><span class="co">#&gt; 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000</span></span>
<span><span class="co">#&gt; 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106</span></span>
<span><span class="co">#&gt;   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll</span></span>
<span><span class="co">#&gt; 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE</span></span>
<span><span class="co">#&gt; 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE</span></span>
<span><span class="co">#&gt;   kids</span></span>
<span><span class="co">#&gt; 1    1</span></span>
<span><span class="co">#&gt; 2    2</span></span>
<span><span class="co">#&gt; 3    4</span></span>
<span><span class="co">#&gt; 4    3</span></span>
<span><span class="co">#&gt; 5    3</span></span>
<span><span class="co">#&gt; 6    0</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li><strong>Model Overview</strong></li>
</ol>
<p>The two-step Heckman selection model proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Selection Equation (Probit)</strong>:<br>
Models the probability of labor force participation (<code>lfp = 1</code>) as a function of variables that affect the decision to work.</p></li>
<li><p><strong>Outcome Equation (Wage)</strong>:<br>
Models log wages conditional on working. A correction term, the IMR, is included to account for the non-random selection into work.</p></li>
</ol>
<p>Step 1: Naive OLS on Observed Wages</p>
<div class="sourceCode" id="cb963"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ols1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span>,</span>
<span>          data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/subset.html">subset</a></span><span class="op">(</span><span class="va">Mroz87</span>, <span class="va">lfp</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ols1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(wage) ~ educ + exper + I(exper^2) + city, data = subset(Mroz87, </span></span>
<span><span class="co">#&gt;     lfp == 1))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -3.10084 -0.32453  0.05292  0.36261  2.34806 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.5308476  0.1990253  -2.667  0.00794 ** </span></span>
<span><span class="co">#&gt; educ         0.1057097  0.0143280   7.378 8.58e-13 ***</span></span>
<span><span class="co">#&gt; exper        0.0410584  0.0131963   3.111  0.00199 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0007973  0.0003938  -2.025  0.04352 *  </span></span>
<span><span class="co">#&gt; city         0.0542225  0.0680903   0.796  0.42629    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.6667 on 423 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.1581, Adjusted R-squared:  0.1501 </span></span>
<span><span class="co">#&gt; F-statistic: 19.86 on 4 and 423 DF,  p-value: 5.389e-15</span></span></code></pre></div>
<p>This OLS is biased because it only includes women who chose to work.</p>
<p>Step 2: Heckman Two-Step Estimation</p>
<div class="sourceCode" id="cb964"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Heckman 2-step estimation</span></span>
<span><span class="va">heck1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">heckit</a></span><span class="op">(</span></span>
<span>    selection <span class="op">=</span> <span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>    outcome <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span>,</span>
<span>    data <span class="op">=</span> <span class="va">Mroz87</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stage 1: Selection equation (probit)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck1</span><span class="op">$</span><span class="va">probit</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Probit binary choice model/Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 4 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -482.8212 </span></span>
<span><span class="co">#&gt; Model: Y == '1' in contrary to '0'</span></span>
<span><span class="co">#&gt; 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)</span></span>
<span><span class="co">#&gt; Estimates:</span></span>
<span><span class="co">#&gt;                  Estimate  Std. error t value   Pr(&gt; t)    </span></span>
<span><span class="co">#&gt; XS(Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** </span></span>
<span><span class="co">#&gt; XSage          0.18608901  0.06517476  2.8552  0.004301 ** </span></span>
<span><span class="co">#&gt; XSI(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** </span></span>
<span><span class="co">#&gt; XSkids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***</span></span>
<span><span class="co">#&gt; XShuswage     -0.04303635  0.01220791 -3.5253  0.000423 ***</span></span>
<span><span class="co">#&gt; XSeduc         0.12502818  0.02277645  5.4894 4.034e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; Significance test:</span></span>
<span><span class="co">#&gt; chi2(5) = 64.10407 (p=1.719042e-12)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span></span>
<span><span class="co"># Stage 2: Wage equation with selection correction</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck1</span><span class="op">$</span><span class="va">lm</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = YO ~ -1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -3.09494 -0.30953  0.05341  0.36530  2.34770 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; XO(Intercept) -0.6143381  0.3768796  -1.630  0.10383    </span></span>
<span><span class="co">#&gt; XOeduc         0.1092363  0.0197062   5.543 5.24e-08 ***</span></span>
<span><span class="co">#&gt; XOexper        0.0419205  0.0136176   3.078  0.00222 ** </span></span>
<span><span class="co">#&gt; XOI(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  </span></span>
<span><span class="co">#&gt; XOcity         0.0510492  0.0692414   0.737  0.46137    </span></span>
<span><span class="co">#&gt; imrData$IMR1   0.0551177  0.2111916   0.261  0.79423    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.6674 on 422 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.7734, Adjusted R-squared:  0.7702 </span></span>
<span><span class="co">#&gt; F-statistic:   240 on 6 and 422 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The variable <code>kids</code> is used only in the selection equation. This follows good practice: at least one variable should appear only in the selection equation (serving as an instrument) to help identify the model.</p>
<div class="sourceCode" id="cb965"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># ML estimation of Heckman selection model</span></span>
<span><span class="va">ml1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span></span>
<span>  selection <span class="op">=</span> <span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>  outcome <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Mroz87</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ml1</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 3 iterations</span></span>
<span><span class="co">#&gt; Return code 8: successive function values within relative tolerance limit (reltol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -914.0777 </span></span>
<span><span class="co">#&gt; 753 observations (325 censored and 428 observed)</span></span>
<span><span class="co">#&gt; 13 free parameters (df = 740)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -4.1484037  1.4109302  -2.940 0.003382 ** </span></span>
<span><span class="co">#&gt; age          0.1842132  0.0658041   2.799 0.005253 ** </span></span>
<span><span class="co">#&gt; I(age^2)    -0.0023925  0.0007664  -3.122 0.001868 ** </span></span>
<span><span class="co">#&gt; kids        -0.1488158  0.0384888  -3.866 0.000120 ***</span></span>
<span><span class="co">#&gt; huswage     -0.0434253  0.0123229  -3.524 0.000451 ***</span></span>
<span><span class="co">#&gt; educ         0.1255639  0.0229229   5.478 5.91e-08 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.5814781  0.3052031  -1.905  0.05714 .  </span></span>
<span><span class="co">#&gt; educ         0.1078481  0.0172998   6.234 7.63e-10 ***</span></span>
<span><span class="co">#&gt; exper        0.0415752  0.0133269   3.120  0.00188 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0008125  0.0003974  -2.044  0.04129 *  </span></span>
<span><span class="co">#&gt; city         0.0522990  0.0682652   0.766  0.44385    </span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma  0.66326    0.02309  28.729   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho    0.05048    0.23169   0.218    0.828    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>The MLE approach jointly estimates both equations, yielding consistent and asymptotically efficient estimates.</p>
<p>Manual Implementation: Constructing the IMR</p>
<div class="sourceCode" id="cb966"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 1: Probit model</span></span>
<span><span class="va">myprob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/binaryChoice.html">probit</a></span><span class="op">(</span><span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>                 data <span class="op">=</span> <span class="va">Mroz87</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">myprob</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Probit binary choice model/Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 4 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -482.8212 </span></span>
<span><span class="co">#&gt; Model: Y == '1' in contrary to '0'</span></span>
<span><span class="co">#&gt; 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)</span></span>
<span><span class="co">#&gt; Estimates:</span></span>
<span><span class="co">#&gt;                Estimate  Std. error t value   Pr(&gt; t)    </span></span>
<span><span class="co">#&gt; (Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** </span></span>
<span><span class="co">#&gt; age          0.18608901  0.06517476  2.8552  0.004301 ** </span></span>
<span><span class="co">#&gt; I(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** </span></span>
<span><span class="co">#&gt; kids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***</span></span>
<span><span class="co">#&gt; huswage     -0.04303635  0.01220791 -3.5253  0.000423 ***</span></span>
<span><span class="co">#&gt; educ         0.12502818  0.02277645  5.4894 4.034e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; Significance test:</span></span>
<span><span class="co">#&gt; chi2(5) = 64.10407 (p=1.719042e-12)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span></span>
<span><span class="co"># Step 2: Compute IMR</span></span>
<span><span class="va">imr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/invMillsRatio.html">invMillsRatio</a></span><span class="op">(</span><span class="va">myprob</span><span class="op">)</span></span>
<span><span class="va">Mroz87</span><span class="op">$</span><span class="va">IMR1</span> <span class="op">&lt;-</span> <span class="va">imr</span><span class="op">$</span><span class="va">IMR1</span></span>
<span></span>
<span><span class="co"># Step 3: Wage regression including IMR</span></span>
<span><span class="va">manually_est</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span> <span class="op">+</span> <span class="va">IMR1</span>,</span>
<span>                   data <span class="op">=</span> <span class="va">Mroz87</span>,</span>
<span>                   subset <span class="op">=</span> <span class="op">(</span><span class="va">lfp</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">manually_est</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(wage) ~ educ + exper + I(exper^2) + city + IMR1, </span></span>
<span><span class="co">#&gt;     data = Mroz87, subset = (lfp == 1))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -3.09494 -0.30953  0.05341  0.36530  2.34770 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.6143381  0.3768796  -1.630  0.10383    </span></span>
<span><span class="co">#&gt; educ         0.1092363  0.0197062   5.543 5.24e-08 ***</span></span>
<span><span class="co">#&gt; exper        0.0419205  0.0136176   3.078  0.00222 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  </span></span>
<span><span class="co">#&gt; city         0.0510492  0.0692414   0.737  0.46137    </span></span>
<span><span class="co">#&gt; IMR1         0.0551177  0.2111916   0.261  0.79423    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.6674 on 422 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.1582, Adjusted R-squared:  0.1482 </span></span>
<span><span class="co">#&gt; F-statistic: 15.86 on 5 and 422 DF,  p-value: 2.505e-14</span></span></code></pre></div>
<p>Equivalent Method Using <code>glm</code> and Manual IMR Calculation</p>
<div class="sourceCode" id="cb967"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Probit via glm</span></span>
<span><span class="va">probit_selection</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Mroz87</span>,</span>
<span>  family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">'probit'</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute predicted latent index and IMR</span></span>
<span><span class="va">probit_lp</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">probit_selection</span><span class="op">)</span></span>
<span><span class="va">inv_mills</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">probit_lp</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">probit_lp</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Mroz87</span><span class="op">$</span><span class="va">inv_mills</span> <span class="op">&lt;-</span> <span class="va">inv_mills</span></span>
<span></span>
<span><span class="co"># Second stage: Wage regression with correction</span></span>
<span><span class="va">probit_outcome</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span> <span class="op">+</span> <span class="va">inv_mills</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Mroz87</span>,</span>
<span>  subset <span class="op">=</span> <span class="op">(</span><span class="va">lfp</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">probit_outcome</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = log(wage) ~ educ + exper + I(exper^2) + city + </span></span>
<span><span class="co">#&gt;     inv_mills, data = Mroz87, subset = (lfp == 1))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.6143383  0.3768798  -1.630  0.10383    </span></span>
<span><span class="co">#&gt; educ         0.1092363  0.0197062   5.543 5.24e-08 ***</span></span>
<span><span class="co">#&gt; exper        0.0419205  0.0136176   3.078  0.00222 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  </span></span>
<span><span class="co">#&gt; city         0.0510492  0.0692414   0.737  0.46137    </span></span>
<span><span class="co">#&gt; inv_mills    0.0551179  0.2111918   0.261  0.79423    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for gaussian family taken to be 0.4454809)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 223.33  on 427  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 187.99  on 422  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 876.49</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 2</span></span></code></pre></div>
<p>Comparing Models</p>
<div class="sourceCode" id="cb968"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stargazer</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://cran.r-project.org/package=plm">plm</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/">sandwich</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Custom robust SE function</span></span>
<span><span class="va">cse</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">reg</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">reg</span>, type <span class="op">=</span> <span class="st">"HC1"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Comparison table</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span></span>
<span>  <span class="va">ols1</span>, <span class="va">heck1</span>, <span class="va">ml1</span>, <span class="va">manually_est</span>,</span>
<span>  se <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="fu">cse</span><span class="op">(</span><span class="va">ols1</span><span class="op">)</span>, <span class="cn">NULL</span>, <span class="cn">NULL</span>, <span class="cn">NULL</span><span class="op">)</span>,</span>
<span>  title <span class="op">=</span> <span class="st">"Married Women's Wage Regressions: OLS vs Heckman Models"</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>  df <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  digits <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  selection.equation <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Married Women's Wage Regressions: OLS vs Heckman Models</span></span>
<span><span class="co">#&gt; =========================================================================</span></span>
<span><span class="co">#&gt;                                      Dependent variable:                 </span></span>
<span><span class="co">#&gt;                     -----------------------------------------------------</span></span>
<span><span class="co">#&gt;                     log(wage)                lfp               log(wage) </span></span>
<span><span class="co">#&gt;                        OLS         Heckman        selection       OLS    </span></span>
<span><span class="co">#&gt;                                   selection                              </span></span>
<span><span class="co">#&gt;                        (1)           (2)             (3)          (4)    </span></span>
<span><span class="co">#&gt; -------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; age                               0.1861***       0.1842***              </span></span>
<span><span class="co">#&gt;                                   (0.0652)        (0.0658)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; I(age2)                          -0.0024***      -0.0024***              </span></span>
<span><span class="co">#&gt;                                   (0.0008)        (0.0008)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; kids                             -0.1496***      -0.1488***              </span></span>
<span><span class="co">#&gt;                                   (0.0383)        (0.0385)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; huswage                          -0.0430***      -0.0434***              </span></span>
<span><span class="co">#&gt;                                   (0.0122)        (0.0123)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; educ                0.1057***     0.1250***       0.1256***    0.1092*** </span></span>
<span><span class="co">#&gt;                      (0.0130)     (0.0228)        (0.0229)      (0.0197) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; exper               0.0411***                                  0.0419*** </span></span>
<span><span class="co">#&gt;                      (0.0154)                                   (0.0136) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; I(exper2)            -0.0008*                                  -0.0008** </span></span>
<span><span class="co">#&gt;                      (0.0004)                                   (0.0004) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; city                  0.0542                                     0.0510  </span></span>
<span><span class="co">#&gt;                      (0.0653)                                   (0.0692) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; IMR1                                                             0.0551  </span></span>
<span><span class="co">#&gt;                                                                 (0.2112) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; Constant            -0.5308***   -4.1815***      -4.1484***     -0.6143  </span></span>
<span><span class="co">#&gt;                      (0.2032)     (1.4024)        (1.4109)      (0.3769) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; -------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; Observations           428           753             753          428    </span></span>
<span><span class="co">#&gt; R2                    0.1581       0.1582                        0.1582  </span></span>
<span><span class="co">#&gt; Adjusted R2           0.1501       0.1482                        0.1482  </span></span>
<span><span class="co">#&gt; Log Likelihood                                    -914.0777              </span></span>
<span><span class="co">#&gt; rho                                0.0830      0.0505 (0.2317)           </span></span>
<span><span class="co">#&gt; Inverse Mills Ratio            0.0551 (0.2099)                           </span></span>
<span><span class="co">#&gt; Residual Std. Error   0.6667                                     0.6674  </span></span>
<span><span class="co">#&gt; F Statistic         19.8561***                                 15.8635***</span></span>
<span><span class="co">#&gt; =========================================================================</span></span>
<span><span class="co">#&gt; Note:                                         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</span></span></code></pre></div>
<ul>
<li><p>IMR: If the coefficient on the IMR is statistically significant, it suggests selection bias is present and that OLS estimates are biased.</p></li>
<li><p><span class="math inline">\(\rho\)</span>: Represents the estimated correlation between the error terms of the selection and outcome equations. A significant <span class="math inline">\(\rho\)</span> implies non-random selection, justifying the Heckman correction.</p></li>
<li><p>In our case, if the IMR coefficient is not statistically different from zero, then selection bias may not be a serious concern.</p></li>
</ul>
</div>
<div id="the-role-of-exclusion-restrictions-in-heckmans-model" class="section level4" number="36.2.4.2">
<h4>
<span class="header-section-number">36.2.4.2</span> The Role of Exclusion Restrictions in Heckman’s Model<a class="anchor" aria-label="anchor" href="#the-role-of-exclusion-restrictions-in-heckmans-model"><i class="fas fa-link"></i></a>
</h4>
<p>This example, adapted from the <code>sampleSelection</code>, demonstrates the identification of the sample selection model using simulated data.</p>
<p>We compare two cases:</p>
<ul>
<li><p>One where an <strong>exclusion restriction</strong> is present (i.e., selection and outcome equations use different regressors).</p></li>
<li><p>One where the <strong>same regressor</strong> is used in both equations.</p></li>
</ul>
<hr>
<p><strong>Case 1: With Exclusion Restriction</strong></p>
<div class="sourceCode" id="cb969"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Simulate bivariate normal error terms with correlation -0.7</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span><span class="fl">500</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="op">-</span><span class="fl">0.7</span>,<span class="op">-</span><span class="fl">0.7</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Independent explanatory variable for selection equation</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Selection: Probit model (latent utility model)</span></span>
<span><span class="va">ys_latent</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">ys_latent</span> <span class="op">&gt;</span> <span class="fl">0</span>  <span class="co"># observed participation indicator (TRUE/FALSE)</span></span>
<span></span>
<span><span class="co"># Independent explanatory variable for outcome equation</span></span>
<span><span class="va">xo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Latent outcome variable</span></span>
<span><span class="va">yo_latent</span> <span class="op">&lt;-</span> <span class="va">xo</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Observed outcome: only when selected (ys == TRUE)</span></span>
<span><span class="va">yo</span> <span class="op">&lt;-</span> <span class="va">yo_latent</span> <span class="op">*</span> <span class="va">ys</span></span>
<span></span>
<span><span class="co"># Estimate Heckman's selection model</span></span>
<span><span class="va">model_with_exclusion</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span>selection <span class="op">=</span> <span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, outcome <span class="op">=</span> <span class="va">yo</span> <span class="op">~</span> <span class="va">xo</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_with_exclusion</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 5 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -712.3163 </span></span>
<span><span class="co">#&gt; 500 observations (172 censored and 328 observed)</span></span>
<span><span class="co">#&gt; 6 free parameters (df = 494)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -0.2228     0.1081  -2.061   0.0399 *  </span></span>
<span><span class="co">#&gt; xs            1.3377     0.2014   6.642 8.18e-11 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.0002265  0.1294178  -0.002    0.999    </span></span>
<span><span class="co">#&gt; xo           0.7299070  0.1635925   4.462 1.01e-05 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma   0.9190     0.0574  16.009  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; rho    -0.5392     0.1521  -3.544 0.000431 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p><strong>Key Observations:</strong></p>
<ul>
<li><p>The variables <code>xs</code> and <code>xo</code> are <strong>independent</strong>, fulfilling the <strong>exclusion restriction</strong>.</p></li>
<li><p>The outcome equation is identified not only by the non-linearity of the model but also by the presence of a regressor in the selection equation that is absent from the outcome equation.</p></li>
</ul>
<p>This mirrors realistic scenarios in applied business settings, such as:</p>
<ul>
<li><p>Participation in the labor force driven by family or geographic factors (<code>xs</code>), while wages depend on skills or education (<code>xo</code>).</p></li>
<li><p>Loan application driven by personal risk preferences, while interest rates depend on credit score or income.</p></li>
</ul>
<hr>
<p><strong>Case 2: Without Exclusion Restriction</strong></p>
<div class="sourceCode" id="cb970"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Now use the same regressor (xs) in both equations</span></span>
<span><span class="va">yo_latent2</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">yo_latent2</span> <span class="op">*</span> <span class="va">ys</span></span>
<span></span>
<span><span class="co"># Re-estimate model without exclusion restriction</span></span>
<span><span class="va">model_no_exclusion</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span>selection <span class="op">=</span> <span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, outcome <span class="op">=</span> <span class="va">yo2</span> <span class="op">~</span> <span class="va">xs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_no_exclusion</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 14 iterations</span></span>
<span><span class="co">#&gt; Return code 8: successive function values within relative tolerance limit (reltol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -712.8298 </span></span>
<span><span class="co">#&gt; 500 observations (172 censored and 328 observed)</span></span>
<span><span class="co">#&gt; 6 free parameters (df = 494)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -0.1984     0.1114  -1.781   0.0756 .  </span></span>
<span><span class="co">#&gt; xs            1.2907     0.2085   6.191 1.25e-09 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   </span></span>
<span><span class="co">#&gt; (Intercept)  -0.5499     0.5644  -0.974  0.33038   </span></span>
<span><span class="co">#&gt; xs            1.3987     0.4482   3.120  0.00191 **</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma  0.85091    0.05352  15.899   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho   -0.13226    0.72684  -0.182    0.856    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p><strong>What changes?</strong></p>
<ul>
<li><p>The estimates remain approximately unbiased because the model is still identified by functional form (non-linear selection mechanism).</p></li>
<li><p>However, the standard errors are substantially larger, leading to less precise inference.</p></li>
</ul>
<p><strong>Why Exclusion Restrictions Matter</strong></p>
<p>The exclusion restriction improves identification by introducing additional variation that affects selection but not the outcome. This is a common recommendation in empirical work:</p>
<ul>
<li><p>In the first case, <code>xs</code> helps explain who is selected, while <code>xo</code> helps explain the outcome, enabling more precise estimates.</p></li>
<li><p>In the second case, all variation comes from <code>xs</code>, meaning the model relies solely on the distributional assumptions (e.g., joint normality of errors) for identification.</p></li>
</ul>
<p><strong>Best Practice (for applied researchers):</strong></p>
<blockquote>
<p>Always try to include at least one variable in the selection equation that does <strong>not appear in the outcome equation</strong>. This helps identify the model and improves estimation precision.</p>
</blockquote>
</div>
</div>
<div id="tobit-5-switching-regression-model" class="section level3" number="36.2.5">
<h3>
<span class="header-section-number">36.2.5</span> Tobit-5: Switching Regression Model<a class="anchor" aria-label="anchor" href="#tobit-5-switching-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Tobit-5</strong> model, also known as the <strong>Switching Regression Model</strong>, generalizes Heckman’s sample selection model to allow for two separate outcome equations:</p>
<ul>
<li>One model for participants</li>
<li>A different model for non-participants</li>
</ul>
<p>Assumptions:</p>
<ul>
<li>The selection process determines which outcome equation is observed.</li>
<li>There is at least one variable in the selection equation that does not appear in the outcome equations (i.e., an exclusion restriction), which improves identification.</li>
<li>The errors from all three equations (selection, outcome1, outcome2) are assumed to be jointly normally distributed.</li>
</ul>
<p>This model is especially relevant when selection is endogenous and both groups (selected and unselected) have distinct data-generating processes.</p>
<hr>
<div id="simulated-example-with-exclusion-restriction" class="section level4" number="36.2.5.1">
<h4>
<span class="header-section-number">36.2.5.1</span> Simulated Example: With Exclusion Restriction<a class="anchor" aria-label="anchor" href="#simulated-example-with-exclusion-restriction"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb971"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define a 3x3 covariance matrix with positive correlations</span></span>
<span><span class="va">vc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html">diag</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">vc</span><span class="op">[</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/lower.tri.html">lower.tri</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="va">vc</span><span class="op">[</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/lower.tri.html">upper.tri</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">[</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/lower.tri.html">upper.tri</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Generate multivariate normal error terms</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span><span class="fl">500</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, sigma <span class="op">=</span> <span class="va">vc</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Selection equation regressor (xs), uniformly distributed</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Binary selection indicator: ys = 1 if selected</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span></span>
<span></span>
<span><span class="co"># Separate regressors for two regimes (fulfilling exclusion restriction)</span></span>
<span><span class="va">xo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span><span class="va">yo1</span> <span class="op">&lt;-</span> <span class="va">xo1</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>  <span class="co"># Outcome for selected group</span></span>
<span></span>
<span><span class="va">xo2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">xo2</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span>  <span class="co"># Outcome for non-selected group</span></span>
<span></span>
<span><span class="co"># Fit switching regression model</span></span>
<span><span class="va">model_switch</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">yo1</span> <span class="op">~</span> <span class="va">xo1</span>, <span class="va">yo2</span> <span class="op">~</span> <span class="va">xo2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_switch</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 5 model (switching regression model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 11 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -895.8201 </span></span>
<span><span class="co">#&gt; 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)</span></span>
<span><span class="co">#&gt; 10 free parameters (df = 490)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -0.1550     0.1051  -1.474    0.141    </span></span>
<span><span class="co">#&gt; xs            1.1408     0.1785   6.390 3.86e-10 ***</span></span>
<span><span class="co">#&gt; Outcome equation 1:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02708    0.16395   0.165    0.869    </span></span>
<span><span class="co">#&gt; xo1          0.83959    0.14968   5.609  3.4e-08 ***</span></span>
<span><span class="co">#&gt; Outcome equation 2:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   0.1583     0.1885   0.840    0.401    </span></span>
<span><span class="co">#&gt; xo2           0.8375     0.1707   4.908 1.26e-06 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;        Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma1  0.93191    0.09211  10.118   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; sigma2  0.90697    0.04434  20.455   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho1    0.88988    0.05353  16.623   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho2    0.17695    0.33139   0.534    0.594    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>The estimated coefficients are close to the true values (intercept = 0, slope = 1), and the model converges well due to correct specification and valid exclusion restriction.</p>
</div>
<div id="example-functional-form-misspecification" class="section level4" number="36.2.5.2">
<h4>
<span class="header-section-number">36.2.5.2</span> Example: Functional Form Misspecification<a class="anchor" aria-label="anchor" href="#example-functional-form-misspecification"><i class="fas fa-link"></i></a>
</h4>
<p>To demonstrate how <strong>non-normal errors</strong> or skewed distributions affect the model, consider the following:</p>
<div class="sourceCode" id="cb972"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span>, sigma <span class="op">=</span> <span class="va">vc</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Induce skewness: squared errors minus 1 to ensure mean zero</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="va">eps</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="fl">1</span></span>
<span></span>
<span><span class="co"># Generate xs on a skewed interval [-1, 0]</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span></span>
<span></span>
<span><span class="va">xo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">yo1</span> <span class="op">&lt;-</span> <span class="va">xo1</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="va">xo2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">xo2</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Attempt model estimation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">yo1</span> <span class="op">~</span> <span class="va">xo1</span>, <span class="va">yo2</span> <span class="op">~</span> <span class="va">xo2</span><span class="op">)</span>, iterlim <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 5 model (switching regression model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 12 iterations</span></span>
<span><span class="co">#&gt; Return code 3: Last step could not find a value above the current.</span></span>
<span><span class="co">#&gt; Boundary of parameter space?  </span></span>
<span><span class="co">#&gt; Consider switching to a more robust optimisation method temporarily.</span></span>
<span><span class="co">#&gt; Log-Likelihood: -1695.102 </span></span>
<span><span class="co">#&gt; 1000 observations: 782 selection 1 (FALSE) and 218 selection 2 (TRUE)</span></span>
<span><span class="co">#&gt; 10 free parameters (df = 990)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.660315   0.082477  -8.006  3.3e-15 ***</span></span>
<span><span class="co">#&gt; xs           0.007167   0.088630   0.081    0.936    </span></span>
<span><span class="co">#&gt; Outcome equation 1:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.31351    0.04868   -6.44 1.86e-10 ***</span></span>
<span><span class="co">#&gt; xo1          1.03862    0.08049   12.90  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation 2:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -2.6835     0.2043 -13.132  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; xo2           1.0230     0.1309   7.814 1.41e-14 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;        Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma1  0.70172    0.02000   35.09   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; sigma2  2.49651        NaN     NaN      NaN    </span></span>
<span><span class="co">#&gt; rho1    0.51564    0.04216   12.23   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho2    1.00000        NaN     NaN      NaN    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>Even though the exclusion restriction is preserved, non-normal errors introduce bias in the intercept estimates, and convergence is less reliable. This illustrates how functional form misspecification (i.e., deviations from assumed distributional forms) impacts performance.</p>
</div>
<div id="example-no-exclusion-restriction" class="section level4" number="36.2.5.3">
<h4>
<span class="header-section-number">36.2.5.3</span> Example: No Exclusion Restriction<a class="anchor" aria-label="anchor" href="#example-no-exclusion-restriction"><i class="fas fa-link"></i></a>
</h4>
<p>Here we remove the exclusion restriction by using the same regressor (<code>xs</code>) in both outcome equations.</p>
<div class="sourceCode" id="cb973"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span></span>
<span></span>
<span><span class="va">yo1</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Fit switching regression without exclusion restriction</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">tmp</span> <span class="op">&lt;-</span></span>
<span>            <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">yo1</span> <span class="op">~</span> <span class="va">xs</span>, <span class="va">yo2</span> <span class="op">~</span> <span class="va">xs</span><span class="op">)</span>, iterlim <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 5 model (switching regression model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 16 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -1879.552 </span></span>
<span><span class="co">#&gt; 1000 observations: 615 selection 1 (FALSE) and 385 selection 2 (TRUE)</span></span>
<span><span class="co">#&gt; 10 free parameters (df = 990)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.33425    0.04280   -7.81 1.46e-14 ***</span></span>
<span><span class="co">#&gt; xs           0.94762    0.07763   12.21  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation 1:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.49592    0.06800  -7.293 6.19e-13 ***</span></span>
<span><span class="co">#&gt; xs           0.84530    0.06789  12.450  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation 2:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  </span></span>
<span><span class="co">#&gt; (Intercept)   0.3861     0.4967   0.777   0.4371  </span></span>
<span><span class="co">#&gt; xs            0.6254     0.3322   1.882   0.0601 .</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;        Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma1  0.61693    0.02054  30.029   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; sigma2  1.59059    0.05745  27.687   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho1    0.19981    0.15863   1.260    0.208    </span></span>
<span><span class="co">#&gt; rho2   -0.01259    0.29339  -0.043    0.966    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>This model may fail to converge or produce biased estimates, especially for the intercepts. Even if it does converge, the reliability of the inference is questionable due to weak identification from using the same regressor across all equations.</p>
<p><strong>Notes on Estimation and Convergence</strong></p>
<p>The log-likelihood function of switching regression models is not globally concave, so the estimation process may:</p>
<ul>
<li><p>Fail to converge</p></li>
<li><p>Converge to a local maximum</p></li>
</ul>
<p>Practical Tips:</p>
<ul>
<li><p>Try different starting values or random seeds</p></li>
<li><p>Use alternative maximization algorithms (e.g., <code>optim</code> control)</p></li>
<li><p>Consider rescaling variables or centering predictors</p></li>
<li><p>Refer to <a href="non-linear-regression.html#non-linear-regression">Non-Linear Regression</a> for advanced diagnostics</p></li>
</ul>
<p>Model Comparison Summary</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="25%">
<col width="18%">
<col width="18%">
<col width="18%">
<col width="18%">
</colgroup>
<thead><tr class="header">
<th>Scenario</th>
<th>Exclusion Restriction</th>
<th>Convergence</th>
<th>Bias</th>
<th>Variance</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Well-specified with exclusion</td>
<td>Yes</td>
<td>Likely</td>
<td>No</td>
<td>Low</td>
</tr>
<tr class="even">
<td>Misspecified distribution</td>
<td>Yes</td>
<td>Risky</td>
<td>Yes (intercepts)</td>
<td>Moderate</td>
</tr>
<tr class="odd">
<td>No exclusion restriction</td>
<td>No</td>
<td>Often fails</td>
<td>Yes</td>
<td>High</td>
</tr>
</tbody>
</table></div>
</div>
</div>
<div id="pattern-mixture-models" class="section level3" number="36.2.6">
<h3>
<span class="header-section-number">36.2.6</span> Pattern-Mixture Models<a class="anchor" aria-label="anchor" href="#pattern-mixture-models"><i class="fas fa-link"></i></a>
</h3>
<p>In the context of endogenous sample selection, one of the central challenges is modeling the joint distribution of the outcome and the selection mechanism when data are <a href="imputation-missing-data.html#missing-not-at-random-mnar">Missing Not At Random (MNAR)</a>. In this framework, the probability that an outcome is observed may depend on unobserved values of that outcome, making the missingness mechanism <strong>nonignorable</strong>.</p>
<p>Previously, we discussed the <strong>selection model</strong> approach (e.g., Heckman’s Tobit-2 model), which factorizes the joint distribution as:</p>
<p><span class="math display">\[
\mathbb{P}(Y, R) = \mathbb{P}(Y) \cdot \mathbb{P}(R \mid Y),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> is the outcome of interest,</p></li>
<li><p><span class="math inline">\(R\)</span> is the response indicator (with <span class="math inline">\(R = 1\)</span> if <span class="math inline">\(Y\)</span> is observed, <span class="math inline">\(R = 0\)</span> otherwise).</p></li>
</ul>
<p>This approach models the selection process explicitly via <span class="math inline">\(\mathbb{P}(R \mid Y)\)</span>, often using a parametric model such as the probit.</p>
<p>However, the <strong>pattern-mixture model (PMM)</strong> offers an alternative and equally valid factorization:</p>
<p><span class="math display">\[
\mathbb{P}(Y, R) = \mathbb{P}(Y \mid R) \cdot \mathbb{P}(R),
\]</span></p>
<p>which decomposes the joint distribution by conditioning on the response pattern. This approach is particularly advantageous when the selection mechanism is complex, or when interest lies in modeling how the outcome distribution varies across response strata.</p>
<hr>
<div id="definition-of-the-pattern-mixture-model" class="section level4" number="36.2.6.1">
<h4>
<span class="header-section-number">36.2.6.1</span> Definition of the Pattern-Mixture Model<a class="anchor" aria-label="anchor" href="#definition-of-the-pattern-mixture-model"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\((Y_i, R_i)\)</span> for <span class="math inline">\(i = 1, \dots, n\)</span> denote the observed data, where:</p>
<ul>
<li>
<span class="math inline">\(Y_i \in \mathbb{R}^p\)</span> is the multivariate outcome of interest,</li>
<li>
<span class="math inline">\(R_i \in \{0,1\}^p\)</span> is a missingness pattern indicator vector, where <span class="math inline">\(R_{ij} = 1\)</span> indicates that <span class="math inline">\(Y_{ij}\)</span> is observed.</li>
</ul>
<p>Define <span class="math inline">\(\mathcal{R}\)</span> as the finite set of all possible response patterns. For each pattern <span class="math inline">\(r \in \mathcal{R}\)</span>, partition the outcome vector <span class="math inline">\(Y\)</span> into:</p>
<ul>
<li>
<span class="math inline">\(Y_{(r)}\)</span>: observed components of <span class="math inline">\(Y\)</span> under pattern <span class="math inline">\(r\)</span>,</li>
<li>
<span class="math inline">\(Y_{(\bar{r})}\)</span>: missing components of <span class="math inline">\(Y\)</span> under pattern <span class="math inline">\(r\)</span>.</li>
</ul>
<p>The joint distribution can then be factorized according to the pattern-mixture model as:</p>
<p><span class="math display">\[
f(Y, R) = f(Y \mid R = r) \cdot \mathbb{P}(R = r), \quad r \in \mathcal{R}.
\]</span></p>
<p>The marginal distribution of <span class="math inline">\(Y\)</span> is obtained by summing over all response patterns:</p>
<p><span class="math display">\[
f(Y) = \sum_{r \in \mathcal{R}} f(Y \mid R = r) \cdot \mathbb{P}(R = r).
\]</span></p>
<p>In the simplest case of binary response patterns (complete responders <span class="math inline">\(R=1\)</span> and complete nonresponders <span class="math inline">\(R=0\)</span>), this reduces to:</p>
<p><span class="math display">\[
\mathbb{P}(Y) = \mathbb{P}(Y \mid R = 1) \cdot \mathbb{P}(R = 1) + \mathbb{P}(Y \mid R = 0) \cdot \mathbb{P}(R = 0).
\]</span></p>
<p>Thus, the overall distribution of <span class="math inline">\(Y\)</span> is explicitly treated as a <strong>mixture</strong> of distributions for responders and nonresponders. While <span class="math inline">\(\mathbb{P}(Y \mid R = 1)\)</span> can be directly estimated from observed data, <span class="math inline">\(\mathbb{P}(Y \mid R = 0)\)</span> cannot be identified from data alone, necessitating additional assumptions or sensitivity parameters for its estimation.</p>
<p>Common approaches for addressing non-identifiability involve defining plausible deviations from <span class="math inline">\(\mathbb{P}(Y \mid R = 1)\)</span> using specific sensitivity parameters such as:</p>
<ul>
<li>
<strong>Shift Bias</strong>: Adjusting imputed values by a constant <span class="math inline">\(\delta\)</span> to reflect systematic differences in nonresponders.</li>
<li>
<strong>Scale Bias</strong>: Scaling imputed values to account for differing variability.</li>
<li>
<strong>Shape Bias</strong>: Reweighting imputations, for example, using methods like Selection-Indicator Reweighting (SIR), to capture distributional shape differences.</li>
</ul>
<p>In this discussion, primary focus is placed on the shift parameter <span class="math inline">\(\delta\)</span>, hypothesizing that nonresponse shifts the mean of <span class="math inline">\(Y\)</span> by <span class="math inline">\(\delta\)</span> units.</p>
<p>In practice, since only observed components <span class="math inline">\(Y_{(r)}\)</span> are available for individuals with missing data, modeling the full conditional distribution <span class="math inline">\(f(Y \mid R = r)\)</span> requires extrapolating from observed to unobserved components.</p>
<hr>
</div>
<div id="modeling-strategy-and-identifiability" class="section level4" number="36.2.6.2">
<h4>
<span class="header-section-number">36.2.6.2</span> Modeling Strategy and Identifiability<a class="anchor" aria-label="anchor" href="#modeling-strategy-and-identifiability"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose the full-data conditional distribution is parameterized by pattern-specific parameters <span class="math inline">\(\theta_r\)</span> for each response pattern <span class="math inline">\(r \in \mathcal{R}\)</span>:</p>
<p><span class="math display">\[
f(Y \mid R = r; \theta_r), \quad \text{with } \theta = \{\theta_r : r \in \mathcal{R}\}.
\]</span></p>
<p>Then, the marginal distribution of <span class="math inline">\(Y\)</span> can be expressed as:</p>
<p><span class="math display">\[
f(Y; \theta, \psi) = \sum_{r \in \mathcal{R}} f(Y \mid R = r; \theta_r) \cdot \mathbb{P}(R = r; \psi),
\]</span></p>
<p>where <span class="math inline">\(\psi\)</span> parameterizes the distribution of response patterns.</p>
<p>Important points about identifiability include:</p>
<ul>
<li>The model is inherently non-identifiable without explicit assumptions regarding the distribution of missing components <span class="math inline">\(Y_{(\bar{r})}\)</span>.</li>
<li>Estimating <span class="math inline">\(f(Y_{(\bar{r})} \mid Y_{(r)}, R = r)\)</span> requires external information, expert knowledge, or assumptions encapsulated in sensitivity parameters.</li>
</ul>
<p>The conditional density for each response pattern can be further decomposed as:</p>
<p><span class="math display">\[
f(Y \mid R = r) = f(Y_{(r)} \mid R = r) \cdot f(Y_{(\bar{r})} \mid Y_{(r)}, R = r).
\]</span></p>
<ul>
<li>The first term <span class="math inline">\(f(Y_{(r)} \mid R = r)\)</span> is estimable directly from observed data.</li>
<li>The second term <span class="math inline">\(f(Y_{(\bar{r})} \mid Y_{(r)}, R = r)\)</span> cannot be identified from observed data alone and must rely on assumptions or additional sensitivity analysis.</li>
</ul>
<hr>
</div>
<div id="location-shift-models" class="section level4" number="36.2.6.3">
<h4>
<span class="header-section-number">36.2.6.3</span> Location Shift Models<a class="anchor" aria-label="anchor" href="#location-shift-models"><i class="fas fa-link"></i></a>
</h4>
<p>A widely used strategy in PMMs is to introduce a <strong>location shift</strong> for the unobserved outcomes. Specifically, assume that:</p>
<p><span class="math display">\[
Y_{(\bar{r})} \mid Y_{(r)}, R = r \sim \mathcal{L}(Y_{(\bar{r})} \mid Y_{(r)}, R = r = r^*) + \delta_r,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(r^*\)</span> denotes a fully observed (reference) pattern (e.g., <span class="math inline">\(R = 1^p\)</span>),</p></li>
<li><p><span class="math inline">\(\delta_r\)</span> is a vector of <strong>sensitivity parameters</strong> that quantify the mean shift for missing outcomes under pattern <span class="math inline">\(r\)</span>.</p></li>
</ul>
<p>More formally, for each <span class="math inline">\(r \in \mathcal{R}\)</span> and unobserved component <span class="math inline">\(j \in \bar{r}\)</span>, we specify:</p>
<p><span class="math display">\[
\mathbb{E}[Y_j \mid Y_{(r)}, R = r] = \mathbb{E}[Y_j \mid Y_{(r)}, R = r^*] + \delta_{rj}.
\]</span></p>
<p>Under this framework:</p>
<ul>
<li>Setting <span class="math inline">\(\delta_r = 0\)</span> corresponds to the MAR assumption (i.e., ignorability).</li>
<li>Nonzero <span class="math inline">\(\delta_r\)</span> introduces controlled deviations from MAR and enables sensitivity analysis.</li>
</ul>
<hr>
</div>
<div id="sensitivity-analysis-in-pattern-mixture-models" class="section level4" number="36.2.6.4">
<h4>
<span class="header-section-number">36.2.6.4</span> Sensitivity Analysis in Pattern-Mixture Models<a class="anchor" aria-label="anchor" href="#sensitivity-analysis-in-pattern-mixture-models"><i class="fas fa-link"></i></a>
</h4>
<p>To evaluate the robustness of inferences to the missing data mechanism, we perform sensitivity analysis by varying <span class="math inline">\(\delta_r\)</span> over plausible ranges.</p>
<p><strong>Bias in Estimation</strong></p>
<p>Let <span class="math inline">\(\mu = \mathbb{E}[Y]\)</span> be the target estimand. Under the pattern-mixture decomposition:</p>
<p><span class="math display">\[
\mu = \sum_{r \in \mathcal{R}} \mathbb{E}[Y \mid R = r] \cdot \mathbb{P}(R = r),
\]</span></p>
<p>and under the shift model:</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid R = r] = \tilde{\mu}_r + \delta_r^{*},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\tilde{\mu}_r\)</span> is the mean computed using observed data (e.g., via imputation under MAR),</p></li>
<li><p><span class="math inline">\(\delta_r^{*}\)</span> is a vector with entries equal to the mean shifts <span class="math inline">\(\delta_{rj}\)</span> for missing components and 0 for observed components.</p></li>
</ul>
<p>Therefore, the overall bias induced by nonzero shifts is:</p>
<p><span class="math display">\[
\Delta = \sum_{r \in \mathcal{R}} \delta_r^{*} \cdot \mathbb{P}(R = r).
\]</span></p>
<p>This highlights how the overall expectation depends linearly on the sensitivity parameters and their associated pattern probabilities.</p>
<p>Practical Recommendations</p>
<ul>
<li>Use subject-matter knowledge to specify plausible ranges for <span class="math inline">\(\delta_r\)</span>.</li>
<li>Consider standard increments (e.g., <span class="math inline">\(\pm 0.2\)</span> SD units) or domain-specific metrics (e.g., <span class="math inline">\(\pm 5\)</span> P/E for valuation).</li>
<li>If the dataset is large and the number of missingness patterns is small (e.g., monotone dropout), more detailed pattern-specific modeling is feasible.</li>
</ul>
<hr>
</div>
<div id="generalization-to-multivariate-and-longitudinal-data" class="section level4" number="36.2.6.5">
<h4>
<span class="header-section-number">36.2.6.5</span> Generalization to Multivariate and Longitudinal Data<a class="anchor" aria-label="anchor" href="#generalization-to-multivariate-and-longitudinal-data"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose <span class="math inline">\(Y = (Y_1, \dots, Y_T)\)</span> is a longitudinal outcome. Let <span class="math inline">\(D \in \{1, \dots, T+1\}\)</span> denote the dropout time, where <span class="math inline">\(D = t\)</span> implies observation up to time <span class="math inline">\(t-1\)</span>.</p>
<p>Then the pattern-mixture factorization becomes:</p>
<p><span class="math display">\[
f(Y, D) = f(Y \mid D = t) \cdot \mathbb{P}(D = t), \quad t = 2, \dots, T+1.
\]</span></p>
<p>Assume a parametric model:</p>
<p><span class="math display">\[
Y_j \mid D = t \sim \mathcal{N}(\beta_{0t} + \beta_{1t} t_j, \sigma_t^2), \quad \text{for } j = 1, \dots, T.
\]</span></p>
<p>Then the overall mean trajectory becomes:</p>
<p><span class="math display">\[
\mathbb{E}[Y_j] = \sum_{t=2}^{T+1} (\beta_{0t} + \beta_{1t} t_j) \cdot \mathbb{P}(D = t).
\]</span></p>
<p>This model allows for dropout-pattern-specific trajectories and can flexibly account for deviations in the distributional shape due to dropout.</p>
<hr>
</div>
<div id="comparison-with-selection-models" class="section level4" number="36.2.6.6">
<h4>
<span class="header-section-number">36.2.6.6</span> Comparison with Selection Models<a class="anchor" aria-label="anchor" href="#comparison-with-selection-models"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="27%">
<col width="36%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Selection Model</th>
<th>Pattern-Mixture Model</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Factorization</td>
<td><span class="math inline">\(\mathbb{P}(Y) \cdot \mathbb{P}(R \mid Y)\)</span></td>
<td><span class="math inline">\(\mathbb{P}(Y \mid R) \cdot \mathbb{P}(R)\)</span></td>
</tr>
<tr class="even">
<td>Target of modeling</td>
<td>Missingness process <span class="math inline">\(\mathbb{P}(R \mid Y)\)</span>
</td>
<td>Distribution of <span class="math inline">\(Y\)</span> under <span class="math inline">\(R\)</span>
</td>
</tr>
<tr class="odd">
<td>Assumption for identifiability</td>
<td>MAR (often Probit)</td>
<td>Extrapolation (e.g., shift <span class="math inline">\(\delta\)</span>)</td>
</tr>
<tr class="even">
<td>Modeling burden</td>
<td>On <span class="math inline">\(\mathbb{P}(R \mid Y)\)</span>
</td>
<td>On <span class="math inline">\(f(Y \mid R)\)</span>
</td>
</tr>
<tr class="odd">
<td>Sensitivity analysis</td>
<td>Less interpretable</td>
<td>Easily parameterized (via <span class="math inline">\(\delta\)</span>)</td>
</tr>
</tbody>
</table></div>
<div class="sourceCode" id="cb974"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/amices/mice">mice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li>Simulate Longitudinal Dropout Data</li>
</ol>
<p>We simulate a 3-time-point longitudinal outcome, where dropout depends on unobserved future outcomes, i.e., MNAR.</p>
<div class="sourceCode" id="cb975"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Parameters</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">time</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">5</span><span class="op">)</span>  <span class="co"># intercept and slope</span></span>
<span><span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span><span class="va">dropout_bias</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">10</span>  <span class="co"># MNAR: lower future Y -&gt; more dropout</span></span>
<span></span>
<span><span class="co"># Simulate full data (Y1, Y2, Y3)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="va">n</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">Y</span><span class="op">[</span>, <span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">beta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">time</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="va">sigma</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Dropout mechanism: higher chance to drop if Y3 is low</span></span>
<span><span class="va">prob_dropout</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="op">(</span><span class="va">dropout_bias</span> <span class="op">-</span> <span class="va">Y</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">drop</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, <span class="va">prob_dropout</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define dropout time: if drop == 1, then Y3 is missing</span></span>
<span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">1</span>, nrow <span class="op">=</span> <span class="va">n</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">R</span><span class="op">[</span><span class="va">drop</span> <span class="op">==</span> <span class="fl">1</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span></span>
<span><span class="co"># Observed data</span></span>
<span><span class="va">Y_obs</span> <span class="op">&lt;-</span> <span class="va">Y</span></span>
<span><span class="va">Y_obs</span><span class="op">[</span><span class="va">R</span> <span class="op">==</span> <span class="fl">0</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span></span>
<span><span class="co"># Combine into a data frame</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>        id <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span>,</span>
<span>        Y1 <span class="op">=</span> <span class="va">Y_obs</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>,</span>
<span>        Y2 <span class="op">=</span> <span class="va">Y_obs</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>,</span>
<span>        Y3 <span class="op">=</span> <span class="va">Y_obs</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span>,</span>
<span>        R3 <span class="op">=</span> <span class="va">R</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span></span>
<span>    <span class="op">)</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Fit Pattern-Mixture Model with Delta Adjustment</li>
</ol>
<p>We model <span class="math inline">\(Y_3\)</span> as a function of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>, using the observed cases (complete cases), and create multiple imputed datasets under various shift parameters <span class="math inline">\(\delta\)</span> to represent different MNAR scenarios.</p>
<div class="sourceCode" id="cb976"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Base linear model for Y3 ~ Y1 + Y2</span></span>
<span><span class="va">model_cc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y3</span> <span class="op">~</span> <span class="va">Y1</span> <span class="op">+</span> <span class="va">Y2</span>, data <span class="op">=</span> <span class="va">df</span>, subset <span class="op">=</span> <span class="va">R3</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predict for missing Y3s</span></span>
<span><span class="va">preds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model_cc</span>, newdata <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sensitivity values for delta</span></span>
<span><span class="va">delta_seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="op">-</span><span class="fl">2.5</span>, <span class="op">-</span><span class="fl">5</span>, <span class="op">-</span><span class="fl">7.5</span>, <span class="op">-</span><span class="fl">10</span><span class="op">)</span>  <span class="co"># in units of Y3</span></span>
<span></span>
<span><span class="co"># Perform delta adjustment</span></span>
<span><span class="va">imputed_dfs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/lapply.html">lapply</a></span><span class="op">(</span><span class="va">delta_seq</span>, <span class="kw">function</span><span class="op">(</span><span class="va">delta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">df_new</span> <span class="op">&lt;-</span> <span class="va">df</span></span>
<span>  <span class="va">df_new</span><span class="op">$</span><span class="va">Y3_imp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">Y3</span><span class="op">)</span>,</span>
<span>                          <span class="va">preds</span> <span class="op">+</span> <span class="va">delta</span>,</span>
<span>                          <span class="va">df</span><span class="op">$</span><span class="va">Y3</span><span class="op">)</span></span>
<span>  <span class="va">df_new</span><span class="op">$</span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="va">delta</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df_new</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stack all results</span></span>
<span><span class="va">df_imp_all</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span><span class="va">imputed_dfs</span><span class="op">)</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Estimate Full-Data Mean and Trajectory under Each Delta</li>
</ol>
<p>We now estimate the mean of <span class="math inline">\(Y_3\)</span> and the full outcome trajectory across delta values.</p>
<div class="sourceCode" id="cb977"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">estimates</span> <span class="op">&lt;-</span> <span class="va">df_imp_all</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span></span>
<span>        mu_Y1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y1</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>        mu_Y2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y2</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>        mu_Y3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y3_imp</span><span class="op">)</span>,</span>
<span>        .groups <span class="op">=</span> <span class="st">"drop"</span></span>
<span>    <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span>cols <span class="op">=</span> <span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"mu"</span><span class="op">)</span>,</span>
<span>                 names_to <span class="op">=</span> <span class="st">"Time"</span>,</span>
<span>                 values_to <span class="op">=</span> <span class="st">"Mean"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Time <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span></span>
<span>        <span class="va">Time</span>,</span>
<span>        levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"mu_Y1"</span>, <span class="st">"mu_Y2"</span>, <span class="st">"mu_Y3"</span><span class="op">)</span>,</span>
<span>        labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Time 1"</span>, <span class="st">"Time 2"</span>, <span class="st">"Time 3"</span><span class="op">)</span></span>
<span>    <span class="op">)</span><span class="op">)</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Visualization: Sensitivity Analysis</li>
</ol>
<p>We visualize how the mean trajectory changes across different sensitivity parameters <span class="math inline">\(\delta\)</span>.</p>
<div class="sourceCode" id="cb978"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">estimates</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">Time</span>,</span>
<span>    y <span class="op">=</span> <span class="va">Mean</span>,</span>
<span>    color <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span>,</span>
<span>    group <span class="op">=</span> <span class="va">delta</span></span>
<span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_color_viridis_d</a></span><span class="op">(</span>name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Pattern-Mixture Model Sensitivity Analysis"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Mean of Y_t"</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"Time"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">causalverse</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/causalverse/man/ama_theme.html">ama_theme</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="36-endogeneity_files/figure-html/unnamed-chunk-31-1.png" width="90%" style="display: block; margin: auto;"></div>
<ol start="5" style="list-style-type: decimal">
<li>Sensitivity Table for Reporting</li>
</ol>
<p>This table quantifies the change in <span class="math inline">\(\mu_{Y3}\)</span> across sensitivity scenarios.</p>
<div class="sourceCode" id="cb979"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_summary</span> <span class="op">&lt;-</span> <span class="va">df_imp_all</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span></span>
<span>        Mean_Y3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y3_imp</span><span class="op">)</span>,</span>
<span>        SD_Y3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">Y3_imp</span><span class="op">)</span>,</span>
<span>        N_Missing <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">Y3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        N_Observed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">Y3</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/kable.html">kable</a></span><span class="op">(</span><span class="va">df_summary</span>, digits <span class="op">=</span> <span class="fl">2</span>,</span>
<span>             caption <span class="op">=</span> <span class="st">"Sensitivity of Estimated Mean of Y3 to Delta Adjustments"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:unnamed-chunk-32">Table 36.1: </span>Sensitivity of Estimated Mean of Y3 to Delta Adjustments</caption>
<thead><tr class="header">
<th align="right">delta</th>
<th align="right">Mean_Y3</th>
<th align="right">SD_Y3</th>
<th align="right">N_Missing</th>
<th align="right">N_Observed</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">-10.0</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="even">
<td align="right">-7.5</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="odd">
<td align="right">-5.0</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="even">
<td align="right">-2.5</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
</tbody>
</table></div>
<ol start="6" style="list-style-type: decimal">
<li>Interpretation and Discussion</li>
</ol>
<ul>
<li><p>The estimated mean of <span class="math inline">\(Y_3\)</span> decreases linearly with increasingly negative <span class="math inline">\(\delta\)</span>, as expected.</p></li>
<li><p>This illustrates the <strong>non-identifiability</strong> of the distribution of missing data without unverifiable assumptions.</p></li>
<li><p>The results provide a <strong>tipping point analysis</strong>: at what <span class="math inline">\(\delta\)</span> does inference about the mean (or treatment effect, in a causal study) substantially change?</p></li>
</ul>
</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></div>
<div class="next"><a href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#endogeneity"><span class="header-section-number">36</span> Endogeneity</a></li>
<li>
<a class="nav-link" href="#endogenous-treatment"><span class="header-section-number">36.1</span> Endogenous Treatment</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#measurement-error"><span class="header-section-number">36.1.1</span> Measurement Error</a></li>
<li><a class="nav-link" href="#simultaneity"><span class="header-section-number">36.1.2</span> Simultaneity</a></li>
<li><a class="nav-link" href="#endogenous-treatment-solutions"><span class="header-section-number">36.1.3</span> Endogenous Treatment Solutions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#endogenous-sample-selection"><span class="header-section-number">36.2</span> Endogenous Sample Selection</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#unifying-model-frameworks"><span class="header-section-number">36.2.1</span> Unifying Model Frameworks</a></li>
<li><a class="nav-link" href="#estimation-methods-2"><span class="header-section-number">36.2.2</span> Estimation Methods</a></li>
<li><a class="nav-link" href="#theoretical-connections"><span class="header-section-number">36.2.3</span> Theoretical Connections</a></li>
<li><a class="nav-link" href="#tobit-2-heckmans-sample-selection-model"><span class="header-section-number">36.2.4</span> Tobit-2: Heckman’s Sample Selection Model</a></li>
<li><a class="nav-link" href="#tobit-5-switching-regression-model"><span class="header-section-number">36.2.5</span> Tobit-5: Switching Regression Model</a></li>
<li><a class="nav-link" href="#pattern-mixture-models"><span class="header-section-number">36.2.6</span> Pattern-Mixture Models</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/36-endogeneity.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/36-endogeneity.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-04-27.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
