<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 11 Imputation (Missing Data) | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Imputation is a statistical procedure where you replace missing data with some reasonable values Unit imputation = single data point Item imputation = single feature value Imputation is usually...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 11 Imputation (Missing Data) | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/imputation-missing-data.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Imputation is a statistical procedure where you replace missing data with some reasonable values Unit imputation = single data point Item imputation = single feature value Imputation is usually...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 11 Imputation (Missing Data) | A Guide on Data Analysis">
<meta name="twitter:description" content="Imputation is a statistical procedure where you replace missing data with some reasonable values Unit imputation = single data point Item imputation = single feature value Imputation is usually...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-stat.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="active" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="" href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="synthetic-difference-in-differences.html"><span class="header-section-number">25</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">26</span> Difference-in-differences</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">27</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">28</span> Event Studies</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">29</span> Instrumental Variables</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">30</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">31</span> Interrupted Time Series</a></li>
<li><a class="" href="matching-methods-1.html"><span class="header-section-number">32</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">33</span> Endogeneity</a></li>
<li><a class="" href="interrupted-time-series-1.html"><span class="header-section-number">34</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="other-biases.html"><span class="header-section-number">35</span> Other Biases</a></li>
<li><a class="" href="endogeneity-1.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases-1.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="controls-1.html"><span class="header-section-number">39</span> Controls</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">40</span> Mediation</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">41</span> Directed Acyclic Graph</a></li>
<li><a class="" href="mediation-1.html"><span class="header-section-number">42</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph-1.html"><span class="header-section-number">43</span> Directed Acyclic Graph</a></li>
<li><a class="" href="report.html"><span class="header-section-number">44</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">45</span> Exploratory Data Analysis</a></li>
<li><a class="" href="report-1.html"><span class="header-section-number">46</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis-1.html"><span class="header-section-number">47</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">48</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">49</span> Replication and Synthetic Data</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check-1.html"><span class="header-section-number">50</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data-1.html"><span class="header-section-number">51</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="imputation-missing-data" class="section level1" number="11">
<h1>
<span class="header-section-number">11</span> Imputation (Missing Data)<a class="anchor" aria-label="anchor" href="#imputation-missing-data"><i class="fas fa-link"></i></a>
</h1>
<p>Imputation is a statistical procedure where you replace missing data with <em>some reasonable values</em></p>
<ul>
<li><p>Unit imputation = single data point</p></li>
<li><p>Item imputation = single feature value</p></li>
</ul>
<p>Imputation is usually seen as the illegitimate child of statistical analysis. Several reasons that contribute to these negative views could be:</p>
<ol style="list-style-type: decimal">
<li>Peopled hardly do imputation correctly (which will introduce bias to your estimates)</li>
<li>Imputation can only be applied to a small range of problems correctly</li>
</ol>
<p>If you have missing data on <span class="math inline">\(y\)</span> (dependent variable), you probably would not be able to do any imputation appropriately. However, if you have certain type of missing data (e.g., non-random missing data) in the <span class="math inline">\(x\)</span>’s variable (independent variables), then you can still salvage your collected data points with imputation.</p>
<p>We also need to talk why you would want to do imputation in the first place. If your purpose is inference/ explanation (valid statistical inference not optimal point prediction), then imputation would not offer much help <span class="citation">(<a href="references.html#ref-Rubin_1996">Rubin 1996</a>)</span>. However, if your purpose is prediction, you would want your standard error to be reduced by including information (non-missing data) on other variables of a data point. Then imputation could be the tool that you’re looking for.</p>
<p>For most software packages, it will use listwise deletion or casewise deletion to have complete case analysis (analysis with only observations with all information). Not until recently that statistician can propose some methods that are a bit better than listwise deletion which are maximum likelihood and multiple imputation.</p>
<p>“Judging the quality of missing data procedures by their ability to recreate the individual missing values (according to hit rate, mean square error, etc) does not lead to choosing procedures that result in valid inference”, <span class="citation">(<a href="references.html#ref-Rubin_1996">Rubin 1996</a>)</span></p>
<p>Missing data can make it more challenging to big datasets.</p>
<div id="assumptions-1" class="section level2" number="11.1">
<h2>
<span class="header-section-number">11.1</span> Assumptions<a class="anchor" aria-label="anchor" href="#assumptions-1"><i class="fas fa-link"></i></a>
</h2>
<div id="missing-completely-at-random-mcar" class="section level3" number="11.1.1">
<h3>
<span class="header-section-number">11.1.1</span> Missing Completely at Random (MCAR)<a class="anchor" aria-label="anchor" href="#missing-completely-at-random-mcar"><i class="fas fa-link"></i></a>
</h3>
<p>Missing Completely at Random, MCAR, means there is no relationship between the missingness of the data and any values, observed or missing. Those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than others.</p>
<p>The probability of missing data on a variable is unrelated to the value of it or to the values of any other variables in the data set.</p>
<p><strong>Note</strong>: the “missingness” on Y can be correlated with the “missingness” on X We can compare the value of other variables for the observations with missing data, and observations without missing data. If we reject the t-test for mean difference, we can say there is evidence that the data are not MCAR. But we cannot say that our data are MCAR if we fail to reject the t-test.</p>
<ul>
<li>the propensity for a data point to be missing is completely random.</li>
<li>There’s no relationship between whether a data point is missing and any values in the data set, missing or observed.</li>
<li>The missing data are just a random subset of the data.</li>
</ul>
<p>Methods include:</p>
<ul>
<li><p>Universal singular value thresholding <span class="citation">(<a href="references.html#ref-chatterjee2015matrix">Chatterjee 2015</a>)</span> (can only recover the mean, not the whole true distribution).</p></li>
<li><p>Softimputet: <span class="citation">(<a href="references.html#ref-hastie2015matrix">Hastie et al. 2015</a>)</span> (doesn’t work well under “Limited” -missing not at random).</p></li>
<li><p>Synthetic nearest neighbor <span class="citation">(<a href="references.html#ref-agarwal2023causal">Agarwal et al. 2023</a>)</span> (still work okay under missing not at random). Available on GitHub: <a href="https://github.com/deshen24/syntheticNN">syntheticNN</a></p></li>
</ul>
</div>
<div id="missing-at-random-mar" class="section level3" number="11.1.2">
<h3>
<span class="header-section-number">11.1.2</span> Missing at Random (MAR)<a class="anchor" aria-label="anchor" href="#missing-at-random-mar"><i class="fas fa-link"></i></a>
</h3>
<p>Missing at Random, MAR, means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individual’s observed variables. So, for example, if men are more likely to tell you their weight than women, weight is MAR.</p>
<p>MAR is weaker than MCAR</p>
<p><span class="math display">\[
P(Y_{missing}|Y,X)= P(Y_{missing}|X)
\]</span></p>
<p>The probability of Y missing given Y and X equal to the probability of of Y missing given X. However, it is impossible to provide evidence to the MAR condition.</p>
<ul>
<li>
<p>the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data. In another word, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.</p>
<ul>
<li>For example, if men are more likely to tell you their weight than women, weight is MAR</li>
</ul>
</li>
<li><p>MAR requires that the cause of the missing data is unrelated to the missing values but may be related to the observed values of other variables.</p></li>
<li><p>MAR means that the missing values are related to observed values on other variables. As an example of CD missing data, missing income data may be unrelated to the actual income values but are related to education. Perhaps people with more education are less likely to reveal their income than those with less education</p></li>
</ul>
</div>
<div id="ignorable" class="section level3" number="11.1.3">
<h3>
<span class="header-section-number">11.1.3</span> Ignorable<a class="anchor" aria-label="anchor" href="#ignorable"><i class="fas fa-link"></i></a>
</h3>
<p>The missing data mechanism is ignorable when</p>
<ol style="list-style-type: decimal">
<li>The data are <a href="imputation-missing-data.html#missing-at-random-mar">MAR</a>
</li>
<li>the parameters in the function of the missing data process are unrelated to the parameters (of interest) that need to be estimated.</li>
</ol>
<p>In this case, you actually don’t need to model the missing data mechanisms unless you would like to improve on your accuracy, in which case you still need to be very rigorous about your approach to improve efficiency in your parameters.</p>
</div>
<div id="nonignorable" class="section level3" number="11.1.4">
<h3>
<span class="header-section-number">11.1.4</span> Nonignorable<a class="anchor" aria-label="anchor" href="#nonignorable"><i class="fas fa-link"></i></a>
</h3>
<p>Missing Not at Random, MNAR, means there is a relationship between the propensity of a value to be missing and its values.</p>
<p>Example: people with the lowest education are missing on education or the sickest people are most likely to drop out of the study.</p>
<p>MNAR is called <a href="imputation-missing-data.html#nonignorable">Nonignorable</a> because the missing data mechanism itself has to be modeled as you deal with the missing data. You have to include some model for why the data are missing and what the likely values are.</p>
<p>Hence, in the case of nonignorable, the data are not MAR. Then, your parameters of interest will be biased if you do not model the missing data mechanism. One of the most widely used approach for nonignorable missing data is <span class="citation">(<a href="references.html#ref-Heckman_1976">James J. Heckman 1976</a>)</span></p>
<ul>
<li>
<p>Another name: Missing Not at Random (MNAR): there is a relationship between the propensity of a value to be missing and its values</p>
<ul>
<li>For example, people with low education will be less likely to report it</li>
</ul>
</li>
<li><p>We need to model why the data are missing and what the likely values are.</p></li>
<li><p>the missing data mechanism is related to the missing values</p></li>
<li><p>It commonly occurs when people do not want to reveal something very personal or unpopular about themselves</p></li>
<li><p>Complete case analysis can give highly biased results for NI missing data. If proportionally more low and moderate income individuals are left in the sample because high income people are missing, an estimate of the mean income will be lower than the actual population mean.</p></li>
</ul>
<p>One can use instrument that can predict the nonresponse process in outcome variable, and unrelated to the outcome of the population to correct for this missingness (but you still have to use complete cases) <span class="citation">(<a href="references.html#ref-sun2018semiparametric">B. Sun et al. 2018</a>; <a href="references.html#ref-tchetgen2017general">E. J. Tchetgen Tchetgen and Wirth 2017</a>)</span></p>
</div>
</div>
<div id="solutions-to-missing-data" class="section level2" number="11.2">
<h2>
<span class="header-section-number">11.2</span> Solutions to Missing data<a class="anchor" aria-label="anchor" href="#solutions-to-missing-data"><i class="fas fa-link"></i></a>
</h2>
<div id="listwise-deletion" class="section level3" number="11.2.1">
<h3>
<span class="header-section-number">11.2.1</span> Listwise Deletion<a class="anchor" aria-label="anchor" href="#listwise-deletion"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as complete case deletion only where you only retain cases with complete data for all features.</p>
<p>Advantages:</p>
<ul>
<li><p>Can be applied to any statistical test (SEM, multi-level regression, etc.)</p></li>
<li><p>In the case of MCAR, both the parameters estimates and its standard errors are unbiased.</p></li>
<li>
<p>In the case of MAR among independent variables (not depend on the values of dependent variables), then listwise deletion parameter estimates can still be unbiased. <span class="citation">(<a href="references.html#ref-Little_1992">Little 1992</a>)</span> For example, you have a model <span class="math inline">\(y=\beta_{0}+\beta_1X_1 + \beta_2X_2 +\epsilon\)</span> if the probability of missing data on <span class="math inline">\(X1\)</span> is independent of <span class="math inline">\(Y\)</span>, but dependent on the value of <span class="math inline">\(X1\)</span> and <span class="math inline">\(X2\)</span>, then the model estimates are still unbiased.</p>
<ul>
<li>The missing data mechanism the depends on the values of the independent variables are the same as stratified sampling. And stratified sampling does not bias your estimates</li>
<li>In the case of logistic regression, if the probability of missing data on any variable depends on the value of the dependent variable, but independent of the value of the independent variables, then the listwise deletion will yield biased intercept estimate, but consistent estimates of the slope and their standard errors <span class="citation">(<a href="references.html#ref-Vach_1994">Vach and Vach 1994</a>)</span>. However, logistic regression will still fail if the probability of missing data is dependent on both the value of the dependent and independent variables.</li>
<li>Under regression analysis, listwise deletion is more robust than maximum likelihood and multiple imputation when MAR assumption is violated.</li>
</ul>
</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>It will yield a larger standard errors than other more sophisticated methods discussed later.</li>
<li>If the data are not MCAR, but MAR, then your listwise deletion can yield biased estimates.</li>
<li>In other cases than regression analysis, other sophisticated methods can yield better estimates compared to listwise deletion.</li>
</ul>
</div>
<div id="pairwise-deletion" class="section level3" number="11.2.2">
<h3>
<span class="header-section-number">11.2.2</span> Pairwise Deletion<a class="anchor" aria-label="anchor" href="#pairwise-deletion"><i class="fas fa-link"></i></a>
</h3>
<p>This method could only be used in the case of linear models such as linear regression, factor analysis, or SEM. The premise of this method based on that the coefficient estimates are calculated based on the means, standard deviations, and correlation matrix. Compared to listwise deletion, we still utilized as many correlation between variables as possible to compute the correlation matrix.</p>
<p>Advantages:</p>
<ul>
<li><p>If the true missing data mechanism is MCAR, pair wise deletion will yield consistent estimates, and unbiased in large samples</p></li>
<li>
<p>Compared to listwise deletion: <span class="citation">(<a href="references.html#ref-Glasser_1964">Glasser 1964</a>)</span></p>
<ul>
<li>If the correlation among variables are low, pairwise deletion is more efficient estimates than listwise</li>
<li>If the correlations among variables are high, listwise deletion is more efficient than pairwise.</li>
</ul>
</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>If the data mechanism is MAR, pairwise deletion will yield biased estimates.</li>
<li>In small sample, sometimes covariance matrix might not be positive definite, which means coefficients estimates cannot be calculated.</li>
</ul>
<p><strong>Note</strong>: You need to read carefully on how your software specify the sample size because it will alter the standard errors.</p>
</div>
<div id="dummy-variable-adjustment" class="section level3" number="11.2.3">
<h3>
<span class="header-section-number">11.2.3</span> Dummy Variable Adjustment<a class="anchor" aria-label="anchor" href="#dummy-variable-adjustment"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as Missing Indicator Method or Proxy Variable</p>
<p>Add another variable in the database to indicate whether a value is missing.</p>
<p>Create 2 variables</p>
<p><span class="math display">\[
D=
\begin{cases}
1 &amp; \text{data on X are missing} \\
0 &amp; \text{otherwise}\\
\end{cases}
\]</span></p>
<p><span class="math display">\[
X^* =
\begin{cases}
X &amp; \text{data are available} \\
c &amp; \text{data are missing}\\
\end{cases}
\]</span></p>
<p><strong>Note</strong>: A typical choice for <span class="math inline">\(c\)</span> is usually the mean of <span class="math inline">\(X\)</span></p>
<p>Interpretation:</p>
<ul>
<li>Coefficient of <span class="math inline">\(D\)</span> is the the difference in the expected value of <span class="math inline">\(Y\)</span> between the group with data and the group without data on <span class="math inline">\(X\)</span>.</li>
<li>Coefficient of <span class="math inline">\(X^*\)</span> is the effect of the group with data on <span class="math inline">\(Y\)</span>
</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>This method yields biased estimates of the coefficient even in the case of MCAR <span class="citation">(<a href="references.html#ref-jones1996indicator">Jones 1996</a>)</span>
</li>
</ul>
</div>
<div id="imputation" class="section level3" number="11.2.4">
<h3>
<span class="header-section-number">11.2.4</span> Imputation<a class="anchor" aria-label="anchor" href="#imputation"><i class="fas fa-link"></i></a>
</h3>
<div id="mean-mode-median-imputation" class="section level4" number="11.2.4.1">
<h4>
<span class="header-section-number">11.2.4.1</span> Mean, Mode, Median Imputation<a class="anchor" aria-label="anchor" href="#mean-mode-median-imputation"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<p>Bad:</p>
<ul>
<li>Mean imputation does not preserve the relationships among variables</li>
<li>Mean imputation leads to An Underestimate of Standard Errors → you’re making Type I errors without realizing it.</li>
<li>Biased estimates of variances and covariances <span class="citation">(<a href="references.html#ref-haitovsky1968missing">Haitovsky 1968</a>)</span>
</li>
<li>In high-dimensions, mean substitution cannot account for dependence structure among features.</li>
</ul>
</li>
</ul>
</div>
<div id="maximum-likelihood" class="section level4" number="11.2.4.2">
<h4>
<span class="header-section-number">11.2.4.2</span> Maximum Likelihood<a class="anchor" aria-label="anchor" href="#maximum-likelihood"><i class="fas fa-link"></i></a>
</h4>
<p>When missing data are MAR and monotonic (such as in the case of panel studies), ML can be adequately in estimating coefficients.</p>
<p>Monotonic means that if you are missing data on X1, then that observation also has missing data on all other variables that come after it.</p>
<p>ML can generally handle linear models, log-linear model, but beyond that, ML still lacks both theory and software to implement.</p>
<div id="expectation-maximization-algorithm-em-algorithm" class="section level5" number="11.2.4.2.1">
<h5>
<span class="header-section-number">11.2.4.2.1</span> Expectation-Maximization Algorithm (EM Algorithm)<a class="anchor" aria-label="anchor" href="#expectation-maximization-algorithm-em-algorithm"><i class="fas fa-link"></i></a>
</h5>
<p>An iterative process:</p>
<ol style="list-style-type: decimal">
<li>Other variables are used to impute a value (Expectation).</li>
<li>Check whether the value is most likely (Maximization).</li>
<li>If not, it re-imputes a more likely value.</li>
</ol>
<p>You start your regression with your estimates based on either listwise deletion or pairwise deletion. After regressing missing variables on available variables, you obtain a regression model. Plug the missing data back into the original model, with modified variances and covariances For example, if you have missing data on <span class="math inline">\(X_{ij}\)</span> you would regress it on available data of <span class="math inline">\(X_{i(j)}\)</span>, then plug the expected value of <span class="math inline">\(X_{ij}\)</span> back with its <span class="math inline">\(X_{ij}^2\)</span> turn into <span class="math inline">\(X_{ij}^2 + s_{j(j)}^2\)</span> where <span class="math inline">\(s_{j(j)}^2\)</span> stands for the residual variance from regressing <span class="math inline">\(X_{ij}\)</span> on <span class="math inline">\(X_{i(j)}\)</span> With the new estimated model, you rerun the process until the estimates converge.</p>
<p>Advantages:</p>
<ol style="list-style-type: decimal">
<li>Easy to use</li>
<li>Preserves the relationship with other variables (important if you use Factor Analysis or Linear Regression later on), but best in the case of Factor Analysis, which doesn’t require standard error of individuals item.</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Standard errors of the coefficients are incorrect (biased usually downward - underestimate)</li>
<li>Models with overidentification, the estimates will not be efficient</li>
</ol>
</div>
<div id="direct-ml-raw-maximum-likelihood" class="section level5" number="11.2.4.2.2">
<h5>
<span class="header-section-number">11.2.4.2.2</span> Direct ML (raw maximum likelihood)<a class="anchor" aria-label="anchor" href="#direct-ml-raw-maximum-likelihood"><i class="fas fa-link"></i></a>
</h5>
<p>Advantages</p>
<ol style="list-style-type: decimal">
<li>Efficient estimates and correct standard errors.</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Hard to implements</li>
</ol>
</div>
</div>
<div id="multiple-imputation" class="section level4" number="11.2.4.3">
<h4>
<span class="header-section-number">11.2.4.3</span> Multiple Imputation<a class="anchor" aria-label="anchor" href="#multiple-imputation"><i class="fas fa-link"></i></a>
</h4>
<p>MI is designed to use “the Bayesian model-based approach to <em>create</em> procedures, and the frequentist (randomization-based approach) to <em>evaluate</em> procedures”. <span class="citation">(<a href="references.html#ref-Rubin_1996">Rubin 1996</a>)</span></p>
<p>MI estimates have the same properties as <a href="imputation-missing-data.html#maximum-likelihood">ML</a> when the data is <a href="imputation-missing-data.html#missing-at-random-mar">MAR</a></p>
<ul>
<li>Consistent</li>
<li>Asymptotically efficient</li>
<li>Asymptotically normal</li>
</ul>
<p>MI can be applied to any type of model, unlike <a href="imputation-missing-data.html#maximum-likelihood">Maximum Likelihood</a> that is only limited to a small set of models.</p>
<p>A drawback of MI is that it will produce slightly different estimates every time you run it. To avoid such problem, you can set seed when doing your analysis to ensure its reproducibility.</p>
<div id="single-random-imputation" class="section level5" number="11.2.4.3.1">
<h5>
<span class="header-section-number">11.2.4.3.1</span> Single Random Imputation<a class="anchor" aria-label="anchor" href="#single-random-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>Random draws form the residual distribution of each imputed variable and add those random numbers to the imputed values.</p>
<p>For example, if we have missing data on <span class="math inline">\(X\)</span>, and it’s MCAR, then</p>
<ol style="list-style-type: decimal">
<li><p>Regress <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> (<a href="imputation-missing-data.html#listwise-deletion">Listwise Deletion</a> method) to get its residual distribution.</p></li>
<li>
<p>For every missing value on X, we substitute with <span class="math inline">\(\tilde{x_i}=\hat{x_i} + \rho u_i\)</span> where</p>
<ul>
<li>
<span class="math inline">\(u_i\)</span> is a random draw from a standard normal distribution</li>
<li>
<span class="math inline">\(x_i\)</span> is the predicted value from the regression of X and Y</li>
<li>
<span class="math inline">\(\rho\)</span> is the standard deviation of the residual distribution of X regressed on Y.</li>
</ul>
</li>
</ol>
<p>However, the model you run with the imputed data still thinks that your data are collected, not imputed, which leads your standard error estimates to be too low and test statistics too high.</p>
<p>To address this problem, we need to repeat the imputation process which leads us to repeated imputation or multiple random imputation.</p>
</div>
<div id="repeated-imputation" class="section level5" number="11.2.4.3.2">
<h5>
<span class="header-section-number">11.2.4.3.2</span> Repeated Imputation<a class="anchor" aria-label="anchor" href="#repeated-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>“Repeated imputations are draws from the posterior predictive distribution of the missing values under a specific model , a particular Bayesian model for both the data and the missing mechanism”.<span class="citation">(<a href="references.html#ref-Rubin_1996">Rubin 1996</a>)</span></p>
<p>Repeated imputation, also known as, multiple random imputation, allows us to have multiple “completed” data sets. The variability across imputations will adjust the standard errors upward.</p>
<p>The estimate of the standard error of <span class="math inline">\(\bar{r}\)</span> (mean correlation estimates between X and Y) is <span class="math display">\[
SE(\bar{r})=\sqrt{\frac{1}{M}\sum_{k}s_k^2+ (1+\frac{1}{M})(\frac{1}{M-1})\sum_{k}(r_k-\bar{r})^2}
\]</span> where M is the number of replications, <span class="math inline">\(r_k\)</span> is the the correlation in replication k, <span class="math inline">\(s_k\)</span> is the estimated standard error in replication k.</p>
<p>However, this method still considers the parameter in predicting <span class="math inline">\(\tilde{x}\)</span> is still fixed, which means we assume that we are using the true parameters to predict <span class="math inline">\(\tilde{x}\)</span>. To overcome this challenge, we need to introduce variability into our model for <span class="math inline">\(\tilde{x}\)</span> by treating the parameters as a random variables and use Bayesian posterior distribution of the parameters to predict the parameters.</p>
<p>However, if your sample is large and the proportion of missing data is small, the extra Bayesian step might not be necessary. If your sample is small or the proportion of missing data is large, the extra Bayesian step is necessary.</p>
<p>Two algorithms to get random draws of the regression parameters from its posterior distribution:</p>
<ul>
<li><a href="imputation-missing-data.html#data-augmentation">Data Augmentation</a></li>
<li>Sampling importance/resampling (SIR)</li>
</ul>
<p>Authors have argued for SIR superiority due to its computer time <span class="citation">(<a href="references.html#ref-king2001analyzing">G. King et al. 2001</a>)</span></p>
<div id="data-augmentation" class="section level6" number="11.2.4.3.2.1">
<h6>
<span class="header-section-number">11.2.4.3.2.1</span> Data Augmentation<a class="anchor" aria-label="anchor" href="#data-augmentation"><i class="fas fa-link"></i></a>
</h6>
<p>Steps for data augmentation:</p>
<ol style="list-style-type: decimal">
<li>Choose starting values for the parameters (e.g., for multivariate normal, choose means and covariance matrix). These values can come from previous values, expert knowledge, or from listwise deletion or pairwise deletion or EM estimation.</li>
<li>Based on the current values of means and covariances calculate the coefficients estimates for the equation that variable with missing data is regressed on all other variables (or variables that you think will help predict the missing values, could also be variables that are not in the final estimation model)</li>
<li>Use the estimates in step (2) to predict values for missing values. For each predicted value, add a random error from the residual normal distribution for that variable.</li>
<li>From the “complete” data set, recalculate the means and covariance matrix. And take a random draw from the posterior distribution of the means and covariances with Jeffreys’ prior.</li>
<li>Using the random draw from step (4), repeat step (2) to (4) until the means and covariances stabilize (converged).</li>
</ol>
<p>The iterative process allows us to get random draws from the joint posterior distribution of both data nd parameters, given the observed data.</p>
<p>Rules of thumb regarding convergence:</p>
<ul>
<li>The higher the proportion of missing, the more iterations</li>
<li>the rate of convergence for EM algorithm should be the minimum threshold for DA.</li>
<li>You can also check if your distribution has been converged by diagnostic statistics Can check <a href="https://bookdown.org/mike/bayesian_analysis/diag.html">Bayesian Diagnostics</a> for some introduction.</li>
</ul>
<p>Types of chains</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Parallel</strong>: Run a separate chain of iterations for each of data set. Different starting values are encouraged. For example, one could use bootstrap to generate different data set with replacement, and for each data set, calculate the starting values by EM estimates.</p>
<ul>
<li>Pro: Run faster, and less likely to have dependence in the resulting data sets.</li>
<li>Con: Sometimes it will not converge</li>
</ul>
</li>
<li>
<p><strong>Sequential</strong> one long chain of data augmentation cycles. After burn-in and thinning, you will have to data sets</p>
<ul>
<li>Pro: Converged to the true posterior distribution is more likely.</li>
<li>Con: The resulting data sets are likely to be dependent. Remedies can be thinning and burn-in.</li>
</ul>
</li>
</ol>
<p><strong>Note on Non-normal or categorical data</strong> The normal-based methods still work well, but you will need to do some transformation. For example,</p>
<ul>
<li>If the data is skewed, then log-transform, then impute the exponentiate to have the missing data back to its original metric.</li>
<li>If the data is proportion, logit-transform, impute, then de-transform the missing data.</li>
</ul>
<p>If you want to impute non-linear relationship, such as interaction between 2 variables and 1 variable is categorical. You can do separate imputation for different levels of that variable separately, then combined for the final analysis.</p>
<ul>
<li>If all variables that have missing data are categorical, then <strong>unrestricted multinomial model</strong> or <strong>log-linear model</strong> is recommended.</li>
<li>If a single categorical variable, <strong>logistic (logit) regression</strong> would be sufficient.</li>
</ul>
</div>
</div>
</div>
<div id="nonparametric-semiparametric-methods" class="section level4" number="11.2.4.4">
<h4>
<span class="header-section-number">11.2.4.4</span> Nonparametric/ Semiparametric Methods<a class="anchor" aria-label="anchor" href="#nonparametric-semiparametric-methods"><i class="fas fa-link"></i></a>
</h4>
<div id="hot-deck-imputation" class="section level5" number="11.2.4.4.1">
<h5>
<span class="header-section-number">11.2.4.4.1</span> Hot Deck Imputation<a class="anchor" aria-label="anchor" href="#hot-deck-imputation"><i class="fas fa-link"></i></a>
</h5>
<ul>
<li>Used by U.S. Census Bureau for public datasets</li>
<li>approximate Bayesian bootstrap</li>
<li>A randomly chosen value from an individual in the sample who has similar values on other variables. In other words, find all the sample subjects who are similar on other variables, then randomly choose one of their values on the missing variable.</li>
</ul>
<p>When we have <span class="math inline">\(n_1\)</span> cases with complete data on <span class="math inline">\(Y\)</span> and <span class="math inline">\(n_0\)</span> cases with missing data on <span class="math inline">\(Y\)</span></p>
<ul>
<li>Step 1: From <span class="math inline">\(n_1\)</span>, take a random sample (with replacement) of <span class="math inline">\(n_1\)</span> cases</li>
<li>Step 2: From the retrieved sample take a random sample (with replacement) of <span class="math inline">\(n_0\)</span> cases</li>
<li>Step 3: Assign the <span class="math inline">\(n_0\)</span> cases in step 2 to <span class="math inline">\(n_0\)</span> missing data cases.</li>
<li>Step 4: Repeat the process for every variable.</li>
<li>Step 5: For multiple imputation, repeat the four steps multiple times.</li>
</ul>
<p>Note:</p>
<ul>
<li><p>If we skip step 1, it reduce variability for estimating standard errors.</p></li>
<li>
<p>Good:</p>
<ul>
<li>Constrained to only possible values.</li>
<li>Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors.</li>
</ul>
</li>
<li><p>Challenge: how can you define “similar” here.</p></li>
</ul>
</div>
<div id="cold-deck-imputation" class="section level5" number="11.2.4.4.2">
<h5>
<span class="header-section-number">11.2.4.4.2</span> Cold Deck Imputation<a class="anchor" aria-label="anchor" href="#cold-deck-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want.</p>
<p>Donor samples of “cold-deck” imputation come from a different data set.</p>
</div>
<div id="predictive-mean-matching" class="section level5" number="11.2.4.4.3">
<h5>
<span class="header-section-number">11.2.4.4.3</span> Predictive Mean Matching<a class="anchor" aria-label="anchor" href="#predictive-mean-matching"><i class="fas fa-link"></i></a>
</h5>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> (matrix of covariates) for the <span class="math inline">\(n_1\)</span> (i.e., non-missing cases) to get coefficients <span class="math inline">\(b\)</span> (a <span class="math inline">\(k \times 1\)</span> vector) and residual variance estimates <span class="math inline">\(s^2\)</span>
</li>
<li>Draw randomly from the posterior predictive distribution of the residual variance (assuming a noninformative prior) by calculating <span class="math inline">\(\frac{(n_1-k)s^2}{\chi^2}\)</span>, where <span class="math inline">\(\chi^2\)</span> is a random draw from a <span class="math inline">\(\chi^2_{n_1-k}\)</span> and let <span class="math inline">\(s^2_{[1]}\)</span> be an i-th random draw</li>
<li>Randomly draw from the posterior distribution of the coefficients <span class="math inline">\(b\)</span>, by drawing from <span class="math inline">\(MVN(b, s^2_{[1]}(X'X)^{-1})\)</span>, where X is an <span class="math inline">\(n_1 \times k\)</span> matrix of X values. Then we have <span class="math inline">\(b_{1}\)</span>
</li>
<li>Using step 1, we can calculate standardized residuals for <span class="math inline">\(n_1\)</span> cases: <span class="math inline">\(e_i = \frac{y_i - bx_i}{\sqrt{s^2(1-k/n_1)}}\)</span>
</li>
<li>Randomly draw a sample (with replacement) of <span class="math inline">\(n_0\)</span> from the <span class="math inline">\(n_1\)</span> residuals in step 4</li>
<li>With <span class="math inline">\(n_0\)</span> cases, we can calculate imputed values of <span class="math inline">\(Y\)</span>: <span class="math inline">\(y_i = b_{[1]}x_i + s_{[1]}e_i\)</span> where <span class="math inline">\(e_i\)</span> are taken from step 5, and <span class="math inline">\(b_{[1]}\)</span> taken from step 3, and <span class="math inline">\(s_{[1]}\)</span> taken from step 2.</li>
<li>Repeat steps 2 through 6 except for step 4.</li>
</ol>
<p>Notes:</p>
<ul>
<li>can be used for multiple variables where each variable is imputed using all other variables as predictor.</li>
<li>can also be used for heteroskedasticity in imputed values.</li>
</ul>
<p>Example from <a href="https://statisticsglobe.com/predictive-mean-matching-imputation-method/">Statistics Globe</a></p>
<div class="sourceCode" id="cb219"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">918273</span><span class="op">)</span> <span class="co"># Seed</span></span>
<span><span class="va">N</span>  <span class="op">&lt;-</span> <span class="fl">3000</span>                                    <span class="co"># Sample size</span></span>
<span><span class="va">y</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span>,<span class="op">-</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>                 <span class="co"># Target variable Y</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="va">y</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">0</span>, <span class="fl">50</span><span class="op">)</span><span class="op">)</span>              <span class="co"># Auxiliary variable 1</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">y</span> <span class="op">+</span> <span class="fl">0.25</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>,<span class="op">-</span><span class="fl">3</span>, <span class="fl">15</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Auxiliary variable 2</span></span>
<span><span class="va">x3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">0.1</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>           <span class="co"># Auxiliary variable 3</span></span>
<span><span class="co"># (categorical variable)</span></span>
<span><span class="va">x4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">0.02</span> <span class="op">*</span> <span class="va">y</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>   <span class="co"># Auxiliary variable 4 </span></span>
<span></span>
<span><span class="co"># Insert 20% missing data in Y</span></span>
<span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, <span class="fl">0.2</span><span class="op">)</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span>               </span>
<span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span>, <span class="va">x4</span><span class="op">)</span>         <span class="co"># Store data in dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="co"># First 6 rows of our data</span></span>
<span><span class="co">#&gt;    y x1  x2 x3 x4</span></span>
<span><span class="co">#&gt; 1  8 38  -3  6  1</span></span>
<span><span class="co">#&gt; 2  1 50  -9  5  0</span></span>
<span><span class="co">#&gt; 3  5 43  20  5  1</span></span>
<span><span class="co">#&gt; 4 NA  9  13  3  0</span></span>
<span><span class="co">#&gt; 5 -4 40 -10  6  0</span></span>
<span><span class="co">#&gt; 6 NA 29  -6  5  1</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/amices/mice">"mice"</a></span><span class="op">)</span> <span class="co"># Load mice package</span></span>
<span></span>
<span><span class="co">##### Impute data via predictive mean matching (single imputation)#####</span></span>
<span></span>
<span><span class="va">imp_single</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data</span>, m <span class="op">=</span> <span class="fl">1</span>, method <span class="op">=</span> <span class="st">"pmm"</span><span class="op">)</span> <span class="co"># Impute missing values</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="va">data_imp_single</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_single</span><span class="op">)</span>         <span class="co"># Store imputed data</span></span>
<span><span class="co"># head(data_imp_single)</span></span>
<span></span>
<span><span class="co"># Since single imputation underestiamtes stnadard errors, </span></span>
<span><span class="co"># we use multiple imputaiton</span></span>
<span></span>
<span><span class="co">##### Predictive mean matching (multiple imputation) #####</span></span>
<span></span>
<span><span class="co"># Impute missing values multiple times</span></span>
<span><span class="va">imp_multi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data</span>, m <span class="op">=</span> <span class="fl">5</span>, method <span class="op">=</span> <span class="st">"pmm"</span><span class="op">)</span>  </span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   1   2  y</span></span>
<span><span class="co">#&gt;   1   3  y</span></span>
<span><span class="co">#&gt;   1   4  y</span></span>
<span><span class="co">#&gt;   1   5  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   2   2  y</span></span>
<span><span class="co">#&gt;   2   3  y</span></span>
<span><span class="co">#&gt;   2   4  y</span></span>
<span><span class="co">#&gt;   2   5  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   3   2  y</span></span>
<span><span class="co">#&gt;   3   3  y</span></span>
<span><span class="co">#&gt;   3   4  y</span></span>
<span><span class="co">#&gt;   3   5  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   4   2  y</span></span>
<span><span class="co">#&gt;   4   3  y</span></span>
<span><span class="co">#&gt;   4   4  y</span></span>
<span><span class="co">#&gt;   4   5  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="co">#&gt;   5   2  y</span></span>
<span><span class="co">#&gt;   5   3  y</span></span>
<span><span class="co">#&gt;   5   4  y</span></span>
<span><span class="co">#&gt;   5   5  y</span></span>
<span><span class="va">data_imp_multi_all</span> <span class="op">&lt;-</span></span>
<span>    <span class="co"># Store multiply imputed data</span></span>
<span>    <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_multi</span>,       </span>
<span>             <span class="st">"repeated"</span>,</span>
<span>             include <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_imp_multi</span> <span class="op">&lt;-</span></span>
<span>    <span class="co"># Combine imputed Y and X1-X4 (for convenience)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">data_imp_multi_all</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span>, <span class="va">data</span><span class="op">[</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data_imp_multi</span><span class="op">)</span></span>
<span><span class="co">#&gt;   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4</span></span>
<span><span class="co">#&gt; 1   8   8   8   8   8   8 38  -3  6  1</span></span>
<span><span class="co">#&gt; 2   1   1   1   1   1   1 50  -9  5  0</span></span>
<span><span class="co">#&gt; 3   5   5   5   5   5   5 43  20  5  1</span></span>
<span><span class="co">#&gt; 4  NA   1  -2  -4   9  -8  9  13  3  0</span></span>
<span><span class="co">#&gt; 5  -4  -4  -4  -4  -4  -4 40 -10  6  0</span></span>
<span><span class="co">#&gt; 6  NA   4   7   7   6   0 29  -6  5  1</span></span></code></pre></div>
<p>Example from <a href="https://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/">UCLA Statistical Consulting</a></p>
<div class="sourceCode" id="cb220"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/amices/mice">mice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/statistikat/VIM">VIM</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lattice.r-forge.r-project.org/">lattice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="co">## set observations to NA</span></span>
<span><span class="va">anscombe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">within</a></span><span class="op">(</span><span class="va">anscombe</span>, <span class="op">{</span></span>
<span>    <span class="va">y1</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span>    <span class="va">y4</span><span class="op">[</span><span class="fl">3</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span><span class="co">## view</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">anscombe</span><span class="op">)</span></span>
<span><span class="co">#&gt;   x1 x2 x3 x4   y1   y2    y3   y4</span></span>
<span><span class="co">#&gt; 1 10 10 10  8   NA 9.14  7.46 6.58</span></span>
<span><span class="co">#&gt; 2  8  8  8  8   NA 8.14  6.77 5.76</span></span>
<span><span class="co">#&gt; 3 13 13 13  8   NA 8.74 12.74   NA</span></span>
<span><span class="co">#&gt; 4  9  9  9  8 8.81 8.77  7.11   NA</span></span>
<span><span class="co">#&gt; 5 11 11 11  8 8.33 9.26  7.81   NA</span></span>
<span><span class="co">#&gt; 6 14 14 14  8 9.96 8.10  8.84 7.04</span></span>
<span></span>
<span><span class="co">## check missing data patterns</span></span>
<span><span class="fu"><a href="https://amices.org/mice/reference/md.pattern.html">md.pattern</a></span><span class="op">(</span><span class="va">anscombe</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt;   x1 x2 x3 x4 y2 y3 y1 y4  
#&gt; 6  1  1  1  1  1  1  1  1 0
#&gt; 2  1  1  1  1  1  1  1  0 1
#&gt; 2  1  1  1  1  1  1  0  1 1
#&gt; 1  1  1  1  1  1  1  0  0 2
#&gt;    0  0  0  0  0  0  3  3 6

## Number of observations per patterns for all pairs of variables
p &lt;- md.pairs(anscombe)
p 
#&gt; $rr
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1 11 11 11 11  8 11 11  8
#&gt; x2 11 11 11 11  8 11 11  8
#&gt; x3 11 11 11 11  8 11 11  8
#&gt; x4 11 11 11 11  8 11 11  8
#&gt; y1  8  8  8  8  8  8  8  6
#&gt; y2 11 11 11 11  8 11 11  8
#&gt; y3 11 11 11 11  8 11 11  8
#&gt; y4  8  8  8  8  6  8  8  8
#&gt; 
#&gt; $rm
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  3  0  0  3
#&gt; x2  0  0  0  0  3  0  0  3
#&gt; x3  0  0  0  0  3  0  0  3
#&gt; x4  0  0  0  0  3  0  0  3
#&gt; y1  0  0  0  0  0  0  0  2
#&gt; y2  0  0  0  0  3  0  0  3
#&gt; y3  0  0  0  0  3  0  0  3
#&gt; y4  0  0  0  0  2  0  0  0
#&gt; 
#&gt; $mr
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  0  0  0  0
#&gt; x2  0  0  0  0  0  0  0  0
#&gt; x3  0  0  0  0  0  0  0  0
#&gt; x4  0  0  0  0  0  0  0  0
#&gt; y1  3  3  3  3  0  3  3  2
#&gt; y2  0  0  0  0  0  0  0  0
#&gt; y3  0  0  0  0  0  0  0  0
#&gt; y4  3  3  3  3  2  3  3  0
#&gt; 
#&gt; $mm
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  0  0  0  0
#&gt; x2  0  0  0  0  0  0  0  0
#&gt; x3  0  0  0  0  0  0  0  0
#&gt; x4  0  0  0  0  0  0  0  0
#&gt; y1  0  0  0  0  3  0  0  1
#&gt; y2  0  0  0  0  0  0  0  0
#&gt; y3  0  0  0  0  0  0  0  0
#&gt; y4  0  0  0  0  1  0  0  3</code></pre>
<ul>
<li>
<code>rr</code> = number of observations where both pairs of values are observed</li>
<li>
<code>rm</code> = the number of observations where both variables are missing values</li>
<li>
<code>mr</code> = the number of observations where the first variable’s value (e.g. the row variable) is observed and second (or column) variable is missing</li>
<li>
<code>mm</code> = the number of observations where the second variable’s value (e.g. the col variable) is observed and first (or row) variable is missing</li>
</ul>
<div class="sourceCode" id="cb222"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Margin plot of y1 and y4</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/VIM/man/marginplot.html">marginplot</a></span><span class="op">(</span><span class="va">anscombe</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">8</span><span class="op">)</span><span class="op">]</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"orange"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb223"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## 5 imputations for all missing values</span></span>
<span><span class="va">imp1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">anscombe</span>, m <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y1  y4</span></span>
<span><span class="co">#&gt;   1   2  y1  y4</span></span>
<span><span class="co">#&gt;   1   3  y1  y4</span></span>
<span><span class="co">#&gt;   1   4  y1  y4</span></span>
<span><span class="co">#&gt;   1   5  y1  y4</span></span>
<span><span class="co">#&gt;   2   1  y1  y4</span></span>
<span><span class="co">#&gt;   2   2  y1  y4</span></span>
<span><span class="co">#&gt;   2   3  y1  y4</span></span>
<span><span class="co">#&gt;   2   4  y1  y4</span></span>
<span><span class="co">#&gt;   2   5  y1  y4</span></span>
<span><span class="co">#&gt;   3   1  y1  y4</span></span>
<span><span class="co">#&gt;   3   2  y1  y4</span></span>
<span><span class="co">#&gt;   3   3  y1  y4</span></span>
<span><span class="co">#&gt;   3   4  y1  y4</span></span>
<span><span class="co">#&gt;   3   5  y1  y4</span></span>
<span><span class="co">#&gt;   4   1  y1  y4</span></span>
<span><span class="co">#&gt;   4   2  y1  y4</span></span>
<span><span class="co">#&gt;   4   3  y1  y4</span></span>
<span><span class="co">#&gt;   4   4  y1  y4</span></span>
<span><span class="co">#&gt;   4   5  y1  y4</span></span>
<span><span class="co">#&gt;   5   1  y1  y4</span></span>
<span><span class="co">#&gt;   5   2  y1  y4</span></span>
<span><span class="co">#&gt;   5   3  y1  y4</span></span>
<span><span class="co">#&gt;   5   4  y1  y4</span></span>
<span><span class="co">#&gt;   5   5  y1  y4</span></span>
<span></span>
<span><span class="co">## linear regression for each imputed data set - 5 regression are run</span></span>
<span><span class="va">fitm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">imp1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y1</span> <span class="op">~</span> <span class="va">y4</span> <span class="op">+</span> <span class="va">x1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fitm</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 15 × 6</span></span>
<span><span class="co">#&gt;    term        estimate std.error statistic p.value  nobs</span></span>
<span><span class="co">#&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;</span></span>
<span><span class="co">#&gt;  1 (Intercept)    8.60      2.67      3.23  0.0121     11</span></span>
<span><span class="co">#&gt;  2 y4            -0.533     0.251    -2.12  0.0667     11</span></span>
<span><span class="co">#&gt;  3 x1             0.334     0.155     2.16  0.0628     11</span></span>
<span><span class="co">#&gt;  4 (Intercept)    4.19      2.93      1.43  0.190      11</span></span>
<span><span class="co">#&gt;  5 y4            -0.213     0.273    -0.782 0.457      11</span></span>
<span><span class="co">#&gt;  6 x1             0.510     0.167     3.05  0.0159     11</span></span>
<span><span class="co">#&gt;  7 (Intercept)    6.51      2.35      2.77  0.0244     11</span></span>
<span><span class="co">#&gt;  8 y4            -0.347     0.215    -1.62  0.145      11</span></span>
<span><span class="co">#&gt;  9 x1             0.395     0.132     3.00  0.0169     11</span></span>
<span><span class="co">#&gt; 10 (Intercept)    5.48      3.02      1.81  0.107      11</span></span>
<span><span class="co">#&gt; 11 y4            -0.316     0.282    -1.12  0.295      11</span></span>
<span><span class="co">#&gt; 12 x1             0.486     0.173     2.81  0.0230     11</span></span>
<span><span class="co">#&gt; 13 (Intercept)    7.12      1.81      3.92  0.00439    11</span></span>
<span><span class="co">#&gt; 14 y4            -0.436     0.173    -2.53  0.0355     11</span></span>
<span><span class="co">#&gt; 15 x1             0.425     0.102     4.18  0.00308    11</span></span>
<span></span>
<span><span class="co">## pool coefficients and standard errors across all 5 regression models</span></span>
<span><span class="fu"><a href="https://amices.org/mice/reference/pool.html">pool</a></span><span class="op">(</span><span class="va">fitm</span><span class="op">)</span></span>
<span><span class="co">#&gt; Class: mipo    m = 5 </span></span>
<span><span class="co">#&gt;          term m   estimate       ubar           b           t dfcom       df</span></span>
<span><span class="co">#&gt; 1 (Intercept) 5  6.3808015 6.72703243 2.785088109 10.06913816     8 3.902859</span></span>
<span><span class="co">#&gt; 2          y4 5 -0.3690455 0.05860053 0.014674911  0.07621042     8 4.716160</span></span>
<span><span class="co">#&gt; 3          x1 5  0.4301588 0.02191260 0.004980516  0.02788922     8 4.856052</span></span>
<span><span class="co">#&gt;         riv    lambda       fmi</span></span>
<span><span class="co">#&gt; 1 0.4968172 0.3319158 0.5254832</span></span>
<span><span class="co">#&gt; 2 0.3005074 0.2310693 0.4303733</span></span>
<span><span class="co">#&gt; 3 0.2727480 0.2142985 0.4143230</span></span>
<span></span>
<span><span class="co">## output parameter estimates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/pool.html">pool</a></span><span class="op">(</span><span class="va">fitm</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;          term   estimate std.error statistic       df    p.value</span></span>
<span><span class="co">#&gt; 1 (Intercept)  6.3808015 3.1731905  2.010847 3.902859 0.11643863</span></span>
<span><span class="co">#&gt; 2          y4 -0.3690455 0.2760624 -1.336819 4.716160 0.24213491</span></span>
<span><span class="co">#&gt; 3          x1  0.4301588 0.1670007  2.575791 4.856052 0.05107581</span></span></code></pre></div>
</div>
<div id="stochastic-imputation" class="section level5" number="11.2.4.4.4">
<h5>
<span class="header-section-number">11.2.4.4.4</span> Stochastic Imputation<a class="anchor" aria-label="anchor" href="#stochastic-imputation"><i class="fas fa-link"></i></a>
</h5>
<p><code>Regression imputation + random residual = Stochastic Imputation</code></p>
<p>Most multiple imputation is based off of some form of stochastic regression imputation.</p>
<p>Good:</p>
<ul>
<li>Has all the advantage of <a href="imputation-missing-data.html#regression-imputation">Regression Imputation</a>
</li>
<li>and also has the random components</li>
</ul>
<p>Bad:</p>
<ul>
<li>might lead to implausible values (e.g. negative values)</li>
<li>can’t handle heteroskadastic data</li>
</ul>
<p><strong>Note</strong><br>
Multiple Imputation usually based on some form of stochastic regression imputation.</p>
<div class="sourceCode" id="cb224"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Income data</span></span>
<span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">91919</span><span class="op">)</span>                              <span class="co"># Set seed</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span>                                    <span class="co"># Sample size</span></span>
<span> </span>
<span><span class="va">income</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">0</span>, <span class="fl">500</span><span class="op">)</span><span class="op">)</span>            <span class="co"># Create some synthetic income data</span></span>
<span><span class="va">income</span><span class="op">[</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">income</span><span class="op">[</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span> </span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="va">income</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1000</span>, <span class="fl">1500</span><span class="op">)</span>          <span class="co"># Auxiliary variables</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="va">income</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="op">-</span> <span class="fl">5000</span>, <span class="fl">2000</span><span class="op">)</span></span>
<span> </span>
<span><span class="va">income</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, <span class="fl">0.1</span><span class="op">)</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span>         <span class="co"># Create 10% missingness in income</span></span>
<span> </span>
<span><span class="va">data_inc_miss</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">income</span>, <span class="va">x1</span>, <span class="va">x2</span><span class="op">)</span></span></code></pre></div>
<p>Single stochastic regression imputation</p>
<div class="sourceCode" id="cb225"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_inc_sri</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_inc_miss</span>, method <span class="op">=</span> <span class="st">"norm.nob"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  income</span></span>
<span><span class="co">#&gt;   2   1  income</span></span>
<span><span class="co">#&gt;   3   1  income</span></span>
<span><span class="co">#&gt;   4   1  income</span></span>
<span><span class="co">#&gt;   5   1  income</span></span>
<span><span class="va">data_inc_sri</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_inc_sri</span><span class="op">)</span></span></code></pre></div>
<p>Single predictive mean matching</p>
<div class="sourceCode" id="cb226"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_inc_pmm</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_inc_miss</span>, method <span class="op">=</span> <span class="st">"pmm"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  income</span></span>
<span><span class="co">#&gt;   2   1  income</span></span>
<span><span class="co">#&gt;   3   1  income</span></span>
<span><span class="co">#&gt;   4   1  income</span></span>
<span><span class="co">#&gt;   5   1  income</span></span>
<span><span class="va">data_inc_pmm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_inc_pmm</span><span class="op">)</span></span></code></pre></div>
<p>Stochastic regression imputation contains negative values</p>
<div class="sourceCode" id="cb227"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data_inc_sri</span><span class="op">$</span><span class="va">income</span><span class="op">[</span><span class="va">data_inc_sri</span><span class="op">$</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1]  -66.055957  -96.980053  -28.921432   -4.175686  -54.480798  -27.207102</span></span>
<span><span class="co">#&gt; [7] -143.603500  -80.960488</span></span>
<span><span class="co"># No values below 0</span></span>
<span><span class="va">data_inc_pmm</span><span class="op">$</span><span class="va">income</span><span class="op">[</span><span class="va">data_inc_pmm</span><span class="op">$</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span> </span>
<span><span class="co">#&gt; numeric(0)</span></span></code></pre></div>
<p>Evidence for heteroskadastic data</p>
<div class="sourceCode" id="cb228"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Heteroscedastic data</span></span>
<span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">654654</span><span class="op">)</span>                             <span class="co"># Set seed</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5000</span>                                  <span class="co"># Sample size</span></span>
<span> </span>
<span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">sigma2</span> <span class="op">&lt;-</span> <span class="va">N</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sigma2</span><span class="op">)</span><span class="op">)</span></span>
<span> </span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">a</span> <span class="op">+</span> <span class="va">b</span> <span class="op">*</span> <span class="va">N</span> <span class="op">+</span> <span class="va">eps</span>                         <span class="co"># Heteroscedastic variable</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">30</span> <span class="op">*</span> <span class="va">N</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">]</span>, <span class="fl">1000</span>, <span class="fl">200</span><span class="op">)</span> <span class="co"># Correlated variable</span></span>
<span> </span>
<span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">]</span>, <span class="fl">1</span>, <span class="fl">0.3</span><span class="op">)</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span>   <span class="co"># 30% missing</span></span>
<span> </span>
<span><span class="va">data_het_miss</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x</span><span class="op">)</span></span></code></pre></div>
<p>Single stochastic regression imputation</p>
<div class="sourceCode" id="cb229"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_het_sri</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_het_miss</span>, method <span class="op">=</span> <span class="st">"norm.nob"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="va">data_het_sri</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_het_sri</span><span class="op">)</span></span></code></pre></div>
<p>Single predictive mean matching</p>
<div class="sourceCode" id="cb230"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_het_pmm</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_het_miss</span>, method <span class="op">=</span> <span class="st">"pmm"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="va">data_het_pmm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_het_pmm</span><span class="op">)</span></span></code></pre></div>
<p>Comparison between predictive mean matching and stochastic regression imputation</p>
<div class="sourceCode" id="cb231"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>                              <span class="co"># Both plots in one graphic</span></span>
<span></span>
<span><span class="co"># Plot of observed values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     <span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     main <span class="op">=</span> <span class="st">""</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"X"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span></span>
<span><span class="co"># Plot of missing values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>, <span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>       col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Title of plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="st">"Stochastic Regression Imputation"</span>,        </span>
<span>      line <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Regression line</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">data_het_sri</span><span class="op">)</span>,                   </span>
<span>       col <span class="op">=</span> <span class="st">"#1b98e0"</span>, lwd <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Legend</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">"topleft"</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observed Values"</span>, <span class="st">"Imputed Values"</span>, <span class="st">"Regression Y ~ X"</span><span class="op">)</span>,</span>
<span>  pch <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>  lty <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="cn">NA</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"#1b98e0"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot of observed values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     <span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     main <span class="op">=</span> <span class="st">""</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"X"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Plot of missing values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>, <span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>       col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Title of plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="st">"Predictive Mean Matching"</span>,</span>
<span>      line <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">data_het_pmm</span><span class="op">)</span>,</span>
<span>       col <span class="op">=</span> <span class="st">"#1b98e0"</span>, lwd <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Legend</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">"topleft"</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observed Values"</span>, <span class="st">"Imputed Values"</span>, <span class="st">"Regression Y ~ X"</span><span class="op">)</span>,</span>
<span>  pch <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>  lty <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="cn">NA</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"#1b98e0"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/mtext.html">mtext</a></span><span class="op">(</span></span>
<span>  <span class="st">"Imputation of Heteroscedastic Data"</span>,</span>
<span>  <span class="co"># Main title of plot</span></span>
<span>  side <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  line <span class="op">=</span> <span class="op">-</span><span class="fl">1.5</span>,</span>
<span>  outer <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  cex <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-11-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="regression-imputation" class="section level4" number="11.2.4.5">
<h4>
<span class="header-section-number">11.2.4.5</span> Regression Imputation<a class="anchor" aria-label="anchor" href="#regression-imputation"><i class="fas fa-link"></i></a>
</h4>
<p>Also known as conditional mean imputation Missing value is based (regress) on other variables.</p>
<ul>
<li>
<p>Good:</p>
<ul>
<li><p>Maintain the relationship with other variables (i.e., preserve dependence structure among features, unlike <a href="imputation-missing-data.html#mean-mode-median-imputation">11.2.4.1</a>).</p></li>
<li>
<p>If the data are MCAR, least-squares coefficients estimates will be consistent, and approximately unbiased in large samples <span class="citation">(<a href="references.html#ref-Gourieroux_1981">Gourieroux and Monfort 1981</a>)</span></p>
<ul>
<li>Can have improvement on efficiency by using weighted least squares <span class="citation">(<a href="references.html#ref-Beale_1975">Beale and Little 1975</a>)</span> or generalized least squares <span class="citation">(<a href="references.html#ref-Gourieroux_1981">Gourieroux and Monfort 1981</a>)</span>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Bad:</p>
<ul>
<li>No variability left. treated data as if they were collected.</li>
<li>Underestimate the standard errors and overestimate test statistics</li>
</ul>
</li>
</ul>
</div>
<div id="interpolation-and-extrapolation" class="section level4" number="11.2.4.6">
<h4>
<span class="header-section-number">11.2.4.6</span> Interpolation and Extrapolation<a class="anchor" aria-label="anchor" href="#interpolation-and-extrapolation"><i class="fas fa-link"></i></a>
</h4>
<p>An estimated value from other observations from the same individual. It usually only works in longitudinal data.</p>
</div>
<div id="k-nearest-neighbor-knn-imputation" class="section level4" number="11.2.4.7">
<h4>
<span class="header-section-number">11.2.4.7</span> K-nearest neighbor (KNN) imputation<a class="anchor" aria-label="anchor" href="#k-nearest-neighbor-knn-imputation"><i class="fas fa-link"></i></a>
</h4>
<p>The above methods are model-based imputation (regression).<br>
This is an example of neighbor-based imputation (K-nearest neighbor).</p>
<p>For every observation that needs to be imputed, the algorithm identifies ‘k’ closest observations based on some types distance (e.g., Euclidean) and computes the weighted average (weighted based on distance) of these ‘k’ obs.</p>
<p>For a discrete variable, it uses the most frequent value among the k nearest neighbors.</p>
<ul>
<li>Distance metrics: Hamming distance.</li>
</ul>
<p>For a continuous variable, it uses the mean or mode.</p>
<ul>
<li>
<p>Distance metrics:</p>
<ul>
<li>Euclidean</li>
<li>Mahalanobis</li>
<li>Manhattan</li>
</ul>
</li>
</ul>
</div>
<div id="bayesian-ridge-regression-implementation" class="section level4" number="11.2.4.8">
<h4>
<span class="header-section-number">11.2.4.8</span> Bayesian Ridge regression implementation<a class="anchor" aria-label="anchor" href="#bayesian-ridge-regression-implementation"><i class="fas fa-link"></i></a>
</h4>
</div>
<div id="matrix-completion" class="section level4" number="11.2.4.9">
<h4>
<span class="header-section-number">11.2.4.9</span> Matrix Completion<a class="anchor" aria-label="anchor" href="#matrix-completion"><i class="fas fa-link"></i></a>
</h4>
<p>Impute items missing at random while accounting for dependence between features by using principal components, which is known as <strong>matrix completion</strong> <span class="citation">(<a href="references.html#ref-james2013">James et al. 2013</a>, Sec 12.3)</span></p>
<p>Consider an <span class="math inline">\(n \times p\)</span> feature matrix, <span class="math inline">\(\mathbf{X}\)</span>, with element <span class="math inline">\(x_{ij}\)</span>, some of which are missing.</p>
<p>Similar to <a href="multivariate-methods.html#principal-components">22.2</a>, we can approximate the matrix <span class="math inline">\(\mathbf{X}\)</span> in terms of its leading PCs.</p>
<p>We consider the <span class="math inline">\(M\)</span> principal components that optimize</p>
<p><span class="math display">\[
\underset{\mathbf{A} \in \mathbb{R}^{n \times M}, \mathbf{B} \in \mathbb{R}^{p \times M}}{\operatorname{min}} \left\{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \right\}
\]</span></p>
<p>where <span class="math inline">\(\mathcal{O}\)</span> is the set of all observed pairs indices <span class="math inline">\((i,j)\)</span>, a subset of the possible <span class="math inline">\(n \times p\)</span> pairs</p>
<p>Once this minimization is solved,</p>
<ul>
<li><p>One can impute a missing observation, <span class="math inline">\(x_{ij}\)</span>, with <span class="math inline">\(\hat{x}_{ij} = \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}\)</span> where <span class="math inline">\(\hat{a}_{im}, \hat{b}_{jm}\)</span> are the <span class="math inline">\((i,m)\)</span> and <span class="math inline">\((j.m)\)</span> elements, respectively, of the matrices <span class="math inline">\(\hat{\mathbf{A}}\)</span> and <span class="math inline">\(\hat{\mathbf{B}}\)</span> from the minimization, and</p></li>
<li><p>One can approximately recover the <span class="math inline">\(M\)</span> principal component scores and loadings, as we did when the data were complete</p></li>
</ul>
<p>The challenge here is to solve this minimization problem: the eigen-decomposition non longer applies (as in <a href="multivariate-methods.html#principal-components">22.2</a></p>
<p>Hence, we have to use iterative algorithm <span class="citation">(<a href="references.html#ref-james2013">James et al. 2013</a> Alg 12.1)</span></p>
<ol style="list-style-type: decimal">
<li>Create a complete data matrix <span class="math inline">\(\tilde{\mathbf{X}}\)</span> of dimension <span class="math inline">\(n \times p\)</span> of which the <span class="math inline">\((i,j)\)</span> element equals</li>
</ol>
<p><span class="math display">\[
\tilde{x}_{ij} =
\begin{cases}
x_{ij} &amp; \text{if } (i,j) \in \mathcal{O} \\
\bar{x}_{j} &amp; \text{if } (i,j) \notin \mathcal{O}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the average of the observed values for the <span class="math inline">\(j\)</span>th variable in the incomplete data matrix <span class="math inline">\(\mathbf{X}\)</span></p>
<p><span class="math inline">\(\mathcal{O}\)</span> indexes the observations that are observed in <span class="math inline">\(\mathbf{X}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Repeat these 3 steps until some objectives are met</li>
</ol>
<p>a. Solve</p>
<p><span class="math display">\[
\underset{\mathbf{A} \in R^{n \times M}, \mathbf{B} \in R^{p \times M}}{\operatorname{min}} \{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \}
\]</span></p>
<p>by computing the principal components of <span class="math inline">\(\tilde{\mathbf{X}}\)</span></p>
<p>b. For each element <span class="math inline">\((i,j) \notin \mathcal{O}\)</span>, set <span class="math inline">\(\tilde{x}_{ij} \leftarrow \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}\)</span></p>
<p>c. Compute the objective</p>
<p><span class="math display">\[
\sum_{(i,j \in \mathcal{O})} (x_{ij} - \sum_{m=1}^M \hat{a}_{im} \hat{b}_{jm})^2
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Return the estimated missing entries <span class="math inline">\(\tilde{x}_{ij}, (i,j) \notin \mathcal{O}\)</span>
</li>
</ol>
</div>
</div>
<div id="other-methods" class="section level3" number="11.2.5">
<h3>
<span class="header-section-number">11.2.5</span> Other methods<a class="anchor" aria-label="anchor" href="#other-methods"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>For panel data, or clustered data, use <code>pan</code> package by Schafer (1997)</li>
</ul>
</div>
</div>
<div id="criteria-for-choosing-an-effective-approach" class="section level2" number="11.3">
<h2>
<span class="header-section-number">11.3</span> Criteria for Choosing an Effective Approach<a class="anchor" aria-label="anchor" href="#criteria-for-choosing-an-effective-approach"><i class="fas fa-link"></i></a>
</h2>
<p>Criteria for an ideal technique in treating missing data:</p>
<ol style="list-style-type: decimal">
<li>Unbiased parameter estimates</li>
<li>Adequate power</li>
<li>Accurate standard errors (p-values, confidence intervals)</li>
</ol>
<p>The Multiple Imputation and Full Information Maximum Likelihood are the the most ideal candidate. Single imputation will generally lead to underestimation of standard errors.</p>
</div>
<div id="another-perspective" class="section level2" number="11.4">
<h2>
<span class="header-section-number">11.4</span> Another Perspective<a class="anchor" aria-label="anchor" href="#another-perspective"><i class="fas fa-link"></i></a>
</h2>
<p>Model bias can arisen from various factors including:</p>
<ul>
<li>Imputation method</li>
<li>Missing data mechanism (<a href="imputation-missing-data.html#missing-completely-at-random-mcar">MCAR</a> vs. <a href="imputation-missing-data.html#missing-at-random-mar">MAR</a>)</li>
<li>Proportion of the missing data</li>
<li>Information available in the data set</li>
</ul>
<p>Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesn’t know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values. So multiple imputation comes up with multiple estimates.</p>
<p>Because multiple imputation have a random component, the multiple estimates are slightly different. This re-introduces some variation that your software can incorporate in order to give your model accurate estimates of standard error. Multiple imputation was a huge breakthrough in statistics about 20 years ago. It solves a lot of problems with missing data (though, unfortunately not all) and if done well, leads to unbiased parameter estimates and accurate standard errors. If your rate of missing data is very, very small (2-3%) it doesn’t matter what technique you use.</p>
<p>Remember that there are three goals of multiple imputation, or any missing data technique:</p>
<ul>
<li>Unbiased parameter estimates in the final analysis (regression coefficients, group means, odds ratios, etc.)</li>
<li>accurate standard errors of those parameter estimates, and therefore, accurate p-values in the analysis</li>
<li>adequate power to find meaningful parameter values significant.</li>
</ul>
<p>Hence,</p>
<ol style="list-style-type: decimal">
<li><p>Don’t round off imputations for dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions for imputing categorical variables were to dummy code them, impute them, then round off imputed values to 0 or 1. Recent research, however, has found that rounding off imputed values actually leads to biased parameter estimates in the analysis model. You actually get better results by leaving the imputed values at impossible values, even though it’s counter-intuitive.</p></li>
<li><p>Don’t transform skewed variables. Likewise, when you transform a variable to meet normality assumptions before imputing, you not only are changing the distribution of that variable but the relationship between that variable and the others you use to impute. Doing so can lead to imputing outliers, creating more bias than just imputing the skewed variable.</p></li>
<li><p>Use more imputations. The advice for years has been that 5-10 imputations are adequate. And while this is true for unbiasedness, you can get inconsistent results if you run the multiple imputation more than once. <span class="citation">(<a href="references.html#ref-Bodner_2008">Bodner 2008</a>)</span> recommends having as many imputations as the percentage of missing data. Since running more imputations isn’t any more work for the data analyst, there’s no reason not to.</p></li>
<li><p>Create multiplicative terms before imputing. When the analysis model contains a multiplicative term, like an interaction term or a quadratic, create the multiplicative terms first, then impute. Imputing first, and then creating the multiplicative terms actually biases the regression parameters of the multiplicative term <span class="citation">(<a href="references.html#ref-von_Hippel_2009">Von Hippel 2009</a>)</span></p></li>
</ol>
</div>
<div id="diagnosing-the-mechanism" class="section level2" number="11.5">
<h2>
<span class="header-section-number">11.5</span> Diagnosing the Mechanism<a class="anchor" aria-label="anchor" href="#diagnosing-the-mechanism"><i class="fas fa-link"></i></a>
</h2>
<div id="mar-vs.-mnar" class="section level3" number="11.5.1">
<h3>
<span class="header-section-number">11.5.1</span> MAR vs. MNAR<a class="anchor" aria-label="anchor" href="#mar-vs.-mnar"><i class="fas fa-link"></i></a>
</h3>
<p>The only true way to distinguish between MNAR and MAR is to measure some of that missing data.</p>
<p>It’s a common practice among professional surveyors to, for example, follow-up on a paper survey with phone calls to a group of the non-respondents and ask a few key survey items. This allows you to compare respondents to non-respondents.</p>
<p>If their responses on those key items differ by very much, that’s good evidence that the data are MNAR.</p>
<p>However in most missing data situations, we can’t get a hold of the missing data. So while we can’t test it directly, we can examine patterns in the data get an idea of what’s the most likely mechanism.</p>
<p>The first thing in diagnosing randomness of the missing data is to use your substantive scientific knowledge of the data and your field. The more sensitive the issue, the less likely people are to tell you. They’re not going to tell you as much about their cocaine usage as they are about their phone usage.</p>
<p>Likewise, many fields have common research situations in which non-ignorable data is common. Educate yourself in your field’s literature.</p>
</div>
<div id="mcar-vs.-mar" class="section level3" number="11.5.2">
<h3>
<span class="header-section-number">11.5.2</span> MCAR vs. MAR<a class="anchor" aria-label="anchor" href="#mcar-vs.-mar"><i class="fas fa-link"></i></a>
</h3>
<p>There is a very useful test for MCAR, Little’s test.</p>
<p>A second technique is to create dummy variables for whether a variable is missing.</p>
<p>1 = missing 0 = observed</p>
<p>You can then run t-tests and chi-square tests between this variable and other variables in the data set to see if the missingness on this variable is related to the values of other variables.</p>
<p>For example, if women really are less likely to tell you their weight than men, a chi-square test will tell you that the percentage of missing data on the weight variable is higher for women than men.</p>
</div>
</div>
<div id="application-7" class="section level2" number="11.6">
<h2>
<span class="header-section-number">11.6</span> Application<a class="anchor" aria-label="anchor" href="#application-7"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb232"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/visdat/">visdat</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/njtierney/naniar">naniar</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://docs.ropensci.org/visdat/reference/vis_miss.html">vis_miss</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/naniar/man/geom_miss_point.html">geom_miss_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span> <span class="op">~</span> <span class="va">group</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/gg_miss_var.html">gg_miss_var</a></span><span class="op">(</span><span class="va">data</span>, facet <span class="op">=</span> <span class="va">group</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/gg_miss_upset.html">gg_miss_upset</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/gg_miss_fct.html">gg_miss_fct</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable1</span>, fct <span class="op">=</span> <span class="va">variable2</span><span class="op">)</span></span></code></pre></div>
<p>Read more on <a href="https://tmb.njtierney.com/">The Missing Book by Nicholas Tierney &amp; Allison Horst</a></p>
<p>How many imputation:</p>
<p><strong>Usually 5</strong>. (unless you have extremely high portion of missing, in which case you probably need to check your data again)</p>
<p>According to Rubin, the relative efficiency of an estimate based on <span class="math inline">\(m\)</span> imputations to infinity imputation is approximately</p>
<p><span class="math display">\[
(1+\frac{\lambda}{m})^{-1}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the rate of missing data</p>
<p>Example 50% of missing data means an estimate based on 5 imputation has standard deviation that is only 5% wider compared to an estimate based on infinity imputation<br>
(<span class="math inline">\(\sqrt{1+0.5/5}=1.049\)</span>)</p>
<div class="sourceCode" id="cb233"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org">missForest</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">#load data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">iris</span></span>
<span></span>
<span><span class="co">#Generate 10% missing values at Random</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris.mis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/prodNA.html">prodNA</a></span><span class="op">(</span><span class="va">iris</span>, noNA <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#remove categorical variables</span></span>
<span><span class="va">iris.mis.cat</span> <span class="op">&lt;-</span> <span class="va">iris.mis</span></span>
<span><span class="va">iris.mis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">iris.mis</span>, select <span class="op">=</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">Species</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div id="imputation-with-mean-median-mode" class="section level3" number="11.6.1">
<h3>
<span class="header-section-number">11.6.1</span> Imputation with mean / median / mode<a class="anchor" aria-label="anchor" href="#imputation-with-mean-median-mode"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb234"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># whole data set</span></span>
<span><span class="fu">e1071</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span>, what <span class="op">=</span> <span class="st">"mean"</span><span class="op">)</span>        <span class="co"># replace with mean</span></span>
<span><span class="fu">e1071</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span>, what <span class="op">=</span> <span class="st">"median"</span><span class="op">)</span>      <span class="co"># replace with median</span></span>
<span></span>
<span><span class="co"># by variables</span></span>
<span><span class="fu">Hmisc</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Length</span>, <span class="va">mean</span><span class="op">)</span>    <span class="co"># mean</span></span>
<span><span class="fu">Hmisc</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Length</span>, <span class="va">median</span><span class="op">)</span>  <span class="co"># median</span></span>
<span><span class="fu">Hmisc</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Length</span>, <span class="fl">0</span><span class="op">)</span>       <span class="co"># replace specific number</span></span></code></pre></div>
<p>check accuracy</p>
<div class="sourceCode" id="cb235"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># library(DMwR2)</span></span>
<span><span class="co"># actuals &lt;- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]</span></span>
<span><span class="co"># predicteds &lt;- rep(mean(iris$Sepal.Width, na.rm=T), length(actuals))</span></span>
<span><span class="co"># regr.eval(actuals, predicteds)</span></span></code></pre></div>
</div>
<div id="knn" class="section level3" number="11.6.2">
<h3>
<span class="header-section-number">11.6.2</span> KNN<a class="anchor" aria-label="anchor" href="#knn"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb236"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># library(DMwR2)</span></span>
<span><span class="co"># # iris.mis[,!names(iris.mis) %in% c("Sepal.Length")]</span></span>
<span><span class="co"># # data should be this line. But since knn cant work with 3 or less variables, </span></span>
<span><span class="co"># # we need to use at least 4 variables.</span></span>
<span><span class="co"># </span></span>
<span><span class="co"># # knn is not appropriate for categorical variables</span></span>
<span><span class="co"># knnOutput &lt;-</span></span>
<span><span class="co">#   knnImputation(data = iris.mis.cat,</span></span>
<span><span class="co">#                 #k = 10,</span></span>
<span><span class="co">#                 meth = "median" # could use "median" or "weighAvg")  </span></span>
<span><span class="co">#                 # should exclude the dependent variable: Sepal.Length</span></span>
<span><span class="co">#                 anyNA(knnOutput)</span></span></code></pre></div>
<div class="sourceCode" id="cb237"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># library(DMwR2)</span></span>
<span><span class="co"># actuals &lt;- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]</span></span>
<span><span class="co"># predicteds &lt;- knnOutput[is.na(iris.mis$Sepal.Width), "Sepal.Width"]</span></span>
<span><span class="co"># regr.eval(actuals, predicteds)</span></span></code></pre></div>
<p>Compared to MAPE (mean absolute percentage error) of mean imputation, we see almost always see improvements.</p>
</div>
<div id="rpart" class="section level3" number="11.6.3">
<h3>
<span class="header-section-number">11.6.3</span> rpart<a class="anchor" aria-label="anchor" href="#rpart"><i class="fas fa-link"></i></a>
</h3>
<p>For categorical (factor) variables, <code>rpart</code> can handle</p>
<div class="sourceCode" id="cb238"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bethatkinson/rpart">rpart</a></span><span class="op">)</span></span>
<span><span class="va">class_mod</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span></span>
<span>    <span class="va">Species</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">Sepal.Length</span>,</span>
<span>    data <span class="op">=</span> <span class="va">iris.mis.cat</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis.cat</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span>,<span class="op">]</span>,</span>
<span>    method <span class="op">=</span> <span class="st">"class"</span>,</span>
<span>    na.action <span class="op">=</span> <span class="va">na.omit</span></span>
<span>  <span class="op">)</span>  <span class="co"># since Species is a factor, and exclude dependent variable "Sepal.Length"</span></span>
<span></span>
<span><span class="va">anova_mod</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span></span>
<span>    <span class="va">Sepal.Width</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">Sepal.Length</span>,</span>
<span>    data <span class="op">=</span> <span class="va">iris.mis</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">)</span>,<span class="op">]</span>,</span>
<span>    method <span class="op">=</span> <span class="st">"anova"</span>,</span>
<span>    na.action <span class="op">=</span> <span class="va">na.omit</span></span>
<span>  <span class="op">)</span>  <span class="co"># since Sepal.Width is numeric.</span></span>
<span></span>
<span></span>
<span><span class="va">species_pred</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">class_mod</span>, <span class="va">iris.mis.cat</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis.cat</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span>,<span class="op">]</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">width_pred</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">anova_mod</span>, <span class="va">iris.mis</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">)</span>,<span class="op">]</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="mice-multivariate-imputation-via-chained-equations" class="section level3" number="11.6.4">
<h3>
<span class="header-section-number">11.6.4</span> MICE (Multivariate Imputation via Chained Equations)<a class="anchor" aria-label="anchor" href="#mice-multivariate-imputation-via-chained-equations"><i class="fas fa-link"></i></a>
</h3>
<p>Assumption: data are <a href="imputation-missing-data.html#missing-at-random-mar">MAR</a></p>
<p>It imputes data per variable by specifying an imputation model for each variable</p>
<p><strong>Example</strong></p>
<p>We have <span class="math inline">\(X_1, X_2,..,X_k\)</span>. If <span class="math inline">\(X_1\)</span> has missing data, then it is regressed on the rest of the variables. Same procedure applies if <span class="math inline">\(X_2\)</span> has missing data. Then, predicted values are used in place of missing values.</p>
<p>By default,</p>
<ul>
<li>
<strong>Continuous variables</strong> use linear regression.</li>
<li>
<strong>Categorical Variables</strong> use logistic regression.</li>
</ul>
<p>Methods in <a href="imputation-missing-data.html#mice-multivariate-imputation-via-chained-equations">MICE</a>:</p>
<ul>
<li>PMM (Predictive Mean Matching) – For numeric variables</li>
<li>
<code>logreg</code>(Logistic Regression) – For Binary Variables( with 2 levels)</li>
<li>
<code>polyreg</code> (Bayesian polytomous regression) – For Factor Variables (&gt;= 2 levels)</li>
<li>Proportional odds model (ordered, &gt;= 2 levels)</li>
</ul>
<div class="sourceCode" id="cb239"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/amices/mice">mice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/statistikat/VIM">VIM</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># check missing values</span></span>
<span><span class="fu"><a href="https://amices.org/mice/reference/md.pattern.html">md.pattern</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-19-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt;     Sepal.Width Sepal.Length Petal.Length Petal.Width   
#&gt; 100           1            1            1           1  0
#&gt; 15            1            1            1           0  1
#&gt; 8             1            1            0           1  1
#&gt; 2             1            1            0           0  2
#&gt; 11            1            0            1           1  1
#&gt; 1             1            0            1           0  2
#&gt; 1             1            0            0           1  2
#&gt; 1             1            0            0           0  3
#&gt; 7             0            1            1           1  1
#&gt; 3             0            1            0           1  2
#&gt; 1             0            0            1           1  2
#&gt;              11           15           15          19 60

#plot the missing values
aggr(
  iris.mis,
  col = mdc(1:2),
  numbers = TRUE,
  sortVars = TRUE,
  labels = names(iris.mis),
  cex.axis = .7,
  gap = 3,
  ylab = c("Proportion of missingness", "Missingness Pattern")
)</code></pre>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-19-2.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; 
#&gt;  Variables sorted by number of missings: 
#&gt;      Variable      Count
#&gt;   Petal.Width 0.12666667
#&gt;  Sepal.Length 0.10000000
#&gt;  Petal.Length 0.10000000
#&gt;   Sepal.Width 0.07333333


mice_plot &lt;- aggr(
  iris.mis,
  col = c('navyblue', 'yellow'),
  numbers = TRUE,
  sortVars = TRUE,
  labels = names(iris.mis),
  cex.axis = .7,
  gap = 3,
  ylab = c("Missing data", "Pattern")
)</code></pre>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-19-3.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; 
#&gt;  Variables sorted by number of missings: 
#&gt;      Variable      Count
#&gt;   Petal.Width 0.12666667
#&gt;  Sepal.Length 0.10000000
#&gt;  Petal.Length 0.10000000
#&gt;   Sepal.Width 0.07333333</code></pre>
<p>Impute Data</p>
<div class="sourceCode" id="cb243"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imputed_Data</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span></span>
<span>        <span class="va">iris.mis</span>,</span>
<span>        m <span class="op">=</span> <span class="fl">5</span>, <span class="co"># number of imputed datasets</span></span>
<span>        maxit <span class="op">=</span> <span class="fl">50</span>, <span class="co"># number of iterations taken to impute missing values</span></span>
<span>        method <span class="op">=</span> <span class="st">'pmm'</span>, <span class="co"># method used in imputation. </span></span>
<span>        <span class="co"># Here, we used predictive mean matching</span></span>
<span>        </span>
<span>        </span>
<span>        <span class="co"># other methods can be </span></span>
<span>        <span class="co"># "pmm": Predictive mean matching</span></span>
<span>        <span class="co"># "midastouch" : weighted predictive mean matching</span></span>
<span>        <span class="co"># "sample": Random sample from observed values</span></span>
<span>        <span class="co"># "cart": classification and regression trees</span></span>
<span>        <span class="co"># "rf": random forest imputations.</span></span>
<span>        <span class="co"># "2lonly.pmm": Level-2 class predictive mean matching</span></span>
<span>        <span class="co"># Other methods based on whether variables are </span></span>
<span>        <span class="co"># (1) numeric, (2) binary, (3) ordered, (4), unordered</span></span>
<span>        seed <span class="op">=</span> <span class="fl">500</span></span>
<span>    <span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb244"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">imputed_Data</span><span class="op">)</span></span>
<span><span class="co">#&gt; Class: mids</span></span>
<span><span class="co">#&gt; Number of multiple imputations:  5 </span></span>
<span><span class="co">#&gt; Imputation methods:</span></span>
<span><span class="co">#&gt; Sepal.Length  Sepal.Width Petal.Length  Petal.Width </span></span>
<span><span class="co">#&gt;        "pmm"        "pmm"        "pmm"        "pmm" </span></span>
<span><span class="co">#&gt; PredictorMatrix:</span></span>
<span><span class="co">#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width</span></span>
<span><span class="co">#&gt; Sepal.Length            0           1            1           1</span></span>
<span><span class="co">#&gt; Sepal.Width             1           0            1           1</span></span>
<span><span class="co">#&gt; Petal.Length            1           1            0           1</span></span>
<span><span class="co">#&gt; Petal.Width             1           1            1           0</span></span>
<span></span>
<span><span class="co">#make a density plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lattice/man/histogram.html">densityplot</a></span><span class="op">(</span><span class="va">imputed_Data</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-21-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb245"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#the red (imputed values) should be similar to the blue (observed)</span></span></code></pre></div>
<p>Check imputed dataset</p>
<div class="sourceCode" id="cb246"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 1st dataset </span></span>
<span><span class="va">completeData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imputed_Data</span>,<span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 2nd dataset</span></span>
<span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imputed_Data</span>,<span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p>Regression model using imputed datasets</p>
<div class="sourceCode" id="cb247"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># regression model</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">imputed_Data</span>,</span>
<span>       exp <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sepal.Width</span> <span class="op">~</span> <span class="va">Sepal.Length</span> <span class="op">+</span> <span class="va">Petal.Width</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#combine results of all 5 models</span></span>
<span><span class="va">combine</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/pool.html">pool</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">combine</span><span class="op">)</span></span>
<span><span class="co">#&gt;           term   estimate  std.error statistic       df      p.value</span></span>
<span><span class="co">#&gt; 1  (Intercept)  1.8963130 0.32453912  5.843095 131.0856 3.838556e-08</span></span>
<span><span class="co">#&gt; 2 Sepal.Length  0.2974293 0.06679204  4.453066 130.2103 1.802241e-05</span></span>
<span><span class="co">#&gt; 3  Petal.Width -0.4811603 0.07376809 -6.522608 108.8253 2.243032e-09</span></span></code></pre></div>
</div>
<div id="amelia" class="section level3" number="11.6.5">
<h3>
<span class="header-section-number">11.6.5</span> Amelia<a class="anchor" aria-label="anchor" href="#amelia"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Use bootstrap based EMB algorithm (faster and robust to impute many variables including cross sectional, time series data etc)</li>
<li>Use parallel imputation feature using multicore CPUs.</li>
</ul>
<p>Assumptions</p>
<ul>
<li>All variables follow Multivariate Normal Distribution (MVN). Hence, this package works best when data is MVN, or transformation to normality.</li>
<li>Missing data is <a href="imputation-missing-data.html#missing-at-random-mar">Missing at Random (MAR)</a>
</li>
</ul>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>m bootstrap samples and applies EMB algorithm to each sample. Then we have m different estimates of mean and variances.</li>
<li>the first set of estimates are used to impute first set of missing values using regression, then second set of estimates are used for second set and so on.</li>
</ol>
<p>However, <a href="imputation-missing-data.html#amelia">Amelia</a> is different from <a href="imputation-missing-data.html#mice-multivariate-imputation-via-chained-equations">MICE</a></p>
<ul>
<li>MICE imputes data on variable by variable basis whereas MVN uses a joint modeling approach based on multivariate normal distribution.</li>
<li>MICE can handle different types of variables while the variables in MVN need to be normally distributed or transformed to approximate normality.</li>
<li>MICE can manage imputation of variables defined on a subset of data whereas MVN cannot.</li>
</ul>
<div class="sourceCode" id="cb248"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://gking.harvard.edu/amelia">Amelia</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"iris"</span><span class="op">)</span></span>
<span><span class="co">#seed 10% missing values</span></span>
<span><span class="va">iris.mis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/prodNA.html">prodNA</a></span><span class="op">(</span><span class="va">iris</span>, noNA <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>idvars – keep all ID variables and other variables which you don’t want to impute</li>
<li>noms – keep nominal variables here</li>
</ul>
<div class="sourceCode" id="cb249"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#specify columns and run amelia</span></span>
<span><span class="va">amelia_fit</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/Amelia/man/amelia.html">amelia</a></span><span class="op">(</span><span class="va">iris.mis</span>,</span>
<span>         m <span class="op">=</span> <span class="fl">5</span>,</span>
<span>         parallel <span class="op">=</span> <span class="st">"multicore"</span>,</span>
<span>         noms <span class="op">=</span> <span class="st">"Species"</span><span class="op">)</span></span>
<span><span class="co">#&gt; -- Imputation 1 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5  6  7  8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 2 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5  6  7  8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 3 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 4 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5  6  7</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 5 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5  6  7</span></span>
<span></span>
<span><span class="co"># access imputed outputs</span></span>
<span><span class="co"># amelia_fit$imputations[[1]]</span></span></code></pre></div>
</div>
<div id="missforest" class="section level3" number="11.6.6">
<h3>
<span class="header-section-number">11.6.6</span> missForest<a class="anchor" aria-label="anchor" href="#missforest"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>an implementation of random forest algorithm (a non parametric imputation method applicable to various variable types). Hence, no assumption about function form of f. Instead, it tries to estimate f such that it can be as close to the data points as possible.</li>
<li>builds a random forest model for each variable. Then it uses the model to predict missing values in the variable with the help of observed values.</li>
<li>It yields out of bag imputation error estimate. Moreover, it provides high level of control on imputation process.</li>
<li>Since bagging works well on categorical variable too, we don’t need to remove them here.</li>
</ul>
<div class="sourceCode" id="cb250"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org">missForest</a></span><span class="op">)</span></span>
<span><span class="co">#impute missing values, using all parameters as default values</span></span>
<span><span class="va">iris.imp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/missForest.html">missForest</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">)</span></span>
<span><span class="co"># check imputed values</span></span>
<span><span class="co"># iris.imp$ximp</span></span></code></pre></div>
<ul>
<li>check imputation error</li>
<li>NRMSE is normalized mean squared error. It is used to represent error derived from imputing continuous values.</li>
<li>PFC (proportion of falsely classified) is used to represent error derived from imputing categorical values.</li>
</ul>
<div class="sourceCode" id="cb251"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">iris.imp</span><span class="op">$</span><span class="va">OOBerror</span></span>
<span><span class="co">#&gt;      NRMSE        PFC </span></span>
<span><span class="co">#&gt; 0.13631893 0.04477612</span></span>
<span></span>
<span><span class="co">#comparing actual data accuracy</span></span>
<span><span class="va">iris.err</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/mixError.html">mixError</a></span><span class="op">(</span><span class="va">iris.imp</span><span class="op">$</span><span class="va">ximp</span>, <span class="va">iris.mis</span>, <span class="va">iris</span><span class="op">)</span></span>
<span><span class="va">iris.err</span></span>
<span><span class="co">#&gt;     NRMSE       PFC </span></span>
<span><span class="co">#&gt; 0.1501524 0.0625000</span></span></code></pre></div>
<p>This means categorical variables are imputed with 5% error and continuous variables are imputed with 14% error.</p>
<p>This can be improved by tuning the values of <code>mtry</code> and <code>ntree</code> parameter.</p>
<ul>
<li>
<code>mtry</code> refers to the number of variables being randomly sampled at each split.</li>
<li>
<code>ntree</code> refers to number of trees to grow in the forest.</li>
</ul>
</div>
<div id="hmisc" class="section level3" number="11.6.7">
<h3>
<span class="header-section-number">11.6.7</span> Hmisc<a class="anchor" aria-label="anchor" href="#hmisc"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<code><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute()</a></code> function imputes missing value using user defined statistical method (mean, max, mean). It’s default is median.<br>
</li>
<li>
<code><a href="https://rdrr.io/pkg/Hmisc/man/aregImpute.html">aregImpute()</a></code> allows mean imputation using additive regression, bootstrapping, and predictive mean matching.</li>
</ul>
<ol style="list-style-type: decimal">
<li>In bootstrapping, different bootstrap resamples are used for each of multiple imputations. Then, a flexible additive model (non parametric regression method) is fitted on samples taken with replacements from original data and missing values (acts as dependent variable) are predicted using non-missing values (independent variable).<br>
</li>
<li>it uses predictive mean matching (default) to impute missing values. Predictive mean matching works well for continuous and categorical (binary &amp; multi-level) without the need for computing residuals and maximum likelihood fit.</li>
</ol>
<p><strong>Note</strong></p>
<ul>
<li>For predicting categorical variables, Fisher’s optimum scoring method is used.</li>
<li>
<code>Hmisc</code> automatically recognizes the variables types and uses bootstrap sample and predictive mean matching to impute missing values.</li>
<li>
<code>missForest</code> can outperform <code>Hmisc</code> if the observed variables have sufficient information.</li>
</ul>
<p>Assumption</p>
<ul>
<li>linearity in the variables being predicted.</li>
</ul>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="imputation-missing-data.html#cb252-1" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb252-2"><a href="imputation-missing-data.html#cb252-2" tabindex="-1"></a><span class="co"># impute with mean value</span></span>
<span id="cb252-3"><a href="imputation-missing-data.html#cb252-3" tabindex="-1"></a>iris.mis<span class="sc">$</span>imputed_age <span class="ot">&lt;-</span> <span class="fu">with</span>(iris.mis, <span class="fu">impute</span>(Sepal.Length, mean))</span>
<span id="cb252-4"><a href="imputation-missing-data.html#cb252-4" tabindex="-1"></a></span>
<span id="cb252-5"><a href="imputation-missing-data.html#cb252-5" tabindex="-1"></a><span class="co"># impute with random value</span></span>
<span id="cb252-6"><a href="imputation-missing-data.html#cb252-6" tabindex="-1"></a>iris.mis<span class="sc">$</span>imputed_age2 <span class="ot">&lt;-</span></span>
<span id="cb252-7"><a href="imputation-missing-data.html#cb252-7" tabindex="-1"></a>  <span class="fu">with</span>(iris.mis, <span class="fu">impute</span>(Sepal.Length, <span class="st">'random'</span>))</span>
<span id="cb252-8"><a href="imputation-missing-data.html#cb252-8" tabindex="-1"></a></span>
<span id="cb252-9"><a href="imputation-missing-data.html#cb252-9" tabindex="-1"></a><span class="co"># could also use min, max, median to impute missing value</span></span>
<span id="cb252-10"><a href="imputation-missing-data.html#cb252-10" tabindex="-1"></a></span>
<span id="cb252-11"><a href="imputation-missing-data.html#cb252-11" tabindex="-1"></a><span class="co"># using argImpute</span></span>
<span id="cb252-12"><a href="imputation-missing-data.html#cb252-12" tabindex="-1"></a>impute_arg <span class="ot">&lt;-</span></span>
<span id="cb252-13"><a href="imputation-missing-data.html#cb252-13" tabindex="-1"></a>  <span class="co"># argImpute() automatically identifies the variable type </span></span>
<span id="cb252-14"><a href="imputation-missing-data.html#cb252-14" tabindex="-1"></a>  <span class="co"># and treats them accordingly.</span></span>
<span id="cb252-15"><a href="imputation-missing-data.html#cb252-15" tabindex="-1"></a>  <span class="fu">aregImpute</span>(</span>
<span id="cb252-16"><a href="imputation-missing-data.html#cb252-16" tabindex="-1"></a>    <span class="sc">~</span> Sepal.Length <span class="sc">+</span> Sepal.Width <span class="sc">+</span> Petal.Length <span class="sc">+</span> Petal.Width <span class="sc">+</span></span>
<span id="cb252-17"><a href="imputation-missing-data.html#cb252-17" tabindex="-1"></a>      Species,</span>
<span id="cb252-18"><a href="imputation-missing-data.html#cb252-18" tabindex="-1"></a>    <span class="at">data =</span> iris.mis,</span>
<span id="cb252-19"><a href="imputation-missing-data.html#cb252-19" tabindex="-1"></a>    <span class="at">n.impute =</span> <span class="dv">5</span></span>
<span id="cb252-20"><a href="imputation-missing-data.html#cb252-20" tabindex="-1"></a>  ) </span>
<span id="cb252-21"><a href="imputation-missing-data.html#cb252-21" tabindex="-1"></a><span class="co">#&gt; Iteration 1 </span></span>
<span id="cb252-22"><a href="imputation-missing-data.html#cb252-22" tabindex="-1"></a>Iteration <span class="dv">2</span> </span>
<span id="cb252-23"><a href="imputation-missing-data.html#cb252-23" tabindex="-1"></a>Iteration <span class="dv">3</span> </span>
<span id="cb252-24"><a href="imputation-missing-data.html#cb252-24" tabindex="-1"></a>Iteration <span class="dv">4</span> </span>
<span id="cb252-25"><a href="imputation-missing-data.html#cb252-25" tabindex="-1"></a>Iteration <span class="dv">5</span> </span>
<span id="cb252-26"><a href="imputation-missing-data.html#cb252-26" tabindex="-1"></a>Iteration <span class="dv">6</span> </span>
<span id="cb252-27"><a href="imputation-missing-data.html#cb252-27" tabindex="-1"></a>Iteration <span class="dv">7</span> </span>
<span id="cb252-28"><a href="imputation-missing-data.html#cb252-28" tabindex="-1"></a>Iteration <span class="dv">8</span> </span>
<span id="cb252-29"><a href="imputation-missing-data.html#cb252-29" tabindex="-1"></a></span>
<span id="cb252-30"><a href="imputation-missing-data.html#cb252-30" tabindex="-1"></a>impute_arg <span class="co"># R-squares are for predicted missing values.</span></span>
<span id="cb252-31"><a href="imputation-missing-data.html#cb252-31" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb252-32"><a href="imputation-missing-data.html#cb252-32" tabindex="-1"></a><span class="co">#&gt; Multiple Imputation using Bootstrap and PMM</span></span>
<span id="cb252-33"><a href="imputation-missing-data.html#cb252-33" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb252-34"><a href="imputation-missing-data.html#cb252-34" tabindex="-1"></a><span class="co">#&gt; aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + </span></span>
<span id="cb252-35"><a href="imputation-missing-data.html#cb252-35" tabindex="-1"></a><span class="co">#&gt;     Petal.Width + Species, data = iris.mis, n.impute = 5)</span></span>
<span id="cb252-36"><a href="imputation-missing-data.html#cb252-36" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb252-37"><a href="imputation-missing-data.html#cb252-37" tabindex="-1"></a><span class="co">#&gt; n: 150   p: 5    Imputations: 5      nk: 3 </span></span>
<span id="cb252-38"><a href="imputation-missing-data.html#cb252-38" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb252-39"><a href="imputation-missing-data.html#cb252-39" tabindex="-1"></a><span class="co">#&gt; Number of NAs:</span></span>
<span id="cb252-40"><a href="imputation-missing-data.html#cb252-40" tabindex="-1"></a><span class="co">#&gt; Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species </span></span>
<span id="cb252-41"><a href="imputation-missing-data.html#cb252-41" tabindex="-1"></a><span class="co">#&gt;           11           11           13           24           16 </span></span>
<span id="cb252-42"><a href="imputation-missing-data.html#cb252-42" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb252-43"><a href="imputation-missing-data.html#cb252-43" tabindex="-1"></a><span class="co">#&gt;              type d.f.</span></span>
<span id="cb252-44"><a href="imputation-missing-data.html#cb252-44" tabindex="-1"></a><span class="co">#&gt; Sepal.Length    s    2</span></span>
<span id="cb252-45"><a href="imputation-missing-data.html#cb252-45" tabindex="-1"></a><span class="co">#&gt; Sepal.Width     s    2</span></span>
<span id="cb252-46"><a href="imputation-missing-data.html#cb252-46" tabindex="-1"></a><span class="co">#&gt; Petal.Length    s    2</span></span>
<span id="cb252-47"><a href="imputation-missing-data.html#cb252-47" tabindex="-1"></a><span class="co">#&gt; Petal.Width     s    2</span></span>
<span id="cb252-48"><a href="imputation-missing-data.html#cb252-48" tabindex="-1"></a><span class="co">#&gt; Species         c    2</span></span>
<span id="cb252-49"><a href="imputation-missing-data.html#cb252-49" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb252-50"><a href="imputation-missing-data.html#cb252-50" tabindex="-1"></a><span class="co">#&gt; Transformation of Target Variables Forced to be Linear</span></span>
<span id="cb252-51"><a href="imputation-missing-data.html#cb252-51" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb252-52"><a href="imputation-missing-data.html#cb252-52" tabindex="-1"></a><span class="co">#&gt; R-squares for Predicting Non-Missing Values for Each Variable</span></span>
<span id="cb252-53"><a href="imputation-missing-data.html#cb252-53" tabindex="-1"></a><span class="co">#&gt; Using Last Imputations of Predictors</span></span>
<span id="cb252-54"><a href="imputation-missing-data.html#cb252-54" tabindex="-1"></a><span class="co">#&gt; Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species </span></span>
<span id="cb252-55"><a href="imputation-missing-data.html#cb252-55" tabindex="-1"></a><span class="co">#&gt;        0.907        0.660        0.978        0.963        0.993</span></span>
<span id="cb252-56"><a href="imputation-missing-data.html#cb252-56" tabindex="-1"></a></span>
<span id="cb252-57"><a href="imputation-missing-data.html#cb252-57" tabindex="-1"></a><span class="co"># check imputed variable Sepal.Length</span></span>
<span id="cb252-58"><a href="imputation-missing-data.html#cb252-58" tabindex="-1"></a>impute_arg<span class="sc">$</span>imputed<span class="sc">$</span>Sepal.Length</span>
<span id="cb252-59"><a href="imputation-missing-data.html#cb252-59" tabindex="-1"></a><span class="co">#&gt;     [,1] [,2] [,3] [,4] [,5]</span></span>
<span id="cb252-60"><a href="imputation-missing-data.html#cb252-60" tabindex="-1"></a><span class="co">#&gt; 19   5.2  5.2  5.2  5.8  5.7</span></span>
<span id="cb252-61"><a href="imputation-missing-data.html#cb252-61" tabindex="-1"></a><span class="co">#&gt; 21   5.1  5.0  5.1  5.7  5.4</span></span>
<span id="cb252-62"><a href="imputation-missing-data.html#cb252-62" tabindex="-1"></a><span class="co">#&gt; 31   4.8  5.0  5.2  5.0  4.8</span></span>
<span id="cb252-63"><a href="imputation-missing-data.html#cb252-63" tabindex="-1"></a><span class="co">#&gt; 35   4.6  4.9  4.9  4.9  4.8</span></span>
<span id="cb252-64"><a href="imputation-missing-data.html#cb252-64" tabindex="-1"></a><span class="co">#&gt; 49   5.0  5.1  5.1  5.1  5.1</span></span>
<span id="cb252-65"><a href="imputation-missing-data.html#cb252-65" tabindex="-1"></a><span class="co">#&gt; 62   6.2  5.7  6.0  6.4  5.6</span></span>
<span id="cb252-66"><a href="imputation-missing-data.html#cb252-66" tabindex="-1"></a><span class="co">#&gt; 65   5.5  5.5  5.2  5.8  5.5</span></span>
<span id="cb252-67"><a href="imputation-missing-data.html#cb252-67" tabindex="-1"></a><span class="co">#&gt; 67   6.5  5.8  5.8  6.3  6.5</span></span>
<span id="cb252-68"><a href="imputation-missing-data.html#cb252-68" tabindex="-1"></a><span class="co">#&gt; 82   5.2  5.1  5.7  5.8  5.5</span></span>
<span id="cb252-69"><a href="imputation-missing-data.html#cb252-69" tabindex="-1"></a><span class="co">#&gt; 113  6.4  6.5  7.4  7.2  6.3</span></span>
<span id="cb252-70"><a href="imputation-missing-data.html#cb252-70" tabindex="-1"></a><span class="co">#&gt; 122  6.2  5.8  5.5  5.8  6.7</span></span></code></pre></div>
</div>
<div id="mi" class="section level3" number="11.6.8">
<h3>
<span class="header-section-number">11.6.8</span> mi<a class="anchor" aria-label="anchor" href="#mi"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>allows graphical diagnostics of imputation models and convergence of imputation process.</li>
<li>uses Bayesian version of regression models to handle issue of separation.</li>
<li>automatically detects irregularities in data (e.g., high collinearity among variables).</li>
<li>adds noise to imputation process to solve the problem of additive constraints.</li>
</ol>
<div class="sourceCode" id="cb253"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stat.columbia.edu/~gelman/">mi</a></span><span class="op">)</span></span>
<span><span class="co"># default values of parameters</span></span>
<span><span class="co"># 1. rand.imp.method as “bootstrap”</span></span>
<span><span class="co"># 2. n.imp (number of multiple imputations) as 3</span></span>
<span><span class="co"># 3. n.iter ( number of iterations) as 30</span></span>
<span><span class="va">mi_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mi/man/04mi.html">mi</a></span><span class="op">(</span><span class="va">iris.mis</span>, seed <span class="op">=</span> <span class="fl">335</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mi_data</span><span class="op">)</span></span></code></pre></div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></div>
<div class="next"><a href="data.html"><span class="header-section-number">12</span> Data</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#imputation-missing-data"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li>
<a class="nav-link" href="#assumptions-1"><span class="header-section-number">11.1</span> Assumptions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#missing-completely-at-random-mcar"><span class="header-section-number">11.1.1</span> Missing Completely at Random (MCAR)</a></li>
<li><a class="nav-link" href="#missing-at-random-mar"><span class="header-section-number">11.1.2</span> Missing at Random (MAR)</a></li>
<li><a class="nav-link" href="#ignorable"><span class="header-section-number">11.1.3</span> Ignorable</a></li>
<li><a class="nav-link" href="#nonignorable"><span class="header-section-number">11.1.4</span> Nonignorable</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#solutions-to-missing-data"><span class="header-section-number">11.2</span> Solutions to Missing data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#listwise-deletion"><span class="header-section-number">11.2.1</span> Listwise Deletion</a></li>
<li><a class="nav-link" href="#pairwise-deletion"><span class="header-section-number">11.2.2</span> Pairwise Deletion</a></li>
<li><a class="nav-link" href="#dummy-variable-adjustment"><span class="header-section-number">11.2.3</span> Dummy Variable Adjustment</a></li>
<li><a class="nav-link" href="#imputation"><span class="header-section-number">11.2.4</span> Imputation</a></li>
<li><a class="nav-link" href="#other-methods"><span class="header-section-number">11.2.5</span> Other methods</a></li>
</ul>
</li>
<li><a class="nav-link" href="#criteria-for-choosing-an-effective-approach"><span class="header-section-number">11.3</span> Criteria for Choosing an Effective Approach</a></li>
<li><a class="nav-link" href="#another-perspective"><span class="header-section-number">11.4</span> Another Perspective</a></li>
<li>
<a class="nav-link" href="#diagnosing-the-mechanism"><span class="header-section-number">11.5</span> Diagnosing the Mechanism</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#mar-vs.-mnar"><span class="header-section-number">11.5.1</span> MAR vs. MNAR</a></li>
<li><a class="nav-link" href="#mcar-vs.-mar"><span class="header-section-number">11.5.2</span> MCAR vs. MAR</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#application-7"><span class="header-section-number">11.6</span> Application</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#imputation-with-mean-median-mode"><span class="header-section-number">11.6.1</span> Imputation with mean / median / mode</a></li>
<li><a class="nav-link" href="#knn"><span class="header-section-number">11.6.2</span> KNN</a></li>
<li><a class="nav-link" href="#rpart"><span class="header-section-number">11.6.3</span> rpart</a></li>
<li><a class="nav-link" href="#mice-multivariate-imputation-via-chained-equations"><span class="header-section-number">11.6.4</span> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li><a class="nav-link" href="#amelia"><span class="header-section-number">11.6.5</span> Amelia</a></li>
<li><a class="nav-link" href="#missforest"><span class="header-section-number">11.6.6</span> missForest</a></li>
<li><a class="nav-link" href="#hmisc"><span class="header-section-number">11.6.7</span> Hmisc</a></li>
<li><a class="nav-link" href="#mi"><span class="header-section-number">11.6.8</span> mi</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/11-imputation.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/11-imputation.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2024-07-31.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
