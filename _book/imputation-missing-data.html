<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 11 Imputation (Missing Data) | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="11.1 Introduction to Missing Data Missing data is a common problem in statistical analyses and data science, impacting the quality and reliability of insights derived from datasets. One widely...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 11 Imputation (Missing Data) | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/imputation-missing-data.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="11.1 Introduction to Missing Data Missing data is a common problem in statistical analyses and data science, impacting the quality and reliability of insights derived from datasets. One widely...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 11 Imputation (Missing Data) | A Guide on Data Analysis">
<meta name="twitter:description" content="11.1 Introduction to Missing Data Missing data is a common problem in statistical analyses and data science, impacting the quality and reliability of insights derived from datasets. One widely...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="active" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="" href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="synthetic-difference-in-differences.html"><span class="header-section-number">25</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">26</span> Difference-in-differences</a></li>
<li><a class="" href="changes-in-changes.html"><span class="header-section-number">27</span> Changes-in-Changes</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">28</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">29</span> Event Studies</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">30</span> Instrumental Variables</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">31</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">32</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">33</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">34</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">35</span> Controls</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">36</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">37</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">38</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">39</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">40</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">41</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="imputation-missing-data" class="section level1" number="11">
<h1>
<span class="header-section-number">11</span> Imputation (Missing Data)<a class="anchor" aria-label="anchor" href="#imputation-missing-data"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-to-missing-data" class="section level2" number="11.1">
<h2>
<span class="header-section-number">11.1</span> Introduction to Missing Data<a class="anchor" aria-label="anchor" href="#introduction-to-missing-data"><i class="fas fa-link"></i></a>
</h2>
<p>Missing data is a common problem in statistical analyses and data science, impacting the quality and reliability of insights derived from datasets. One widely used approach to address this issue is <strong>imputation</strong>, where missing data is replaced with <em>reasonable estimates</em>.</p>
<div id="types-of-imputation" class="section level3" number="11.1.1">
<h3>
<span class="header-section-number">11.1.1</span> Types of Imputation<a class="anchor" aria-label="anchor" href="#types-of-imputation"><i class="fas fa-link"></i></a>
</h3>
<p>Imputation can be categorized into:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Unit Imputation</strong>: Replacing an entire missing observation (i.e., all features for a single data point are missing).</li>
<li>
<strong>Item Imputation</strong>: Replacing missing values for specific variables (features) within a dataset.</li>
</ol>
<p>While imputation offers a means to make use of incomplete datasets, it has historically been viewed skeptically. This skepticism arises from:</p>
<ol style="list-style-type: decimal">
<li>Frequent <strong>misapplication</strong> of imputation techniques, which can introduce significant <strong>bias</strong> to estimates.</li>
<li>Limited <strong>applicability</strong>, as imputation works well only under certain assumptions about the missing data mechanism and research objectives.</li>
</ol>
<p><strong>Biases in imputation</strong> can arise from various factors, including:</p>
<ul>
<li><p><strong>Imputation method</strong>: The chosen method can influence the results and introduce biases.</p></li>
<li><p><strong>Missing data mechanism</strong>: The nature of the missing data—whether it is <a href="imputation-missing-data.html#missing-completely-at-random-mcar">Missing Completely at Random (MCAR)</a> or <a href="imputation-missing-data.html#missing-at-random-mar">Missing at Random (MAR)</a>—affects the accuracy of imputation.</p></li>
<li><p><strong>Proportion of missing data</strong>: The amount of missing data significantly impacts the reliability of the imputation.</p></li>
<li><p><strong>Available information in the dataset</strong>: Limited information reduces the robustness of the imputed values.</p></li>
</ul>
</div>
<div id="when-and-why-to-use-imputation" class="section level3" number="11.1.2">
<h3>
<span class="header-section-number">11.1.2</span> When and Why to Use Imputation<a class="anchor" aria-label="anchor" href="#when-and-why-to-use-imputation"><i class="fas fa-link"></i></a>
</h3>
<p>The appropriateness of imputation depends on the nature of the missing data and the research goal:</p>
<ul>
<li><p><strong>Missing Data in the Outcome Variable</strong> (<span class="math inline">\(y\)</span>): Imputation in such cases is generally problematic, as it can distort statistical models and lead to misleading conclusions. For example, imputing outcomes in regression or classification problems can alter the underlying relationship between the dependent and independent variables.</p></li>
<li><p><strong>Missing Data in Predictive Variables</strong> (<span class="math inline">\(x\)</span>): Imputation is more commonly applied here, especially for <strong>non-random missing data</strong>. Properly handled, imputation can enable the use of incomplete datasets while minimizing bias.</p></li>
</ul>
<div id="objectives-of-imputation" class="section level4" number="11.1.2.1">
<h4>
<span class="header-section-number">11.1.2.1</span> Objectives of Imputation<a class="anchor" aria-label="anchor" href="#objectives-of-imputation"><i class="fas fa-link"></i></a>
</h4>
<p>The utility of imputation methods differs substantially depending on whether the goal of the analysis is <em>inference/explanation</em> or <em>prediction</em>. Each goal has distinct priorities and tolerances for bias, variance, and assumptions about the missing data mechanism:</p>
<div id="inferenceexplanation" class="section level5" number="11.1.2.1.1">
<h5>
<span class="header-section-number">11.1.2.1.1</span> Inference/Explanation<a class="anchor" aria-label="anchor" href="#inferenceexplanation"><i class="fas fa-link"></i></a>
</h5>
<p>In causal inference or explanatory analyses, the primary objective is to ensure valid statistical inference, emphasizing unbiased estimation of parameters and accurate representation of uncertainty. The treatment of missing data must align closely with the assumptions about the mechanism behind the missing data—whether it is Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR):</p>
<ul>
<li><p><strong>Bias Sensitivity:</strong> Inference analyses require that imputed data preserve the integrity of the relationships among variables. Poorly executed imputation can introduce bias, even when it addresses missingness superficially.</p></li>
<li><p><strong>Variance and Confidence Intervals:</strong> For inference, the quality of the standard errors, confidence intervals, and test statistics is critical. Naive imputation methods (e.g., mean imputation) often fail to appropriately reflect the uncertainty due to missingness, leading to overconfidence in parameter estimates.</p></li>
<li><p><strong>Mechanism Considerations:</strong> Imputation methods, such as multiple imputation (MI), attempt to generate values consistent with the observed data distribution while accounting for missing data uncertainty. However, MI’s performance depends heavily on the validity of the MAR assumption. If the missingness mechanism is MNAR and not addressed adequately, the imputed data could yield biased parameter estimates, undermining the purpose of inference.</p></li>
</ul>
</div>
<div id="prediction-1" class="section level5" number="11.1.2.1.2">
<h5>
<span class="header-section-number">11.1.2.1.2</span> Prediction<a class="anchor" aria-label="anchor" href="#prediction-1"><i class="fas fa-link"></i></a>
</h5>
<p>In predictive modeling, the primary goal is to maximize model accuracy (e.g., minimizing mean squared error for continuous outcomes or maximizing classification accuracy). Here, the focus shifts to optimizing predictive performance rather than ensuring unbiased parameter estimates:</p>
<ul>
<li><p><strong>Loss of Information:</strong> Missing data reduces the amount of usable information in a dataset. Imputation allows the model to leverage all available data, rather than excluding incomplete cases via listwise deletion, which can significantly reduce sample size and model performance.</p></li>
<li><p><strong>Impact on Model Fit:</strong> In predictive contexts, imputation can reduce standard errors of the predictions and stabilize model coefficients by incorporating plausible estimates for missing values.</p></li>
<li><p><strong>Flexibility with Mechanism:</strong> Predictive models are less sensitive to the missing data mechanism than inferential models, as long as the imputed values help reduce variability and align with patterns in the observed data. Methods like K-Nearest Neighbors (KNN), iterative imputation, or even machine learning models (e.g., random forests for imputation) can be valuable, regardless of strict adherence to MAR or MCAR assumptions.</p></li>
<li><p><strong>Trade-offs:</strong> Overimputation, where too much noise or complexity is introduced in the imputation process, can harm prediction by introducing artifacts that degrade model generalizability.</p></li>
</ul>
</div>
<div id="key-takeaways-1" class="section level5" number="11.1.2.1.3">
<h5>
<span class="header-section-number">11.1.2.1.3</span> Key Takeaways<a class="anchor" aria-label="anchor" href="#key-takeaways-1"><i class="fas fa-link"></i></a>
</h5>
<p>The usefulness of imputation depends on whether the goal of the analysis is <strong>inference</strong> or <strong>prediction</strong>:</p>
<ul>
<li><p><strong>Inference/Explanation:</strong> The primary concern is valid statistical inference, where biased estimates are unacceptable. Imputation is often of limited value for this purpose, as it may not address the underlying missing data mechanism appropriately <span class="citation">(<a href="references.html#ref-Rubin_1996">Rubin 1996</a>)</span>.</p></li>
<li><p><strong>Prediction:</strong> Imputation can be more useful in predictive modeling, as it reduces the loss of information from incomplete cases. By leveraging observed data, imputation can lower standard errors and improve model accuracy.</p></li>
</ul>
<hr>
</div>
</div>
</div>
<div id="importance-of-missing-data-treatment-in-statistical-modeling" class="section level3" number="11.1.3">
<h3>
<span class="header-section-number">11.1.3</span> Importance of Missing Data Treatment in Statistical Modeling<a class="anchor" aria-label="anchor" href="#importance-of-missing-data-treatment-in-statistical-modeling"><i class="fas fa-link"></i></a>
</h3>
<p>Proper handling of missing data ensures:</p>
<ul>
<li>
<strong>Unbiased Estimates:</strong> Avoiding distortions in parameter estimates.</li>
<li>
<strong>Accurate Standard Errors:</strong> Ensuring valid hypothesis testing and confidence intervals.</li>
<li>
<strong>Adequate Statistical Power:</strong> Maximizing the use of available data.</li>
</ul>
<p>Ignoring or mishandling missing data can lead to:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Bias:</strong> Systematic errors in parameter estimates, especially under MAR or MNAR mechanisms.</li>
<li>
<strong>Loss of Power:</strong> Reduced sample size leads to larger standard errors and weaker statistical significance.</li>
<li>
<strong>Misleading Conclusions:</strong> Over-simplistic imputation methods (e.g., mean substitution) can distort relationships among variables.</li>
</ol>
<hr>
</div>
<div id="prevalence-of-missing-data-across-domains" class="section level3" number="11.1.4">
<h3>
<span class="header-section-number">11.1.4</span> Prevalence of Missing Data Across Domains<a class="anchor" aria-label="anchor" href="#prevalence-of-missing-data-across-domains"><i class="fas fa-link"></i></a>
</h3>
<p>Missing data affects virtually all fields:</p>
<ul>
<li>
<strong>Business:</strong> Non-responses in customer surveys, incomplete sales records, and transactional errors.</li>
<li>
<strong>Healthcare:</strong> Missing data in electronic health records (EHRs) due to incomplete patient histories or inconsistent data entry.</li>
<li>
<strong>Social Sciences:</strong> Non-responses or partial responses in large-scale surveys, leading to biased conclusions.</li>
</ul>
<hr>
</div>
<div id="practical-considerations-for-imputation" class="section level3" number="11.1.5">
<h3>
<span class="header-section-number">11.1.5</span> Practical Considerations for Imputation<a class="anchor" aria-label="anchor" href="#practical-considerations-for-imputation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<strong>Diagnostic Checks:</strong> Always examine the patterns and mechanisms of missing data before applying imputation (<a href="imputation-missing-data.html#diagnosing-the-missing-data-mechanism">Diagnosing the Missing Data Mechanism</a>).</li>
<li>
<strong>Model Selection:</strong> Align the imputation method with the missing data mechanism and research goal.</li>
<li>
<strong>Validation:</strong> Assess the impact of imputation on results through sensitivity analyses or cross-validation.</li>
</ul>
<hr>
</div>
</div>
<div id="theoretical-foundations-of-missing-data" class="section level2" number="11.2">
<h2>
<span class="header-section-number">11.2</span> Theoretical Foundations of Missing Data<a class="anchor" aria-label="anchor" href="#theoretical-foundations-of-missing-data"><i class="fas fa-link"></i></a>
</h2>
<div id="definition-and-classification-of-missing-data" class="section level3" number="11.2.1">
<h3>
<span class="header-section-number">11.2.1</span> Definition and Classification of Missing Data<a class="anchor" aria-label="anchor" href="#definition-and-classification-of-missing-data"><i class="fas fa-link"></i></a>
</h3>
<p>Missing data refers to the absence of values for some variables in a dataset. The mechanisms underlying missingness significantly impact the validity of statistical analyses and the choice of handling methods. These mechanisms are classified into three categories:</p>
<ol style="list-style-type: decimal">
<li>
<li>
<li>
<a href="imputation-missing-data.html#missing-not-at-random-mnar">Missing Not at Random (MNAR)</a>: Missingness depends on unobserved variables or the missing values themselves.</li>
</ol>
<div id="missing-completely-at-random-mcar" class="section level4" number="11.2.1.1">
<h4>
<span class="header-section-number">11.2.1.1</span> Missing Completely at Random (MCAR)<a class="anchor" aria-label="anchor" href="#missing-completely-at-random-mcar"><i class="fas fa-link"></i></a>
</h4>
<p>MCAR occurs when the probability of missingness is entirely random and unrelated to either observed or unobserved variables. Under this mechanism, missing data do not introduce bias in parameter estimates when ignored, although statistical efficiency is reduced due to the smaller sample size.</p>
<p><strong>Mathematical Definition:</strong> The missingness is independent of all data, both observed and unobserved:</p>
<p><span class="math display">\[
P(Y_{\text{missing}} | Y, X) = P(Y_{\text{missing}})
\]</span></p>
<p><strong>Characteristics of MCAR:</strong></p>
<ul>
<li>Missingness is completely unrelated to both observed and unobserved data.</li>
<li>Analyses remain unbiased even if missing data are ignored, though they may lack efficiency due to reduced sample size.</li>
<li>The missing data points represent a random subset of the overall data.</li>
</ul>
<p><strong>Examples:</strong></p>
<ul>
<li>A sensor randomly fails at specific time points, unrelated to environmental or operational conditions.</li>
<li>Survey participants randomly omit responses to certain questions without any systematic pattern.</li>
</ul>
<p><strong>Methods for Testing MCAR:</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Little’s MCAR Test:</strong> A formal statistical test to assess whether data are MCAR. A significant result suggests deviation from MCAR.</p></li>
<li>
<p><strong>Mean Comparison Tests:</strong></p>
<ul>
<li>T-tests or similar approaches compare observed and missing data groups on other variables. Significant differences indicate potential bias.</li>
<li>Failure to reject the null hypothesis of no difference does not confirm MCAR but suggests consistency with the MCAR assumption.</li>
</ul>
</li>
</ol>
<p><strong>Handling MCAR:</strong></p>
<p>Since MCAR data introduce no bias, they can be handled using the following techniques:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Complete Case Analysis (Listwise Deletion):</strong>
<ul>
<li>Analyses are performed only on cases with complete data. While unbiased under MCAR, this method reduces sample size and efficiency.</li>
</ul>
</li>
<li>
<strong>Universal Singular Value Thresholding (USVT):</strong>
<ul>
<li>This technique is effective for MCAR data recovery but can only recover the mean structure, not the entire true distribution <span class="citation">(<a href="references.html#ref-chatterjee2015matrix">Chatterjee 2015</a>)</span>.</li>
</ul>
</li>
<li>
<strong>SoftImpute:</strong>
<ul>
<li>A matrix completion method useful for some missing data problems but less effective when missingness is not MCAR <span class="citation">(<a href="references.html#ref-hastie2015matrix">Hastie et al. 2015</a>)</span>.</li>
</ul>
</li>
<li>
<strong>Synthetic Nearest Neighbor Imputation:</strong>
<ul>
<li>A robust method for imputing missing data. While primarily designed for MCAR, it can also handle certain cases of missing not at random (MNAR) <span class="citation">(<a href="references.html#ref-agarwal2023causal">Agarwal et al. 2023</a>)</span>. Available on GitHub: <a href="https://github.com/deshen24/syntheticNN">syntheticNN</a>.</li>
</ul>
</li>
</ol>
<p><strong>Notes:</strong></p>
<ul>
<li>The “missingness” on one variable can be correlated with the “missingness” on another variable without violating the MCAR assumption.</li>
<li>Absence of evidence for bias (e.g., failing to reject a t-test) does not confirm that the data are MCAR.</li>
</ul>
</div>
<div id="missing-at-random-mar" class="section level4" number="11.2.1.2">
<h4>
<span class="header-section-number">11.2.1.2</span> Missing at Random (MAR)<a class="anchor" aria-label="anchor" href="#missing-at-random-mar"><i class="fas fa-link"></i></a>
</h4>
<p>Missing at Random (MAR) occurs when missingness depends on observed variables but not the missing values themselves. This mechanism assumes that observed data provide sufficient information to explain the missingness. In other words, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.</p>
<p><strong>Mathematical Definition</strong>:</p>
<p>The probability of missingness is conditional only on observed data:</p>
<p><span class="math display">\[
P(Y_{\text{missing}} | Y, X) = P(Y_{\text{missing}} | X)
\]</span></p>
<p>This implies that whether an observation is missing is unrelated to the missing values themselves but is related to the observed values of other variables.</p>
<p><strong>Characteristics of MAR</strong>:</p>
<ul>
<li>Missingness is systematically related to observed variables.</li>
<li>The propensity for a data point to be missing is not related to the missing data but is related to some of the observed data.</li>
<li>Analyses must account for observed data to mitigate bias.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>Women are less likely to disclose their weight, but their gender is recorded. In this case, weight is MAR.</li>
<li>Missing income data is correlated with education, which is observed. For example, individuals with higher education levels might be less likely to reveal their income.</li>
</ul>
<p><strong>Challenges in MAR</strong>:</p>
<ul>
<li>MAR is weaker than Missing Completely at Random (MCAR).</li>
<li>It is impossible to directly test for MAR. Evidence for MAR relies on domain expertise and indirect statistical checks rather than direct tests.</li>
</ul>
<p><strong>Handling MAR</strong>:</p>
<p>Common methods for handling MAR include:</p>
<ul>
<li><p><strong>Multiple Imputation by Chained Equations (MICE):</strong> Iteratively imputes missing values based on observed data.</p></li>
<li><p><strong>Maximum Likelihood Estimation:</strong> Estimates model parameters directly while accounting for MAR assumptions.</p></li>
<li><p><strong>Regression-Based Imputation:</strong> Predicts missing values using observed covariates.</p></li>
</ul>
<p>These methods assume that observed variables fully explain the missingness. Effective handling of MAR requires careful modeling and often domain-specific knowledge to validate the assumptions underlying the analysis.</p>
</div>
<div id="missing-not-at-random-mnar" class="section level4" number="11.2.1.3">
<h4>
<span class="header-section-number">11.2.1.3</span> Missing Not at Random (MNAR)<a class="anchor" aria-label="anchor" href="#missing-not-at-random-mnar"><i class="fas fa-link"></i></a>
</h4>
<p>Missing Not at Random (MNAR) is the most complex missing data mechanism. Here, missingness depends on unobserved variables or the values of the missing data themselves. This makes MNAR particularly challenging, as ignoring this dependency introduces significant bias in analyses.</p>
<p><strong>Mathematical Definition</strong>:</p>
<p>The probability of missingness depends on the missing values:</p>
<p><span class="math display">\[
P(Y_{\text{missing}} | Y, X) \neq P(Y_{\text{missing}} | X)
\]</span></p>
<p><strong>Characteristics of MNAR</strong>:</p>
<ul>
<li>Missingness cannot be fully explained by observed data.</li>
<li>The cause of missingness is directly related to the unobserved values.</li>
<li>Ignoring MNAR introduces significant bias in parameter estimates, often leading to invalid conclusions.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>High-income individuals are less likely to disclose their income, and income itself is unobserved.</li>
<li>Patients with severe symptoms drop out of a clinical study, leaving their health outcomes unrecorded.</li>
</ul>
<p><strong>Challenges in MNAR</strong>:</p>
<ul>
<li>MNAR is the most difficult missingness mechanism to address because the missing data mechanism must be explicitly modeled.</li>
<li>Identifying MNAR often requires domain knowledge and auxiliary information beyond the observed dataset.</li>
</ul>
<p><strong>Handling MNAR</strong>:</p>
<p>MNAR requires explicit modeling of the missingness mechanism. Common approaches include:</p>
<ul>
<li><p><strong>Heckman Selection Models:</strong> These models explicitly account for the selection process leading to missing data, adjusting for potential bias <span class="citation">(<a href="references.html#ref-Heckman_1976">James J. Heckman 1976</a>)</span>.</p></li>
<li><p><strong>Instrumental Variables:</strong> Variables predictive of missingness but unrelated to the outcome can be used to mitigate bias <span class="citation">(<a href="references.html#ref-sun2018semiparametric">B. Sun et al. 2018</a>; <a href="references.html#ref-tchetgen2017general">E. J. Tchetgen Tchetgen and Wirth 2017</a>)</span>.</p></li>
<li><p><strong>Pattern-Mixture Models</strong>: These models separate the data into groups (patterns) based on missingness and model each group separately. They are particularly useful when the relationship between missingness and missing values is complex.</p></li>
<li><p><strong>Sensitivity Analysis:</strong> Examines how conclusions change under different assumptions about the missing data mechanism.</p></li>
<li>
<p><strong>Use of Auxiliary Data</strong></p>
<p>Auxiliary data refers to external data sources or variables that can help explain the missingness mechanism.</p>
<ul>
<li><p><strong>Surrogate Variables</strong>: Adding variables that correlate with missing data can improve imputation accuracy and mitigate the MNAR challenge.</p></li>
<li><p><strong>Linking External Datasets</strong>: Merging datasets from different sources can provide additional context or predictors for missingness.</p></li>
<li><p><strong>Applications in Business</strong>: In marketing, customer demographics or transaction histories often serve as auxiliary data to predict missing responses in surveys.</p></li>
</ul>
</li>
</ul>
<p>Additionally, data collection strategies, such as follow-up surveys or targeted sampling, can help mitigate MNAR effects by collecting information that directly addresses the missingness mechanism. However, such approaches can be resource-intensive and require careful planning.</p>
</div>
</div>
<div id="missing-data-mechanisms" class="section level3" number="11.2.2">
<h3>
<span class="header-section-number">11.2.2</span> Missing Data Mechanisms<a class="anchor" aria-label="anchor" href="#missing-data-mechanisms"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="24%">
<col width="24%">
<col width="26%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th><strong>Mechanism</strong></th>
<th><strong>Missingness Depends On</strong></th>
<th><strong>Implications</strong></th>
<th><strong>Examples</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>MCAR</strong></td>
<td>Neither observed nor missing data</td>
<td>No bias; simplest to handle; decreases efficiency due to data loss.</td>
<td>Random sensor failure.</td>
</tr>
<tr class="even">
<td><strong>MAR</strong></td>
<td>Observed data only</td>
<td>Requires observed data to explain missingness; common assumption in imputation methods.</td>
<td>Gender-based missingness of weight.</td>
</tr>
<tr class="odd">
<td><strong>MNAR</strong></td>
<td>Missing data itself or unobserved variables</td>
<td>Requires explicit modeling of the missingness mechanism; significant bias if ignored.</td>
<td>High-income individuals not disclosing income.</td>
</tr>
</tbody>
</table></div>
</div>
<div id="relationship-between-mechanisms-and-ignorability" class="section level3" number="11.2.3">
<h3>
<span class="header-section-number">11.2.3</span> Relationship Between Mechanisms and Ignorability<a class="anchor" aria-label="anchor" href="#relationship-between-mechanisms-and-ignorability"><i class="fas fa-link"></i></a>
</h3>
<p>The concept of ignorability is central to determining whether the missingness process must be explicitly modeled. Ignorability impacts the choice of methods for handling missing data and whether the missing data mechanism can be safely disregarded or must be explicitly accounted for.</p>
<div id="ignorable-missing-data" class="section level4" number="11.2.3.1">
<h4>
<span class="header-section-number">11.2.3.1</span> Ignorable Missing Data<a class="anchor" aria-label="anchor" href="#ignorable-missing-data"><i class="fas fa-link"></i></a>
</h4>
<p>Missing data is <strong>ignorable</strong> under the following conditions:</p>
<ol style="list-style-type: decimal">
<li>The missing data mechanism is <a href="imputation-missing-data.html#missing-at-random-mar">MAR</a> or <a href="imputation-missing-data.html#missing-completely-at-random-mcar">MCAR</a>.</li>
<li>The parameters governing the missing data process are unrelated to the parameters of interest in the analysis.</li>
</ol>
<p>In cases of ignorable missing data, there is no need to model the missingness mechanism explicitly unless you aim to improve the efficiency or precision of parameter estimates. Common imputation techniques, such as multiple imputation or maximum likelihood estimation, rely on the assumption of ignorability to produce unbiased parameter estimates.</p>
<p><strong>Practical Considerations for Ignorable Missingness</strong></p>
<p>Even though ignorable mechanisms simplify analysis, researchers must rigorously assess whether the missingness mechanism meets the MAR or MCAR criteria. Violations can lead to biased results, even if unintentionally overlooked.</p>
<p>For example: A survey on income may assume MAR if missingness is associated with respondent age (observed variable) but not income itself (unobserved variable). However, if income directly influences nonresponse, the assumption of MAR is violated.</p>
<hr>
</div>
<div id="non-ignorable" class="section level4" number="11.2.3.2">
<h4>
<span class="header-section-number">11.2.3.2</span> Non-Ignorable Missing Data<a class="anchor" aria-label="anchor" href="#non-ignorable"><i class="fas fa-link"></i></a>
</h4>
<p>Missing data is <strong>non-ignorable</strong> when:</p>
<ol style="list-style-type: decimal">
<li>The missingness mechanism depends on the values of the missing data themselves or on unobserved variables.</li>
<li>The missing data mechanism is related to the parameters of interest, resulting in bias if the mechanism is not modeled explicitly.</li>
</ol>
<p>This type of missingness (i.e., <a href="imputation-missing-data.html#missing-not-at-random-mnar">Missing Not at Random (MNAR)</a> requires modeling the missing data mechanism directly to produce unbiased estimates.</p>
<p><strong>Characteristics of Non-Ignorable Missingness</strong></p>
<ul>
<li>
<strong>Dependence on Missing Values</strong>: The likelihood of missingness is associated with the missing values themselves.
<ul>
<li>Example: In a study on health, individuals with more severe conditions are more likely to drop out, leading to an underrepresentation of the sickest individuals in the data.</li>
</ul>
</li>
<li>
<strong>Bias in Complete Case Analysis</strong>: Analyses based solely on complete cases can lead to substantial bias.
<ul>
<li>Example: In income surveys, if wealthier individuals are less likely to report their income, the estimated mean income will be systematically lower than the true population mean.</li>
</ul>
</li>
<li>
<strong>Need for Explicit Modeling</strong>: To address MNAR, the analyst must model the missing data mechanism. This often involves specifying relationships between observed data, missing data, and the missingness process itself.</li>
</ul>
</div>
<div id="implications-of-non-ignorable-missingness" class="section level4" number="11.2.3.3">
<h4>
<span class="header-section-number">11.2.3.3</span> Implications of Non-Ignorable Missingness<a class="anchor" aria-label="anchor" href="#implications-of-non-ignorable-missingness"><i class="fas fa-link"></i></a>
</h4>
<p>Non-ignorable mechanisms are often associated with sensitive or personal data:</p>
<ul>
<li>
<p><strong>Examples</strong>:</p>
<ul>
<li><p>Individuals with lower education levels may omit their education information.</p></li>
<li><p>Participants with controversial or stigmatized health conditions might opt out of surveys entirely.</p></li>
</ul>
</li>
<li>
<p><strong>Impact on Policy and Decision-Making</strong>:</p>
<ul>
<li>Biases introduced by MNAR can have serious consequences for policymaking, such as underestimating the prevalence of poverty or mischaracterizing population health needs.</li>
</ul>
</li>
</ul>
<p>By explicitly addressing non-ignorable missingness, researchers can mitigate biases and ensure that findings accurately reflect the underlying population.</p>
<hr>
</div>
</div>
</div>
<div id="diagnosing-the-missing-data-mechanism" class="section level2" number="11.3">
<h2>
<span class="header-section-number">11.3</span> Diagnosing the Missing Data Mechanism<a class="anchor" aria-label="anchor" href="#diagnosing-the-missing-data-mechanism"><i class="fas fa-link"></i></a>
</h2>
<p>Understanding the mechanism behind missing data is critical to choosing the appropriate methods for handling it. The three main mechanisms for missing data are <strong>MCAR (Missing Completely at Random)</strong>, <strong>MAR (Missing at Random)</strong>, and <strong>MNAR (Missing Not at Random)</strong>. This section discusses methods for diagnosing these mechanisms, including descriptive and inferential approaches.</p>
<hr>
<div id="descriptive-methods" class="section level3" number="11.3.1">
<h3>
<span class="header-section-number">11.3.1</span> Descriptive Methods<a class="anchor" aria-label="anchor" href="#descriptive-methods"><i class="fas fa-link"></i></a>
</h3>
<div id="visualizing-missing-data-patterns" class="section level4" number="11.3.1.1">
<h4>
<span class="header-section-number">11.3.1.1</span> Visualizing Missing Data Patterns<a class="anchor" aria-label="anchor" href="#visualizing-missing-data-patterns"><i class="fas fa-link"></i></a>
</h4>
<p>Visualization tools are essential for detecting patterns in missing data. Heatmaps and correlation plots can help identify systematic missingness and provide insights into the underlying mechanism.</p>
<div class="sourceCode" id="cb342"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Visualizing missing data</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://gking.harvard.edu/amelia">Amelia</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Amelia/man/missmap.html">missmap</a></span><span class="op">(</span></span>
<span>    <span class="va">airquality</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Missing Data Heatmap"</span>,</span>
<span>    col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"yellow"</span>, <span class="st">"black"</span><span class="op">)</span>,</span>
<span>    legend <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p><strong>Heatmaps</strong>: Highlight where missingness occurs in a dataset.</p></li>
<li><p><strong>Correlation Plots</strong>: Show relationships between missingness indicators of different variables.</p></li>
</ul>
<p><strong>Exploring Univariate and Multivariate Missingness</strong></p>
<ul>
<li>
<strong>Univariate Analysis</strong>: Calculate the proportion of missing data for each variable.</li>
</ul>
<div class="sourceCode" id="cb343"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Proportion of missing values</span></span>
<span><span class="va">missing_proportions</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/colSums.html">colSums</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">missing_proportions</span><span class="op">)</span></span>
<span><span class="co">#&gt;      Ozone    Solar.R       Wind       Temp      Month        Day </span></span>
<span><span class="co">#&gt; 0.24183007 0.04575163 0.00000000 0.00000000 0.00000000 0.00000000</span></span></code></pre></div>
<ul>
<li>
<strong>Multivariate Analysis</strong>: Examine whether missingness in one variable is related to others. This can be visualized using scatterplots of observed vs. missing values.</li>
</ul>
<div class="sourceCode" id="cb344"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Missingness correlation</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/njtierney/naniar">naniar</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://docs.ropensci.org/visdat/reference/vis_miss.html">vis_miss</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb345"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/gg_miss_upset.html">gg_miss_upset</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span> <span class="co"># Displays a missingness upset plot</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-3-2.png" width="90%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="statistical-tests-for-missing-data-mechanisms" class="section level3" number="11.3.2">
<h3>
<span class="header-section-number">11.3.2</span> Statistical Tests for Missing Data Mechanisms<a class="anchor" aria-label="anchor" href="#statistical-tests-for-missing-data-mechanisms"><i class="fas fa-link"></i></a>
</h3>
<div id="diagnosing-mcar-littles-test" class="section level4" number="11.3.2.1">
<h4>
<span class="header-section-number">11.3.2.1</span> Diagnosing MCAR: Little’s Test<a class="anchor" aria-label="anchor" href="#diagnosing-mcar-littles-test"><i class="fas fa-link"></i></a>
</h4>
<p>Little’s test is a hypothesis test to determine if the missing data mechanism is <strong>MCAR</strong>. It tests whether the means of observed and missing data are significantly different. The null hypothesis is that the data are MCAR.</p>
<p><span class="math display">\[
\chi^2 = \sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(O_i\)</span>= Observed frequency</p></li>
<li><p><span class="math inline">\(E_i\)</span>= Expected frequency under MCAR</p></li>
</ul>
<div class="sourceCode" id="cb346"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Little's test</span></span>
<span><span class="fu">naniar</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/mcar_test.html">mcar_test</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 1 × 4</span></span>
<span><span class="co">#&gt;   statistic    df p.value missing.patterns</span></span>
<span><span class="co">#&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;</span></span>
<span><span class="co">#&gt; 1      35.1    14 0.00142                4</span></span>
<span><span class="fu">misty</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/misty/man/na.test.html">na.test</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span>
<span><span class="co">#&gt;  Little's MCAR Test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     n nIncomp nPattern  chi2 df  pval </span></span>
<span><span class="co">#&gt;   153      42        4 35.15 14 0.001</span></span></code></pre></div>
</div>
<div id="diagnosing-mcar-via-dummy-variables" class="section level4" number="11.3.2.2">
<h4>
<span class="header-section-number">11.3.2.2</span> Diagnosing MCAR via Dummy Variables<a class="anchor" aria-label="anchor" href="#diagnosing-mcar-via-dummy-variables"><i class="fas fa-link"></i></a>
</h4>
<p>Creating a binary indicator for missingness allows you to test whether the presence of missing data is related to observed data. For instance:</p>
<ol style="list-style-type: decimal">
<li>
<p>Create a dummy variable:</p>
<ul>
<li><p>1 = Missing</p></li>
<li><p>0 = Observed</p></li>
</ul>
</li>
<li>
<p>Conduct a chi-square test or t-test:</p>
<ul>
<li><p>Chi-square: Compare proportions of missingness across groups.</p></li>
<li><p>T-test: Compare means of (other) observed variables with missingness indicators.</p></li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb347"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Chi-square test</span></span>
<span><span class="va">airquality</span><span class="op">$</span><span class="va">missing_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">$</span><span class="va">Ozone</span><span class="op">)</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Across groups of months</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">$</span><span class="va">missing_var</span>, <span class="va">airquality</span><span class="op">$</span><span class="va">Month</span><span class="op">)</span></span>
<span><span class="co">#&gt;    </span></span>
<span><span class="co">#&gt;      5  6  7  8  9</span></span>
<span><span class="co">#&gt;   0 26  9 26 26 29</span></span>
<span><span class="co">#&gt;   1  5 21  5  5  1</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">$</span><span class="va">missing_var</span>, <span class="va">airquality</span><span class="op">$</span><span class="va">Month</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Pearson's Chi-squared test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  table(airquality$missing_var, airquality$Month)</span></span>
<span><span class="co">#&gt; X-squared = 44.751, df = 4, p-value = 4.48e-09</span></span>
<span></span>
<span><span class="co"># Example: T-test (of other variable)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">Wind</span> <span class="op">~</span> <span class="va">missing_var</span>, data <span class="op">=</span> <span class="va">airquality</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Welch Two Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  Wind by missing_var</span></span>
<span><span class="co">#&gt; t = -0.60911, df = 63.646, p-value = 0.5446</span></span>
<span><span class="co">#&gt; alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -1.6893132  0.8999377</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean in group 0 mean in group 1 </span></span>
<span><span class="co">#&gt;        9.862069       10.256757</span></span></code></pre></div>
</div>
</div>
<div id="assessing-mar-and-mnar" class="section level3" number="11.3.3">
<h3>
<span class="header-section-number">11.3.3</span> Assessing MAR and MNAR<a class="anchor" aria-label="anchor" href="#assessing-mar-and-mnar"><i class="fas fa-link"></i></a>
</h3>
<div id="sensitivity-analysis" class="section level4" number="11.3.3.1">
<h4>
<span class="header-section-number">11.3.3.1</span> Sensitivity Analysis<a class="anchor" aria-label="anchor" href="#sensitivity-analysis"><i class="fas fa-link"></i></a>
</h4>
<p>Sensitivity analysis involves simulating different scenarios of missing data and assessing how the results change. For example, imputing missing values under different assumptions can provide insight into whether the data are MAR or MNAR.</p>
</div>
<div id="proxy-variables-and-external-data" class="section level4" number="11.3.3.2">
<h4>
<span class="header-section-number">11.3.3.2</span> Proxy Variables and External Data<a class="anchor" aria-label="anchor" href="#proxy-variables-and-external-data"><i class="fas fa-link"></i></a>
</h4>
<p>Using proxy variables or external data sources can help assess whether missingness depends on unobserved variables (MNAR). For example, in surveys, follow-ups with non-respondents can reveal systematic differences.</p>
</div>
<div id="practical-challenges-in-distinguishing-mar-from-mnar" class="section level4" number="11.3.3.3">
<h4>
<span class="header-section-number">11.3.3.3</span> Practical Challenges in Distinguishing MAR from MNAR<a class="anchor" aria-label="anchor" href="#practical-challenges-in-distinguishing-mar-from-mnar"><i class="fas fa-link"></i></a>
</h4>
<p>Distinguishing between Missing at Random (MAR) and Missing Not at Random (MNAR) is a critical and challenging task in data analysis. Properly identifying the nature of the missing data has significant implications for the choice of imputation strategies, model robustness, and the validity of conclusions. While statistical tests can sometimes aid in this determination, the process often relies heavily on domain knowledge, intuition, and exploratory analysis. Below, we discuss key considerations and examples that highlight these challenges:</p>
<ul>
<li><p><strong>Sensitive Topics</strong>: Missing data related to sensitive or stigmatized topics, such as income, drug use, or health conditions, are often MNAR. For example, individuals with higher incomes might deliberately choose not to report their earnings due to privacy concerns. Similarly, participants in a health survey may avoid answering questions about smoking if they perceive social disapproval. In such cases, the probability of missingness is directly related to the unobserved value itself, making MNAR likely.</p></li>
<li><p><strong>Field-Specific Norms</strong>: Understanding norms and typical data collection practices in a specific field can provide insights into missingness patterns. For instance, in marketing surveys, respondents may skip questions about spending habits if they consider the questions intrusive. Prior research or historical data from the same domain can help infer whether missingness is more likely MAR (e.g., random skipping due to survey fatigue) or MNAR (e.g., deliberate omission by higher spenders).</p></li>
<li><p><strong>Analyzing Auxiliary Variables</strong>: Leveraging auxiliary variables—those correlated with the missing variable—can help infer the missingness mechanism. For example, if missing income data strongly correlates with employment status, this suggests a MAR mechanism, as the missingness depends on observed variables. However, if missingness persists even after accounting for observable predictors, MNAR might be at play.</p></li>
<li><p><strong>Experimental Design and Follow-Up</strong>: In longitudinal studies, dropout rates can signal MAR or MNAR patterns. For example, if dropouts occur disproportionately among participants reporting lower satisfaction in early surveys, this indicates an MNAR mechanism. Designing follow-up surveys to specifically investigate dropout reasons can clarify missingness patterns.</p></li>
<li><p><strong>Sensitivity Analysis</strong>: To account for uncertainty in the missingness mechanism, researchers can conduct sensitivity analyses by comparing results under different assumptions (e.g., imputing data using both MAR and MNAR approaches). This process helps to quantify the potential impact of misclassifying the missingness mechanism on study conclusions.</p></li>
<li>
<p><strong>Real-World Examples</strong>:</p>
<ul>
<li>In customer feedback surveys, higher ratings might be overrepresented due to non-response bias. Customers with negative experiences might be less likely to complete surveys, leading to an MNAR scenario.</li>
<li>In financial reporting, missing audit data might correlate with companies in financial distress, a classic MNAR case where the missingness depends on unobserved financial health metrics.</li>
</ul>
</li>
</ul>
<p>Summary</p>
<ul>
<li><p><strong>MCAR</strong>: No pattern in missingness; use Little’s test or dummy variable analysis.</p></li>
<li><p><strong>MAR</strong>: Missingness related to observed data; requires modeling assumptions or proxy analysis.</p></li>
<li><p><strong>MNAR</strong>: Missingness depends on unobserved data; requires external validation or sensitivity analysis.</p></li>
</ul>
</div>
</div>
</div>
<div id="methods-for-handling-missing-data" class="section level2" number="11.4">
<h2>
<span class="header-section-number">11.4</span> Methods for Handling Missing Data<a class="anchor" aria-label="anchor" href="#methods-for-handling-missing-data"><i class="fas fa-link"></i></a>
</h2>
<div id="basic-methods" class="section level3" number="11.4.1">
<h3>
<span class="header-section-number">11.4.1</span> Basic Methods<a class="anchor" aria-label="anchor" href="#basic-methods"><i class="fas fa-link"></i></a>
</h3>
<div id="complete-case-analysis-listwise-deletion" class="section level4" number="11.4.1.1">
<h4>
<span class="header-section-number">11.4.1.1</span> Complete Case Analysis (Listwise Deletion)<a class="anchor" aria-label="anchor" href="#complete-case-analysis-listwise-deletion"><i class="fas fa-link"></i></a>
</h4>
<p>Listwise deletion retains only cases with complete data for all features, discarding rows with any missing values.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Universally applicable to various statistical tests (e.g., SEM, multilevel regression).</li>
<li>When data are Missing Completely at Random (MCAR), parameter estimates and standard errors are unbiased.</li>
<li>Under specific Missing at Random (MAR) conditions, such as when the probability of missing data depends only on independent variables, listwise deletion can still yield unbiased estimates. For instance, in the model <span class="math inline">\(y = \beta_{0} + \beta_1X_1 + \beta_2X_2 + \epsilon\)</span>, if missingness in <span class="math inline">\(X_1\)</span> is independent of <span class="math inline">\(y\)</span> but depends on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, the estimates remain unbiased <span class="citation">(<a href="references.html#ref-Little_1992">Little 1992</a>)</span>.
<ul>
<li>This aligns with principles of stratified sampling, which does not bias estimates.</li>
<li>In logistic regression, if missing data depend only on the dependent variable but not on independent variables, listwise deletion produces consistent slope estimates, though the intercept may be biased <span class="citation">(<a href="references.html#ref-Vach_1994">Vach and Vach 1994</a>)</span>.</li>
</ul>
</li>
<li>For regression analysis, listwise deletion is more robust than Maximum Likelihood (ML) or Multiple Imputation (MI) when the MAR assumption is violated.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Results in larger standard errors compared to advanced methods.</li>
<li>If data are MAR but not MCAR, biased estimates can occur.</li>
<li>In non-regression contexts, more sophisticated methods often outperform listwise deletion.</li>
</ul>
<hr>
</div>
<div id="available-case-analysis-pairwise-deletion" class="section level4" number="11.4.1.2">
<h4>
<span class="header-section-number">11.4.1.2</span> Available Case Analysis (Pairwise Deletion)<a class="anchor" aria-label="anchor" href="#available-case-analysis-pairwise-deletion"><i class="fas fa-link"></i></a>
</h4>
<p>Pairwise deletion calculates estimates using all available data for each pair of variables, without requiring complete cases. It is particularly suitable for methods like linear regression, factor analysis, and SEM, which rely on correlation or covariance matrices.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Under MCAR, pairwise deletion produces consistent and unbiased estimates in large samples.</li>
<li>Compared to listwise deletion <span class="citation">(<a href="references.html#ref-Glasser_1964">Glasser 1964</a>)</span>:
<ul>
<li>When variable correlations are low, pairwise deletion provides more efficient estimates.</li>
<li>When correlations are high, listwise deletion becomes more efficient.</li>
</ul>
</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Yields biased estimates under MAR conditions.</li>
<li>In small samples, covariance matrices might not be positive definite, rendering coefficient estimation infeasible.</li>
<li>Software implementation varies in how sample size is handled, potentially affecting standard errors.</li>
</ul>
<p><strong>Note</strong>: Carefully review software documentation to understand how sample size is treated, as this influences standard error calculations.</p>
<hr>
</div>
<div id="indicator-method-dummy-variable-adjustment" class="section level4" number="11.4.1.3">
<h4>
<span class="header-section-number">11.4.1.3</span> Indicator Method (Dummy Variable Adjustment)<a class="anchor" aria-label="anchor" href="#indicator-method-dummy-variable-adjustment"><i class="fas fa-link"></i></a>
</h4>
<p>Also known as the Missing Indicator Method, this approach introduces an additional variable to indicate missingness in the dataset.</p>
<p><strong>Implementation</strong>:</p>
<ol style="list-style-type: decimal">
<li>Create an indicator variable:</li>
</ol>
<p><span class="math display">\[
D =
\begin{cases}
1 &amp; \text{if data on } X \text{ are missing} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Modify the original variable to accommodate missingness:</li>
</ol>
<p><span class="math display">\[
X^* =
\begin{cases}
X &amp; \text{if data are available} \\
c &amp; \text{if data are missing}
\end{cases}
\]</span></p>
<p><strong>Note</strong>: A common choice for <span class="math inline">\(c\)</span> is the mean of <span class="math inline">\(X\)</span>.</p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>The coefficient of <span class="math inline">\(D\)</span> represents the difference in the expected value of <span class="math inline">\(Y\)</span> between cases with missing data and those without.</li>
<li>The coefficient of <span class="math inline">\(X^*\)</span> reflects the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> for cases with observed data.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Produces biased estimates of coefficients, even under MCAR conditions <span class="citation">(<a href="references.html#ref-jones1996indicator">Jones 1996</a>)</span>.</li>
<li>May lead to overinterpretation of the “missingness effect,” complicating model interpretation.</li>
</ul>
<hr>
</div>
<div id="advantages-and-limitations-of-basic-methods" class="section level4" number="11.4.1.4">
<h4>
<span class="header-section-number">11.4.1.4</span> Advantages and Limitations of Basic Methods<a class="anchor" aria-label="anchor" href="#advantages-and-limitations-of-basic-methods"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="25%">
<col width="37%">
<col width="37%">
</colgroup>
<thead><tr class="header">
<th><strong>Method</strong></th>
<th><strong>Advantages</strong></th>
<th><strong>Disadvantages</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Listwise Deletion</strong></td>
<td>Simple and universally applicable; unbiased under MCAR; robust in certain MAR scenarios.</td>
<td>Inefficient (larger standard errors); biased under MAR in many cases; discards potentially useful data.</td>
</tr>
<tr class="even">
<td><strong>Pairwise Deletion</strong></td>
<td>Utilizes all available data; efficient under MCAR with low correlations; avoids discarding all cases.</td>
<td>Biased under MAR; prone to non-positive-definite covariance matrices in small samples.</td>
</tr>
<tr class="odd">
<td><strong>Indicator Method</strong></td>
<td>Simple implementation; explicitly models missingness effect.</td>
<td>Biased even under MCAR; complicates interpretation; may not reflect true underlying relationships.</td>
</tr>
</tbody>
</table></div>
</div>
</div>
<div id="single-imputation-techniques" class="section level3" number="11.4.2">
<h3>
<span class="header-section-number">11.4.2</span> Single Imputation Techniques<a class="anchor" aria-label="anchor" href="#single-imputation-techniques"><i class="fas fa-link"></i></a>
</h3>
<p>Single imputation methods replace missing data with a single value, generating a complete dataset that can be analyzed using standard techniques. While convenient, single imputation generally underestimates variability and risks biasing results.</p>
<hr>
<div id="deterministic-methods" class="section level4" number="11.4.2.1">
<h4>
<span class="header-section-number">11.4.2.1</span> Deterministic Methods<a class="anchor" aria-label="anchor" href="#deterministic-methods"><i class="fas fa-link"></i></a>
</h4>
<div id="mean-median-mode-imputation" class="section level5" number="11.4.2.1.1">
<h5>
<span class="header-section-number">11.4.2.1.1</span> Mean, Median, Mode Imputation<a class="anchor" aria-label="anchor" href="#mean-median-mode-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>This method replaces missing values with the mean, median, or mode of the observed data.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simplicity and ease of implementation.</li>
<li>Useful for quick exploratory data analysis.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>
<strong>Bias in Variances and Relationships</strong>: Mean imputation reduces variance and disrupts relationships among variables, leading to biased estimates of variances and covariances <span class="citation">(<a href="references.html#ref-haitovsky1968missing">Haitovsky 1968</a>)</span>.</li>
<li>
<strong>Underestimated Standard Errors</strong>: Results in overly optimistic conclusions and increased risk of Type I errors.</li>
<li>
<strong>Dependency Structure Ignored</strong>: Particularly problematic in high-dimensional data, as it fails to capture dependencies among features.</li>
</ul>
</div>
<div id="forward-and-backward-filling-time-series-contexts" class="section level5" number="11.4.2.1.2">
<h5>
<span class="header-section-number">11.4.2.1.2</span> Forward and Backward Filling (Time Series Contexts)<a class="anchor" aria-label="anchor" href="#forward-and-backward-filling-time-series-contexts"><i class="fas fa-link"></i></a>
</h5>
<p>Used in time series analysis, this method replaces missing values using the preceding (forward filling) or succeeding (backward filling) values.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and preserves temporal ordering.</li>
<li>Suitable for datasets where adjacent values are strongly correlated.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Biased if missingness spans long gaps or occurs systematically.</li>
<li>Cannot capture trends or changes in the underlying process.</li>
</ul>
<hr>
</div>
</div>
<div id="statistical-prediction-models" class="section level4" number="11.4.2.2">
<h4>
<span class="header-section-number">11.4.2.2</span> Statistical Prediction Models<a class="anchor" aria-label="anchor" href="#statistical-prediction-models"><i class="fas fa-link"></i></a>
</h4>
<div id="linear-regression-imputation" class="section level5" number="11.4.2.2.1">
<h5>
<span class="header-section-number">11.4.2.2.1</span> Linear Regression Imputation<a class="anchor" aria-label="anchor" href="#linear-regression-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>Missing values in a variable are imputed based on a linear regression model using observed values of other variables.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Preserves relationships between variables.</li>
<li>More sophisticated than mean or median imputation.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Assumes linear relationships, which may not hold in all datasets.</li>
<li>Fails to capture variability, leading to downwardly biased standard errors.</li>
</ul>
</div>
<div id="logistic-regression-for-categorical-variables" class="section level5" number="11.4.2.2.2">
<h5>
<span class="header-section-number">11.4.2.2.2</span> Logistic Regression for Categorical Variables<a class="anchor" aria-label="anchor" href="#logistic-regression-for-categorical-variables"><i class="fas fa-link"></i></a>
</h5>
<p>Similar to linear regression imputation but used for categorical variables. The missing category is predicted using a logistic regression model.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Useful for binary or multinomial categorical data.</li>
<li>Preserves relationships with other variables.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Assumes the underlying logistic model is appropriate.</li>
<li>Does not account for uncertainty in the imputed values.</li>
</ul>
<hr>
</div>
</div>
<div id="non-parametric-methods" class="section level4" number="11.4.2.3">
<h4>
<span class="header-section-number">11.4.2.3</span> Non-Parametric Methods<a class="anchor" aria-label="anchor" href="#non-parametric-methods"><i class="fas fa-link"></i></a>
</h4>
<div id="hot-deck-imputation" class="section level5" number="11.4.2.3.1">
<h5>
<span class="header-section-number">11.4.2.3.1</span> Hot Deck Imputation<a class="anchor" aria-label="anchor" href="#hot-deck-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>Hot Deck Imputation is a method of handling missing data where missing values are replaced with observed values from “donor” cases that are similar in other characteristics. This technique has been widely used in survey data, including by organizations like the U.S. Census Bureau, due to its flexibility and ability to maintain observed data distributions.</p>
<p><strong>Advantages of Hot Deck Imputation</strong></p>
<ul>
<li>
<strong>Retains observed data distributions</strong>: Since missing values are imputed using actual observed data, the overall distribution remains realistic.</li>
<li>
<strong>Flexible</strong>: This method is applicable to both categorical and continuous variables.</li>
<li>
<strong>Constrained imputations</strong>: Imputed values are always feasible, as they come from observed cases.</li>
<li>
<strong>Adds variability</strong>: By randomly selecting donors, this method introduces variability, which can aid in robust standard error estimation.</li>
</ul>
<p><strong>Disadvantages of Hot Deck Imputation</strong></p>
<ul>
<li>
<strong>Sensitivity to similarity definitions</strong>: The quality of imputed values depends on the criteria used to define similarity between cases.</li>
<li>
<strong>Computational intensity</strong>: Identifying similar cases and randomly selecting donors can be computationally expensive, especially for large datasets.</li>
<li>
<strong>Subjectivity</strong>: Deciding how to define “similar” can introduce subjectivity or bias.</li>
</ul>
<p><strong>Algorithm for Hot Deck Imputation</strong></p>
<p>Let <span class="math inline">\(n_1\)</span> represent the number of cases with complete data on the variable <span class="math inline">\(Y\)</span>, and <span class="math inline">\(n_0\)</span> represent the number of cases with missing data on <span class="math inline">\(Y\)</span>. The steps are as follows:</p>
<ol style="list-style-type: decimal">
<li>From the <span class="math inline">\(n_1\)</span> cases with complete data, take a random sample (with replacement) of <span class="math inline">\(n_1\)</span> cases.</li>
<li>From this sampled pool, take another random sample (with replacement) of size <span class="math inline">\(n_0\)</span>.</li>
<li>Assign the values from the sampled <span class="math inline">\(n_0\)</span> cases to the cases with missing data in <span class="math inline">\(Y\)</span>.</li>
<li>Repeat this process for every variable in the dataset.</li>
<li>For multiple imputation, repeat the above four steps multiple times to create multiple imputed datasets.</li>
</ol>
<p><strong>Variations and Considerations</strong></p>
<ul>
<li>
<strong>Skipping Step 1</strong>: If Step 1 is skipped, the variability of imputed values is reduced. This approach might not fully account for the uncertainty in missing data, which can underestimate standard errors.</li>
<li>
<strong>Defining similarity</strong>: A major challenge in this method is deciding what constitutes “similarity” between cases. Common approaches include matching based on distance metrics (e.g., Euclidean distance) or grouping cases by strata or clusters.</li>
</ul>
<p><strong>Practical Example</strong></p>
<p>The U.S. Census Bureau employs an approximate Bayesian bootstrap variation of Hot Deck Imputation. In this approach:</p>
<ul>
<li><p>Similar cases are identified based on shared characteristics or grouping variables.</p></li>
<li><p>A randomly chosen value from a similar individual in the sample is used to replace the missing value.</p></li>
</ul>
<p>This method ensures imputed values are plausible while incorporating variability.</p>
<p><strong>Key Notes</strong></p>
<ul>
<li>
<strong>Good aspects</strong>:
<ul>
<li>Imputed values are constrained to observed possibilities.</li>
<li>Random selection introduces variability, helpful for multiple imputation scenarios.</li>
</ul>
</li>
<li>
<strong>Challenges</strong>:
<ul>
<li>Defining and operationalizing “similarity” remains a critical step in applying this method effectively.</li>
</ul>
</li>
</ul>
<p>Below is an example code snippet illustrating Hot Deck Imputation in R:</p>
<div class="sourceCode" id="cb348"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://hbiostat.org/R/Hmisc/">Hmisc</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example dataset with missing values</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  ID <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,</span>
<span>  Age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">25</span>, <span class="fl">30</span>, <span class="cn">NA</span>, <span class="fl">40</span>, <span class="cn">NA</span>, <span class="fl">50</span>, <span class="fl">60</span>, <span class="cn">NA</span>, <span class="fl">70</span>, <span class="fl">80</span><span class="op">)</span>,</span>
<span>  Gender <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"M"</span>, <span class="st">"F"</span>, <span class="st">"F"</span>, <span class="st">"M"</span>, <span class="st">"M"</span>, <span class="st">"F"</span>, <span class="st">"M"</span>, <span class="st">"F"</span>, <span class="st">"M"</span>, <span class="st">"F"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform Hot Deck Imputation using Hmisc::impute</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">Age_imputed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">Age</span>, <span class="st">"random"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the imputed dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="co">#&gt;    ID Age Gender Age_imputed</span></span>
<span><span class="co">#&gt; 1   1  25      M          25</span></span>
<span><span class="co">#&gt; 2   2  30      F          30</span></span>
<span><span class="co">#&gt; 3   3  NA      F          25</span></span>
<span><span class="co">#&gt; 4   4  40      M          40</span></span>
<span><span class="co">#&gt; 5   5  NA      M          70</span></span>
<span><span class="co">#&gt; 6   6  50      F          50</span></span>
<span><span class="co">#&gt; 7   7  60      M          60</span></span>
<span><span class="co">#&gt; 8   8  NA      F          25</span></span>
<span><span class="co">#&gt; 9   9  70      M          70</span></span>
<span><span class="co">#&gt; 10 10  80      F          80</span></span></code></pre></div>
<p>This code randomly imputes missing values in the <code>Age</code> column based on observed data using the <code>Hmisc</code> package’s <code>impute</code> function.</p>
</div>
<div id="cold-deck-imputation" class="section level5" number="11.4.2.3.2">
<h5>
<span class="header-section-number">11.4.2.3.2</span> Cold Deck Imputation<a class="anchor" aria-label="anchor" href="#cold-deck-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>Cold Deck Imputation is a systematic variant of Hot Deck Imputation where the donor pool is predefined. Instead of selecting donors dynamically from within the same dataset, Cold Deck Imputation relies on an external reference dataset, such as historical data or other high-quality external sources.</p>
<p><strong>Advantages of Cold Deck Imputation</strong></p>
<ul>
<li>
<strong>Utilizes high-quality external data</strong>: This method is particularly useful when reliable external reference datasets are available, allowing for accurate and consistent imputations.</li>
<li>
<strong>Consistency</strong>: If the same donor pool is used across multiple datasets, imputations remain consistent, which can be advantageous in longitudinal studies or standardized processes.</li>
</ul>
<p><strong>Disadvantages of Cold Deck Imputation</strong></p>
<ul>
<li>
<strong>Lack of adaptability</strong>: External data may not adequately reflect the unique characteristics or variability of the current dataset.</li>
<li>
<strong>Potential for systematic bias</strong>: If the donor pool is significantly different from the target dataset, imputations may introduce bias.</li>
<li>
<strong>Reduces variability</strong>: Unlike Hot Deck Imputation, Cold Deck Imputation systematically selects values, which removes random variation. This can affect the estimation of standard errors and other inferential statistics.</li>
</ul>
<p><strong>Key Characteristics</strong></p>
<ul>
<li>
<strong>Systematic Selection</strong>: Cold Deck Imputation selects donor values systematically based on predefined rules or matching criteria, rather than using random sampling.</li>
<li>
<strong>External Donor Pool</strong>: Donors are typically drawn from a separate dataset or historical records.</li>
</ul>
<p><strong>Algorithm for Cold Deck Imputation</strong></p>
<ol style="list-style-type: decimal">
<li>Identify an external reference dataset or predefined donor pool.</li>
<li>Define the matching criteria to find “similar” cases between the donor pool and the current dataset (e.g., based on covariates or stratification).</li>
<li>Systematically assign values from the donor pool to missing values in the current dataset based on the matching criteria.</li>
<li>Repeat the process for each variable with missing data.</li>
</ol>
<p><strong>Practical Considerations</strong></p>
<ul>
<li>Cold Deck Imputation works well when external data closely resemble the target dataset. However, when there are significant differences in distributions or relationships between variables, imputations may be biased or unrealistic.<br>
</li>
<li>This method is less useful for datasets without access to reliable external reference data.</li>
</ul>
<p>Suppose we have a current dataset with missing values and a historical dataset with similar variables. The following example demonstrates how Cold Deck Imputation can be implemented:</p>
<div class="sourceCode" id="cb349"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Current dataset with missing values</span></span>
<span><span class="va">current_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  ID <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,</span>
<span>  Age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">25</span>, <span class="fl">30</span>, <span class="cn">NA</span>, <span class="fl">45</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>  Gender <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"M"</span>, <span class="st">"F"</span>, <span class="st">"F"</span>, <span class="st">"M"</span>, <span class="st">"M"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># External reference dataset (donor pool)</span></span>
<span><span class="va">reference_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  Age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">28</span>, <span class="fl">35</span>, <span class="fl">42</span>, <span class="fl">50</span><span class="op">)</span>,</span>
<span>  Gender <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"M"</span>, <span class="st">"F"</span>, <span class="st">"F"</span>, <span class="st">"M"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform Cold Deck Imputation</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define a matching function to find closest donor</span></span>
<span><span class="va">impute_cold_deck</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">missing_row</span>, <span class="va">reference_data</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Filter donors with the same gender</span></span>
<span>  <span class="va">possible_donors</span> <span class="op">&lt;-</span> <span class="va">reference_data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">Gender</span> <span class="op">==</span> <span class="va">missing_row</span><span class="op">$</span><span class="va">Gender</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Return the mean age of matching donors as an example of systematic imputation</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">possible_donors</span><span class="op">$</span><span class="va">Age</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Apply Cold Deck Imputation to the missing rows</span></span>
<span><span class="va">current_data</span> <span class="op">&lt;-</span> <span class="va">current_data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rowwise.html">rowwise</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>    Age_imputed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">Age</span><span class="op">)</span>,</span>
<span>      <span class="fu">impute_cold_deck</span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/deprec-context.html">cur_data</a></span><span class="op">(</span><span class="op">)</span>, <span class="va">reference_data</span><span class="op">)</span>,</span>
<span>      <span class="va">Age</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the imputed dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">current_data</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 5 × 4</span></span>
<span><span class="co">#&gt; # Rowwise: </span></span>
<span><span class="co">#&gt;      ID   Age Gender Age_imputed</span></span>
<span><span class="co">#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1     1    25 M             25  </span></span>
<span><span class="co">#&gt; 2     2    30 F             30  </span></span>
<span><span class="co">#&gt; 3     3    NA F             38.8</span></span>
<span><span class="co">#&gt; 4     4    45 M             45  </span></span>
<span><span class="co">#&gt; 5     5    NA M             38.8</span></span></code></pre></div>
<p><strong>Comparison to Hot Deck Imputation</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="25%">
<col width="37%">
<col width="37%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Hot Deck Imputation</th>
<th>Cold Deck Imputation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Donor Pool</strong></td>
<td>Internal (within the dataset)</td>
<td>External (predefined dataset)</td>
</tr>
<tr class="even">
<td><strong>Selection</strong></td>
<td>Random</td>
<td>Systematic</td>
</tr>
<tr class="odd">
<td><strong>Variability</strong></td>
<td>Retained</td>
<td>Reduced</td>
</tr>
<tr class="even">
<td><strong>Bias Potential</strong></td>
<td>Lower</td>
<td>Higher (if donor pool differs)</td>
</tr>
</tbody>
</table></div>
<p>This method suits situations where external reference datasets are trusted and representative. However, careful consideration is required to ensure alignment between the donor pool and the target dataset to avoid systematic biases.</p>
</div>
<div id="random-draw-from-observed-distribution" class="section level5" number="11.4.2.3.3">
<h5>
<span class="header-section-number">11.4.2.3.3</span> Random Draw from Observed Distribution<a class="anchor" aria-label="anchor" href="#random-draw-from-observed-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>This imputation method replaces missing values by randomly sampling from the observed distribution of the variable with missing data. It is a simple, non-parametric approach that retains the variability of the original data.</p>
<p><strong>Advantages</strong></p>
<ul>
<li>
<strong>Preserves variability</strong>:
<ul>
<li>By randomly drawing values from the observed data, this method ensures that the imputed values reflect the inherent variability of the variable.</li>
</ul>
</li>
<li>
<strong>Computational simplicity</strong>:
<ul>
<li>The process is straightforward and does not require model fitting or complex calculations.</li>
</ul>
</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>
<strong>Ignores relationships among variables</strong>:
<ul>
<li>Since the imputation is based solely on the observed distribution of the variable, it does not consider relationships or dependencies with other variables.</li>
</ul>
</li>
<li>
<strong>May not align with trends</strong>:
<ul>
<li>Imputed values are random and may fail to align with patterns or trends present in the data, such as time series structures or interactions.</li>
</ul>
</li>
</ul>
<p><strong>Steps in Random Draw Imputation</strong></p>
<ol style="list-style-type: decimal">
<li>Identify the observed (non-missing) values of the variable.</li>
<li>For each missing value, randomly sample one value from the observed distribution with or without replacement.</li>
<li>Replace the missing value with the randomly sampled value.</li>
</ol>
<p>The following example demonstrates how to use random draw imputation to fill in missing values:</p>
<div class="sourceCode" id="cb350"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example dataset with missing values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  ID <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,</span>
<span>  Value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">20</span>, <span class="cn">NA</span>, <span class="fl">30</span>, <span class="fl">40</span>, <span class="cn">NA</span>, <span class="fl">50</span>, <span class="fl">60</span>, <span class="cn">NA</span>, <span class="fl">70</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform random draw imputation</span></span>
<span><span class="va">random_draw_impute</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span>, <span class="va">variable</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">observed_values</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="va">variable</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="va">variable</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">]</span> <span class="co"># Observed values</span></span>
<span>  <span class="va">data</span><span class="op">[[</span><span class="va">variable</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="va">variable</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">observed_values</span>, </span>
<span>                                                      <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="va">variable</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                                                      replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Apply the imputation</span></span>
<span><span class="va">imputed_data</span> <span class="op">&lt;-</span> <span class="fu">random_draw_impute</span><span class="op">(</span><span class="va">data</span>, variable <span class="op">=</span> <span class="st">"Value"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the imputed dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">imputed_data</span><span class="op">)</span></span>
<span><span class="co">#&gt;    ID Value</span></span>
<span><span class="co">#&gt; 1   1    10</span></span>
<span><span class="co">#&gt; 2   2    20</span></span>
<span><span class="co">#&gt; 3   3    70</span></span>
<span><span class="co">#&gt; 4   4    30</span></span>
<span><span class="co">#&gt; 5   5    40</span></span>
<span><span class="co">#&gt; 6   6    70</span></span>
<span><span class="co">#&gt; 7   7    50</span></span>
<span><span class="co">#&gt; 8   8    60</span></span>
<span><span class="co">#&gt; 9   9    30</span></span>
<span><span class="co">#&gt; 10 10    70</span></span></code></pre></div>
<p><strong>Considerations</strong></p>
<ul>
<li>
<p><strong>When to Use</strong>:</p>
<ul>
<li>This method is suitable for exploratory analysis or as a quick way to handle missing data in univariate contexts.</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Random draws may result in values that do not fit well in the broader context of the dataset, especially in cases where the variable has strong relationships with others.</li>
</ul>
</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="30%">
<col width="38%">
<col width="30%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Random Draw from Observed Distribution</th>
<th>Regression-Based Imputation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Complexity</strong></td>
<td>Simple</td>
<td>Moderate to High</td>
</tr>
<tr class="even">
<td><strong>Preserves Variability</strong></td>
<td>Yes</td>
<td>Limited in deterministic forms</td>
</tr>
<tr class="odd">
<td><strong>Considers Relationships</strong></td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><strong>Risk of Implausible Values</strong></td>
<td>Low (if observed values are plausible)</td>
<td>Moderate to High</td>
</tr>
</tbody>
</table></div>
<p>This method is a quick and computationally efficient way to address missing data but is best complemented by more sophisticated methods when relationships between variables are important.</p>
</div>
</div>
<div id="semi-parametric-methods" class="section level4" number="11.4.2.4">
<h4>
<span class="header-section-number">11.4.2.4</span> Semi-Parametric Methods<a class="anchor" aria-label="anchor" href="#semi-parametric-methods"><i class="fas fa-link"></i></a>
</h4>
<div id="predictive-mean-matching-pmm" class="section level5" number="11.4.2.4.1">
<h5>
<span class="header-section-number">11.4.2.4.1</span> Predictive Mean Matching (PMM)<a class="anchor" aria-label="anchor" href="#predictive-mean-matching-pmm"><i class="fas fa-link"></i></a>
</h5>
<p>Predictive Mean Matching (PMM) imputes missing values by finding observed values closest in predicted value (based on a regression model) to the missing data. The donor values are then used to fill in the gaps.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li><p>Maintains observed variability in the data.</p></li>
<li><p>Ensures imputed values are realistic since they are drawn from observed data.</p></li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li><p>Requires a suitable predictive model.</p></li>
<li><p>Computationally intensive for large datasets.</p></li>
</ul>
<p><strong>Steps for PMM</strong>:</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> (matrix of covariates) for the <span class="math inline">\(n_1\)</span> (non-missing cases) to estimate coefficients <span class="math inline">\(\hat{b}\)</span> and residual variance <span class="math inline">\(s^2\)</span>.</li>
<li>Draw from the posterior predictive distribution of residual variance: <span class="math display">\[s^2_{[1]} = \frac{(n_1-k)s^2}{\chi^2},\]</span> where <span class="math inline">\(\chi^2\)</span> is a random draw from <span class="math inline">\(\chi^2_{n_1-k}\)</span>.</li>
<li>Randomly sample from the posterior distribution of <span class="math inline">\(\hat{b}\)</span>: <span class="math display">\[b_{[1]} \sim MVN(\hat{b}, s^2_{[1]}(X'X)^{-1}).\]</span>
</li>
<li>Standardize residuals for <span class="math inline">\(n_1\)</span> cases: <span class="math display">\[e_i = \frac{y_i - \hat{b}x_i}{\sqrt{s^2(1-k/n_1)}}.\]</span>
</li>
<li>Randomly draw a sample (with replacement) of <span class="math inline">\(n_0\)</span> residuals from Step 4.</li>
<li>Calculate imputed values for <span class="math inline">\(n_0\)</span> missing cases: <span class="math display">\[y_i = b_{[1]}x_i + s_{[1]}e_i.\]</span>
</li>
<li>Repeat Steps 2–6 (except Step 4) to create multiple imputations.</li>
</ol>
<p><strong>Notes</strong>:</p>
<ul>
<li><p>PMM can handle heteroskedasticity</p></li>
<li><p>works for multiple variables, imputing each using all others as predictors.</p></li>
</ul>
<p><strong>Example</strong>:</p>
<p>Example from <a href="https://statisticsglobe.com/predictive-mean-matching-imputation-method/">Statistics Globe</a></p>
<div class="sourceCode" id="cb351"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span> <span class="co"># Seed</span></span>
<span><span class="va">N</span>  <span class="op">&lt;-</span> <span class="fl">100</span>                                    <span class="co"># Sample size</span></span>
<span><span class="va">y</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span>,<span class="op">-</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>                 <span class="co"># Target variable Y</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="va">y</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">0</span>, <span class="fl">50</span><span class="op">)</span><span class="op">)</span>              <span class="co"># Auxiliary variable 1</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">y</span> <span class="op">+</span> <span class="fl">0.25</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>,<span class="op">-</span><span class="fl">3</span>, <span class="fl">15</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Auxiliary variable 2</span></span>
<span><span class="va">x3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">0.1</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>           <span class="co"># Auxiliary variable 3</span></span>
<span><span class="co"># (categorical variable)</span></span>
<span><span class="va">x4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">0.02</span> <span class="op">*</span> <span class="va">y</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>   <span class="co"># Auxiliary variable 4 </span></span>
<span></span>
<span><span class="co"># Insert 20% missing data in Y</span></span>
<span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, <span class="fl">0.2</span><span class="op">)</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span>               </span>
<span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span>, <span class="va">x4</span><span class="op">)</span>         <span class="co"># Store data in dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="co"># First 6 rows of our data</span></span>
<span><span class="co">#&gt;    y x1  x2 x3 x4</span></span>
<span><span class="co">#&gt; 1 NA 28 -10  5  0</span></span>
<span><span class="co">#&gt; 2 NA 15  -2  2  1</span></span>
<span><span class="co">#&gt; 3  1 15 -12  6  1</span></span>
<span><span class="co">#&gt; 4  8 58  22 10  1</span></span>
<span><span class="co">#&gt; 5 NA 26 -12  7  0</span></span>
<span><span class="co">#&gt; 6 NA 19  36  5  1</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/amices/mice">"mice"</a></span><span class="op">)</span> <span class="co"># Load mice package</span></span>
<span></span>
<span><span class="co">##### Impute data via predictive mean matching (single imputation)#####</span></span>
<span></span>
<span><span class="va">imp_single</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data</span>, m <span class="op">=</span> <span class="fl">1</span>, method <span class="op">=</span> <span class="st">"pmm"</span><span class="op">)</span> <span class="co"># Impute missing values</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="va">data_imp_single</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_single</span><span class="op">)</span>         <span class="co"># Store imputed data</span></span>
<span><span class="co"># head(data_imp_single)</span></span>
<span></span>
<span><span class="co"># Since single imputation underestiamtes stnadard errors, </span></span>
<span><span class="co"># we use multiple imputaiton</span></span>
<span></span>
<span><span class="co">##### Predictive mean matching (multiple imputation) #####</span></span>
<span></span>
<span><span class="co"># Impute missing values multiple times</span></span>
<span><span class="va">imp_multi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data</span>, m <span class="op">=</span> <span class="fl">5</span>, method <span class="op">=</span> <span class="st">"pmm"</span><span class="op">)</span>  </span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   1   2  y</span></span>
<span><span class="co">#&gt;   1   3  y</span></span>
<span><span class="co">#&gt;   1   4  y</span></span>
<span><span class="co">#&gt;   1   5  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   2   2  y</span></span>
<span><span class="co">#&gt;   2   3  y</span></span>
<span><span class="co">#&gt;   2   4  y</span></span>
<span><span class="co">#&gt;   2   5  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   3   2  y</span></span>
<span><span class="co">#&gt;   3   3  y</span></span>
<span><span class="co">#&gt;   3   4  y</span></span>
<span><span class="co">#&gt;   3   5  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   4   2  y</span></span>
<span><span class="co">#&gt;   4   3  y</span></span>
<span><span class="co">#&gt;   4   4  y</span></span>
<span><span class="co">#&gt;   4   5  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="co">#&gt;   5   2  y</span></span>
<span><span class="co">#&gt;   5   3  y</span></span>
<span><span class="co">#&gt;   5   4  y</span></span>
<span><span class="co">#&gt;   5   5  y</span></span>
<span><span class="va">data_imp_multi_all</span> <span class="op">&lt;-</span></span>
<span>    <span class="co"># Store multiply imputed data</span></span>
<span>    <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_multi</span>,       </span>
<span>             <span class="st">"repeated"</span>,</span>
<span>             include <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data_imp_multi</span> <span class="op">&lt;-</span></span>
<span>    <span class="co"># Combine imputed Y and X1-X4 (for convenience)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">data_imp_multi_all</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span>, <span class="va">data</span><span class="op">[</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">data_imp_multi</span><span class="op">)</span></span>
<span><span class="co">#&gt;   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4</span></span>
<span><span class="co">#&gt; 1  NA  -1   6  -1  -3   3 28 -10  5  0</span></span>
<span><span class="co">#&gt; 2  NA -10  10   4   0   2 15  -2  2  1</span></span>
<span><span class="co">#&gt; 3   1   1   1   1   1   1 15 -12  6  1</span></span>
<span><span class="co">#&gt; 4   8   8   8   8   8   8 58  22 10  1</span></span>
<span><span class="co">#&gt; 5  NA   0  -1  -6   2   0 26 -12  7  0</span></span>
<span><span class="co">#&gt; 6  NA   4   0   3   3   3 19  36  5  1</span></span></code></pre></div>
<p>Example from <a href="https://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/">UCLA Statistical Consulting</a></p>
<div class="sourceCode" id="cb352"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/amices/mice">mice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/statistikat/VIM">VIM</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lattice.r-forge.r-project.org/">lattice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="co">## set observations to NA</span></span>
<span><span class="va">anscombe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">within</a></span><span class="op">(</span><span class="va">anscombe</span>, <span class="op">{</span></span>
<span>    <span class="va">y1</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span>    <span class="va">y4</span><span class="op">[</span><span class="fl">3</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span><span class="co">## view</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">anscombe</span><span class="op">)</span></span>
<span><span class="co">#&gt;   x1 x2 x3 x4   y1   y2    y3   y4</span></span>
<span><span class="co">#&gt; 1 10 10 10  8   NA 9.14  7.46 6.58</span></span>
<span><span class="co">#&gt; 2  8  8  8  8   NA 8.14  6.77 5.76</span></span>
<span><span class="co">#&gt; 3 13 13 13  8   NA 8.74 12.74   NA</span></span>
<span><span class="co">#&gt; 4  9  9  9  8 8.81 8.77  7.11   NA</span></span>
<span><span class="co">#&gt; 5 11 11 11  8 8.33 9.26  7.81   NA</span></span>
<span><span class="co">#&gt; 6 14 14 14  8 9.96 8.10  8.84 7.04</span></span>
<span></span>
<span><span class="co">## check missing data patterns</span></span>
<span><span class="fu"><a href="https://amices.org/mice/reference/md.pattern.html">md.pattern</a></span><span class="op">(</span><span class="va">anscombe</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-10-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt;   x1 x2 x3 x4 y2 y3 y1 y4  
#&gt; 6  1  1  1  1  1  1  1  1 0
#&gt; 2  1  1  1  1  1  1  1  0 1
#&gt; 2  1  1  1  1  1  1  0  1 1
#&gt; 1  1  1  1  1  1  1  0  0 2
#&gt;    0  0  0  0  0  0  3  3 6

## Number of observations per patterns for all pairs of variables
p &lt;- md.pairs(anscombe)
p 
#&gt; $rr
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1 11 11 11 11  8 11 11  8
#&gt; x2 11 11 11 11  8 11 11  8
#&gt; x3 11 11 11 11  8 11 11  8
#&gt; x4 11 11 11 11  8 11 11  8
#&gt; y1  8  8  8  8  8  8  8  6
#&gt; y2 11 11 11 11  8 11 11  8
#&gt; y3 11 11 11 11  8 11 11  8
#&gt; y4  8  8  8  8  6  8  8  8
#&gt; 
#&gt; $rm
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  3  0  0  3
#&gt; x2  0  0  0  0  3  0  0  3
#&gt; x3  0  0  0  0  3  0  0  3
#&gt; x4  0  0  0  0  3  0  0  3
#&gt; y1  0  0  0  0  0  0  0  2
#&gt; y2  0  0  0  0  3  0  0  3
#&gt; y3  0  0  0  0  3  0  0  3
#&gt; y4  0  0  0  0  2  0  0  0
#&gt; 
#&gt; $mr
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  0  0  0  0
#&gt; x2  0  0  0  0  0  0  0  0
#&gt; x3  0  0  0  0  0  0  0  0
#&gt; x4  0  0  0  0  0  0  0  0
#&gt; y1  3  3  3  3  0  3  3  2
#&gt; y2  0  0  0  0  0  0  0  0
#&gt; y3  0  0  0  0  0  0  0  0
#&gt; y4  3  3  3  3  2  3  3  0
#&gt; 
#&gt; $mm
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  0  0  0  0
#&gt; x2  0  0  0  0  0  0  0  0
#&gt; x3  0  0  0  0  0  0  0  0
#&gt; x4  0  0  0  0  0  0  0  0
#&gt; y1  0  0  0  0  3  0  0  1
#&gt; y2  0  0  0  0  0  0  0  0
#&gt; y3  0  0  0  0  0  0  0  0
#&gt; y4  0  0  0  0  1  0  0  3</code></pre>
<ul>
<li>
<code>rr</code> = number of observations where both pairs of values are observed</li>
<li>
<code>rm</code> = the number of observations where both variables are missing values</li>
<li>
<code>mr</code> = the number of observations where the first variable’s value (e.g. the row variable) is observed and second (or column) variable is missing</li>
<li>
<code>mm</code> = the number of observations where the second variable’s value (e.g. the col variable) is observed and first (or row) variable is missing</li>
</ul>
<div class="sourceCode" id="cb354"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Margin plot of y1 and y4</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/VIM/man/marginplot.html">marginplot</a></span><span class="op">(</span><span class="va">anscombe</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">8</span><span class="op">)</span><span class="op">]</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"orange"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-11-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb355"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## 5 imputations for all missing values</span></span>
<span><span class="va">imp1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">anscombe</span>, m <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y1  y4</span></span>
<span><span class="co">#&gt;   1   2  y1  y4</span></span>
<span><span class="co">#&gt;   1   3  y1  y4</span></span>
<span><span class="co">#&gt;   1   4  y1  y4</span></span>
<span><span class="co">#&gt;   1   5  y1  y4</span></span>
<span><span class="co">#&gt;   2   1  y1  y4</span></span>
<span><span class="co">#&gt;   2   2  y1  y4</span></span>
<span><span class="co">#&gt;   2   3  y1  y4</span></span>
<span><span class="co">#&gt;   2   4  y1  y4</span></span>
<span><span class="co">#&gt;   2   5  y1  y4</span></span>
<span><span class="co">#&gt;   3   1  y1  y4</span></span>
<span><span class="co">#&gt;   3   2  y1  y4</span></span>
<span><span class="co">#&gt;   3   3  y1  y4</span></span>
<span><span class="co">#&gt;   3   4  y1  y4</span></span>
<span><span class="co">#&gt;   3   5  y1  y4</span></span>
<span><span class="co">#&gt;   4   1  y1  y4</span></span>
<span><span class="co">#&gt;   4   2  y1  y4</span></span>
<span><span class="co">#&gt;   4   3  y1  y4</span></span>
<span><span class="co">#&gt;   4   4  y1  y4</span></span>
<span><span class="co">#&gt;   4   5  y1  y4</span></span>
<span><span class="co">#&gt;   5   1  y1  y4</span></span>
<span><span class="co">#&gt;   5   2  y1  y4</span></span>
<span><span class="co">#&gt;   5   3  y1  y4</span></span>
<span><span class="co">#&gt;   5   4  y1  y4</span></span>
<span><span class="co">#&gt;   5   5  y1  y4</span></span>
<span></span>
<span><span class="co">## linear regression for each imputed data set - 5 regression are run</span></span>
<span><span class="va">fitm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">imp1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y1</span> <span class="op">~</span> <span class="va">y4</span> <span class="op">+</span> <span class="va">x1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fitm</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 15 × 6</span></span>
<span><span class="co">#&gt;    term        estimate std.error statistic p.value  nobs</span></span>
<span><span class="co">#&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;</span></span>
<span><span class="co">#&gt;  1 (Intercept)    7.33      2.44       3.01  0.0169    11</span></span>
<span><span class="co">#&gt;  2 y4            -0.416     0.223     -1.86  0.0996    11</span></span>
<span><span class="co">#&gt;  3 x1             0.371     0.141      2.63  0.0302    11</span></span>
<span><span class="co">#&gt;  4 (Intercept)    7.27      2.90       2.51  0.0365    11</span></span>
<span><span class="co">#&gt;  5 y4            -0.435     0.273     -1.59  0.150     11</span></span>
<span><span class="co">#&gt;  6 x1             0.387     0.160      2.41  0.0422    11</span></span>
<span><span class="co">#&gt;  7 (Intercept)    6.54      2.80       2.33  0.0479    11</span></span>
<span><span class="co">#&gt;  8 y4            -0.322     0.255     -1.26  0.243     11</span></span>
<span><span class="co">#&gt;  9 x1             0.362     0.156      2.32  0.0491    11</span></span>
<span><span class="co">#&gt; 10 (Intercept)    5.93      3.08       1.92  0.0907    11</span></span>
<span><span class="co">#&gt; 11 y4            -0.286     0.282     -1.02  0.339     11</span></span>
<span><span class="co">#&gt; 12 x1             0.418     0.176      2.37  0.0451    11</span></span>
<span><span class="co">#&gt; 13 (Intercept)    8.16      2.67       3.05  0.0158    11</span></span>
<span><span class="co">#&gt; 14 y4            -0.489     0.251     -1.95  0.0867    11</span></span>
<span><span class="co">#&gt; 15 x1             0.326     0.151      2.17  0.0622    11</span></span>
<span></span>
<span><span class="co">## pool coefficients and standard errors across all 5 regression models</span></span>
<span><span class="fu"><a href="https://amices.org/mice/reference/pool.html">pool</a></span><span class="op">(</span><span class="va">fitm</span><span class="op">)</span></span>
<span><span class="co">#&gt; Class: mipo    m = 5 </span></span>
<span><span class="co">#&gt;          term m   estimate       ubar           b          t dfcom       df</span></span>
<span><span class="co">#&gt; 1 (Intercept) 5  7.0445966 7.76794670 0.719350800 8.63116766     8 5.805314</span></span>
<span><span class="co">#&gt; 2          y4 5 -0.3896685 0.06634920 0.006991497 0.07473900     8 5.706243</span></span>
<span><span class="co">#&gt; 3          x1 5  0.3727865 0.02473847 0.001134293 0.02609962     8 6.178032</span></span>
<span><span class="co">#&gt;          riv     lambda       fmi</span></span>
<span><span class="co">#&gt; 1 0.11112601 0.10001207 0.3044313</span></span>
<span><span class="co">#&gt; 2 0.12644909 0.11225460 0.3161877</span></span>
<span><span class="co">#&gt; 3 0.05502168 0.05215218 0.2586992</span></span>
<span></span>
<span><span class="co">## output parameter estimates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/pool.html">pool</a></span><span class="op">(</span><span class="va">fitm</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;          term   estimate std.error statistic       df    p.value</span></span>
<span><span class="co">#&gt; 1 (Intercept)  7.0445966 2.9378849  2.397846 5.805314 0.05483678</span></span>
<span><span class="co">#&gt; 2          y4 -0.3896685 0.2733843 -1.425350 5.706243 0.20638512</span></span>
<span><span class="co">#&gt; 3          x1  0.3727865 0.1615538  2.307508 6.178032 0.05923999</span></span></code></pre></div>
</div>
<div id="stochastic-imputation" class="section level5" number="11.4.2.4.2">
<h5>
<span class="header-section-number">11.4.2.4.2</span> Stochastic Imputation<a class="anchor" aria-label="anchor" href="#stochastic-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>Stochastic Imputation is an enhancement of regression imputation that introduces randomness into the imputation process by adding a random residual to the predicted values from a regression model. This approach aims to retain the variability of the original data while reducing the bias introduced by deterministic regression imputation.</p>
<p>Stochastic Imputation can be described as:</p>
<p><span class="math display">\[
\text{Imputed Value} = \text{Predicted Value (from regression)} + \text{Random Residual}
\]</span></p>
<p>This method is commonly used as a foundation for multiple imputation techniques.</p>
<p><strong>Advantages of Stochastic Imputation</strong></p>
<ul>
<li>
<strong>Retains all the benefits of regression imputation</strong>:
<ul>
<li>Preserves relationships between variables in the dataset.</li>
<li>Utilizes information from observed data to inform imputations.</li>
</ul>
</li>
<li>
<strong>Introduces randomness</strong>:
<ul>
<li>Adds variability by including a random residual term, making imputed values more realistic and better representing the uncertainty of missing data.</li>
</ul>
</li>
<li>
<strong>Supports multiple imputation</strong>:
<ul>
<li>By generating different random residuals for each iteration, it facilitates the creation of multiple plausible datasets for robust statistical analysis.</li>
</ul>
</li>
</ul>
<p><strong>Disadvantages of Stochastic Imputation</strong></p>
<ul>
<li>
<strong>Implausible values</strong>:
<ul>
<li>Depending on the random residuals, imputed values may fall outside the plausible range (e.g., negative values for variables like age or income).</li>
</ul>
</li>
<li>
<strong>Cannot handle heteroskedasticity</strong>:
<ul>
<li>If the data exhibit heteroskedasticity (i.e., non-constant variance of residuals), the randomness added by stochastic imputation may not accurately reflect the underlying variability.</li>
</ul>
</li>
</ul>
<p><strong>Steps in Stochastic Imputation</strong></p>
<ol style="list-style-type: decimal">
<li>Fit a regression model using cases with complete data for the variable with missing values.</li>
<li>Predict missing values using the fitted model.</li>
<li>Generate random residuals based on the distribution of residuals from the regression model.</li>
<li>Add the random residuals to the predicted values to impute missing values.</li>
</ol>
<div class="sourceCode" id="cb356"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example dataset with missing values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10</span>, mean <span class="op">=</span> <span class="fl">50</span>, sd <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>  Y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">105</span>, <span class="fl">110</span>, <span class="cn">NA</span>, <span class="fl">120</span>, <span class="cn">NA</span>, <span class="fl">130</span>, <span class="fl">135</span>, <span class="fl">140</span>, <span class="cn">NA</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform stochastic imputation</span></span>
<span><span class="va">stochastic_impute</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span>, <span class="va">predictor</span>, <span class="va">target</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Subset data with complete cases</span></span>
<span>  <span class="va">complete_data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="va">target</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Fit a regression model</span></span>
<span>  <span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html">as.formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">target</span>, <span class="st">"~"</span>, <span class="va">predictor</span><span class="op">)</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">complete_data</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Predict missing values</span></span>
<span>  <span class="va">missing_data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="va">target</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="op">]</span></span>
<span>  <span class="va">predictions</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model</span>, newdata <span class="op">=</span> <span class="va">missing_data</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Add random residuals</span></span>
<span>  <span class="va">residual_sd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">residuals</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  <span class="va">stochastic_values</span> <span class="op">&lt;-</span> <span class="va">predictions</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">predictions</span><span class="op">)</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="va">residual_sd</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Impute missing values</span></span>
<span>  <span class="va">data</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="va">target</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="va">target</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">stochastic_values</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Apply stochastic imputation</span></span>
<span><span class="va">imputed_data</span> <span class="op">&lt;-</span> <span class="fu">stochastic_impute</span><span class="op">(</span><span class="va">data</span>, predictor <span class="op">=</span> <span class="st">"X"</span>, target <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the imputed dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">imputed_data</span><span class="op">)</span></span></code></pre></div>
<p>Notes</p>
<ul>
<li><p><strong>Multiple Imputation</strong>: Most multiple imputation methods are extensions of stochastic regression imputation. By repeating the imputation process with different random seeds, multiple datasets can be generated to account for uncertainty in the imputed values.</p></li>
<li><p><strong>Dealing with Implausible Values</strong>: Additional constraints or transformations (e.g., truncating imputed values to a plausible range) may be necessary to address the issue of implausible values.</p></li>
</ul>
<p><strong>Comparison to Deterministic Regression Imputation</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="34%">
<col width="40%">
<col width="25%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Deterministic Regression Imputation</th>
<th>Stochastic Imputation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Randomness</strong></td>
<td>None</td>
<td>Adds random residuals</td>
</tr>
<tr class="even">
<td><strong>Preserves Variability</strong></td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td><strong>Use in Multiple Imputation</strong></td>
<td>Limited</td>
<td>Well-suited</td>
</tr>
<tr class="even">
<td><strong>Bias Potential</strong></td>
<td>Higher</td>
<td>Lower</td>
</tr>
</tbody>
</table></div>
<div class="sourceCode" id="cb357"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Income data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>                              <span class="co"># Set seed</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span>                                    <span class="co"># Sample size</span></span>
<span></span>
<span><span class="va">income</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">0</span>, <span class="fl">500</span><span class="op">)</span><span class="op">)</span>            <span class="co"># Create some synthetic income data</span></span>
<span><span class="va">income</span><span class="op">[</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">income</span><span class="op">[</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="va">income</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1000</span>, <span class="fl">1500</span><span class="op">)</span>          <span class="co"># Auxiliary variables</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="va">income</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>,<span class="op">-</span><span class="fl">5000</span>, <span class="fl">2000</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Create 10% missingness in income</span></span>
<span><span class="va">income</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, <span class="fl">0.1</span><span class="op">)</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span></span>
<span><span class="va">data_inc_miss</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">income</span>, <span class="va">x1</span>, <span class="va">x2</span><span class="op">)</span></span></code></pre></div>
<p>Single stochastic regression imputation</p>
<div class="sourceCode" id="cb358"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_inc_sri</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_inc_miss</span>, method <span class="op">=</span> <span class="st">"norm.nob"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  income</span></span>
<span><span class="co">#&gt;   2   1  income</span></span>
<span><span class="co">#&gt;   3   1  income</span></span>
<span><span class="co">#&gt;   4   1  income</span></span>
<span><span class="co">#&gt;   5   1  income</span></span>
<span><span class="va">data_inc_sri</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_inc_sri</span><span class="op">)</span></span></code></pre></div>
<p>Single predictive mean matching</p>
<div class="sourceCode" id="cb359"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_inc_pmm</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_inc_miss</span>, method <span class="op">=</span> <span class="st">"pmm"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  income</span></span>
<span><span class="co">#&gt;   2   1  income</span></span>
<span><span class="co">#&gt;   3   1  income</span></span>
<span><span class="co">#&gt;   4   1  income</span></span>
<span><span class="co">#&gt;   5   1  income</span></span>
<span><span class="va">data_inc_pmm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_inc_pmm</span><span class="op">)</span></span></code></pre></div>
<p>Stochastic regression imputation contains negative values</p>
<div class="sourceCode" id="cb360"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data_inc_sri</span><span class="op">$</span><span class="va">income</span><span class="op">[</span><span class="va">data_inc_sri</span><span class="op">$</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span></span>
<span><span class="co">#&gt;  [1]  -23.85404  -58.37790  -61.86396  -57.47909  -21.29221  -73.26549</span></span>
<span><span class="co">#&gt;  [7]  -61.76194  -42.45942 -351.02991 -317.69090</span></span>
<span><span class="co"># No values below 0</span></span>
<span><span class="va">data_inc_pmm</span><span class="op">$</span><span class="va">income</span><span class="op">[</span><span class="va">data_inc_pmm</span><span class="op">$</span><span class="va">income</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span> </span>
<span><span class="co">#&gt; numeric(0)</span></span></code></pre></div>
<p>Evidence for heteroskadastic data</p>
<div class="sourceCode" id="cb361"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Heteroscedastic data</span></span>
<span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>                             <span class="co"># Set seed</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>                                  <span class="co"># Sample size</span></span>
<span> </span>
<span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">sigma2</span> <span class="op">&lt;-</span> <span class="va">N</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sigma2</span><span class="op">)</span><span class="op">)</span></span>
<span> </span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">a</span> <span class="op">+</span> <span class="va">b</span> <span class="op">*</span> <span class="va">N</span> <span class="op">+</span> <span class="va">eps</span>                         <span class="co"># Heteroscedastic variable</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">30</span> <span class="op">*</span> <span class="va">N</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">]</span>, <span class="fl">1000</span>, <span class="fl">200</span><span class="op">)</span> <span class="co"># Correlated variable</span></span>
<span> </span>
<span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">]</span>, <span class="fl">1</span>, <span class="fl">0.3</span><span class="op">)</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span>   <span class="co"># 30% missing</span></span>
<span> </span>
<span><span class="va">data_het_miss</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x</span><span class="op">)</span></span></code></pre></div>
<p>Single stochastic regression imputation</p>
<div class="sourceCode" id="cb362"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_het_sri</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_het_miss</span>, method <span class="op">=</span> <span class="st">"norm.nob"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="va">data_het_sri</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_het_sri</span><span class="op">)</span></span></code></pre></div>
<p>Single predictive mean matching</p>
<div class="sourceCode" id="cb363"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">imp_het_pmm</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span><span class="va">data_het_miss</span>, method <span class="op">=</span> <span class="st">"pmm"</span>, m <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  iter imp variable</span></span>
<span><span class="co">#&gt;   1   1  y</span></span>
<span><span class="co">#&gt;   2   1  y</span></span>
<span><span class="co">#&gt;   3   1  y</span></span>
<span><span class="co">#&gt;   4   1  y</span></span>
<span><span class="co">#&gt;   5   1  y</span></span>
<span><span class="va">data_het_pmm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imp_het_pmm</span><span class="op">)</span></span></code></pre></div>
<p>Comparison between predictive mean matching and stochastic regression imputation</p>
<div class="sourceCode" id="cb364"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>                              <span class="co"># Both plots in one graphic</span></span>
<span></span>
<span><span class="co"># Plot of observed values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     <span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     main <span class="op">=</span> <span class="st">""</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"X"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span></span>
<span><span class="co"># Plot of missing values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>, <span class="va">data_het_sri</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>       col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Title of plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="st">"Stochastic Regression Imputation"</span>,        </span>
<span>      line <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Regression line</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">data_het_sri</span><span class="op">)</span>,                   </span>
<span>       col <span class="op">=</span> <span class="st">"#1b98e0"</span>, lwd <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Legend</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">"topleft"</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observed Values"</span>, <span class="st">"Imputed Values"</span>, <span class="st">"Regression Y ~ X"</span><span class="op">)</span>,</span>
<span>  pch <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>  lty <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="cn">NA</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"#1b98e0"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot of observed values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     <span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>     main <span class="op">=</span> <span class="st">""</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"X"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Y"</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Plot of missing values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>, <span class="va">data_het_pmm</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span>,</span>
<span>       col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Title of plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="st">"Predictive Mean Matching"</span>,</span>
<span>      line <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, <span class="va">data_het_pmm</span><span class="op">)</span>,</span>
<span>       col <span class="op">=</span> <span class="st">"#1b98e0"</span>, lwd <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Legend</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">"topleft"</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Observed Values"</span>, <span class="st">"Imputed Values"</span>, <span class="st">"Regression Y ~ X"</span><span class="op">)</span>,</span>
<span>  pch <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>  lty <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="cn">NA</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"#1b98e0"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/mtext.html">mtext</a></span><span class="op">(</span></span>
<span>  <span class="st">"Imputation of Heteroscedastic Data"</span>,</span>
<span>  <span class="co"># Main title of plot</span></span>
<span>  side <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  line <span class="op">=</span> <span class="op">-</span><span class="fl">1.5</span>,</span>
<span>  outer <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  cex <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-19-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="matrix-completion" class="section level4" number="11.4.2.5">
<h4>
<span class="header-section-number">11.4.2.5</span> Matrix Completion<a class="anchor" aria-label="anchor" href="#matrix-completion"><i class="fas fa-link"></i></a>
</h4>
<p>Matrix completion is a method used to impute missing data in a feature matrix while accounting for dependence between features. This approach leverages principal components to approximate the data matrix, a process referred to as <strong>matrix completion</strong> <span class="citation">(<a href="references.html#ref-james2013">James et al. 2013</a>, Sec 12.3)</span>.</p>
<p><strong>Problem Setup</strong></p>
<p>Consider an <span class="math inline">\(n \times p\)</span> feature matrix <span class="math inline">\(\mathbf{X}\)</span>, where the element <span class="math inline">\(x_{ij}\)</span> represents the value for the <span class="math inline">\(i\)</span>th observation and <span class="math inline">\(j\)</span>th feature. Some elements of <span class="math inline">\(\mathbf{X}\)</span> are missing, and we aim to impute these missing values.</p>
<p>Similar to the process described in <a href="multivariate-methods.html#principal-components">22.2</a>, the matrix <span class="math inline">\(\mathbf{X}\)</span> can be approximated using its leading principal components. Specifically, we consider <span class="math inline">\(M\)</span> principal components that minimize the following objective:</p>
<p><span class="math display">\[
\underset{\mathbf{A} \in \mathbb{R}^{n \times M}, \mathbf{B} \in \mathbb{R}^{p \times M}}{\operatorname{min}} \left\{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \right\}
\]</span></p>
<p>where <span class="math inline">\(\mathcal{O}\)</span> is the set of observed indices <span class="math inline">\((i,j)\)</span>, which is a subset of the total <span class="math inline">\(n \times p\)</span> pairs. Here: - <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(n \times M\)</span> matrix of principal component scores. - <span class="math inline">\(\mathbf{B}\)</span> is a <span class="math inline">\(p \times M\)</span> matrix of principal component loadings.</p>
<p><strong>Imputation of Missing Values</strong></p>
<p>After solving the minimization problem:</p>
<ol style="list-style-type: decimal">
<li>Missing observations <span class="math inline">\(x_{ij}\)</span> can be imputed using the formula: <span class="math display">\[
   \hat{x}_{ij} = \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}
   \]</span> where <span class="math inline">\(\hat{a}_{im}\)</span> and <span class="math inline">\(\hat{b}_{jm}\)</span> are the estimated elements of <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span>, respectively.</li>
<li>The leading <span class="math inline">\(M\)</span> principal component scores and loadings can be approximately recovered, as is done in complete data scenarios.</li>
</ol>
<p><strong>Iterative Algorithm</strong></p>
<p>The eigen-decomposition used in standard principal component analysis is not applicable here because of missing values. Instead, an iterative algorithm, as described in <span class="citation">(<a href="references.html#ref-james2013">James et al. 2013</a>, Alg 12.1)</span>, is employed:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Initialize the Complete Matrix</strong>: Construct an initial complete matrix <span class="math inline">\(\tilde{\mathbf{X}}\)</span> of dimension <span class="math inline">\(n \times p\)</span> where: <span class="math display">\[
\tilde{x}_{ij} =
\begin{cases}
x_{ij} &amp; \text{if } (i,j) \in \mathcal{O} \\
\bar{x}_j &amp; \text{if } (i,j) \notin \mathcal{O}
\end{cases}
\]</span> Here, <span class="math inline">\(\bar{x}_j\)</span> is the mean of the observed values for the <span class="math inline">\(j\)</span>th variable in the incomplete data matrix <span class="math inline">\(\mathbf{X}\)</span>. <span class="math inline">\(\mathcal{O}\)</span> indexes the observed elements of <span class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li>
<p><strong>Iterative Steps</strong>: Repeat the following steps until convergence:</p>
<ul>
<li><p><strong>Minimize the Objective</strong>: Solve the problem: <span class="math display">\[
\underset{\mathbf{A} \in R^{n \times M}, \mathbf{B} \in R^{p \times M}}{\operatorname{min}} \left\{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \right\}
\]</span> by computing the principal components of the current <span class="math inline">\(\tilde{\mathbf{X}}\)</span>.</p></li>
<li><p><strong>Update Missing Values</strong>: For each missing element <span class="math inline">\((i,j) \notin \mathcal{O}\)</span>, set: <span class="math display">\[
\tilde{x}_{ij} \leftarrow \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}
\]</span></p></li>
<li><p><strong>Recalculate the Objective</strong>: Compute the objective: <span class="math display">\[
\sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M \hat{a}_{im} \hat{b}_{jm})^2
\]</span></p></li>
</ul>
</li>
<li><p><strong>Return Imputed Values</strong>: Once the algorithm converges, return the estimated missing entries <span class="math inline">\(\tilde{x}_{ij}\)</span> for <span class="math inline">\((i,j) \notin \mathcal{O}\)</span>.</p></li>
</ol>
<p><strong>Key Considerations</strong></p>
<ul>
<li>This approach assumes that the missing data are missing at random (MAR).</li>
<li>Convergence criteria for the iterative algorithm often involve achieving a threshold for the change in the objective function or limiting the number of iterations.</li>
<li>The choice of <span class="math inline">\(M\)</span>, the number of principal components, can be guided by cross-validation or other model selection techniques.</li>
</ul>
</div>
<div id="comparison-of-single-imputation-techniques" class="section level4" number="11.4.2.6">
<h4>
<span class="header-section-number">11.4.2.6</span> Comparison of Single Imputation Techniques<a class="anchor" aria-label="anchor" href="#comparison-of-single-imputation-techniques"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="25%">
<col width="30%">
<col width="44%">
</colgroup>
<thead><tr class="header">
<th><strong>Method</strong></th>
<th><strong>Advantages</strong></th>
<th><strong>Disadvantages</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Mean, Median, Mode Imputation</strong></td>
<td>Simple, quick implementation.</td>
<td>Biased variances and covariances; ignores relationships among variables.</td>
</tr>
<tr class="even">
<td><strong>Forward/Backward Filling</strong></td>
<td>Preserves temporal ordering.</td>
<td>Biased for systematic gaps or long missing sequences.</td>
</tr>
<tr class="odd">
<td><strong>Linear Regression Imputation</strong></td>
<td>Preserves relationships among variables.</td>
<td>Fails to capture variability; assumes linearity.</td>
</tr>
<tr class="even">
<td><strong>Logistic Regression Imputation</strong></td>
<td>Handles categorical variables well.</td>
<td>Requires appropriate model assumptions; ignores variability.</td>
</tr>
<tr class="odd">
<td><strong>PMM</strong></td>
<td>Maintains variability; imputes realistic values.</td>
<td>Computationally intensive; requires a good predictive model.</td>
</tr>
<tr class="even">
<td><strong>Hot Deck Imputation</strong></td>
<td>Flexible; maintains data distribution.</td>
<td>Sensitive to donor selection; computationally demanding.</td>
</tr>
<tr class="odd">
<td><strong>Cold Deck Imputation</strong></td>
<td>Consistent across datasets with predefined donor pools.</td>
<td>Risk of bias if donor data are not representative.</td>
</tr>
<tr class="even">
<td><strong>Random Draw from Observed</strong></td>
<td>Simple; retains variability in data.</td>
<td>Does not preserve relationships among variables; random imputation may distort trends.</td>
</tr>
<tr class="odd">
<td><strong>Matrix Completion</strong></td>
<td>Captures dependencies; imputes structurally consistent values.</td>
<td>Computationally intensive; assumes principal components capture data relationships.</td>
</tr>
</tbody>
</table></div>
<p>Single imputation techniques are straightforward and accessible, but they often underestimate uncertainty and fail to fully leverage relationships among variables. These limitations make them less ideal for rigorous analyses compared to multiple imputation or model-based approaches.</p>
</div>
</div>
<div id="machine-learning-and-modern-approaches" class="section level3" number="11.4.3">
<h3>
<span class="header-section-number">11.4.3</span> Machine Learning and Modern Approaches<a class="anchor" aria-label="anchor" href="#machine-learning-and-modern-approaches"><i class="fas fa-link"></i></a>
</h3>
<div id="tree-based-methods" class="section level4" number="11.4.3.1">
<h4>
<span class="header-section-number">11.4.3.1</span> Tree-Based Methods<a class="anchor" aria-label="anchor" href="#tree-based-methods"><i class="fas fa-link"></i></a>
</h4>
<div id="random-forest-imputation-missforest" class="section level5" number="11.4.3.1.1">
<h5>
<span class="header-section-number">11.4.3.1.1</span> Random Forest Imputation (missForest)<a class="anchor" aria-label="anchor" href="#random-forest-imputation-missforest"><i class="fas fa-link"></i></a>
</h5>
<p>Random Forest Imputation uses an iterative process where a random forest model predicts missing values for one variable at a time, treating other variables as predictors. This process continues until convergence.</p>
<ul>
<li>
<strong>Mathematical Framework</strong>:
<ol style="list-style-type: decimal">
<li>For a variable <span class="math inline">\(X_j\)</span> with missing values, treat <span class="math inline">\(X_j\)</span> as the response variable.</li>
<li>Fit a random forest model <span class="math inline">\(f(X_{-j})\)</span> using the other variables <span class="math inline">\(X_{-j}\)</span> as predictors.</li>
<li>Predict missing values <span class="math inline">\(\hat{X}_j = f(X_{-j})\)</span>.</li>
<li>Repeat for all variables with missing data until imputed values stabilize.</li>
</ol>
</li>
<li>
<strong>Advantages</strong>:
<ul>
<li>Captures complex interactions and non-linearities.</li>
<li>Handles mixed data types seamlessly.</li>
</ul>
</li>
<li>
<strong>Limitations</strong>:
<ul>
<li>Computationally intensive for large datasets.</li>
<li>Sensitive to the quality of data relationships.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="gradient-boosting-machines-gbm" class="section level5" number="11.4.3.1.2">
<h5>
<span class="header-section-number">11.4.3.1.2</span> Gradient Boosting Machines (GBM)<a class="anchor" aria-label="anchor" href="#gradient-boosting-machines-gbm"><i class="fas fa-link"></i></a>
</h5>
<p>Gradient Boosting Machines iteratively build models to minimize loss functions. For imputation, missing values are treated as a target variable to be predicted.</p>
<ul>
<li><p><strong>Mathematical Framework</strong>: The GBM algorithm minimizes the loss function: <span class="math display">\[
  L = \sum_{i=1}^n \ell(y_i, f(x_i)),
  \]</span> where <span class="math inline">\(\ell\)</span> is the loss function (e.g., mean squared error), <span class="math inline">\(y_i\)</span> are observed values, and <span class="math inline">\(f(x_i)\)</span> are predictions.</p></li>
<li><p>Missing values are treated as the <span class="math inline">\(y_i\)</span> and predicted iteratively.</p></li>
<li>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Highly accurate predictions.</li>
<li>Captures variable importance.</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Overfitting risks.</li>
<li>Requires careful parameter tuning.</li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="neural-network-based-imputation" class="section level4" number="11.4.3.2">
<h4>
<span class="header-section-number">11.4.3.2</span> Neural Network-Based Imputation<a class="anchor" aria-label="anchor" href="#neural-network-based-imputation"><i class="fas fa-link"></i></a>
</h4>
<div id="autoencoders" class="section level5" number="11.4.3.2.1">
<h5>
<span class="header-section-number">11.4.3.2.1</span> Autoencoders<a class="anchor" aria-label="anchor" href="#autoencoders"><i class="fas fa-link"></i></a>
</h5>
<p>Autoencoders are unsupervised neural networks that compress and reconstruct data. Missing values are estimated during reconstruction.</p>
<ul>
<li>
<p><strong>Mathematical Framework</strong>: An autoencoder consists of:</p>
<ol style="list-style-type: decimal">
<li>An encoder function: <span class="math inline">\(h = g(Wx + b)\)</span>, which compresses the input <span class="math inline">\(x\)</span>.</li>
<li>A decoder function: <span class="math inline">\(\hat{x} = g'(W'h + b')\)</span>, which reconstructs the data.</li>
</ol>
</li>
<li><p>The network minimizes the reconstruction loss: <span class="math display">\[
  L = \sum_{i=1}^n (x_i - \hat{x}_i)^2.
  \]</span></p></li>
<li>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Handles high-dimensional and non-linear data.</li>
<li>Unsupervised learning.</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Computationally demanding.</li>
<li>Requires large datasets for effective training.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="generative-adversarial-networks-gans-for-data-imputation" class="section level5" number="11.4.3.2.2">
<h5>
<span class="header-section-number">11.4.3.2.2</span> Generative Adversarial Networks (GANs) for Data Imputation<a class="anchor" aria-label="anchor" href="#generative-adversarial-networks-gans-for-data-imputation"><i class="fas fa-link"></i></a>
</h5>
<p>GANs consist of a generator and a discriminator. For imputation, the generator fills in missing values, and the discriminator evaluates the quality of the imputations.</p>
<ul>
<li>
<strong>Mathematical Framework</strong>: GAN training involves optimizing: <span class="math display">\[
  \min_G \max_D \mathbb{E}[\log D(x)] + \mathbb{E}[\log(1 - D(G(z)))].
  \]</span>
<ul>
<li>
<span class="math inline">\(D(x)\)</span>: Discriminator’s probability that <span class="math inline">\(x\)</span> is real.</li>
<li>
<span class="math inline">\(G(z)\)</span>: Generator’s output for latent input <span class="math inline">\(z\)</span>.</li>
</ul>
</li>
<li>
<strong>Advantages</strong>:
<ul>
<li>Realistic imputations that reflect underlying distributions.</li>
<li>Handles complex data types.</li>
</ul>
</li>
<li>
<strong>Limitations</strong>:
<ul>
<li>Difficult to train and tune.</li>
<li>Computationally intensive.</li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="matrix-factorization-and-matrix-completion" class="section level4" number="11.4.3.3">
<h4>
<span class="header-section-number">11.4.3.3</span> Matrix Factorization and Matrix Completion<a class="anchor" aria-label="anchor" href="#matrix-factorization-and-matrix-completion"><i class="fas fa-link"></i></a>
</h4>
<div id="singular-value-decomposition-svd" class="section level5" number="11.4.3.3.1">
<h5>
<span class="header-section-number">11.4.3.3.1</span> Singular Value Decomposition (SVD)<a class="anchor" aria-label="anchor" href="#singular-value-decomposition-svd"><i class="fas fa-link"></i></a>
</h5>
<p>SVD decomposes a matrix <span class="math inline">\(A\)</span> into three matrices: <span class="math display">\[
A = U\Sigma V^T,
\]</span> where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal matrices, and <span class="math inline">\(\Sigma\)</span> contains singular values. Missing values are estimated by reconstructing <span class="math inline">\(A\)</span> using a low-rank approximation: <span class="math display">\[
\hat{A} = U_k \Sigma_k V_k^T.
\]</span></p>
<ul>
<li>
<strong>Advantages</strong>:
<ul>
<li>Captures global patterns.</li>
<li>Efficient for structured data.</li>
</ul>
</li>
<li>
<strong>Limitations</strong>:
<ul>
<li>Assumes linear relationships.</li>
<li>Sensitive to sparsity.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="collaborative-filtering-approaches" class="section level5" number="11.4.3.3.2">
<h5>
<span class="header-section-number">11.4.3.3.2</span> Collaborative Filtering Approaches<a class="anchor" aria-label="anchor" href="#collaborative-filtering-approaches"><i class="fas fa-link"></i></a>
</h5>
<p>Collaborative filtering uses similarities between rows (users) or columns (items) to impute missing data. For instance, the value of <span class="math inline">\(X_{ij}\)</span> is predicted as: <span class="math display">\[
\hat{X}_{ij} = \frac{\sum_{k \in N(i)} w_{ik} X_{kj}}{\sum_{k \in N(i)} w_{ik}},
\]</span> where <span class="math inline">\(w_{ik}\)</span> represents similarity weights and <span class="math inline">\(N(i)\)</span> is the set of neighbors.</p>
<hr>
</div>
</div>
<div id="k-nearest-neighbor-knn-imputation" class="section level4" number="11.4.3.4">
<h4>
<span class="header-section-number">11.4.3.4</span> K-Nearest Neighbor (KNN) Imputation<a class="anchor" aria-label="anchor" href="#k-nearest-neighbor-knn-imputation"><i class="fas fa-link"></i></a>
</h4>
<p>KNN identifies the <span class="math inline">\(k\)</span> nearest observations based on a distance metric and imputes missing values using a weighted average (continuous variables) or mode (categorical variables).</p>
<ul>
<li><p><strong>Mathematical Framework</strong>: For a missing value <span class="math inline">\(x\)</span>, its imputed value is: <span class="math display">\[
  \hat{x} = \frac{\sum_{i=1}^k w_i x_i}{\sum_{i=1}^k w_i},
  \]</span> where <span class="math inline">\(w_i = \frac{1}{d(x, x_i)}\)</span> and <span class="math inline">\(d(x, x_i)\)</span> is a distance metric (e.g., Euclidean or Manhattan).</p></li>
<li>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and interpretable.</li>
<li>Non-parametric.</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Computationally expensive for large datasets.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="hybrid-methods" class="section level4" number="11.4.3.5">
<h4>
<span class="header-section-number">11.4.3.5</span> Hybrid Methods<a class="anchor" aria-label="anchor" href="#hybrid-methods"><i class="fas fa-link"></i></a>
</h4>
<p>Hybrid methods combine statistical and machine learning approaches. For example, mean imputation followed by fine-tuning with machine learning models. These methods aim to leverage the strengths of multiple techniques.</p>
<hr>
</div>
<div id="summary-table" class="section level4" number="11.4.3.6">
<h4>
<span class="header-section-number">11.4.3.6</span> Summary Table<a class="anchor" aria-label="anchor" href="#summary-table"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="24%">
<col width="26%">
<col width="24%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th>Method</th>
<th>Advantages</th>
<th>Limitations</th>
<th>Applications</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Random Forest (missForest)</td>
<td>Handles mixed data types, captures interactions</td>
<td>Computationally intensive</td>
<td>Mixed data types</td>
</tr>
<tr class="even">
<td>Gradient Boosting Machines</td>
<td>High accuracy, feature importance</td>
<td>Sensitive to parameters</td>
<td>Predictive tasks</td>
</tr>
<tr class="odd">
<td>Autoencoders</td>
<td>Handles high-dimensional, non-linear data</td>
<td>Computationally expensive</td>
<td>Complex datasets</td>
</tr>
<tr class="even">
<td>GANs</td>
<td>Realistic imputations, complex distributions</td>
<td>Difficult to train, resource-intensive</td>
<td>Healthcare, finance</td>
</tr>
<tr class="odd">
<td>SVD</td>
<td>Captures global patterns, efficient</td>
<td>Assumes linear relationships</td>
<td>Recommendation systems</td>
</tr>
<tr class="even">
<td>Collaborative Filtering</td>
<td>Intuitive for user-item data</td>
<td>Struggles with sparse or new data</td>
<td>Recommender systems</td>
</tr>
<tr class="odd">
<td>KNN Imputation</td>
<td>Simple, interpretable</td>
<td>Computationally intensive, sensitive to k</td>
<td>General-purpose</td>
</tr>
<tr class="even">
<td>Hybrid Methods</td>
<td>Combines multiple strengths</td>
<td>Complexity in design</td>
<td>Flexible</td>
</tr>
</tbody>
</table></div>
</div>
</div>
<div id="multiple-imputation" class="section level3" number="11.4.4">
<h3>
<span class="header-section-number">11.4.4</span> Multiple Imputation<a class="anchor" aria-label="anchor" href="#multiple-imputation"><i class="fas fa-link"></i></a>
</h3>
<p>Multiple Imputation (MI) is a statistical technique for handling missing data by creating several plausible datasets through imputation, analyzing each dataset separately, and then combining the results to account for uncertainty in the imputations. MI operates under the assumption that missing data is either <a href="imputation-missing-data.html#missing-completely-at-random-mcar">Missing Completely at Random (MCAR)</a> or <a href="imputation-missing-data.html#missing-at-random-mar">Missing at Random (MAR)</a>.</p>
<p>Unlike <a href="imputation-missing-data.html#single-imputation-techniques">Single Imputation Techniques</a>, MI reflects the uncertainty inherent in the missing data by introducing variability in the imputed values. It avoids biases introduced by ad hoc methods and produces more reliable statistical inferences.</p>
<p>The three fundamental steps in MI are:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Imputation</strong>: Replace missing values with a set of plausible values to create multiple “completed” datasets.</li>
<li>
<strong>Analysis</strong>: Perform the desired statistical analysis on each imputed dataset.</li>
<li>
<strong>Combination</strong>: Combine the results using rules to account for within- and between-imputation variability.</li>
</ol>
<div id="why-multiple-imputation-is-important" class="section level4" number="11.4.4.1">
<h4>
<span class="header-section-number">11.4.4.1</span> Why Multiple Imputation is Important<a class="anchor" aria-label="anchor" href="#why-multiple-imputation-is-important"><i class="fas fa-link"></i></a>
</h4>
<p>Imputed values are estimates and inherently include random error. However, when these estimates are treated as exact values in subsequent analysis, the software may overlook this additional error. This oversight results in <strong>underestimated standard errors and overly small p-values</strong>, leading to misleading conclusions.</p>
<p><strong>Multiple imputation</strong> addresses this issue by generating multiple estimates for each missing value. These estimates differ slightly due to their random component, which reintroduces variation. This variation helps the software incorporate the uncertainty of imputed values, resulting in:</p>
<ul>
<li><p><strong>Unbiased parameter estimates</strong></p></li>
<li><p><strong>Accurate standard errors</strong></p></li>
<li><p><strong>Improved p-values</strong></p></li>
</ul>
<p>Multiple imputation was a significant breakthrough in statistics approximately 20 years ago. It provides solutions for many missing data issues (though not all) and, when applied correctly, leads to reliable parameter estimates.</p>
<p>If the proportion of missing data is very small (e.g., 2-3%), the choice of imputation method is less critical.</p>
</div>
<div id="goals-of-multiple-imputation" class="section level4" number="11.4.4.2">
<h4>
<span class="header-section-number">11.4.4.2</span> Goals of Multiple Imputation<a class="anchor" aria-label="anchor" href="#goals-of-multiple-imputation"><i class="fas fa-link"></i></a>
</h4>
<p>The primary goals of any missing data technique, including multiple imputation, are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Unbiased parameter estimates</strong>: Ensuring accurate regression coefficients, group means, odds ratios, etc.</p></li>
<li><p><strong>Accurate standard errors</strong>: This leads to reliable p-values and appropriate statistical inferences.</p></li>
<li><p><strong>Adequate power</strong>: To detect meaningful and significant parameter values.</p></li>
</ol>
</div>
<div id="overview-of-rubins-framework" class="section level4" number="11.4.4.3">
<h4>
<span class="header-section-number">11.4.4.3</span> Overview of Rubin’s Framework<a class="anchor" aria-label="anchor" href="#overview-of-rubins-framework"><i class="fas fa-link"></i></a>
</h4>
<p>Rubin’s Framework provides the theoretical foundation for MI. It uses a Bayesian model-based approach for generating imputations and a frequentist approach for evaluating the results. The central goals of Rubin’s framework are to ensure that imputations:</p>
<ul>
<li>Retain the statistical relationships present in the data.</li>
<li>Reflect the uncertainty about the true values of the missing data.</li>
</ul>
<p>Under Rubin’s framework, MI offers the following advantages:</p>
<ul>
<li>
<strong>Generalizability</strong>: Unlike Maximum Likelihood Estimation (MLE), MI can be applied to a wide range of models.</li>
<li>
<strong>Statistical Properties</strong>: When data is MAR or MCAR, MI estimates are consistent, asymptotically normal, and efficient.</li>
</ul>
<p>Rubin also emphasized the importance of using multiple imputations, as single imputations fail to account for variability in the imputed values, leading to underestimated standard errors and overly optimistic test statistics.</p>
</div>
<div id="multivariate-imputation-via-chained-equations-mice" class="section level4" number="11.4.4.4">
<h4>
<span class="header-section-number">11.4.4.4</span> Multivariate Imputation via Chained Equations (MICE)<a class="anchor" aria-label="anchor" href="#multivariate-imputation-via-chained-equations-mice"><i class="fas fa-link"></i></a>
</h4>
<p>Multivariate Imputation via Chained Equations (MICE) is a widely used algorithm for implementing MI, particularly in datasets with mixed variable types. The steps of MICE include:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Initialization</strong>: Replace missing values with initial guesses, such as the mean or median of the observed data.</li>
<li>
<strong>Iterative Imputation</strong>:
<ul>
<li>For each variable with missing values, regress it on all other variables (or a subset of relevant predictors).</li>
<li>Use the regression model to predict missing values, adding a random error term drawn from the residual distribution.</li>
</ul>
</li>
<li>
<strong>Convergence</strong>: Repeat the imputation process until parameter estimates stabilize.</li>
</ol>
<p>MICE offers flexibility in specifying regression models for each variable, accommodating continuous, categorical, and binary data.</p>
</div>
<div id="bayesian-ridge-regression-for-imputation" class="section level4" number="11.4.4.5">
<h4>
<span class="header-section-number">11.4.4.5</span> Bayesian Ridge Regression for Imputation<a class="anchor" aria-label="anchor" href="#bayesian-ridge-regression-for-imputation"><i class="fas fa-link"></i></a>
</h4>
<p>Bayesian ridge regression is an advanced imputation method that incorporates prior distributions on the regression coefficients, making it particularly useful when:</p>
<ul>
<li>Predictors are highly correlated.</li>
<li>Sample sizes are small.</li>
<li>Missingness is substantial.</li>
</ul>
<p>This method treats the regression coefficients as random variables and samples from their posterior distribution, introducing variability into the imputation process. Bayesian ridge regression is more computationally intensive than simpler methods like MICE but offers greater robustness.</p>
</div>
<div id="combining-results-from-mi-rubins-rules" class="section level4" number="11.4.4.6">
<h4>
<span class="header-section-number">11.4.4.6</span> Combining Results from MI (Rubin’s Rules)<a class="anchor" aria-label="anchor" href="#combining-results-from-mi-rubins-rules"><i class="fas fa-link"></i></a>
</h4>
<p>Once multiple datasets are imputed and analyzed, Rubin’s Rules are used to combine the results. The goal is to properly account for the uncertainty introduced by missing data. For a parameter of interest <span class="math inline">\(\theta\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Estimate Combination</strong>: <span class="math display">\[
\bar{\theta} = \frac{1}{M} \sum_{m=1}^M \theta_m
\]</span> where <span class="math inline">\(\theta_m\)</span> is the estimate from the <span class="math inline">\(m\)</span>th imputed dataset, and <span class="math inline">\(M\)</span> is the number of imputations.</p></li>
<li>
<p><strong>Variance Combination</strong>: <span class="math display">\[
T = \bar{W} + \left(1 + \frac{1}{M}\right) B
\]</span> where:</p>
<ul>
<li>
<span class="math inline">\(\bar{W}\)</span> is the average within-imputation variance.</li>
<li>
<span class="math inline">\(B\)</span> is the between-imputation variance: <span class="math display">\[
B = \frac{1}{M-1} \sum_{m=1}^M (\theta_m - \bar{\theta})^2
\]</span>
</li>
</ul>
</li>
</ol>
<p>These formulas adjust the final variance to reflect uncertainty both within and across imputations.</p>
<div id="challenges" class="section level5" number="11.4.4.6.1">
<h5>
<span class="header-section-number">11.4.4.6.1</span> Challenges<a class="anchor" aria-label="anchor" href="#challenges"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li>
<strong>Stochastic Variability</strong>: MI results vary slightly between runs due to its reliance on random draws. To ensure reproducibility, always set a random seed.</li>
<li>
<strong>Convergence</strong>: Iterative algorithms like MICE may struggle to converge, especially with high proportions of missing data.</li>
<li>
<strong>Assumption of MAR</strong>: MI assumes that missing data is MAR. If data is Missing Not at Random (MNAR), MI can produce biased results.</li>
</ol>
</div>
<div id="best-practices" class="section level5" number="11.4.4.6.2">
<h5>
<span class="header-section-number">11.4.4.6.2</span> Best Practices<a class="anchor" aria-label="anchor" href="#best-practices"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li>
<strong>Algorithm Selection</strong>:
<ul>
<li>Use Multiple Imputation by Chained Equations (MICE) for datasets with mixed data types or when relationships between variables are complex.</li>
<li>Apply Bayesian Ridge Regression for small datasets or when predictors are highly correlated.</li>
</ul>
</li>
<li>
<strong>Diagnostic Checks</strong>:
<ul>
<li>Evaluate the quality of imputations and assess convergence using trace plots or diagnostic statistics to ensure reliable results.</li>
</ul>
</li>
<li>
<strong>Data Transformations</strong>:
<ul>
<li>For skewed or proportion data, consider applying log or logit transformations before imputation and inverse-transforming afterward to preserve the data’s original scale.</li>
</ul>
</li>
<li>
<strong>Handling Non-Linear Relationships</strong>:
<ul>
<li>For non-linear relationships or interactions, stratify imputations by the levels of the categorical variable involved to ensure accurate estimates.</li>
</ul>
</li>
<li>
<strong>Number of Imputations</strong>:
<ul>
<li>Use at least 20 imputations for small datasets or datasets with high missingness. This ensures robust and reliable results in downstream analyses.</li>
</ul>
</li>
<li>
<strong>Avoid Rounding Imputations for Dummy Variables</strong>:
<ul>
<li>Many imputation methods (e.g., Markov Chain Monte Carlo [MCMC]) assume normality, even for dummy variables. While it was historically recommended to round imputed values to 0 or 1 for binary variables, research shows that this introduces bias in parameter estimates. Instead, leave imputed values as fractional, even though this may seem counter-intuitive.</li>
</ul>
</li>
<li>
<strong>Do Not Transform Skewed Variables Before Imputation</strong>:
<ul>
<li>Transforming variables to meet normality assumptions before imputation can distort their relationships with other variables, leading to biased imputations and possibly introducing outliers. It is better to directly impute the skewed variable.</li>
</ul>
</li>
<li>
<strong>Use More Imputations</strong>:
<ul>
<li>Traditional advice suggests 5–10 imputations are sufficient for unbiased estimates, but inconsistencies may arise in repeated analyses. [@Bodner_2008] suggests using a number of imputations equal to the percentage of missing data. As additional imputations generally do not significantly increase the computational workload, using more imputations is a prudent choice.</li>
</ul>
</li>
<li>
<strong>Create Multiplicative Terms Before Imputation</strong>:
<ul>
<li>When your model includes interaction or quadratic terms, generate these terms before imputing missing values. Imputing first and then generating these terms can introduce bias in their regression parameters, as highlighted by [@von_Hippel_2009].</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
</div>
<div id="evaluation-of-imputation-methods" class="section level2" number="11.5">
<h2>
<span class="header-section-number">11.5</span> Evaluation of Imputation Methods<a class="anchor" aria-label="anchor" href="#evaluation-of-imputation-methods"><i class="fas fa-link"></i></a>
</h2>
<div id="statistical-metrics-for-assessing-imputation-quality" class="section level3" number="11.5.1">
<h3>
<span class="header-section-number">11.5.1</span> Statistical Metrics for Assessing Imputation Quality<a class="anchor" aria-label="anchor" href="#statistical-metrics-for-assessing-imputation-quality"><i class="fas fa-link"></i></a>
</h3>
<p>To evaluate the quality of imputed data, several statistical metrics are commonly used. These metrics compare the imputed values to the observed values (in cases where missingness is simulated or artificially introduced) or assess the overall impact of imputation on the quality of subsequent analyses. Key metrics include:</p>
<ul>
<li><p><strong>Root Mean Squared Error (RMSE):</strong> RMSE is calculated as: <span class="math display">\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\]</span> It measures the average magnitude of errors between the true and imputed values. Lower RMSE indicates better imputation accuracy.</p></li>
<li><p><strong>Mean Absolute Error (MAE):</strong> MAE measures the average absolute difference between observed and imputed values: <span class="math display">\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]</span> MAE provides a straightforward assessment of imputation performance and is less sensitive to outliers than RMSE.</p></li>
<li><p><strong>Log-Likelihood and Deviance Measures:</strong> Log-likelihood can be used to evaluate how well the imputation model fits the data. Deviance measures, based on likelihood comparisons, assess the relative goodness of fit of imputation models. These are particularly useful in evaluating methods like maximum likelihood estimation.</p></li>
</ul>
<p>In practice, these metrics may be combined with graphical methods such as density plots and residual analysis to understand imputation performance more thoroughly.</p>
<hr>
</div>
<div id="bias-variance-tradeoff-in-imputation" class="section level3" number="11.5.2">
<h3>
<span class="header-section-number">11.5.2</span> Bias-Variance Tradeoff in Imputation<a class="anchor" aria-label="anchor" href="#bias-variance-tradeoff-in-imputation"><i class="fas fa-link"></i></a>
</h3>
<p>Imputation methods must balance bias and variance to achieve reliable results. Simpler methods, such as mean or mode imputation, often lead to biased parameter estimates, particularly if the missingness mechanism is non-random. These methods underestimate variability, shrinking standard errors and potentially leading to overconfidence in statistical inferences.</p>
<p>Conversely, advanced methods like <a href="imputation-missing-data.html#multiple-imputation">Multiple Imputation</a> or Full Information Maximum Likelihood (FIML) typically yield unbiased estimates with appropriately calibrated variances. However, these methods may increase computational complexity and require careful tuning of assumptions and parameters.</p>
<p>The tradeoff is summarized as follows:</p>
<ul>
<li><p><strong>High Bias, Low Variance:</strong> Simpler methods (e.g., single imputation, mean imputation).</p></li>
<li><p><strong>Low Bias, Moderate Variance:</strong> Advanced methods (e.g., MI, FIML, Bayesian methods).</p></li>
</ul>
<hr>
</div>
<div id="sensitivity-analysis-1" class="section level3" number="11.5.3">
<h3>
<span class="header-section-number">11.5.3</span> Sensitivity Analysis<a class="anchor" aria-label="anchor" href="#sensitivity-analysis-1"><i class="fas fa-link"></i></a>
</h3>
<p>Sensitivity analysis is crucial to assess the robustness of imputation methods under varying assumptions. Two primary areas of focus include:</p>
<ul>
<li><p><strong>Assessing Robustness to Assumptions:</strong> Imputation models often rely on assumptions about the missingness mechanism (See <a href="imputation-missing-data.html#definition-and-classification-of-missing-data">Definition and Classification of Missing Data</a>). Sensitivity analysis involves testing how results vary when these assumptions are slightly relaxed or modified.</p></li>
<li>
<p><strong>Impact on Downstream Analysis:</strong> The quality of imputation should also be evaluated based on its influence on downstream analyses (<a href="imputation-missing-data.html#objectives-of-imputation">Objectives of Imputation</a>). For instance:</p>
<ul>
<li>Does the imputation affect causal inference in regression models?</li>
<li>Are the conclusions from hypothesis testing or predictive modeling robust to the imputation technique?</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="validation-using-simulated-data-and-real-world-case-studies" class="section level3" number="11.5.4">
<h3>
<span class="header-section-number">11.5.4</span> Validation Using Simulated Data and Real-World Case Studies<a class="anchor" aria-label="anchor" href="#validation-using-simulated-data-and-real-world-case-studies"><i class="fas fa-link"></i></a>
</h3>
<p>Validation of imputation methods is best performed through a combination of simulated data and real-world examples:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Simulated Data:</strong> - Create datasets with known missingness patterns and true values. - Apply various imputation methods and assess their performance using RMSE, MAE, and other metrics.</li>
<li>
<strong>Real-World Case Studies:</strong>
<ul>
<li>Use datasets from actual studies, such as customer transaction data in marketing or financial data in portfolio analysis.</li>
<li>Evaluate the impact of imputation on actionable outcomes (e.g., market segmentation, risk assessment).</li>
</ul>
</li>
</ol>
<p>Combining these approaches ensures that methods generalize well across different contexts and data structures.</p>
<hr>
</div>
</div>
<div id="criteria-for-choosing-an-effective-approach" class="section level2" number="11.6">
<h2>
<span class="header-section-number">11.6</span> Criteria for Choosing an Effective Approach<a class="anchor" aria-label="anchor" href="#criteria-for-choosing-an-effective-approach"><i class="fas fa-link"></i></a>
</h2>
<p>Choosing an appropriate imputation method depends on the following criteria:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Unbiased Parameter Estimates:</strong> The technique should ensure that key estimates, such as means, variances, and regression coefficients, are unbiased, particularly in the presence of MAR or MNAR data.</p></li>
<li><p><strong>Adequate Power:</strong> The method should preserve statistical power, enabling robust hypothesis testing and model estimation. This ensures that important effects are not missed due to inflated type II error.</p></li>
<li><p><strong>Accurate Standard Errors:</strong> Accurate estimation of standard errors is critical for reliable p-values and confidence intervals. Methods like single imputation often underestimate standard errors, leading to overconfident conclusions.</p></li>
</ol>
<p><strong>Preferred Methods: Multiple Imputation and Full Information Maximum Likelihood</strong></p>
<p><strong>Multiple Imputation (MI):</strong></p>
<ul>
<li><p>MI replaces missing values with multiple plausible values drawn from a predictive distribution. It generates multiple complete datasets, analyzes each dataset, and combines the results.</p></li>
<li><p>Pros: Handles uncertainty well, provides valid standard errors, and is robust under MAR.</p></li>
<li><p>Cons: Computationally intensive, sensitive to model mis-specification.</p></li>
</ul>
<p><strong>Full Information Maximum Likelihood (FIML):</strong></p>
<ul>
<li><p>FIML uses all available data to estimate parameters directly, avoiding the need to impute missing values explicitly.</p></li>
<li><p>Pros: Efficient, unbiased under MAR, and computationally elegant.</p></li>
<li><p>Cons: Requires correctly specified models and may be sensitive to MNAR data.</p></li>
</ul>
<p><strong>Methods to Avoid</strong></p>
<ul>
<li>
<strong>Single Imputation (e.g., Mean, Mode):</strong>
<ul>
<li>Leads to biased estimates and underestimates variability.</li>
</ul>
</li>
<li>
<strong>Listwise Deletion:</strong>
<ul>
<li>Discards rows with missing data, reducing sample size and potentially introducing bias if the data is not MCAR.</li>
</ul>
</li>
</ul>
<p><strong>Practical Considerations</strong></p>
<ul>
<li>Computational efficiency and ease of implementation.</li>
<li>Compatibility with downstream analysis methods.</li>
<li>Alignment with the data’s missingness mechanism.</li>
</ul>
</div>
<div id="challenges-and-ethical-considerations" class="section level2" number="11.7">
<h2>
<span class="header-section-number">11.7</span> Challenges and Ethical Considerations<a class="anchor" aria-label="anchor" href="#challenges-and-ethical-considerations"><i class="fas fa-link"></i></a>
</h2>
<div id="challenges-in-high-dimensional-data" class="section level3" number="11.7.1">
<h3>
<span class="header-section-number">11.7.1</span> Challenges in High-Dimensional Data<a class="anchor" aria-label="anchor" href="#challenges-in-high-dimensional-data"><i class="fas fa-link"></i></a>
</h3>
<p>High-dimensional data, where the number of variables exceeds the number of observations, poses unique challenges for missing data analysis.</p>
<ul>
<li><p><strong>Curse of Dimensionality</strong>: Standard imputation methods, such as mean or regression imputation, struggle with high-dimensional spaces due to sparse data distribution.</p></li>
<li><p><strong>Regularized Methods</strong>: Techniques such as LASSO, Ridge Regression, and Elastic Net can be used to handle high-dimensional missing data. These methods shrink model coefficients, preventing overfitting.</p></li>
<li><p><strong>Matrix Factorization</strong>: Methods like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) are often adapted to impute missing values in high-dimensional datasets by reducing the dimensionality first.</p></li>
</ul>
<hr>
</div>
<div id="missing-data-in-big-data-contexts" class="section level3" number="11.7.2">
<h3>
<span class="header-section-number">11.7.2</span> Missing Data in Big Data Contexts<a class="anchor" aria-label="anchor" href="#missing-data-in-big-data-contexts"><i class="fas fa-link"></i></a>
</h3>
<p>The advent of big data introduces additional complexities for missing data handling, including computational scalability and storage constraints.</p>
<div id="distributed-imputation-techniques" class="section level4" number="11.7.2.1">
<h4>
<span class="header-section-number">11.7.2.1</span> Distributed Imputation Techniques<a class="anchor" aria-label="anchor" href="#distributed-imputation-techniques"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>MapReduce Frameworks</strong>: Algorithms like k-nearest neighbor (KNN) imputation or multiple imputation can be adapted for distributed environments using MapReduce or similar frameworks.</p></li>
<li><p><strong>Federated Learning</strong>: In scenarios where data is distributed across multiple locations (e.g., in healthcare or banking), federated learning allows imputation without centralizing data, ensuring privacy.</p></li>
</ul>
</div>
<div id="cloud-based-implementations" class="section level4" number="11.7.2.2">
<h4>
<span class="header-section-number">11.7.2.2</span> Cloud-Based Implementations<a class="anchor" aria-label="anchor" href="#cloud-based-implementations"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Cloud-Native Algorithms</strong>: Cloud platforms like AWS, Google Cloud, and Azure provide scalable solutions for implementing advanced imputation algorithms on large datasets.</p></li>
<li><p><strong>AutoML Integration</strong>: Automated Machine Learning (AutoML) pipelines often include missing data handling as a preprocessing step, leveraging cloud-based computational power.</p></li>
<li><p><strong>Real-Time Imputation</strong>: In e-commerce, cloud-based solutions enable real-time imputation for recommendation systems or fraud detection, ensuring seamless user experiences.</p></li>
</ul>
<hr>
</div>
</div>
<div id="ethical-concerns" class="section level3" number="11.7.3">
<h3>
<span class="header-section-number">11.7.3</span> Ethical Concerns<a class="anchor" aria-label="anchor" href="#ethical-concerns"><i class="fas fa-link"></i></a>
</h3>
<div id="bias-amplification" class="section level4" number="11.7.3.1">
<h4>
<span class="header-section-number">11.7.3.1</span> Bias Amplification<a class="anchor" aria-label="anchor" href="#bias-amplification"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Introduction of Systematic Bias</strong>: Imputation methods can inadvertently reinforce existing biases. For example, imputing salary data based on demographic variables may propagate societal inequalities.</p></li>
<li><p><strong>Business Implications</strong>: In credit scoring, biased imputation of missing financial data can lead to unfair credit decisions, disproportionately affecting marginalized groups.</p></li>
<li><p><strong>Mitigation Strategies</strong>: Techniques such as fairness-aware machine learning and bias auditing can help identify and reduce bias introduced during imputation.</p></li>
</ul>
</div>
<div id="transparency-in-reporting-imputation-decisions" class="section level4" number="11.7.3.2">
<h4>
<span class="header-section-number">11.7.3.2</span> Transparency in Reporting Imputation Decisions<a class="anchor" aria-label="anchor" href="#transparency-in-reporting-imputation-decisions"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Reproducibility and Documentation</strong>: Transparent reporting of imputation methods and assumptions is essential for reproducibility. Analysts should provide clear documentation of the imputation pipeline.</p></li>
<li><p><strong>Stakeholder Communication</strong>: In business settings, communicating imputation decisions to stakeholders ensures informed decision-making and trust in the results.</p></li>
<li><p><strong>Ethical Frameworks</strong>: Ethical guidelines, such as those provided by the European Union’s GDPR or industry-specific codes, emphasize the importance of transparency in data handling.</p></li>
</ul>
<hr>
</div>
</div>
</div>
<div id="emerging-trends-in-missing-data-handling" class="section level2" number="11.8">
<h2>
<span class="header-section-number">11.8</span> Emerging Trends in Missing Data Handling<a class="anchor" aria-label="anchor" href="#emerging-trends-in-missing-data-handling"><i class="fas fa-link"></i></a>
</h2>
<div id="advances-in-neural-network-approaches" class="section level3" number="11.8.1">
<h3>
<span class="header-section-number">11.8.1</span> Advances in Neural Network Approaches<a class="anchor" aria-label="anchor" href="#advances-in-neural-network-approaches"><i class="fas fa-link"></i></a>
</h3>
<p>Neural networks have transformed the landscape of missing data imputation, offering flexible, scalable, and powerful solutions that go beyond traditional methods.</p>
<div id="variational-autoencoders-vaes" class="section level4" number="11.8.1.1">
<h4>
<span class="header-section-number">11.8.1.1</span> Variational Autoencoders (VAEs)<a class="anchor" aria-label="anchor" href="#variational-autoencoders-vaes"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Overview</strong>: Variational Autoencoders (VAEs) are generative models that encode data into a latent space and reconstruct it, filling in missing values during reconstruction.</p></li>
<li>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Handle complex, non-linear relationships between variables.</li>
<li>Scalable to high-dimensional datasets.</li>
<li>Generate probabilistic imputations, reflecting uncertainty.</li>
</ul>
</li>
<li>
<p><strong>Applications</strong>:</p>
<ul>
<li>In marketing, VAEs can impute missing customer behavior data while accounting for seasonal and demographic variations.</li>
<li>In finance, VAEs assist in imputing missing stock price data by modeling dependencies among assets.</li>
</ul>
</li>
</ul>
</div>
<div id="gans-for-missing-data" class="section level4" number="11.8.1.2">
<h4>
<span class="header-section-number">11.8.1.2</span> GANs for Missing Data<a class="anchor" aria-label="anchor" href="#gans-for-missing-data"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Generative Adversarial Networks (GANs)</strong>: GANs consist of a generator and a discriminator, with the generator imputing missing data and the discriminator evaluating its quality.</p></li>
<li>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Preserve data distributions and avoid over-smoothing.</li>
<li>Suitable for imputation in datasets with complex patterns or multi-modal distributions.</li>
</ul>
</li>
<li>
<p><strong>Applications</strong>:</p>
<ul>
<li>In healthcare, GANs have been used to impute missing patient records while preserving patient privacy and data integrity.</li>
<li>In retail, GANs can model missing sales data to predict trends and optimize inventory.</li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="integration-with-reinforcement-learning" class="section level3" number="11.8.2">
<h3>
<span class="header-section-number">11.8.2</span> Integration with Reinforcement Learning<a class="anchor" aria-label="anchor" href="#integration-with-reinforcement-learning"><i class="fas fa-link"></i></a>
</h3>
<p>Reinforcement learning (RL) is increasingly being integrated into missing data strategies, particularly in dynamic or sequential data environments.</p>
<ul>
<li><p><strong>Markov Decision Processes (MDPs)</strong>: RL models missing data handling as an MDP, where actions (imputations) are optimized based on rewards (accuracy of predictions or decisions).</p></li>
<li>
<p><strong>Active Imputation</strong>:</p>
<ul>
<li>RL can be used to actively query for missing data points, prioritizing those with the highest impact on downstream tasks.</li>
<li>Example: In customer churn prediction, RL can optimize the imputation of high-value customer records.</li>
</ul>
</li>
<li>
<p><strong>Applications</strong>:</p>
<ul>
<li>Financial forecasting: RL models are used to impute missing transaction data dynamically, optimizing portfolio decisions.</li>
<li>Smart cities: RL-based models handle missing sensor data to enhance real-time decision-making in traffic management.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="synthetic-data-generation-for-missing-data" class="section level3" number="11.8.3">
<h3>
<span class="header-section-number">11.8.3</span> Synthetic Data Generation for Missing Data<a class="anchor" aria-label="anchor" href="#synthetic-data-generation-for-missing-data"><i class="fas fa-link"></i></a>
</h3>
<p>Synthetic data generation has emerged as a robust solution to address missing data, providing flexibility and privacy.</p>
<ul>
<li><p><strong>Data Augmentation</strong>: Synthetic data is generated to augment datasets with missing values, reducing biases introduced by imputation.</p></li>
<li>
<p><strong>Techniques</strong>:</p>
<ul>
<li>
<strong>Simulations</strong>: Monte Carlo simulations create plausible data points based on observed distributions.</li>
<li>
<strong>Generative Models</strong>: GANs and VAEs generate realistic synthetic data that aligns with existing patterns.</li>
</ul>
</li>
<li>
<p><strong>Applications</strong>:</p>
<ul>
<li>In fraud detection, synthetic datasets balance the impact of missing values on anomaly detection.</li>
<li>In insurance, synthetic data supports pricing models by filling in gaps from incomplete policyholder records.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="federated-learning-and-privacy-preserving-imputation" class="section level3" number="11.8.4">
<h3>
<span class="header-section-number">11.8.4</span> Federated Learning and Privacy-Preserving Imputation<a class="anchor" aria-label="anchor" href="#federated-learning-and-privacy-preserving-imputation"><i class="fas fa-link"></i></a>
</h3>
<p>Federated learning has gained traction as a method for collaborative analysis while preserving data privacy.</p>
<ul>
<li>
<strong>Federated Imputation</strong>:
<ul>
<li>Distributed imputation algorithms operate on decentralized data, ensuring that sensitive information remains local.</li>
<li>Example: Hospitals collaboratively impute missing patient data without sharing individual records.</li>
</ul>
</li>
<li>
<strong>Privacy Mechanisms</strong>:
<ul>
<li>Differential privacy adds noise to imputed values, protecting individual-level data.</li>
<li>Homomorphic encryption allows computations on encrypted data, ensuring privacy throughout the imputation process.</li>
</ul>
</li>
<li>
<strong>Applications</strong>:
<ul>
<li>Healthcare: Federated learning imputes missing diagnostic data across clinics.</li>
<li>Banking: Collaborative imputation of financial transaction data supports risk modeling while adhering to regulations.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="imputation-in-streaming-and-online-data-environments" class="section level3" number="11.8.5">
<h3>
<span class="header-section-number">11.8.5</span> Imputation in Streaming and Online Data Environments<a class="anchor" aria-label="anchor" href="#imputation-in-streaming-and-online-data-environments"><i class="fas fa-link"></i></a>
</h3>
<p>The increasing use of streaming data in business and technology requires real-time imputation methods to ensure uninterrupted analysis.</p>
<ul>
<li>
<strong>Challenges</strong>:
<ul>
<li>Imputation must occur dynamically as data streams in.</li>
<li>Low latency and high accuracy are essential to maintain real-time decision-making.</li>
</ul>
</li>
<li>
<strong>Techniques</strong>:
<ul>
<li>
<strong>Online Learning Algorithms</strong>: Update imputation models incrementally as new data arrives.</li>
<li>
<strong>Sliding Window Methods</strong>: Use recent data to estimate and impute missing values in real time.</li>
</ul>
</li>
<li>
<strong>Applications</strong>:
<ul>
<li>IoT devices: Imputation in sensor networks for smart homes or industrial monitoring ensures continuous operation despite data transmission issues.</li>
<li>Financial markets: Streaming imputation models predict and fill gaps in real-time stock price feeds to inform trading algorithms.</li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="application-of-imputation-in-r" class="section level2" number="11.9">
<h2>
<span class="header-section-number">11.9</span> Application of Imputation in R<a class="anchor" aria-label="anchor" href="#application-of-imputation-in-r"><i class="fas fa-link"></i></a>
</h2>
<p>This section demonstrates how to visualize missing data and handle it using different imputation techniques.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="13%">
<col width="13%">
<col width="12%">
<col width="12%">
</colgroup>
<thead><tr class="header">
<th><strong>Package</strong></th>
<th><strong>Algorithm</strong></th>
<th><strong>Cont Var</strong></th>
<th><strong>Cate Var</strong></th>
<th><strong>Diagnostics</strong></th>
<th><strong>Complexity Handling</strong></th>
<th><strong>Best Use Case</strong></th>
<th><strong>Limitations</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>missForest</strong></td>
<td>Random Forest</td>
<td>Yes</td>
<td>Yes</td>
<td>Out-of-bag error (NRMSE, PFC)</td>
<td>Handles complex interactions</td>
<td>Mixed data types with complex interactions</td>
<td>May overfit with small datasets</td>
</tr>
<tr class="even">
<td><strong>Hmisc</strong></td>
<td>Additive Regression, Bootstrap, Predictive Mean Matching</td>
<td>Yes</td>
<td>Yes</td>
<td>
<span class="math inline">\(R^2\)</span> for imputed values</td>
<td>Basic to intermediate complexity</td>
<td>Simple datasets with low complexity</td>
<td>Limited to simple imputation methods</td>
</tr>
<tr class="odd">
<td><strong>mi</strong></td>
<td>Bayesian Regression</td>
<td>Yes</td>
<td>Yes</td>
<td>Graphical diagnostics,convergence</td>
<td>Detects issues like collinearity</td>
<td>Datasets with irregularities</td>
<td>Computationally intensive for large data</td>
</tr>
<tr class="even">
<td><strong>MICE</strong></td>
<td>Multivariate Imputation via Chained Equations</td>
<td>Yes</td>
<td>Yes</td>
<td>Density plots, pooling of results</td>
<td>Handles variable interactions</td>
<td>General-purpose imputation for MAR data</td>
<td>Requires proper method selection for variable types</td>
</tr>
<tr class="odd">
<td><strong>Amelia</strong></td>
<td>Bootstrap-based Expectation Maximization (EMB)</td>
<td>Yes</td>
<td>Limited (requires normality)</td>
<td>Diagnostics supported</td>
<td>Works well with large/time-series data</td>
<td>Time-series or datasets approximating MVN</td>
<td>Assumes MVN, requires transformations for non-MVN</td>
</tr>
</tbody>
</table></div>
<div id="visualizing-missing-data" class="section level3" number="11.9.1">
<h3>
<span class="header-section-number">11.9.1</span> Visualizing Missing Data<a class="anchor" aria-label="anchor" href="#visualizing-missing-data"><i class="fas fa-link"></i></a>
</h3>
<p>Visualizing missing data is an essential first step in understanding the patterns and extent of missingness in your dataset.</p>
<div class="sourceCode" id="cb365"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/visdat/">visdat</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/njtierney/naniar">naniar</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualizing missing data</span></span>
<span><span class="fu"><a href="https://docs.ropensci.org/visdat/reference/vis_miss.html">vis_miss</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-20-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb366"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Missingness patterns using an upset plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/gg_miss_upset.html">gg_miss_upset</a></span><span class="op">(</span><span class="va">airquality</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-20-2.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb367"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Scatter plot of missing data with faceting</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">airquality</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/naniar/man/geom_miss_point.html">geom_miss_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span> <span class="va">group</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Missing values by variable</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/gg_miss_var.html">gg_miss_var</a></span><span class="op">(</span><span class="va">data</span>, facet <span class="op">=</span> <span class="va">group</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Missingness in relation to factors</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/naniar/man/gg_miss_fct.html">gg_miss_fct</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable1</span>, fct <span class="op">=</span> <span class="va">variable2</span><span class="op">)</span></span></code></pre></div>
<p>For more details, read <a href="https://tmb.njtierney.com/">The Missing Book by Nicholas Tierney &amp; Allison Horst</a>.</p>
</div>
<div id="how-many-imputations" class="section level3" number="11.9.2">
<h3>
<span class="header-section-number">11.9.2</span> How Many Imputations?<a class="anchor" aria-label="anchor" href="#how-many-imputations"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Usually, 5 imputations</strong> are sufficient unless there is an extremely high proportion of missing data. High proportions require revisiting data collection processes.</p>
<p><strong>Rubin’s Rule for Relative Efficiency</strong></p>
<p>According to Rubin, the relative efficiency of an estimate based on <span class="math inline">\(m\)</span> imputations (relative to infinite imputations) is given by:</p>
<p><span class="math display">\[
\text{Relative Efficiency} = ( 1 + \frac{\lambda}{m})^{-1}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the rate of missing data.</p>
<p>For example, with 50% missing data (<span class="math inline">\(\lambda = 0.5\)</span>), the standard deviation of an estimate based on 5 imputations is only about 5% wider than that from infinite imputations:</p>
<p><span class="math display">\[
\sqrt{1 + \frac{0.5}{5}} = 1.049
\]</span></p>
</div>
<div id="generating-missing-data-for-demonstration" class="section level3" number="11.9.3">
<h3>
<span class="header-section-number">11.9.3</span> Generating Missing Data for Demonstration<a class="anchor" aria-label="anchor" href="#generating-missing-data-for-demonstration"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb368"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org">missForest</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load the data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">iris</span></span>
<span></span>
<span><span class="co"># Generate 10% missing values at random</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">iris.mis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/prodNA.html">prodNA</a></span><span class="op">(</span><span class="va">iris</span>, noNA <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Remove categorical variables for numeric imputation</span></span>
<span><span class="va">iris.mis.cat</span> <span class="op">&lt;-</span> <span class="va">iris.mis</span></span>
<span><span class="va">iris.mis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">iris.mis</span>, select <span class="op">=</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">Species</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="imputation-with-mean-median-and-mode" class="section level3" number="11.9.4">
<h3>
<span class="header-section-number">11.9.4</span> Imputation with Mean, Median, and Mode<a class="anchor" aria-label="anchor" href="#imputation-with-mean-median-and-mode"><i class="fas fa-link"></i></a>
</h3>
<p>Mean, median, or mode imputation is a simple yet commonly used technique.</p>
<div class="sourceCode" id="cb369"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Imputation for the entire dataset</span></span>
<span><span class="fu">e1071</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span>, what <span class="op">=</span> <span class="st">"mean"</span><span class="op">)</span>        <span class="co"># Replace with mean</span></span>
<span><span class="fu">e1071</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span>, what <span class="op">=</span> <span class="st">"median"</span><span class="op">)</span>      <span class="co"># Replace with median</span></span>
<span></span>
<span><span class="co"># Imputation by variable</span></span>
<span><span class="fu">Hmisc</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Length</span>, <span class="va">mean</span><span class="op">)</span>    <span class="co"># Replace with mean</span></span>
<span><span class="fu">Hmisc</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Length</span>, <span class="va">median</span><span class="op">)</span>  <span class="co"># Replace with median</span></span>
<span><span class="fu">Hmisc</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Length</span>, <span class="fl">0</span><span class="op">)</span>       <span class="co"># Replace with a specific value</span></span></code></pre></div>
<p>Checking Accuracy</p>
<p>Accuracy can be checked by comparing predictions with actual values.</p>
<div class="sourceCode" id="cb370"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example data</span></span>
<span><span class="va">actuals</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">predicteds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Sepal.Width</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">actuals</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Using MLmetrics package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/yanyachen/MLmetrics">MLmetrics</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/MLmetrics/man/MAE.html">MAE</a></span><span class="op">(</span><span class="va">predicteds</span>, <span class="va">actuals</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.2870303</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/MLmetrics/man/MSE.html">MSE</a></span><span class="op">(</span><span class="va">predicteds</span>, <span class="va">actuals</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.1301598</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/MLmetrics/man/RMSE.html">RMSE</a></span><span class="op">(</span><span class="va">predicteds</span>, <span class="va">actuals</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.3607767</span></span></code></pre></div>
</div>
<div id="k-nearest-neighbors-knn-imputation" class="section level3" number="11.9.5">
<h3>
<span class="header-section-number">11.9.5</span> K-Nearest Neighbors (KNN) Imputation<a class="anchor" aria-label="anchor" href="#k-nearest-neighbors-knn-imputation"><i class="fas fa-link"></i></a>
</h3>
<p>KNN is a more sophisticated method, leveraging similar observations to fill in missing values.</p>
<div class="sourceCode" id="cb371"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/ltorgo/DMwR2">DMwR2</a></span><span class="op">)</span></span>
<span><span class="va">knnOutput</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/DMwR2/man/knnImputation.html">knnImputation</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">iris.mis.cat</span>, meth <span class="op">=</span> <span class="st">"median"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">anyNA</a></span><span class="op">(</span><span class="va">knnOutput</span><span class="op">)</span>  <span class="co"># Check for remaining missing values</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span></code></pre></div>
<div class="sourceCode" id="cb372"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">actuals</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">predicteds</span> <span class="op">&lt;-</span> <span class="va">knnOutput</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">)</span>, <span class="st">"Sepal.Width"</span><span class="op">]</span></span>
<span><span class="co"># Using MLmetrics package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/yanyachen/MLmetrics">MLmetrics</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/MLmetrics/man/MAE.html">MAE</a></span><span class="op">(</span><span class="va">predicteds</span>, <span class="va">actuals</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.2318182</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/MLmetrics/man/MSE.html">MSE</a></span><span class="op">(</span><span class="va">predicteds</span>, <span class="va">actuals</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.1038636</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/MLmetrics/man/RMSE.html">RMSE</a></span><span class="op">(</span><span class="va">predicteds</span>, <span class="va">actuals</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.3222788</span></span></code></pre></div>
<p>KNN typically improves upon mean or median imputation in terms of predictive accuracy.</p>
</div>
<div id="imputation-with-decision-trees-rpart" class="section level3" number="11.9.6">
<h3>
<span class="header-section-number">11.9.6</span> Imputation with Decision Trees (rpart)<a class="anchor" aria-label="anchor" href="#imputation-with-decision-trees-rpart"><i class="fas fa-link"></i></a>
</h3>
<p>Decision trees, such as those implemented in <code>rpart</code>, are effective for both numeric and categorical variables.</p>
<div class="sourceCode" id="cb373"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bethatkinson/rpart">rpart</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Imputation for a categorical variable</span></span>
<span><span class="va">class_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span></span>
<span>  <span class="va">Species</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">Sepal.Length</span>,</span>
<span>  data <span class="op">=</span> <span class="va">iris.mis.cat</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis.cat</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span>, <span class="op">]</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"class"</span>,</span>
<span>  na.action <span class="op">=</span> <span class="va">na.omit</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Imputation for a numeric variable</span></span>
<span><span class="va">anova_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span></span>
<span>  <span class="va">Sepal.Width</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="va">Sepal.Length</span>,</span>
<span>  data <span class="op">=</span> <span class="va">iris.mis</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">)</span>, <span class="op">]</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"anova"</span>,</span>
<span>  na.action <span class="op">=</span> <span class="va">na.omit</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictions</span></span>
<span><span class="va">species_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">class_mod</span>, <span class="va">iris.mis.cat</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis.cat</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="va">width_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">anova_mod</span>, <span class="va">iris.mis</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">$</span><span class="va">Sepal.Width</span><span class="op">)</span>, <span class="op">]</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="mice-multivariate-imputation-via-chained-equations" class="section level3" number="11.9.7">
<h3>
<span class="header-section-number">11.9.7</span> MICE (Multivariate Imputation via Chained Equations)<a class="anchor" aria-label="anchor" href="#mice-multivariate-imputation-via-chained-equations"><i class="fas fa-link"></i></a>
</h3>
<p>MICE assumes that the data are <a href="imputation-missing-data.html#missing-at-random-mar"><strong>Missing at Random (MAR)</strong></a>. It imputes data for each variable by specifying an imputation model tailored to the variable type.</p>
<div id="how-mice-works" class="section level4" number="11.9.7.1">
<h4>
<span class="header-section-number">11.9.7.1</span> How MICE Works<a class="anchor" aria-label="anchor" href="#how-mice-works"><i class="fas fa-link"></i></a>
</h4>
<p>For a dataset with variables <span class="math inline">\(X_1, X_2, \dots, X_k\)</span>:</p>
<ul>
<li><p>If <span class="math inline">\(X_1\)</span> has missing data, it is regressed on the other variables.</p></li>
<li><p>This process is repeated for all variables with missing data, using the previously predicted values as needed.</p></li>
</ul>
<p>By default:</p>
<ul>
<li><p><strong>Continuous variables</strong> use linear regression.</p></li>
<li><p><strong>Categorical variables</strong> use logistic regression.</p></li>
</ul>
</div>
<div id="methods-available-in-mice" class="section level4" number="11.9.7.2">
<h4>
<span class="header-section-number">11.9.7.2</span> Methods Available in MICE<a class="anchor" aria-label="anchor" href="#methods-available-in-mice"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<strong><code>pmm</code> (Predictive Mean Matching)</strong>: For numeric variables.</li>
<li>
<strong><code>logreg</code> (Logistic Regression)</strong>: For binary variables (2 levels).</li>
<li>
<strong><code>polyreg</code> (Bayesian polytomous regression)</strong>: For factor variables (≥2 levels).</li>
<li>
<strong>Proportional Odds Model</strong>: For ordered factor variables (≥2 levels).</li>
</ul>
<div class="sourceCode" id="cb374"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/amices/mice">mice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/statistikat/VIM">VIM</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Check missing values pattern</span></span>
<span><span class="fu"><a href="https://amices.org/mice/reference/md.pattern.html">md.pattern</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-28-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt;     Sepal.Width Sepal.Length Petal.Length Petal.Width   
#&gt; 100           1            1            1           1  0
#&gt; 15            1            1            1           0  1
#&gt; 8             1            1            0           1  1
#&gt; 2             1            1            0           0  2
#&gt; 11            1            0            1           1  1
#&gt; 1             1            0            1           0  2
#&gt; 1             1            0            0           1  2
#&gt; 1             1            0            0           0  3
#&gt; 7             0            1            1           1  1
#&gt; 3             0            1            0           1  2
#&gt; 1             0            0            1           1  2
#&gt;              11           15           15          19 60

# Plot missing values
aggr(
  iris.mis,
  col = c('navyblue', 'yellow'),
  numbers = TRUE,
  sortVars = TRUE,
  labels = names(iris.mis),
  cex.axis = 0.7,
  gap = 3,
  ylab = c("Missing data", "Pattern")
)</code></pre>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-28-2.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; 
#&gt;  Variables sorted by number of missings: 
#&gt;      Variable      Count
#&gt;   Petal.Width 0.12666667
#&gt;  Sepal.Length 0.10000000
#&gt;  Petal.Length 0.10000000
#&gt;   Sepal.Width 0.07333333</code></pre>
<p>Imputing Data</p>
<div class="sourceCode" id="cb377"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Perform multiple imputation using MICE</span></span>
<span><span class="va">imputed_Data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/mice.html">mice</a></span><span class="op">(</span></span>
<span>  <span class="va">iris.mis</span>,</span>
<span>  m <span class="op">=</span> <span class="fl">5</span>,             <span class="co"># Number of imputed datasets</span></span>
<span>  maxit <span class="op">=</span> <span class="fl">10</span>,        <span class="co"># Number of iterations</span></span>
<span>  method <span class="op">=</span> <span class="st">'pmm'</span>,    <span class="co"># Imputation method</span></span>
<span>  seed <span class="op">=</span> <span class="fl">500</span>         <span class="co"># Random seed for reproducibility</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Evaluating Imputed Data</p>
<div class="sourceCode" id="cb378"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Summary of imputed data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">imputed_Data</span><span class="op">)</span></span>
<span><span class="co">#&gt; Class: mids</span></span>
<span><span class="co">#&gt; Number of multiple imputations:  5 </span></span>
<span><span class="co">#&gt; Imputation methods:</span></span>
<span><span class="co">#&gt; Sepal.Length  Sepal.Width Petal.Length  Petal.Width </span></span>
<span><span class="co">#&gt;        "pmm"        "pmm"        "pmm"        "pmm" </span></span>
<span><span class="co">#&gt; PredictorMatrix:</span></span>
<span><span class="co">#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width</span></span>
<span><span class="co">#&gt; Sepal.Length            0           1            1           1</span></span>
<span><span class="co">#&gt; Sepal.Width             1           0            1           1</span></span>
<span><span class="co">#&gt; Petal.Length            1           1            0           1</span></span>
<span><span class="co">#&gt; Petal.Width             1           1            1           0</span></span>
<span></span>
<span><span class="co"># Density plot: compare imputed values (red) with observed values (blue)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lattice/man/histogram.html">densityplot</a></span><span class="op">(</span><span class="va">imputed_Data</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="11-imputation_files/figure-html/unnamed-chunk-30-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Accessing and Using Imputed Data</p>
<div class="sourceCode" id="cb379"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Access the complete datasets</span></span>
<span><span class="va">completeData1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imputed_Data</span>, <span class="fl">1</span><span class="op">)</span>  <span class="co"># First imputed dataset</span></span>
<span><span class="va">completeData2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/complete.html">complete</a></span><span class="op">(</span><span class="va">imputed_Data</span>, <span class="fl">2</span><span class="op">)</span>  <span class="co"># Second imputed dataset</span></span></code></pre></div>
<p>Regression Model with Imputed Dataset</p>
<div class="sourceCode" id="cb380"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit a regression model using imputed datasets</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">imputed_Data</span>, exp <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Sepal.Width</span> <span class="op">~</span> <span class="va">Sepal.Length</span> <span class="op">+</span> <span class="va">Petal.Width</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Combine results of all 5 models</span></span>
<span><span class="va">combine</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/pool.html">pool</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">combine</span><span class="op">)</span></span>
<span><span class="co">#&gt;           term   estimate  std.error statistic        df      p.value</span></span>
<span><span class="co">#&gt; 1  (Intercept)  1.9054698 0.33454626  5.695684 105.12438 1.127064e-07</span></span>
<span><span class="co">#&gt; 2 Sepal.Length  0.2936285 0.07011405  4.187870  88.69066 6.625536e-05</span></span>
<span><span class="co">#&gt; 3  Petal.Width -0.4742921 0.08138313 -5.827892  46.94941 4.915270e-07</span></span></code></pre></div>
</div>
</div>
<div id="amelia" class="section level3" number="11.9.8">
<h3>
<span class="header-section-number">11.9.8</span> Amelia<a class="anchor" aria-label="anchor" href="#amelia"><i class="fas fa-link"></i></a>
</h3>
<p>Amelia uses a <strong>bootstrap-based Expectation-Maximization with Bootstrapping (EMB) algorithm</strong> for imputation, making it faster and suitable for cross-sectional and time-series data.</p>
<div id="assumptions" class="section level4" number="11.9.8.1">
<h4>
<span class="header-section-number">11.9.8.1</span> Assumptions<a class="anchor" aria-label="anchor" href="#assumptions"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p>All variables must follow a <strong>Multivariate Normal Distribution (MVN)</strong>. Transformations may be required for non-normal data.</p></li>
<li><p>Data must be <a href="imputation-missing-data.html#missing-at-random-mar"><strong>Missing at Random (MAR)</strong></a>.</p></li>
</ul>
</div>
<div id="comparison-amelia-vs.-mice" class="section level4" number="11.9.8.2">
<h4>
<span class="header-section-number">11.9.8.2</span> Comparison: Amelia vs. MICE<a class="anchor" aria-label="anchor" href="#comparison-amelia-vs.-mice"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>MICE</strong> imputes on a variable-by-variable basis using separate models.</p></li>
<li><p><strong>Amelia</strong> uses a joint modeling approach based on MVN.</p></li>
<li><p><strong>MICE</strong> handles multiple data types, while <strong>Amelia</strong> requires variables to approximate normality.</p></li>
</ul>
</div>
<div id="imputation-with-amelia" class="section level4" number="11.9.8.3">
<h4>
<span class="header-section-number">11.9.8.3</span> Imputation with Amelia<a class="anchor" aria-label="anchor" href="#imputation-with-amelia"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb381"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://gking.harvard.edu/amelia">Amelia</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"iris"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Seed 10% missing values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">iris.mis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/prodNA.html">prodNA</a></span><span class="op">(</span><span class="va">iris</span>, noNA <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Specify columns and run Amelia</span></span>
<span><span class="va">amelia_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Amelia/man/amelia.html">amelia</a></span><span class="op">(</span></span>
<span>  <span class="va">iris.mis</span>,</span>
<span>  m <span class="op">=</span> <span class="fl">5</span>,                      <span class="co"># Number of imputations</span></span>
<span>  parallel <span class="op">=</span> <span class="st">"multicore"</span>,     <span class="co"># Use multicore processing</span></span>
<span>  noms <span class="op">=</span> <span class="st">"Species"</span>            <span class="co"># Nominal variables</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; -- Imputation 1 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5  6  7</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 2 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 3 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 4 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5  6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; -- Imputation 5 --</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1  2  3  4  5  6  7  8  9 10</span></span>
<span></span>
<span><span class="co"># Access imputed outputs</span></span>
<span><span class="co"># amelia_fit$imputations[[1]]</span></span></code></pre></div>
<p>Amelia’s workflow includes bootstrapping multiple imputations to generate robust estimates of means and variances. This process ensures flexibility and speed for large datasets.</p>
</div>
</div>
<div id="missforest" class="section level3" number="11.9.9">
<h3>
<span class="header-section-number">11.9.9</span> missForest<a class="anchor" aria-label="anchor" href="#missforest"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>missForest</code> package provides a robust non-parametric imputation method using the Random Forest algorithm. It is versatile, handling both continuous and categorical variables without requiring assumptions about the underlying functional forms.</p>
<p><strong>Key Features of missForest</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Non-Parametric</strong>: No assumptions about the functional form.</li>
<li>
<strong>Variable-Specific Models</strong>: Builds a random forest model for each variable to impute missing values.</li>
<li>
<strong>Error Estimates</strong>: Provides out-of-bag (OOB) imputation error estimates.
<ul>
<li>
<strong>NRMSE</strong> (Normalized Root Mean Squared Error) for continuous variables.</li>
<li>
<strong>PFC</strong> (Proportion of Falsely Classified) for categorical variables.</li>
</ul>
</li>
<li>
<strong>High Control</strong>: Offers customizable parameters like <code>mtry</code> and <code>ntree</code>.</li>
</ol>
<div class="sourceCode" id="cb382"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org">missForest</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Impute missing values using default parameters</span></span>
<span><span class="va">iris.imp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/missForest.html">missForest</a></span><span class="op">(</span><span class="va">iris.mis</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Check imputed values</span></span>
<span><span class="co"># View the imputed dataset</span></span>
<span><span class="co"># iris.imp$ximp</span></span></code></pre></div>
<div class="sourceCode" id="cb383"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Out-of-bag error estimates</span></span>
<span><span class="va">iris.imp</span><span class="op">$</span><span class="va">OOBerror</span></span>
<span><span class="co">#&gt;      NRMSE        PFC </span></span>
<span><span class="co">#&gt; 0.14004144 0.02877698</span></span>
<span></span>
<span><span class="co"># Compare imputed data with original data to calculate error</span></span>
<span><span class="va">iris.err</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/missForest/man/mixError.html">mixError</a></span><span class="op">(</span><span class="va">iris.imp</span><span class="op">$</span><span class="va">ximp</span>, <span class="va">iris.mis</span>, <span class="va">iris</span><span class="op">)</span></span>
<span><span class="va">iris.err</span></span>
<span><span class="co">#&gt;      NRMSE        PFC </span></span>
<span><span class="co">#&gt; 0.14420833 0.09090909</span></span></code></pre></div>
</div>
<div id="hmisc" class="section level3" number="11.9.10">
<h3>
<span class="header-section-number">11.9.10</span> Hmisc<a class="anchor" aria-label="anchor" href="#hmisc"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>Hmisc</code> package provides a suite of tools for imputing missing data, offering both simple methods (like mean or median imputation) and more advanced approaches like <code>aregImpute</code>.</p>
<p><strong>Features of Hmisc</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong><code><a href="https://rdrr.io/pkg/Hmisc/man/impute.html">impute()</a></code></strong>: Simple imputation using user-defined methods like mean, median, or a random value.</p></li>
<li>
<p><strong><code><a href="https://rdrr.io/pkg/Hmisc/man/aregImpute.html">aregImpute()</a></code></strong>:</p>
<ul>
<li><p>Combines additive regression, bootstrapping, and predictive mean matching.</p></li>
<li><p>Handles continuous and categorical variables.</p></li>
<li><p>Automatically recognizes variable types and applies appropriate methods.</p></li>
</ul>
</li>
</ol>
<p><strong>Assumptions</strong></p>
<ul>
<li><p>Linearity in the variables being predicted.</p></li>
<li><p>Fisher’s optimum scoring is used for categorical variable prediction.</p></li>
</ul>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="imputation-missing-data.html#cb384-1" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb384-2"><a href="imputation-missing-data.html#cb384-2" tabindex="-1"></a></span>
<span id="cb384-3"><a href="imputation-missing-data.html#cb384-3" tabindex="-1"></a><span class="co"># Impute using mean</span></span>
<span id="cb384-4"><a href="imputation-missing-data.html#cb384-4" tabindex="-1"></a>iris.mis<span class="sc">$</span>imputed_SepalLength <span class="ot">&lt;-</span> <span class="fu">with</span>(iris.mis, <span class="fu">impute</span>(Sepal.Length, mean))</span>
<span id="cb384-5"><a href="imputation-missing-data.html#cb384-5" tabindex="-1"></a></span>
<span id="cb384-6"><a href="imputation-missing-data.html#cb384-6" tabindex="-1"></a><span class="co"># Impute using random value</span></span>
<span id="cb384-7"><a href="imputation-missing-data.html#cb384-7" tabindex="-1"></a>iris.mis<span class="sc">$</span>imputed_SepalLength2 <span class="ot">&lt;-</span> <span class="fu">with</span>(iris.mis, <span class="fu">impute</span>(Sepal.Length, <span class="st">'random'</span>))</span>
<span id="cb384-8"><a href="imputation-missing-data.html#cb384-8" tabindex="-1"></a></span>
<span id="cb384-9"><a href="imputation-missing-data.html#cb384-9" tabindex="-1"></a><span class="co"># Advanced imputation using aregImpute</span></span>
<span id="cb384-10"><a href="imputation-missing-data.html#cb384-10" tabindex="-1"></a>impute_arg <span class="ot">&lt;-</span> <span class="fu">aregImpute</span>(</span>
<span id="cb384-11"><a href="imputation-missing-data.html#cb384-11" tabindex="-1"></a>  <span class="sc">~</span> Sepal.Length <span class="sc">+</span> Sepal.Width <span class="sc">+</span> Petal.Length <span class="sc">+</span> Petal.Width <span class="sc">+</span> Species,</span>
<span id="cb384-12"><a href="imputation-missing-data.html#cb384-12" tabindex="-1"></a>  <span class="at">data =</span> iris.mis,</span>
<span id="cb384-13"><a href="imputation-missing-data.html#cb384-13" tabindex="-1"></a>  <span class="at">n.impute =</span> <span class="dv">5</span></span>
<span id="cb384-14"><a href="imputation-missing-data.html#cb384-14" tabindex="-1"></a>)</span>
<span id="cb384-15"><a href="imputation-missing-data.html#cb384-15" tabindex="-1"></a><span class="co">#&gt; Iteration 1 </span></span>
<span id="cb384-16"><a href="imputation-missing-data.html#cb384-16" tabindex="-1"></a>Iteration <span class="dv">2</span> </span>
<span id="cb384-17"><a href="imputation-missing-data.html#cb384-17" tabindex="-1"></a>Iteration <span class="dv">3</span> </span>
<span id="cb384-18"><a href="imputation-missing-data.html#cb384-18" tabindex="-1"></a>Iteration <span class="dv">4</span> </span>
<span id="cb384-19"><a href="imputation-missing-data.html#cb384-19" tabindex="-1"></a>Iteration <span class="dv">5</span> </span>
<span id="cb384-20"><a href="imputation-missing-data.html#cb384-20" tabindex="-1"></a>Iteration <span class="dv">6</span> </span>
<span id="cb384-21"><a href="imputation-missing-data.html#cb384-21" tabindex="-1"></a>Iteration <span class="dv">7</span> </span>
<span id="cb384-22"><a href="imputation-missing-data.html#cb384-22" tabindex="-1"></a>Iteration <span class="dv">8</span> </span>
<span id="cb384-23"><a href="imputation-missing-data.html#cb384-23" tabindex="-1"></a></span>
<span id="cb384-24"><a href="imputation-missing-data.html#cb384-24" tabindex="-1"></a><span class="co"># Check R-squared values for predicted missing values</span></span>
<span id="cb384-25"><a href="imputation-missing-data.html#cb384-25" tabindex="-1"></a>impute_arg</span>
<span id="cb384-26"><a href="imputation-missing-data.html#cb384-26" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb384-27"><a href="imputation-missing-data.html#cb384-27" tabindex="-1"></a><span class="co">#&gt; Multiple Imputation using Bootstrap and PMM</span></span>
<span id="cb384-28"><a href="imputation-missing-data.html#cb384-28" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb384-29"><a href="imputation-missing-data.html#cb384-29" tabindex="-1"></a><span class="co">#&gt; aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + </span></span>
<span id="cb384-30"><a href="imputation-missing-data.html#cb384-30" tabindex="-1"></a><span class="co">#&gt;     Petal.Width + Species, data = iris.mis, n.impute = 5)</span></span>
<span id="cb384-31"><a href="imputation-missing-data.html#cb384-31" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb384-32"><a href="imputation-missing-data.html#cb384-32" tabindex="-1"></a><span class="co">#&gt; n: 150   p: 5    Imputations: 5      nk: 3 </span></span>
<span id="cb384-33"><a href="imputation-missing-data.html#cb384-33" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb384-34"><a href="imputation-missing-data.html#cb384-34" tabindex="-1"></a><span class="co">#&gt; Number of NAs:</span></span>
<span id="cb384-35"><a href="imputation-missing-data.html#cb384-35" tabindex="-1"></a><span class="co">#&gt; Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species </span></span>
<span id="cb384-36"><a href="imputation-missing-data.html#cb384-36" tabindex="-1"></a><span class="co">#&gt;           17           19           12           16           11 </span></span>
<span id="cb384-37"><a href="imputation-missing-data.html#cb384-37" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb384-38"><a href="imputation-missing-data.html#cb384-38" tabindex="-1"></a><span class="co">#&gt;              type d.f.</span></span>
<span id="cb384-39"><a href="imputation-missing-data.html#cb384-39" tabindex="-1"></a><span class="co">#&gt; Sepal.Length    s    2</span></span>
<span id="cb384-40"><a href="imputation-missing-data.html#cb384-40" tabindex="-1"></a><span class="co">#&gt; Sepal.Width     s    2</span></span>
<span id="cb384-41"><a href="imputation-missing-data.html#cb384-41" tabindex="-1"></a><span class="co">#&gt; Petal.Length    s    2</span></span>
<span id="cb384-42"><a href="imputation-missing-data.html#cb384-42" tabindex="-1"></a><span class="co">#&gt; Petal.Width     s    2</span></span>
<span id="cb384-43"><a href="imputation-missing-data.html#cb384-43" tabindex="-1"></a><span class="co">#&gt; Species         c    2</span></span>
<span id="cb384-44"><a href="imputation-missing-data.html#cb384-44" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb384-45"><a href="imputation-missing-data.html#cb384-45" tabindex="-1"></a><span class="co">#&gt; Transformation of Target Variables Forced to be Linear</span></span>
<span id="cb384-46"><a href="imputation-missing-data.html#cb384-46" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb384-47"><a href="imputation-missing-data.html#cb384-47" tabindex="-1"></a><span class="co">#&gt; R-squares for Predicting Non-Missing Values for Each Variable</span></span>
<span id="cb384-48"><a href="imputation-missing-data.html#cb384-48" tabindex="-1"></a><span class="co">#&gt; Using Last Imputations of Predictors</span></span>
<span id="cb384-49"><a href="imputation-missing-data.html#cb384-49" tabindex="-1"></a><span class="co">#&gt; Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species </span></span>
<span id="cb384-50"><a href="imputation-missing-data.html#cb384-50" tabindex="-1"></a><span class="co">#&gt;        0.895        0.536        0.987        0.967        0.984</span></span>
<span id="cb384-51"><a href="imputation-missing-data.html#cb384-51" tabindex="-1"></a></span>
<span id="cb384-52"><a href="imputation-missing-data.html#cb384-52" tabindex="-1"></a><span class="co"># Access imputed values for Sepal.Length</span></span>
<span id="cb384-53"><a href="imputation-missing-data.html#cb384-53" tabindex="-1"></a>impute_arg<span class="sc">$</span>imputed<span class="sc">$</span>Sepal.Length</span>
<span id="cb384-54"><a href="imputation-missing-data.html#cb384-54" tabindex="-1"></a><span class="co">#&gt;     [,1] [,2] [,3] [,4] [,5]</span></span>
<span id="cb384-55"><a href="imputation-missing-data.html#cb384-55" tabindex="-1"></a><span class="co">#&gt; 13   4.4  4.9  4.9  5.0  4.9</span></span>
<span id="cb384-56"><a href="imputation-missing-data.html#cb384-56" tabindex="-1"></a><span class="co">#&gt; 14   4.8  4.4  5.0  4.5  4.5</span></span>
<span id="cb384-57"><a href="imputation-missing-data.html#cb384-57" tabindex="-1"></a><span class="co">#&gt; 23   4.8  5.1  5.1  5.1  4.8</span></span>
<span id="cb384-58"><a href="imputation-missing-data.html#cb384-58" tabindex="-1"></a><span class="co">#&gt; 26   5.0  4.8  4.9  4.9  5.0</span></span>
<span id="cb384-59"><a href="imputation-missing-data.html#cb384-59" tabindex="-1"></a><span class="co">#&gt; 34   5.0  5.8  6.0  5.7  5.8</span></span>
<span id="cb384-60"><a href="imputation-missing-data.html#cb384-60" tabindex="-1"></a><span class="co">#&gt; 39   4.4  4.9  5.0  4.5  4.6</span></span>
<span id="cb384-61"><a href="imputation-missing-data.html#cb384-61" tabindex="-1"></a><span class="co">#&gt; 41   5.2  5.1  4.8  5.0  4.8</span></span>
<span id="cb384-62"><a href="imputation-missing-data.html#cb384-62" tabindex="-1"></a><span class="co">#&gt; 69   5.8  6.0  6.3  6.0  6.1</span></span>
<span id="cb384-63"><a href="imputation-missing-data.html#cb384-63" tabindex="-1"></a><span class="co">#&gt; 72   5.6  5.7  5.7  5.8  6.1</span></span>
<span id="cb384-64"><a href="imputation-missing-data.html#cb384-64" tabindex="-1"></a><span class="co">#&gt; 89   6.1  5.7  5.7  5.6  6.9</span></span>
<span id="cb384-65"><a href="imputation-missing-data.html#cb384-65" tabindex="-1"></a><span class="co">#&gt; 90   5.5  6.2  5.2  6.0  5.8</span></span>
<span id="cb384-66"><a href="imputation-missing-data.html#cb384-66" tabindex="-1"></a><span class="co">#&gt; 91   5.7  6.9  6.0  6.4  6.4</span></span>
<span id="cb384-67"><a href="imputation-missing-data.html#cb384-67" tabindex="-1"></a><span class="co">#&gt; 116  5.9  6.8  6.4  6.6  6.9</span></span>
<span id="cb384-68"><a href="imputation-missing-data.html#cb384-68" tabindex="-1"></a><span class="co">#&gt; 118  7.9  7.9  7.9  7.9  7.9</span></span>
<span id="cb384-69"><a href="imputation-missing-data.html#cb384-69" tabindex="-1"></a><span class="co">#&gt; 135  6.7  6.7  6.7  6.9  6.7</span></span>
<span id="cb384-70"><a href="imputation-missing-data.html#cb384-70" tabindex="-1"></a><span class="co">#&gt; 141  7.0  6.3  5.9  6.7  7.0</span></span>
<span id="cb384-71"><a href="imputation-missing-data.html#cb384-71" tabindex="-1"></a><span class="co">#&gt; 143  5.7  6.7  5.8  6.3  5.4</span></span></code></pre></div>
<p><strong>Note</strong>: While <code>missForest</code> often outperforms <code>Hmisc</code> in terms of accuracy, the latter is useful for datasets with simpler requirements.</p>
</div>
<div id="mi" class="section level3" number="11.9.11">
<h3>
<span class="header-section-number">11.9.11</span> mi<a class="anchor" aria-label="anchor" href="#mi"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>mi</code> package is a powerful tool for imputation, using Bayesian methods and providing rich diagnostics for model evaluation and convergence.</p>
<p><strong>Features of mi</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Graphical Diagnostics</strong>: Visualize imputation models and convergence.</p></li>
<li><p><strong>Bayesian Regression</strong>: Handles separation and other issues in data.</p></li>
<li><p><strong>Irregularity Detection</strong>: Automatically detects issues like high collinearity.</p></li>
<li><p><strong>Noise Addition</strong>: Adds noise to address additive constraints.</p></li>
</ol>
<div class="sourceCode" id="cb385"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stat.columbia.edu/~gelman/">mi</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform imputation using mi</span></span>
<span><span class="va">mi_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mi/man/04mi.html">mi</a></span><span class="op">(</span><span class="va">iris.mis</span>, seed <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the imputation process</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mi_data</span><span class="op">)</span></span>
<span><span class="co">#&gt; $Sepal.Length</span></span>
<span><span class="co">#&gt; $Sepal.Length$is_missing</span></span>
<span><span class="co">#&gt; missing</span></span>
<span><span class="co">#&gt; FALSE  TRUE </span></span>
<span><span class="co">#&gt;   133    17 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Sepal.Length$imputed</span></span>
<span><span class="co">#&gt;       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. </span></span>
<span><span class="co">#&gt; -0.6355172 -0.0703238 -0.0005039 -0.0052716  0.0765631  0.3731257 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Sepal.Length$observed</span></span>
<span><span class="co">#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. </span></span>
<span><span class="co">#&gt; -0.90110 -0.47329 -0.04549  0.00000  0.32120  1.23792 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Sepal.Width</span></span>
<span><span class="co">#&gt; $Sepal.Width$is_missing</span></span>
<span><span class="co">#&gt; missing</span></span>
<span><span class="co">#&gt; FALSE  TRUE </span></span>
<span><span class="co">#&gt;   131    19 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Sepal.Width$imputed</span></span>
<span><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">#&gt; -2.1083 -0.4216 -0.1925 -0.1940  0.1589  0.7330 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Sepal.Width$observed</span></span>
<span><span class="co">#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. </span></span>
<span><span class="co">#&gt; -1.01272 -0.30642 -0.07099  0.00000  0.39988  1.34161 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Petal.Length</span></span>
<span><span class="co">#&gt; $Petal.Length$is_missing</span></span>
<span><span class="co">#&gt; missing</span></span>
<span><span class="co">#&gt; FALSE  TRUE </span></span>
<span><span class="co">#&gt;   138    12 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Petal.Length$imputed</span></span>
<span><span class="co">#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. </span></span>
<span><span class="co">#&gt; -0.86312 -0.58453  0.23556  0.04176  0.48870  0.77055 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Petal.Length$observed</span></span>
<span><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">#&gt; -0.7797 -0.6088  0.1459  0.0000  0.3880  0.9006 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Petal.Width</span></span>
<span><span class="co">#&gt; $Petal.Width$is_missing</span></span>
<span><span class="co">#&gt; missing</span></span>
<span><span class="co">#&gt; FALSE  TRUE </span></span>
<span><span class="co">#&gt;   134    16 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Petal.Width$imputed</span></span>
<span><span class="co">#&gt;       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. </span></span>
<span><span class="co">#&gt; -0.9116177 -0.0000042  0.2520468  0.1734543  0.5147010  0.8411324 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Petal.Width$observed</span></span>
<span><span class="co">#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. </span></span>
<span><span class="co">#&gt; -0.69624 -0.56602  0.08503  0.00000  0.41055  0.86629 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Species</span></span>
<span><span class="co">#&gt; $Species$crosstab</span></span>
<span><span class="co">#&gt;             </span></span>
<span><span class="co">#&gt;              observed imputed</span></span>
<span><span class="co">#&gt;   setosa          180      16</span></span>
<span><span class="co">#&gt;   versicolor      192      11</span></span>
<span><span class="co">#&gt;   virginica       184      17</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $imputed_SepalLength</span></span>
<span><span class="co">#&gt; $imputed_SepalLength$is_missing</span></span>
<span><span class="co">#&gt; [1] "all values observed"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $imputed_SepalLength$observed</span></span>
<span><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">#&gt; -0.9574 -0.4379  0.0000  0.0000  0.3413  1.3152 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $imputed_SepalLength2</span></span>
<span><span class="co">#&gt; $imputed_SepalLength2$is_missing</span></span>
<span><span class="co">#&gt; [1] "all values observed"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $imputed_SepalLength2$observed</span></span>
<span><span class="co">#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. </span></span>
<span><span class="co">#&gt; -0.90570 -0.48398 -0.06225  0.00000  0.35947  1.20292</span></span></code></pre></div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></div>
<div class="next"><a href="data.html"><span class="header-section-number">12</span> Data</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#imputation-missing-data"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li>
<a class="nav-link" href="#introduction-to-missing-data"><span class="header-section-number">11.1</span> Introduction to Missing Data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#types-of-imputation"><span class="header-section-number">11.1.1</span> Types of Imputation</a></li>
<li><a class="nav-link" href="#when-and-why-to-use-imputation"><span class="header-section-number">11.1.2</span> When and Why to Use Imputation</a></li>
<li><a class="nav-link" href="#importance-of-missing-data-treatment-in-statistical-modeling"><span class="header-section-number">11.1.3</span> Importance of Missing Data Treatment in Statistical Modeling</a></li>
<li><a class="nav-link" href="#prevalence-of-missing-data-across-domains"><span class="header-section-number">11.1.4</span> Prevalence of Missing Data Across Domains</a></li>
<li><a class="nav-link" href="#practical-considerations-for-imputation"><span class="header-section-number">11.1.5</span> Practical Considerations for Imputation</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#theoretical-foundations-of-missing-data"><span class="header-section-number">11.2</span> Theoretical Foundations of Missing Data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition-and-classification-of-missing-data"><span class="header-section-number">11.2.1</span> Definition and Classification of Missing Data</a></li>
<li><a class="nav-link" href="#missing-data-mechanisms"><span class="header-section-number">11.2.2</span> Missing Data Mechanisms</a></li>
<li><a class="nav-link" href="#relationship-between-mechanisms-and-ignorability"><span class="header-section-number">11.2.3</span> Relationship Between Mechanisms and Ignorability</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#diagnosing-the-missing-data-mechanism"><span class="header-section-number">11.3</span> Diagnosing the Missing Data Mechanism</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#descriptive-methods"><span class="header-section-number">11.3.1</span> Descriptive Methods</a></li>
<li><a class="nav-link" href="#statistical-tests-for-missing-data-mechanisms"><span class="header-section-number">11.3.2</span> Statistical Tests for Missing Data Mechanisms</a></li>
<li><a class="nav-link" href="#assessing-mar-and-mnar"><span class="header-section-number">11.3.3</span> Assessing MAR and MNAR</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#methods-for-handling-missing-data"><span class="header-section-number">11.4</span> Methods for Handling Missing Data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#basic-methods"><span class="header-section-number">11.4.1</span> Basic Methods</a></li>
<li><a class="nav-link" href="#single-imputation-techniques"><span class="header-section-number">11.4.2</span> Single Imputation Techniques</a></li>
<li><a class="nav-link" href="#machine-learning-and-modern-approaches"><span class="header-section-number">11.4.3</span> Machine Learning and Modern Approaches</a></li>
<li><a class="nav-link" href="#multiple-imputation"><span class="header-section-number">11.4.4</span> Multiple Imputation</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#evaluation-of-imputation-methods"><span class="header-section-number">11.5</span> Evaluation of Imputation Methods</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#statistical-metrics-for-assessing-imputation-quality"><span class="header-section-number">11.5.1</span> Statistical Metrics for Assessing Imputation Quality</a></li>
<li><a class="nav-link" href="#bias-variance-tradeoff-in-imputation"><span class="header-section-number">11.5.2</span> Bias-Variance Tradeoff in Imputation</a></li>
<li><a class="nav-link" href="#sensitivity-analysis-1"><span class="header-section-number">11.5.3</span> Sensitivity Analysis</a></li>
<li><a class="nav-link" href="#validation-using-simulated-data-and-real-world-case-studies"><span class="header-section-number">11.5.4</span> Validation Using Simulated Data and Real-World Case Studies</a></li>
</ul>
</li>
<li><a class="nav-link" href="#criteria-for-choosing-an-effective-approach"><span class="header-section-number">11.6</span> Criteria for Choosing an Effective Approach</a></li>
<li>
<a class="nav-link" href="#challenges-and-ethical-considerations"><span class="header-section-number">11.7</span> Challenges and Ethical Considerations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#challenges-in-high-dimensional-data"><span class="header-section-number">11.7.1</span> Challenges in High-Dimensional Data</a></li>
<li><a class="nav-link" href="#missing-data-in-big-data-contexts"><span class="header-section-number">11.7.2</span> Missing Data in Big Data Contexts</a></li>
<li><a class="nav-link" href="#ethical-concerns"><span class="header-section-number">11.7.3</span> Ethical Concerns</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#emerging-trends-in-missing-data-handling"><span class="header-section-number">11.8</span> Emerging Trends in Missing Data Handling</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#advances-in-neural-network-approaches"><span class="header-section-number">11.8.1</span> Advances in Neural Network Approaches</a></li>
<li><a class="nav-link" href="#integration-with-reinforcement-learning"><span class="header-section-number">11.8.2</span> Integration with Reinforcement Learning</a></li>
<li><a class="nav-link" href="#synthetic-data-generation-for-missing-data"><span class="header-section-number">11.8.3</span> Synthetic Data Generation for Missing Data</a></li>
<li><a class="nav-link" href="#federated-learning-and-privacy-preserving-imputation"><span class="header-section-number">11.8.4</span> Federated Learning and Privacy-Preserving Imputation</a></li>
<li><a class="nav-link" href="#imputation-in-streaming-and-online-data-environments"><span class="header-section-number">11.8.5</span> Imputation in Streaming and Online Data Environments</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#application-of-imputation-in-r"><span class="header-section-number">11.9</span> Application of Imputation in R</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#visualizing-missing-data"><span class="header-section-number">11.9.1</span> Visualizing Missing Data</a></li>
<li><a class="nav-link" href="#how-many-imputations"><span class="header-section-number">11.9.2</span> How Many Imputations?</a></li>
<li><a class="nav-link" href="#generating-missing-data-for-demonstration"><span class="header-section-number">11.9.3</span> Generating Missing Data for Demonstration</a></li>
<li><a class="nav-link" href="#imputation-with-mean-median-and-mode"><span class="header-section-number">11.9.4</span> Imputation with Mean, Median, and Mode</a></li>
<li><a class="nav-link" href="#k-nearest-neighbors-knn-imputation"><span class="header-section-number">11.9.5</span> K-Nearest Neighbors (KNN) Imputation</a></li>
<li><a class="nav-link" href="#imputation-with-decision-trees-rpart"><span class="header-section-number">11.9.6</span> Imputation with Decision Trees (rpart)</a></li>
<li><a class="nav-link" href="#mice-multivariate-imputation-via-chained-equations"><span class="header-section-number">11.9.7</span> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li><a class="nav-link" href="#amelia"><span class="header-section-number">11.9.8</span> Amelia</a></li>
<li><a class="nav-link" href="#missforest"><span class="header-section-number">11.9.9</span> missForest</a></li>
<li><a class="nav-link" href="#hmisc"><span class="header-section-number">11.9.10</span> Hmisc</a></li>
<li><a class="nav-link" href="#mi"><span class="header-section-number">11.9.11</span> mi</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/11-imputation.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/11-imputation.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-01-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
