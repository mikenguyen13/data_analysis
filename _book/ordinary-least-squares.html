<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.1 Ordinary Least Squares | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="5.1 Ordinary Least Squares | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg" />
  <meta property="og:description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.1 Ordinary Least Squares | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg" />

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2025-05-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="logo.png" />
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="generalized-least-squares.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DMNX2X65HQ');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="how-to-cite-this-book.html"><a href="how-to-cite-this-book.html"><i class="fa fa-check"></i>How to cite this book</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="general-recommendations.html"><a href="general-recommendations.html"><i class="fa fa-check"></i><b>1.1</b> General Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>2.1</b> Matrix Theory</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.1</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Inverse of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Definiteness of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>2.1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="2.1.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization-in-scalar-and-vector-spaces"><i class="fa fa-check"></i><b>2.1.5</b> Optimization in Scalar and Vector Spaces</a></li>
<li class="chapter" data-level="2.1.6" data-path="matrix-theory.html"><a href="matrix-theory.html#cholesky-decomposition"><i class="fa fa-check"></i><b>2.1.6</b> Cholesky Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.2</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#axioms-and-theorems-of-probability"><i class="fa fa-check"></i><b>2.2.1</b> Axioms and Theorems of Probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>2.2.3</b> Random Variable</a></li>
<li class="chapter" data-level="2.2.4" data-path="probability-theory.html"><a href="probability-theory.html#moment-generating-function"><i class="fa fa-check"></i><b>2.2.4</b> Moment Generating Function</a></li>
<li class="chapter" data-level="2.2.5" data-path="probability-theory.html"><a href="probability-theory.html#moments"><i class="fa fa-check"></i><b>2.2.5</b> Moments</a></li>
<li class="chapter" data-level="2.2.6" data-path="probability-theory.html"><a href="probability-theory.html#skewness"><i class="fa fa-check"></i><b>2.2.6</b> Skewness</a></li>
<li class="chapter" data-level="2.2.7" data-path="probability-theory.html"><a href="probability-theory.html#kurtosis"><i class="fa fa-check"></i><b>2.2.7</b> Kurtosis</a></li>
<li class="chapter" data-level="2.2.8" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>2.2.8</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>2.3</b> General Math</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="general-math.html"><a href="general-math.html#number-sets"><i class="fa fa-check"></i><b>2.3.1</b> Number Sets</a></li>
<li class="chapter" data-level="2.3.2" data-path="general-math.html"><a href="general-math.html#summation-notation-and-series"><i class="fa fa-check"></i><b>2.3.2</b> Summation Notation and Series</a></li>
<li class="chapter" data-level="2.3.3" data-path="general-math.html"><a href="general-math.html#taylor-expansion"><i class="fa fa-check"></i><b>2.3.3</b> Taylor Expansion</a></li>
<li class="chapter" data-level="2.3.4" data-path="general-math.html"><a href="general-math.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.3.4</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="2.3.5" data-path="general-math.html"><a href="general-math.html#convergence"><i class="fa fa-check"></i><b>2.3.5</b> Convergence</a></li>
<li class="chapter" data-level="2.3.6" data-path="general-math.html"><a href="general-math.html#sufficient-statistics-and-likelihood"><i class="fa fa-check"></i><b>2.3.6</b> Sufficient Statistics and Likelihood</a></li>
<li class="chapter" data-level="2.3.7" data-path="general-math.html"><a href="general-math.html#parameter-transformations"><i class="fa fa-check"></i><b>2.3.7</b> Parameter Transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-importexport.html"><a href="data-importexport.html"><i class="fa fa-check"></i><b>2.4</b> Data Import/Export</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-importexport.html"><a href="data-importexport.html#key-limitations-of-r"><i class="fa fa-check"></i><b>2.4.1</b> Key Limitations of R</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-importexport.html"><a href="data-importexport.html#solutions-and-workarounds"><i class="fa fa-check"></i><b>2.4.2</b> Solutions and Workarounds</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-importexport.html"><a href="data-importexport.html#medium-size"><i class="fa fa-check"></i><b>2.4.3</b> Medium size</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-importexport.html"><a href="data-importexport.html#large-size"><i class="fa fa-check"></i><b>2.4.4</b> Large size</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>2.5</b> Data Manipulation</a></li>
</ul></li>
<li class="part"><span><b>I. BASIC</b></span></li>
<li class="chapter" data-level="3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
<li class="chapter" data-level="3.2.2" data-path="graphical-measures.html"><a href="graphical-measures.html#scatterplot"><i class="fa fa-check"></i><b>3.2.2</b> Scatterplot</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="bivariate-statistics.html"><a href="bivariate-statistics.html"><i class="fa fa-check"></i><b>3.4</b> Bivariate Statistics</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="bivariate-statistics.html"><a href="bivariate-statistics.html#two-continuous"><i class="fa fa-check"></i><b>3.4.1</b> Two Continuous</a></li>
<li class="chapter" data-level="3.4.2" data-path="bivariate-statistics.html"><a href="bivariate-statistics.html#categorical-and-continuous"><i class="fa fa-check"></i><b>3.4.2</b> Categorical and Continuous</a></li>
<li class="chapter" data-level="3.4.3" data-path="bivariate-statistics.html"><a href="bivariate-statistics.html#two-discrete"><i class="fa fa-check"></i><b>3.4.3</b> Two Discrete</a></li>
<li class="chapter" data-level="3.4.4" data-path="bivariate-statistics.html"><a href="bivariate-statistics.html#general-approach-to-bivariate-statistics"><i class="fa fa-check"></i><b>3.4.4</b> General Approach to Bivariate Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="4.1" data-path="hypothesis-testing-framework.html"><a href="hypothesis-testing-framework.html"><i class="fa fa-check"></i><b>4.1</b> Hypothesis Testing Framework</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="hypothesis-testing-framework.html"><a href="hypothesis-testing-framework.html#null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>4.1.1</b> Null and Alternative Hypotheses</a></li>
<li class="chapter" data-level="4.1.2" data-path="hypothesis-testing-framework.html"><a href="hypothesis-testing-framework.html#errors-in-hypothesis-testing"><i class="fa fa-check"></i><b>4.1.2</b> Errors in Hypothesis Testing</a></li>
<li class="chapter" data-level="4.1.3" data-path="hypothesis-testing-framework.html"><a href="hypothesis-testing-framework.html#the-role-of-distributions-in-hypothesis-testing"><i class="fa fa-check"></i><b>4.1.3</b> The Role of Distributions in Hypothesis Testing</a></li>
<li class="chapter" data-level="4.1.4" data-path="hypothesis-testing-framework.html"><a href="hypothesis-testing-framework.html#the-test-statistic"><i class="fa fa-check"></i><b>4.1.4</b> The Test Statistic</a></li>
<li class="chapter" data-level="4.1.5" data-path="hypothesis-testing-framework.html"><a href="hypothesis-testing-framework.html#critical-values-and-rejection-regions-1"><i class="fa fa-check"></i><b>4.1.5</b> Critical Values and Rejection Regions</a></li>
<li class="chapter" data-level="4.1.6" data-path="hypothesis-testing-framework.html"><a href="hypothesis-testing-framework.html#visualizing-hypothesis-testing"><i class="fa fa-check"></i><b>4.1.6</b> Visualizing Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="key-concepts-and-definitions.html"><a href="key-concepts-and-definitions.html"><i class="fa fa-check"></i><b>4.2</b> Key Concepts and Definitions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="key-concepts-and-definitions.html"><a href="key-concepts-and-definitions.html#random-sample"><i class="fa fa-check"></i><b>4.2.1</b> Random Sample</a></li>
<li class="chapter" data-level="4.2.2" data-path="key-concepts-and-definitions.html"><a href="key-concepts-and-definitions.html#sample-statistics"><i class="fa fa-check"></i><b>4.2.2</b> Sample Statistics</a></li>
<li class="chapter" data-level="4.2.3" data-path="key-concepts-and-definitions.html"><a href="key-concepts-and-definitions.html#distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>4.2.3</b> Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.3</b> One-Sample Inference</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html#for-single-mean"><i class="fa fa-check"></i><b>4.3.1</b> For Single Mean</a></li>
<li class="chapter" data-level="4.3.2" data-path="one-sample-inference.html"><a href="one-sample-inference.html#for-difference-of-means-independent-samples"><i class="fa fa-check"></i><b>4.3.2</b> For Difference of Means, Independent Samples</a></li>
<li class="chapter" data-level="4.3.3" data-path="one-sample-inference.html"><a href="one-sample-inference.html#for-difference-of-means-paired-samples"><i class="fa fa-check"></i><b>4.3.3</b> For Difference of Means, Paired Samples</a></li>
<li class="chapter" data-level="4.3.4" data-path="one-sample-inference.html"><a href="one-sample-inference.html#for-difference-of-two-proportions"><i class="fa fa-check"></i><b>4.3.4</b> For Difference of Two Proportions</a></li>
<li class="chapter" data-level="4.3.5" data-path="one-sample-inference.html"><a href="one-sample-inference.html#for-single-proportion"><i class="fa fa-check"></i><b>4.3.5</b> For Single Proportion</a></li>
<li class="chapter" data-level="4.3.6" data-path="one-sample-inference.html"><a href="one-sample-inference.html#for-single-variance"><i class="fa fa-check"></i><b>4.3.6</b> For Single Variance</a></li>
<li class="chapter" data-level="4.3.7" data-path="one-sample-inference.html"><a href="one-sample-inference.html#non-parametric-tests"><i class="fa fa-check"></i><b>4.3.7</b> Non-parametric Tests</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.4</b> Two-Sample Inference</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="two-sample-inference.html"><a href="two-sample-inference.html#for-means"><i class="fa fa-check"></i><b>4.4.1</b> For Means</a></li>
<li class="chapter" data-level="4.4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html#for-variances"><i class="fa fa-check"></i><b>4.4.2</b> For Variances</a></li>
<li class="chapter" data-level="4.4.3" data-path="two-sample-inference.html"><a href="two-sample-inference.html#power"><i class="fa fa-check"></i><b>4.4.3</b> Power</a></li>
<li class="chapter" data-level="4.4.4" data-path="two-sample-inference.html"><a href="two-sample-inference.html#matched-pair-designs"><i class="fa fa-check"></i><b>4.4.4</b> Matched Pair Designs</a></li>
<li class="chapter" data-level="4.4.5" data-path="two-sample-inference.html"><a href="two-sample-inference.html#nonparametric-tests-for-two-samples"><i class="fa fa-check"></i><b>4.4.5</b> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.5</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#association-tests"><i class="fa fa-check"></i><b>4.5.1</b> Association Tests</a></li>
<li class="chapter" data-level="4.5.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#ordinal-association"><i class="fa fa-check"></i><b>4.5.2</b> Ordinal Association</a></li>
<li class="chapter" data-level="4.5.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#ordinal-trend"><i class="fa fa-check"></i><b>4.5.3</b> Ordinal Trend</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html"><i class="fa fa-check"></i><b>4.6</b> Divergence Metrics and Tests for Comparing Distributions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#kolmogorov-smirnov-test-1"><i class="fa fa-check"></i><b>4.6.1</b> Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="4.6.2" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#anderson-darling-test-1"><i class="fa fa-check"></i><b>4.6.2</b> Anderson-Darling Test</a></li>
<li class="chapter" data-level="4.6.3" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>4.6.3</b> Chi-Square Goodness-of-Fit Test</a></li>
<li class="chapter" data-level="4.6.4" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#cramér-von-mises-test"><i class="fa fa-check"></i><b>4.6.4</b> Cramér-von Mises Test</a></li>
<li class="chapter" data-level="4.6.5" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>4.6.5</b> Kullback-Leibler Divergence</a></li>
<li class="chapter" data-level="4.6.6" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#jensen-shannon-divergence"><i class="fa fa-check"></i><b>4.6.6</b> Jensen-Shannon Divergence</a></li>
<li class="chapter" data-level="4.6.7" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#hellinger-distance"><i class="fa fa-check"></i><b>4.6.7</b> Hellinger Distance</a></li>
<li class="chapter" data-level="4.6.8" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#bhattacharyya-distance"><i class="fa fa-check"></i><b>4.6.8</b> Bhattacharyya Distance</a></li>
<li class="chapter" data-level="4.6.9" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#wasserstein-distance"><i class="fa fa-check"></i><b>4.6.9</b> Wasserstein Distance</a></li>
<li class="chapter" data-level="4.6.10" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#energy-distance"><i class="fa fa-check"></i><b>4.6.10</b> Energy Distance</a></li>
<li class="chapter" data-level="4.6.11" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#total-variation-distance"><i class="fa fa-check"></i><b>4.6.11</b> Total Variation Distance</a></li>
<li class="chapter" data-level="4.6.12" data-path="divergence-metrics-and-tests-for-comparing-distributions.html"><a href="divergence-metrics-and-tests-for-comparing-distributions.html#summary"><i class="fa fa-check"></i><b>4.6.12</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II. REGRESSION</b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>5.1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#simple-regression-basic-model"><i class="fa fa-check"></i><b>5.1.1</b> Simple Regression (Basic) Model</a></li>
<li class="chapter" data-level="5.1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multiple-linear-regression"><i class="fa fa-check"></i><b>5.1.2</b> Multiple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>5.2</b> Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#infeasible-generalized-least-squares"><i class="fa fa-check"></i><b>5.2.1</b> Infeasible Generalized Least Squares</a></li>
<li class="chapter" data-level="5.2.2" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#feasible-generalized-least-squares"><i class="fa fa-check"></i><b>5.2.2</b> Feasible Generalized Least Squares</a></li>
<li class="chapter" data-level="5.2.3" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#weighted-least-squares"><i class="fa fa-check"></i><b>5.2.3</b> Weighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html"><i class="fa fa-check"></i><b>5.3</b> Maximum Likelihood</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html#motivation-for-mle"><i class="fa fa-check"></i><b>5.3.1</b> Motivation for MLE</a></li>
<li class="chapter" data-level="5.3.2" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html#key-quantities-for-inference"><i class="fa fa-check"></i><b>5.3.2</b> Key Quantities for Inference</a></li>
<li class="chapter" data-level="5.3.3" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html#assumptions-of-mle"><i class="fa fa-check"></i><b>5.3.3</b> Assumptions of MLE</a></li>
<li class="chapter" data-level="5.3.4" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html#properties-of-mle"><i class="fa fa-check"></i><b>5.3.4</b> Properties of MLE</a></li>
<li class="chapter" data-level="5.3.5" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html#practical-considerations"><i class="fa fa-check"></i><b>5.3.5</b> Practical Considerations</a></li>
<li class="chapter" data-level="5.3.6" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html#comparison-of-mle-and-ols"><i class="fa fa-check"></i><b>5.3.6</b> Comparison of MLE and OLS</a></li>
<li class="chapter" data-level="5.3.7" data-path="maximum-likelihood-estimator.html"><a href="maximum-likelihood-estimator.html#applications-of-mle"><i class="fa fa-check"></i><b>5.3.7</b> Applications of MLE</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="penalized-regularized-estimators.html"><a href="penalized-regularized-estimators.html"><i class="fa fa-check"></i><b>5.4</b> Penalized (Regularized) Estimators</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="penalized-regularized-estimators.html"><a href="penalized-regularized-estimators.html#motivation-for-penalized-estimators"><i class="fa fa-check"></i><b>5.4.1</b> Motivation for Penalized Estimators</a></li>
<li class="chapter" data-level="5.4.2" data-path="penalized-regularized-estimators.html"><a href="penalized-regularized-estimators.html#ridge-regression"><i class="fa fa-check"></i><b>5.4.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.4.3" data-path="penalized-regularized-estimators.html"><a href="penalized-regularized-estimators.html#lasso-regression"><i class="fa fa-check"></i><b>5.4.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="5.4.4" data-path="penalized-regularized-estimators.html"><a href="penalized-regularized-estimators.html#elastic-net"><i class="fa fa-check"></i><b>5.4.4</b> Elastic Net</a></li>
<li class="chapter" data-level="5.4.5" data-path="penalized-regularized-estimators.html"><a href="penalized-regularized-estimators.html#tuning-parameter-selection"><i class="fa fa-check"></i><b>5.4.5</b> Tuning Parameter Selection</a></li>
<li class="chapter" data-level="5.4.6" data-path="penalized-regularized-estimators.html"><a href="penalized-regularized-estimators.html#properties-of-penalized-estimators"><i class="fa fa-check"></i><b>5.4.6</b> Properties of Penalized Estimators</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="robust-estimators.html"><a href="robust-estimators.html"><i class="fa fa-check"></i><b>5.5</b> Robust Estimators</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="robust-estimators.html"><a href="robust-estimators.html#motivation-for-robust-estimation"><i class="fa fa-check"></i><b>5.5.1</b> Motivation for Robust Estimation</a></li>
<li class="chapter" data-level="5.5.2" data-path="robust-estimators.html"><a href="robust-estimators.html#m-estimators"><i class="fa fa-check"></i><b>5.5.2</b> <span class="math inline">\(M\)</span>-Estimators</a></li>
<li class="chapter" data-level="5.5.3" data-path="robust-estimators.html"><a href="robust-estimators.html#r-estimators"><i class="fa fa-check"></i><b>5.5.3</b> <span class="math inline">\(R\)</span>-Estimators</a></li>
<li class="chapter" data-level="5.5.4" data-path="robust-estimators.html"><a href="robust-estimators.html#l-estimators"><i class="fa fa-check"></i><b>5.5.4</b> <span class="math inline">\(L\)</span>-Estimators</a></li>
<li class="chapter" data-level="5.5.5" data-path="robust-estimators.html"><a href="robust-estimators.html#least-trimmed-squares-lts"><i class="fa fa-check"></i><b>5.5.5</b> Least Trimmed Squares (LTS)</a></li>
<li class="chapter" data-level="5.5.6" data-path="robust-estimators.html"><a href="robust-estimators.html#s-estimators"><i class="fa fa-check"></i><b>5.5.6</b> <span class="math inline">\(S\)</span>-Estimators</a></li>
<li class="chapter" data-level="5.5.7" data-path="robust-estimators.html"><a href="robust-estimators.html#mm-estimators"><i class="fa fa-check"></i><b>5.5.7</b> <span class="math inline">\(MM\)</span>-Estimators</a></li>
<li class="chapter" data-level="5.5.8" data-path="robust-estimators.html"><a href="robust-estimators.html#practical-considerations-1"><i class="fa fa-check"></i><b>5.5.8</b> Practical Considerations</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="partial-least-squares.html"><a href="partial-least-squares.html"><i class="fa fa-check"></i><b>5.6</b> Partial Least Squares</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="partial-least-squares.html"><a href="partial-least-squares.html#motivation-for-pls"><i class="fa fa-check"></i><b>5.6.1</b> Motivation for PLS</a></li>
<li class="chapter" data-level="5.6.2" data-path="partial-least-squares.html"><a href="partial-least-squares.html#steps-to-construct-pls-components"><i class="fa fa-check"></i><b>5.6.2</b> Steps to Construct PLS Components</a></li>
<li class="chapter" data-level="5.6.3" data-path="partial-least-squares.html"><a href="partial-least-squares.html#properties-of-pls"><i class="fa fa-check"></i><b>5.6.3</b> Properties of PLS</a></li>
<li class="chapter" data-level="5.6.4" data-path="partial-least-squares.html"><a href="partial-least-squares.html#comparison-with-related-methods"><i class="fa fa-check"></i><b>5.6.4</b> Comparison with Related Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Non-Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>6.1</b> Inference</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inference.html"><a href="inference.html#linear-functions-of-the-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Linear Functions of the Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="inference.html"><a href="inference.html#nonlinear-functions-of-parameters"><i class="fa fa-check"></i><b>6.1.2</b> Nonlinear Functions of Parameters</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-linear-least-squares-estimation.html"><a href="non-linear-least-squares-estimation.html"><i class="fa fa-check"></i><b>6.2</b> Non-linear Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="non-linear-least-squares-estimation.html"><a href="non-linear-least-squares-estimation.html#iterative-optimization-nonlinear-regression"><i class="fa fa-check"></i><b>6.2.1</b> Iterative Optimization</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-linear-least-squares-estimation.html"><a href="non-linear-least-squares-estimation.html#derivative-free"><i class="fa fa-check"></i><b>6.2.2</b> Derivative-Free</a></li>
<li class="chapter" data-level="6.2.3" data-path="non-linear-least-squares-estimation.html"><a href="non-linear-least-squares-estimation.html#stochastic-heuristic-nolinear-regression"><i class="fa fa-check"></i><b>6.2.3</b> Stochastic Heuristic</a></li>
<li class="chapter" data-level="6.2.4" data-path="non-linear-least-squares-estimation.html"><a href="non-linear-least-squares-estimation.html#linearization-nonlinear-regression-optimization"><i class="fa fa-check"></i><b>6.2.4</b> Linearization</a></li>
<li class="chapter" data-level="6.2.5" data-path="non-linear-least-squares-estimation.html"><a href="non-linear-least-squares-estimation.html#hybrid-nonlinear-regression-optimization"><i class="fa fa-check"></i><b>6.2.5</b> Hybrid</a></li>
<li class="chapter" data-level="6.2.6" data-path="non-linear-least-squares-estimation.html"><a href="non-linear-least-squares-estimation.html#comparison-of-nonlinear-optimizers"><i class="fa fa-check"></i><b>6.2.6</b> Comparison of Nonlinear Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="practical-considerations-2.html"><a href="practical-considerations-2.html"><i class="fa fa-check"></i><b>6.3</b> Practical Considerations</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="practical-considerations-2.html"><a href="practical-considerations-2.html#selecting-starting-values"><i class="fa fa-check"></i><b>6.3.1</b> Selecting Starting Values</a></li>
<li class="chapter" data-level="6.3.2" data-path="practical-considerations-2.html"><a href="practical-considerations-2.html#handling-constrained-parameters"><i class="fa fa-check"></i><b>6.3.2</b> Handling Constrained Parameters</a></li>
<li class="chapter" data-level="6.3.3" data-path="practical-considerations-2.html"><a href="practical-considerations-2.html#failure-to-converge"><i class="fa fa-check"></i><b>6.3.3</b> Failure to Converge</a></li>
<li class="chapter" data-level="6.3.4" data-path="practical-considerations-2.html"><a href="practical-considerations-2.html#convergence-to-a-local-minimum"><i class="fa fa-check"></i><b>6.3.4</b> Convergence to a Local Minimum</a></li>
<li class="chapter" data-level="6.3.5" data-path="practical-considerations-2.html"><a href="practical-considerations-2.html#model-adequacy-and-estimation-considerations"><i class="fa fa-check"></i><b>6.3.5</b> Model Adequacy and Estimation Considerations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>6.4</b> Application</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="application.html"><a href="application.html#nonlinear-estimation-using-gauss-newton-algorithm"><i class="fa fa-check"></i><b>6.4.1</b> Nonlinear Estimation Using Gauss-Newton Algorithm</a></li>
<li class="chapter" data-level="6.4.2" data-path="application.html"><a href="application.html#logistic-growth-model"><i class="fa fa-check"></i><b>6.4.2</b> Logistic Growth Model</a></li>
<li class="chapter" data-level="6.4.3" data-path="application.html"><a href="application.html#nonlinear-plateau-model"><i class="fa fa-check"></i><b>6.4.3</b> Nonlinear Plateau Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec-logistic-regression.html"><a href="sec-logistic-regression.html"><i class="fa fa-check"></i><b>7.1</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="sec-logistic-regression.html"><a href="sec-logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>7.1.1</b> Logistic Model</a></li>
<li class="chapter" data-level="7.1.2" data-path="sec-logistic-regression.html"><a href="sec-logistic-regression.html#sec-likelihood-function-logistic"><i class="fa fa-check"></i><b>7.1.2</b> Likelihood Function</a></li>
<li class="chapter" data-level="7.1.3" data-path="sec-logistic-regression.html"><a href="sec-logistic-regression.html#fisher-information-matrix"><i class="fa fa-check"></i><b>7.1.3</b> Fisher Information Matrix</a></li>
<li class="chapter" data-level="7.1.4" data-path="sec-logistic-regression.html"><a href="sec-logistic-regression.html#inference-in-logistic-regression"><i class="fa fa-check"></i><b>7.1.4</b> Inference in Logistic Regression</a></li>
<li class="chapter" data-level="7.1.5" data-path="sec-logistic-regression.html"><a href="sec-logistic-regression.html#application-logistic-regression"><i class="fa fa-check"></i><b>7.1.5</b> Application: Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="sec-probit-regression.html"><a href="sec-probit-regression.html"><i class="fa fa-check"></i><b>7.2</b> Probit Regression</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="sec-probit-regression.html"><a href="sec-probit-regression.html#probit-model"><i class="fa fa-check"></i><b>7.2.1</b> Probit Model</a></li>
<li class="chapter" data-level="7.2.2" data-path="sec-probit-regression.html"><a href="sec-probit-regression.html#application-probit-regression"><i class="fa fa-check"></i><b>7.2.2</b> Application: Probit Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="sec-binomial-regression.html"><a href="sec-binomial-regression.html"><i class="fa fa-check"></i><b>7.3</b> Binomial Regression</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="sec-binomial-regression.html"><a href="sec-binomial-regression.html#dataset-overview"><i class="fa fa-check"></i><b>7.3.1</b> Dataset Overview</a></li>
<li class="chapter" data-level="7.3.2" data-path="sec-binomial-regression.html"><a href="sec-binomial-regression.html#apply-logistic-model"><i class="fa fa-check"></i><b>7.3.2</b> Apply Logistic Model</a></li>
<li class="chapter" data-level="7.3.3" data-path="sec-binomial-regression.html"><a href="sec-binomial-regression.html#apply-probit-model"><i class="fa fa-check"></i><b>7.3.3</b> Apply Probit Model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="sec-poisson-regression.html"><a href="sec-poisson-regression.html"><i class="fa fa-check"></i><b>7.4</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="sec-poisson-regression.html"><a href="sec-poisson-regression.html#the-poisson-distribution"><i class="fa fa-check"></i><b>7.4.1</b> The Poisson Distribution</a></li>
<li class="chapter" data-level="7.4.2" data-path="sec-poisson-regression.html"><a href="sec-poisson-regression.html#poisson-model"><i class="fa fa-check"></i><b>7.4.2</b> Poisson Model</a></li>
<li class="chapter" data-level="7.4.3" data-path="sec-poisson-regression.html"><a href="sec-poisson-regression.html#link-function-choices"><i class="fa fa-check"></i><b>7.4.3</b> Link Function Choices</a></li>
<li class="chapter" data-level="7.4.4" data-path="sec-poisson-regression.html"><a href="sec-poisson-regression.html#application-poisson-regression"><i class="fa fa-check"></i><b>7.4.4</b> Application: Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="sec-negative-binomial-regression.html"><a href="sec-negative-binomial-regression.html"><i class="fa fa-check"></i><b>7.5</b> Negative Binomial Regression</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="sec-negative-binomial-regression.html"><a href="sec-negative-binomial-regression.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>7.5.1</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="7.5.2" data-path="sec-negative-binomial-regression.html"><a href="sec-negative-binomial-regression.html#application-negative-binomial-regression"><i class="fa fa-check"></i><b>7.5.2</b> Application: Negative Binomial Regression</a></li>
<li class="chapter" data-level="7.5.3" data-path="sec-negative-binomial-regression.html"><a href="sec-negative-binomial-regression.html#fitting-a-zero-inflated-negative-binomial-model"><i class="fa fa-check"></i><b>7.5.3</b> Fitting a Zero-Inflated Negative Binomial Model</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="sec-quasi-poisson-regression.html"><a href="sec-quasi-poisson-regression.html"><i class="fa fa-check"></i><b>7.6</b> Quasi-Poisson Regression</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="sec-quasi-poisson-regression.html"><a href="sec-quasi-poisson-regression.html#is-quasi-poisson-regression-a-generalized-linear-model"><i class="fa fa-check"></i><b>7.6.1</b> Is Quasi-Poisson Regression a Generalized Linear Model?</a></li>
<li class="chapter" data-level="7.6.2" data-path="sec-quasi-poisson-regression.html"><a href="sec-quasi-poisson-regression.html#application-quasi-poisson-regression"><i class="fa fa-check"></i><b>7.6.2</b> Application: Quasi-Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>7.7</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>7.7.1</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="7.7.2" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#modeling-probabilities-using-log-odds"><i class="fa fa-check"></i><b>7.7.2</b> Modeling Probabilities Using Log-Odds</a></li>
<li class="chapter" data-level="7.7.3" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#softmax-representation"><i class="fa fa-check"></i><b>7.7.3</b> Softmax Representation</a></li>
<li class="chapter" data-level="7.7.4" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#log-odds-ratio-between-two-categories"><i class="fa fa-check"></i><b>7.7.4</b> Log-Odds Ratio Between Two Categories</a></li>
<li class="chapter" data-level="7.7.5" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#estimation"><i class="fa fa-check"></i><b>7.7.5</b> Estimation</a></li>
<li class="chapter" data-level="7.7.6" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>7.7.6</b> Interpretation of Coefficients</a></li>
<li class="chapter" data-level="7.7.7" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#application-multinomial-logistic-regression"><i class="fa fa-check"></i><b>7.7.7</b> Application: Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="7.7.8" data-path="sec-multinomial-logistic-regression.html"><a href="sec-multinomial-logistic-regression.html#application-gamma-regression"><i class="fa fa-check"></i><b>7.7.8</b> Application: Gamma Regression</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html"><i class="fa fa-check"></i><b>7.8</b> Generalization of Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#exponential-family"><i class="fa fa-check"></i><b>7.8.1</b> Exponential Family</a></li>
<li class="chapter" data-level="7.8.2" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#properties-of-glm-exponential-families"><i class="fa fa-check"></i><b>7.8.2</b> Properties of GLM Exponential Families</a></li>
<li class="chapter" data-level="7.8.3" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#structure-of-a-generalized-linear-model"><i class="fa fa-check"></i><b>7.8.3</b> Structure of a Generalized Linear Model</a></li>
<li class="chapter" data-level="7.8.4" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#components-of-a-glm"><i class="fa fa-check"></i><b>7.8.4</b> Components of a GLM</a></li>
<li class="chapter" data-level="7.8.5" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#canonical-link"><i class="fa fa-check"></i><b>7.8.5</b> Canonical Link</a></li>
<li class="chapter" data-level="7.8.6" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#inverse-link-functions"><i class="fa fa-check"></i><b>7.8.6</b> Inverse Link Functions</a></li>
<li class="chapter" data-level="7.8.7" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#estimation-of-parameters-in-glms"><i class="fa fa-check"></i><b>7.8.7</b> Estimation of Parameters in GLMs</a></li>
<li class="chapter" data-level="7.8.8" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#inference-1"><i class="fa fa-check"></i><b>7.8.8</b> Inference</a></li>
<li class="chapter" data-level="7.8.9" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#deviance"><i class="fa fa-check"></i><b>7.8.9</b> Deviance</a></li>
<li class="chapter" data-level="7.8.10" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#diagnostic-plots"><i class="fa fa-check"></i><b>7.8.10</b> Diagnostic Plots</a></li>
<li class="chapter" data-level="7.8.11" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#goodness-of-fit"><i class="fa fa-check"></i><b>7.8.11</b> Goodness of Fit</a></li>
<li class="chapter" data-level="7.8.12" data-path="sec-generalization-of-generalized-linear-models.html"><a href="sec-generalization-of-generalized-linear-models.html#over-dispersion"><i class="fa fa-check"></i><b>7.8.12</b> Over-Dispersion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="sec-linear-mixed-models.html"><a href="sec-linear-mixed-models.html"><i class="fa fa-check"></i><b>8</b> Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dependent-data.html"><a href="dependent-data.html"><i class="fa fa-check"></i><b>8.1</b> Dependent Data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="dependent-data.html"><a href="dependent-data.html#motivation-a-repeated-measurements-example"><i class="fa fa-check"></i><b>8.1.1</b> Motivation: A Repeated Measurements Example</a></li>
<li class="chapter" data-level="8.1.2" data-path="dependent-data.html"><a href="dependent-data.html#example-linear-mixed-model-for-repeated-measurements"><i class="fa fa-check"></i><b>8.1.2</b> Example: Linear Mixed Model for Repeated Measurements</a></li>
<li class="chapter" data-level="8.1.3" data-path="dependent-data.html"><a href="dependent-data.html#sec-random-intercepts-model-lmm"><i class="fa fa-check"></i><b>8.1.3</b> Random-Intercepts Model</a></li>
<li class="chapter" data-level="8.1.4" data-path="dependent-data.html"><a href="dependent-data.html#covariance-models-in-linear-mixed-models"><i class="fa fa-check"></i><b>8.1.4</b> Covariance Models in Linear Mixed Models</a></li>
<li class="chapter" data-level="8.1.5" data-path="dependent-data.html"><a href="dependent-data.html#covariance-structures-in-mixed-models"><i class="fa fa-check"></i><b>8.1.5</b> Covariance Structures in Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="estimation-in-linear-mixed-models.html"><a href="estimation-in-linear-mixed-models.html"><i class="fa fa-check"></i><b>8.2</b> Estimation in Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="estimation-in-linear-mixed-models.html"><a href="estimation-in-linear-mixed-models.html#interpretation-of-the-mixed-model-equations"><i class="fa fa-check"></i><b>8.2.1</b> Interpretation of the Mixed Model Equations</a></li>
<li class="chapter" data-level="8.2.2" data-path="estimation-in-linear-mixed-models.html"><a href="estimation-in-linear-mixed-models.html#derivation-of-the-mixed-model-equations"><i class="fa fa-check"></i><b>8.2.2</b> Derivation of the Mixed Model Equations</a></li>
<li class="chapter" data-level="8.2.3" data-path="estimation-in-linear-mixed-models.html"><a href="estimation-in-linear-mixed-models.html#bayesian-interpretation-of-linear-mixed-models"><i class="fa fa-check"></i><b>8.2.3</b> Bayesian Interpretation of Linear Mixed Models</a></li>
<li class="chapter" data-level="8.2.4" data-path="estimation-in-linear-mixed-models.html"><a href="estimation-in-linear-mixed-models.html#estimating-the-variance-covariance-matrix"><i class="fa fa-check"></i><b>8.2.4</b> Estimating the Variance-Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="inference-in-linear-mixed-models.html"><a href="inference-in-linear-mixed-models.html"><i class="fa fa-check"></i><b>8.3</b> Inference in Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="inference-in-linear-mixed-models.html"><a href="inference-in-linear-mixed-models.html#inference-for-fixed-effects-beta"><i class="fa fa-check"></i><b>8.3.1</b> Inference for Fixed Effects (<span class="math inline">\(\beta\)</span>)</a></li>
<li class="chapter" data-level="8.3.2" data-path="inference-in-linear-mixed-models.html"><a href="inference-in-linear-mixed-models.html#inference-for-variance-components-theta"><i class="fa fa-check"></i><b>8.3.2</b> Inference for Variance Components (<span class="math inline">\(\theta\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html"><i class="fa fa-check"></i><b>8.4</b> Information Criteria for Model Selection</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#sec-akaike-information-criterion-lmm"><i class="fa fa-check"></i><b>8.4.1</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="8.4.2" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#sec-corrected-aic-lmm"><i class="fa fa-check"></i><b>8.4.2</b> Corrected AIC</a></li>
<li class="chapter" data-level="8.4.3" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#sec-bayesian-information-criterion-lmm"><i class="fa fa-check"></i><b>8.4.3</b> Bayesian Information Criterion</a></li>
<li class="chapter" data-level="8.4.4" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#practical-example-with-linear-mixed-models"><i class="fa fa-check"></i><b>8.4.4</b> Practical Example with Linear Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="split-plot-designs.html"><a href="split-plot-designs.html"><i class="fa fa-check"></i><b>8.5</b> Split-Plot Designs</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="split-plot-designs.html"><a href="split-plot-designs.html#example-setup"><i class="fa fa-check"></i><b>8.5.1</b> Example Setup</a></li>
<li class="chapter" data-level="8.5.2" data-path="split-plot-designs.html"><a href="split-plot-designs.html#statistical-model-for-split-plot-designs"><i class="fa fa-check"></i><b>8.5.2</b> Statistical Model for Split-Plot Designs</a></li>
<li class="chapter" data-level="8.5.3" data-path="split-plot-designs.html"><a href="split-plot-designs.html#approaches-to-analyzing-split-plot-designs"><i class="fa fa-check"></i><b>8.5.3</b> Approaches to Analyzing Split-Plot Designs</a></li>
<li class="chapter" data-level="8.5.4" data-path="split-plot-designs.html"><a href="split-plot-designs.html#application-split-plot-design"><i class="fa fa-check"></i><b>8.5.4</b> Application: Split-Plot Design</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="repeated-measures-in-mixed-models.html"><a href="repeated-measures-in-mixed-models.html"><i class="fa fa-check"></i><b>8.6</b> Repeated Measures in Mixed Models</a></li>
<li class="chapter" data-level="8.7" data-path="unbalanced-or-unequally-spaced-data.html"><a href="unbalanced-or-unequally-spaced-data.html"><i class="fa fa-check"></i><b>8.7</b> Unbalanced or Unequally Spaced Data</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="unbalanced-or-unequally-spaced-data.html"><a href="unbalanced-or-unequally-spaced-data.html#variance-covariance-structure-power-model"><i class="fa fa-check"></i><b>8.7.1</b> Variance-Covariance Structure: Power Model</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="application-mixed-models-in-practice.html"><a href="application-mixed-models-in-practice.html"><i class="fa fa-check"></i><b>8.8</b> Application: Mixed Models in Practice</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="application-mixed-models-in-practice.html"><a href="application-mixed-models-in-practice.html#example-1-pulp-brightness-analysis"><i class="fa fa-check"></i><b>8.8.1</b> Example 1: Pulp Brightness Analysis</a></li>
<li class="chapter" data-level="8.8.2" data-path="application-mixed-models-in-practice.html"><a href="application-mixed-models-in-practice.html#example-2-penicillin-yield-glmm-with-blocking"><i class="fa fa-check"></i><b>8.8.2</b> Example 2: Penicillin Yield (GLMM with Blocking)</a></li>
<li class="chapter" data-level="8.8.3" data-path="application-mixed-models-in-practice.html"><a href="application-mixed-models-in-practice.html#example-3-growth-in-rats-over-time"><i class="fa fa-check"></i><b>8.8.3</b> Example 3: Growth in Rats Over Time</a></li>
<li class="chapter" data-level="8.8.4" data-path="application-mixed-models-in-practice.html"><a href="application-mixed-models-in-practice.html#example-4-tree-water-use-agridat"><i class="fa fa-check"></i><b>8.8.4</b> Example 4: Tree Water Use (Agridat)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sec-nonlinear-and-generalized-linear-mixed-models.html"><a href="sec-nonlinear-and-generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>9</b> Nonlinear and Generalized Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec-nonlinear-mixed-models.html"><a href="sec-nonlinear-mixed-models.html"><i class="fa fa-check"></i><b>9.1</b> Nonlinear Mixed Models</a></li>
<li class="chapter" data-level="9.2" data-path="sec-generalized-linear-mixed-models.html"><a href="sec-generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>9.2</b> Generalized Linear Mixed Models</a></li>
<li class="chapter" data-level="9.3" data-path="relationship-between-nlmms-and-glmms.html"><a href="relationship-between-nlmms-and-glmms.html"><i class="fa fa-check"></i><b>9.3</b> Relationship Between NLMMs and GLMMs</a></li>
<li class="chapter" data-level="9.4" data-path="marginal-properties-of-glmms.html"><a href="marginal-properties-of-glmms.html"><i class="fa fa-check"></i><b>9.4</b> Marginal Properties of GLMMs</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="marginal-properties-of-glmms.html"><a href="marginal-properties-of-glmms.html#marginal-mean-of-y_i"><i class="fa fa-check"></i><b>9.4.1</b> Marginal Mean of <span class="math inline">\(y_i\)</span></a></li>
<li class="chapter" data-level="9.4.2" data-path="marginal-properties-of-glmms.html"><a href="marginal-properties-of-glmms.html#marginal-variance-of-y_i"><i class="fa fa-check"></i><b>9.4.2</b> Marginal Variance of <span class="math inline">\(y_i\)</span></a></li>
<li class="chapter" data-level="9.4.3" data-path="marginal-properties-of-glmms.html"><a href="marginal-properties-of-glmms.html#marginal-covariance-of-mathbfy"><i class="fa fa-check"></i><b>9.4.3</b> Marginal Covariance of <span class="math inline">\(\mathbf{y}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="estimation-in-nonlinear-and-generalized-linear-mixed-models.html"><a href="estimation-in-nonlinear-and-generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>9.5</b> Estimation in Nonlinear and Generalized Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="estimation-in-nonlinear-and-generalized-linear-mixed-models.html"><a href="estimation-in-nonlinear-and-generalized-linear-mixed-models.html#estimation-by-numerical-integration"><i class="fa fa-check"></i><b>9.5.1</b> Estimation by Numerical Integration</a></li>
<li class="chapter" data-level="9.5.2" data-path="estimation-in-nonlinear-and-generalized-linear-mixed-models.html"><a href="estimation-in-nonlinear-and-generalized-linear-mixed-models.html#sec-estimation-by-linearization-glmm"><i class="fa fa-check"></i><b>9.5.2</b> Estimation by Linearization</a></li>
<li class="chapter" data-level="9.5.3" data-path="estimation-in-nonlinear-and-generalized-linear-mixed-models.html"><a href="estimation-in-nonlinear-and-generalized-linear-mixed-models.html#estimation-by-bayesian-hierarchical-models"><i class="fa fa-check"></i><b>9.5.3</b> Estimation by Bayesian Hierarchical Models</a></li>
<li class="chapter" data-level="9.5.4" data-path="estimation-in-nonlinear-and-generalized-linear-mixed-models.html"><a href="estimation-in-nonlinear-and-generalized-linear-mixed-models.html#practical-implementation-in-r"><i class="fa fa-check"></i><b>9.5.4</b> Practical Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="application-nonlinear-and-generalized-linear-mixed-models.html"><a href="application-nonlinear-and-generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>9.6</b> Application: Nonlinear and Generalized Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="application-nonlinear-and-generalized-linear-mixed-models.html"><a href="application-nonlinear-and-generalized-linear-mixed-models.html#binomial-data-cbpp-dataset"><i class="fa fa-check"></i><b>9.6.1</b> Binomial Data: CBPP Dataset</a></li>
<li class="chapter" data-level="9.6.2" data-path="application-nonlinear-and-generalized-linear-mixed-models.html"><a href="application-nonlinear-and-generalized-linear-mixed-models.html#count-data-owl-dataset"><i class="fa fa-check"></i><b>9.6.2</b> Count Data: Owl Dataset</a></li>
<li class="chapter" data-level="9.6.3" data-path="application-nonlinear-and-generalized-linear-mixed-models.html"><a href="application-nonlinear-and-generalized-linear-mixed-models.html#binomial-example-gotway-hessian-fly-data"><i class="fa fa-check"></i><b>9.6.3</b> Binomial Example: Gotway Hessian Fly Data</a></li>
<li class="chapter" data-level="9.6.4" data-path="application-nonlinear-and-generalized-linear-mixed-models.html"><a href="application-nonlinear-and-generalized-linear-mixed-models.html#nonlinear-mixed-model-yellow-poplar-data"><i class="fa fa-check"></i><b>9.6.4</b> Nonlinear Mixed Model: Yellow Poplar Data</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>9.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="sec-nonparametric-regression.html"><a href="sec-nonparametric-regression.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="why-nonparametric.html"><a href="why-nonparametric.html"><i class="fa fa-check"></i><b>10.1</b> Why Nonparametric?</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="why-nonparametric.html"><a href="why-nonparametric.html#flexibility"><i class="fa fa-check"></i><b>10.1.1</b> Flexibility</a></li>
<li class="chapter" data-level="10.1.2" data-path="why-nonparametric.html"><a href="why-nonparametric.html#fewer-assumptions"><i class="fa fa-check"></i><b>10.1.2</b> Fewer Assumptions</a></li>
<li class="chapter" data-level="10.1.3" data-path="why-nonparametric.html"><a href="why-nonparametric.html#interpretability"><i class="fa fa-check"></i><b>10.1.3</b> Interpretability</a></li>
<li class="chapter" data-level="10.1.4" data-path="why-nonparametric.html"><a href="why-nonparametric.html#practical-considerations-3"><i class="fa fa-check"></i><b>10.1.4</b> Practical Considerations</a></li>
<li class="chapter" data-level="10.1.5" data-path="why-nonparametric.html"><a href="why-nonparametric.html#balancing-parametric-and-nonparametric-approaches"><i class="fa fa-check"></i><b>10.1.5</b> Balancing Parametric and Nonparametric Approaches</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="basic-concepts-in-nonparametric-estimation.html"><a href="basic-concepts-in-nonparametric-estimation.html"><i class="fa fa-check"></i><b>10.2</b> Basic Concepts in Nonparametric Estimation</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="basic-concepts-in-nonparametric-estimation.html"><a href="basic-concepts-in-nonparametric-estimation.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>10.2.1</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="10.2.2" data-path="basic-concepts-in-nonparametric-estimation.html"><a href="basic-concepts-in-nonparametric-estimation.html#kernel-smoothing-and-local-averages"><i class="fa fa-check"></i><b>10.2.2</b> Kernel Smoothing and Local Averages</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html"><i class="fa fa-check"></i><b>10.3</b> Kernel Regression</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#basic-setup"><i class="fa fa-check"></i><b>10.3.1</b> Basic Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#sec-nadaraya-watson-kernel-estimator"><i class="fa fa-check"></i><b>10.3.2</b> Nadaraya-Watson Kernel Estimator</a></li>
<li class="chapter" data-level="10.3.3" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#sec-priestley-chao-kernel-estimator"><i class="fa fa-check"></i><b>10.3.3</b> Priestley–Chao Kernel Estimator</a></li>
<li class="chapter" data-level="10.3.4" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#sec-gasser-mueller-kernel-estimator"><i class="fa fa-check"></i><b>10.3.4</b> Gasser–Müller Kernel Estimator</a></li>
<li class="chapter" data-level="10.3.5" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#comparison-of-kernel-based-estimators"><i class="fa fa-check"></i><b>10.3.5</b> Comparison of Kernel-Based Estimators</a></li>
<li class="chapter" data-level="10.3.6" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#bandwidth-selection"><i class="fa fa-check"></i><b>10.3.6</b> Bandwidth Selection</a></li>
<li class="chapter" data-level="10.3.7" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#asymptotic-properties"><i class="fa fa-check"></i><b>10.3.7</b> Asymptotic Properties</a></li>
<li class="chapter" data-level="10.3.8" data-path="sec-kernel-regression.html"><a href="sec-kernel-regression.html#derivation-of-the-nadaraya-watson-estimator"><i class="fa fa-check"></i><b>10.3.8</b> Derivation of the Nadaraya-Watson Estimator</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="sec-local-polynomial-regression.html"><a href="sec-local-polynomial-regression.html"><i class="fa fa-check"></i><b>10.4</b> Local Polynomial Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec-local-polynomial-regression.html"><a href="sec-local-polynomial-regression.html#local-polynomial-fitting"><i class="fa fa-check"></i><b>10.4.1</b> Local Polynomial Fitting</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec-local-polynomial-regression.html"><a href="sec-local-polynomial-regression.html#mathematical-form-of-the-solution"><i class="fa fa-check"></i><b>10.4.2</b> Mathematical Form of the Solution</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec-local-polynomial-regression.html"><a href="sec-local-polynomial-regression.html#bias-variance-and-asymptotics"><i class="fa fa-check"></i><b>10.4.3</b> Bias, Variance, and Asymptotics</a></li>
<li class="chapter" data-level="10.4.4" data-path="sec-local-polynomial-regression.html"><a href="sec-local-polynomial-regression.html#sec-special-case-local-linear-regression"><i class="fa fa-check"></i><b>10.4.4</b> Special Case: Local Linear Regression</a></li>
<li class="chapter" data-level="10.4.5" data-path="sec-local-polynomial-regression.html"><a href="sec-local-polynomial-regression.html#bandwidth-selection-1"><i class="fa fa-check"></i><b>10.4.5</b> Bandwidth Selection</a></li>
<li class="chapter" data-level="10.4.6" data-path="sec-local-polynomial-regression.html"><a href="sec-local-polynomial-regression.html#asymptotic-properties-summary"><i class="fa fa-check"></i><b>10.4.6</b> Asymptotic Properties Summary</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec-smoothing-splines.html"><a href="sec-smoothing-splines.html"><i class="fa fa-check"></i><b>10.5</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="sec-smoothing-splines.html"><a href="sec-smoothing-splines.html#properties-and-form-of-the-smoothing-spline"><i class="fa fa-check"></i><b>10.5.1</b> Properties and Form of the Smoothing Spline</a></li>
<li class="chapter" data-level="10.5.2" data-path="sec-smoothing-splines.html"><a href="sec-smoothing-splines.html#choice-of-lambda"><i class="fa fa-check"></i><b>10.5.2</b> Choice of <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="10.5.3" data-path="sec-smoothing-splines.html"><a href="sec-smoothing-splines.html#connection-to-reproducing-kernel-hilbert-spaces"><i class="fa fa-check"></i><b>10.5.3</b> Connection to Reproducing Kernel Hilbert Spaces</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="confidence-intervals-in-nonparametric-regression.html"><a href="confidence-intervals-in-nonparametric-regression.html"><i class="fa fa-check"></i><b>10.6</b> Confidence Intervals in Nonparametric Regression</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="confidence-intervals-in-nonparametric-regression.html"><a href="confidence-intervals-in-nonparametric-regression.html#asymptotic-normality"><i class="fa fa-check"></i><b>10.6.1</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="10.6.2" data-path="confidence-intervals-in-nonparametric-regression.html"><a href="confidence-intervals-in-nonparametric-regression.html#bootstrap-methods"><i class="fa fa-check"></i><b>10.6.2</b> Bootstrap Methods</a></li>
<li class="chapter" data-level="10.6.3" data-path="confidence-intervals-in-nonparametric-regression.html"><a href="confidence-intervals-in-nonparametric-regression.html#practical-considerations-4"><i class="fa fa-check"></i><b>10.6.3</b> Practical Considerations</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="sec-generalized-additive-models.html"><a href="sec-generalized-additive-models.html"><i class="fa fa-check"></i><b>10.7</b> Generalized Additive Models</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="sec-generalized-additive-models.html"><a href="sec-generalized-additive-models.html#estimation-via-penalized-likelihood"><i class="fa fa-check"></i><b>10.7.1</b> Estimation via Penalized Likelihood</a></li>
<li class="chapter" data-level="10.7.2" data-path="sec-generalized-additive-models.html"><a href="sec-generalized-additive-models.html#interpretation-of-gams"><i class="fa fa-check"></i><b>10.7.2</b> Interpretation of GAMs</a></li>
<li class="chapter" data-level="10.7.3" data-path="sec-generalized-additive-models.html"><a href="sec-generalized-additive-models.html#model-selection-and-smoothing-parameter-estimation"><i class="fa fa-check"></i><b>10.7.3</b> Model Selection and Smoothing Parameter Estimation</a></li>
<li class="chapter" data-level="10.7.4" data-path="sec-generalized-additive-models.html"><a href="sec-generalized-additive-models.html#extensions-of-gams"><i class="fa fa-check"></i><b>10.7.4</b> Extensions of GAMs</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="regression-trees-and-random-forests.html"><a href="regression-trees-and-random-forests.html"><i class="fa fa-check"></i><b>10.8</b> Regression Trees and Random Forests</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="regression-trees-and-random-forests.html"><a href="regression-trees-and-random-forests.html#sec-regression-trees"><i class="fa fa-check"></i><b>10.8.1</b> Regression Trees</a></li>
<li class="chapter" data-level="10.8.2" data-path="regression-trees-and-random-forests.html"><a href="regression-trees-and-random-forests.html#sec-random-forests"><i class="fa fa-check"></i><b>10.8.2</b> Random Forests</a></li>
<li class="chapter" data-level="10.8.3" data-path="regression-trees-and-random-forests.html"><a href="regression-trees-and-random-forests.html#theoretical-insights"><i class="fa fa-check"></i><b>10.8.3</b> Theoretical Insights</a></li>
<li class="chapter" data-level="10.8.4" data-path="regression-trees-and-random-forests.html"><a href="regression-trees-and-random-forests.html#feature-importance-in-random-forests"><i class="fa fa-check"></i><b>10.8.4</b> Feature Importance in Random Forests</a></li>
<li class="chapter" data-level="10.8.5" data-path="regression-trees-and-random-forests.html"><a href="regression-trees-and-random-forests.html#advantages-and-limitations-of-tree-based-methods"><i class="fa fa-check"></i><b>10.8.5</b> Advantages and Limitations of Tree-Based Methods</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="sec-wavelet-regression.html"><a href="sec-wavelet-regression.html"><i class="fa fa-check"></i><b>10.9</b> Wavelet Regression</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="sec-wavelet-regression.html"><a href="sec-wavelet-regression.html#wavelet-series-expansion"><i class="fa fa-check"></i><b>10.9.1</b> Wavelet Series Expansion</a></li>
<li class="chapter" data-level="10.9.2" data-path="sec-wavelet-regression.html"><a href="sec-wavelet-regression.html#wavelet-regression-model"><i class="fa fa-check"></i><b>10.9.2</b> Wavelet Regression Model</a></li>
<li class="chapter" data-level="10.9.3" data-path="sec-wavelet-regression.html"><a href="sec-wavelet-regression.html#wavelet-shrinkage-and-thresholding"><i class="fa fa-check"></i><b>10.9.3</b> Wavelet Shrinkage and Thresholding</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="multivariate-nonparametric-regression.html"><a href="multivariate-nonparametric-regression.html"><i class="fa fa-check"></i><b>10.10</b> Multivariate Nonparametric Regression</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="multivariate-nonparametric-regression.html"><a href="multivariate-nonparametric-regression.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>10.10.1</b> The Curse of Dimensionality</a></li>
<li class="chapter" data-level="10.10.2" data-path="multivariate-nonparametric-regression.html"><a href="multivariate-nonparametric-regression.html#multivariate-kernel-regression"><i class="fa fa-check"></i><b>10.10.2</b> Multivariate Kernel Regression</a></li>
<li class="chapter" data-level="10.10.3" data-path="multivariate-nonparametric-regression.html"><a href="multivariate-nonparametric-regression.html#multivariate-splines"><i class="fa fa-check"></i><b>10.10.3</b> Multivariate Splines</a></li>
<li class="chapter" data-level="10.10.4" data-path="multivariate-nonparametric-regression.html"><a href="multivariate-nonparametric-regression.html#additive-models-gams"><i class="fa fa-check"></i><b>10.10.4</b> Additive Models (GAMs)</a></li>
<li class="chapter" data-level="10.10.5" data-path="multivariate-nonparametric-regression.html"><a href="multivariate-nonparametric-regression.html#radial-basis-functions"><i class="fa fa-check"></i><b>10.10.5</b> Radial Basis Functions</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="conclusion-the-evolving-landscape-of-regression-analysis.html"><a href="conclusion-the-evolving-landscape-of-regression-analysis.html"><i class="fa fa-check"></i><b>10.11</b> Conclusion: The Evolving Landscape of Regression Analysis</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="conclusion-the-evolving-landscape-of-regression-analysis.html"><a href="conclusion-the-evolving-landscape-of-regression-analysis.html#key-takeaways-1"><i class="fa fa-check"></i><b>10.11.1</b> Key Takeaways</a></li>
<li class="chapter" data-level="10.11.2" data-path="conclusion-the-evolving-landscape-of-regression-analysis.html"><a href="conclusion-the-evolving-landscape-of-regression-analysis.html#the-art-and-science-of-regression"><i class="fa fa-check"></i><b>10.11.2</b> The Art and Science of Regression</a></li>
<li class="chapter" data-level="10.11.3" data-path="conclusion-the-evolving-landscape-of-regression-analysis.html"><a href="conclusion-the-evolving-landscape-of-regression-analysis.html#looking-forward"><i class="fa fa-check"></i><b>10.11.3</b> Looking Forward</a></li>
<li class="chapter" data-level="10.11.4" data-path="conclusion-the-evolving-landscape-of-regression-analysis.html"><a href="conclusion-the-evolving-landscape-of-regression-analysis.html#final-thoughts"><i class="fa fa-check"></i><b>10.11.4</b> Final Thoughts</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III. RAMIFICATIONS</b></span></li>
<li class="chapter" data-level="11" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>11</b> Data</a>
<ul>
<li class="chapter" data-level="11.1" data-path="data-types.html"><a href="data-types.html"><i class="fa fa-check"></i><b>11.1</b> Data Types</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="data-types.html"><a href="data-types.html#qualitative-vs.-quantitative-data"><i class="fa fa-check"></i><b>11.1.1</b> Qualitative vs. Quantitative Data</a></li>
<li class="chapter" data-level="11.1.2" data-path="data-types.html"><a href="data-types.html#other-ways-to-classify-data"><i class="fa fa-check"></i><b>11.1.2</b> Other Ways to Classify Data</a></li>
<li class="chapter" data-level="11.1.3" data-path="data-types.html"><a href="data-types.html#data-by-observational-structure-over-time"><i class="fa fa-check"></i><b>11.1.3</b> Data by Observational Structure Over Time</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec-cross-sectional-data.html"><a href="sec-cross-sectional-data.html"><i class="fa fa-check"></i><b>11.2</b> Cross-Sectional Data</a></li>
<li class="chapter" data-level="11.3" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html"><i class="fa fa-check"></i><b>11.3</b> Time Series Data</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#statistical-properties-of-time-series-models"><i class="fa fa-check"></i><b>11.3.1</b> Statistical Properties of Time Series Models</a></li>
<li class="chapter" data-level="11.3.2" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#common-time-series-processes"><i class="fa fa-check"></i><b>11.3.2</b> Common Time Series Processes</a></li>
<li class="chapter" data-level="11.3.3" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#deterministic-time-trends"><i class="fa fa-check"></i><b>11.3.3</b> Deterministic Time Trends</a></li>
<li class="chapter" data-level="11.3.4" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#violations-of-exogeneity-in-time-series-models"><i class="fa fa-check"></i><b>11.3.4</b> Violations of Exogeneity in Time Series Models</a></li>
<li class="chapter" data-level="11.3.5" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#consequences-of-exogeneity-violations"><i class="fa fa-check"></i><b>11.3.5</b> Consequences of Exogeneity Violations</a></li>
<li class="chapter" data-level="11.3.6" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#highly-persistent-data"><i class="fa fa-check"></i><b>11.3.6</b> Highly Persistent Data</a></li>
<li class="chapter" data-level="11.3.7" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#sec-unit-root-testing"><i class="fa fa-check"></i><b>11.3.7</b> Unit Root Testing</a></li>
<li class="chapter" data-level="11.3.8" data-path="sec-time-series-data.html"><a href="sec-time-series-data.html#sec-newey-west-standard-errors"><i class="fa fa-check"></i><b>11.3.8</b> Newey-West Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="sec-repeated-cross-sectional-data.html"><a href="sec-repeated-cross-sectional-data.html"><i class="fa fa-check"></i><b>11.4</b> Repeated Cross-Sectional Data</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="sec-repeated-cross-sectional-data.html"><a href="sec-repeated-cross-sectional-data.html#key-characteristics"><i class="fa fa-check"></i><b>11.4.1</b> Key Characteristics</a></li>
<li class="chapter" data-level="11.4.2" data-path="sec-repeated-cross-sectional-data.html"><a href="sec-repeated-cross-sectional-data.html#statistical-modeling-for-repeated-cross-sections"><i class="fa fa-check"></i><b>11.4.2</b> Statistical Modeling for Repeated Cross-Sections</a></li>
<li class="chapter" data-level="11.4.3" data-path="sec-repeated-cross-sectional-data.html"><a href="sec-repeated-cross-sectional-data.html#advantages-of-repeated-cross-sectional-data"><i class="fa fa-check"></i><b>11.4.3</b> Advantages of Repeated Cross-Sectional Data</a></li>
<li class="chapter" data-level="11.4.4" data-path="sec-repeated-cross-sectional-data.html"><a href="sec-repeated-cross-sectional-data.html#disadvantages-of-repeated-cross-sectional-data"><i class="fa fa-check"></i><b>11.4.4</b> Disadvantages of Repeated Cross-Sectional Data</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="sec-panel-data.html"><a href="sec-panel-data.html"><i class="fa fa-check"></i><b>11.5</b> Panel Data</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="sec-panel-data.html"><a href="sec-panel-data.html#advantages-of-panel-data"><i class="fa fa-check"></i><b>11.5.1</b> Advantages of Panel Data</a></li>
<li class="chapter" data-level="11.5.2" data-path="sec-panel-data.html"><a href="sec-panel-data.html#disadvantages-of-panel-data"><i class="fa fa-check"></i><b>11.5.2</b> Disadvantages of Panel Data</a></li>
<li class="chapter" data-level="11.5.3" data-path="sec-panel-data.html"><a href="sec-panel-data.html#sources-of-variation-in-panel-data"><i class="fa fa-check"></i><b>11.5.3</b> Sources of Variation in Panel Data</a></li>
<li class="chapter" data-level="11.5.4" data-path="sec-panel-data.html"><a href="sec-panel-data.html#sec-pooled-ols-estimator"><i class="fa fa-check"></i><b>11.5.4</b> Pooled OLS Estimator</a></li>
<li class="chapter" data-level="11.5.5" data-path="sec-panel-data.html"><a href="sec-panel-data.html#individual-specific-effects-model"><i class="fa fa-check"></i><b>11.5.5</b> Individual-Specific Effects Model</a></li>
<li class="chapter" data-level="11.5.6" data-path="sec-panel-data.html"><a href="sec-panel-data.html#sec-random-effects-estimator"><i class="fa fa-check"></i><b>11.5.6</b> Random Effects Estimator</a></li>
<li class="chapter" data-level="11.5.7" data-path="sec-panel-data.html"><a href="sec-panel-data.html#sec-fixed-effects-estimator"><i class="fa fa-check"></i><b>11.5.7</b> Fixed Effects Estimator</a></li>
<li class="chapter" data-level="11.5.8" data-path="sec-panel-data.html"><a href="sec-panel-data.html#tests-for-assumptions-in-panel-data-analysis"><i class="fa fa-check"></i><b>11.5.8</b> Tests for Assumptions in Panel Data Analysis</a></li>
<li class="chapter" data-level="11.5.9" data-path="sec-panel-data.html"><a href="sec-panel-data.html#model-selection-in-panel-data"><i class="fa fa-check"></i><b>11.5.9</b> Model Selection in Panel Data</a></li>
<li class="chapter" data-level="11.5.10" data-path="sec-panel-data.html"><a href="sec-panel-data.html#alternative-estimators"><i class="fa fa-check"></i><b>11.5.10</b> Alternative Estimators</a></li>
<li class="chapter" data-level="11.5.11" data-path="sec-panel-data.html"><a href="sec-panel-data.html#application-1"><i class="fa fa-check"></i><b>11.5.11</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="choosing-the-right-type-of-data.html"><a href="choosing-the-right-type-of-data.html"><i class="fa fa-check"></i><b>11.6</b> Choosing the Right Type of Data</a></li>
<li class="chapter" data-level="11.7" data-path="data-quality-and-ethical-considerations.html"><a href="data-quality-and-ethical-considerations.html"><i class="fa fa-check"></i><b>11.7</b> Data Quality and Ethical Considerations</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="variable-transformation.html"><a href="variable-transformation.html"><i class="fa fa-check"></i><b>12</b> Variable Transformation</a>
<ul>
<li class="chapter" data-level="12.1" data-path="continuous-variables.html"><a href="continuous-variables.html"><i class="fa fa-check"></i><b>12.1</b> Continuous Variables</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="continuous-variables.html"><a href="continuous-variables.html#standardization-z-score-normalization"><i class="fa fa-check"></i><b>12.1.1</b> Standardization (Z-score Normalization)</a></li>
<li class="chapter" data-level="12.1.2" data-path="continuous-variables.html"><a href="continuous-variables.html#min-max-scaling-normalization"><i class="fa fa-check"></i><b>12.1.2</b> Min-Max Scaling (Normalization)</a></li>
<li class="chapter" data-level="12.1.3" data-path="continuous-variables.html"><a href="continuous-variables.html#square-root-and-cube-root-transformations"><i class="fa fa-check"></i><b>12.1.3</b> Square Root and Cube Root Transformations</a></li>
<li class="chapter" data-level="12.1.4" data-path="continuous-variables.html"><a href="continuous-variables.html#sec-logarithmic-transformation"><i class="fa fa-check"></i><b>12.1.4</b> Logarithmic Transformation</a></li>
<li class="chapter" data-level="12.1.5" data-path="continuous-variables.html"><a href="continuous-variables.html#exponential-transformation"><i class="fa fa-check"></i><b>12.1.5</b> Exponential Transformation</a></li>
<li class="chapter" data-level="12.1.6" data-path="continuous-variables.html"><a href="continuous-variables.html#power-transformation"><i class="fa fa-check"></i><b>12.1.6</b> Power Transformation</a></li>
<li class="chapter" data-level="12.1.7" data-path="continuous-variables.html"><a href="continuous-variables.html#inverse-reciprocal-transformation"><i class="fa fa-check"></i><b>12.1.7</b> Inverse (Reciprocal) Transformation</a></li>
<li class="chapter" data-level="12.1.8" data-path="continuous-variables.html"><a href="continuous-variables.html#hyperbolic-arcsine-transformation"><i class="fa fa-check"></i><b>12.1.8</b> Hyperbolic Arcsine Transformation</a></li>
<li class="chapter" data-level="12.1.9" data-path="continuous-variables.html"><a href="continuous-variables.html#ordered-quantile-normalization-rank-based-transformation"><i class="fa fa-check"></i><b>12.1.9</b> Ordered Quantile Normalization (Rank-Based Transformation)</a></li>
<li class="chapter" data-level="12.1.10" data-path="continuous-variables.html"><a href="continuous-variables.html#lambert-w-x-f-transformation"><i class="fa fa-check"></i><b>12.1.10</b> Lambert W x F Transformation</a></li>
<li class="chapter" data-level="12.1.11" data-path="continuous-variables.html"><a href="continuous-variables.html#inverse-hyperbolic-sine-transformation"><i class="fa fa-check"></i><b>12.1.11</b> Inverse Hyperbolic Sine Transformation</a></li>
<li class="chapter" data-level="12.1.12" data-path="continuous-variables.html"><a href="continuous-variables.html#sec-box-cox-transformation"><i class="fa fa-check"></i><b>12.1.12</b> Box-Cox Transformation</a></li>
<li class="chapter" data-level="12.1.13" data-path="continuous-variables.html"><a href="continuous-variables.html#yeo-johnson-transformation"><i class="fa fa-check"></i><b>12.1.13</b> Yeo-Johnson Transformation</a></li>
<li class="chapter" data-level="12.1.14" data-path="continuous-variables.html"><a href="continuous-variables.html#rankgauss-transformation"><i class="fa fa-check"></i><b>12.1.14</b> RankGauss Transformation</a></li>
<li class="chapter" data-level="12.1.15" data-path="continuous-variables.html"><a href="continuous-variables.html#automatically-choosing-the-best-transformation"><i class="fa fa-check"></i><b>12.1.15</b> Automatically Choosing the Best Transformation</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="categorical-variables.html"><a href="categorical-variables.html"><i class="fa fa-check"></i><b>12.2</b> Categorical Variables</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="categorical-variables.html"><a href="categorical-variables.html#one-hot-encoding-dummy-variables"><i class="fa fa-check"></i><b>12.2.1</b> One-Hot Encoding (Dummy Variables)</a></li>
<li class="chapter" data-level="12.2.2" data-path="categorical-variables.html"><a href="categorical-variables.html#label-encoding"><i class="fa fa-check"></i><b>12.2.2</b> Label Encoding</a></li>
<li class="chapter" data-level="12.2.3" data-path="categorical-variables.html"><a href="categorical-variables.html#feature-hashing-hash-encoding"><i class="fa fa-check"></i><b>12.2.3</b> Feature Hashing (Hash Encoding)</a></li>
<li class="chapter" data-level="12.2.4" data-path="categorical-variables.html"><a href="categorical-variables.html#binary-encoding"><i class="fa fa-check"></i><b>12.2.4</b> Binary Encoding</a></li>
<li class="chapter" data-level="12.2.5" data-path="categorical-variables.html"><a href="categorical-variables.html#base-n-encoding-generalized-binary-encoding"><i class="fa fa-check"></i><b>12.2.5</b> Base-N Encoding (Generalized Binary Encoding)</a></li>
<li class="chapter" data-level="12.2.6" data-path="categorical-variables.html"><a href="categorical-variables.html#frequency-encoding"><i class="fa fa-check"></i><b>12.2.6</b> Frequency Encoding</a></li>
<li class="chapter" data-level="12.2.7" data-path="categorical-variables.html"><a href="categorical-variables.html#target-encoding-mean-encoding"><i class="fa fa-check"></i><b>12.2.7</b> Target Encoding (Mean Encoding)</a></li>
<li class="chapter" data-level="12.2.8" data-path="categorical-variables.html"><a href="categorical-variables.html#ordinal-encoding"><i class="fa fa-check"></i><b>12.2.8</b> Ordinal Encoding</a></li>
<li class="chapter" data-level="12.2.9" data-path="categorical-variables.html"><a href="categorical-variables.html#weight-of-evidence-encoding"><i class="fa fa-check"></i><b>12.2.9</b> Weight of Evidence Encoding</a></li>
<li class="chapter" data-level="12.2.10" data-path="categorical-variables.html"><a href="categorical-variables.html#helmert-encoding"><i class="fa fa-check"></i><b>12.2.10</b> Helmert Encoding</a></li>
<li class="chapter" data-level="12.2.11" data-path="categorical-variables.html"><a href="categorical-variables.html#probability-ratio-encoding"><i class="fa fa-check"></i><b>12.2.11</b> Probability Ratio Encoding</a></li>
<li class="chapter" data-level="12.2.12" data-path="categorical-variables.html"><a href="categorical-variables.html#backward-difference-encoding"><i class="fa fa-check"></i><b>12.2.12</b> Backward Difference Encoding</a></li>
<li class="chapter" data-level="12.2.13" data-path="categorical-variables.html"><a href="categorical-variables.html#leave-one-out-encoding"><i class="fa fa-check"></i><b>12.2.13</b> Leave-One-Out Encoding</a></li>
<li class="chapter" data-level="12.2.14" data-path="categorical-variables.html"><a href="categorical-variables.html#james-stein-encoding"><i class="fa fa-check"></i><b>12.2.14</b> James-Stein Encoding</a></li>
<li class="chapter" data-level="12.2.15" data-path="categorical-variables.html"><a href="categorical-variables.html#m-estimator-encoding"><i class="fa fa-check"></i><b>12.2.15</b> M-Estimator Encoding</a></li>
<li class="chapter" data-level="12.2.16" data-path="categorical-variables.html"><a href="categorical-variables.html#thermometer-encoding"><i class="fa fa-check"></i><b>12.2.16</b> Thermometer Encoding</a></li>
<li class="chapter" data-level="12.2.17" data-path="categorical-variables.html"><a href="categorical-variables.html#choosing-the-right-encoding-method"><i class="fa fa-check"></i><b>12.2.17</b> Choosing the Right Encoding Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>13</b> Imputation (Missing Data)</a>
<ul>
<li class="chapter" data-level="13.1" data-path="introduction-to-missing-data.html"><a href="introduction-to-missing-data.html"><i class="fa fa-check"></i><b>13.1</b> Introduction to Missing Data</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="introduction-to-missing-data.html"><a href="introduction-to-missing-data.html#types-of-imputation"><i class="fa fa-check"></i><b>13.1.1</b> Types of Imputation</a></li>
<li class="chapter" data-level="13.1.2" data-path="introduction-to-missing-data.html"><a href="introduction-to-missing-data.html#when-and-why-to-use-imputation"><i class="fa fa-check"></i><b>13.1.2</b> When and Why to Use Imputation</a></li>
<li class="chapter" data-level="13.1.3" data-path="introduction-to-missing-data.html"><a href="introduction-to-missing-data.html#importance-of-missing-data-treatment-in-statistical-modeling"><i class="fa fa-check"></i><b>13.1.3</b> Importance of Missing Data Treatment in Statistical Modeling</a></li>
<li class="chapter" data-level="13.1.4" data-path="introduction-to-missing-data.html"><a href="introduction-to-missing-data.html#prevalence-of-missing-data-across-domains"><i class="fa fa-check"></i><b>13.1.4</b> Prevalence of Missing Data Across Domains</a></li>
<li class="chapter" data-level="13.1.5" data-path="introduction-to-missing-data.html"><a href="introduction-to-missing-data.html#practical-considerations-for-imputation"><i class="fa fa-check"></i><b>13.1.5</b> Practical Considerations for Imputation</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="theoretical-foundations-of-missing-data.html"><a href="theoretical-foundations-of-missing-data.html"><i class="fa fa-check"></i><b>13.2</b> Theoretical Foundations of Missing Data</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="theoretical-foundations-of-missing-data.html"><a href="theoretical-foundations-of-missing-data.html#definition-and-classification-of-missing-data"><i class="fa fa-check"></i><b>13.2.1</b> Definition and Classification of Missing Data</a></li>
<li class="chapter" data-level="13.2.2" data-path="theoretical-foundations-of-missing-data.html"><a href="theoretical-foundations-of-missing-data.html#missing-data-mechanisms"><i class="fa fa-check"></i><b>13.2.2</b> Missing Data Mechanisms</a></li>
<li class="chapter" data-level="13.2.3" data-path="theoretical-foundations-of-missing-data.html"><a href="theoretical-foundations-of-missing-data.html#relationship-between-mechanisms-and-ignorability"><i class="fa fa-check"></i><b>13.2.3</b> Relationship Between Mechanisms and Ignorability</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="diagnosing-the-missing-data-mechanism.html"><a href="diagnosing-the-missing-data-mechanism.html"><i class="fa fa-check"></i><b>13.3</b> Diagnosing the Missing Data Mechanism</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="diagnosing-the-missing-data-mechanism.html"><a href="diagnosing-the-missing-data-mechanism.html#descriptive-methods"><i class="fa fa-check"></i><b>13.3.1</b> Descriptive Methods</a></li>
<li class="chapter" data-level="13.3.2" data-path="diagnosing-the-missing-data-mechanism.html"><a href="diagnosing-the-missing-data-mechanism.html#statistical-tests-for-missing-data-mechanisms"><i class="fa fa-check"></i><b>13.3.2</b> Statistical Tests for Missing Data Mechanisms</a></li>
<li class="chapter" data-level="13.3.3" data-path="diagnosing-the-missing-data-mechanism.html"><a href="diagnosing-the-missing-data-mechanism.html#assessing-mar-and-mnar"><i class="fa fa-check"></i><b>13.3.3</b> Assessing MAR and MNAR</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="methods-for-handling-missing-data.html"><a href="methods-for-handling-missing-data.html"><i class="fa fa-check"></i><b>13.4</b> Methods for Handling Missing Data</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="methods-for-handling-missing-data.html"><a href="methods-for-handling-missing-data.html#basic-methods"><i class="fa fa-check"></i><b>13.4.1</b> Basic Methods</a></li>
<li class="chapter" data-level="13.4.2" data-path="methods-for-handling-missing-data.html"><a href="methods-for-handling-missing-data.html#single-imputation-techniques"><i class="fa fa-check"></i><b>13.4.2</b> Single Imputation Techniques</a></li>
<li class="chapter" data-level="13.4.3" data-path="methods-for-handling-missing-data.html"><a href="methods-for-handling-missing-data.html#machine-learning-and-modern-approaches"><i class="fa fa-check"></i><b>13.4.3</b> Machine Learning and Modern Approaches</a></li>
<li class="chapter" data-level="13.4.4" data-path="methods-for-handling-missing-data.html"><a href="methods-for-handling-missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>13.4.4</b> Multiple Imputation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="evaluation-of-imputation-methods.html"><a href="evaluation-of-imputation-methods.html"><i class="fa fa-check"></i><b>13.5</b> Evaluation of Imputation Methods</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="evaluation-of-imputation-methods.html"><a href="evaluation-of-imputation-methods.html#statistical-metrics-for-assessing-imputation-quality"><i class="fa fa-check"></i><b>13.5.1</b> Statistical Metrics for Assessing Imputation Quality</a></li>
<li class="chapter" data-level="13.5.2" data-path="evaluation-of-imputation-methods.html"><a href="evaluation-of-imputation-methods.html#bias-variance-tradeoff-in-imputation"><i class="fa fa-check"></i><b>13.5.2</b> Bias-Variance Tradeoff in Imputation</a></li>
<li class="chapter" data-level="13.5.3" data-path="evaluation-of-imputation-methods.html"><a href="evaluation-of-imputation-methods.html#sensitivity-analysis-1"><i class="fa fa-check"></i><b>13.5.3</b> Sensitivity Analysis</a></li>
<li class="chapter" data-level="13.5.4" data-path="evaluation-of-imputation-methods.html"><a href="evaluation-of-imputation-methods.html#validation-using-simulated-data-and-real-world-case-studies"><i class="fa fa-check"></i><b>13.5.4</b> Validation Using Simulated Data and Real-World Case Studies</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="criteria-for-choosing-an-effective-approach.html"><a href="criteria-for-choosing-an-effective-approach.html"><i class="fa fa-check"></i><b>13.6</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="13.7" data-path="challenges-and-ethical-considerations.html"><a href="challenges-and-ethical-considerations.html"><i class="fa fa-check"></i><b>13.7</b> Challenges and Ethical Considerations</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="challenges-and-ethical-considerations.html"><a href="challenges-and-ethical-considerations.html#challenges-in-high-dimensional-data"><i class="fa fa-check"></i><b>13.7.1</b> Challenges in High-Dimensional Data</a></li>
<li class="chapter" data-level="13.7.2" data-path="challenges-and-ethical-considerations.html"><a href="challenges-and-ethical-considerations.html#missing-data-in-big-data-contexts"><i class="fa fa-check"></i><b>13.7.2</b> Missing Data in Big Data Contexts</a></li>
<li class="chapter" data-level="13.7.3" data-path="challenges-and-ethical-considerations.html"><a href="challenges-and-ethical-considerations.html#ethical-concerns"><i class="fa fa-check"></i><b>13.7.3</b> Ethical Concerns</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="emerging-trends-in-missing-data-handling.html"><a href="emerging-trends-in-missing-data-handling.html"><i class="fa fa-check"></i><b>13.8</b> Emerging Trends in Missing Data Handling</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="emerging-trends-in-missing-data-handling.html"><a href="emerging-trends-in-missing-data-handling.html#advances-in-neural-network-approaches"><i class="fa fa-check"></i><b>13.8.1</b> Advances in Neural Network Approaches</a></li>
<li class="chapter" data-level="13.8.2" data-path="emerging-trends-in-missing-data-handling.html"><a href="emerging-trends-in-missing-data-handling.html#integration-with-reinforcement-learning"><i class="fa fa-check"></i><b>13.8.2</b> Integration with Reinforcement Learning</a></li>
<li class="chapter" data-level="13.8.3" data-path="emerging-trends-in-missing-data-handling.html"><a href="emerging-trends-in-missing-data-handling.html#synthetic-data-generation-for-missing-data"><i class="fa fa-check"></i><b>13.8.3</b> Synthetic Data Generation for Missing Data</a></li>
<li class="chapter" data-level="13.8.4" data-path="emerging-trends-in-missing-data-handling.html"><a href="emerging-trends-in-missing-data-handling.html#federated-learning-and-privacy-preserving-imputation"><i class="fa fa-check"></i><b>13.8.4</b> Federated Learning and Privacy-Preserving Imputation</a></li>
<li class="chapter" data-level="13.8.5" data-path="emerging-trends-in-missing-data-handling.html"><a href="emerging-trends-in-missing-data-handling.html#imputation-in-streaming-and-online-data-environments"><i class="fa fa-check"></i><b>13.8.5</b> Imputation in Streaming and Online Data Environments</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="application-of-imputation.html"><a href="application-of-imputation.html"><i class="fa fa-check"></i><b>13.9</b> Application of Imputation</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="application-of-imputation.html"><a href="application-of-imputation.html#visualizing-missing-data"><i class="fa fa-check"></i><b>13.9.1</b> Visualizing Missing Data</a></li>
<li class="chapter" data-level="13.9.2" data-path="application-of-imputation.html"><a href="application-of-imputation.html#how-many-imputations"><i class="fa fa-check"></i><b>13.9.2</b> How Many Imputations?</a></li>
<li class="chapter" data-level="13.9.3" data-path="application-of-imputation.html"><a href="application-of-imputation.html#generating-missing-data-for-demonstration"><i class="fa fa-check"></i><b>13.9.3</b> Generating Missing Data for Demonstration</a></li>
<li class="chapter" data-level="13.9.4" data-path="application-of-imputation.html"><a href="application-of-imputation.html#imputation-with-mean-median-and-mode"><i class="fa fa-check"></i><b>13.9.4</b> Imputation with Mean, Median, and Mode</a></li>
<li class="chapter" data-level="13.9.5" data-path="application-of-imputation.html"><a href="application-of-imputation.html#k-nearest-neighbors-knn-imputation"><i class="fa fa-check"></i><b>13.9.5</b> K-Nearest Neighbors (KNN) Imputation</a></li>
<li class="chapter" data-level="13.9.6" data-path="application-of-imputation.html"><a href="application-of-imputation.html#imputation-with-decision-trees-rpart"><i class="fa fa-check"></i><b>13.9.6</b> Imputation with Decision Trees (rpart)</a></li>
<li class="chapter" data-level="13.9.7" data-path="application-of-imputation.html"><a href="application-of-imputation.html#mice-multivariate-imputation-via-chained-equations"><i class="fa fa-check"></i><b>13.9.7</b> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li class="chapter" data-level="13.9.8" data-path="application-of-imputation.html"><a href="application-of-imputation.html#amelia"><i class="fa fa-check"></i><b>13.9.8</b> Amelia</a></li>
<li class="chapter" data-level="13.9.9" data-path="application-of-imputation.html"><a href="application-of-imputation.html#missforest"><i class="fa fa-check"></i><b>13.9.9</b> missForest</a></li>
<li class="chapter" data-level="13.9.10" data-path="application-of-imputation.html"><a href="application-of-imputation.html#hmisc"><i class="fa fa-check"></i><b>13.9.10</b> Hmisc</a></li>
<li class="chapter" data-level="13.9.11" data-path="application-of-imputation.html"><a href="application-of-imputation.html#mi"><i class="fa fa-check"></i><b>13.9.11</b> mi</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="model-specification-tests.html"><a href="model-specification-tests.html"><i class="fa fa-check"></i><b>14</b> Model Specification Tests</a>
<ul>
<li class="chapter" data-level="14.1" data-path="nested-model-tests.html"><a href="nested-model-tests.html"><i class="fa fa-check"></i><b>14.1</b> Nested Model Tests</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="nested-model-tests.html"><a href="nested-model-tests.html#sec-wald-test-nested"><i class="fa fa-check"></i><b>14.1.1</b> Wald Test</a></li>
<li class="chapter" data-level="14.1.2" data-path="nested-model-tests.html"><a href="nested-model-tests.html#sec-likelihood-ratio-test-nested"><i class="fa fa-check"></i><b>14.1.2</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="14.1.3" data-path="nested-model-tests.html"><a href="nested-model-tests.html#sec-f-test-for-linear-regression-nested"><i class="fa fa-check"></i><b>14.1.3</b> F-Test (for Linear Regression)</a></li>
<li class="chapter" data-level="14.1.4" data-path="nested-model-tests.html"><a href="nested-model-tests.html#sec-chow-test"><i class="fa fa-check"></i><b>14.1.4</b> Chow Test</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="non-nested-model-tests.html"><a href="non-nested-model-tests.html"><i class="fa fa-check"></i><b>14.2</b> Non-Nested Model Tests</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="non-nested-model-tests.html"><a href="non-nested-model-tests.html#sec-vuong-test"><i class="fa fa-check"></i><b>14.2.1</b> Vuong Test</a></li>
<li class="chapter" data-level="14.2.2" data-path="non-nested-model-tests.html"><a href="non-nested-model-tests.html#sec-davidson--mackinnon-j-test"><i class="fa fa-check"></i><b>14.2.2</b> Davidson–MacKinnon J-Test</a></li>
<li class="chapter" data-level="14.2.3" data-path="non-nested-model-tests.html"><a href="non-nested-model-tests.html#adjusted-r2"><i class="fa fa-check"></i><b>14.2.3</b> Adjusted <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="14.2.4" data-path="non-nested-model-tests.html"><a href="non-nested-model-tests.html#comparing-models-with-transformed-dependent-variables"><i class="fa fa-check"></i><b>14.2.4</b> Comparing Models with Transformed Dependent Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="heteroskedasticity-tests.html"><a href="heteroskedasticity-tests.html"><i class="fa fa-check"></i><b>14.3</b> Heteroskedasticity Tests</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="heteroskedasticity-tests.html"><a href="heteroskedasticity-tests.html#sec-breusch--pagan-test"><i class="fa fa-check"></i><b>14.3.1</b> Breusch–Pagan Test</a></li>
<li class="chapter" data-level="14.3.2" data-path="heteroskedasticity-tests.html"><a href="heteroskedasticity-tests.html#sec-white-test-hetero"><i class="fa fa-check"></i><b>14.3.2</b> White Test</a></li>
<li class="chapter" data-level="14.3.3" data-path="heteroskedasticity-tests.html"><a href="heteroskedasticity-tests.html#sec-goldfeld--quandt-test"><i class="fa fa-check"></i><b>14.3.3</b> Goldfeld–Quandt Test</a></li>
<li class="chapter" data-level="14.3.4" data-path="heteroskedasticity-tests.html"><a href="heteroskedasticity-tests.html#sec-park-test"><i class="fa fa-check"></i><b>14.3.4</b> Park Test</a></li>
<li class="chapter" data-level="14.3.5" data-path="heteroskedasticity-tests.html"><a href="heteroskedasticity-tests.html#sec-glejser-test"><i class="fa fa-check"></i><b>14.3.5</b> Glejser Test</a></li>
<li class="chapter" data-level="14.3.6" data-path="heteroskedasticity-tests.html"><a href="heteroskedasticity-tests.html#summary-of-heteroskedasticity-tests"><i class="fa fa-check"></i><b>14.3.6</b> Summary of Heteroskedasticity Tests</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="functional-form-tests.html"><a href="functional-form-tests.html"><i class="fa fa-check"></i><b>14.4</b> Functional Form Tests</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="functional-form-tests.html"><a href="functional-form-tests.html#sec-ramsey-reset-test"><i class="fa fa-check"></i><b>14.4.1</b> Ramsey RESET Test (Regression Equation Specification Error Test)</a></li>
<li class="chapter" data-level="14.4.2" data-path="functional-form-tests.html"><a href="functional-form-tests.html#sec-harvey--collier-test"><i class="fa fa-check"></i><b>14.4.2</b> Harvey–Collier Test</a></li>
<li class="chapter" data-level="14.4.3" data-path="functional-form-tests.html"><a href="functional-form-tests.html#sec-rainbow-test"><i class="fa fa-check"></i><b>14.4.3</b> Rainbow Test</a></li>
<li class="chapter" data-level="14.4.4" data-path="functional-form-tests.html"><a href="functional-form-tests.html#summary-of-functional-form-tests"><i class="fa fa-check"></i><b>14.4.4</b> Summary of Functional Form Tests</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="autocorrelation-tests.html"><a href="autocorrelation-tests.html"><i class="fa fa-check"></i><b>14.5</b> Autocorrelation Tests</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="autocorrelation-tests.html"><a href="autocorrelation-tests.html#sec-durbin--watson-test"><i class="fa fa-check"></i><b>14.5.1</b> Durbin–Watson Test</a></li>
<li class="chapter" data-level="14.5.2" data-path="autocorrelation-tests.html"><a href="autocorrelation-tests.html#sec-breusch--godfrey-test"><i class="fa fa-check"></i><b>14.5.2</b> Breusch–Godfrey Test</a></li>
<li class="chapter" data-level="14.5.3" data-path="autocorrelation-tests.html"><a href="autocorrelation-tests.html#sec-ljung--box-test"><i class="fa fa-check"></i><b>14.5.3</b> Ljung–Box Test (or Box–Pierce Test)</a></li>
<li class="chapter" data-level="14.5.4" data-path="autocorrelation-tests.html"><a href="autocorrelation-tests.html#sec-runs-test"><i class="fa fa-check"></i><b>14.5.4</b> Runs Test</a></li>
<li class="chapter" data-level="14.5.5" data-path="autocorrelation-tests.html"><a href="autocorrelation-tests.html#summary-of-autocorrelation-tests"><i class="fa fa-check"></i><b>14.5.5</b> Summary of Autocorrelation Tests</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html"><i class="fa fa-check"></i><b>14.6</b> Multicollinearity Diagnostics</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html#sec-variance-inflation-factor"><i class="fa fa-check"></i><b>14.6.1</b> Variance Inflation Factor</a></li>
<li class="chapter" data-level="14.6.2" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html#sec-tolerance-statistic"><i class="fa fa-check"></i><b>14.6.2</b> Tolerance Statistic</a></li>
<li class="chapter" data-level="14.6.3" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html#sec-condition-index-and-eigenvalue-decomposition"><i class="fa fa-check"></i><b>14.6.3</b> Condition Index and Eigenvalue Decomposition</a></li>
<li class="chapter" data-level="14.6.4" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html#sec-pairwise-correlation-matrix"><i class="fa fa-check"></i><b>14.6.4</b> Pairwise Correlation Matrix</a></li>
<li class="chapter" data-level="14.6.5" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html#sec-determinant-of-the-correlation-matrix"><i class="fa fa-check"></i><b>14.6.5</b> Determinant of the Correlation Matrix</a></li>
<li class="chapter" data-level="14.6.6" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html#summary-of-multicollinearity-diagnostics"><i class="fa fa-check"></i><b>14.6.6</b> Summary of Multicollinearity Diagnostics</a></li>
<li class="chapter" data-level="14.6.7" data-path="multicollinearity-diagnostics.html"><a href="multicollinearity-diagnostics.html#addressing-multicollinearity"><i class="fa fa-check"></i><b>14.6.7</b> Addressing Multicollinearity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="variable-selection.html"><a href="variable-selection.html"><i class="fa fa-check"></i><b>15</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="15.1" data-path="sec-filter-methods.html"><a href="sec-filter-methods.html"><i class="fa fa-check"></i><b>15.1</b> Filter Methods (Statistical Criteria, Model-Agnostic)</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="sec-filter-methods.html"><a href="sec-filter-methods.html#information-criteria-based-selection"><i class="fa fa-check"></i><b>15.1.1</b> Information Criteria-Based Selection</a></li>
<li class="chapter" data-level="15.1.2" data-path="sec-filter-methods.html"><a href="sec-filter-methods.html#univariate-selection-methods"><i class="fa fa-check"></i><b>15.1.2</b> Univariate Selection Methods</a></li>
<li class="chapter" data-level="15.1.3" data-path="sec-filter-methods.html"><a href="sec-filter-methods.html#correlation-based-feature-selection"><i class="fa fa-check"></i><b>15.1.3</b> Correlation-Based Feature Selection</a></li>
<li class="chapter" data-level="15.1.4" data-path="sec-filter-methods.html"><a href="sec-filter-methods.html#variance-thresholding"><i class="fa fa-check"></i><b>15.1.4</b> Variance Thresholding</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="wrapper-methods-model-based-subset-evaluation.html"><a href="wrapper-methods-model-based-subset-evaluation.html"><i class="fa fa-check"></i><b>15.2</b> Wrapper Methods (Model-Based Subset Evaluation)</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="wrapper-methods-model-based-subset-evaluation.html"><a href="wrapper-methods-model-based-subset-evaluation.html#best-subsets-algorithm-1"><i class="fa fa-check"></i><b>15.2.1</b> Best Subsets Algorithm</a></li>
<li class="chapter" data-level="15.2.2" data-path="wrapper-methods-model-based-subset-evaluation.html"><a href="wrapper-methods-model-based-subset-evaluation.html#stepwise-selection-methods-1"><i class="fa fa-check"></i><b>15.2.2</b> Stepwise Selection Methods</a></li>
<li class="chapter" data-level="15.2.3" data-path="wrapper-methods-model-based-subset-evaluation.html"><a href="wrapper-methods-model-based-subset-evaluation.html#branch-and-bound-algorithm-1"><i class="fa fa-check"></i><b>15.2.3</b> Branch-and-Bound Algorithm</a></li>
<li class="chapter" data-level="15.2.4" data-path="wrapper-methods-model-based-subset-evaluation.html"><a href="wrapper-methods-model-based-subset-evaluation.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>15.2.4</b> Recursive Feature Elimination</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="embedded-methods-integrated-into-model-training.html"><a href="embedded-methods-integrated-into-model-training.html"><i class="fa fa-check"></i><b>15.3</b> Embedded Methods (Integrated into Model Training)</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="embedded-methods-integrated-into-model-training.html"><a href="embedded-methods-integrated-into-model-training.html#regularization-based-selection"><i class="fa fa-check"></i><b>15.3.1</b> Regularization-Based Selection</a></li>
<li class="chapter" data-level="15.3.2" data-path="embedded-methods-integrated-into-model-training.html"><a href="embedded-methods-integrated-into-model-training.html#tree-based-feature-importance"><i class="fa fa-check"></i><b>15.3.2</b> Tree-Based Feature Importance</a></li>
<li class="chapter" data-level="15.3.3" data-path="embedded-methods-integrated-into-model-training.html"><a href="embedded-methods-integrated-into-model-training.html#genetic-algorithms-1"><i class="fa fa-check"></i><b>15.3.3</b> Genetic Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="summary-table-1.html"><a href="summary-table-1.html"><i class="fa fa-check"></i><b>15.4</b> Summary Table</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>16</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="16.1" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html"><i class="fa fa-check"></i><b>16.1</b> Null Hypothesis Significance Testing</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#error-types-in-hypothesis-testing"><i class="fa fa-check"></i><b>16.1.1</b> Error Types in Hypothesis Testing</a></li>
<li class="chapter" data-level="16.1.2" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#hypothesis-testing-framework-1"><i class="fa fa-check"></i><b>16.1.2</b> Hypothesis Testing Framework</a></li>
<li class="chapter" data-level="16.1.3" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#interpreting-hypothesis-testing-results"><i class="fa fa-check"></i><b>16.1.3</b> Interpreting Hypothesis Testing Results</a></li>
<li class="chapter" data-level="16.1.4" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#understanding-p-values"><i class="fa fa-check"></i><b>16.1.4</b> Understanding p-Values</a></li>
<li class="chapter" data-level="16.1.5" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#the-role-of-sample-size"><i class="fa fa-check"></i><b>16.1.5</b> The Role of Sample Size</a></li>
<li class="chapter" data-level="16.1.6" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#p-value-hacking"><i class="fa fa-check"></i><b>16.1.6</b> p-Value Hacking</a></li>
<li class="chapter" data-level="16.1.7" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#practical-vs.-statistical-significance"><i class="fa fa-check"></i><b>16.1.7</b> Practical vs. Statistical Significance</a></li>
<li class="chapter" data-level="16.1.8" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#mitigating-the-misuse-of-p-values"><i class="fa fa-check"></i><b>16.1.8</b> Mitigating the Misuse of p-Values</a></li>
<li class="chapter" data-level="16.1.9" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#sec-wald-test"><i class="fa fa-check"></i><b>16.1.9</b> Wald Test</a></li>
<li class="chapter" data-level="16.1.10" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#sec-likelihood-ratio-test"><i class="fa fa-check"></i><b>16.1.10</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="16.1.11" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#lagrange-multiplier-score"><i class="fa fa-check"></i><b>16.1.11</b> Lagrange Multiplier (Score) Test</a></li>
<li class="chapter" data-level="16.1.12" data-path="sec-null-hypothesis-significance-testing.html"><a href="sec-null-hypothesis-significance-testing.html#comparing-hypothesis-tests"><i class="fa fa-check"></i><b>16.1.12</b> Comparing Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="sec-two-one-sided-tests-equivalence-testing.html"><a href="sec-two-one-sided-tests-equivalence-testing.html"><i class="fa fa-check"></i><b>16.2</b> Two One-Sided Tests Equivalence Testing</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="sec-two-one-sided-tests-equivalence-testing.html"><a href="sec-two-one-sided-tests-equivalence-testing.html#when-to-use-tost"><i class="fa fa-check"></i><b>16.2.1</b> When to Use TOST?</a></li>
<li class="chapter" data-level="16.2.2" data-path="sec-two-one-sided-tests-equivalence-testing.html"><a href="sec-two-one-sided-tests-equivalence-testing.html#interpretation-of-the-tost-procedure"><i class="fa fa-check"></i><b>16.2.2</b> Interpretation of the TOST Procedure</a></li>
<li class="chapter" data-level="16.2.3" data-path="sec-two-one-sided-tests-equivalence-testing.html"><a href="sec-two-one-sided-tests-equivalence-testing.html#relationship-to-confidence-intervals"><i class="fa fa-check"></i><b>16.2.3</b> Relationship to Confidence Intervals</a></li>
<li class="chapter" data-level="16.2.4" data-path="sec-two-one-sided-tests-equivalence-testing.html"><a href="sec-two-one-sided-tests-equivalence-testing.html#example-1-testing-the-equivalence-of-two-means"><i class="fa fa-check"></i><b>16.2.4</b> Example 1: Testing the Equivalence of Two Means</a></li>
<li class="chapter" data-level="16.2.5" data-path="sec-two-one-sided-tests-equivalence-testing.html"><a href="sec-two-one-sided-tests-equivalence-testing.html#advantages-of-tost-equivalence-testing"><i class="fa fa-check"></i><b>16.2.5</b> Advantages of TOST Equivalence Testing</a></li>
<li class="chapter" data-level="16.2.6" data-path="sec-two-one-sided-tests-equivalence-testing.html"><a href="sec-two-one-sided-tests-equivalence-testing.html#when-not-to-use-tost"><i class="fa fa-check"></i><b>16.2.6</b> When <em>Not</em> to Use TOST</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="sec-false-discovery-rate.html"><a href="sec-false-discovery-rate.html"><i class="fa fa-check"></i><b>16.3</b> False Discovery Rate</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="sec-false-discovery-rate.html"><a href="sec-false-discovery-rate.html#sec-benjamini-hochberg-procedure"><i class="fa fa-check"></i><b>16.3.1</b> Benjamini-Hochberg Procedure</a></li>
<li class="chapter" data-level="16.3.2" data-path="sec-false-discovery-rate.html"><a href="sec-false-discovery-rate.html#sec-benjamini-yekutieli-procedure"><i class="fa fa-check"></i><b>16.3.2</b> Benjamini-Yekutieli Procedure</a></li>
<li class="chapter" data-level="16.3.3" data-path="sec-false-discovery-rate.html"><a href="sec-false-discovery-rate.html#sec-storeys-q-value-approach"><i class="fa fa-check"></i><b>16.3.3</b> Storey’s q-value Approach</a></li>
<li class="chapter" data-level="16.3.4" data-path="sec-false-discovery-rate.html"><a href="sec-false-discovery-rate.html#summary-false-discovery-rate-methods"><i class="fa fa-check"></i><b>16.3.4</b> Summary: False Discovery Rate Methods</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="comparison-of-testing-frameworks.html"><a href="comparison-of-testing-frameworks.html"><i class="fa fa-check"></i><b>16.4</b> Comparison of Testing Frameworks</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="sec-marginal-effects.html"><a href="sec-marginal-effects.html"><i class="fa fa-check"></i><b>17</b> Marginal Effects</a>
<ul>
<li class="chapter" data-level="17.1" data-path="definition-of-marginal-effects.html"><a href="definition-of-marginal-effects.html"><i class="fa fa-check"></i><b>17.1</b> Definition of Marginal Effects</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="definition-of-marginal-effects.html"><a href="definition-of-marginal-effects.html#sec-analytical-derivation-of-marginal-effects"><i class="fa fa-check"></i><b>17.1.1</b> Analytical Derivation of Marginal Effects</a></li>
<li class="chapter" data-level="17.1.2" data-path="definition-of-marginal-effects.html"><a href="definition-of-marginal-effects.html#sec-numerical-approximation-of-marginal-effects"><i class="fa fa-check"></i><b>17.1.2</b> Numerical Approximation of Marginal Effects</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="marginal-effects-in-different-contexts.html"><a href="marginal-effects-in-different-contexts.html"><i class="fa fa-check"></i><b>17.2</b> Marginal Effects in Different Contexts</a></li>
<li class="chapter" data-level="17.3" data-path="marginal-effects-interpretation.html"><a href="marginal-effects-interpretation.html"><i class="fa fa-check"></i><b>17.3</b> Marginal Effects Interpretation</a></li>
<li class="chapter" data-level="17.4" data-path="sec-delta-method.html"><a href="sec-delta-method.html"><i class="fa fa-check"></i><b>17.4</b> Delta Method</a></li>
<li class="chapter" data-level="17.5" data-path="comparison-delta-method-vs.-alternative-approaches.html"><a href="comparison-delta-method-vs.-alternative-approaches.html"><i class="fa fa-check"></i><b>17.5</b> Comparison: Delta Method vs. Alternative Approaches</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="comparison-delta-method-vs.-alternative-approaches.html"><a href="comparison-delta-method-vs.-alternative-approaches.html#example-applying-the-delta-method-in-a-logistic-regression"><i class="fa fa-check"></i><b>17.5.1</b> Example: Applying the Delta Method in a logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="types-of-marginal-effect.html"><a href="types-of-marginal-effect.html"><i class="fa fa-check"></i><b>17.6</b> Types of Marginal Effect</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="types-of-marginal-effect.html"><a href="types-of-marginal-effect.html#sec-average-marginal-effect"><i class="fa fa-check"></i><b>17.6.1</b> Average Marginal Effect</a></li>
<li class="chapter" data-level="17.6.2" data-path="types-of-marginal-effect.html"><a href="types-of-marginal-effect.html#sec-marginal-effects-at-the-mean"><i class="fa fa-check"></i><b>17.6.2</b> Marginal Effects at the Mean</a></li>
<li class="chapter" data-level="17.6.3" data-path="types-of-marginal-effect.html"><a href="types-of-marginal-effect.html#sec-marginal-effects-at-the-average"><i class="fa fa-check"></i><b>17.6.3</b> Marginal Effects at the Average</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="packages-for-marginal-effects.html"><a href="packages-for-marginal-effects.html"><i class="fa fa-check"></i><b>17.7</b> Packages for Marginal Effects</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="packages-for-marginal-effects.html"><a href="packages-for-marginal-effects.html#marginaleffects-package-recommended"><i class="fa fa-check"></i><b>17.7.1</b> <code>marginaleffects</code> Package (Recommended)</a></li>
<li class="chapter" data-level="17.7.2" data-path="packages-for-marginal-effects.html"><a href="packages-for-marginal-effects.html#margins-package"><i class="fa fa-check"></i><b>17.7.2</b> <code>margins</code> Package</a></li>
<li class="chapter" data-level="17.7.3" data-path="packages-for-marginal-effects.html"><a href="packages-for-marginal-effects.html#mfx-package"><i class="fa fa-check"></i><b>17.7.3</b> <code>mfx</code> Package</a></li>
<li class="chapter" data-level="17.7.4" data-path="packages-for-marginal-effects.html"><a href="packages-for-marginal-effects.html#comparison-of-packages"><i class="fa fa-check"></i><b>17.7.4</b> Comparison of Packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="moderation.html"><a href="moderation.html"><i class="fa fa-check"></i><b>18</b> Moderation</a>
<ul>
<li class="chapter" data-level="18.1" data-path="types-of-moderation-analyses.html"><a href="types-of-moderation-analyses.html"><i class="fa fa-check"></i><b>18.1</b> Types of Moderation Analyses</a></li>
<li class="chapter" data-level="18.2" data-path="key-terminology.html"><a href="key-terminology.html"><i class="fa fa-check"></i><b>18.2</b> Key Terminology</a></li>
<li class="chapter" data-level="18.3" data-path="moderation-model.html"><a href="moderation-model.html"><i class="fa fa-check"></i><b>18.3</b> Moderation Model</a></li>
<li class="chapter" data-level="18.4" data-path="types-of-interactions.html"><a href="types-of-interactions.html"><i class="fa fa-check"></i><b>18.4</b> Types of Interactions</a></li>
<li class="chapter" data-level="18.5" data-path="three-way-interactions.html"><a href="three-way-interactions.html"><i class="fa fa-check"></i><b>18.5</b> Three-Way Interactions</a></li>
<li class="chapter" data-level="18.6" data-path="additional-resources.html"><a href="additional-resources.html"><i class="fa fa-check"></i><b>18.6</b> Additional Resources</a></li>
<li class="chapter" data-level="18.7" data-path="application-2.html"><a href="application-2.html"><i class="fa fa-check"></i><b>18.7</b> Application</a>
<ul>
<li class="chapter" data-level="18.7.1" data-path="application-2.html"><a href="application-2.html#emmeans-package"><i class="fa fa-check"></i><b>18.7.1</b> <code>emmeans</code> Package</a></li>
<li class="chapter" data-level="18.7.2" data-path="application-2.html"><a href="application-2.html#probemod-package"><i class="fa fa-check"></i><b>18.7.2</b> <code>probemod</code> Package</a></li>
<li class="chapter" data-level="18.7.3" data-path="application-2.html"><a href="application-2.html#interactions-package"><i class="fa fa-check"></i><b>18.7.3</b> <code>interactions</code> Package</a></li>
<li class="chapter" data-level="18.7.4" data-path="application-2.html"><a href="application-2.html#interactionr-package"><i class="fa fa-check"></i><b>18.7.4</b> <code>interactionR</code> Package</a></li>
<li class="chapter" data-level="18.7.5" data-path="application-2.html"><a href="application-2.html#sjplot-package"><i class="fa fa-check"></i><b>18.7.5</b> <code>sjPlot</code> Package</a></li>
<li class="chapter" data-level="18.7.6" data-path="application-2.html"><a href="application-2.html#summary-of-moderation-analysis-packages"><i class="fa fa-check"></i><b>18.7.6</b> Summary of Moderation Analysis Packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="mediation.html"><a href="mediation.html"><i class="fa fa-check"></i><b>19</b> Mediation</a>
<ul>
<li class="chapter" data-level="19.1" data-path="traditional-approach.html"><a href="traditional-approach.html"><i class="fa fa-check"></i><b>19.1</b> Traditional Approach</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="traditional-approach.html"><a href="traditional-approach.html#steps-in-the-traditional-mediation-model"><i class="fa fa-check"></i><b>19.1.1</b> Steps in the Traditional Mediation Model</a></li>
<li class="chapter" data-level="19.1.2" data-path="traditional-approach.html"><a href="traditional-approach.html#graphical-representation-of-mediation"><i class="fa fa-check"></i><b>19.1.2</b> Graphical Representation of Mediation</a></li>
<li class="chapter" data-level="19.1.3" data-path="traditional-approach.html"><a href="traditional-approach.html#measuring-mediation"><i class="fa fa-check"></i><b>19.1.3</b> Measuring Mediation</a></li>
<li class="chapter" data-level="19.1.4" data-path="traditional-approach.html"><a href="traditional-approach.html#assumptions-in-linear-mediation-models"><i class="fa fa-check"></i><b>19.1.4</b> Assumptions in Linear Mediation Models</a></li>
<li class="chapter" data-level="19.1.5" data-path="traditional-approach.html"><a href="traditional-approach.html#testing-for-mediation"><i class="fa fa-check"></i><b>19.1.5</b> Testing for Mediation</a></li>
<li class="chapter" data-level="19.1.6" data-path="traditional-approach.html"><a href="traditional-approach.html#additional-considerations"><i class="fa fa-check"></i><b>19.1.6</b> Additional Considerations</a></li>
<li class="chapter" data-level="19.1.7" data-path="traditional-approach.html"><a href="traditional-approach.html#assumptions-in-mediation-analysis"><i class="fa fa-check"></i><b>19.1.7</b> Assumptions in Mediation Analysis</a></li>
<li class="chapter" data-level="19.1.8" data-path="traditional-approach.html"><a href="traditional-approach.html#indirect-effect-tests"><i class="fa fa-check"></i><b>19.1.8</b> Indirect Effect Tests</a></li>
<li class="chapter" data-level="19.1.9" data-path="traditional-approach.html"><a href="traditional-approach.html#power-analysis-for-mediation"><i class="fa fa-check"></i><b>19.1.9</b> Power Analysis for Mediation</a></li>
<li class="chapter" data-level="19.1.10" data-path="traditional-approach.html"><a href="traditional-approach.html#multiple-mediation-analysis"><i class="fa fa-check"></i><b>19.1.10</b> Multiple Mediation Analysis</a></li>
<li class="chapter" data-level="19.1.11" data-path="traditional-approach.html"><a href="traditional-approach.html#multiple-treatments-in-mediation"><i class="fa fa-check"></i><b>19.1.11</b> Multiple Treatments in Mediation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="causal-inference-approach-to-mediation.html"><a href="causal-inference-approach-to-mediation.html"><i class="fa fa-check"></i><b>19.2</b> Causal Inference Approach to Mediation</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="causal-inference-approach-to-mediation.html"><a href="causal-inference-approach-to-mediation.html#sec-example-traditional-mediation-analysis"><i class="fa fa-check"></i><b>19.2.1</b> Example: Traditional Mediation Analysis</a></li>
<li class="chapter" data-level="19.2.2" data-path="causal-inference-approach-to-mediation.html"><a href="causal-inference-approach-to-mediation.html#two-approaches-in-causal-mediation-analysis"><i class="fa fa-check"></i><b>19.2.2</b> Two Approaches in Causal Mediation Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="prediction-and-estimation.html"><a href="prediction-and-estimation.html"><i class="fa fa-check"></i><b>20</b> Prediction and Estimation</a>
<ul>
<li class="chapter" data-level="20.1" data-path="conceptual-framing.html"><a href="conceptual-framing.html"><i class="fa fa-check"></i><b>20.1</b> Conceptual Framing</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="conceptual-framing.html"><a href="conceptual-framing.html#predictive-modeling"><i class="fa fa-check"></i><b>20.1.1</b> Predictive Modeling</a></li>
<li class="chapter" data-level="20.1.2" data-path="conceptual-framing.html"><a href="conceptual-framing.html#estimation-or-causal-inference"><i class="fa fa-check"></i><b>20.1.2</b> Estimation or Causal Inference</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="mathematical-setup.html"><a href="mathematical-setup.html"><i class="fa fa-check"></i><b>20.2</b> Mathematical Setup</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="mathematical-setup.html"><a href="mathematical-setup.html#probability-space-and-data"><i class="fa fa-check"></i><b>20.2.1</b> Probability Space and Data</a></li>
<li class="chapter" data-level="20.2.2" data-path="mathematical-setup.html"><a href="mathematical-setup.html#loss-functions-and-risk"><i class="fa fa-check"></i><b>20.2.2</b> Loss Functions and Risk</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="prediction-in-detail.html"><a href="prediction-in-detail.html"><i class="fa fa-check"></i><b>20.3</b> Prediction in Detail</a>
<ul>
<li class="chapter" data-level="20.3.1" data-path="prediction-in-detail.html"><a href="prediction-in-detail.html#empirical-risk-minimization-and-generalization"><i class="fa fa-check"></i><b>20.3.1</b> Empirical Risk Minimization and Generalization</a></li>
<li class="chapter" data-level="20.3.2" data-path="prediction-in-detail.html"><a href="prediction-in-detail.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>20.3.2</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="20.3.3" data-path="prediction-in-detail.html"><a href="prediction-in-detail.html#example-linear-regression-for-prediction"><i class="fa fa-check"></i><b>20.3.3</b> Example: Linear Regression for Prediction</a></li>
<li class="chapter" data-level="20.3.4" data-path="prediction-in-detail.html"><a href="prediction-in-detail.html#applications-in-economics"><i class="fa fa-check"></i><b>20.3.4</b> Applications in Economics</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="parameter-estimation-and-causal-inference.html"><a href="parameter-estimation-and-causal-inference.html"><i class="fa fa-check"></i><b>20.4</b> Parameter Estimation and Causal Inference</a>
<ul>
<li class="chapter" data-level="20.4.1" data-path="parameter-estimation-and-causal-inference.html"><a href="parameter-estimation-and-causal-inference.html#estimation-in-parametric-models"><i class="fa fa-check"></i><b>20.4.1</b> Estimation in Parametric Models</a></li>
<li class="chapter" data-level="20.4.2" data-path="parameter-estimation-and-causal-inference.html"><a href="parameter-estimation-and-causal-inference.html#causal-inference-fundamentals"><i class="fa fa-check"></i><b>20.4.2</b> Causal Inference Fundamentals</a></li>
<li class="chapter" data-level="20.4.3" data-path="parameter-estimation-and-causal-inference.html"><a href="parameter-estimation-and-causal-inference.html#role-of-identification"><i class="fa fa-check"></i><b>20.4.3</b> Role of Identification</a></li>
<li class="chapter" data-level="20.4.4" data-path="parameter-estimation-and-causal-inference.html"><a href="parameter-estimation-and-causal-inference.html#challenges-1"><i class="fa fa-check"></i><b>20.4.4</b> Challenges</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="causation-versus-prediction.html"><a href="causation-versus-prediction.html"><i class="fa fa-check"></i><b>20.5</b> Causation versus Prediction</a></li>
<li class="chapter" data-level="20.6" data-path="illustrative-equations-and-mathematical-contrasts.html"><a href="illustrative-equations-and-mathematical-contrasts.html"><i class="fa fa-check"></i><b>20.6</b> Illustrative Equations and Mathematical Contrasts</a>
<ul>
<li class="chapter" data-level="20.6.1" data-path="illustrative-equations-and-mathematical-contrasts.html"><a href="illustrative-equations-and-mathematical-contrasts.html#risk-minimization-vs.-consistency"><i class="fa fa-check"></i><b>20.6.1</b> Risk Minimization vs. Consistency</a></li>
<li class="chapter" data-level="20.6.2" data-path="illustrative-equations-and-mathematical-contrasts.html"><a href="illustrative-equations-and-mathematical-contrasts.html#partial-derivatives-vs.-predictions"><i class="fa fa-check"></i><b>20.6.2</b> Partial Derivatives vs. Predictions</a></li>
<li class="chapter" data-level="20.6.3" data-path="illustrative-equations-and-mathematical-contrasts.html"><a href="illustrative-equations-and-mathematical-contrasts.html#example-high-dimensional-regularization"><i class="fa fa-check"></i><b>20.6.3</b> Example: High-Dimensional Regularization</a></li>
<li class="chapter" data-level="20.6.4" data-path="illustrative-equations-and-mathematical-contrasts.html"><a href="illustrative-equations-and-mathematical-contrasts.html#potential-outcomes-notation"><i class="fa fa-check"></i><b>20.6.4</b> Potential Outcomes Notation</a></li>
</ul></li>
<li class="chapter" data-level="20.7" data-path="extended-mathematical-points.html"><a href="extended-mathematical-points.html"><i class="fa fa-check"></i><b>20.7</b> Extended Mathematical Points</a>
<ul>
<li class="chapter" data-level="20.7.1" data-path="extended-mathematical-points.html"><a href="extended-mathematical-points.html#m-estimation-and-asymptotic-theory"><i class="fa fa-check"></i><b>20.7.1</b> M-Estimation and Asymptotic Theory</a></li>
<li class="chapter" data-level="20.7.2" data-path="extended-mathematical-points.html"><a href="extended-mathematical-points.html#the-danger-of-omitted-variables"><i class="fa fa-check"></i><b>20.7.2</b> The Danger of Omitted Variables</a></li>
<li class="chapter" data-level="20.7.3" data-path="extended-mathematical-points.html"><a href="extended-mathematical-points.html#cross-validation-vs.-statistical-testing"><i class="fa fa-check"></i><b>20.7.3</b> Cross-Validation vs. Statistical Testing</a></li>
</ul></li>
<li class="chapter" data-level="20.8" data-path="putting-it-all-together-comparing-objectives.html"><a href="putting-it-all-together-comparing-objectives.html"><i class="fa fa-check"></i><b>20.8</b> Putting It All Together: Comparing Objectives</a></li>
<li class="chapter" data-level="20.9" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>20.9</b> Conclusion</a></li>
</ul></li>
<li class="part"><span><b>IV. CAUSAL INFERENCE</b></span></li>
<li class="chapter" data-level="21" data-path="sec-causal-inference.html"><a href="sec-causal-inference.html"><i class="fa fa-check"></i><b>21</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="21.1" data-path="sec-the-ladder-of-causation.html"><a href="sec-the-ladder-of-causation.html"><i class="fa fa-check"></i><b>21.1</b> The Ladder of Causation</a></li>
<li class="chapter" data-level="21.2" data-path="the-formal-notation-of-causality.html"><a href="the-formal-notation-of-causality.html"><i class="fa fa-check"></i><b>21.2</b> The Formal Notation of Causality</a></li>
<li class="chapter" data-level="21.3" data-path="the-7-tools-of-structural-causal-models.html"><a href="the-7-tools-of-structural-causal-models.html"><i class="fa fa-check"></i><b>21.3</b> The 7 Tools of Structural Causal Models</a></li>
<li class="chapter" data-level="21.4" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html"><i class="fa fa-check"></i><b>21.4</b> Simpson’s Paradox</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#what-is-simpsons-paradox"><i class="fa fa-check"></i><b>21.4.1</b> What is Simpson’s Paradox?</a></li>
<li class="chapter" data-level="21.4.2" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#why-is-this-important"><i class="fa fa-check"></i><b>21.4.2</b> Why is this Important?</a></li>
<li class="chapter" data-level="21.4.3" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#comparison-between-simpsons-paradox-and-omitted-variable-bias"><i class="fa fa-check"></i><b>21.4.3</b> Comparison between Simpson’s Paradox and Omitted Variable Bias</a></li>
<li class="chapter" data-level="21.4.4" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#illustrating-simpsons-paradox-marketing-campaign-success-rates"><i class="fa fa-check"></i><b>21.4.4</b> Illustrating Simpson’s Paradox: Marketing Campaign Success Rates</a></li>
<li class="chapter" data-level="21.4.5" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#why-does-this-happen"><i class="fa fa-check"></i><b>21.4.5</b> Why Does This Happen?</a></li>
<li class="chapter" data-level="21.4.6" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#how-does-causal-inference-solve-this"><i class="fa fa-check"></i><b>21.4.6</b> How Does Causal Inference Solve This?</a></li>
<li class="chapter" data-level="21.4.7" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#correcting-simpsons-paradox-with-regression-adjustment"><i class="fa fa-check"></i><b>21.4.7</b> Correcting Simpson’s Paradox with Regression Adjustment</a></li>
<li class="chapter" data-level="21.4.8" data-path="simpsons-paradox.html"><a href="simpsons-paradox.html#key-takeaways-3"><i class="fa fa-check"></i><b>21.4.8</b> Key Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="additional-resources-1.html"><a href="additional-resources-1.html"><i class="fa fa-check"></i><b>21.5</b> Additional Resources</a></li>
<li class="chapter" data-level="21.6" data-path="experimental-vs.-quasi-experimental-designs.html"><a href="experimental-vs.-quasi-experimental-designs.html"><i class="fa fa-check"></i><b>21.6</b> Experimental vs. Quasi-Experimental Designs</a>
<ul>
<li class="chapter" data-level="21.6.1" data-path="experimental-vs.-quasi-experimental-designs.html"><a href="experimental-vs.-quasi-experimental-designs.html#criticisms-of-quasi-experimental-designs"><i class="fa fa-check"></i><b>21.6.1</b> Criticisms of Quasi-Experimental Designs</a></li>
</ul></li>
<li class="chapter" data-level="21.7" data-path="hierarchical-ordering-of-causal-tools.html"><a href="hierarchical-ordering-of-causal-tools.html"><i class="fa fa-check"></i><b>21.7</b> Hierarchical Ordering of Causal Tools</a></li>
<li class="chapter" data-level="21.8" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html"><i class="fa fa-check"></i><b>21.8</b> Types of Validity in Research</a>
<ul>
<li class="chapter" data-level="21.8.1" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#sec-measurement-validity"><i class="fa fa-check"></i><b>21.8.1</b> Measurement Validity</a></li>
<li class="chapter" data-level="21.8.2" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#sec-construct-validity"><i class="fa fa-check"></i><b>21.8.2</b> Construct Validity</a></li>
<li class="chapter" data-level="21.8.3" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#sec-criterion-validity"><i class="fa fa-check"></i><b>21.8.3</b> Criterion Validity</a></li>
<li class="chapter" data-level="21.8.4" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#sec-internal-validity"><i class="fa fa-check"></i><b>21.8.4</b> Internal Validity</a></li>
<li class="chapter" data-level="21.8.5" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#sec-external-validity"><i class="fa fa-check"></i><b>21.8.5</b> External Validity</a></li>
<li class="chapter" data-level="21.8.6" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#sec-ecological-validity"><i class="fa fa-check"></i><b>21.8.6</b> Ecological Validity</a></li>
<li class="chapter" data-level="21.8.7" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#sec-statistical-conclusion-validity"><i class="fa fa-check"></i><b>21.8.7</b> Statistical Conclusion Validity</a></li>
<li class="chapter" data-level="21.8.8" data-path="types-of-validity-in-research.html"><a href="types-of-validity-in-research.html#putting-it-all-together"><i class="fa fa-check"></i><b>21.8.8</b> Putting It All Together</a></li>
</ul></li>
<li class="chapter" data-level="21.9" data-path="types-of-subjects-in-a-treatment-setting.html"><a href="types-of-subjects-in-a-treatment-setting.html"><i class="fa fa-check"></i><b>21.9</b> Types of Subjects in a Treatment Setting</a>
<ul>
<li class="chapter" data-level="21.9.1" data-path="types-of-subjects-in-a-treatment-setting.html"><a href="types-of-subjects-in-a-treatment-setting.html#non-switchers"><i class="fa fa-check"></i><b>21.9.1</b> Non-Switchers</a></li>
<li class="chapter" data-level="21.9.2" data-path="types-of-subjects-in-a-treatment-setting.html"><a href="types-of-subjects-in-a-treatment-setting.html#switchers"><i class="fa fa-check"></i><b>21.9.2</b> Switchers</a></li>
<li class="chapter" data-level="21.9.3" data-path="types-of-subjects-in-a-treatment-setting.html"><a href="types-of-subjects-in-a-treatment-setting.html#classification-of-individuals-based-on-treatment-assignment"><i class="fa fa-check"></i><b>21.9.3</b> Classification of Individuals Based on Treatment Assignment</a></li>
</ul></li>
<li class="chapter" data-level="21.10" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html"><i class="fa fa-check"></i><b>21.10</b> Types of Treatment Effects</a>
<ul>
<li class="chapter" data-level="21.10.1" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#sec-average-treatment-effect"><i class="fa fa-check"></i><b>21.10.1</b> Average Treatment Effect</a></li>
<li class="chapter" data-level="21.10.2" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#sec-conditional-average-treatment-effect-"><i class="fa fa-check"></i><b>21.10.2</b> Conditional Average Treatment Effect</a></li>
<li class="chapter" data-level="21.10.3" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#sec-intention-to-treat-effect"><i class="fa fa-check"></i><b>21.10.3</b> Intention-to-Treat Effect</a></li>
<li class="chapter" data-level="21.10.4" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#sec-local-average-treatment-effects"><i class="fa fa-check"></i><b>21.10.4</b> Local Average Treatment Effects</a></li>
<li class="chapter" data-level="21.10.5" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#population-vs.-sample-average-treatment-effects"><i class="fa fa-check"></i><b>21.10.5</b> Population vs. Sample Average Treatment Effects</a></li>
<li class="chapter" data-level="21.10.6" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#average-treatment-effects-on-the-treated-and-control"><i class="fa fa-check"></i><b>21.10.6</b> Average Treatment Effects on the Treated and Control</a></li>
<li class="chapter" data-level="21.10.7" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#sec-quantile-average-treatment-effects"><i class="fa fa-check"></i><b>21.10.7</b> Quantile Average Treatment Effects</a></li>
<li class="chapter" data-level="21.10.8" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#sec-log-odds-treatment-effects-for-binary-outcomes"><i class="fa fa-check"></i><b>21.10.8</b> Log-Odds Treatment Effects for Binary Outcomes</a></li>
<li class="chapter" data-level="21.10.9" data-path="types-of-treatment-effects.html"><a href="types-of-treatment-effects.html#summary-table-treatment-effect-estimands"><i class="fa fa-check"></i><b>21.10.9</b> Summary Table: Treatment Effect Estimands</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>A. EXPERIMENTAL DESIGN</b></span></li>
<li class="chapter" data-level="22" data-path="sec-experimental-design.html"><a href="sec-experimental-design.html"><i class="fa fa-check"></i><b>22</b> Experimental Design</a>
<ul>
<li class="chapter" data-level="22.1" data-path="principles-of-experimental-design.html"><a href="principles-of-experimental-design.html"><i class="fa fa-check"></i><b>22.1</b> Principles of Experimental Design</a></li>
<li class="chapter" data-level="22.2" data-path="sec-the-gold-standard-randomized-controlled-trials.html"><a href="sec-the-gold-standard-randomized-controlled-trials.html"><i class="fa fa-check"></i><b>22.2</b> The Gold Standard: Randomized Controlled Trials</a></li>
<li class="chapter" data-level="22.3" data-path="selection-problem.html"><a href="selection-problem.html"><i class="fa fa-check"></i><b>22.3</b> Selection Problem</a>
<ul>
<li class="chapter" data-level="22.3.1" data-path="selection-problem.html"><a href="selection-problem.html#the-observed-difference-in-outcomes"><i class="fa fa-check"></i><b>22.3.1</b> The Observed Difference in Outcomes</a></li>
<li class="chapter" data-level="22.3.2" data-path="selection-problem.html"><a href="selection-problem.html#eliminating-selection-bias-with-random-assignment"><i class="fa fa-check"></i><b>22.3.2</b> Eliminating Selection Bias with Random Assignment</a></li>
<li class="chapter" data-level="22.3.3" data-path="selection-problem.html"><a href="selection-problem.html#another-representation-under-regression"><i class="fa fa-check"></i><b>22.3.3</b> Another Representation Under Regression</a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="classical-experimental-designs.html"><a href="classical-experimental-designs.html"><i class="fa fa-check"></i><b>22.4</b> Classical Experimental Designs</a>
<ul>
<li class="chapter" data-level="22.4.1" data-path="classical-experimental-designs.html"><a href="classical-experimental-designs.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.4.1</b> Completely Randomized Design</a></li>
<li class="chapter" data-level="22.4.2" data-path="classical-experimental-designs.html"><a href="classical-experimental-designs.html#randomized-block-design"><i class="fa fa-check"></i><b>22.4.2</b> Randomized Block Design</a></li>
<li class="chapter" data-level="22.4.3" data-path="classical-experimental-designs.html"><a href="classical-experimental-designs.html#factorial-design"><i class="fa fa-check"></i><b>22.4.3</b> Factorial Design</a></li>
<li class="chapter" data-level="22.4.4" data-path="classical-experimental-designs.html"><a href="classical-experimental-designs.html#crossover-design"><i class="fa fa-check"></i><b>22.4.4</b> Crossover Design</a></li>
<li class="chapter" data-level="22.4.5" data-path="classical-experimental-designs.html"><a href="classical-experimental-designs.html#split-plot-design"><i class="fa fa-check"></i><b>22.4.5</b> Split-Plot Design</a></li>
<li class="chapter" data-level="22.4.6" data-path="classical-experimental-designs.html"><a href="classical-experimental-designs.html#latin-square-design"><i class="fa fa-check"></i><b>22.4.6</b> Latin Square Design</a></li>
</ul></li>
<li class="chapter" data-level="22.5" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html"><i class="fa fa-check"></i><b>22.5</b> Advanced Experimental Designs</a>
<ul>
<li class="chapter" data-level="22.5.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#semi-random-experiments"><i class="fa fa-check"></i><b>22.5.1</b> Semi-Random Experiments</a></li>
<li class="chapter" data-level="22.5.2" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#re-randomization"><i class="fa fa-check"></i><b>22.5.2</b> Re-Randomization</a></li>
<li class="chapter" data-level="22.5.3" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#two-stage-randomized-experiments"><i class="fa fa-check"></i><b>22.5.3</b> Two-Stage Randomized Experiments</a></li>
<li class="chapter" data-level="22.5.4" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#two-stage-randomized-experiments-with-interference-and-noncompliance"><i class="fa fa-check"></i><b>22.5.4</b> Two-Stage Randomized Experiments with Interference and Noncompliance</a></li>
</ul></li>
<li class="chapter" data-level="22.6" data-path="emerging-research.html"><a href="emerging-research.html"><i class="fa fa-check"></i><b>22.6</b> Emerging Research</a>
<ul>
<li class="chapter" data-level="22.6.1" data-path="emerging-research.html"><a href="emerging-research.html#covariate-balancing-in-online-ab-testing-the-pigeonhole-design"><i class="fa fa-check"></i><b>22.6.1</b> Covariate Balancing in Online A/B Testing: The Pigeonhole Design</a></li>
<li class="chapter" data-level="22.6.2" data-path="emerging-research.html"><a href="emerging-research.html#sec-handling-zero-valued-outcomes"><i class="fa fa-check"></i><b>22.6.2</b> Handling Zero-Valued Outcomes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>23</b> Sampling</a>
<ul>
<li class="chapter" data-level="23.1" data-path="population-and-sample.html"><a href="population-and-sample.html"><i class="fa fa-check"></i><b>23.1</b> Population and Sample</a></li>
<li class="chapter" data-level="23.2" data-path="probability-sampling.html"><a href="probability-sampling.html"><i class="fa fa-check"></i><b>23.2</b> Probability Sampling</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="probability-sampling.html"><a href="probability-sampling.html#sec-simple-random-sampling"><i class="fa fa-check"></i><b>23.2.1</b> Simple Random Sampling</a></li>
<li class="chapter" data-level="23.2.2" data-path="probability-sampling.html"><a href="probability-sampling.html#sec-stratified-sampling"><i class="fa fa-check"></i><b>23.2.2</b> Stratified Sampling</a></li>
<li class="chapter" data-level="23.2.3" data-path="probability-sampling.html"><a href="probability-sampling.html#systematic-sampling"><i class="fa fa-check"></i><b>23.2.3</b> Systematic Sampling</a></li>
<li class="chapter" data-level="23.2.4" data-path="probability-sampling.html"><a href="probability-sampling.html#cluster-sampling"><i class="fa fa-check"></i><b>23.2.4</b> Cluster Sampling</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="non-probability-sampling.html"><a href="non-probability-sampling.html"><i class="fa fa-check"></i><b>23.3</b> Non-Probability Sampling</a>
<ul>
<li class="chapter" data-level="23.3.1" data-path="non-probability-sampling.html"><a href="non-probability-sampling.html#convenience-sampling"><i class="fa fa-check"></i><b>23.3.1</b> Convenience Sampling</a></li>
<li class="chapter" data-level="23.3.2" data-path="non-probability-sampling.html"><a href="non-probability-sampling.html#quota-sampling"><i class="fa fa-check"></i><b>23.3.2</b> Quota Sampling</a></li>
<li class="chapter" data-level="23.3.3" data-path="non-probability-sampling.html"><a href="non-probability-sampling.html#snowball-sampling"><i class="fa fa-check"></i><b>23.3.3</b> Snowball Sampling</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="sec-unequal-probability-sampling.html"><a href="sec-unequal-probability-sampling.html"><i class="fa fa-check"></i><b>23.4</b> Unequal Probability Sampling</a></li>
<li class="chapter" data-level="23.5" data-path="sec-balanced-sampling.html"><a href="sec-balanced-sampling.html"><i class="fa fa-check"></i><b>23.5</b> Balanced Sampling</a>
<ul>
<li class="chapter" data-level="23.5.1" data-path="sec-balanced-sampling.html"><a href="sec-balanced-sampling.html#cube-method-for-balanced-sampling"><i class="fa fa-check"></i><b>23.5.1</b> Cube Method for Balanced Sampling</a></li>
<li class="chapter" data-level="23.5.2" data-path="sec-balanced-sampling.html"><a href="sec-balanced-sampling.html#balanced-sampling-with-stratification"><i class="fa fa-check"></i><b>23.5.2</b> Balanced Sampling with Stratification</a></li>
<li class="chapter" data-level="23.5.3" data-path="sec-balanced-sampling.html"><a href="sec-balanced-sampling.html#balanced-sampling-in-cluster-sampling"><i class="fa fa-check"></i><b>23.5.3</b> Balanced Sampling in Cluster Sampling</a></li>
<li class="chapter" data-level="23.5.4" data-path="sec-balanced-sampling.html"><a href="sec-balanced-sampling.html#balanced-sampling-in-two-stage-sampling"><i class="fa fa-check"></i><b>23.5.4</b> Balanced Sampling in Two-Stage Sampling</a></li>
</ul></li>
<li class="chapter" data-level="23.6" data-path="sample-size-determination.html"><a href="sample-size-determination.html"><i class="fa fa-check"></i><b>23.6</b> Sample Size Determination</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="sec-analysis-of-variance-anova.html"><a href="sec-analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>24</b> Analysis of Variance</a>
<ul>
<li class="chapter" data-level="24.1" data-path="sec-completely-randomized-design.html"><a href="sec-completely-randomized-design.html"><i class="fa fa-check"></i><b>24.1</b> Completely Randomized Design</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="sec-completely-randomized-design.html"><a href="sec-completely-randomized-design.html#sec-single-factor-fixed-effects-model"><i class="fa fa-check"></i><b>24.1.1</b> Single-Factor Fixed Effects ANOVA</a></li>
<li class="chapter" data-level="24.1.2" data-path="sec-completely-randomized-design.html"><a href="sec-completely-randomized-design.html#sec-single-factor-random-effects-model"><i class="fa fa-check"></i><b>24.1.2</b> Single Factor Random Effects ANOVA</a></li>
<li class="chapter" data-level="24.1.3" data-path="sec-completely-randomized-design.html"><a href="sec-completely-randomized-design.html#sec-two-factor-fixed-effects-anova"><i class="fa fa-check"></i><b>24.1.3</b> Two-Factor Fixed Effects ANOVA</a></li>
<li class="chapter" data-level="24.1.4" data-path="sec-completely-randomized-design.html"><a href="sec-completely-randomized-design.html#sec-two-way-random-effects-anova"><i class="fa fa-check"></i><b>24.1.4</b> Two-Way Random Effects ANOVA</a></li>
<li class="chapter" data-level="24.1.5" data-path="sec-completely-randomized-design.html"><a href="sec-completely-randomized-design.html#sec-two-way-mixed-effects-anova"><i class="fa fa-check"></i><b>24.1.5</b> Two-Way Mixed Effects ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="sec-nonparametric-anova.html"><a href="sec-nonparametric-anova.html"><i class="fa fa-check"></i><b>24.2</b> Nonparametric ANOVA</a>
<ul>
<li class="chapter" data-level="24.2.1" data-path="sec-nonparametric-anova.html"><a href="sec-nonparametric-anova.html#kruskal-wallis-test-one-way-nonparametric-anova"><i class="fa fa-check"></i><b>24.2.1</b> Kruskal-Wallis Test (One-Way Nonparametric ANOVA)</a></li>
<li class="chapter" data-level="24.2.2" data-path="sec-nonparametric-anova.html"><a href="sec-nonparametric-anova.html#friedman-test-nonparametric-two-way-anova"><i class="fa fa-check"></i><b>24.2.2</b> Friedman Test (Nonparametric Two-Way ANOVA)</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="sec-randomized-block-designs.html"><a href="sec-randomized-block-designs.html"><i class="fa fa-check"></i><b>24.3</b> Randomized Block Designs</a></li>
<li class="chapter" data-level="24.4" data-path="nested-designs.html"><a href="nested-designs.html"><i class="fa fa-check"></i><b>24.4</b> Nested Designs</a>
<ul>
<li class="chapter" data-level="24.4.1" data-path="nested-designs.html"><a href="nested-designs.html#two-factor-nested-design"><i class="fa fa-check"></i><b>24.4.1</b> Two-Factor Nested Design</a></li>
<li class="chapter" data-level="24.4.2" data-path="nested-designs.html"><a href="nested-designs.html#unbalanced-nested-two-factor-designs"><i class="fa fa-check"></i><b>24.4.2</b> Unbalanced Nested Two-Factor Designs</a></li>
<li class="chapter" data-level="24.4.3" data-path="nested-designs.html"><a href="nested-designs.html#random-factor-effects"><i class="fa fa-check"></i><b>24.4.3</b> Random Factor Effects</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html"><i class="fa fa-check"></i><b>24.5</b> Sample Size Planning for ANOVA</a>
<ul>
<li class="chapter" data-level="24.5.1" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#balanced-designs"><i class="fa fa-check"></i><b>24.5.1</b> Balanced Designs</a></li>
<li class="chapter" data-level="24.5.2" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#single-factor-studies"><i class="fa fa-check"></i><b>24.5.2</b> Single Factor Studies</a></li>
<li class="chapter" data-level="24.5.3" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#multi-factor-studies"><i class="fa fa-check"></i><b>24.5.3</b> Multi-Factor Studies</a></li>
<li class="chapter" data-level="24.5.4" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#procedure-for-sample-size-selection"><i class="fa fa-check"></i><b>24.5.4</b> Procedure for Sample Size Selection</a></li>
<li class="chapter" data-level="24.5.5" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#randomized-block-experiments"><i class="fa fa-check"></i><b>24.5.5</b> Randomized Block Experiments</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="single-factor-covariance-model.html"><a href="single-factor-covariance-model.html"><i class="fa fa-check"></i><b>24.6</b> Single Factor Covariance Model</a>
<ul>
<li class="chapter" data-level="24.6.1" data-path="single-factor-covariance-model.html"><a href="single-factor-covariance-model.html#statistical-inference-for-treatment-effects"><i class="fa fa-check"></i><b>24.6.1</b> Statistical Inference for Treatment Effects</a></li>
<li class="chapter" data-level="24.6.2" data-path="single-factor-covariance-model.html"><a href="single-factor-covariance-model.html#testing-for-parallel-slopes"><i class="fa fa-check"></i><b>24.6.2</b> Testing for Parallel Slopes</a></li>
<li class="chapter" data-level="24.6.3" data-path="single-factor-covariance-model.html"><a href="single-factor-covariance-model.html#adjusted-means"><i class="fa fa-check"></i><b>24.6.3</b> Adjusted Means</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="sec-multivariate-methods.html"><a href="sec-multivariate-methods.html"><i class="fa fa-check"></i><b>25</b> Multivariate Methods</a>
<ul>
<li class="chapter" data-level="25.1" data-path="basic-understanding.html"><a href="basic-understanding.html"><i class="fa fa-check"></i><b>25.1</b> Basic Understanding</a>
<ul>
<li class="chapter" data-level="25.1.1" data-path="basic-understanding.html"><a href="basic-understanding.html#multivariate-random-vectors"><i class="fa fa-check"></i><b>25.1.1</b> Multivariate Random Vectors</a></li>
<li class="chapter" data-level="25.1.2" data-path="basic-understanding.html"><a href="basic-understanding.html#sec-covariance-matrix-multivariate"><i class="fa fa-check"></i><b>25.1.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="25.1.3" data-path="basic-understanding.html"><a href="basic-understanding.html#equalities-in-expectation-and-variance"><i class="fa fa-check"></i><b>25.1.3</b> Equalities in Expectation and Variance</a></li>
<li class="chapter" data-level="25.1.4" data-path="basic-understanding.html"><a href="basic-understanding.html#multivariate-normal-distribution-1"><i class="fa fa-check"></i><b>25.1.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="25.1.5" data-path="basic-understanding.html"><a href="basic-understanding.html#test-of-multivariate-normality"><i class="fa fa-check"></i><b>25.1.5</b> Test of Multivariate Normality</a></li>
<li class="chapter" data-level="25.1.6" data-path="basic-understanding.html"><a href="basic-understanding.html#mean-vector-inference"><i class="fa fa-check"></i><b>25.1.6</b> Mean Vector Inference</a></li>
<li class="chapter" data-level="25.1.7" data-path="basic-understanding.html"><a href="basic-understanding.html#general-hypothesis-testing"><i class="fa fa-check"></i><b>25.1.7</b> General Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="sec-multivariate-analysis-of-variance.html"><a href="sec-multivariate-analysis-of-variance.html"><i class="fa fa-check"></i><b>25.2</b> Multivariate Analysis of Variance</a>
<ul>
<li class="chapter" data-level="25.2.1" data-path="sec-multivariate-analysis-of-variance.html"><a href="sec-multivariate-analysis-of-variance.html#one-way-manova"><i class="fa fa-check"></i><b>25.2.1</b> One-Way MANOVA</a></li>
<li class="chapter" data-level="25.2.2" data-path="sec-multivariate-analysis-of-variance.html"><a href="sec-multivariate-analysis-of-variance.html#sec-profile-analysis"><i class="fa fa-check"></i><b>25.2.2</b> Profile Analysis</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="statistical-test-selection-for-comparing-means.html"><a href="statistical-test-selection-for-comparing-means.html"><i class="fa fa-check"></i><b>25.3</b> Statistical Test Selection for Comparing Means</a></li>
</ul></li>
<li class="part"><span><b>B. QUASI-EXPERIMENTAL DESIGN</b></span></li>
<li class="chapter" data-level="26" data-path="sec-quasi-experimental.html"><a href="sec-quasi-experimental.html"><i class="fa fa-check"></i><b>26</b> Quasi-Experimental Methods</a>
<ul>
<li class="chapter" data-level="26.1" data-path="identification-strategy-in-quasi-experiments.html"><a href="identification-strategy-in-quasi-experiments.html"><i class="fa fa-check"></i><b>26.1</b> Identification Strategy in Quasi-Experiments</a></li>
<li class="chapter" data-level="26.2" data-path="robustness-checks.html"><a href="robustness-checks.html"><i class="fa fa-check"></i><b>26.2</b> Robustness Checks</a></li>
<li class="chapter" data-level="26.3" data-path="establishing-mechanisms.html"><a href="establishing-mechanisms.html"><i class="fa fa-check"></i><b>26.3</b> Establishing Mechanisms</a></li>
<li class="chapter" data-level="26.4" data-path="limitations-of-quasi-experiments.html"><a href="limitations-of-quasi-experiments.html"><i class="fa fa-check"></i><b>26.4</b> Limitations of Quasi-Experiments</a></li>
<li class="chapter" data-level="26.5" data-path="assumptions-for-identifying-treatment-effects.html"><a href="assumptions-for-identifying-treatment-effects.html"><i class="fa fa-check"></i><b>26.5</b> Assumptions for Identifying Treatment Effects</a>
<ul>
<li class="chapter" data-level="26.5.1" data-path="assumptions-for-identifying-treatment-effects.html"><a href="assumptions-for-identifying-treatment-effects.html#sec-sutva"><i class="fa fa-check"></i><b>26.5.1</b> Stable Unit Treatment Value Assumption</a></li>
<li class="chapter" data-level="26.5.2" data-path="assumptions-for-identifying-treatment-effects.html"><a href="assumptions-for-identifying-treatment-effects.html#sec-conditional-ignorability-assumption"><i class="fa fa-check"></i><b>26.5.2</b> Conditional Ignorability Assumption</a></li>
<li class="chapter" data-level="26.5.3" data-path="assumptions-for-identifying-treatment-effects.html"><a href="assumptions-for-identifying-treatment-effects.html#sec-overlap-positivity-assumption"><i class="fa fa-check"></i><b>26.5.3</b> Overlap (Positivity) Assumption</a></li>
</ul></li>
<li class="chapter" data-level="26.6" data-path="sec-natural-experiments.html"><a href="sec-natural-experiments.html"><i class="fa fa-check"></i><b>26.6</b> Natural Experiments</a>
<ul>
<li class="chapter" data-level="26.6.1" data-path="sec-natural-experiments.html"><a href="sec-natural-experiments.html#the-problem-of-reusing-natural-experiments"><i class="fa fa-check"></i><b>26.6.1</b> The Problem of Reusing Natural Experiments</a></li>
<li class="chapter" data-level="26.6.2" data-path="sec-natural-experiments.html"><a href="sec-natural-experiments.html#statistical-challenges-in-reusing-natural-experiments"><i class="fa fa-check"></i><b>26.6.2</b> Statistical Challenges in Reusing Natural Experiments</a></li>
<li class="chapter" data-level="26.6.3" data-path="sec-natural-experiments.html"><a href="sec-natural-experiments.html#solutions-multiple-testing-corrections"><i class="fa fa-check"></i><b>26.6.3</b> Solutions: Multiple Testing Corrections</a></li>
</ul></li>
<li class="chapter" data-level="26.7" data-path="design-vs.-model-based-approaches.html"><a href="design-vs.-model-based-approaches.html"><i class="fa fa-check"></i><b>26.7</b> Design vs. Model-Based Approaches</a>
<ul>
<li class="chapter" data-level="26.7.1" data-path="design-vs.-model-based-approaches.html"><a href="design-vs.-model-based-approaches.html#sec-design-based"><i class="fa fa-check"></i><b>26.7.1</b> Design-Based Perspective</a></li>
<li class="chapter" data-level="26.7.2" data-path="design-vs.-model-based-approaches.html"><a href="design-vs.-model-based-approaches.html#sec-model-based-perspective"><i class="fa fa-check"></i><b>26.7.2</b> Model-Based Perspective</a></li>
<li class="chapter" data-level="26.7.3" data-path="design-vs.-model-based-approaches.html"><a href="design-vs.-model-based-approaches.html#placing-methods-along-a-spectrum"><i class="fa fa-check"></i><b>26.7.3</b> Placing Methods Along a Spectrum</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="sec-regression-discontinuity.html"><a href="sec-regression-discontinuity.html"><i class="fa fa-check"></i><b>27</b> Regression Discontinuity</a>
<ul>
<li class="chapter" data-level="27.1" data-path="conceptual-framework.html"><a href="conceptual-framework.html"><i class="fa fa-check"></i><b>27.1</b> Conceptual Framework</a>
<ul>
<li class="chapter" data-level="27.1.1" data-path="conceptual-framework.html"><a href="conceptual-framework.html#types-of-regression-discontinuity-designs"><i class="fa fa-check"></i><b>27.1.1</b> Types of Regression Discontinuity Designs</a></li>
<li class="chapter" data-level="27.1.2" data-path="conceptual-framework.html"><a href="conceptual-framework.html#assumptions-for-rd-validity"><i class="fa fa-check"></i><b>27.1.2</b> Assumptions for RD Validity</a></li>
<li class="chapter" data-level="27.1.3" data-path="conceptual-framework.html"><a href="conceptual-framework.html#threats-to-rd-validity"><i class="fa fa-check"></i><b>27.1.3</b> Threats to RD Validity</a></li>
</ul></li>
<li class="chapter" data-level="27.2" data-path="model-estimation-strategies.html"><a href="model-estimation-strategies.html"><i class="fa fa-check"></i><b>27.2</b> Model Estimation Strategies</a>
<ul>
<li class="chapter" data-level="27.2.1" data-path="model-estimation-strategies.html"><a href="model-estimation-strategies.html#parametric-models-polynomial-regression"><i class="fa fa-check"></i><b>27.2.1</b> Parametric Models: Polynomial Regression</a></li>
<li class="chapter" data-level="27.2.2" data-path="model-estimation-strategies.html"><a href="model-estimation-strategies.html#nonparametric-models-local-regression"><i class="fa fa-check"></i><b>27.2.2</b> Nonparametric Models: Local Regression</a></li>
</ul></li>
<li class="chapter" data-level="27.3" data-path="formal-definition.html"><a href="formal-definition.html"><i class="fa fa-check"></i><b>27.3</b> Formal Definition</a>
<ul>
<li class="chapter" data-level="27.3.1" data-path="formal-definition.html"><a href="formal-definition.html#identification-assumptions"><i class="fa fa-check"></i><b>27.3.1</b> Identification Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="27.4" data-path="estimation-and-inference.html"><a href="estimation-and-inference.html"><i class="fa fa-check"></i><b>27.4</b> Estimation and Inference</a>
<ul>
<li class="chapter" data-level="27.4.1" data-path="estimation-and-inference.html"><a href="estimation-and-inference.html#local-randomization-based-approach"><i class="fa fa-check"></i><b>27.4.1</b> Local Randomization-Based Approach</a></li>
<li class="chapter" data-level="27.4.2" data-path="estimation-and-inference.html"><a href="estimation-and-inference.html#sec-continuity-based-approach"><i class="fa fa-check"></i><b>27.4.2</b> Continuity-Based Approach</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="specification-checks.html"><a href="specification-checks.html"><i class="fa fa-check"></i><b>27.5</b> Specification Checks</a>
<ul>
<li class="chapter" data-level="27.5.1" data-path="specification-checks.html"><a href="specification-checks.html#sec-balance-checks"><i class="fa fa-check"></i><b>27.5.1</b> Balance Checks</a></li>
<li class="chapter" data-level="27.5.2" data-path="specification-checks.html"><a href="specification-checks.html#sec-sorting-bunching-and-manipulation"><i class="fa fa-check"></i><b>27.5.2</b> Sorting, Bunching, and Manipulation</a></li>
<li class="chapter" data-level="27.5.3" data-path="specification-checks.html"><a href="specification-checks.html#placebo-tests"><i class="fa fa-check"></i><b>27.5.3</b> Placebo Tests</a></li>
<li class="chapter" data-level="27.5.4" data-path="specification-checks.html"><a href="specification-checks.html#sensitivity-to-bandwidth-choice"><i class="fa fa-check"></i><b>27.5.4</b> Sensitivity to Bandwidth Choice</a></li>
<li class="chapter" data-level="27.5.5" data-path="specification-checks.html"><a href="specification-checks.html#assessing-sensitivity"><i class="fa fa-check"></i><b>27.5.5</b> Assessing Sensitivity</a></li>
<li class="chapter" data-level="27.5.6" data-path="specification-checks.html"><a href="specification-checks.html#manipulation-robust-regression-discontinuity-bounds"><i class="fa fa-check"></i><b>27.5.6</b> Manipulation-Robust Regression Discontinuity Bounds</a></li>
</ul></li>
<li class="chapter" data-level="27.6" data-path="sec-fuzzy-regression-discontinuity-design.html"><a href="sec-fuzzy-regression-discontinuity-design.html"><i class="fa fa-check"></i><b>27.6</b> Fuzzy Regression Discontinuity Design</a>
<ul>
<li class="chapter" data-level="27.6.1" data-path="sec-fuzzy-regression-discontinuity-design.html"><a href="sec-fuzzy-regression-discontinuity-design.html#compliance-types"><i class="fa fa-check"></i><b>27.6.1</b> Compliance Types</a></li>
<li class="chapter" data-level="27.6.2" data-path="sec-fuzzy-regression-discontinuity-design.html"><a href="sec-fuzzy-regression-discontinuity-design.html#estimating-the-local-average-treatment-effect"><i class="fa fa-check"></i><b>27.6.2</b> Estimating the Local Average Treatment Effect</a></li>
<li class="chapter" data-level="27.6.3" data-path="sec-fuzzy-regression-discontinuity-design.html"><a href="sec-fuzzy-regression-discontinuity-design.html#equivalent-representation-using-expectations"><i class="fa fa-check"></i><b>27.6.3</b> Equivalent Representation Using Expectations</a></li>
<li class="chapter" data-level="27.6.4" data-path="sec-fuzzy-regression-discontinuity-design.html"><a href="sec-fuzzy-regression-discontinuity-design.html#estimation-strategies"><i class="fa fa-check"></i><b>27.6.4</b> Estimation Strategies</a></li>
<li class="chapter" data-level="27.6.5" data-path="sec-fuzzy-regression-discontinuity-design.html"><a href="sec-fuzzy-regression-discontinuity-design.html#practical-considerations-6"><i class="fa fa-check"></i><b>27.6.5</b> Practical Considerations</a></li>
<li class="chapter" data-level="27.6.6" data-path="sec-fuzzy-regression-discontinuity-design.html"><a href="sec-fuzzy-regression-discontinuity-design.html#steps-for-fuzzy-rd"><i class="fa fa-check"></i><b>27.6.6</b> Steps for Fuzzy RD</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="sec-sharp-regression-discontinuity-design.html"><a href="sec-sharp-regression-discontinuity-design.html"><i class="fa fa-check"></i><b>27.7</b> Sharp Regression Discontinuity Design</a>
<ul>
<li class="chapter" data-level="27.7.1" data-path="sec-sharp-regression-discontinuity-design.html"><a href="sec-sharp-regression-discontinuity-design.html#assumptions-for-identification"><i class="fa fa-check"></i><b>27.7.1</b> Assumptions for Identification</a></li>
<li class="chapter" data-level="27.7.2" data-path="sec-sharp-regression-discontinuity-design.html"><a href="sec-sharp-regression-discontinuity-design.html#estimating-the-local-average-treatment-effect-1"><i class="fa fa-check"></i><b>27.7.2</b> Estimating the Local Average Treatment Effect</a></li>
<li class="chapter" data-level="27.7.3" data-path="sec-sharp-regression-discontinuity-design.html"><a href="sec-sharp-regression-discontinuity-design.html#estimation-methods"><i class="fa fa-check"></i><b>27.7.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="27.7.4" data-path="sec-sharp-regression-discontinuity-design.html"><a href="sec-sharp-regression-discontinuity-design.html#steps-for-sharp-rd"><i class="fa fa-check"></i><b>27.7.4</b> Steps for Sharp RD</a></li>
</ul></li>
<li class="chapter" data-level="27.8" data-path="sec-regression-kink-design.html"><a href="sec-regression-kink-design.html"><i class="fa fa-check"></i><b>27.8</b> Regression Kink Design</a>
<ul>
<li class="chapter" data-level="27.8.1" data-path="sec-regression-kink-design.html"><a href="sec-regression-kink-design.html#sec-identification-in-sharp-regression-kink-design"><i class="fa fa-check"></i><b>27.8.1</b> Identification in Sharp Regression Kink Design</a></li>
<li class="chapter" data-level="27.8.2" data-path="sec-regression-kink-design.html"><a href="sec-regression-kink-design.html#sec-identification-in-fuzzy-regression-kink-design"><i class="fa fa-check"></i><b>27.8.2</b> Identification in Fuzzy Regression Kink Design</a></li>
<li class="chapter" data-level="27.8.3" data-path="sec-regression-kink-design.html"><a href="sec-regression-kink-design.html#estimation-of-rkd-effects"><i class="fa fa-check"></i><b>27.8.3</b> Estimation of RKD Effects</a></li>
<li class="chapter" data-level="27.8.4" data-path="sec-regression-kink-design.html"><a href="sec-regression-kink-design.html#robustness-checks-1"><i class="fa fa-check"></i><b>27.8.4</b> Robustness Checks</a></li>
</ul></li>
<li class="chapter" data-level="27.9" data-path="sec-multi-cutoff-regression-discontinuity-design.html"><a href="sec-multi-cutoff-regression-discontinuity-design.html"><i class="fa fa-check"></i><b>27.9</b> Multi-Cutoff Regression Discontinuity Design</a>
<ul>
<li class="chapter" data-level="27.9.1" data-path="sec-multi-cutoff-regression-discontinuity-design.html"><a href="sec-multi-cutoff-regression-discontinuity-design.html#identification"><i class="fa fa-check"></i><b>27.9.1</b> Identification</a></li>
<li class="chapter" data-level="27.9.2" data-path="sec-multi-cutoff-regression-discontinuity-design.html"><a href="sec-multi-cutoff-regression-discontinuity-design.html#key-assumptions-1"><i class="fa fa-check"></i><b>27.9.2</b> Key Assumptions</a></li>
<li class="chapter" data-level="27.9.3" data-path="sec-multi-cutoff-regression-discontinuity-design.html"><a href="sec-multi-cutoff-regression-discontinuity-design.html#estimation-approaches"><i class="fa fa-check"></i><b>27.9.3</b> Estimation Approaches</a></li>
<li class="chapter" data-level="27.9.4" data-path="sec-multi-cutoff-regression-discontinuity-design.html"><a href="sec-multi-cutoff-regression-discontinuity-design.html#robustness-checks-2"><i class="fa fa-check"></i><b>27.9.4</b> Robustness Checks</a></li>
</ul></li>
<li class="chapter" data-level="27.10" data-path="sec-multi-score-regression-discontinuity-design.html"><a href="sec-multi-score-regression-discontinuity-design.html"><i class="fa fa-check"></i><b>27.10</b> Multi-Score Regression Discontinuity Design</a>
<ul>
<li class="chapter" data-level="27.10.1" data-path="sec-multi-score-regression-discontinuity-design.html"><a href="sec-multi-score-regression-discontinuity-design.html#general-framework"><i class="fa fa-check"></i><b>27.10.1</b> General Framework</a></li>
<li class="chapter" data-level="27.10.2" data-path="sec-multi-score-regression-discontinuity-design.html"><a href="sec-multi-score-regression-discontinuity-design.html#identification-1"><i class="fa fa-check"></i><b>27.10.2</b> Identification</a></li>
<li class="chapter" data-level="27.10.3" data-path="sec-multi-score-regression-discontinuity-design.html"><a href="sec-multi-score-regression-discontinuity-design.html#key-assumptions-2"><i class="fa fa-check"></i><b>27.10.3</b> Key Assumptions</a></li>
<li class="chapter" data-level="27.10.4" data-path="sec-multi-score-regression-discontinuity-design.html"><a href="sec-multi-score-regression-discontinuity-design.html#estimation-approaches-1"><i class="fa fa-check"></i><b>27.10.4</b> Estimation Approaches</a></li>
<li class="chapter" data-level="27.10.5" data-path="sec-multi-score-regression-discontinuity-design.html"><a href="sec-multi-score-regression-discontinuity-design.html#robustness-checks-3"><i class="fa fa-check"></i><b>27.10.5</b> Robustness Checks</a></li>
</ul></li>
<li class="chapter" data-level="27.11" data-path="evaluation-of-a-regression-discontinuity-design.html"><a href="evaluation-of-a-regression-discontinuity-design.html"><i class="fa fa-check"></i><b>27.11</b> Evaluation of a Regression Discontinuity Design</a>
<ul>
<li class="chapter" data-level="27.11.1" data-path="evaluation-of-a-regression-discontinuity-design.html"><a href="evaluation-of-a-regression-discontinuity-design.html#graphical-and-formal-evidence"><i class="fa fa-check"></i><b>27.11.1</b> Graphical and Formal Evidence</a></li>
<li class="chapter" data-level="27.11.2" data-path="evaluation-of-a-regression-discontinuity-design.html"><a href="evaluation-of-a-regression-discontinuity-design.html#functional-form-of-the-running-variable"><i class="fa fa-check"></i><b>27.11.2</b> Functional Form of the Running Variable</a></li>
<li class="chapter" data-level="27.11.3" data-path="evaluation-of-a-regression-discontinuity-design.html"><a href="evaluation-of-a-regression-discontinuity-design.html#bandwidth-selection-2"><i class="fa fa-check"></i><b>27.11.3</b> Bandwidth Selection</a></li>
<li class="chapter" data-level="27.11.4" data-path="evaluation-of-a-regression-discontinuity-design.html"><a href="evaluation-of-a-regression-discontinuity-design.html#addressing-potential-confounders"><i class="fa fa-check"></i><b>27.11.4</b> Addressing Potential Confounders</a></li>
<li class="chapter" data-level="27.11.5" data-path="evaluation-of-a-regression-discontinuity-design.html"><a href="evaluation-of-a-regression-discontinuity-design.html#external-validity-in-rd"><i class="fa fa-check"></i><b>27.11.5</b> External Validity in RD</a></li>
</ul></li>
<li class="chapter" data-level="27.12" data-path="applications-of-rd-designs.html"><a href="applications-of-rd-designs.html"><i class="fa fa-check"></i><b>27.12</b> Applications of RD Designs</a>
<ul>
<li class="chapter" data-level="27.12.1" data-path="applications-of-rd-designs.html"><a href="applications-of-rd-designs.html#applications-in-marketing"><i class="fa fa-check"></i><b>27.12.1</b> Applications in Marketing</a></li>
<li class="chapter" data-level="27.12.2" data-path="applications-of-rd-designs.html"><a href="applications-of-rd-designs.html#r-packages-for-rd-estimation"><i class="fa fa-check"></i><b>27.12.2</b> R Packages for RD Estimation</a></li>
<li class="chapter" data-level="27.12.3" data-path="applications-of-rd-designs.html"><a href="applications-of-rd-designs.html#example-of-regression-discontinuity-in-education"><i class="fa fa-check"></i><b>27.12.3</b> Example of Regression Discontinuity in Education</a></li>
<li class="chapter" data-level="27.12.4" data-path="applications-of-rd-designs.html"><a href="applications-of-rd-designs.html#example-of-occupational-licensing-and-market-efficiency"><i class="fa fa-check"></i><b>27.12.4</b> Example of Occupational Licensing and Market Efficiency</a></li>
<li class="chapter" data-level="27.12.5" data-path="applications-of-rd-designs.html"><a href="applications-of-rd-designs.html#replicating-carpenter2009effect"><i class="fa fa-check"></i><b>27.12.5</b> Replicating <span class="citation">(Carpenter and Dobkin 2009)</span></a></li>
<li class="chapter" data-level="27.12.6" data-path="applications-of-rd-designs.html"><a href="applications-of-rd-designs.html#additional-rd-applications"><i class="fa fa-check"></i><b>27.12.6</b> Additional RD Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="temporal-discontinuity-designs.html"><a href="temporal-discontinuity-designs.html"><i class="fa fa-check"></i><b>28</b> Temporal Discontinuity Designs</a>
<ul>
<li class="chapter" data-level="28.1" data-path="sec-regression-discontinuity-in-time.html"><a href="sec-regression-discontinuity-in-time.html"><i class="fa fa-check"></i><b>28.1</b> Regression Discontinuity in Time</a>
<ul>
<li class="chapter" data-level="28.1.1" data-path="sec-regression-discontinuity-in-time.html"><a href="sec-regression-discontinuity-in-time.html#estimation-and-model-selection"><i class="fa fa-check"></i><b>28.1.1</b> Estimation and Model Selection</a></li>
<li class="chapter" data-level="28.1.2" data-path="sec-regression-discontinuity-in-time.html"><a href="sec-regression-discontinuity-in-time.html#strengths-of-rdit"><i class="fa fa-check"></i><b>28.1.2</b> Strengths of RDiT</a></li>
<li class="chapter" data-level="28.1.3" data-path="sec-regression-discontinuity-in-time.html"><a href="sec-regression-discontinuity-in-time.html#limitations-and-challenges-of-rdit"><i class="fa fa-check"></i><b>28.1.3</b> Limitations and Challenges of RDiT</a></li>
<li class="chapter" data-level="28.1.4" data-path="sec-regression-discontinuity-in-time.html"><a href="sec-regression-discontinuity-in-time.html#recommendations-for-robustness-checks"><i class="fa fa-check"></i><b>28.1.4</b> Recommendations for Robustness Checks</a></li>
<li class="chapter" data-level="28.1.5" data-path="sec-regression-discontinuity-in-time.html"><a href="sec-regression-discontinuity-in-time.html#applications-of-rdit"><i class="fa fa-check"></i><b>28.1.5</b> Applications of RDiT</a></li>
<li class="chapter" data-level="28.1.6" data-path="sec-regression-discontinuity-in-time.html"><a href="sec-regression-discontinuity-in-time.html#empirical-example"><i class="fa fa-check"></i><b>28.1.6</b> Empirical Example</a></li>
</ul></li>
<li class="chapter" data-level="28.2" data-path="sec-interrupted-time-series.html"><a href="sec-interrupted-time-series.html"><i class="fa fa-check"></i><b>28.2</b> Interrupted Time Series</a>
<ul>
<li class="chapter" data-level="28.2.1" data-path="sec-interrupted-time-series.html"><a href="sec-interrupted-time-series.html#advantages-of-its"><i class="fa fa-check"></i><b>28.2.1</b> Advantages of ITS</a></li>
<li class="chapter" data-level="28.2.2" data-path="sec-interrupted-time-series.html"><a href="sec-interrupted-time-series.html#limitations-of-its"><i class="fa fa-check"></i><b>28.2.2</b> Limitations of ITS</a></li>
<li class="chapter" data-level="28.2.3" data-path="sec-interrupted-time-series.html"><a href="sec-interrupted-time-series.html#empirical-example-1"><i class="fa fa-check"></i><b>28.2.3</b> Empirical Example</a></li>
</ul></li>
<li class="chapter" data-level="28.3" data-path="combining-both-rdit-and-its.html"><a href="combining-both-rdit-and-its.html"><i class="fa fa-check"></i><b>28.3</b> Combining both RDiT and ITS</a>
<ul>
<li class="chapter" data-level="28.3.1" data-path="combining-both-rdit-and-its.html"><a href="combining-both-rdit-and-its.html#augment-an-its-model-with-a-local-discontinuity-term"><i class="fa fa-check"></i><b>28.3.1</b> Augment an ITS Model with a Local Discontinuity Term</a></li>
<li class="chapter" data-level="28.3.2" data-path="combining-both-rdit-and-its.html"><a href="combining-both-rdit-and-its.html#two-stage-or-multi-stage-modeling"><i class="fa fa-check"></i><b>28.3.2</b> Two-Stage (or Multi-Stage) Modeling</a></li>
<li class="chapter" data-level="28.3.3" data-path="combining-both-rdit-and-its.html"><a href="combining-both-rdit-and-its.html#hierarchical-or-multi-level-modeling"><i class="fa fa-check"></i><b>28.3.3</b> Hierarchical or Multi-Level Modeling</a></li>
<li class="chapter" data-level="28.3.4" data-path="combining-both-rdit-and-its.html"><a href="combining-both-rdit-and-its.html#empirical-example-2"><i class="fa fa-check"></i><b>28.3.4</b> Empirical Example</a></li>
<li class="chapter" data-level="28.3.5" data-path="combining-both-rdit-and-its.html"><a href="combining-both-rdit-and-its.html#practical-guidance"><i class="fa fa-check"></i><b>28.3.5</b> Practical Guidance</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="case-crossover-study-design.html"><a href="case-crossover-study-design.html"><i class="fa fa-check"></i><b>28.4</b> Case-Crossover Study Design</a>
<ul>
<li class="chapter" data-level="28.4.1" data-path="case-crossover-study-design.html"><a href="case-crossover-study-design.html#mathematical-foundations"><i class="fa fa-check"></i><b>28.4.1</b> Mathematical Foundations</a></li>
<li class="chapter" data-level="28.4.2" data-path="case-crossover-study-design.html"><a href="case-crossover-study-design.html#selection-of-control-periods"><i class="fa fa-check"></i><b>28.4.2</b> Selection of Control Periods</a></li>
<li class="chapter" data-level="28.4.3" data-path="case-crossover-study-design.html"><a href="case-crossover-study-design.html#assumptions-1"><i class="fa fa-check"></i><b>28.4.3</b> Assumptions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="sec-synthetic-difference-in-differences.html"><a href="sec-synthetic-difference-in-differences.html"><i class="fa fa-check"></i><b>29</b> Synthetic Difference-in-Differences</a>
<ul>
<li class="chapter" data-level="29.1" data-path="understanding.html"><a href="understanding.html"><i class="fa fa-check"></i><b>29.1</b> Understanding</a>
<ul>
<li class="chapter" data-level="29.1.1" data-path="understanding.html"><a href="understanding.html#steps-in-sdid-estimation"><i class="fa fa-check"></i><b>29.1.1</b> Steps in SDID Estimation</a></li>
<li class="chapter" data-level="29.1.2" data-path="understanding.html"><a href="understanding.html#comparison-of-methods-1"><i class="fa fa-check"></i><b>29.1.2</b> Comparison of Methods</a></li>
<li class="chapter" data-level="29.1.3" data-path="understanding.html"><a href="understanding.html#why-use-weights"><i class="fa fa-check"></i><b>29.1.3</b> <strong>Why Use Weights?</strong></a></li>
<li class="chapter" data-level="29.1.4" data-path="understanding.html"><a href="understanding.html#benefits-of-localization-in-sdid"><i class="fa fa-check"></i><b>29.1.4</b> <strong>Benefits of Localization in SDID</strong></a></li>
<li class="chapter" data-level="29.1.5" data-path="understanding.html"><a href="understanding.html#designing-sdid-weights"><i class="fa fa-check"></i><b>29.1.5</b> <strong>Designing SDID Weights</strong></a></li>
<li class="chapter" data-level="29.1.6" data-path="understanding.html"><a href="understanding.html#how-sdid-enhances-dids-plausibility"><i class="fa fa-check"></i><b>29.1.6</b> <strong>How SDID Enhances DID’s Plausibility</strong></a></li>
<li class="chapter" data-level="29.1.7" data-path="understanding.html"><a href="understanding.html#choosing-sdid-weights"><i class="fa fa-check"></i><b>29.1.7</b> <strong>Choosing SDID Weights</strong></a></li>
<li class="chapter" data-level="29.1.8" data-path="understanding.html"><a href="understanding.html#accounting-for-time-varying-covariates-in-weight-estimation"><i class="fa fa-check"></i><b>29.1.8</b> <strong>Accounting for Time-Varying Covariates in Weight Estimation</strong></a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="application-3.html"><a href="application-3.html"><i class="fa fa-check"></i><b>29.2</b> Application</a>
<ul>
<li class="chapter" data-level="29.2.1" data-path="application-3.html"><a href="application-3.html#block-treatment"><i class="fa fa-check"></i><b>29.2.1</b> Block Treatment</a></li>
<li class="chapter" data-level="29.2.2" data-path="application-3.html"><a href="application-3.html#staggered-adoption"><i class="fa fa-check"></i><b>29.2.2</b> Staggered Adoption</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="30" data-path="sec-difference-in-differences.html"><a href="sec-difference-in-differences.html"><i class="fa fa-check"></i><b>30</b> Difference-in-Differences</a>
<ul>
<li class="chapter" data-level="30.1" data-path="empirical-studies.html"><a href="empirical-studies.html"><i class="fa fa-check"></i><b>30.1</b> Empirical Studies</a>
<ul>
<li class="chapter" data-level="30.1.1" data-path="empirical-studies.html"><a href="empirical-studies.html#applications-of-did-in-marketing"><i class="fa fa-check"></i><b>30.1.1</b> Applications of DID in Marketing</a></li>
<li class="chapter" data-level="30.1.2" data-path="empirical-studies.html"><a href="empirical-studies.html#applications-of-did-in-economics"><i class="fa fa-check"></i><b>30.1.2</b> Applications of DID in Economics</a></li>
</ul></li>
<li class="chapter" data-level="30.2" data-path="sec-visualization-did.html"><a href="sec-visualization-did.html"><i class="fa fa-check"></i><b>30.2</b> Visualization</a></li>
<li class="chapter" data-level="30.3" data-path="sec-simple-difference-in-differences.html"><a href="sec-simple-difference-in-differences.html"><i class="fa fa-check"></i><b>30.3</b> Simple Difference-in-Differences</a>
<ul>
<li class="chapter" data-level="30.3.1" data-path="sec-simple-difference-in-differences.html"><a href="sec-simple-difference-in-differences.html#basic-setup-of-did"><i class="fa fa-check"></i><b>30.3.1</b> Basic Setup of DID</a></li>
<li class="chapter" data-level="30.3.2" data-path="sec-simple-difference-in-differences.html"><a href="sec-simple-difference-in-differences.html#extensions-of-did"><i class="fa fa-check"></i><b>30.3.2</b> Extensions of DID</a></li>
<li class="chapter" data-level="30.3.3" data-path="sec-simple-difference-in-differences.html"><a href="sec-simple-difference-in-differences.html#goals-of-did"><i class="fa fa-check"></i><b>30.3.3</b> Goals of DID</a></li>
</ul></li>
<li class="chapter" data-level="30.4" data-path="empirical-research-walkthrough.html"><a href="empirical-research-walkthrough.html"><i class="fa fa-check"></i><b>30.4</b> Empirical Research Walkthrough</a>
<ul>
<li class="chapter" data-level="30.4.1" data-path="empirical-research-walkthrough.html"><a href="empirical-research-walkthrough.html#example-the-unintended-consequences-of-ban-the-box-policies"><i class="fa fa-check"></i><b>30.4.1</b> Example: The Unintended Consequences of “Ban the Box” Policies</a></li>
<li class="chapter" data-level="30.4.2" data-path="empirical-research-walkthrough.html"><a href="empirical-research-walkthrough.html#example-minimum-wage-and-employment"><i class="fa fa-check"></i><b>30.4.2</b> Example: Minimum Wage and Employment</a></li>
<li class="chapter" data-level="30.4.3" data-path="empirical-research-walkthrough.html"><a href="empirical-research-walkthrough.html#example-the-effects-of-grade-policies-on-major-choice"><i class="fa fa-check"></i><b>30.4.3</b> Example: The Effects of Grade Policies on Major Choice</a></li>
</ul></li>
<li class="chapter" data-level="30.5" data-path="one-difference.html"><a href="one-difference.html"><i class="fa fa-check"></i><b>30.5</b> One Difference</a></li>
<li class="chapter" data-level="30.6" data-path="sec-two-way-fixed-effects.html"><a href="sec-two-way-fixed-effects.html"><i class="fa fa-check"></i><b>30.6</b> Two-Way Fixed Effects</a>
<ul>
<li class="chapter" data-level="30.6.1" data-path="sec-two-way-fixed-effects.html"><a href="sec-two-way-fixed-effects.html#canonical-twfe-model"><i class="fa fa-check"></i><b>30.6.1</b> Canonical TWFE Model</a></li>
<li class="chapter" data-level="30.6.2" data-path="sec-two-way-fixed-effects.html"><a href="sec-two-way-fixed-effects.html#limitations-of-twfe"><i class="fa fa-check"></i><b>30.6.2</b> Limitations of TWFE</a></li>
<li class="chapter" data-level="30.6.3" data-path="sec-two-way-fixed-effects.html"><a href="sec-two-way-fixed-effects.html#diagnosing-and-addressing-bias-in-twfe"><i class="fa fa-check"></i><b>30.6.3</b> Diagnosing and Addressing Bias in TWFE</a></li>
<li class="chapter" data-level="30.6.4" data-path="sec-two-way-fixed-effects.html"><a href="sec-two-way-fixed-effects.html#remedies-for-twfes-shortcomings"><i class="fa fa-check"></i><b>30.6.4</b> Remedies for TWFE’s Shortcomings</a></li>
<li class="chapter" data-level="30.6.5" data-path="sec-two-way-fixed-effects.html"><a href="sec-two-way-fixed-effects.html#best-practices-and-recommendations"><i class="fa fa-check"></i><b>30.6.5</b> Best Practices and Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="30.7" data-path="sec-multiple-periods-and-variation-in-treatment-timing.html"><a href="sec-multiple-periods-and-variation-in-treatment-timing.html"><i class="fa fa-check"></i><b>30.7</b> Multiple Periods and Variation in Treatment Timing</a>
<ul>
<li class="chapter" data-level="30.7.1" data-path="sec-multiple-periods-and-variation-in-treatment-timing.html"><a href="sec-multiple-periods-and-variation-in-treatment-timing.html#sec-staggered-difference-in-differences"><i class="fa fa-check"></i><b>30.7.1</b> Staggered Difference-in-Differences</a></li>
</ul></li>
<li class="chapter" data-level="30.8" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html"><i class="fa fa-check"></i><b>30.8</b> Modern Estimators for Staggered Adoption</a>
<ul>
<li class="chapter" data-level="30.8.1" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#sec-group-time-average-treatment-effects-callaway2021difference"><i class="fa fa-check"></i><b>30.8.1</b> Group-Time Average Treatment Effects <span class="citation">(Callaway and Sant’Anna 2021)</span></a></li>
<li class="chapter" data-level="30.8.2" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#sec-cohort-average-treatment-effects-sun2021estimating"><i class="fa fa-check"></i><b>30.8.2</b> Cohort Average Treatment Effects <span class="citation">(L. Sun and Abraham 2021)</span></a></li>
<li class="chapter" data-level="30.8.3" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#sec-stacked-difference-in-differences"><i class="fa fa-check"></i><b>30.8.3</b> Stacked Difference-in-Differences</a></li>
<li class="chapter" data-level="30.8.4" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#sec-panel-match-did-estimator-with-in-and-out-treatment-conditions"><i class="fa fa-check"></i><b>30.8.4</b> Panel Match DiD Estimator with In-and-Out Treatment Conditions</a></li>
<li class="chapter" data-level="30.8.5" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#counterfactual-estimators"><i class="fa fa-check"></i><b>30.8.5</b> Counterfactual Estimators</a></li>
<li class="chapter" data-level="30.8.6" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#sec-matrix-completion-estimator"><i class="fa fa-check"></i><b>30.8.6</b> Matrix Completion Estimator</a></li>
<li class="chapter" data-level="30.8.7" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#sec-reshaped-inverse-probability-weighting-twfe-estimator"><i class="fa fa-check"></i><b>30.8.7</b> Reshaped Inverse Probability Weighting - TWFE Estimator</a></li>
<li class="chapter" data-level="30.8.8" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#gardner2022two-and-borusyak2024revisiting"><i class="fa fa-check"></i><b>30.8.8</b> <span class="citation">Gardner (2022)</span> and <span class="citation">Borusyak, Jaravel, and Spiess (2024)</span></a></li>
<li class="chapter" data-level="30.8.9" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#dynamic-treatment-effect-estimation-with-interactive-fixed-effects-and-short-panels"><i class="fa fa-check"></i><b>30.8.9</b> Dynamic Treatment Effect Estimation with Interactive Fixed Effects and Short Panels</a></li>
<li class="chapter" data-level="30.8.10" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#sec-switching-difference-in-differences-estimator-de2020two"><i class="fa fa-check"></i><b>30.8.10</b> Switching Difference-in-Differences Estimator <span class="citation">(Clément De Chaisemartin and d’Haultfoeuille 2020)</span></a></li>
<li class="chapter" data-level="30.8.11" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#augmentedforward-did"><i class="fa fa-check"></i><b>30.8.11</b> Augmented/Forward DID</a></li>
<li class="chapter" data-level="30.8.12" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#doubly-robust-difference-in-differences-estimators"><i class="fa fa-check"></i><b>30.8.12</b> Doubly Robust Difference-in-Differences Estimators</a></li>
<li class="chapter" data-level="30.8.13" data-path="sec-modern-estimators-for-staggered-adoption.html"><a href="sec-modern-estimators-for-staggered-adoption.html#nonlinear-difference-in-differences"><i class="fa fa-check"></i><b>30.8.13</b> Nonlinear Difference-in-Differences</a></li>
</ul></li>
<li class="chapter" data-level="30.9" data-path="multiple-treatments.html"><a href="multiple-treatments.html"><i class="fa fa-check"></i><b>30.9</b> Multiple Treatments</a>
<ul>
<li class="chapter" data-level="30.9.1" data-path="multiple-treatments.html"><a href="multiple-treatments.html#multiple-treatment-groups-model-specification"><i class="fa fa-check"></i><b>30.9.1</b> Multiple Treatment Groups: Model Specification</a></li>
<li class="chapter" data-level="30.9.2" data-path="multiple-treatments.html"><a href="multiple-treatments.html#understanding-the-control-group-in-multiple-treatment-did"><i class="fa fa-check"></i><b>30.9.2</b> Understanding the Control Group in Multiple Treatment DiD</a></li>
<li class="chapter" data-level="30.9.3" data-path="multiple-treatments.html"><a href="multiple-treatments.html#alternative-approaches-separate-regressions-vs.-one-model"><i class="fa fa-check"></i><b>30.9.3</b> Alternative Approaches: Separate Regressions vs. One Model</a></li>
<li class="chapter" data-level="30.9.4" data-path="multiple-treatments.html"><a href="multiple-treatments.html#handling-treatment-intensity"><i class="fa fa-check"></i><b>30.9.4</b> Handling Treatment Intensity</a></li>
<li class="chapter" data-level="30.9.5" data-path="multiple-treatments.html"><a href="multiple-treatments.html#considerations-when-individuals-can-move-between-treatment-groups"><i class="fa fa-check"></i><b>30.9.5</b> Considerations When Individuals Can Move Between Treatment Groups</a></li>
<li class="chapter" data-level="30.9.6" data-path="multiple-treatments.html"><a href="multiple-treatments.html#parallel-trends-assumption-in-multiple-treatment-did"><i class="fa fa-check"></i><b>30.9.6</b> Parallel Trends Assumption in Multiple-Treatment DiD</a></li>
</ul></li>
<li class="chapter" data-level="30.10" data-path="mediation-under-did.html"><a href="mediation-under-did.html"><i class="fa fa-check"></i><b>30.10</b> Mediation Under DiD</a>
<ul>
<li class="chapter" data-level="30.10.1" data-path="mediation-under-did.html"><a href="mediation-under-did.html#mediation-model-in-did"><i class="fa fa-check"></i><b>30.10.1</b> Mediation Model in DiD</a></li>
<li class="chapter" data-level="30.10.2" data-path="mediation-under-did.html"><a href="mediation-under-did.html#interpreting-the-results"><i class="fa fa-check"></i><b>30.10.2</b> Interpreting the Results</a></li>
<li class="chapter" data-level="30.10.3" data-path="mediation-under-did.html"><a href="mediation-under-did.html#challenges-in-mediation-analysis-for-did"><i class="fa fa-check"></i><b>30.10.3</b> Challenges in Mediation Analysis for DiD</a></li>
<li class="chapter" data-level="30.10.4" data-path="mediation-under-did.html"><a href="mediation-under-did.html#alternative-approach-instrumental-variables-for-mediation"><i class="fa fa-check"></i><b>30.10.4</b> Alternative Approach: Instrumental Variables for Mediation</a></li>
</ul></li>
<li class="chapter" data-level="30.11" data-path="assumptions-3.html"><a href="assumptions-3.html"><i class="fa fa-check"></i><b>30.11</b> Assumptions</a>
<ul>
<li class="chapter" data-level="30.11.1" data-path="assumptions-3.html"><a href="assumptions-3.html#prior-parallel-trends-test"><i class="fa fa-check"></i><b>30.11.1</b> Prior Parallel Trends Test</a></li>
<li class="chapter" data-level="30.11.2" data-path="assumptions-3.html"><a href="assumptions-3.html#sec-placebo-test-did"><i class="fa fa-check"></i><b>30.11.2</b> Placebo Test</a></li>
</ul></li>
<li class="chapter" data-level="30.12" data-path="robustness-checks-4.html"><a href="robustness-checks-4.html"><i class="fa fa-check"></i><b>30.12</b> Robustness Checks</a>
<ul>
<li class="chapter" data-level="30.12.1" data-path="robustness-checks-4.html"><a href="robustness-checks-4.html#robustness-checks-to-strengthen-causal-interpretation"><i class="fa fa-check"></i><b>30.12.1</b> Robustness Checks to Strengthen Causal Interpretation</a></li>
<li class="chapter" data-level="30.12.2" data-path="robustness-checks-4.html"><a href="robustness-checks-4.html#best-practices-for-reliable-did-implementation"><i class="fa fa-check"></i><b>30.12.2</b> Best Practices for Reliable DiD Implementation</a></li>
</ul></li>
<li class="chapter" data-level="30.13" data-path="concerns-in-did.html"><a href="concerns-in-did.html"><i class="fa fa-check"></i><b>30.13</b> Concerns in DID</a>
<ul>
<li class="chapter" data-level="30.13.1" data-path="concerns-in-did.html"><a href="concerns-in-did.html#matching-methods-in-did"><i class="fa fa-check"></i><b>30.13.1</b> Matching Methods in DID</a></li>
<li class="chapter" data-level="30.13.2" data-path="concerns-in-did.html"><a href="concerns-in-did.html#control-variables-in-did"><i class="fa fa-check"></i><b>30.13.2</b> Control Variables in DID</a></li>
<li class="chapter" data-level="30.13.3" data-path="concerns-in-did.html"><a href="concerns-in-did.html#did-for-count-data-fixed-effects-poisson-model"><i class="fa fa-check"></i><b>30.13.3</b> DID for Count Data: Fixed-Effects Poisson Model</a></li>
<li class="chapter" data-level="30.13.4" data-path="concerns-in-did.html"><a href="concerns-in-did.html#handling-zero-valued-outcomes-in-did"><i class="fa fa-check"></i><b>30.13.4</b> Handling Zero-Valued Outcomes in DID</a></li>
<li class="chapter" data-level="30.13.5" data-path="concerns-in-did.html"><a href="concerns-in-did.html#standard-errors-1"><i class="fa fa-check"></i><b>30.13.5</b> Standard Errors</a></li>
<li class="chapter" data-level="30.13.6" data-path="concerns-in-did.html"><a href="concerns-in-did.html#sec-partial-identification-did"><i class="fa fa-check"></i><b>30.13.6</b> Partial Identification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="31" data-path="sec-changes-in-changes.html"><a href="sec-changes-in-changes.html"><i class="fa fa-check"></i><b>31</b> Changes-in-Changes</a>
<ul>
<li class="chapter" data-level="31.1" data-path="key-concepts.html"><a href="key-concepts.html"><i class="fa fa-check"></i><b>31.1</b> Key Concepts</a></li>
<li class="chapter" data-level="31.2" data-path="estimating-qtt-with-cic.html"><a href="estimating-qtt-with-cic.html"><i class="fa fa-check"></i><b>31.2</b> Estimating QTT with CiC</a></li>
<li class="chapter" data-level="31.3" data-path="application-4.html"><a href="application-4.html"><i class="fa fa-check"></i><b>31.3</b> Application</a>
<ul>
<li class="chapter" data-level="31.3.1" data-path="application-4.html"><a href="application-4.html#ecic-package"><i class="fa fa-check"></i><b>31.3.1</b> ECIC package</a></li>
<li class="chapter" data-level="31.3.2" data-path="application-4.html"><a href="application-4.html#qte-package"><i class="fa fa-check"></i><b>31.3.2</b> QTE package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="sec-synthetic-control.html"><a href="sec-synthetic-control.html"><i class="fa fa-check"></i><b>32</b> Synthetic Control</a>
<ul>
<li class="chapter" data-level="32.1" data-path="marketing-applications.html"><a href="marketing-applications.html"><i class="fa fa-check"></i><b>32.1</b> Marketing Applications</a></li>
<li class="chapter" data-level="32.2" data-path="key-features-of-scm.html"><a href="key-features-of-scm.html"><i class="fa fa-check"></i><b>32.2</b> Key Features of SCM</a></li>
<li class="chapter" data-level="32.3" data-path="advantages-of-scm.html"><a href="advantages-of-scm.html"><i class="fa fa-check"></i><b>32.3</b> Advantages of SCM</a>
<ul>
<li class="chapter" data-level="32.3.1" data-path="advantages-of-scm.html"><a href="advantages-of-scm.html#compared-to-did"><i class="fa fa-check"></i><b>32.3.1</b> Compared to DiD</a></li>
<li class="chapter" data-level="32.3.2" data-path="advantages-of-scm.html"><a href="advantages-of-scm.html#compared-to-linear-regression"><i class="fa fa-check"></i><b>32.3.2</b> Compared to Linear Regression</a></li>
<li class="chapter" data-level="32.3.3" data-path="advantages-of-scm.html"><a href="advantages-of-scm.html#additional-advantages"><i class="fa fa-check"></i><b>32.3.3</b> Additional Advantages</a></li>
</ul></li>
<li class="chapter" data-level="32.4" data-path="disadvantages-of-scm.html"><a href="disadvantages-of-scm.html"><i class="fa fa-check"></i><b>32.4</b> Disadvantages of SCM</a></li>
<li class="chapter" data-level="32.5" data-path="assumptions-4.html"><a href="assumptions-4.html"><i class="fa fa-check"></i><b>32.5</b> Assumptions</a></li>
<li class="chapter" data-level="32.6" data-path="estimation-3.html"><a href="estimation-3.html"><i class="fa fa-check"></i><b>32.6</b> Estimation</a>
<ul>
<li class="chapter" data-level="32.6.1" data-path="estimation-3.html"><a href="estimation-3.html#constructing-the-synthetic-control"><i class="fa fa-check"></i><b>32.6.1</b> Constructing the Synthetic Control</a></li>
<li class="chapter" data-level="32.6.2" data-path="estimation-3.html"><a href="estimation-3.html#penalized-synthetic-control"><i class="fa fa-check"></i><b>32.6.2</b> Penalized Synthetic Control</a></li>
</ul></li>
<li class="chapter" data-level="32.7" data-path="theoretical-considerations.html"><a href="theoretical-considerations.html"><i class="fa fa-check"></i><b>32.7</b> Theoretical Considerations</a></li>
<li class="chapter" data-level="32.8" data-path="inference-in-scm.html"><a href="inference-in-scm.html"><i class="fa fa-check"></i><b>32.8</b> Inference in SCM</a>
<ul>
<li class="chapter" data-level="32.8.1" data-path="inference-in-scm.html"><a href="inference-in-scm.html#permutation-placebo-inference"><i class="fa fa-check"></i><b>32.8.1</b> Permutation (Placebo) Inference</a></li>
<li class="chapter" data-level="32.8.2" data-path="inference-in-scm.html"><a href="inference-in-scm.html#one-sided-inference"><i class="fa fa-check"></i><b>32.8.2</b> One-Sided Inference</a></li>
</ul></li>
<li class="chapter" data-level="32.9" data-path="sec-augmented-synthetic-control.html"><a href="sec-augmented-synthetic-control.html"><i class="fa fa-check"></i><b>32.9</b> Augmented Synthetic Control Method</a></li>
<li class="chapter" data-level="32.10" data-path="synthetic-control-with-staggered-adoption.html"><a href="synthetic-control-with-staggered-adoption.html"><i class="fa fa-check"></i><b>32.10</b> Synthetic Control with Staggered Adoption</a>
<ul>
<li class="chapter" data-level="32.10.1" data-path="synthetic-control-with-staggered-adoption.html"><a href="synthetic-control-with-staggered-adoption.html#partially-pooled-synthetic-control"><i class="fa fa-check"></i><b>32.10.1</b> Partially Pooled Synthetic Control</a></li>
</ul></li>
<li class="chapter" data-level="32.11" data-path="generalized-synthetic-control.html"><a href="generalized-synthetic-control.html"><i class="fa fa-check"></i><b>32.11</b> Generalized Synthetic Control</a>
<ul>
<li class="chapter" data-level="32.11.1" data-path="generalized-synthetic-control.html"><a href="generalized-synthetic-control.html#the-problem-with-traditional-methods"><i class="fa fa-check"></i><b>32.11.1</b> The Problem with Traditional Methods</a></li>
<li class="chapter" data-level="32.11.2" data-path="generalized-synthetic-control.html"><a href="generalized-synthetic-control.html#generalized-synthetic-control-model"><i class="fa fa-check"></i><b>32.11.2</b> Generalized Synthetic Control Model</a></li>
<li class="chapter" data-level="32.11.3" data-path="generalized-synthetic-control.html"><a href="generalized-synthetic-control.html#identification-and-estimation"><i class="fa fa-check"></i><b>32.11.3</b> Identification and Estimation</a></li>
<li class="chapter" data-level="32.11.4" data-path="generalized-synthetic-control.html"><a href="generalized-synthetic-control.html#bootstrap-procedure-for-standard-errors"><i class="fa fa-check"></i><b>32.11.4</b> Bootstrap Procedure for Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="32.12" data-path="bayesian-synthetic-control.html"><a href="bayesian-synthetic-control.html"><i class="fa fa-check"></i><b>32.12</b> Bayesian Synthetic Control</a>
<ul>
<li class="chapter" data-level="32.12.1" data-path="bayesian-synthetic-control.html"><a href="bayesian-synthetic-control.html#bayesian-causal-inference-framework"><i class="fa fa-check"></i><b>32.12.1</b> Bayesian Causal Inference Framework</a></li>
<li class="chapter" data-level="32.12.2" data-path="bayesian-synthetic-control.html"><a href="bayesian-synthetic-control.html#bayesian-dynamic-multilevel-factor-model"><i class="fa fa-check"></i><b>32.12.2</b> Bayesian Dynamic Multilevel Factor Model</a></li>
<li class="chapter" data-level="32.12.3" data-path="bayesian-synthetic-control.html"><a href="bayesian-synthetic-control.html#bayesian-sparse-synthetic-control"><i class="fa fa-check"></i><b>32.12.3</b> Bayesian Sparse Synthetic Control</a></li>
<li class="chapter" data-level="32.12.4" data-path="bayesian-synthetic-control.html"><a href="bayesian-synthetic-control.html#bayesian-inference-and-mcmc-estimation"><i class="fa fa-check"></i><b>32.12.4</b> Bayesian Inference and MCMC Estimation</a></li>
</ul></li>
<li class="chapter" data-level="32.13" data-path="using-multiple-outcomes-to-improve-the-synthetic-control-method.html"><a href="using-multiple-outcomes-to-improve-the-synthetic-control-method.html"><i class="fa fa-check"></i><b>32.13</b> Using Multiple Outcomes to Improve the Synthetic Control Method</a>
<ul>
<li class="chapter" data-level="32.13.1" data-path="using-multiple-outcomes-to-improve-the-synthetic-control-method.html"><a href="using-multiple-outcomes-to-improve-the-synthetic-control-method.html#standard-synthetic-control-method"><i class="fa fa-check"></i><b>32.13.1</b> Standard Synthetic Control Method</a></li>
<li class="chapter" data-level="32.13.2" data-path="using-multiple-outcomes-to-improve-the-synthetic-control-method.html"><a href="using-multiple-outcomes-to-improve-the-synthetic-control-method.html#using-multiple-outcomes-for-bias-reduction"><i class="fa fa-check"></i><b>32.13.2</b> Using Multiple Outcomes for Bias Reduction</a></li>
<li class="chapter" data-level="32.13.3" data-path="using-multiple-outcomes-to-improve-the-synthetic-control-method.html"><a href="using-multiple-outcomes-to-improve-the-synthetic-control-method.html#estimation-methods-1"><i class="fa fa-check"></i><b>32.13.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="32.13.4" data-path="using-multiple-outcomes-to-improve-the-synthetic-control-method.html"><a href="using-multiple-outcomes-to-improve-the-synthetic-control-method.html#empirical-application-flint-water-crisis"><i class="fa fa-check"></i><b>32.13.4</b> Empirical Application: Flint Water Crisis</a></li>
</ul></li>
<li class="chapter" data-level="32.14" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>32.14</b> Applications</a>
<ul>
<li class="chapter" data-level="32.14.1" data-path="applications.html"><a href="applications.html#synthetic-control-estimation"><i class="fa fa-check"></i><b>32.14.1</b> Synthetic Control Estimation</a></li>
<li class="chapter" data-level="32.14.2" data-path="applications.html"><a href="applications.html#the-basque-country-policy-change"><i class="fa fa-check"></i><b>32.14.2</b> The Basque Country Policy Change</a></li>
<li class="chapter" data-level="32.14.3" data-path="applications.html"><a href="applications.html#micro-synthetic-control-with-microsynth"><i class="fa fa-check"></i><b>32.14.3</b> Micro-Synthetic Control with <code>microsynth</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="33" data-path="sec-event-studies.html"><a href="sec-event-studies.html"><i class="fa fa-check"></i><b>33</b> Event Studies</a>
<ul>
<li class="chapter" data-level="33.1" data-path="review-of-event-studies-across-disciplines.html"><a href="review-of-event-studies-across-disciplines.html"><i class="fa fa-check"></i><b>33.1</b> Review of Event Studies Across Disciplines</a>
<ul>
<li class="chapter" data-level="33.1.1" data-path="review-of-event-studies-across-disciplines.html"><a href="review-of-event-studies-across-disciplines.html#finance-applications"><i class="fa fa-check"></i><b>33.1.1</b> Finance Applications</a></li>
<li class="chapter" data-level="33.1.2" data-path="review-of-event-studies-across-disciplines.html"><a href="review-of-event-studies-across-disciplines.html#management-applications"><i class="fa fa-check"></i><b>33.1.2</b> Management Applications</a></li>
<li class="chapter" data-level="33.1.3" data-path="review-of-event-studies-across-disciplines.html"><a href="review-of-event-studies-across-disciplines.html#marketing-applications-1"><i class="fa fa-check"></i><b>33.1.3</b> Marketing Applications</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="key-assumptions-3.html"><a href="key-assumptions-3.html"><i class="fa fa-check"></i><b>33.2</b> Key Assumptions</a></li>
<li class="chapter" data-level="33.3" data-path="steps-for-conducting-an-event-study.html"><a href="steps-for-conducting-an-event-study.html"><i class="fa fa-check"></i><b>33.3</b> Steps for Conducting an Event Study</a>
<ul>
<li class="chapter" data-level="33.3.1" data-path="steps-for-conducting-an-event-study.html"><a href="steps-for-conducting-an-event-study.html#step-1-event-identification"><i class="fa fa-check"></i><b>33.3.1</b> Step 1: Event Identification</a></li>
<li class="chapter" data-level="33.3.2" data-path="steps-for-conducting-an-event-study.html"><a href="steps-for-conducting-an-event-study.html#step-2-define-the-event-and-estimation-windows"><i class="fa fa-check"></i><b>33.3.2</b> Step 2: Define the Event and Estimation Windows</a></li>
<li class="chapter" data-level="33.3.3" data-path="steps-for-conducting-an-event-study.html"><a href="steps-for-conducting-an-event-study.html#step-3-compute-normal-vs.-abnormal-returns"><i class="fa fa-check"></i><b>33.3.3</b> Step 3: Compute Normal vs. Abnormal Returns</a></li>
<li class="chapter" data-level="33.3.4" data-path="steps-for-conducting-an-event-study.html"><a href="steps-for-conducting-an-event-study.html#step-4-compute-cumulative-abnormal-returns"><i class="fa fa-check"></i><b>33.3.4</b> Step 4: Compute Cumulative Abnormal Returns</a></li>
<li class="chapter" data-level="33.3.5" data-path="steps-for-conducting-an-event-study.html"><a href="steps-for-conducting-an-event-study.html#step-5-statistical-tests-for-significance"><i class="fa fa-check"></i><b>33.3.5</b> Step 5: Statistical Tests for Significance</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="event-studies-in-marketing.html"><a href="event-studies-in-marketing.html"><i class="fa fa-check"></i><b>33.4</b> Event Studies in Marketing</a>
<ul>
<li class="chapter" data-level="33.4.1" data-path="event-studies-in-marketing.html"><a href="event-studies-in-marketing.html#definition"><i class="fa fa-check"></i><b>33.4.1</b> Definition</a></li>
<li class="chapter" data-level="33.4.2" data-path="event-studies-in-marketing.html"><a href="event-studies-in-marketing.html#when-can-marketing-events-affect-non-operating-assets-or-debt"><i class="fa fa-check"></i><b>33.4.2</b> When Can Marketing Events Affect Non-Operating Assets or Debt?</a></li>
<li class="chapter" data-level="33.4.3" data-path="event-studies-in-marketing.html"><a href="event-studies-in-marketing.html#calculating-the-leverage-effect"><i class="fa fa-check"></i><b>33.4.3</b> Calculating the Leverage Effect</a></li>
<li class="chapter" data-level="33.4.4" data-path="event-studies-in-marketing.html"><a href="event-studies-in-marketing.html#computing-leverage-effect-from-compustat-data"><i class="fa fa-check"></i><b>33.4.4</b> Computing Leverage Effect from Compustat Data</a></li>
</ul></li>
<li class="chapter" data-level="33.5" data-path="economic-significance.html"><a href="economic-significance.html"><i class="fa fa-check"></i><b>33.5</b> Economic Significance</a></li>
<li class="chapter" data-level="33.6" data-path="testing-in-event-studies.html"><a href="testing-in-event-studies.html"><i class="fa fa-check"></i><b>33.6</b> Testing in Event Studies</a>
<ul>
<li class="chapter" data-level="33.6.1" data-path="testing-in-event-studies.html"><a href="testing-in-event-studies.html#statistical-power-in-event-studies"><i class="fa fa-check"></i><b>33.6.1</b> Statistical Power in Event Studies</a></li>
<li class="chapter" data-level="33.6.2" data-path="testing-in-event-studies.html"><a href="testing-in-event-studies.html#parametric-tests"><i class="fa fa-check"></i><b>33.6.2</b> Parametric Tests</a></li>
<li class="chapter" data-level="33.6.3" data-path="testing-in-event-studies.html"><a href="testing-in-event-studies.html#non-parametric-tests-1"><i class="fa fa-check"></i><b>33.6.3</b> Non-Parametric Tests</a></li>
</ul></li>
<li class="chapter" data-level="33.7" data-path="sample-in-event-studies.html"><a href="sample-in-event-studies.html"><i class="fa fa-check"></i><b>33.7</b> Sample in Event Studies</a></li>
<li class="chapter" data-level="33.8" data-path="confounders-in-event-studies.html"><a href="confounders-in-event-studies.html"><i class="fa fa-check"></i><b>33.8</b> Confounders in Event Studies</a>
<ul>
<li class="chapter" data-level="33.8.1" data-path="confounders-in-event-studies.html"><a href="confounders-in-event-studies.html#types-of-confounding-events"><i class="fa fa-check"></i><b>33.8.1</b> Types of Confounding Events</a></li>
<li class="chapter" data-level="33.8.2" data-path="confounders-in-event-studies.html"><a href="confounders-in-event-studies.html#should-we-exclude-confounded-observations"><i class="fa fa-check"></i><b>33.8.2</b> Should We Exclude Confounded Observations?</a></li>
<li class="chapter" data-level="33.8.3" data-path="confounders-in-event-studies.html"><a href="confounders-in-event-studies.html#simulation-study-should-we-exclude-correlated-and-uncorrelated-events"><i class="fa fa-check"></i><b>33.8.3</b> Simulation Study: Should We Exclude Correlated and Uncorrelated Events?</a></li>
</ul></li>
<li class="chapter" data-level="33.9" data-path="biases-in-event-studies.html"><a href="biases-in-event-studies.html"><i class="fa fa-check"></i><b>33.9</b> Biases in Event Studies</a>
<ul>
<li class="chapter" data-level="33.9.1" data-path="biases-in-event-studies.html"><a href="biases-in-event-studies.html#timing-bias-different-market-closing-times"><i class="fa fa-check"></i><b>33.9.1</b> Timing Bias: Different Market Closing Times</a></li>
<li class="chapter" data-level="33.9.2" data-path="biases-in-event-studies.html"><a href="biases-in-event-studies.html#upward-bias-in-cumulative-abnormal-returns"><i class="fa fa-check"></i><b>33.9.2</b> Upward Bias in Cumulative Abnormal Returns</a></li>
<li class="chapter" data-level="33.9.3" data-path="biases-in-event-studies.html"><a href="biases-in-event-studies.html#cross-sectional-dependence-bias"><i class="fa fa-check"></i><b>33.9.3</b> Cross-Sectional Dependence Bias</a></li>
<li class="chapter" data-level="33.9.4" data-path="biases-in-event-studies.html"><a href="biases-in-event-studies.html#sample-selection-bias"><i class="fa fa-check"></i><b>33.9.4</b> Sample Selection Bias</a></li>
<li class="chapter" data-level="33.9.5" data-path="biases-in-event-studies.html"><a href="biases-in-event-studies.html#corrections-for-sample-selection-bias"><i class="fa fa-check"></i><b>33.9.5</b> Corrections for Sample Selection Bias</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="long-run-event-studies.html"><a href="long-run-event-studies.html"><i class="fa fa-check"></i><b>33.10</b> Long-run Event Studies</a>
<ul>
<li class="chapter" data-level="33.10.1" data-path="long-run-event-studies.html"><a href="long-run-event-studies.html#sec-buy-and-hold-abnormal-returns-bhar"><i class="fa fa-check"></i><b>33.10.1</b> Buy-and-Hold Abnormal Returns (BHAR)</a></li>
<li class="chapter" data-level="33.10.2" data-path="long-run-event-studies.html"><a href="long-run-event-studies.html#long-term-cumulative-abnormal-returns-lcars"><i class="fa fa-check"></i><b>33.10.2</b> Long-term Cumulative Abnormal Returns (LCARs)</a></li>
<li class="chapter" data-level="33.10.3" data-path="long-run-event-studies.html"><a href="long-run-event-studies.html#calendar-time-portfolio-abnormal-returns-ctars"><i class="fa fa-check"></i><b>33.10.3</b> Calendar-time Portfolio Abnormal Returns (CTARs)</a></li>
</ul></li>
<li class="chapter" data-level="33.11" data-path="aggregation.html"><a href="aggregation.html"><i class="fa fa-check"></i><b>33.11</b> Aggregation</a>
<ul>
<li class="chapter" data-level="33.11.1" data-path="aggregation.html"><a href="aggregation.html#over-time"><i class="fa fa-check"></i><b>33.11.1</b> Over Time</a></li>
<li class="chapter" data-level="33.11.2" data-path="aggregation.html"><a href="aggregation.html#across-firms-and-over-time"><i class="fa fa-check"></i><b>33.11.2</b> Across Firms and Over Time</a></li>
<li class="chapter" data-level="33.11.3" data-path="aggregation.html"><a href="aggregation.html#statistical-tests-1"><i class="fa fa-check"></i><b>33.11.3</b> Statistical Tests</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="heterogeneity-in-the-event-effect.html"><a href="heterogeneity-in-the-event-effect.html"><i class="fa fa-check"></i><b>33.12</b> Heterogeneity in the Event Effect</a>
<ul>
<li class="chapter" data-level="33.12.1" data-path="heterogeneity-in-the-event-effect.html"><a href="heterogeneity-in-the-event-effect.html#common-variables-affecting-car-in-marketing-and-finance"><i class="fa fa-check"></i><b>33.12.1</b> Common Variables Affecting CAR in Marketing and Finance</a></li>
</ul></li>
<li class="chapter" data-level="33.13" data-path="expected-return-calculation.html"><a href="expected-return-calculation.html"><i class="fa fa-check"></i><b>33.13</b> Expected Return Calculation</a>
<ul>
<li class="chapter" data-level="33.13.1" data-path="expected-return-calculation.html"><a href="expected-return-calculation.html#statistical-models-for-expected-returns"><i class="fa fa-check"></i><b>33.13.1</b> <strong>Statistical Models for Expected Returns</strong></a></li>
<li class="chapter" data-level="33.13.2" data-path="expected-return-calculation.html"><a href="expected-return-calculation.html#economic-models-for-expected-returns"><i class="fa fa-check"></i><b>33.13.2</b> <strong>Economic Models for Expected Returns</strong></a></li>
</ul></li>
<li class="chapter" data-level="33.14" data-path="application-of-event-study.html"><a href="application-of-event-study.html"><i class="fa fa-check"></i><b>33.14</b> Application of Event Study</a>
<ul>
<li class="chapter" data-level="33.14.1" data-path="application-of-event-study.html"><a href="application-of-event-study.html#sorting-portfolios-for-expected-returns"><i class="fa fa-check"></i><b>33.14.1</b> <strong>Sorting Portfolios for Expected Returns</strong></a></li>
<li class="chapter" data-level="33.14.2" data-path="application-of-event-study.html"><a href="application-of-event-study.html#erer-package"><i class="fa fa-check"></i><b>33.14.2</b> <strong><code>erer</code> Package</strong></a></li>
<li class="chapter" data-level="33.14.3" data-path="application-of-event-study.html"><a href="application-of-event-study.html#eventus"><i class="fa fa-check"></i><b>33.14.3</b> Eventus</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="sec-instrumental-variables.html"><a href="sec-instrumental-variables.html"><i class="fa fa-check"></i><b>34</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="34.1" data-path="challenges-with-instrumental-variables.html"><a href="challenges-with-instrumental-variables.html"><i class="fa fa-check"></i><b>34.1</b> Challenges with Instrumental Variables</a></li>
<li class="chapter" data-level="34.2" data-path="framework-for-instrumental-variables.html"><a href="framework-for-instrumental-variables.html"><i class="fa fa-check"></i><b>34.2</b> Framework for Instrumental Variables</a>
<ul>
<li class="chapter" data-level="34.2.1" data-path="framework-for-instrumental-variables.html"><a href="framework-for-instrumental-variables.html#constant-treatment-effect-model"><i class="fa fa-check"></i><b>34.2.1</b> Constant-Treatment-Effect Model</a></li>
<li class="chapter" data-level="34.2.2" data-path="framework-for-instrumental-variables.html"><a href="framework-for-instrumental-variables.html#instrumental-variable-solution"><i class="fa fa-check"></i><b>34.2.2</b> Instrumental Variable Solution</a></li>
<li class="chapter" data-level="34.2.3" data-path="framework-for-instrumental-variables.html"><a href="framework-for-instrumental-variables.html#heterogeneous-treatment-effects-and-the-late-framework"><i class="fa fa-check"></i><b>34.2.3</b> Heterogeneous Treatment Effects and the LATE Framework</a></li>
<li class="chapter" data-level="34.2.4" data-path="framework-for-instrumental-variables.html"><a href="framework-for-instrumental-variables.html#assumptions-for-late-identification"><i class="fa fa-check"></i><b>34.2.4</b> Assumptions for LATE Identification</a></li>
<li class="chapter" data-level="34.2.5" data-path="framework-for-instrumental-variables.html"><a href="framework-for-instrumental-variables.html#local-average-treatment-effect-theorem"><i class="fa fa-check"></i><b>34.2.5</b> Local Average Treatment Effect Theorem</a></li>
<li class="chapter" data-level="34.2.6" data-path="framework-for-instrumental-variables.html"><a href="framework-for-instrumental-variables.html#iv-in-randomized-trials-noncompliance"><i class="fa fa-check"></i><b>34.2.6</b> IV in Randomized Trials (Noncompliance)</a></li>
</ul></li>
<li class="chapter" data-level="34.3" data-path="sec-estimation.html"><a href="sec-estimation.html"><i class="fa fa-check"></i><b>34.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="34.3.1" data-path="sec-estimation.html"><a href="sec-estimation.html#sec-two-stage-least-squares-estimation"><i class="fa fa-check"></i><b>34.3.1</b> Two-Stage Least Squares Estimation</a></li>
<li class="chapter" data-level="34.3.2" data-path="sec-estimation.html"><a href="sec-estimation.html#iv-gmm"><i class="fa fa-check"></i><b>34.3.2</b> IV-GMM</a></li>
<li class="chapter" data-level="34.3.3" data-path="sec-estimation.html"><a href="sec-estimation.html#limited-information-maximum-likelihood"><i class="fa fa-check"></i><b>34.3.3</b> Limited Information Maximum Likelihood</a></li>
<li class="chapter" data-level="34.3.4" data-path="sec-estimation.html"><a href="sec-estimation.html#jackknife-iv"><i class="fa fa-check"></i><b>34.3.4</b> Jackknife IV</a></li>
<li class="chapter" data-level="34.3.5" data-path="sec-estimation.html"><a href="sec-estimation.html#sec-control-function-approach"><i class="fa fa-check"></i><b>34.3.5</b> Control Function Approach</a></li>
<li class="chapter" data-level="34.3.6" data-path="sec-estimation.html"><a href="sec-estimation.html#fuller-and-bias-reduced-iv"><i class="fa fa-check"></i><b>34.3.6</b> Fuller and Bias-Reduced IV</a></li>
</ul></li>
<li class="chapter" data-level="34.4" data-path="asymptotic-properties-of-the-iv-estimator.html"><a href="asymptotic-properties-of-the-iv-estimator.html"><i class="fa fa-check"></i><b>34.4</b> Asymptotic Properties of the IV Estimator</a>
<ul>
<li class="chapter" data-level="34.4.1" data-path="asymptotic-properties-of-the-iv-estimator.html"><a href="asymptotic-properties-of-the-iv-estimator.html#sec-consistency-iv"><i class="fa fa-check"></i><b>34.4.1</b> Consistency</a></li>
<li class="chapter" data-level="34.4.2" data-path="asymptotic-properties-of-the-iv-estimator.html"><a href="asymptotic-properties-of-the-iv-estimator.html#asymptotic-normality-1"><i class="fa fa-check"></i><b>34.4.2</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="34.4.3" data-path="asymptotic-properties-of-the-iv-estimator.html"><a href="asymptotic-properties-of-the-iv-estimator.html#asymptotic-efficiency-1"><i class="fa fa-check"></i><b>34.4.3</b> Asymptotic Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="34.5" data-path="sec-inference-iv.html"><a href="sec-inference-iv.html"><i class="fa fa-check"></i><b>34.5</b> Inference</a>
<ul>
<li class="chapter" data-level="34.5.1" data-path="sec-inference-iv.html"><a href="sec-inference-iv.html#sec-weak-instruments-problem"><i class="fa fa-check"></i><b>34.5.1</b> Weak Instruments Problem</a></li>
<li class="chapter" data-level="34.5.2" data-path="sec-inference-iv.html"><a href="sec-inference-iv.html#solutions-and-approaches-for-valid-inference"><i class="fa fa-check"></i><b>34.5.2</b> Solutions and Approaches for Valid Inference</a></li>
<li class="chapter" data-level="34.5.3" data-path="sec-inference-iv.html"><a href="sec-inference-iv.html#sec-anderson-rubin-approach"><i class="fa fa-check"></i><b>34.5.3</b> Anderson-Rubin Approach</a></li>
<li class="chapter" data-level="34.5.4" data-path="sec-inference-iv.html"><a href="sec-inference-iv.html#sec-tf-procedure"><i class="fa fa-check"></i><b>34.5.4</b> tF Procedure</a></li>
<li class="chapter" data-level="34.5.5" data-path="sec-inference-iv.html"><a href="sec-inference-iv.html#sec-ak-approach"><i class="fa fa-check"></i><b>34.5.5</b> AK Approach</a></li>
</ul></li>
<li class="chapter" data-level="34.6" data-path="testing-assumptions.html"><a href="testing-assumptions.html"><i class="fa fa-check"></i><b>34.6</b> Testing Assumptions</a>
<ul>
<li class="chapter" data-level="34.6.1" data-path="testing-assumptions.html"><a href="testing-assumptions.html#sec-relevance-assumption"><i class="fa fa-check"></i><b>34.6.1</b> Relevance Assumption</a></li>
<li class="chapter" data-level="34.6.2" data-path="testing-assumptions.html"><a href="testing-assumptions.html#independence-unconfoundedness"><i class="fa fa-check"></i><b>34.6.2</b> Independence (Unconfoundedness)</a></li>
<li class="chapter" data-level="34.6.3" data-path="testing-assumptions.html"><a href="testing-assumptions.html#sec-monotonicity-assumption"><i class="fa fa-check"></i><b>34.6.3</b> Monotonicity Assumption</a></li>
<li class="chapter" data-level="34.6.4" data-path="testing-assumptions.html"><a href="testing-assumptions.html#homogeneous-treatment-effects-optional"><i class="fa fa-check"></i><b>34.6.4</b> Homogeneous Treatment Effects (Optional)</a></li>
<li class="chapter" data-level="34.6.5" data-path="testing-assumptions.html"><a href="testing-assumptions.html#sec-linearity-and-additivity"><i class="fa fa-check"></i><b>34.6.5</b> Linearity and Additivity</a></li>
<li class="chapter" data-level="34.6.6" data-path="testing-assumptions.html"><a href="testing-assumptions.html#instrument-exogeneity-exclusion-restriction"><i class="fa fa-check"></i><b>34.6.6</b> Instrument Exogeneity (Exclusion Restriction)</a></li>
<li class="chapter" data-level="34.6.7" data-path="testing-assumptions.html"><a href="testing-assumptions.html#sec-exogeneity-assumption"><i class="fa fa-check"></i><b>34.6.7</b> Exogeneity Assumption</a></li>
</ul></li>
<li class="chapter" data-level="34.7" data-path="cautions-in-iv.html"><a href="cautions-in-iv.html"><i class="fa fa-check"></i><b>34.7</b> Cautions in IV</a>
<ul>
<li class="chapter" data-level="34.7.1" data-path="cautions-in-iv.html"><a href="cautions-in-iv.html#negative-r2-in-iv"><i class="fa fa-check"></i><b>34.7.1</b> Negative <span class="math inline">\(R^2\)</span> in IV</a></li>
<li class="chapter" data-level="34.7.2" data-path="cautions-in-iv.html"><a href="cautions-in-iv.html#sec-many-instruments-bias"><i class="fa fa-check"></i><b>34.7.2</b> Many-Instruments Bias</a></li>
<li class="chapter" data-level="34.7.3" data-path="cautions-in-iv.html"><a href="cautions-in-iv.html#heterogeneous-effects-in-iv-estimation"><i class="fa fa-check"></i><b>34.7.3</b> Heterogeneous Effects in IV Estimation</a></li>
<li class="chapter" data-level="34.7.4" data-path="cautions-in-iv.html"><a href="cautions-in-iv.html#zero-valued-outcomes"><i class="fa fa-check"></i><b>34.7.4</b> Zero-Valued Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="34.8" data-path="types-of-iv.html"><a href="types-of-iv.html"><i class="fa fa-check"></i><b>34.8</b> Types of IV</a>
<ul>
<li class="chapter" data-level="34.8.1" data-path="types-of-iv.html"><a href="types-of-iv.html#treatment-intensity"><i class="fa fa-check"></i><b>34.8.1</b> Treatment Intensity</a></li>
<li class="chapter" data-level="34.8.2" data-path="types-of-iv.html"><a href="types-of-iv.html#decision-maker-iv"><i class="fa fa-check"></i><b>34.8.2</b> Decision-Maker IV</a></li>
<li class="chapter" data-level="34.8.3" data-path="types-of-iv.html"><a href="types-of-iv.html#sec-proxy-variables"><i class="fa fa-check"></i><b>34.8.3</b> Proxy Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="sec-matching-methods.html"><a href="sec-matching-methods.html"><i class="fa fa-check"></i><b>35</b> Matching Methods</a>
<ul>
<li class="chapter" data-level="35.1" data-path="introduction-and-motivation.html"><a href="introduction-and-motivation.html"><i class="fa fa-check"></i><b>35.1</b> Introduction and Motivation</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="introduction-and-motivation.html"><a href="introduction-and-motivation.html#why-match"><i class="fa fa-check"></i><b>35.1.1</b> Why Match?</a></li>
<li class="chapter" data-level="35.1.2" data-path="introduction-and-motivation.html"><a href="introduction-and-motivation.html#matching-as-pruning"><i class="fa fa-check"></i><b>35.1.2</b> Matching as “Pruning”</a></li>
<li class="chapter" data-level="35.1.3" data-path="introduction-and-motivation.html"><a href="introduction-and-motivation.html#matching-with-did"><i class="fa fa-check"></i><b>35.1.3</b> Matching with DiD</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="key-assumptions-4.html"><a href="key-assumptions-4.html"><i class="fa fa-check"></i><b>35.2</b> Key Assumptions</a></li>
<li class="chapter" data-level="35.3" data-path="framework-for-generalization.html"><a href="framework-for-generalization.html"><i class="fa fa-check"></i><b>35.3</b> Framework for Generalization</a></li>
<li class="chapter" data-level="35.4" data-path="steps-for-matching.html"><a href="steps-for-matching.html"><i class="fa fa-check"></i><b>35.4</b> Steps for Matching</a>
<ul>
<li class="chapter" data-level="35.4.1" data-path="steps-for-matching.html"><a href="steps-for-matching.html#step-1-define-closeness-distance-metrics"><i class="fa fa-check"></i><b>35.4.1</b> Step 1: Define “Closeness” (Distance Metrics)</a></li>
<li class="chapter" data-level="35.4.2" data-path="steps-for-matching.html"><a href="steps-for-matching.html#step-2-matching-algorithms"><i class="fa fa-check"></i><b>35.4.2</b> Step 2: Matching Algorithms</a></li>
<li class="chapter" data-level="35.4.3" data-path="steps-for-matching.html"><a href="steps-for-matching.html#step-3-diagnosing-match-quality"><i class="fa fa-check"></i><b>35.4.3</b> Step 3: Diagnosing Match Quality</a></li>
<li class="chapter" data-level="35.4.4" data-path="steps-for-matching.html"><a href="steps-for-matching.html#step-4-estimating-treatment-effects"><i class="fa fa-check"></i><b>35.4.4</b> Step 4: Estimating Treatment Effects</a></li>
</ul></li>
<li class="chapter" data-level="35.5" data-path="special-considerations.html"><a href="special-considerations.html"><i class="fa fa-check"></i><b>35.5</b> Special Considerations</a></li>
<li class="chapter" data-level="35.6" data-path="choosing-a-matching-strategy.html"><a href="choosing-a-matching-strategy.html"><i class="fa fa-check"></i><b>35.6</b> Choosing a Matching Strategy</a>
<ul>
<li class="chapter" data-level="35.6.1" data-path="choosing-a-matching-strategy.html"><a href="choosing-a-matching-strategy.html#based-on-estimand"><i class="fa fa-check"></i><b>35.6.1</b> Based on Estimand</a></li>
<li class="chapter" data-level="35.6.2" data-path="choosing-a-matching-strategy.html"><a href="choosing-a-matching-strategy.html#based-on-diagnostics"><i class="fa fa-check"></i><b>35.6.2</b> Based on Diagnostics</a></li>
<li class="chapter" data-level="35.6.3" data-path="choosing-a-matching-strategy.html"><a href="choosing-a-matching-strategy.html#selection-criteria"><i class="fa fa-check"></i><b>35.6.3</b> Selection Criteria</a></li>
</ul></li>
<li class="chapter" data-level="35.7" data-path="matching-vs.-regression.html"><a href="matching-vs.-regression.html"><i class="fa fa-check"></i><b>35.7</b> Matching vs. Regression</a>
<ul>
<li class="chapter" data-level="35.7.1" data-path="matching-vs.-regression.html"><a href="matching-vs.-regression.html#matching-estimand"><i class="fa fa-check"></i><b>35.7.1</b> Matching Estimand</a></li>
<li class="chapter" data-level="35.7.2" data-path="matching-vs.-regression.html"><a href="matching-vs.-regression.html#regression-estimand"><i class="fa fa-check"></i><b>35.7.2</b> Regression Estimand</a></li>
<li class="chapter" data-level="35.7.3" data-path="matching-vs.-regression.html"><a href="matching-vs.-regression.html#interpretation-weighting-differences"><i class="fa fa-check"></i><b>35.7.3</b> Interpretation: Weighting Differences</a></li>
</ul></li>
<li class="chapter" data-level="35.8" data-path="software-and-practical-implementation.html"><a href="software-and-practical-implementation.html"><i class="fa fa-check"></i><b>35.8</b> Software and Practical Implementation</a></li>
<li class="chapter" data-level="35.9" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html"><i class="fa fa-check"></i><b>35.9</b> Selection on Observables</a>
<ul>
<li class="chapter" data-level="35.9.1" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#matching-with-matchit"><i class="fa fa-check"></i><b>35.9.1</b> Matching with <code>MatchIt</code></a></li>
<li class="chapter" data-level="35.9.2" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#reporting-standards"><i class="fa fa-check"></i><b>35.9.2</b> Reporting Standards</a></li>
<li class="chapter" data-level="35.9.3" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#optimization-based-matching-via-designmatch"><i class="fa fa-check"></i><b>35.9.3</b> Optimization-Based Matching via <code>designmatch</code></a></li>
<li class="chapter" data-level="35.9.4" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#matchingfrontier"><i class="fa fa-check"></i><b>35.9.4</b> MatchingFrontier</a></li>
<li class="chapter" data-level="35.9.5" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#sec-propensity-scores"><i class="fa fa-check"></i><b>35.9.5</b> Propensity Scores</a></li>
<li class="chapter" data-level="35.9.6" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#sec-mahalanobis"><i class="fa fa-check"></i><b>35.9.6</b> Mahalanobis Distance Matching</a></li>
<li class="chapter" data-level="35.9.7" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#sec-cem"><i class="fa fa-check"></i><b>35.9.7</b> Coarsened Exact Matching (CEM)</a></li>
<li class="chapter" data-level="35.9.8" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#sec-genetic-matching"><i class="fa fa-check"></i><b>35.9.8</b> Genetic Matching</a></li>
<li class="chapter" data-level="35.9.9" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#entropy-balancing"><i class="fa fa-check"></i><b>35.9.9</b> Entropy Balancing</a></li>
<li class="chapter" data-level="35.9.10" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#matching-for-high-dimensional-data"><i class="fa fa-check"></i><b>35.9.10</b> Matching for High-Dimensional Data</a></li>
<li class="chapter" data-level="35.9.11" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#matching-for-multiple-treatments"><i class="fa fa-check"></i><b>35.9.11</b> Matching for Multiple Treatments</a></li>
<li class="chapter" data-level="35.9.12" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#matching-for-multi-level-treatments"><i class="fa fa-check"></i><b>35.9.12</b> Matching for Multi-Level Treatments</a></li>
<li class="chapter" data-level="35.9.13" data-path="sec-selection-on-observables.html"><a href="sec-selection-on-observables.html#matching-for-repeated-treatments-time-varying-treatments"><i class="fa fa-check"></i><b>35.9.13</b> Matching for Repeated Treatments (Time-Varying Treatments)</a></li>
</ul></li>
<li class="chapter" data-level="35.10" data-path="sec-selection-on-unobservables.html"><a href="sec-selection-on-unobservables.html"><i class="fa fa-check"></i><b>35.10</b> Selection on Unobservables</a>
<ul>
<li class="chapter" data-level="35.10.1" data-path="sec-selection-on-unobservables.html"><a href="sec-selection-on-unobservables.html#sec-rosenbaum-bounds"><i class="fa fa-check"></i><b>35.10.1</b> Rosenbaum Bounds</a></li>
<li class="chapter" data-level="35.10.2" data-path="sec-selection-on-unobservables.html"><a href="sec-selection-on-unobservables.html#sec-relative-correlation-restrictions"><i class="fa fa-check"></i><b>35.10.2</b> Relative Correlation Restrictions</a></li>
<li class="chapter" data-level="35.10.3" data-path="sec-selection-on-unobservables.html"><a href="sec-selection-on-unobservables.html#sec-coefficient-stability-bounds"><i class="fa fa-check"></i><b>35.10.3</b> Coefficient-Stability Bounds</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>C. OTHER CONCERNS</b></span></li>
<li class="chapter" data-level="36" data-path="sec-endogeneity.html"><a href="sec-endogeneity.html"><i class="fa fa-check"></i><b>36</b> Endogeneity</a>
<ul>
<li class="chapter" data-level="36.1" data-path="sec-endogenous-treatment.html"><a href="sec-endogenous-treatment.html"><i class="fa fa-check"></i><b>36.1</b> Endogenous Treatment</a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="sec-endogenous-treatment.html"><a href="sec-endogenous-treatment.html#sec-measurement-error"><i class="fa fa-check"></i><b>36.1.1</b> Measurement Errors</a></li>
<li class="chapter" data-level="36.1.2" data-path="sec-endogenous-treatment.html"><a href="sec-endogenous-treatment.html#sec-simultaneity"><i class="fa fa-check"></i><b>36.1.2</b> Simultaneity</a></li>
<li class="chapter" data-level="36.1.3" data-path="sec-endogenous-treatment.html"><a href="sec-endogenous-treatment.html#sec-reverse-causality"><i class="fa fa-check"></i><b>36.1.3</b> Reverse Causality</a></li>
<li class="chapter" data-level="36.1.4" data-path="sec-endogenous-treatment.html"><a href="sec-endogenous-treatment.html#sec-omitted-variable-bias"><i class="fa fa-check"></i><b>36.1.4</b> Omitted Variable Bias</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="sec-endogenous-sample-selection.html"><a href="sec-endogenous-sample-selection.html"><i class="fa fa-check"></i><b>36.2</b> Endogenous Sample Selection</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="sec-endogenous-sample-selection.html"><a href="sec-endogenous-sample-selection.html#unifying-model-frameworks"><i class="fa fa-check"></i><b>36.2.1</b> Unifying Model Frameworks</a></li>
<li class="chapter" data-level="36.2.2" data-path="sec-endogenous-sample-selection.html"><a href="sec-endogenous-sample-selection.html#estimation-methods-2"><i class="fa fa-check"></i><b>36.2.2</b> Estimation Methods</a></li>
<li class="chapter" data-level="36.2.3" data-path="sec-endogenous-sample-selection.html"><a href="sec-endogenous-sample-selection.html#theoretical-connections"><i class="fa fa-check"></i><b>36.2.3</b> Theoretical Connections</a></li>
<li class="chapter" data-level="36.2.4" data-path="sec-endogenous-sample-selection.html"><a href="sec-endogenous-sample-selection.html#tobit-2-heckmans-sample-selection-model"><i class="fa fa-check"></i><b>36.2.4</b> Tobit-2: Heckman’s Sample Selection Model</a></li>
<li class="chapter" data-level="36.2.5" data-path="sec-endogenous-sample-selection.html"><a href="sec-endogenous-sample-selection.html#tobit-5-switching-regression-model"><i class="fa fa-check"></i><b>36.2.5</b> Tobit-5: Switching Regression Model</a></li>
<li class="chapter" data-level="36.2.6" data-path="sec-endogenous-sample-selection.html"><a href="sec-endogenous-sample-selection.html#sec-pattern-mixture-models"><i class="fa fa-check"></i><b>36.2.6</b> Pattern-Mixture Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="other-biases.html"><a href="other-biases.html"><i class="fa fa-check"></i><b>37</b> Other Biases</a>
<ul>
<li class="chapter" data-level="37.1" data-path="sec-aggregation-bias.html"><a href="sec-aggregation-bias.html"><i class="fa fa-check"></i><b>37.1</b> Aggregation Bias</a>
<ul>
<li class="chapter" data-level="37.1.1" data-path="sec-aggregation-bias.html"><a href="sec-aggregation-bias.html#simpsons-paradox-1"><i class="fa fa-check"></i><b>37.1.1</b> Simpson’s Paradox</a></li>
</ul></li>
<li class="chapter" data-level="37.2" data-path="sec-contamination-bias.html"><a href="sec-contamination-bias.html"><i class="fa fa-check"></i><b>37.2</b> Contamination Bias</a></li>
<li class="chapter" data-level="37.3" data-path="sec-survivorship-bias.html"><a href="sec-survivorship-bias.html"><i class="fa fa-check"></i><b>37.3</b> Survivorship Bias</a></li>
<li class="chapter" data-level="37.4" data-path="sec-publication-bias.html"><a href="sec-publication-bias.html"><i class="fa fa-check"></i><b>37.4</b> Publication Bias</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="sec-directed-acyclic-graphs.html"><a href="sec-directed-acyclic-graphs.html"><i class="fa fa-check"></i><b>38</b> Directed Acyclic Graphs</a>
<ul>
<li class="chapter" data-level="38.1" data-path="basic-notation-and-graph-structures.html"><a href="basic-notation-and-graph-structures.html"><i class="fa fa-check"></i><b>38.1</b> Basic Notation and Graph Structures</a></li>
<li class="chapter" data-level="38.2" data-path="rule-of-thumb-for-causal-inference.html"><a href="rule-of-thumb-for-causal-inference.html"><i class="fa fa-check"></i><b>38.2</b> Rule of Thumb for Causal Inference</a></li>
<li class="chapter" data-level="38.3" data-path="example-dag.html"><a href="example-dag.html"><i class="fa fa-check"></i><b>38.3</b> Example DAG</a></li>
<li class="chapter" data-level="38.4" data-path="causal-discovery.html"><a href="causal-discovery.html"><i class="fa fa-check"></i><b>38.4</b> Causal Discovery</a></li>
<li class="chapter" data-level="38.5" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>38.5</b> </a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="sec-controls.html"><a href="sec-controls.html"><i class="fa fa-check"></i><b>39</b> Controls</a>
<ul>
<li class="chapter" data-level="39.1" data-path="bad-controls.html"><a href="bad-controls.html"><i class="fa fa-check"></i><b>39.1</b> Bad Controls</a>
<ul>
<li class="chapter" data-level="39.1.1" data-path="bad-controls.html"><a href="bad-controls.html#m-bias"><i class="fa fa-check"></i><b>39.1.1</b> M-bias</a></li>
<li class="chapter" data-level="39.1.2" data-path="bad-controls.html"><a href="bad-controls.html#bias-amplification-1"><i class="fa fa-check"></i><b>39.1.2</b> Bias Amplification</a></li>
<li class="chapter" data-level="39.1.3" data-path="bad-controls.html"><a href="bad-controls.html#sec-overcontrol-bias"><i class="fa fa-check"></i><b>39.1.3</b> Overcontrol Bias</a></li>
<li class="chapter" data-level="39.1.4" data-path="bad-controls.html"><a href="bad-controls.html#selection-bias"><i class="fa fa-check"></i><b>39.1.4</b> Selection Bias</a></li>
<li class="chapter" data-level="39.1.5" data-path="bad-controls.html"><a href="bad-controls.html#case-control-bias"><i class="fa fa-check"></i><b>39.1.5</b> Case-Control Bias</a></li>
<li class="chapter" data-level="39.1.6" data-path="bad-controls.html"><a href="bad-controls.html#summary-3"><i class="fa fa-check"></i><b>39.1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="39.2" data-path="good-controls.html"><a href="good-controls.html"><i class="fa fa-check"></i><b>39.2</b> Good Controls</a>
<ul>
<li class="chapter" data-level="39.2.1" data-path="good-controls.html"><a href="good-controls.html#omitted-variable-bias-correction"><i class="fa fa-check"></i><b>39.2.1</b> Omitted Variable Bias Correction</a></li>
<li class="chapter" data-level="39.2.2" data-path="good-controls.html"><a href="good-controls.html#omitted-variable-bias-in-mediation-correction"><i class="fa fa-check"></i><b>39.2.2</b> Omitted Variable Bias in Mediation Correction</a></li>
</ul></li>
<li class="chapter" data-level="39.3" data-path="neutral-controls.html"><a href="neutral-controls.html"><i class="fa fa-check"></i><b>39.3</b> Neutral Controls</a>
<ul>
<li class="chapter" data-level="39.3.1" data-path="neutral-controls.html"><a href="neutral-controls.html#good-predictive-controls"><i class="fa fa-check"></i><b>39.3.1</b> Good Predictive Controls</a></li>
<li class="chapter" data-level="39.3.2" data-path="neutral-controls.html"><a href="neutral-controls.html#good-selection-bias"><i class="fa fa-check"></i><b>39.3.2</b> Good Selection Bias</a></li>
<li class="chapter" data-level="39.3.3" data-path="neutral-controls.html"><a href="neutral-controls.html#bad-predictive-controls"><i class="fa fa-check"></i><b>39.3.3</b> Bad Predictive Controls</a></li>
<li class="chapter" data-level="39.3.4" data-path="neutral-controls.html"><a href="neutral-controls.html#bad-selection-bias"><i class="fa fa-check"></i><b>39.3.4</b> Bad Selection Bias</a></li>
<li class="chapter" data-level="39.3.5" data-path="neutral-controls.html"><a href="neutral-controls.html#summary-table-predictive-vs.-causal-utility-of-controls"><i class="fa fa-check"></i><b>39.3.5</b> Summary Table: Predictive vs. Causal Utility of Controls</a></li>
</ul></li>
<li class="chapter" data-level="39.4" data-path="choosing-controls.html"><a href="choosing-controls.html"><i class="fa fa-check"></i><b>39.4</b> Choosing Controls</a>
<ul>
<li class="chapter" data-level="39.4.1" data-path="choosing-controls.html"><a href="choosing-controls.html#step-1-use-a-causal-diagram-dag"><i class="fa fa-check"></i><b>39.4.1</b> Step 1: Use a Causal Diagram (DAG)</a></li>
<li class="chapter" data-level="39.4.2" data-path="choosing-controls.html"><a href="choosing-controls.html#step-2-use-algorithmic-tools"><i class="fa fa-check"></i><b>39.4.2</b> Step 2: Use Algorithmic Tools</a></li>
<li class="chapter" data-level="39.4.3" data-path="choosing-controls.html"><a href="choosing-controls.html#step-3-theoretical-principles"><i class="fa fa-check"></i><b>39.4.3</b> Step 3: Theoretical Principles</a></li>
<li class="chapter" data-level="39.4.4" data-path="choosing-controls.html"><a href="choosing-controls.html#step-4-consider-sensitivity-analysis"><i class="fa fa-check"></i><b>39.4.4</b> Step 4: Consider Sensitivity Analysis</a></li>
<li class="chapter" data-level="39.4.5" data-path="choosing-controls.html"><a href="choosing-controls.html#step-5-know-when-not-to-control"><i class="fa fa-check"></i><b>39.4.5</b> Step 5: Know When Not to Control</a></li>
<li class="chapter" data-level="39.4.6" data-path="choosing-controls.html"><a href="choosing-controls.html#summary-control-selection-pipeline"><i class="fa fa-check"></i><b>39.4.6</b> Summary: Control Selection Pipeline</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V. MISCELLANEOUS</b></span></li>
<li class="chapter" data-level="40" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>40</b> Report</a>
<ul>
<li class="chapter" data-level="40.1" data-path="one-summary-table.html"><a href="one-summary-table.html"><i class="fa fa-check"></i><b>40.1</b> One summary table</a></li>
<li class="chapter" data-level="40.2" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>40.2</b> Model Comparison</a></li>
<li class="chapter" data-level="40.3" data-path="changes-in-an-estimate.html"><a href="changes-in-an-estimate.html"><i class="fa fa-check"></i><b>40.3</b> Changes in an estimate</a></li>
<li class="chapter" data-level="40.4" data-path="standard-errors-2.html"><a href="standard-errors-2.html"><i class="fa fa-check"></i><b>40.4</b> Standard Errors</a></li>
<li class="chapter" data-level="40.5" data-path="coefficient-uncertainty-and-distribution.html"><a href="coefficient-uncertainty-and-distribution.html"><i class="fa fa-check"></i><b>40.5</b> Coefficient Uncertainty and Distribution</a></li>
<li class="chapter" data-level="40.6" data-path="descriptive-tables.html"><a href="descriptive-tables.html"><i class="fa fa-check"></i><b>40.6</b> Descriptive Tables</a></li>
<li class="chapter" data-level="40.7" data-path="visualizations-and-plots.html"><a href="visualizations-and-plots.html"><i class="fa fa-check"></i><b>40.7</b> Visualizations and Plots</a></li>
</ul></li>
<li class="chapter" data-level="41" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>41</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="42" data-path="sensitivity-analysis-robustness-check.html"><a href="sensitivity-analysis-robustness-check.html"><i class="fa fa-check"></i><b>42</b> Sensitivity Analysis/ Robustness Check</a>
<ul>
<li class="chapter" data-level="42.1" data-path="specification-curve.html"><a href="specification-curve.html"><i class="fa fa-check"></i><b>42.1</b> Specification curve</a>
<ul>
<li class="chapter" data-level="42.1.1" data-path="specification-curve.html"><a href="specification-curve.html#starbility"><i class="fa fa-check"></i><b>42.1.1</b> starbility</a></li>
<li class="chapter" data-level="42.1.2" data-path="specification-curve.html"><a href="specification-curve.html#rdfanalysis"><i class="fa fa-check"></i><b>42.1.2</b> rdfanalysis</a></li>
</ul></li>
<li class="chapter" data-level="42.2" data-path="coefficient-stability.html"><a href="coefficient-stability.html"><i class="fa fa-check"></i><b>42.2</b> Coefficient stability</a></li>
<li class="chapter" data-level="42.3" data-path="omitted-variable-bias-quantification.html"><a href="omitted-variable-bias-quantification.html"><i class="fa fa-check"></i><b>42.3</b> Omitted Variable Bias Quantification</a></li>
</ul></li>
<li class="chapter" data-level="43" data-path="replication-and-synthetic-data.html"><a href="replication-and-synthetic-data.html"><i class="fa fa-check"></i><b>43</b> Replication and Synthetic Data</a>
<ul>
<li class="chapter" data-level="43.1" data-path="the-replication-standard.html"><a href="the-replication-standard.html"><i class="fa fa-check"></i><b>43.1</b> The Replication Standard</a>
<ul>
<li class="chapter" data-level="43.1.1" data-path="the-replication-standard.html"><a href="the-replication-standard.html#solutions-for-empirical-replication"><i class="fa fa-check"></i><b>43.1.1</b> Solutions for Empirical Replication</a></li>
<li class="chapter" data-level="43.1.2" data-path="the-replication-standard.html"><a href="the-replication-standard.html#free-data-repositories"><i class="fa fa-check"></i><b>43.1.2</b> Free Data Repositories</a></li>
<li class="chapter" data-level="43.1.3" data-path="the-replication-standard.html"><a href="the-replication-standard.html#exceptions-to-replication"><i class="fa fa-check"></i><b>43.1.3</b> Exceptions to Replication</a></li>
<li class="chapter" data-level="43.1.4" data-path="the-replication-standard.html"><a href="the-replication-standard.html#replication-landscape"><i class="fa fa-check"></i><b>43.1.4</b> Replication Landscape</a></li>
</ul></li>
<li class="chapter" data-level="43.2" data-path="synthetic-data.html"><a href="synthetic-data.html"><i class="fa fa-check"></i><b>43.2</b> Synthetic Data</a>
<ul>
<li class="chapter" data-level="43.2.1" data-path="synthetic-data.html"><a href="synthetic-data.html#benefits-of-synthetic-data"><i class="fa fa-check"></i><b>43.2.1</b> Benefits of Synthetic Data</a></li>
<li class="chapter" data-level="43.2.2" data-path="synthetic-data.html"><a href="synthetic-data.html#concerns-and-limitations"><i class="fa fa-check"></i><b>43.2.2</b> Concerns and Limitations</a></li>
<li class="chapter" data-level="43.2.3" data-path="synthetic-data.html"><a href="synthetic-data.html#further-insights-on-synthetic-data"><i class="fa fa-check"></i><b>43.2.3</b> Further Insights on Synthetic Data</a></li>
<li class="chapter" data-level="43.2.4" data-path="synthetic-data.html"><a href="synthetic-data.html#generating-synthetic-data"><i class="fa fa-check"></i><b>43.2.4</b> Generating Synthetic Data</a></li>
</ul></li>
<li class="chapter" data-level="43.3" data-path="application-5.html"><a href="application-5.html"><i class="fa fa-check"></i><b>43.3</b> Application</a>
<ul>
<li class="chapter" data-level="43.3.1" data-path="application-5.html"><a href="application-5.html#original-dataset"><i class="fa fa-check"></i><b>43.3.1</b> Original Dataset</a></li>
<li class="chapter" data-level="43.3.2" data-path="application-5.html"><a href="application-5.html#restricted-dataset"><i class="fa fa-check"></i><b>43.3.2</b> Restricted Dataset</a></li>
<li class="chapter" data-level="43.3.3" data-path="application-5.html"><a href="application-5.html#synthpop"><i class="fa fa-check"></i><b>43.3.3</b> Synthpop</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="44" data-path="high-performance-computing.html"><a href="high-performance-computing.html"><i class="fa fa-check"></i><b>44</b> High-Performance Computing</a>
<ul>
<li class="chapter" data-level="44.1" data-path="best-practices-for-hpc-in-data-analysis.html"><a href="best-practices-for-hpc-in-data-analysis.html"><i class="fa fa-check"></i><b>44.1</b> Best Practices for HPC in Data Analysis</a></li>
<li class="chapter" data-level="44.2" data-path="example-workflow-in-r.html"><a href="example-workflow-in-r.html"><i class="fa fa-check"></i><b>44.2</b> Example Workflow in R</a></li>
<li class="chapter" data-level="44.3" data-path="recommendations.html"><a href="recommendations.html"><i class="fa fa-check"></i><b>44.3</b> Recommendations</a></li>
<li class="chapter" data-level="44.4" data-path="demonstration.html"><a href="demonstration.html"><i class="fa fa-check"></i><b>44.4</b> Demonstration</a></li>
</ul></li>
<li class="appendix"><span><b>APPENDIX</b></span></li>
<li class="chapter" data-level="A" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>A</b> Appendix</a>
<ul>
<li class="chapter" data-level="A.1" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>A.1</b> Git</a></li>
<li class="chapter" data-level="A.2" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>A.2</b> Short-cut</a></li>
<li class="chapter" data-level="A.3" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>A.3</b> Function short-cut</a></li>
<li class="chapter" data-level="A.4" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>A.4</b> Citation</a></li>
<li class="chapter" data-level="A.5" data-path="install-all-necessary-packageslibaries-on-your-local-machine.html"><a href="install-all-necessary-packageslibaries-on-your-local-machine.html"><i class="fa fa-check"></i><b>A.5</b> Install all necessary packages/libaries on your local machine</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>B</b> Bookdown cheat sheet</a>
<ul>
<li class="chapter" data-level="B.1" data-path="operation.html"><a href="operation.html"><i class="fa fa-check"></i><b>B.1</b> Operation</a></li>
<li class="chapter" data-level="B.2" data-path="math-expression-syntax.html"><a href="math-expression-syntax.html"><i class="fa fa-check"></i><b>B.2</b> Math Expression/ Syntax</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="math-expression-syntax.html"><a href="math-expression-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>B.2.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>B.3</b> Table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ordinary-least-squares" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Ordinary Least Squares<a href="ordinary-least-squares.html#ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ordinary Least Squares (OLS) is the backbone of statistical modeling, a method so foundational that it often serves as the starting point for understanding data relationships. Whether predicting sales, estimating economic trends, or uncovering patterns in scientific research, OLS remains a critical tool. Its appeal lies in simplicity: OLS models the relationship between a dependent variable and one or more predictors by minimizing the squared differences between observed and predicted values.</p>
<hr />
<p><strong>Why OLS Works: Linear and Nonlinear Relationships</strong></p>
<p><a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> rests on the Conditional Expectation Function (CEF), <span class="math inline">\(E[Y | X]\)</span>, which describes the expected value of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Regression shines in two key scenarios:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Perfect Fit (Linear CEF):</strong><br />
If <span class="math inline">\(E[Y_i | X_{1i}, \dots, X_{Ki}] = a + \sum_{k=1}^K b_k X_{ki}\)</span>, the regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(X_{1i}, \dots, X_{Ki}\)</span> exactly equals the CEF. In other words, the regression gives the true average relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.<br />
If the true relationship is linear, regression delivers the exact CEF. For instance, imagine you’re estimating the relationship between advertising spend and sales revenue. If the true impact is linear, <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> will perfectly capture it.</p></li>
<li><p><strong>Approximation (Nonlinear CEF):</strong><br />
If <span class="math inline">\(E[Y_i | X_{1i}, \dots, X_{Ki}]\)</span> is nonlinear, <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> provides the best linear approximation to this relationship. Specifically, it minimizes the expected squared deviation between the linear regression line and the nonlinear CEF.<br />
For example, the effect of advertising diminishes at higher spending levels? <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> still works, providing the best linear approximation to this nonlinear relationship by minimizing the squared deviations between predictions and the true (but unknown) CEF.</p></li>
</ol>
<p>In other words, regression is not just a tool for “linear” relationships—it’s a workhorse that adapts remarkably well to messy, real-world data.</p>
<p><img src="images/meme-linear-regerssion.jpg" style="width:80.0%" /></p>
<hr />
<div id="simple-regression-basic-model" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Simple Regression (Basic) Model<a href="ordinary-least-squares.html#simple-regression-basic-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The simplest form of regression is a straight line:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(Y_i\)</span>: The dependent variable or outcome we’re trying to predict (e.g., sales, temperature).</li>
<li><span class="math inline">\(X_i\)</span>: The independent variable or predictor (e.g., advertising spend, time).</li>
<li><span class="math inline">\(\beta_0\)</span>: The intercept—where the line crosses the <span class="math inline">\(Y\)</span>-axis when <span class="math inline">\(X = 0\)</span>.</li>
<li><span class="math inline">\(\beta_1\)</span>: The slope, representing the change in <span class="math inline">\(Y\)</span> for a one-unit increase in <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(\epsilon_i\)</span>: The error term, accounting for random factors that <span class="math inline">\(X\)</span> cannot explain.</li>
</ul>
<p>Assumptions About the Error Term (<span class="math inline">\(\epsilon_i\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
E(\epsilon_i) &amp;= 0 \\
\text{Var}(\epsilon_i) &amp;= \sigma^2 \\
\text{Cov}(\epsilon_i, \epsilon_j) &amp;= 0 \quad \text{for all } i \neq j
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\epsilon_i\)</span> is random, <span class="math inline">\(Y_i\)</span> is also random:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_i) &amp;= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&amp;= \beta_0 + \beta_1 X_i
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(Y_i) &amp;= \text{Var}(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&amp;= \text{Var}(\epsilon_i) \\
&amp;= \sigma^2
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\text{Cov}(\epsilon_i, \epsilon_j) = 0\)</span>, the outcomes across observations are independent. Hence, <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> are uncorrelated as well, conditioned on the <span class="math inline">\(X\)</span>’s.</p>
<hr />
<div id="estimation-in-ordinary-least-squares" class="section level4 hasAnchor" number="5.1.1.1">
<h4><span class="header-section-number">5.1.1.1</span> Estimation in Ordinary Least Squares<a href="ordinary-least-squares.html#estimation-in-ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The goal of <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> is to estimate the regression parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>) that best describe the relationship between the dependent variable <span class="math inline">\(Y\)</span> and the independent variable <span class="math inline">\(X\)</span>. To achieve this, we minimize the sum of squared deviations between observed values of <span class="math inline">\(Y_i\)</span> and their expected values predicted by the model.</p>
<p>The deviation of an observed value <span class="math inline">\(Y_i\)</span> from its expected value, based on the regression model, is:</p>
<p><span class="math display">\[
Y_i - E(Y_i) = Y_i - (\beta_0 + \beta_1 X_i).
\]</span></p>
<p>This deviation represents the error in prediction for the <span class="math inline">\(i\)</span>-th observation.</p>
<p>To ensure that the errors don’t cancel each other out and to prioritize larger deviations, we consider the squared deviations. The sum of squared deviations, denoted by <span class="math inline">\(Q\)</span>, is defined as:</p>
<p><span class="math display">\[
Q = \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2.
\]</span></p>
<p>The goal of <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> is to find the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize <span class="math inline">\(Q\)</span>. These values are called the <strong>OLS estimators</strong>.</p>
<p>To minimize <span class="math inline">\(Q\)</span>, we take partial derivatives with respect to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, set them to zero, and solve the resulting system of equations. After simplifying, the estimators for the slope (<span class="math inline">\(b_1\)</span>) and intercept (<span class="math inline">\(b_0\)</span>) are obtained as follows:</p>
<p>Slope (<span class="math inline">\(b_1\)</span>):</p>
<p><span class="math display">\[
b_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>Here, <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{Y}\)</span> represent the means of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively. This formula reveals that the slope is proportional to the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, scaled by the variance of <span class="math inline">\(X\)</span>.</p>
<p>Intercept (<span class="math inline">\(b_0\)</span>):</p>
<p><span class="math display">\[
b_0 = \frac{1}{n} \left( \sum_{i=1}^{n} Y_i - b_1 \sum_{i=1}^{n} X_i \right) = \bar{Y} - b_1 \bar{X}.
\]</span></p>
<p>The intercept is determined by aligning the regression line with the center of the data.</p>
<hr />
<p><strong>Intuition Behind the Estimators</strong></p>
<ul>
<li><p><span class="math inline">\(b_1\)</span> (Slope): This measures the average change in <span class="math inline">\(Y\)</span> for a one-unit increase in <span class="math inline">\(X\)</span>. The formula uses deviations from the mean to ensure that the relationship captures the joint variability of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p><span class="math inline">\(b_0\)</span> (Intercept): This ensures that the regression line passes through the mean of the data points <span class="math inline">\((\bar{X}, \bar{Y})\)</span>, anchoring the model in the center of the observed data.</p></li>
</ul>
<hr />
<p>Equivalently, we can also write these parameters in terms of covariances.</p>
<p>The covariance between two variables is defined as:</p>
<p><span class="math display">\[ \text{Cov}(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])] \]</span></p>
<p>Properties of Covariance:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{Cov}(X_i, X_i) = \sigma^2_X\)</span></li>
<li>If <span class="math inline">\(E(X_i) = 0\)</span> or <span class="math inline">\(E(Y_i) = 0\)</span>, then <span class="math inline">\(\text{Cov}(X_i, Y_i) = E[X_i Y_i]\)</span></li>
<li>For <span class="math inline">\(W_i = a + b X_i\)</span> and <span class="math inline">\(Z_i = c + d Y_i\)</span>,<br />
<span class="math inline">\(\text{Cov}(W_i, Z_i) = bd \cdot \text{Cov}(X_i, Y_i)\)</span></li>
</ol>
<p>For a bivariate regression, the slope <span class="math inline">\(\beta\)</span> in a bivariate regression is given by:</p>
<p><span class="math display">\[ \beta = \frac{\text{Cov}(Y_i, X_i)}{\text{Var}(X_i)} \]</span></p>
<p>For a multivariate case, the slope for <span class="math inline">\(X_k\)</span> is:</p>
<p><span class="math display">\[ \beta_k = \frac{\text{Cov}(Y_i, \tilde{X}_{ki})}{\text{Var}(\tilde{X}_{ki})} \]</span></p>
<p>Where <span class="math inline">\(\tilde{X}_{ki}\)</span> represents the residual from a regression of <span class="math inline">\(X_{ki}\)</span> on the <span class="math inline">\(K-1\)</span> other covariates in the model.</p>
<p>The intercept is:</p>
<p><span class="math display">\[ \beta_0 = E[Y_i] - \beta_1 E(X_i) \]</span></p>
<p>Note:</p>
<ul>
<li><a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> does not require the assumption of a specific distribution for the variables. Its robustness is based on the minimization of squared errors (i.e., no distributional assumptions).</li>
</ul>
</div>
<div id="properties-of-least-squares-estimators" class="section level4 hasAnchor" number="5.1.1.2">
<h4><span class="header-section-number">5.1.1.2</span> Properties of Least Squares Estimators<a href="ordinary-least-squares.html#properties-of-least-squares-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The properties of the Ordinary Least Squares estimators (<span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>) are derived based on their statistical behavior. These properties provide insights into the accuracy, variability, and reliability of the estimates.</p>
<hr />
<div id="expectation-of-the-ols-estimators" class="section level5 hasAnchor" number="5.1.1.2.1">
<h5><span class="header-section-number">5.1.1.2.1</span> Expectation of the OLS Estimators<a href="ordinary-least-squares.html#expectation-of-the-ols-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The OLS estimators <span class="math inline">\(b_0\)</span> (intercept) and <span class="math inline">\(b_1\)</span> (slope) are unbiased. This means their expected values equal the true population parameters:</p>
<p><span class="math display">\[
\begin{aligned}
E(b_1) &amp;= \beta_1, \\
E(b_0) &amp;= E(\bar{Y}) - \bar{X}\beta_1.
\end{aligned}
\]</span></p>
<p>Since the expected value of the sample mean of <span class="math inline">\(Y\)</span>, <span class="math inline">\(E(\bar{Y})\)</span>, is:</p>
<p><span class="math display">\[
E(\bar{Y}) = \beta_0 + \beta_1 \bar{X},
\]</span></p>
<p>the expected value of <span class="math inline">\(b_0\)</span> simplifies to:</p>
<p><span class="math display">\[
E(b_0) = \beta_0.
\]</span></p>
<p>Thus, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are unbiased estimators of their respective population parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<hr />
</div>
<div id="variance-of-the-ols-estimators" class="section level5 hasAnchor" number="5.1.1.2.2">
<h5><span class="header-section-number">5.1.1.2.2</span> Variance of the OLS Estimators<a href="ordinary-least-squares.html#variance-of-the-ols-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The variability of the OLS estimators depends on the spread of the predictor variable <span class="math inline">\(X\)</span> and the error variance <span class="math inline">\(\sigma^2\)</span>. The variances are given by:</p>
<p>Variance of <span class="math inline">\(b_1\)</span> (Slope):</p>
<p><span class="math display">\[
\text{Var}(b_1) = \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>Variance of <span class="math inline">\(b_0\)</span> (Intercept):</p>
<p><span class="math display">\[
\text{Var}(b_0) = \sigma^2 \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>These formulas highlight that:</p>
<ul>
<li><span class="math inline">\(\text{Var}(b_1) \to 0\)</span> as the number of observations increases, provided <span class="math inline">\(X_i\)</span> values are distributed around their mean <span class="math inline">\(\bar{X}\)</span>.</li>
<li><span class="math inline">\(\text{Var}(b_0) \to 0\)</span> as <span class="math inline">\(n\)</span> increases, assuming <span class="math inline">\(X_i\)</span> values are appropriately selected (i.e., not all clustered near the mean).</li>
</ul>
<hr />
</div>
</div>
<div id="mean-square-error-mse" class="section level4 hasAnchor" number="5.1.1.3">
<h4><span class="header-section-number">5.1.1.3</span> Mean Square Error (MSE)<a href="ordinary-least-squares.html#mean-square-error-mse" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Mean Square Error (MSE) quantifies the average squared residual (error) in the model:</p>
<p><span class="math display">\[
MSE = \frac{SSE}{n-2} = \frac{\sum_{i=1}^{n} e_i^2}{n-2} = \frac{\sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2}{n-2},
\]</span></p>
<p>where <span class="math inline">\(SSE\)</span> is the Sum of Squared Errors and <span class="math inline">\(n-2\)</span> represents the degrees of freedom for a simple linear regression model (two parameters estimated: <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>).</p>
<p>The expected value of the MSE equals the error variance (i.e., unbiased Estimator of MSE:):</p>
<p><span class="math display">\[
E(MSE) = \sigma^2.
\]</span></p>
<hr />
</div>
<div id="estimating-variance-of-the-ols-coefficients" class="section level4 hasAnchor" number="5.1.1.4">
<h4><span class="header-section-number">5.1.1.4</span> Estimating Variance of the OLS Coefficients<a href="ordinary-least-squares.html#estimating-variance-of-the-ols-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The sample-based estimates of the variances of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are expressed as follows:</p>
<p>Estimated Variance of <span class="math inline">\(b_1\)</span> (Slope):</p>
<p><span class="math display">\[
s^2(b_1) = \widehat{\text{Var}}(b_1) = \frac{MSE}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>Estimated Variance of <span class="math inline">\(b_0\)</span> (Intercept):</p>
<p><span class="math display">\[
s^2(b_0) = \widehat{\text{Var}}(b_0) = MSE \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>These estimates rely on the MSE to approximate <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The variance estimates are unbiased:</p>
<p><span class="math display">\[
\begin{aligned}
E(s^2(b_1)) &amp;= \text{Var}(b_1), \\
E(s^2(b_0)) &amp;= \text{Var}(b_0).
\end{aligned}
\]</span></p>
<hr />
<p><strong>Implications of These Properties</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Unbiasedness:</strong> The unbiased nature of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> ensures that, on average, the regression model accurately reflects the true relationship in the population.</li>
<li><strong>Decreasing Variance:</strong> As the sample size <span class="math inline">\(n\)</span> increases or as the spread of <span class="math inline">\(X_i\)</span> values grows, the variances of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> decrease, leading to more precise estimates.</li>
<li><strong>Error Estimation with MSE:</strong> MSE provides a reliable estimate of the error variance <span class="math inline">\(\sigma^2\)</span>, which feeds directly into assessing the reliability of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>.</li>
</ol>
</div>
<div id="residuals-in-ordinary-least-squares" class="section level4 hasAnchor" number="5.1.1.5">
<h4><span class="header-section-number">5.1.1.5</span> Residuals in Ordinary Least Squares<a href="ordinary-least-squares.html#residuals-in-ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Residuals are the differences between observed values (<span class="math inline">\(Y_i\)</span>) and their predicted counterparts (<span class="math inline">\(\hat{Y}_i\)</span>). They play a central role in assessing model fit and ensuring the assumptions of OLS are met.</p>
<p>The residual for the <span class="math inline">\(i\)</span>-th observation is defined as:</p>
<p><span class="math display">\[
e_i = Y_i - \hat{Y}_i = Y_i - (b_0 + b_1 X_i),
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(e_i\)</span>: Residual for the <span class="math inline">\(i\)</span>-th observation.</li>
<li><span class="math inline">\(\hat{Y}_i\)</span>: Predicted value based on the regression model.</li>
<li><span class="math inline">\(Y_i\)</span>: Actual observed value.</li>
</ul>
<p>Residuals estimate the unobservable error terms <span class="math inline">\(\epsilon_i\)</span>:</p>
<ul>
<li><span class="math inline">\(e_i\)</span> is an estimate of <span class="math inline">\(\epsilon_i = Y_i - E(Y_i)\)</span>.</li>
<li><span class="math inline">\(\epsilon_i\)</span> is always unknown because we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
</ul>
<hr />
<div id="key-properties-of-residuals" class="section level5 hasAnchor" number="5.1.1.5.1">
<h5><span class="header-section-number">5.1.1.5.1</span> Key Properties of Residuals<a href="ordinary-least-squares.html#key-properties-of-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Residuals exhibit several mathematical properties that align with the OLS estimation process:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Sum of Residuals</strong>:<br />
The residuals sum to zero:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} e_i = 0.
\]</span></p>
<p>This ensures that the regression line passes through the centroid of the data, <span class="math inline">\((\bar{X}, \bar{Y})\)</span>.</p></li>
<li><p><strong>Orthogonality of Residuals to Predictors</strong>:<br />
The residuals are orthogonal (uncorrelated) to the predictor variable <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} X_i e_i = 0.
\]</span></p>
<p>This reflects the fact that the OLS minimizes the squared deviations of residuals along the <span class="math inline">\(Y\)</span>-axis, not the <span class="math inline">\(X\)</span>-axis.</p></li>
</ol>
<hr />
</div>
<div id="expected-values-of-residuals" class="section level5 hasAnchor" number="5.1.1.5.2">
<h5><span class="header-section-number">5.1.1.5.2</span> Expected Values of Residuals<a href="ordinary-least-squares.html#expected-values-of-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The expected values of residuals reinforce the unbiased nature of OLS:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Mean of Residuals</strong>:<br />
The residuals have an expected value of zero:</p>
<p><span class="math display">\[
E[e_i] = 0.
\]</span></p></li>
<li><p><strong>Orthogonality to Predictors and Fitted Values</strong>:<br />
Residuals are uncorrelated with both the predictor variables and the fitted values:</p>
<p><span class="math display">\[
\begin{aligned}
E[X_i e_i] &amp;= 0, \\
E[\hat{Y}_i e_i] &amp;= 0.
\end{aligned}
\]</span></p></li>
</ol>
<p>These properties highlight that residuals do not contain systematic information about the predictors or the fitted values, reinforcing the idea that the model has captured the underlying relationship effectively.</p>
<hr />
</div>
<div id="practical-importance-of-residuals" class="section level5 hasAnchor" number="5.1.1.5.3">
<h5><span class="header-section-number">5.1.1.5.3</span> Practical Importance of Residuals<a href="ordinary-least-squares.html#practical-importance-of-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li><p><strong>Model Diagnostics:</strong><br />
Residuals are analyzed to check the assumptions of OLS, including linearity, homoscedasticity (constant variance), and independence of errors. Patterns in residual plots can signal issues such as nonlinearity or heteroscedasticity.</p></li>
<li><p><strong>Goodness-of-Fit:</strong><br />
The sum of squared residuals, <span class="math inline">\(\sum e_i^2\)</span>, measures the total unexplained variation in <span class="math inline">\(Y\)</span>. A smaller sum indicates a better fit.</p></li>
<li><p><strong>Influence Analysis:</strong><br />
Large residuals may indicate outliers or influential points that disproportionately affect the regression line.</p></li>
</ol>
</div>
</div>
<div id="inference-in-ordinary-least-squares" class="section level4 hasAnchor" number="5.1.1.6">
<h4><span class="header-section-number">5.1.1.6</span> Inference in Ordinary Least Squares<a href="ordinary-least-squares.html#inference-in-ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Inference allows us to make probabilistic statements about the regression parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>) and predictions (<span class="math inline">\(Y_h\)</span>). To perform valid inference, certain assumptions about the distribution of errors are necessary.</p>
<hr />
<p>Normality Assumption</p>
<ul>
<li>OLS estimation itself does <strong>not</strong> require the assumption of normality.</li>
<li>However, to conduct hypothesis tests or construct confidence intervals for <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and predictions, distributional assumptions are necessary.</li>
<li>Inference on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is <strong>robust</strong> to moderate departures from normality, especially in large samples due to the <a href="probability-theory.html#central-limit-theorem">Central Limit Theorem</a>.</li>
<li>Inference on predicted values, <span class="math inline">\(Y_{pred}\)</span>, is more sensitive to normality violations.</li>
</ul>
<hr />
<p>When we assume a <strong>normal error model</strong>, the response variable <span class="math inline">\(Y_i\)</span> is modeled as:</p>
<p><span class="math display">\[
Y_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2),
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>: Mean response</li>
<li><span class="math inline">\(\sigma^2\)</span>: Variance of the errors</li>
</ul>
<p>Under this model, the sampling distributions of the OLS estimators, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, can be derived.</p>
<hr />
<div id="inference-for-beta_1-slope" class="section level5 hasAnchor" number="5.1.1.6.1">
<h5><span class="header-section-number">5.1.1.6.1</span> Inference for <span class="math inline">\(\beta_1\)</span> (Slope)<a href="ordinary-least-squares.html#inference-for-beta_1-slope" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Under the normal error model:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Sampling Distribution of</strong> <span class="math inline">\(b_1\)</span>:</p>
<p><span class="math display">\[
b_1 \sim N\left(\beta_1, \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right).
\]</span></p>
<p>This indicates that <span class="math inline">\(b_1\)</span> is an unbiased estimator of <span class="math inline">\(\beta_1\)</span> with variance proportional to <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p><strong>Test Statistic:</strong></p>
<p><span class="math display">\[
t = \frac{b_1 - \beta_1}{s(b_1)} \sim t_{n-2},
\]</span></p>
<p>where <span class="math inline">\(s(b_1)\)</span> is the standard error of <span class="math inline">\(b_1\)</span>: <span class="math display">\[
s(b_1) = \sqrt{\frac{MSE}{\sum_{i=1}^{n} (X_i - \bar{X})^2}}.
\]</span></p></li>
<li><p><strong>Confidence Interval:</strong></p>
<p>A <span class="math inline">\((1-\alpha) 100\%\)</span> confidence interval for <span class="math inline">\(\beta_1\)</span> is:</p>
<p><span class="math display">\[
b_1 \pm t_{1-\alpha/2; n-2} \cdot s(b_1).
\]</span></p></li>
</ol>
<hr />
</div>
<div id="inference-for-beta_0-intercept" class="section level5 hasAnchor" number="5.1.1.6.2">
<h5><span class="header-section-number">5.1.1.6.2</span> Inference for <span class="math inline">\(\beta_0\)</span> (Intercept)<a href="ordinary-least-squares.html#inference-for-beta_0-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li><p><strong>Sampling Distribution of</strong> <span class="math inline">\(b_0\)</span>:</p>
<p>Under the normal error model, the sampling distribution of <span class="math inline">\(b_0\)</span> is:</p>
<p><span class="math display">\[
b_0 \sim N\left(\beta_0, \sigma^2 \left(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right)\right).
\]</span></p></li>
<li><p><strong>Test Statistic:</strong></p>
<p><span class="math display">\[
t = \frac{b_0 - \beta_0}{s(b_0)} \sim t_{n-2},
\]</span></p>
<p>where <span class="math inline">\(s(b_0)\)</span> is the standard error of <span class="math inline">\(b_0\)</span>: <span class="math display">\[
s(b_0) = \sqrt{MSE \left(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right)}.
\]</span></p></li>
<li><p><strong>Confidence Interval:</strong></p>
<p>A <span class="math inline">\((1-\alpha) 100\%\)</span> confidence interval for <span class="math inline">\(\beta_0\)</span> is:</p>
<p><span class="math display">\[
b_0 \pm t_{1-\alpha/2; n-2} \cdot s(b_0).
\]</span></p></li>
</ol>
<hr />
</div>
<div id="mean-response" class="section level5 hasAnchor" number="5.1.1.6.3">
<h5><span class="header-section-number">5.1.1.6.3</span> Mean Response<a href="ordinary-least-squares.html#mean-response" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In regression, we often estimate the mean response of the dependent variable <span class="math inline">\(Y\)</span> for a given level of the predictor variable <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(X_h\)</span>. This estimation provides a predicted average outcome for a specific value of <span class="math inline">\(X\)</span> based on the fitted regression model.</p>
<ul>
<li>Let <span class="math inline">\(X_h\)</span> represent the level of <span class="math inline">\(X\)</span> for which we want to estimate the mean response.</li>
<li>The mean response when <span class="math inline">\(X = X_h\)</span> is denoted as <span class="math inline">\(E(Y_h)\)</span>.</li>
<li>A point estimator for <span class="math inline">\(E(Y_h)\)</span> is <span class="math inline">\(\hat{Y}_h\)</span>, which is the predicted value from the regression model:</li>
</ul>
<p><span class="math display">\[
\hat{Y}_h = b_0 + b_1 X_h.
\]</span></p>
<p>The estimator <span class="math inline">\(\hat{Y}_h\)</span> is unbiased because its expected value equals the true mean response <span class="math inline">\(E(Y_h)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E(\hat{Y}_h) &amp;= E(b_0 + b_1 X_h) \\
&amp;= \beta_0 + \beta_1 X_h \\
&amp;= E(Y_h).
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\hat{Y}_h\)</span> provides a reliable estimate of the mean response at <span class="math inline">\(X_h\)</span>.</p>
<hr />
<p>The variance of <span class="math inline">\(\hat{Y}_h\)</span> reflects the uncertainty in the estimate of the mean response:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{Y}_h) &amp;= \text{Var}(b_0 + b_1 X_h) \quad\text{(definition of }\hat{Y}_h\text{)}\\[6pt]&amp;= \text{Var}\left((\bar{Y} - b_1 \bar{X}) + b_1 X_h\right)\quad\text{(since } b_0 = \bar{Y} - b_1 \bar{X}\text{)}\\[6pt]&amp;= \text{Var}\left(\bar{Y} + b_1(X_h - \bar{X})\right)\quad\text{(factor out } b_1\text{)}\\[6pt]&amp;= \text{Var}\left(\bar{Y} + b_1 (X_h - \bar{X}) \right) \\
&amp;= \text{Var}(\bar{Y}) + (X_h - \bar{X})^2 \text{Var}(b_1) + 2(X_h - \bar{X}) \text{Cov}(\bar{Y}, b_1).
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\text{Cov}(\bar{Y}, b_1) = 0\)</span> (due to the independence of the errors, <span class="math inline">\(\epsilon_i\)</span>), the variance simplifies to:</p>
<p><span class="math display">\[
\text{Var}(\hat{Y}_h) = \frac{\sigma^2}{n} + (X_h - \bar{X})^2 \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>This can also be expressed as:</p>
<p><span class="math display">\[
\text{Var}(\hat{Y}_h) = \sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>To estimate the variance of <span class="math inline">\(\hat{Y}_h\)</span>, we replace <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(MSE\)</span>, the mean squared error from the regression:</p>
<p><span class="math display">\[
s^2(\hat{Y}_h) = MSE \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<hr />
<p>Under the normal error model, the sampling distribution of <span class="math inline">\(\hat{Y}_h\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{Y}_h &amp;\sim N\left(E(Y_h), \text{Var}(\hat{Y}_h)\right), \\
\frac{\hat{Y}_h - E(Y_h)}{s(\hat{Y}_h)} &amp;\sim t_{n-2}.
\end{aligned}
\]</span></p>
<p>This result follows because <span class="math inline">\(\hat{Y}_h\)</span> is a linear combination of normally distributed random variables, and its variance is estimated using <span class="math inline">\(s^2(\hat{Y}_h)\)</span>.</p>
<hr />
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for the mean response <span class="math inline">\(E(Y_h)\)</span> is given by:</p>
<p><span class="math display">\[
\hat{Y}_h \pm t_{1-\alpha/2; n-2} \cdot s(\hat{Y}_h),
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat{Y}_h\)</span>: Point estimate of the mean response,</li>
<li><span class="math inline">\(s(\hat{Y}_h)\)</span>: Estimated standard error of the mean response,</li>
<li><span class="math inline">\(t_{1-\alpha/2; n-2}\)</span>: Critical value from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</li>
</ul>
<hr />
</div>
<div id="prediction-of-a-new-observation" class="section level5 hasAnchor" number="5.1.1.6.4">
<h5><span class="header-section-number">5.1.1.6.4</span> Prediction of a New Observation<a href="ordinary-least-squares.html#prediction-of-a-new-observation" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>When analyzing regression results, it is important to distinguish between:</p>
<ol style="list-style-type: decimal">
<li><strong>Estimating the mean response</strong> at a particular value of <span class="math inline">\(X\)</span>.</li>
<li><strong>Predicting an individual outcome</strong> for a particular value of <span class="math inline">\(X\)</span>.</li>
</ol>
<hr />
<p>Mean Response vs. Individual Outcome</p>
<ul>
<li><p><strong>Same Point Estimate</strong><br />
The formula for both the estimated mean response and the predicted individual outcome at <span class="math inline">\(X = X_h\)</span> is identical:<br />
<span class="math display">\[
\hat{Y}_{pred} = \hat{Y}_h = b_0 + b_1 X_h.
\]</span></p></li>
<li><p><strong>Different Variance</strong><br />
Although the point estimates are the same, the level of uncertainty differs. When predicting an individual outcome, we must consider not only the uncertainty in estimating the mean response (<span class="math inline">\(\hat{Y}_h\)</span>) but also the additional random variation within the distribution of <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p>Therefore, <strong>prediction intervals</strong> (for individual outcomes) account for more uncertainty and are consequently wider than <strong>confidence intervals</strong> (for the mean response).</p>
<hr />
<p>To predict an individual outcome for a given <span class="math inline">\(X_h\)</span>, we combine the mean response with the random error:</p>
<p><span class="math display">\[
Y_{pred} = \beta_0 + \beta_1 X_h + \epsilon.
\]</span></p>
<p>Using the least squares predictor:</p>
<p><span class="math display">\[
\hat{Y}_{pred} = b_0 + b_1 X_h,
\]</span></p>
<p>since <span class="math inline">\(E(\epsilon) = 0\)</span>.</p>
<hr />
<p>The variance of the predicted value for a new observation, <span class="math inline">\(Y_{pred}\)</span>, includes both:</p>
<ol style="list-style-type: decimal">
<li>Variance of the estimated mean response: <span class="math display">\[
\sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right),
\]</span></li>
<li>Variance of the error term, <span class="math inline">\(\epsilon\)</span>, which is <span class="math inline">\(\sigma^2\)</span>.</li>
</ol>
<p>Thus, the total variance is:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(Y_{pred}) &amp;= \text{Var}(b_0 + b_1 X_h + \epsilon) \\
&amp;= \text{Var}(b_0 + b_1 X_h) + \text{Var}(\epsilon) \\
&amp;= \sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right) + \sigma^2 \\
&amp;= \sigma^2 \left( 1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\end{aligned}
\]</span></p>
<p>We estimate the variance of the prediction using <span class="math inline">\(MSE\)</span>, the mean squared error:</p>
<p><span class="math display">\[
s^2(pred) = MSE \left( 1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>Under the normal error model, the standardized predicted value follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\frac{Y_{pred} - \hat{Y}_h}{s(pred)} \sim t_{n-2}.
\]</span></p>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> prediction interval for <span class="math inline">\(Y_{pred}\)</span> is:</p>
<p><span class="math display">\[
\hat{Y}_{pred} \pm t_{1-\alpha/2; n-2} \cdot s(pred).
\]</span></p>
<hr />
</div>
<div id="confidence-band" class="section level5 hasAnchor" number="5.1.1.6.5">
<h5><span class="header-section-number">5.1.1.6.5</span> Confidence Band<a href="ordinary-least-squares.html#confidence-band" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In regression analysis, we often want to evaluate the uncertainty around the entire regression line, not just at a single value of the predictor variable <span class="math inline">\(X\)</span>. This is achieved using a <strong>confidence band</strong>, which provides a confidence interval for the mean response, <span class="math inline">\(E(Y) = \beta_0 + \beta_1 X\)</span>, over the entire range of <span class="math inline">\(X\)</span> values.</p>
<p>The Working-Hotelling confidence band is a method to construct simultaneous confidence intervals for the regression line. For a given <span class="math inline">\(X_h\)</span>, the confidence band is expressed as:</p>
<p><span class="math display">\[
\hat{Y}_h \pm W s(\hat{Y}_h),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(W^2 = 2F_{1-\alpha; 2, n-2}\)</span>,</p>
<ul>
<li><span class="math inline">\(F_{1-\alpha; 2, n-2}\)</span> is the critical value from the <span class="math inline">\(F\)</span>-distribution with 2 and <span class="math inline">\(n-2\)</span> degrees of freedom.</li>
</ul></li>
<li><p><span class="math inline">\(s(\hat{Y}_h)\)</span> is the standard error of the estimated mean response at <span class="math inline">\(X_h\)</span>:</p>
<p><span class="math display">\[
s^2(\hat{Y}_h) = MSE \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p></li>
</ul>
<hr />
<p><strong>Key Properties of the Confidence Band</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Width of the Interval:</strong>
<ul>
<li>The width of the confidence band changes with <span class="math inline">\(X_h\)</span> because <span class="math inline">\(s(\hat{Y}_h)\)</span> depends on how far <span class="math inline">\(X_h\)</span> is from the mean of <span class="math inline">\(X\)</span> (<span class="math inline">\(\bar{X}\)</span>).</li>
<li>The interval is narrowest at <span class="math inline">\(X = \bar{X}\)</span>, where the variance of the estimated mean response is minimized.</li>
</ul></li>
<li><strong>Shape of the Band:</strong>
<ul>
<li>The boundaries of the confidence band form a hyperbolic shape around the regression line.</li>
<li>This reflects the increasing uncertainty in the mean response as <span class="math inline">\(X_h\)</span> moves farther from <span class="math inline">\(\bar{X}\)</span>.</li>
</ul></li>
<li><strong>Simultaneous Coverage:</strong>
<ul>
<li>The Working-Hotelling band ensures that the true regression line <span class="math inline">\(E(Y) = \beta_0 + \beta_1 X\)</span> lies within the band across all values of <span class="math inline">\(X\)</span> with a specified confidence level (e.g., <span class="math inline">\(95\%\)</span>).</li>
</ul></li>
</ol>
</div>
</div>
<div id="analysis-of-variance-anova-in-regression" class="section level4 hasAnchor" number="5.1.1.7">
<h4><span class="header-section-number">5.1.1.7</span> Analysis of Variance (ANOVA) in Regression<a href="ordinary-least-squares.html#analysis-of-variance-anova-in-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>ANOVA in regression decomposes the total variability in the response variable (<span class="math inline">\(Y\)</span>) into components attributed to the regression model and residual error. In the context of regression, ANOVA provides a mechanism to assess the fit of the model and test hypotheses about the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>The <strong>corrected Total Sum of Squares (SSTO)</strong> quantifies the total variation in <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
SSTO = \sum_{i=1}^n (Y_i - \bar{Y})^2,
\]</span></p>
<p>where <span class="math inline">\(\bar{Y}\)</span> is the mean of the response variable. The term “corrected” refers to the fact that the sum of squares is calculated relative to the mean (i.e., the uncorrected total sum of squares is given by <span class="math inline">\(\sum Y_i^2\)</span>)</p>
<p>Using the fitted regression model <span class="math inline">\(\hat{Y}_i = b_0 + b_1 X_i\)</span>, we estimate the conditional mean of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X_i\)</span>. The total sum of squares can be decomposed as:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \bar{Y})^2 &amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y})^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2 + 2 \sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y}) \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2
\end{aligned}
\]</span></p>
<ul>
<li>The cross-product term is zero, as shown below.</li>
<li>This decomposition simplifies to:</li>
</ul>
<p><span class="math display">\[
SSTO = SSE + SSR,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(SSE = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2\)</span>: Error Sum of Squares (variation unexplained by the model).</p></li>
<li><p><span class="math inline">\(SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2\)</span>: Regression Sum of Squares (variation explained by the model), which measure how the conditional mean varies about a central value.</p></li>
</ul>
<p>Degrees of freedom are partitioned as:</p>
<p><span class="math display">\[
\begin{aligned}
SSTO &amp;= SSR + SSE \\
(n-1) &amp;= (1) + (n-2) \\
\end{aligned}
\]</span></p>
<hr />
<p>To confirm that the cross-product term is zero:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y})
&amp;= \sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))(\bar{Y} + b_1 (X_i - \bar{X})-\bar{Y}) \quad \text{(Expand } Y_i - \hat{Y}_i \text{ and } \hat{Y}_i - \bar{Y}\text{)} \\
&amp;=\sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))( b_1 (X_i - \bar{X}))  \\
&amp;= b_1 \sum_{i=1}^n (Y_i - \bar{Y})(X_i - \bar{X}) - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 \quad \text{(Distribute terms in the product)} \\
&amp;= b_1 \frac{\sum_{i=1}^n (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^n (X_i - \bar{X})^2} \sum_{i=1}^n (X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 \quad \text{(Substitute } b_1 \text{ definition)} \\
&amp;= b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2  \\
&amp;= 0
\end{aligned}
\]</span></p>
<hr />
<p>The ANOVA table summarizes the partitioning of variability:</p>
<table>
<colgroup>
<col width="22%" />
<col width="17%" />
<col width="7%" />
<col width="27%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of Variation</th>
<th>Sum of Squares</th>
<th>df</th>
<th>Mean Square</th>
<th><span class="math inline">\(F\)</span> Statistic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression (Model)</td>
<td><span class="math inline">\(SSR\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(MSR = \frac{SSR}{1}\)</span></td>
<td><span class="math inline">\(F = \frac{MSR}{MSE}\)</span></td>
</tr>
<tr class="even">
<td>Error</td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(n-2\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{n-2}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total (Corrected)</td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(n-1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<hr />
<p>The expected values of the mean squares are:</p>
<p><span class="math display">\[
\begin{aligned}
E(MSE) &amp;= \sigma^2, \\
E(MSR) &amp;= \sigma^2 + \beta_1^2 \sum_{i=1}^n (X_i - \bar{X})^2.
\end{aligned}
\]</span></p>
<ul>
<li><strong>If</strong> <span class="math inline">\(\beta_1 = 0\)</span>:
<ul>
<li>The regression model does not explain any variation in <span class="math inline">\(Y\)</span> beyond the mean, and <span class="math inline">\(E(MSR) = E(MSE) = \sigma^2\)</span>.</li>
<li>This condition corresponds to the null hypothesis, <span class="math inline">\(H_0: \beta_1 = 0\)</span>.</li>
</ul></li>
<li><strong>If</strong> <span class="math inline">\(\beta_1 \neq 0\)</span>:
<ul>
<li>The regression model explains some variation in <span class="math inline">\(Y\)</span>, and <span class="math inline">\(E(MSR) &gt; E(MSE)\)</span>.</li>
<li>The additional term <span class="math inline">\(\beta_1^2 \sum_{i=1}^{n} (X_i - \bar{X})^2\)</span> represents the variance explained by the predictor <span class="math inline">\(X\)</span>.</li>
</ul></li>
</ul>
<p>The difference between <span class="math inline">\(E(MSR)\)</span> and <span class="math inline">\(E(MSE)\)</span> allows us to infer whether <span class="math inline">\(\beta_1 \neq 0\)</span> by comparing their ratio.</p>
<hr />
<p>Assuming the errors <span class="math inline">\(\epsilon_i\)</span> are independent and identically distributed as <span class="math inline">\(N(0, \sigma^2)\)</span>, and under the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>, we have:</p>
<ol style="list-style-type: decimal">
<li><p>The scaled <span class="math inline">\(MSE\)</span> follows a chi-square distribution with <span class="math inline">\(n-2\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\frac{MSE}{\sigma^2} \sim \chi_{n-2}^2.
\]</span></p></li>
<li><p>The scaled <span class="math inline">\(MSR\)</span> follows a chi-square distribution with <span class="math inline">\(1\)</span> degree of freedom:</p>
<p><span class="math display">\[
\frac{MSR}{\sigma^2} \sim \chi_{1}^2.
\]</span></p></li>
<li><p>These two chi-square random variables are independent.</p></li>
</ol>
<p>The ratio of two independent chi-square random variables, scaled by their respective degrees of freedom, follows an <span class="math inline">\(F\)</span>-distribution. Therefore, under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
F = \frac{MSR}{MSE} \sim F_{1, n-2}.
\]</span></p>
<p>The <span class="math inline">\(F\)</span>-statistic tests whether the regression model provides a significant improvement over the null model (constant <span class="math inline">\(E(Y)\)</span>).</p>
<p>The hypotheses for the <span class="math inline">\(F\)</span>-test are:</p>
<ul>
<li><strong>Null Hypothesis</strong> (<span class="math inline">\(H_0\)</span>): <span class="math inline">\(\beta_1 = 0\)</span> (no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>).</li>
<li><strong>Alternative Hypothesis</strong> (<span class="math inline">\(H_a\)</span>): <span class="math inline">\(\beta_1 \neq 0\)</span> (a significant relationship exists between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>).</li>
</ul>
<p>The rejection rule for <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> is:</p>
<p><span class="math display">\[
F &gt; F_{1-\alpha;1,n-2},
\]</span></p>
<p>where <span class="math inline">\(F_{1-\alpha;1,n-2}\)</span> is the critical value from the <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(1\)</span> and <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<ol style="list-style-type: decimal">
<li><strong>If</strong> <span class="math inline">\(F \leq F_{1-\alpha;1,n-2}\)</span>:
<ul>
<li>Fail to reject <span class="math inline">\(H_0\)</span>. There is insufficient evidence to conclude that <span class="math inline">\(X\)</span> significantly explains variation in <span class="math inline">\(Y\)</span>.</li>
</ul></li>
<li><strong>If</strong> <span class="math inline">\(F &gt; F_{1-\alpha;1,n-2}\)</span>:
<ul>
<li>Reject <span class="math inline">\(H_0\)</span>. There is significant evidence that <span class="math inline">\(X\)</span> explains some of the variation in <span class="math inline">\(Y\)</span>.</li>
</ul></li>
</ol>
</div>
<div id="coefficient-of-determination-r2" class="section level4 hasAnchor" number="5.1.1.8">
<h4><span class="header-section-number">5.1.1.8</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)<a href="ordinary-least-squares.html#coefficient-of-determination-r2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>Coefficient of Determination</strong> (<span class="math inline">\(R^2\)</span>) measures how well the linear regression model accounts for the variability in the response variable <span class="math inline">\(Y\)</span>. It is defined as:</p>
<p><span class="math display">\[
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO},
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(SSR\)</span>: Regression Sum of Squares (variation explained by the model).</li>
<li><span class="math inline">\(SSTO\)</span>: Total Sum of Squares (total variation in <span class="math inline">\(Y\)</span> about its mean).</li>
<li><span class="math inline">\(SSE\)</span>: Error Sum of Squares (variation unexplained by the model).</li>
</ul>
<hr />
<p><strong>Properties of</strong> <span class="math inline">\(R^2\)</span></p>
<ol style="list-style-type: decimal">
<li><p><strong>Range</strong>: <span class="math display">\[
0 \leq R^2 \leq 1.
\]</span></p>
<ul>
<li><span class="math inline">\(R^2 = 0\)</span>: The model explains none of the variability in <span class="math inline">\(Y\)</span> (e.g., <span class="math inline">\(\beta_1 = 0\)</span>).</li>
<li><span class="math inline">\(R^2 = 1\)</span>: The model explains all the variability in <span class="math inline">\(Y\)</span> (perfect fit).</li>
</ul></li>
<li><p><strong>Proportionate Reduction in Variance</strong>: <span class="math inline">\(R^2\)</span> represents the proportionate reduction in the total variation of <span class="math inline">\(Y\)</span> after fitting the model. It quantifies how much better the model predicts <span class="math inline">\(Y\)</span> compared to simply using <span class="math inline">\(\bar{Y}\)</span>.</p></li>
<li><p><strong>Potential Misinterpretation</strong>: It is not really correct to say <span class="math inline">\(R^2\)</span> is the “variation in <span class="math inline">\(Y\)</span> explained by <span class="math inline">\(X\)</span>.” The term “variation explained” assumes a causative or deterministic explanation, which is not always correct. For example:</p>
<ul>
<li><p><span class="math inline">\(R^2\)</span> shows how much variance in <span class="math inline">\(Y\)</span> is accounted for by the regression model, but it does not imply causation.</p></li>
<li><p>In cases with confounding variables or spurious correlations, <span class="math inline">\(R^2\)</span> can still be high, even if there’s no direct causal link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
</ul></li>
</ol>
<hr />
<p>For simple linear regression, <span class="math inline">\(R^2\)</span> is the square of the Pearson correlation coefficient, <span class="math inline">\(r\)</span>:</p>
<p><span class="math display">\[
R^2 = (r)^2,
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(r = \text{corr}(X, Y)\)</span> is the sample correlation coefficient.</li>
</ul>
<p>The relationship between <span class="math inline">\(b_1\)</span> (the slope of the regression line) and <span class="math inline">\(r\)</span> is given by:</p>
<p><span class="math display">\[
b_1 = \left(\frac{\sum_{i=1}^n (Y_i - \bar{Y})^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\right)^{1/2}.
\]</span></p>
<p>Additionally, <span class="math inline">\(r\)</span> can be expressed as:</p>
<p><span class="math display">\[
r = \frac{s_y}{s_x} \cdot r,
\]</span></p>
<p>where <span class="math inline">\(s_y\)</span> and <span class="math inline">\(s_x\)</span> are the sample standard deviations of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, respectively.</p>
</div>
<div id="lack-of-fit-in-regression" class="section level4 hasAnchor" number="5.1.1.9">
<h4><span class="header-section-number">5.1.1.9</span> Lack of Fit in Regression<a href="ordinary-least-squares.html#lack-of-fit-in-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>lack of fit</strong> test evaluates whether the chosen regression model adequately captures the relationship between the predictor variable <span class="math inline">\(X\)</span> and the response variable <span class="math inline">\(Y\)</span>. When there are repeated observations at specific values of <span class="math inline">\(X\)</span>, we can partition the Error Sum of Squares (<span class="math inline">\(SSE\)</span>) into two components:</p>
<ol style="list-style-type: decimal">
<li><strong>Pure Error</strong></li>
<li><strong>Lack of Fit</strong>.</li>
</ol>
<p>Given the observations:</p>
<ul>
<li><span class="math inline">\(Y_{ij}\)</span>: The <span class="math inline">\(j\)</span>-th replicate for the <span class="math inline">\(i\)</span>-th distinct value of <span class="math inline">\(X\)</span>,
<ul>
<li><span class="math inline">\(Y_{11}, Y_{21}, \dots, Y_{n_1, 1}\)</span>: <span class="math inline">\(n_1\)</span> repeated observations of <span class="math inline">\(X_1\)</span></li>
<li><span class="math inline">\(Y_{1c}, Y_{2c}, \dots, Y_{n_c,c}\)</span>: <span class="math inline">\(n_c\)</span> repeated observations of <span class="math inline">\(X_c\)</span></li>
</ul></li>
<li><span class="math inline">\(\bar{Y}_j\)</span>: The mean response for replicates at <span class="math inline">\(X_j\)</span>,</li>
<li><span class="math inline">\(\hat{Y}_{ij}\)</span>: The predicted value from the regression model for <span class="math inline">\(X_j\)</span>,</li>
</ul>
<p>the Error Sum of Squares (<span class="math inline">\(SSE\)</span>) can be decomposed as:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i} \sum_{j} (Y_{ij} - \hat{Y}_{ij})^2 &amp;= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j + \bar{Y}_j - \hat{Y}_{ij})^2 \\
&amp;= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{j} n_j (\bar{Y}_j - \hat{Y}_{ij})^2 + \text{cross product term} \\
&amp;= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{j} n_j (\bar{Y}_j - \hat{Y}_{ij})^2
\end{aligned}
\]</span></p>
<ul>
<li>The <strong>cross product term</strong> is zero because the deviations within replicates and the deviations between replicates are orthogonal.</li>
<li>This simplifies to:</li>
</ul>
<p><span class="math display">\[
SSE = SSPE + SSLF,
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(SSPE\)</span> (Pure Error Sum of Squares): Variation within replicates for the same <span class="math inline">\(X_j\)</span>, reflecting natural variability in the response.
<ul>
<li>Degrees of freedom: <span class="math inline">\(df_{pe} = n - c\)</span>, where <span class="math inline">\(n\)</span> is the total number of observations, and <span class="math inline">\(c\)</span> is the number of distinct <span class="math inline">\(X\)</span> values.</li>
</ul></li>
<li><span class="math inline">\(SSLF\)</span> (Lack of Fit Sum of Squares): Variation between the replicate means <span class="math inline">\(\bar{Y}_j\)</span> and the model-predicted values <span class="math inline">\(\hat{Y}_{ij}\)</span>. If SSLF is large, it suggests the model may not adequately describe the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
<ul>
<li>Degrees of freedom: <span class="math inline">\(df_{lf} = c - 2\)</span>, where 2 accounts for the parameters in the linear regression model (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>).</li>
</ul></li>
</ul>
<hr />
<ul>
<li><p><strong>Mean Square for Pure Error (MSPE):</strong><br />
<span class="math display">\[
MSPE = \frac{SSPE}{df_{pe}} = \frac{SSPE}{n-c}.
\]</span></p></li>
<li><p><strong>Mean Square for Lack of Fit (MSLF):</strong><br />
<span class="math display">\[
MSLF = \frac{SSLF}{df_{lf}} = \frac{SSLF}{c-2}.
\]</span></p></li>
</ul>
<div id="the-f-test-for-lack-of-fit" class="section level5 hasAnchor" number="5.1.1.9.1">
<h5><span class="header-section-number">5.1.1.9.1</span> The F-Test for Lack of Fit<a href="ordinary-least-squares.html#the-f-test-for-lack-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The <strong>F-test for lack of fit</strong> evaluates whether the chosen regression model adequately captures the relationship between the predictor variable <span class="math inline">\(X\)</span> and the response variable <span class="math inline">\(Y\)</span>. Specifically, it tests whether any systematic deviations from the model exist that are not accounted for by random error.</p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>):<br />
The regression model is adequate: <span class="math display">\[
H_0: Y_{ij} = \beta_0 + \beta_1 X_i + \epsilon_{ij}, \quad \epsilon_{ij} \sim \text{i.i.d. } N(0, \sigma^2).
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_a\)</span>):<br />
The regression model is not adequate and includes an additional function <span class="math inline">\(f(X_i, Z_1, \dots)\)</span> to account for the lack of fit: <span class="math display">\[
H_a: Y_{ij} = \alpha_0 + \alpha_1 X_i + f(X_i, Z_1, \dots) + \epsilon_{ij}^*, \quad \epsilon_{ij}^* \sim \text{i.i.d. } N(0, \sigma^2).
\]</span></p></li>
</ul>
<hr />
<p><strong>Expected Mean Squares</strong></p>
<ul>
<li><p>The expected Mean Square for Pure Error (MSPE) is the same under both <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span>:</p>
<p><span class="math display">\[
E(MSPE) = \sigma^2.
\]</span></p></li>
<li><p>The expected Mean Square for Lack of Fit (MSLF) depends on whether <span class="math inline">\(H_0\)</span> is true:</p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span> (model is adequate): <span class="math display">\[
E(MSLF) = \sigma^2.
\]</span></li>
<li>Under <span class="math inline">\(H_a\)</span> (model is not adequate): <span class="math display">\[
E(MSLF) = \sigma^2 + \frac{\sum n_j f(X_i, Z_1, \dots)^2}{n-2}.
\]</span></li>
</ul></li>
</ul>
<hr />
<p>The test statistic for the lack-of-fit test is:</p>
<p><span class="math display">\[
F = \frac{MSLF}{MSPE},
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(MSLF = \frac{SSLF}{c-2}\)</span>,<br />
and <span class="math inline">\(SSLF\)</span> is the Lack of Fit Sum of Squares.</li>
<li><span class="math inline">\(MSPE = \frac{SSPE}{n-c}\)</span>,<br />
and <span class="math inline">\(SSPE\)</span> is the Pure Error Sum of Squares.</li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>, the <span class="math inline">\(F\)</span>-statistic follows an <span class="math inline">\(F\)</span>-distribution:</p>
<p><span class="math display">\[
F \sim F_{c-2, n-c}.
\]</span></p>
<hr />
<p><strong>Decision Rule</strong></p>
<ul>
<li><p>Reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if: <span class="math display">\[
F &gt; F_{1-\alpha; c-2, n-c}.
\]</span></p></li>
<li><p><strong>Failing to reject</strong> <span class="math inline">\(H_0\)</span>:</p>
<ul>
<li>Indicates that there is no evidence of lack of fit.</li>
<li>Does not imply the model is “true,” but it suggests that the model provides a reasonable approximation to the true relationship.</li>
</ul></li>
</ul>
<p>To summarize, when repeat observations exist at some levels of <span class="math inline">\(X\)</span>, the <strong>Error Sum of Squares (SSE)</strong> can be further partitioned into <strong>Lack of Fit (SSLF)</strong> and <strong>Pure Error (SSPE)</strong>. This leads to an extended ANOVA table:</p>
<table>
<colgroup>
<col width="21%" />
<col width="16%" />
<col width="7%" />
<col width="28%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of Variation</th>
<th>Sum of Squares</th>
<th>df</th>
<th>Mean Square</th>
<th>F Statistic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td>SSR</td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(MSR = \frac{SSR}{1}\)</span></td>
<td><span class="math inline">\(F = \frac{MSR}{MSE}\)</span></td>
</tr>
<tr class="even">
<td>Error</td>
<td>SSE</td>
<td><span class="math inline">\(n-2\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{n-2}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Lack of fit</td>
<td>SSLF</td>
<td><span class="math inline">\(c-2\)</span></td>
<td><span class="math inline">\(MSLF = \frac{SSLF}{c-2}\)</span></td>
<td><span class="math inline">\(F = \frac{MSLF}{MSPE}\)</span></td>
</tr>
<tr class="even">
<td>Pure Error</td>
<td>SSPE</td>
<td><span class="math inline">\(n-c\)</span></td>
<td><span class="math inline">\(MSPE = \frac{SSPE}{n-c}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total (Corrected)</td>
<td>SSTO</td>
<td><span class="math inline">\(n-1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Repeat observations have important implications for the coefficient of determination (<span class="math inline">\(R^2\)</span>):</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R^2\)</span> Can’t Attain 1 with Repeat Observations:
<ul>
<li>With repeat observations, <span class="math inline">\(SSE\)</span> (Error Sum of Squares) cannot be reduced to 0 because <span class="math inline">\(SSPE &gt; 0\)</span> (variability within replicates).</li>
</ul></li>
<li><strong>Maximum</strong> <span class="math inline">\(R^2\)</span>:
<ul>
<li><p>The maximum attainable <span class="math inline">\(R^2\)</span> in the presence of repeat observations is:</p>
<p><span class="math display">\[
R^2_{\text{max}} = \frac{SSTO - SSPE}{SSTO}.
\]</span></p></li>
</ul></li>
<li><strong>Importance of Repeat Observations:</strong>
<ul>
<li>Not all levels of <span class="math inline">\(X\)</span> need repeat observations, but their presence enables the separation of pure error from lack of fit, making the <span class="math inline">\(F\)</span>-test for lack of fit possible.</li>
</ul></li>
</ol>
<hr />
<p>Estimation of <span class="math inline">\(\sigma^2\)</span> with Repeat Observations</p>
<ol style="list-style-type: decimal">
<li><strong>Use of MSE:</strong>
<ul>
<li>When <span class="math inline">\(H_0\)</span> is appropriate (the model fits well), <span class="math inline">\(MSE\)</span> is typically used as the estimate of <span class="math inline">\(\sigma^2\)</span> instead of <span class="math inline">\(MSPE\)</span> because it has more degrees of freedom and provides a more reliable estimate.</li>
</ul></li>
<li><strong>Pooling Estimates:</strong>
<ul>
<li>In practice, <span class="math inline">\(MSE\)</span> and <span class="math inline">\(MSPE\)</span> may be pooled if <span class="math inline">\(H_0\)</span> holds, resulting in a more precise estimate of <span class="math inline">\(\sigma^2\)</span>.</li>
</ul></li>
</ol>
<hr />
</div>
</div>
<div id="joint-inference-for-regression-parameters" class="section level4 hasAnchor" number="5.1.1.10">
<h4><span class="header-section-number">5.1.1.10</span> Joint Inference for Regression Parameters<a href="ordinary-least-squares.html#joint-inference-for-regression-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Joint inference considers the simultaneous coverage of confidence intervals for multiple regression parameters, such as <span class="math inline">\(\beta_0\)</span> (intercept) and <span class="math inline">\(\beta_1\)</span> (slope). Ensuring adequate confidence for both parameters together requires adjustments to maintain the desired family-wise confidence level.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(\bar{A}_1\)</span>: The event that the confidence interval for <span class="math inline">\(\beta_0\)</span> covers its true value.</li>
<li><span class="math inline">\(\bar{A}_2\)</span>: The event that the confidence interval for <span class="math inline">\(\beta_1\)</span> covers its true value.</li>
</ul>
<p>The individual confidence levels are:</p>
<p><span class="math display">\[
\begin{aligned}
P(\bar{A}_1) &amp;= 1 - \alpha, \\
P(\bar{A}_2) &amp;= 1 - \alpha.
\end{aligned}
\]</span></p>
<p>The joint confidence coefficient, <span class="math inline">\(P(\bar{A}_1 \cap \bar{A}_2)\)</span>, is:</p>
<p><span class="math display">\[
\begin{aligned}
P(\bar{A}_1 \cap \bar{A}_2) &amp;= 1 - P(\bar{A}_1 \cup \bar{A}_2), \\
&amp;= 1 - P(A_1) - P(A_2) + P(A_1 \cap A_2), \\
&amp;\geq 1 - P(A_1) - P(A_2), \\
&amp;= 1 - 2\alpha.
\end{aligned}
\]</span></p>
<p>This means that if <span class="math inline">\(\alpha\)</span> is the significance level for each parameter, the joint confidence coefficient is at least <span class="math inline">\(1 - 2\alpha\)</span>. This inequality is known as the <strong>Bonferroni Inequality</strong>.</p>
<hr />
<p><strong>Bonferroni Confidence Intervals</strong></p>
<p>To ensure a desired joint confidence level of <span class="math inline">\((1-\alpha)\)</span> for both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the Bonferroni method adjusts the confidence level for each parameter by dividing <span class="math inline">\(\alpha\)</span> by the number of parameters. For two parameters:</p>
<ol style="list-style-type: decimal">
<li><p>The confidence level for each parameter is <span class="math inline">\((1-\alpha/2)\)</span>.</p></li>
<li><p>The resulting <strong>Bonferroni-adjusted confidence intervals</strong> are:</p>
<p><span class="math display">\[
\begin{aligned}
b_0 &amp;\pm B \cdot s(b_0), \\
b_1 &amp;\pm B \cdot s(b_1),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(B = t_{1-\alpha/4; n-2}\)</span> is the critical value from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</p></li>
</ol>
<hr />
<p><strong>Interpretation of Bonferroni Confidence Intervals</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Coverage Probability:</strong>
<ul>
<li>If repeated samples were taken, <span class="math inline">\((1-\alpha)100\%\)</span> of the joint intervals would contain the true values of <span class="math inline">\((\beta_0, \beta_1)\)</span>.</li>
<li>This implies that <span class="math inline">\(\alpha \times 100\%\)</span> of the samples would miss at least one of the true parameter values.</li>
</ul></li>
<li><strong>Conservatism:</strong>
<ul>
<li>The Bonferroni method ensures the family-wise confidence level but is <strong>conservative</strong>. The actual joint confidence level is often higher than <span class="math inline">\((1-\alpha)100\%\)</span>.</li>
<li>This conservatism reduces statistical power.</li>
</ul></li>
</ol>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="ordinary-least-squares.html#cb166-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb166-2"><a href="ordinary-least-squares.html#cb166-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb166-3"><a href="ordinary-least-squares.html#cb166-3" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb166-4"><a href="ordinary-least-squares.html#cb166-4" tabindex="-1"></a></span>
<span id="cb166-5"><a href="ordinary-least-squares.html#cb166-5" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb166-6"><a href="ordinary-least-squares.html#cb166-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb166-7"><a href="ordinary-least-squares.html#cb166-7" tabindex="-1"></a></span>
<span id="cb166-8"><a href="ordinary-least-squares.html#cb166-8" tabindex="-1"></a><span class="co"># Generate synthetic data</span></span>
<span id="cb166-9"><a href="ordinary-least-squares.html#cb166-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># Number of observations</span></span>
<span id="cb166-10"><a href="ordinary-least-squares.html#cb166-10" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)  <span class="co"># Predictor</span></span>
<span id="cb166-11"><a href="ordinary-least-squares.html#cb166-11" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="dv">2</span>  <span class="co"># True intercept</span></span>
<span id="cb166-12"><a href="ordinary-least-squares.html#cb166-12" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="dv">3</span>  <span class="co"># True slope</span></span>
<span id="cb166-13"><a href="ordinary-least-squares.html#cb166-13" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># Standard deviation of error</span></span>
<span id="cb166-14"><a href="ordinary-least-squares.html#cb166-14" tabindex="-1"></a>y <span class="ot">&lt;-</span></span>
<span id="cb166-15"><a href="ordinary-least-squares.html#cb166-15" tabindex="-1"></a>    beta_0 <span class="sc">+</span> beta_1 <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma)  <span class="co"># Response</span></span>
<span id="cb166-16"><a href="ordinary-least-squares.html#cb166-16" tabindex="-1"></a></span>
<span id="cb166-17"><a href="ordinary-least-squares.html#cb166-17" tabindex="-1"></a><span class="co"># Fit linear model</span></span>
<span id="cb166-18"><a href="ordinary-least-squares.html#cb166-18" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb166-19"><a href="ordinary-least-squares.html#cb166-19" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb166-20"><a href="ordinary-least-squares.html#cb166-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-21"><a href="ordinary-least-squares.html#cb166-21" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb166-22"><a href="ordinary-least-squares.html#cb166-22" tabindex="-1"></a><span class="co">#&gt; lm(formula = y ~ x)</span></span>
<span id="cb166-23"><a href="ordinary-least-squares.html#cb166-23" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-24"><a href="ordinary-least-squares.html#cb166-24" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb166-25"><a href="ordinary-least-squares.html#cb166-25" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb166-26"><a href="ordinary-least-squares.html#cb166-26" tabindex="-1"></a><span class="co">#&gt; -1.9073 -0.6835 -0.0875  0.5806  3.2904 </span></span>
<span id="cb166-27"><a href="ordinary-least-squares.html#cb166-27" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-28"><a href="ordinary-least-squares.html#cb166-28" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb166-29"><a href="ordinary-least-squares.html#cb166-29" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb166-30"><a href="ordinary-least-squares.html#cb166-30" tabindex="-1"></a><span class="co">#&gt; (Intercept)  1.89720    0.09755   19.45   &lt;2e-16 ***</span></span>
<span id="cb166-31"><a href="ordinary-least-squares.html#cb166-31" tabindex="-1"></a><span class="co">#&gt; x            2.94753    0.10688   27.58   &lt;2e-16 ***</span></span>
<span id="cb166-32"><a href="ordinary-least-squares.html#cb166-32" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb166-33"><a href="ordinary-least-squares.html#cb166-33" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb166-34"><a href="ordinary-least-squares.html#cb166-34" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb166-35"><a href="ordinary-least-squares.html#cb166-35" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.9707 on 98 degrees of freedom</span></span>
<span id="cb166-36"><a href="ordinary-least-squares.html#cb166-36" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8859, Adjusted R-squared:  0.8847 </span></span>
<span id="cb166-37"><a href="ordinary-least-squares.html#cb166-37" tabindex="-1"></a><span class="co">#&gt; F-statistic: 760.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16</span></span>
<span id="cb166-38"><a href="ordinary-least-squares.html#cb166-38" tabindex="-1"></a></span>
<span id="cb166-39"><a href="ordinary-least-squares.html#cb166-39" tabindex="-1"></a><span class="co"># Extract coefficients and standard errors</span></span>
<span id="cb166-40"><a href="ordinary-least-squares.html#cb166-40" tabindex="-1"></a>b0_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(model)[<span class="dv">1</span>]</span>
<span id="cb166-41"><a href="ordinary-least-squares.html#cb166-41" tabindex="-1"></a>b1_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(model)[<span class="dv">2</span>]</span>
<span id="cb166-42"><a href="ordinary-least-squares.html#cb166-42" tabindex="-1"></a>s_b0 <span class="ot">&lt;-</span></span>
<span id="cb166-43"><a href="ordinary-least-squares.html#cb166-43" tabindex="-1"></a>    <span class="fu">summary</span>(model)<span class="sc">$</span>coefficients[<span class="dv">1</span>, <span class="dv">2</span>]  <span class="co"># Standard error of intercept</span></span>
<span id="cb166-44"><a href="ordinary-least-squares.html#cb166-44" tabindex="-1"></a>s_b1 <span class="ot">&lt;-</span> <span class="fu">summary</span>(model)<span class="sc">$</span>coefficients[<span class="dv">2</span>, <span class="dv">2</span>]  <span class="co"># Standard error of slope</span></span>
<span id="cb166-45"><a href="ordinary-least-squares.html#cb166-45" tabindex="-1"></a></span>
<span id="cb166-46"><a href="ordinary-least-squares.html#cb166-46" tabindex="-1"></a><span class="co"># Desired confidence level</span></span>
<span id="cb166-47"><a href="ordinary-least-squares.html#cb166-47" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span>  <span class="co"># Overall significance level</span></span>
<span id="cb166-48"><a href="ordinary-least-squares.html#cb166-48" tabindex="-1"></a></span>
<span id="cb166-49"><a href="ordinary-least-squares.html#cb166-49" tabindex="-1"></a><span class="co"># Bonferroni correction</span></span>
<span id="cb166-50"><a href="ordinary-least-squares.html#cb166-50" tabindex="-1"></a>adjusted_alpha <span class="ot">&lt;-</span> alpha <span class="sc">/</span> <span class="dv">2</span>  <span class="co"># Adjusted alpha for each parameter</span></span>
<span id="cb166-51"><a href="ordinary-least-squares.html#cb166-51" tabindex="-1"></a></span>
<span id="cb166-52"><a href="ordinary-least-squares.html#cb166-52" tabindex="-1"></a><span class="co"># Critical t-value for Bonferroni adjustment</span></span>
<span id="cb166-53"><a href="ordinary-least-squares.html#cb166-53" tabindex="-1"></a>t_crit <span class="ot">&lt;-</span></span>
<span id="cb166-54"><a href="ordinary-least-squares.html#cb166-54" tabindex="-1"></a>    <span class="fu">qt</span>(<span class="dv">1</span> <span class="sc">-</span> adjusted_alpha, <span class="at">df =</span> n <span class="sc">-</span> <span class="dv">2</span>)  <span class="co"># n-2 degrees of freedom</span></span>
<span id="cb166-55"><a href="ordinary-least-squares.html#cb166-55" tabindex="-1"></a></span>
<span id="cb166-56"><a href="ordinary-least-squares.html#cb166-56" tabindex="-1"></a><span class="co"># Bonferroni confidence intervals</span></span>
<span id="cb166-57"><a href="ordinary-least-squares.html#cb166-57" tabindex="-1"></a>ci_b0 <span class="ot">&lt;-</span> <span class="fu">c</span>(b0_hat <span class="sc">-</span> t_crit <span class="sc">*</span> s_b0, b0_hat <span class="sc">+</span> t_crit <span class="sc">*</span> s_b0)</span>
<span id="cb166-58"><a href="ordinary-least-squares.html#cb166-58" tabindex="-1"></a>ci_b1 <span class="ot">&lt;-</span> <span class="fu">c</span>(b1_hat <span class="sc">-</span> t_crit <span class="sc">*</span> s_b1, b1_hat <span class="sc">+</span> t_crit <span class="sc">*</span> s_b1)</span>
<span id="cb166-59"><a href="ordinary-least-squares.html#cb166-59" tabindex="-1"></a></span>
<span id="cb166-60"><a href="ordinary-least-squares.html#cb166-60" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb166-61"><a href="ordinary-least-squares.html#cb166-61" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Bonferroni Confidence Intervals:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb166-62"><a href="ordinary-least-squares.html#cb166-62" tabindex="-1"></a><span class="co">#&gt; Bonferroni Confidence Intervals:</span></span>
<span id="cb166-63"><a href="ordinary-least-squares.html#cb166-63" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Intercept (beta_0): [&quot;</span>,</span>
<span id="cb166-64"><a href="ordinary-least-squares.html#cb166-64" tabindex="-1"></a>    <span class="fu">round</span>(ci_b0[<span class="dv">1</span>], <span class="dv">2</span>),</span>
<span id="cb166-65"><a href="ordinary-least-squares.html#cb166-65" tabindex="-1"></a>    <span class="st">&quot;,&quot;</span>,</span>
<span id="cb166-66"><a href="ordinary-least-squares.html#cb166-66" tabindex="-1"></a>    <span class="fu">round</span>(ci_b0[<span class="dv">2</span>], <span class="dv">2</span>),</span>
<span id="cb166-67"><a href="ordinary-least-squares.html#cb166-67" tabindex="-1"></a>    <span class="st">&quot;]</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb166-68"><a href="ordinary-least-squares.html#cb166-68" tabindex="-1"></a><span class="co">#&gt; Intercept (beta_0): [ 1.7 , 2.09 ]</span></span>
<span id="cb166-69"><a href="ordinary-least-squares.html#cb166-69" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Slope (beta_1): [&quot;</span>,</span>
<span id="cb166-70"><a href="ordinary-least-squares.html#cb166-70" tabindex="-1"></a>    <span class="fu">round</span>(ci_b1[<span class="dv">1</span>], <span class="dv">2</span>),</span>
<span id="cb166-71"><a href="ordinary-least-squares.html#cb166-71" tabindex="-1"></a>    <span class="st">&quot;,&quot;</span>,</span>
<span id="cb166-72"><a href="ordinary-least-squares.html#cb166-72" tabindex="-1"></a>    <span class="fu">round</span>(ci_b1[<span class="dv">2</span>], <span class="dv">2</span>),</span>
<span id="cb166-73"><a href="ordinary-least-squares.html#cb166-73" tabindex="-1"></a>    <span class="st">&quot;]</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb166-74"><a href="ordinary-least-squares.html#cb166-74" tabindex="-1"></a><span class="co">#&gt; Slope (beta_1): [ 2.74 , 3.16 ]</span></span>
<span id="cb166-75"><a href="ordinary-least-squares.html#cb166-75" tabindex="-1"></a></span>
<span id="cb166-76"><a href="ordinary-least-squares.html#cb166-76" tabindex="-1"></a><span class="co"># Calculate the covariance matrix of coefficients</span></span>
<span id="cb166-77"><a href="ordinary-least-squares.html#cb166-77" tabindex="-1"></a>cov_matrix <span class="ot">&lt;-</span> <span class="fu">vcov</span>(model)</span>
<span id="cb166-78"><a href="ordinary-least-squares.html#cb166-78" tabindex="-1"></a></span>
<span id="cb166-79"><a href="ordinary-least-squares.html#cb166-79" tabindex="-1"></a><span class="co"># Generate points for confidence ellipse</span></span>
<span id="cb166-80"><a href="ordinary-least-squares.html#cb166-80" tabindex="-1"></a>ellipse_points <span class="ot">&lt;-</span></span>
<span id="cb166-81"><a href="ordinary-least-squares.html#cb166-81" tabindex="-1"></a>    MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>,</span>
<span id="cb166-82"><a href="ordinary-least-squares.html#cb166-82" tabindex="-1"></a>                  <span class="at">mu =</span> <span class="fu">coef</span>(model),</span>
<span id="cb166-83"><a href="ordinary-least-squares.html#cb166-83" tabindex="-1"></a>                  <span class="at">Sigma =</span> cov_matrix)</span>
<span id="cb166-84"><a href="ordinary-least-squares.html#cb166-84" tabindex="-1"></a></span>
<span id="cb166-85"><a href="ordinary-least-squares.html#cb166-85" tabindex="-1"></a><span class="co"># Convert to data frame for plotting</span></span>
<span id="cb166-86"><a href="ordinary-least-squares.html#cb166-86" tabindex="-1"></a>ellipse_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(ellipse_points)</span>
<span id="cb166-87"><a href="ordinary-least-squares.html#cb166-87" tabindex="-1"></a><span class="fu">colnames</span>(ellipse_df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;beta_0&quot;</span>, <span class="st">&quot;beta_1&quot;</span>)</span>
<span id="cb166-88"><a href="ordinary-least-squares.html#cb166-88" tabindex="-1"></a></span>
<span id="cb166-89"><a href="ordinary-least-squares.html#cb166-89" tabindex="-1"></a><span class="co"># Plot confidence intervals and ellipse</span></span>
<span id="cb166-90"><a href="ordinary-least-squares.html#cb166-90" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb166-91"><a href="ordinary-least-squares.html#cb166-91" tabindex="-1"></a>    <span class="co"># Confidence ellipse</span></span>
<span id="cb166-92"><a href="ordinary-least-squares.html#cb166-92" tabindex="-1"></a>    <span class="fu">geom_point</span>(</span>
<span id="cb166-93"><a href="ordinary-least-squares.html#cb166-93" tabindex="-1"></a>        <span class="at">data =</span> ellipse_df,</span>
<span id="cb166-94"><a href="ordinary-least-squares.html#cb166-94" tabindex="-1"></a>        <span class="fu">aes</span>(<span class="at">x =</span> beta_0, <span class="at">y =</span> beta_1),</span>
<span id="cb166-95"><a href="ordinary-least-squares.html#cb166-95" tabindex="-1"></a>        <span class="at">alpha =</span> <span class="fl">0.1</span>,</span>
<span id="cb166-96"><a href="ordinary-least-squares.html#cb166-96" tabindex="-1"></a>        <span class="at">color =</span> <span class="st">&quot;grey&quot;</span></span>
<span id="cb166-97"><a href="ordinary-least-squares.html#cb166-97" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb166-98"><a href="ordinary-least-squares.html#cb166-98" tabindex="-1"></a>    <span class="co"># Point estimate</span></span>
<span id="cb166-99"><a href="ordinary-least-squares.html#cb166-99" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> b0_hat, <span class="at">y =</span> b1_hat),</span>
<span id="cb166-100"><a href="ordinary-least-squares.html#cb166-100" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb166-101"><a href="ordinary-least-squares.html#cb166-101" tabindex="-1"></a>               <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb166-102"><a href="ordinary-least-squares.html#cb166-102" tabindex="-1"></a>    <span class="co"># Bonferroni confidence intervals</span></span>
<span id="cb166-103"><a href="ordinary-least-squares.html#cb166-103" tabindex="-1"></a>    <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">x =</span> b0_hat, <span class="at">ymin =</span> ci_b1[<span class="dv">1</span>], <span class="at">ymax =</span> ci_b1[<span class="dv">2</span>]),</span>
<span id="cb166-104"><a href="ordinary-least-squares.html#cb166-104" tabindex="-1"></a>                  <span class="at">width =</span> <span class="fl">0.1</span>,</span>
<span id="cb166-105"><a href="ordinary-least-squares.html#cb166-105" tabindex="-1"></a>                  <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb166-106"><a href="ordinary-least-squares.html#cb166-106" tabindex="-1"></a>    <span class="fu">geom_errorbarh</span>(<span class="fu">aes</span>(<span class="at">y =</span> b1_hat, <span class="at">xmin =</span> ci_b0[<span class="dv">1</span>], <span class="at">xmax =</span> ci_b0[<span class="dv">2</span>]),</span>
<span id="cb166-107"><a href="ordinary-least-squares.html#cb166-107" tabindex="-1"></a>                   <span class="at">height =</span> <span class="fl">0.1</span>,</span>
<span id="cb166-108"><a href="ordinary-least-squares.html#cb166-108" tabindex="-1"></a>                   <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb166-109"><a href="ordinary-least-squares.html#cb166-109" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Bonferroni Confidence Intervals and Joint Confidence Region&quot;</span>,</span>
<span id="cb166-110"><a href="ordinary-least-squares.html#cb166-110" tabindex="-1"></a>         <span class="at">x =</span> <span class="st">&quot;Intercept (beta_0)&quot;</span>,</span>
<span id="cb166-111"><a href="ordinary-least-squares.html#cb166-111" tabindex="-1"></a>         <span class="at">y =</span> <span class="st">&quot;Slope (beta_1)&quot;</span>) <span class="sc">+</span></span>
<span id="cb166-112"><a href="ordinary-least-squares.html#cb166-112" tabindex="-1"></a>    <span class="fu">theme_minimal</span>()</span>
<span id="cb166-113"><a href="ordinary-least-squares.html#cb166-113" tabindex="-1"></a></span>
<span id="cb166-114"><a href="ordinary-least-squares.html#cb166-114" tabindex="-1"></a><span class="fu">print</span>(p)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li>The red point represents the estimated coefficients (b0_hat, b1_hat).</li>
<li>The blue lines represent the Bonferroni-adjusted confidence intervals for beta_0 and beta_1.</li>
<li>The grey points represent the joint confidence region based on the covariance matrix of coefficients.</li>
<li>The Bonferroni intervals ensure family-wise confidence level but are conservative.</li>
<li>Simulation results demonstrate how often the true values are captured in the intervals when repeated samples are drawn.</li>
</ol>
<p><strong>Notes</strong>:</p>
<ol style="list-style-type: decimal">
<li>Conservatism of Bonferroni Intervals
<ul>
<li>The <strong>Bonferroni interval is conservative</strong>:
<ul>
<li>The joint confidence level is a lower bound, ensuring family-wise coverage of at least <span class="math inline">\((1-\alpha)100\%\)</span>.</li>
<li>This conservatism results in wider intervals, reducing the statistical power of the test.</li>
</ul></li>
<li>Adjustments for Conservatism:
<ul>
<li>Practitioners often choose a larger <span class="math inline">\(\alpha\)</span> (e.g., <span class="math inline">\(\alpha = 0.1\)</span>) to reduce the width of the intervals in Bonferroni joint tests.</li>
<li>A higher <span class="math inline">\(\alpha\)</span> allows for a better balance between confidence and precision, especially for exploratory analyses.</li>
</ul></li>
</ul></li>
<li><strong>Extending Bonferroni to Multiple Parameters</strong>: The Bonferroni method is not limited to two parameters. For testing <span class="math inline">\(g\)</span> parameters, such as <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_{g-1}\)</span>:
<ul>
<li><strong>Adjusted Confidence Level for Each Parameter:</strong>
<ul>
<li>The confidence level for each individual parameter is <span class="math inline">\((1-\alpha/g)\)</span>.</li>
</ul></li>
<li><strong>Critical</strong> <span class="math inline">\(t\)</span>-Value:
<ul>
<li>For two-sided intervals, the critical value for each parameter is: <span class="math display">\[
t_{1-\frac{\alpha}{2g}; n-p},
\]</span> where <span class="math inline">\(p\)</span> is the total number of parameters in the regression model.</li>
</ul></li>
<li><strong>Example:</strong>
<ul>
<li>If <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(g = 10\)</span>, each individual confidence interval is constructed at the: <span class="math display">\[
(1 - \frac{0.05}{10}) = 99.5\% \text{ confidence level}.
\]</span></li>
<li>This corresponds to using <span class="math inline">\(t_{1-\frac{0.005}{2}; n-p}\)</span> in the formula for the confidence intervals.</li>
</ul></li>
</ul></li>
<li><strong>Limitations</strong> for Large <span class="math inline">\(g\)</span>
<ul>
<li><strong>Wide Intervals:</strong>
<ul>
<li>As <span class="math inline">\(g\)</span> increases, the intervals become excessively wide, often leading to reduced usefulness in practical applications.</li>
<li>This issue stems from the conservatism of the Bonferroni method, which prioritizes family-wise error control.</li>
</ul></li>
<li><strong>Suitability for Small</strong> <span class="math inline">\(g\)</span>:
<ul>
<li>The Bonferroni procedure works well when <span class="math inline">\(g\)</span> is relatively small (e.g., <span class="math inline">\(g \leq 5\)</span>).</li>
<li>For larger <span class="math inline">\(g\)</span>, alternative methods (discussed below) are more efficient.</li>
</ul></li>
</ul></li>
<li>Correlation Between Parameters: Correlation of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>:
<ul>
<li>The estimated regression coefficients <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are often correlated:
<ul>
<li><strong>Negative correlation</strong> if <span class="math inline">\(\bar{X} &gt; 0\)</span>.</li>
<li><strong>Positive correlation</strong> if <span class="math inline">\(\bar{X} &lt; 0\)</span>.</li>
</ul></li>
<li>This correlation can complicate joint inference but does not affect the validity of Bonferroni-adjusted intervals.</li>
</ul></li>
<li>Alternatives to Bonferroni</li>
</ol>
<p>Several alternative procedures provide more precise joint inference, especially for larger <span class="math inline">\(g\)</span>:</p>
<ol style="list-style-type: decimal">
<li><strong>Scheffé’s Method:</strong>
<ul>
<li>Constructs simultaneous confidence regions for all possible linear combinations of parameters.</li>
<li>Suitable for exploratory analyses but may result in even wider intervals than Bonferroni.</li>
</ul></li>
<li><strong>Tukey’s Honest Significant Difference:</strong>
<ul>
<li>Designed for pairwise comparisons in ANOVA but can be adapted for regression parameters.</li>
</ul></li>
<li><strong>Holm’s Step-Down Procedure:</strong>
<ul>
<li>A sequential testing procedure that is less conservative than Bonferroni while still controlling the family-wise error rate.</li>
</ul></li>
<li><strong>Likelihood Ratio Tests:</strong>
<ul>
<li>Construct joint confidence regions based on the likelihood function, offering more precision for large <span class="math inline">\(g\)</span>.</li>
</ul></li>
</ol>
</div>
<div id="assumptions-of-linear-regression" class="section level4 hasAnchor" number="5.1.1.11">
<h4><span class="header-section-number">5.1.1.11</span> Assumptions of Linear Regression<a href="ordinary-least-squares.html#assumptions-of-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To ensure valid inference and reliable predictions in linear regression, the following assumptions must hold. We’ll cover them in depth in the next section.</p>
<table>
<colgroup>
<col width="32%" />
<col width="67%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Assumption</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Linearity</strong></td>
<td>Linear relationship between predictors and response.</td>
</tr>
<tr class="even">
<td><strong>Independence of Errors</strong></td>
<td>Errors are independent (important in time-series/clustering).</td>
</tr>
<tr class="odd">
<td><strong>Homoscedasticity</strong></td>
<td>Constant variance of residuals across predictors.</td>
</tr>
<tr class="even">
<td><strong>Normality of Errors</strong></td>
<td>Residuals are normally distributed.</td>
</tr>
<tr class="odd">
<td><strong>No Multicollinearity</strong></td>
<td>Predictors are not highly correlated.</td>
</tr>
<tr class="even">
<td><strong>No Outliers/Leverage Points</strong></td>
<td>No undue influence from outliers or high-leverage points.</td>
</tr>
<tr class="odd">
<td><strong>Exogeneity</strong></td>
<td>Predictors are uncorrelated with the error term (no endogeneity).</td>
</tr>
<tr class="even">
<td><strong>Full Rank</strong></td>
<td>Predictors are linearly independent (no perfect multicollinearity).</td>
</tr>
</tbody>
</table>
</div>
<div id="diagnostics-for-model-assumptions" class="section level4 hasAnchor" number="5.1.1.12">
<h4><span class="header-section-number">5.1.1.12</span> Diagnostics for Model Assumptions<a href="ordinary-least-squares.html#diagnostics-for-model-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Constant Variance</strong></p>
<ul>
<li>To check homoscedasticity:
<ul>
<li>Plot residuals vs. fitted values or residuals vs. predictors.</li>
<li>Look for patterns or a funnel-shaped spread indicating heteroscedasticity.</li>
</ul></li>
</ul>
<p><strong>Outliers</strong></p>
<ul>
<li>Detect outliers using:
<ul>
<li>Residuals vs. predictors plot.</li>
<li>Box plots.</li>
<li>Stem-and-leaf plots.</li>
<li>Scatter plots.</li>
</ul></li>
</ul>
<p><strong>Standardized Residuals</strong>:</p>
<p>Residuals can be standardized to have unit variance, known as <strong>studentized residuals</strong>: <span class="math display">\[
  r_i = \frac{e_i}{s(e_i)}.
  \]</span></p>
<p><strong>Semi-Studentized Residuals</strong>:</p>
<p>A simplified standardization using the mean squared error (MSE): <span class="math display">\[
  e_i^* = \frac{e_i}{\sqrt{MSE}}.
  \]</span></p>
<p><strong>Non-Independent Error Terms</strong></p>
<ul>
<li>To detect non-independence:
<ul>
<li>Plot residuals vs. time for time-series data.</li>
<li>Residuals <span class="math inline">\(e_i\)</span> are not independent because they depend on <span class="math inline">\(\hat{Y}_i\)</span>, which is derived from the same regression function.</li>
</ul></li>
<li>Detect dependency by plotting the residual for the <span class="math inline">\(i\)</span>-th response vs. the <span class="math inline">\((i-1)\)</span>-th.</li>
</ul>
<p><strong>Non-Normality of Error Terms</strong></p>
<ul>
<li>To assess normality:
<ul>
<li>Plot distribution of residuals.</li>
<li>Create box plots, stem-and-leaf plots, or normal probability plots.</li>
</ul></li>
<li>Issues such as an incorrect regression function or non-constant error variance can distort the residual distribution.</li>
<li>Normality tests require relatively large sample sizes to detect deviations.</li>
</ul>
<p><strong>Normality of Residuals</strong></p>
<ul>
<li>Use tests based on the empirical cumulative distribution function (ECDF) (check <a href="normality-assessment.html#normality-assessment">Normality Assessment</a>)</li>
</ul>
<p><strong>Constancy of Error Variance</strong></p>
<ul>
<li>Statistical tests for homoscedasticity:
<ul>
<li><strong>Brown-Forsythe Test (Modified Levene Test)</strong>:
<ul>
<li>Robust against non-normality, examines the variance of residuals across levels of predictors.</li>
</ul></li>
<li><strong>Breusch-Pagan Test (Cook-Weisberg Test)</strong>:
<ul>
<li>Tests for heteroscedasticity by regressing squared residuals on predictors.</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="remedial-measures-for-violations-of-assumptions" class="section level4 hasAnchor" number="5.1.1.13">
<h4><span class="header-section-number">5.1.1.13</span> Remedial Measures for Violations of Assumptions<a href="ordinary-least-squares.html#remedial-measures-for-violations-of-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When the assumptions of simple linear regression are violated, appropriate remedial measures can be applied to address these issues. Below is a list of measures for specific deviations from the assumptions.</p>
<div id="general-remedies" class="section level5 hasAnchor" number="5.1.1.13.1">
<h5><span class="header-section-number">5.1.1.13.1</span> General Remedies<a href="ordinary-least-squares.html#general-remedies" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Use more <strong>complicated models</strong> (e.g., non-linear models, generalized linear models).</li>
<li>Apply <strong>transformations</strong> (see <a href="variable-transformation.html#variable-transformation">Variable Transformation</a>) on <span class="math inline">\(X\)</span> and/or <span class="math inline">\(Y\)</span> to stabilize variance, linearize relationships, or normalize residuals. Note that transformations may not always yield “optimal” results.</li>
</ul>
<hr />
</div>
<div id="specific-remedies-for-assumption-violations" class="section level5 hasAnchor" number="5.1.1.13.2">
<h5><span class="header-section-number">5.1.1.13.2</span> Specific Remedies for Assumption Violations<a href="ordinary-least-squares.html#specific-remedies-for-assumption-violations" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<table>
<colgroup>
<col width="16%" />
<col width="39%" />
<col width="44%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Issue</strong></th>
<th><strong>Remedy</strong></th>
<th><strong>Explanation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Non-Linearity</strong></td>
<td>- Apply transformations (e.g., log, square root).</td>
<td>Transformation of variables can help linearize the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</td>
</tr>
<tr class="even">
<td></td>
<td>- Use more complicated models (e.g., polynomial regression, splines).</td>
<td>Higher-order terms or non-linear models can capture non-linear relationships.</td>
</tr>
<tr class="odd">
<td><strong>Non-Constant Error Variance</strong></td>
<td>- Apply <strong>Weighted Least Squares</strong>.</td>
<td>WLS assigns weights to observations based on the inverse of their variance.</td>
</tr>
<tr class="even">
<td></td>
<td>- Use transformations (e.g., log, square root).</td>
<td>Transformations can stabilize error variance.</td>
</tr>
<tr class="odd">
<td><strong>Correlated Errors</strong></td>
<td>- Use serially correlated error models (e.g., ARIMA for time-series data).</td>
<td>Time-series models account for serial dependence in the errors.</td>
</tr>
<tr class="even">
<td><strong>Non-Normality of Errors</strong></td>
<td>- Transform <span class="math inline">\(Y\)</span> or use non-parametric methods.</td>
<td>Transformations can normalize residuals; non-parametric methods do not assume normality.</td>
</tr>
<tr class="odd">
<td><strong>Omitted Variables</strong></td>
<td>- Use multiple regression to include additional relevant predictors.</td>
<td>Adding relevant variables reduces omitted variable bias and improves model accuracy.</td>
</tr>
<tr class="even">
<td><strong>Outliers</strong></td>
<td>- Apply robust estimation techniques (e.g., Huber regression, M-estimation).</td>
<td>Robust methods reduce the influence of outliers on parameter estimates.</td>
</tr>
</tbody>
</table>
<hr />
</div>
<div id="remedies-in-detail" class="section level5 hasAnchor" number="5.1.1.13.3">
<h5><span class="header-section-number">5.1.1.13.3</span> Remedies in Detail<a href="ordinary-least-squares.html#remedies-in-detail" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li><strong>Non-Linearity:</strong>
<ul>
<li>Transformations: Apply transformations to the response variable <span class="math inline">\(Y\)</span> or the predictor variable <span class="math inline">\(X\)</span>. Common transformations include:
<ul>
<li>Logarithmic transformation: <span class="math inline">\(Y&#39; = \log(Y)\)</span> or <span class="math inline">\(X&#39; = \log(X)\)</span>.</li>
<li>Polynomial terms: Include <span class="math inline">\(X^2\)</span>, <span class="math inline">\(X^3\)</span>, etc., to capture curvature.</li>
</ul></li>
<li>Alternative Models:
<ul>
<li>Polynomial regression or splines for flexibility in modeling non-linear relationships.</li>
</ul></li>
</ul></li>
<li><strong>Non-Constant Error Variance:</strong>
<ul>
<li>Weighted Least Squares:
<ul>
<li>Assigns weights to observations inversely proportional to their variance.</li>
</ul></li>
<li>Transformations:
<ul>
<li>Use a log or square root transformation to stabilize variance.</li>
</ul></li>
</ul></li>
<li><strong>Correlated Errors:</strong>
<ul>
<li>For time-series data:
<ul>
<li>Use serially correlated error models such as AR(1) or ARIMA.</li>
<li>These models explicitly account for dependency in residuals over time.</li>
</ul></li>
</ul></li>
<li><strong>Non-Normality:</strong>
<ul>
<li>Transformations:
<ul>
<li>Apply a transformation to <span class="math inline">\(Y\)</span> (e.g., log or square root) to make the residuals approximately normal.</li>
</ul></li>
<li>Non-parametric regression:
<ul>
<li>Methods like LOESS or Theil-Sen regression do not require the normality assumption.</li>
</ul></li>
</ul></li>
<li><strong>Omitted Variables:</strong>
<ul>
<li>Introduce additional predictors:
<ul>
<li>Use multiple regression to include all relevant independent variables.</li>
</ul></li>
<li>Check for multicollinearity when adding new variables.</li>
</ul></li>
<li><strong>Outliers:</strong>
<ul>
<li>Robust Regression:
<ul>
<li>Use methods such as Huber regression or M-estimation to reduce the impact of outliers on model coefficients.</li>
</ul></li>
<li>Diagnostics:
<ul>
<li>Identify outliers using Cook’s Distance, leverage statistics, or studentized residuals.</li>
</ul></li>
</ul></li>
</ol>
</div>
</div>
<div id="transformations-in-regression-analysis" class="section level4 hasAnchor" number="5.1.1.14">
<h4><span class="header-section-number">5.1.1.14</span> Transformations in Regression Analysis<a href="ordinary-least-squares.html#transformations-in-regression-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Transformations involve modifying one or both variables to address issues such as non-linearity, non-constant variance, or non-normality. However, it’s important to note that the properties of least-squares estimates apply to the <strong>transformed model</strong>, not the original variables.</p>
<p>When transforming the dependent variable <span class="math inline">\(Y\)</span>, we fit the model as:</p>
<p><span class="math display">\[
g(Y_i) = b_0 + b_1 X_i,
\]</span></p>
<p>where <span class="math inline">\(g(Y_i)\)</span> is the transformed response. To interpret the regression results in terms of the original <span class="math inline">\(Y\)</span>, we need to <strong>transform back</strong>:</p>
<p><span class="math display">\[
\hat{Y}_i = g^{-1}(b_0 + b_1 X_i).
\]</span></p>
<hr />
<p>Direct back-transformation of predictions can introduce <strong>bias</strong>. For example, in a log-transformed model:</p>
<p><span class="math display">\[
\log(Y_i) = b_0 + b_1 X_i,
\]</span></p>
<p>the unbiased back-transformed prediction of <span class="math inline">\(Y_i\)</span> is:</p>
<p><span class="math display">\[
\hat{Y}_i = \exp(b_0 + b_1 X_i + \frac{\sigma^2}{2}),
\]</span></p>
<p>where <span class="math inline">\(\frac{\sigma^2}{2}\)</span> accounts for the bias correction due to the log transformation.</p>
<hr />
<div id="box-cox-family-of-transformations" class="section level5 hasAnchor" number="5.1.1.14.1">
<h5><span class="header-section-number">5.1.1.14.1</span> Box-Cox Family of Transformations<a href="ordinary-least-squares.html#box-cox-family-of-transformations" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The <strong>Box-Cox transformation</strong> is a versatile family of transformations defined as:</p>
<p><span class="math display">\[
Y&#39; =
\begin{cases}
\frac{Y^\lambda - 1}{\lambda}, &amp; \text{if } \lambda \neq 0, \\
\ln(Y), &amp; \text{if } \lambda = 0.
\end{cases}
\]</span></p>
<p>This transformation introduces a parameter <span class="math inline">\(\lambda\)</span> that is estimated from the data. Common transformations include:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(\lambda\)</span></th>
<th align="center">Transformation <span class="math inline">\(Y&#39;\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">2</td>
<td align="center"><span class="math inline">\(Y^2\)</span></td>
</tr>
<tr class="even">
<td align="center">0.5</td>
<td align="center"><span class="math inline">\(\sqrt{Y}\)</span></td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center"><span class="math inline">\(\ln(Y)\)</span></td>
</tr>
<tr class="even">
<td align="center">-0.5</td>
<td align="center"><span class="math inline">\(1/\sqrt{Y}\)</span></td>
</tr>
<tr class="odd">
<td align="center">-1</td>
<td align="center"><span class="math inline">\(1/Y\)</span></td>
</tr>
</tbody>
</table>
<hr />
<p>Choosing the Transformation Parameter <span class="math inline">\(\lambda\)</span></p>
<p>The value of <span class="math inline">\(\lambda\)</span> can be selected using one of the following methods:</p>
<ol style="list-style-type: decimal">
<li><strong>Trial and Error</strong>:
<ul>
<li>Apply different transformations and compare the residual plots or model fit statistics (e.g., <span class="math inline">\(R^2\)</span> or AIC).</li>
</ul></li>
<li><strong>Maximum Likelihood Estimation</strong>:
<ul>
<li>Choose <span class="math inline">\(\lambda\)</span> to maximize the likelihood function under the assumption of normally distributed errors.</li>
</ul></li>
<li><strong>Numerical Search</strong>:
<ul>
<li>Use computational optimization techniques to minimize the residual sum of squares (RSS) or another goodness-of-fit criterion.</li>
</ul></li>
</ol>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="ordinary-least-squares.html#cb167-1" tabindex="-1"></a><span class="co"># Install and load the necessary library</span></span>
<span id="cb167-2"><a href="ordinary-least-squares.html#cb167-2" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;MASS&quot;</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;MASS&quot;</span>)</span>
<span id="cb167-3"><a href="ordinary-least-squares.html#cb167-3" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb167-4"><a href="ordinary-least-squares.html#cb167-4" tabindex="-1"></a></span>
<span id="cb167-5"><a href="ordinary-least-squares.html#cb167-5" tabindex="-1"></a><span class="co"># Fit a linear model</span></span>
<span id="cb167-6"><a href="ordinary-least-squares.html#cb167-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb167-7"><a href="ordinary-least-squares.html#cb167-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb167-8"><a href="ordinary-least-squares.html#cb167-8" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb167-9"><a href="ordinary-least-squares.html#cb167-9" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb167-10"><a href="ordinary-least-squares.html#cb167-10" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb167-11"><a href="ordinary-least-squares.html#cb167-11" tabindex="-1"></a></span>
<span id="cb167-12"><a href="ordinary-least-squares.html#cb167-12" tabindex="-1"></a><span class="co"># Apply Box-Cox Transformation</span></span>
<span id="cb167-13"><a href="ordinary-least-squares.html#cb167-13" tabindex="-1"></a>boxcox_result <span class="ot">&lt;-</span> <span class="fu">boxcox</span>(model, <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.1</span>), <span class="at">plotit =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="ordinary-least-squares.html#cb168-1" tabindex="-1"></a></span>
<span id="cb168-2"><a href="ordinary-least-squares.html#cb168-2" tabindex="-1"></a><span class="co"># Find the optimal lambda</span></span>
<span id="cb168-3"><a href="ordinary-least-squares.html#cb168-3" tabindex="-1"></a>optimal_lambda <span class="ot">&lt;-</span> boxcox_result<span class="sc">$</span>x[<span class="fu">which.max</span>(boxcox_result<span class="sc">$</span>y)]</span>
<span id="cb168-4"><a href="ordinary-least-squares.html#cb168-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Optimal lambda for Box-Cox transformation:&quot;</span>, optimal_lambda, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb168-5"><a href="ordinary-least-squares.html#cb168-5" tabindex="-1"></a><span class="co">#&gt; Optimal lambda for Box-Cox transformation: 0.8686869</span></span></code></pre></div>
<p><strong>Notes</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Benefits of Transformations</strong>:</p>
<ul>
<li><p><strong>Stabilize Variance</strong>: Helps address heteroscedasticity.</p></li>
<li><p><strong>Linearize Relationships</strong>: Useful for non-linear data.</p></li>
<li><p><strong>Normalize Residuals</strong>: Addresses non-normality issues.</p></li>
</ul></li>
<li><p><strong>Caveats</strong>:</p>
<ul>
<li><p>Interpretability: Transformed variables may complicate interpretation.</p></li>
<li><p>Over-Transformation: Excessive transformations can distort the relationship between variables.</p></li>
</ul></li>
<li><p><strong>Applicability</strong>:</p>
<ul>
<li>Transformations are most effective for issues like non-linearity or non-constant variance. They are less effective for correcting independence violations or omitted variables.</li>
</ul></li>
</ol>
</div>
<div id="variance-stabilizing-transformations" class="section level5 hasAnchor" number="5.1.1.14.2">
<h5><span class="header-section-number">5.1.1.14.2</span> Variance Stabilizing Transformations<a href="ordinary-least-squares.html#variance-stabilizing-transformations" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Variance stabilizing transformations are used when the standard deviation of the response variable depends on its mean. The <strong>delta method</strong>, which applies a Taylor series expansion, provides a systematic approach to find such transformations.</p>
<p>Given that the standard deviation of <span class="math inline">\(Y\)</span> is a function of its mean:</p>
<p><span class="math display">\[
\sigma = \sqrt{\text{var}(Y)} = f(\mu),
\]</span></p>
<p>where <span class="math inline">\(\mu = E(Y)\)</span> and <span class="math inline">\(f(\mu)\)</span> is a smooth function of the mean, we aim to find a transformation <span class="math inline">\(h(Y)\)</span> such that the variance of the transformed variable <span class="math inline">\(h(Y)\)</span> is constant for all values of <span class="math inline">\(\mu\)</span>.</p>
<p>Expanding <span class="math inline">\(h(Y)\)</span> in a <a href="general-math.html#taylor-expansion">Taylor Expansion</a> series around <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
h(Y) = h(\mu) + h&#39;(\mu)(Y - \mu) + \text{higher-order terms}.
\]</span></p>
<p>Ignoring higher-order terms, the variance of <span class="math inline">\(h(Y)\)</span> can be approximated as:</p>
<p><span class="math display">\[
\text{var}(h(Y)) = \text{var}(h(\mu) + h&#39;(\mu)(Y - \mu)).
\]</span></p>
<p>Since <span class="math inline">\(h(\mu)\)</span> is a constant:</p>
<p><span class="math display">\[
\text{var}(h(Y)) = \left(h&#39;(\mu)\right)^2 \text{var}(Y).
\]</span></p>
<p>Substituting <span class="math inline">\(\text{var}(Y) = \left(f(\mu)\right)^2\)</span>, we get:</p>
<p><span class="math display">\[
\text{var}(h(Y)) = \left(h&#39;(\mu)\right)^2 \left(f(\mu)\right)^2.
\]</span></p>
<p>To stabilize the variance (make it constant for all <span class="math inline">\(\mu\)</span>), we require:</p>
<p><span class="math display">\[
\left(h&#39;(\mu)\right)^2 \left(f(\mu)\right)^2 = \text{constant}.
\]</span></p>
<p>Thus, the derivative of <span class="math inline">\(h(\mu)\)</span> must be proportional to the inverse of <span class="math inline">\(f(\mu)\)</span>:</p>
<p><span class="math display">\[
h&#39;(\mu) \propto \frac{1}{f(\mu)}.
\]</span></p>
<p>Integrating both sides gives:</p>
<p><span class="math display">\[
h(\mu) = \int \frac{1}{f(\mu)} \, d\mu.
\]</span></p>
<p>The specific form of <span class="math inline">\(h(\mu)\)</span> depends on the function <span class="math inline">\(f(\mu)\)</span>, which describes the relationship between the standard deviation and the mean.</p>
<hr />
<p>Examples of Variance Stabilizing Transformations</p>
<table>
<colgroup>
<col width="11%" />
<col width="37%" />
<col width="51%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(f(\mu)\)</span></th>
<th><strong>Transformation</strong> <span class="math inline">\(h(Y)\)</span></th>
<th><strong>Purpose</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\sqrt{\mu}\)</span></td>
<td><span class="math inline">\(\int \frac{1}{\sqrt{\mu}} d\mu = 2\sqrt{Y}\)</span></td>
<td>Stabilizes variance for Poisson data.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\int \frac{1}{\mu} d\mu = \ln(Y)\)</span></td>
<td>Stabilizes variance for exponential or multiplicative models.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mu^2\)</span></td>
<td><span class="math inline">\(\int \frac{1}{\mu^2} d\mu = -\frac{1}{Y}\)</span></td>
<td>Stabilizes variance for certain power law data.</td>
</tr>
</tbody>
</table>
<p>Variance stabilizing transformations are particularly useful for:</p>
<ol style="list-style-type: decimal">
<li><strong>Poisson-distributed data</strong>: Use <span class="math inline">\(h(Y) = 2\sqrt{Y}\)</span> to stabilize variance.</li>
<li><strong>Exponential or multiplicative models</strong>: Use <span class="math inline">\(h(Y) = \ln(Y)\)</span> for stabilization.</li>
<li><strong>Power law relationships</strong>: Use transformations like <span class="math inline">\(h(Y) = Y^{-1}\)</span> or other forms derived from <span class="math inline">\(f(\mu)\)</span>.</li>
</ol>
<p>Example: Variance Stabilizing Transformation for the Poisson Distribution</p>
<p>For a <strong>Poisson distribution</strong>, the variance of <span class="math inline">\(Y\)</span> is equal to its mean:</p>
<p><span class="math display">\[
\sigma^2 = \text{var}(Y) = E(Y) = \mu.
\]</span></p>
<p>Thus, the standard deviation is:</p>
<p><span class="math display">\[
\sigma = f(\mu) = \sqrt{\mu}.
\]</span></p>
<p>Using the relationship for variance stabilizing transformations:</p>
<p><span class="math display">\[
h&#39;(\mu) \propto \frac{1}{f(\mu)} = \mu^{-0.5}.
\]</span></p>
<p>Integrating <span class="math inline">\(h&#39;(\mu)\)</span> gives the variance stabilizing transformation:</p>
<p><span class="math display">\[
h(\mu) = \int \mu^{-0.5} \, d\mu = 2\sqrt{\mu}.
\]</span></p>
<p>Hence, the variance stabilizing transformation is:</p>
<p><span class="math display">\[
h(Y) = \sqrt{Y}.
\]</span></p>
<p>This transformation is widely used in Poisson regression to stabilize the variance of the response variable.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="ordinary-least-squares.html#cb169-1" tabindex="-1"></a><span class="co"># Simulate Poisson data</span></span>
<span id="cb169-2"><a href="ordinary-least-squares.html#cb169-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb169-3"><a href="ordinary-least-squares.html#cb169-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb169-4"><a href="ordinary-least-squares.html#cb169-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb169-5"><a href="ordinary-least-squares.html#cb169-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n, <span class="at">lambda =</span> <span class="fu">exp</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.3</span> <span class="sc">*</span> x))  <span class="co"># Poisson-distributed Y</span></span>
<span id="cb169-6"><a href="ordinary-least-squares.html#cb169-6" tabindex="-1"></a></span>
<span id="cb169-7"><a href="ordinary-least-squares.html#cb169-7" tabindex="-1"></a><span class="co"># Fit linear model without transformation</span></span>
<span id="cb169-8"><a href="ordinary-least-squares.html#cb169-8" tabindex="-1"></a>model_raw <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb169-9"><a href="ordinary-least-squares.html#cb169-9" tabindex="-1"></a></span>
<span id="cb169-10"><a href="ordinary-least-squares.html#cb169-10" tabindex="-1"></a><span class="co"># Apply square root transformation</span></span>
<span id="cb169-11"><a href="ordinary-least-squares.html#cb169-11" tabindex="-1"></a>y_trans <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(y)</span>
<span id="cb169-12"><a href="ordinary-least-squares.html#cb169-12" tabindex="-1"></a>model_trans <span class="ot">&lt;-</span> <span class="fu">lm</span>(y_trans <span class="sc">~</span> x)</span>
<span id="cb169-13"><a href="ordinary-least-squares.html#cb169-13" tabindex="-1"></a></span>
<span id="cb169-14"><a href="ordinary-least-squares.html#cb169-14" tabindex="-1"></a><span class="co"># Compare residual plots</span></span>
<span id="cb169-15"><a href="ordinary-least-squares.html#cb169-15" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb169-16"><a href="ordinary-least-squares.html#cb169-16" tabindex="-1"></a></span>
<span id="cb169-17"><a href="ordinary-least-squares.html#cb169-17" tabindex="-1"></a><span class="co"># Residual plot for raw data</span></span>
<span id="cb169-18"><a href="ordinary-least-squares.html#cb169-18" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb169-19"><a href="ordinary-least-squares.html#cb169-19" tabindex="-1"></a>    <span class="fu">fitted</span>(model_raw),</span>
<span id="cb169-20"><a href="ordinary-least-squares.html#cb169-20" tabindex="-1"></a>    <span class="fu">resid</span>(model_raw),</span>
<span id="cb169-21"><a href="ordinary-least-squares.html#cb169-21" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;Residuals Raw Data&quot;</span>,</span>
<span id="cb169-22"><a href="ordinary-least-squares.html#cb169-22" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>,</span>
<span id="cb169-23"><a href="ordinary-least-squares.html#cb169-23" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span></span>
<span id="cb169-24"><a href="ordinary-least-squares.html#cb169-24" tabindex="-1"></a>)</span>
<span id="cb169-25"><a href="ordinary-least-squares.html#cb169-25" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb169-26"><a href="ordinary-least-squares.html#cb169-26" tabindex="-1"></a></span>
<span id="cb169-27"><a href="ordinary-least-squares.html#cb169-27" tabindex="-1"></a><span class="co"># Residual plot for transformed data</span></span>
<span id="cb169-28"><a href="ordinary-least-squares.html#cb169-28" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb169-29"><a href="ordinary-least-squares.html#cb169-29" tabindex="-1"></a>    <span class="fu">fitted</span>(model_trans),</span>
<span id="cb169-30"><a href="ordinary-least-squares.html#cb169-30" tabindex="-1"></a>    <span class="fu">resid</span>(model_trans),</span>
<span id="cb169-31"><a href="ordinary-least-squares.html#cb169-31" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;Residuals: Transformed Data (sqrt(Y))&quot;</span>,</span>
<span id="cb169-32"><a href="ordinary-least-squares.html#cb169-32" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>,</span>
<span id="cb169-33"><a href="ordinary-least-squares.html#cb169-33" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span></span>
<span id="cb169-34"><a href="ordinary-least-squares.html#cb169-34" tabindex="-1"></a>)</span>
<span id="cb169-35"><a href="ordinary-least-squares.html#cb169-35" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="general-strategy-when-fmu-is-unknown" class="section level5 hasAnchor" number="5.1.1.14.3">
<h5><span class="header-section-number">5.1.1.14.3</span> General Strategy When <span class="math inline">\(f(\mu)\)</span> Is Unknown<a href="ordinary-least-squares.html#general-strategy-when-fmu-is-unknown" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>If the relationship between <span class="math inline">\(\text{var}(Y)\)</span> and <span class="math inline">\(\mu\)</span> (i.e., <span class="math inline">\(f(\mu)\)</span>) is unknown, the following steps can help:</p>
<ol style="list-style-type: decimal">
<li><strong>Trial and Error</strong>:
<ul>
<li>Apply common transformations (e.g., <span class="math inline">\(\log(Y)\)</span>, <span class="math inline">\(\sqrt{Y}\)</span>) and examine residual plots.</li>
<li>Select the transformation that results in stabilized variance (residuals show no pattern in plots).</li>
</ul></li>
<li><strong>Leverage Prior Research</strong>:
<ul>
<li>Consult researchers or literature on similar experiments to determine the transformations typically used.</li>
</ul></li>
<li><strong>Analyze Observations with the Same Predictor Value</strong>:
<ul>
<li>If multiple observations <span class="math inline">\(Y_{ij}\)</span> are available at the same <span class="math inline">\(X\)</span> value:
<ul>
<li>Compute the mean <span class="math inline">\(\bar{Y}_i\)</span> and standard deviation <span class="math inline">\(s_i\)</span> for each group.</li>
<li>Check if <span class="math inline">\(s_i \propto \bar{Y}_i^{\lambda}\)</span>.
<ul>
<li>For example, assume: <span class="math display">\[
s_i = a \bar{Y}_i^{\lambda}.
\]</span></li>
<li>Taking the natural logarithm: <span class="math display">\[
\ln(s_i) = \ln(a) + \lambda \ln(\bar{Y}_i).
\]</span></li>
</ul></li>
<li>Perform a regression of <span class="math inline">\(\ln(s_i)\)</span> on <span class="math inline">\(\ln(\bar{Y}_i)\)</span> to estimate <span class="math inline">\(\lambda\)</span> and suggest the form of <span class="math inline">\(f(\mu)\)</span>.</li>
</ul></li>
</ul></li>
<li><strong>Group Observations</strong>:
<ul>
<li>If individual observations are sparse, try grouping similar observations by <span class="math inline">\(X\)</span> values to compute <span class="math inline">\(\bar{Y}_i\)</span> and <span class="math inline">\(s_i\)</span> for each group.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="common-transformations-and-their-applications" class="section level5 hasAnchor" number="5.1.1.14.4">
<h5><span class="header-section-number">5.1.1.14.4</span> Common Transformations and Their Applications<a href="ordinary-least-squares.html#common-transformations-and-their-applications" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The table below summarizes common transformations used to stabilize variance under various conditions, along with their appropriate contexts and comments:</p>
<table>
<colgroup>
<col width="19%" />
<col width="32%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Transformation</strong></th>
<th><strong>Situation</strong></th>
<th><strong>Comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\sqrt{Y}\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, E(Y_i)\)</span></td>
<td>For counts following a Poisson distribution.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sqrt{Y} + \sqrt{Y+1}\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, E(Y_i)\)</span></td>
<td>Useful for small counts or datasets with zeros.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\log(Y)\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, (E(Y_i))^2\)</span></td>
<td>Appropriate for positive integers with a wide range.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\log(Y+1)\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, (E(Y_i))^2\)</span></td>
<td>Used when the data includes zero counts.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(1/Y\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, (E(Y_i))^4\)</span></td>
<td>For responses mostly near zero, with occasional large values.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\arcsin(\sqrt{Y})\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, E(Y_i)(1-E(Y_i))\)</span></td>
<td>Suitable for binomial proportions or percentage data.</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: decimal">
<li><strong>Choosing the Transformation</strong>:
<ul>
<li>Start by identifying the relationship between the variance of the residuals (<span class="math inline">\(var(\epsilon_i)\)</span>) and the mean of the response variable (<span class="math inline">\(E(Y_i)\)</span>).</li>
<li>Select the transformation that matches the identified variance structure.</li>
</ul></li>
<li><strong>Transformations for Zero Values</strong>:
<ul>
<li>For data with zeros, transformations like <span class="math inline">\(\sqrt{Y+1}\)</span> or <span class="math inline">\(\log(Y+1)\)</span> can be used to avoid undefined values. But this will seriously jeopardize model assumption <span class="citation">(<a href="#ref-chen2024logs">J. Chen and Roth 2024</a>)</span>.</li>
</ul></li>
<li><strong>Use in Regression Models</strong>:
<ul>
<li>Apply these transformations to the dependent variable <span class="math inline">\(Y\)</span> in the regression model.</li>
<li>Always check residual plots to confirm that the transformation stabilizes variance and resolves non-linearity.</li>
</ul></li>
<li><strong>Interpretation After Transformation</strong>:
<ul>
<li>After transforming <span class="math inline">\(Y\)</span>, interpret the results in terms of the transformed variable.</li>
<li>For practical interpretation, back-transform predictions and account for any associated bias.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Multiple Linear Regression<a href="ordinary-least-squares.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The geometry of least squares regression involves projecting the response vector <span class="math inline">\(\mathbf{y}\)</span> onto the space spanned by the columns of the design matrix <span class="math inline">\(\mathbf{X}\)</span>. The fitted values <span class="math inline">\(\mathbf{\hat{y}}\)</span> can be expressed as:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{\hat{y}} &amp;= \mathbf{Xb} \\
&amp;= \mathbf{X(X&#39;X)^{-1}X&#39;y} \\
&amp;= \mathbf{Hy},
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{H} = \mathbf{X(X&#39;X)^{-1}X&#39;}\)</span> is the projection operator (sometimes denoted as <span class="math inline">\(\mathbf{P}\)</span>).</li>
<li><span class="math inline">\(\mathbf{\hat{y}}\)</span> is the projection of <span class="math inline">\(\mathbf{y}\)</span> onto the linear space spanned by the columns of <span class="math inline">\(\mathbf{X}\)</span> (the model space).</li>
</ul>
<p>The dimension of the model space is equal to the rank of <span class="math inline">\(\mathbf{X}\)</span> (i.e., the number of linearly independent columns in <span class="math inline">\(\mathbf{X}\)</span>).</p>
<hr />
<p>Properties of the Projection Matrix <span class="math inline">\(\mathbf{H}\)</span></p>
<ol style="list-style-type: decimal">
<li>Symmetry:
<ul>
<li>The projection matrix <span class="math inline">\(\mathbf{H}\)</span> is symmetric: <span class="math display">\[
\mathbf{H} = \mathbf{H}&#39;.
\]</span></li>
</ul></li>
<li>Idempotence:
<ul>
<li>Applying <span class="math inline">\(\mathbf{H}\)</span> twice gives the same result: <span class="math display">\[
\mathbf{HH} = \mathbf{H}.
\]</span> Proof: <span class="math display">\[
\begin{aligned}
\mathbf{HH} &amp;= \mathbf{X(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}X&#39;} \\
&amp;= \mathbf{X(X&#39;X)^{-1}IX&#39;} \\
&amp;= \mathbf{X(X&#39;X)^{-1}X&#39;} \\
&amp;= \mathbf{H}.
\end{aligned}
\]</span></li>
</ul></li>
<li>Dimensionality:
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix (where <span class="math inline">\(n\)</span> is the number of observations).</li>
<li>The rank of <span class="math inline">\(\mathbf{H}\)</span> is equal to the rank of <span class="math inline">\(\mathbf{X}\)</span>, which is typically the number of predictors (including the intercept).</li>
</ul></li>
<li>Orthogonal Complement:
<ul>
<li>The matrix <span class="math inline">\(\mathbf{(I - H)}\)</span>, where: <span class="math display">\[
\mathbf{I - H} = \mathbf{I - X(X&#39;X)^{-1}X&#39;},
\]</span> is also a projection operator.</li>
<li>It projects onto the orthogonal complement of the space spanned by the columns of <span class="math inline">\(\mathbf{X}\)</span> (i.e., the space orthogonal to the model space).</li>
</ul></li>
<li>Orthogonality of Projections:
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{(I - H)}\)</span> are orthogonal: <span class="math display">\[
\mathbf{H(I - H)} = \mathbf{0}.
\]</span></li>
<li>Similarly: <span class="math display">\[
\mathbf{(I - H)H} = \mathbf{0}.
\]</span></li>
</ul></li>
</ol>
<hr />
<p>Intuition for <span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{(I - H)}\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span>: Projects <span class="math inline">\(\mathbf{y}\)</span> onto the model space, giving the fitted values <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</li>
<li><span class="math inline">\(\mathbf{I - H}\)</span>: Projects <span class="math inline">\(\mathbf{y}\)</span> onto the residual space, giving the residuals <span class="math inline">\(\mathbf{e}\)</span>: <span class="math display">\[
\mathbf{e} = \mathbf{(I - H)y}.
\]</span></li>
<li><span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{(I - H)}\)</span> divide the response vector <span class="math inline">\(\mathbf{y}\)</span> into two components: <span class="math display">\[
\mathbf{y} = \mathbf{\hat{y}} + \mathbf{e}.
\]</span>
<ul>
<li><span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Hy}\)</span> (fitted values).</li>
<li><span class="math inline">\(\mathbf{e} = \mathbf{(I - H)y}\)</span> (residuals).</li>
</ul></li>
<li>The properties of <span class="math inline">\(\mathbf{H}\)</span> (symmetry, idempotence, dimensionality) reflect its role as a linear transformation that projects vectors onto the model space.</li>
</ul>
<p>This geometric perspective provides insight into the mechanics of least squares regression, particularly how the response variable <span class="math inline">\(\mathbf{y}\)</span> is decomposed into fitted values and residuals.</p>
<hr />
<p>Similar to simple regression, the total sum of squares in multiple regression analysis can be partitioned into components corresponding to the regression (model fit) and the residuals (errors).</p>
<p>The uncorrected total sum of squares is:</p>
<p><span class="math display">\[
\mathbf{y&#39;y} = \mathbf{\hat{y}&#39;\hat{y} + e&#39;e},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{\hat{y} = Hy}\)</span> (fitted values, projected onto the model space).</p></li>
<li><p><span class="math inline">\(\mathbf{e = (I - H)y}\)</span> (residuals, projected onto the orthogonal complement of the model space).</p></li>
</ul>
<p>Expanding this using projection matrices:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y&#39;y} &amp;= \mathbf{(Hy)&#39;(Hy) + ((I-H)y)&#39;((I-H)y)} \\
&amp;= \mathbf{y&#39;H&#39;Hy + y&#39;(I-H)&#39;(I-H)y} \\
&amp;= \mathbf{y&#39;Hy + y&#39;(I-H)y}.
\end{aligned}
\]</span></p>
<p>This equation shows the partition of <span class="math inline">\(\mathbf{y&#39;y}\)</span> into components explained by the model (<span class="math inline">\(\mathbf{\hat{y}}\)</span>) and the unexplained variation (residuals).</p>
<p>For the corrected total sum of squares, we adjust for the mean (using the projection matrix <span class="math inline">\(\mathbf{H_1}\)</span>):</p>
<p><span class="math display">\[
\mathbf{y&#39;(I-H_1)y = y&#39;(H-H_1)y + y&#39;(I-H)y}.
\]</span></p>
<p>Here:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{H_1} = \frac{1}{n} \mathbf{J}\)</span>, where <span class="math inline">\(\mathbf{J}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix of ones.</p></li>
<li><p><span class="math inline">\(\mathbf{H - H_1}\)</span> projects onto the subspace explained by the predictors after centering.</p></li>
</ul>
<table style="width:99%;">
<colgroup>
<col width="12%" />
<col width="39%" />
<col width="46%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Uncorrected Total Sum of Squares (<span class="math inline">\(\mathbf{y&#39;y}\)</span>)</th>
<th>Corrected Total Sum of Squares (<span class="math inline">\(\mathbf{y&#39;(I-H_1)y}\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Definition</td>
<td>Total variation in <span class="math inline">\(y\)</span> relative to the origin.</td>
<td>Total variation in <span class="math inline">\(y\)</span> relative to its mean (centered data).</td>
</tr>
<tr class="even">
<td>Adjustment</td>
<td>No adjustment for the mean of <span class="math inline">\(y\)</span>.</td>
<td>Adjusts for the mean of <span class="math inline">\(y\)</span> by centering it.</td>
</tr>
<tr class="odd">
<td>Equation</td>
<td><span class="math inline">\(\mathbf{y&#39;y} = \mathbf{\hat{y}&#39;\hat{y}} + \mathbf{e&#39;e}\)</span></td>
<td><span class="math inline">\(\mathbf{y&#39;(I-H_1)y} = \mathbf{y&#39;(H-H_1)y} + \mathbf{y&#39;(I-H)y}\)</span></td>
</tr>
<tr class="even">
<td>Projection Matrices</td>
<td><p><span class="math inline">\(\mathbf{H}\)</span>: Projects onto model space.</p>
<p><span class="math inline">\(\mathbf{I-H}\)</span>: Projects onto residuals.</p></td>
<td><p><span class="math inline">\(\mathbf{H_1} = \frac{1}{n} \mathbf{J}\)</span>: Adjusts for the mean.</p>
<p><span class="math inline">\(\mathbf{H-H_1}\)</span>: Projects onto predictors after centering.</p>
<p><span class="math inline">\(\mathbf{I-H}\)</span>: Projects onto residuals.</p></td>
</tr>
<tr class="odd">
<td>Interpretation</td>
<td>Includes variation due to the mean of <span class="math inline">\(y\)</span>.</td>
<td>Focuses on variation in <span class="math inline">\(y\)</span> around its mean.</td>
</tr>
<tr class="even">
<td>Usage</td>
<td>Suitable for raw, uncentered data.</td>
<td>Common in regression and ANOVA to isolate variability explained by predictors.</td>
</tr>
<tr class="odd">
<td>Application</td>
<td>Measures total variability in <span class="math inline">\(y\)</span>, including overall level (mean).</td>
<td>Measures variability explained by predictors relative to the mean.</td>
</tr>
</tbody>
</table>
<p>Why the Correction Matters</p>
<ul>
<li><p>In ANOVA and regression, removing the contribution of the mean helps isolate the variability explained by predictors from the overall level of the response variable.</p></li>
<li><p>Corrected sums of squares are more common when comparing models or computing <span class="math inline">\(R^2\)</span>, which requires centering to ensure consistency in proportionate variance explained.</p></li>
</ul>
<p>The corrected total sum of squares can be decomposed into the sum of squares for regression (SSR) and the sum of squares for error (SSE):</p>
<table style="width:100%;">
<colgroup>
<col width="12%" />
<col width="42%" />
<col width="9%" />
<col width="21%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th>Source</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td><span class="math inline">\(SSR = \mathbf{y&#39;(H - \frac{1}{n} J)y}\)</span></td>
<td><span class="math inline">\(p - 1\)</span></td>
<td><span class="math inline">\(MSR = SSR / (p-1)\)</span></td>
<td><span class="math inline">\(MSR / MSE\)</span></td>
</tr>
<tr class="even">
<td>Error</td>
<td><span class="math inline">\(SSE = \mathbf{y&#39;(I - H)y}\)</span></td>
<td><span class="math inline">\(n - p\)</span></td>
<td><span class="math inline">\(MSE = SSE / (n-p)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(SST = \mathbf{y&#39;(I - H_1)y}\)</span></td>
<td><span class="math inline">\(n - 1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(p\)</span>: Number of parameters (including intercept).</p></li>
<li><p><span class="math inline">\(n\)</span>: Number of observations.</p></li>
</ul>
<hr />
<p>Alternatively, the regression model can be expressed as:</p>
<p><span class="math display">\[
\mathbf{Y = X\hat{\beta} + (Y - X\hat{\beta})},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{\hat{Y} = X\hat{\beta}}\)</span>: Vector of fitted values (in the subspace spanned by <span class="math inline">\(\mathbf{X}\)</span>).</p></li>
<li><p><span class="math inline">\(\mathbf{e = Y - X\hat{\beta}}\)</span>: Vector of residuals (in the orthogonal complement of the subspace spanned by <span class="math inline">\(\mathbf{X}\)</span>).</p></li>
<li><p><span class="math inline">\(\mathbf{Y}\)</span> is an <span class="math inline">\(n \times 1\)</span> vector in the <span class="math inline">\(n\)</span>-dimensional space <span class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times p\)</span> full-rank matrix, with its columns generating a <span class="math inline">\(p\)</span>-dimensional subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. Hence, any estimator <span class="math inline">\(\mathbf{X\hat{\beta}}\)</span> is also in this subspace.</p></li>
</ul>
<p>In linear regression, the Ordinary Least Squares estimator <span class="math inline">\(\hat{\beta}\)</span> minimizes the squared Euclidean distance <span class="math inline">\(\|\mathbf{Y} - \mathbf{X}\beta\|^2\)</span> between the observed response vector <span class="math inline">\(\mathbf{Y}\)</span> and the fitted values <span class="math inline">\(\mathbf{X}\beta\)</span>. This minimization corresponds to the orthogonal projection of <span class="math inline">\(\mathbf{Y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>We solve the optimization problem:</p>
<p><span class="math display">\[
\min_{\beta} \|\mathbf{Y} - \mathbf{X}\beta\|^2.
\]</span></p>
<p>The objective function can be expanded as:</p>
<p><span class="math display">\[
\|\mathbf{Y} - \mathbf{X}\beta\|^2
= (\mathbf{Y} - \mathbf{X}\beta)^\top (\mathbf{Y} - \mathbf{X}\beta).
\]</span></p>
<p>Perform the multiplication:</p>
<p><span class="math display">\[
\begin{aligned}
(\mathbf{Y} - \mathbf{X}\beta)^\top (\mathbf{Y} - \mathbf{X}\beta)
&amp;= \mathbf{Y}^\top \mathbf{Y}
- \mathbf{Y}^\top \mathbf{X}\beta
- \beta^\top \mathbf{X}^\top \mathbf{Y}
+ \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta.
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{Y}^\top \mathbf{X}\beta\)</span> is a scalar, it equals <span class="math inline">\(\beta^\top \mathbf{X}^\top \mathbf{Y}\)</span>. Therefore, the expanded expression becomes:</p>
<p><span class="math display">\[
\|\mathbf{Y} - \mathbf{X}\beta\|^2
= \mathbf{Y}^\top \mathbf{Y}
- 2\beta^\top \mathbf{X}^\top \mathbf{Y}
+ \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta.
\]</span></p>
<p>To find the <span class="math inline">\(\beta\)</span> that minimizes this expression, take the derivative with respect to <span class="math inline">\(\beta\)</span> and set it to 0:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \beta}
\left[
  \mathbf{Y}^\top \mathbf{Y}
  - 2\beta^\top \mathbf{X}^\top \mathbf{Y}
  + \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta
\right]
= 0
\]</span></p>
<p>Computing the gradient:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \beta} = -2\mathbf{X}^\top \mathbf{Y} + 2(\mathbf{X}^\top \mathbf{X})\beta.
\]</span></p>
<p>Setting this to zero:</p>
<p><span class="math display">\[
-2\mathbf{X}^\top \mathbf{Y} + 2\mathbf{X}^\top \mathbf{X}\beta = 0.
\]</span></p>
<p>Simplify:</p>
<p><span class="math display">\[
\mathbf{X}^\top \mathbf{X}\beta = \mathbf{X}^\top \mathbf{Y}.
\]</span></p>
<p>If <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is invertible, the solution is:</p>
<p><span class="math display">\[
\hat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}.
\]</span></p>
<hr />
<p>Orthogonal Projection Interpretation</p>
<p>The fitted values are:</p>
<p><span class="math display">\[
\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta}.
\]</span></p>
<p>From the normal equations, <span class="math inline">\(\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\hat{\beta}) = 0\)</span>, which implies that the residual vector <span class="math inline">\(\mathbf{Y} - \hat{\mathbf{Y}}\)</span> is orthogonal to every column of <span class="math inline">\(\mathbf{X}\)</span>. Therefore:</p>
<ul>
<li><span class="math inline">\(\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta}\)</span> is the orthogonal projection of <span class="math inline">\(\mathbf{Y}\)</span> onto <span class="math inline">\(\mathrm{Col}(\mathbf{X})\)</span>.</li>
<li><span class="math inline">\(\mathbf{Y} - \mathbf{X}\hat{\beta}\)</span> lies in the orthogonal complement of <span class="math inline">\(\mathrm{Col}(\mathbf{X})\)</span>.</li>
</ul>
<hr />
<p>Pythagoras Decomposition</p>
<p>The geometric interpretation gives us the decomposition:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X}\hat{\beta} + (\mathbf{Y} - \mathbf{X}\hat{\beta}),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}\hat{\beta}\)</span> is the projection of <span class="math inline">\(\mathbf{Y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li><p><span class="math inline">\((\mathbf{Y} - \mathbf{X}\hat{\beta})\)</span> is the residual vector, orthogonal to <span class="math inline">\(\mathbf{X}\hat{\beta}\)</span>.</p></li>
</ul>
<p>Since the two components are orthogonal, their squared norms satisfy:</p>
<p><span class="math display">\[
\begin{aligned}\|\mathbf{Y}\|^2 &amp;= \mathbf{Y}^\top \mathbf{Y}&amp;&amp; \text{(definition of norm squared)} \\[6pt]&amp;= (\mathbf{Y} - \mathbf{X}\hat{\beta} + \mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta} + \mathbf{X}\hat{\beta})&amp;&amp; \text{(add and subtract the same term } \mathbf{X}\hat{\beta}\text{)} \\[6pt]&amp;= (\mathbf{Y} - \mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; 2\,(\mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; (\mathbf{X}\hat{\beta})^\top(\mathbf{X}\hat{\beta})&amp;&amp; \text{(expand }(a+b)^\top(a+b)\text{)} \\[6pt]&amp;= \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2\;+\; 2\,(\mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; \|\mathbf{X}\hat{\beta}\|^2&amp;&amp; \text{(rewrite each quadratic form as a norm)} \\[6pt]&amp;= \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2 + \|\mathbf{X}\hat{\beta}\|^2&amp;&amp; \text{(use that }(\mathbf{X}\hat{\beta})^\top(\mathbf{Y}-\mathbf{X}\hat{\beta}) = 0\text{, i.e. orthogonality)} \\[6pt]&amp; \quad = \|\mathbf{X}\hat{\beta}\|^2 \;+\; \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2.
\end{aligned}
\]</span></p>
<p>where the norm of a vector <span class="math inline">\(\mathbf{a}\)</span> in <span class="math inline">\(\mathbb{R}^p\)</span> is defined as:</p>
<p><span class="math display">\[
\|\mathbf{a}\| = \sqrt{\mathbf{a}^\top \mathbf{a}} = \sqrt{\sum_{i=1}^p a_i^2}.
\]</span></p>
<p>We are saying that <span class="math inline">\(\mathbf{Y}\)</span> is decomposed into two orthogonal components:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf{X}\hat{\beta}\)</span> (the projection onto <span class="math inline">\(\mathrm{Col}(\mathbf{X})\)</span></p>
<ul>
<li><span class="math inline">\(\|\mathbf{X}\hat{\beta}\|\)</span> measures the part of <span class="math inline">\(\mathbf{Y}\)</span> explained by the model.</li>
</ul></li>
<li><p><span class="math inline">\(\mathbf{Y} - \mathbf{X}\hat{\beta}\)</span> (the residual lying in the orthogonal complement).</p>
<ul>
<li><span class="math inline">\(\|\mathbf{Y} - \mathbf{X}\hat{\beta}\|\)</span> measures the residual error.</li>
</ul></li>
</ol>
<p>This geometric interpretation (projection plus orthogonal remainder) is exactly why we call <span class="math inline">\(\mathbf{X}\hat{\beta}\)</span> the <em>orthogonal projection</em> of <span class="math inline">\(\mathbf{Y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span>. This decomposition also underlies the analysis of variance (ANOVA) in regression.</p>
<hr />
<p>The coefficient of multiple determination, denoted <span class="math inline">\(R^2\)</span>, measures the proportion of the total variation in the response variable (<span class="math inline">\(\mathbf{Y}\)</span>) that is explained by the regression model. It is defined as:</p>
<p><span class="math display">\[
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(SSR\)</span>: Regression sum of squares (variation explained by the model).</p></li>
<li><p><span class="math inline">\(SSE\)</span>: Error sum of squares (unexplained variation).</p></li>
<li><p><span class="math inline">\(SSTO\)</span>: Total sum of squares (total variation in <span class="math inline">\(\mathbf{Y}\)</span>).</p></li>
</ul>
<p>The adjusted <span class="math inline">\(R^2\)</span> adjusts <span class="math inline">\(R^2\)</span> for the number of predictors in the model, penalizing for adding predictors that do not improve the model’s fit substantially. It is defined as:</p>
<p><span class="math display">\[
R^2_a = 1 - \frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \frac{(n-1)SSE}{(n-p)SSTO},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: Number of observations.</p></li>
<li><p><span class="math inline">\(p\)</span>: Number of parameters (including the intercept).</p></li>
</ul>
<hr />
<p>Key Differences Between <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_a\)</span></p>
<table>
<colgroup>
<col width="9%" />
<col width="44%" />
<col width="45%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th><span class="math inline">\(R^2\)</span></th>
<th><span class="math inline">\(R^2_a\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Behavior with Predictors</td>
<td>Always increases (or remains constant) when more predictors are added, even if they are not statistically significant.</td>
<td>Includes a penalty for the number of predictors. May decrease if added predictors do not improve the model sufficiently.</td>
</tr>
<tr class="even">
<td>Interpretation</td>
<td>Proportion of the total variation in <span class="math inline">\(\mathbf{Y}\)</span> explained by the regression model.</td>
<td>Adjusted measure of explained variance, accounting for model complexity.</td>
</tr>
<tr class="odd">
<td>Range</td>
<td>Ranges between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</td>
<td>Can be lower than <span class="math inline">\(R^2\)</span>, particularly when the model includes irrelevant predictors.</td>
</tr>
<tr class="even">
<td>Usefulness</td>
<td>Useful for understanding the overall fit of the model.</td>
<td>Useful for comparing models with different numbers of predictors.</td>
</tr>
</tbody>
</table>
<p>In multiple regression, <span class="math inline">\(R^2_a\)</span> provides a more reliable measure of model fit, especially when comparing models with different numbers of predictors.</p>
<hr />
<p>In a regression model with coefficients <span class="math inline">\(\beta = (\beta_0, \beta_1, \dots, \beta_{p-1})^\top\)</span>, the sums of squares are used to evaluate the contribution of predictors to explaining the variation in the response variable.</p>
<ol style="list-style-type: decimal">
<li>Model Sums of Squares:
<ul>
<li><span class="math inline">\(SSM\)</span>: Total model sum of squares, capturing the variation explained by all predictors: <span class="math display">\[
SSM = SS(\beta_0, \beta_1, \dots, \beta_{p-1}).
\]</span></li>
</ul></li>
<li>Marginal Contribution:
<ul>
<li><span class="math inline">\(SSM_m\)</span>: Conditional model sum of squares, capturing the variation explained by predictors after accounting for others: <span class="math display">\[
SSM_m = SS(\beta_0, \beta_1, \dots, \beta_{p-1} | \beta_0).
\]</span></li>
</ul></li>
</ol>
<hr />
<p>Decompositions of <span class="math inline">\(SSM_m\)</span></p>
<ol style="list-style-type: decimal">
<li>Sequential Sums of Squares (Type I SS)</li>
</ol>
<ul>
<li><p>Definition:</p>
<ul>
<li>Sequential SS depends on the order in which predictors are added to the model.</li>
<li>It represents the additional contribution of each predictor given only the predictors that precede it in the sequence.</li>
</ul></li>
<li><p>Formula: <span class="math display">\[
SSM_m = SS(\beta_1 | \beta_0) + SS(\beta_2 | \beta_0, \beta_1) + \dots + SS(\beta_{p-1} | \beta_0, \dots, \beta_{p-2}).
\]</span></p></li>
<li><p>Key Points:</p>
<ul>
<li>Sequential SS is not unique; it depends on the order of the predictors.</li>
<li>Default in many statistical software functions (e.g., <code>anova()</code> in R).</li>
</ul></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Marginal Sums of Squares (Type II SS)</li>
</ol>
<ul>
<li>Definition:
<ul>
<li>Marginal SS evaluates the contribution of a predictor after accounting for all other predictors except those with which it is collinear.</li>
<li>It ignores hierarchical relationships or interactions, focusing on independent contributions.</li>
</ul></li>
<li>Formula: <span class="math inline">\(SSM_m = SS(\beta_j | \beta_1, \dots, \beta_{j-1}, \beta_{j + 1}, \dots, \beta_{p-1})\)</span> where Type II SS evaluates the contribution of <span class="math inline">\(\beta_j\)</span> while excluding any terms collinear with <span class="math inline">\(\beta_j\)</span>.</li>
<li>Key Points:
<ul>
<li>Type II SS is independent of predictor order.</li>
<li>Suitable for models without interaction terms or when predictors are balanced.</li>
</ul></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Partial Sums of Squares (Type III SS)</li>
</ol>
<ul>
<li><p>Definition:</p>
<ul>
<li>Partial SS evaluates the contribution of each predictor after accounting for all other predictors in the model.</li>
<li>It quantifies the unique contribution of a predictor, controlling for the presence of others.</li>
</ul></li>
<li><p>Formula: <span class="math display">\[
SSM_m = SS(\beta_1 | \beta_0, \beta_2, \dots, \beta_{p-1}) + \dots + SS(\beta_{p-1} | \beta_0, \beta_1, \dots, \beta_{p-2}).
\]</span></p></li>
<li><p>Key Points:</p>
<ul>
<li>Partial SS is unique for a given model.</li>
<li>More commonly used in practice for assessing individual predictor importance.</li>
</ul></li>
</ul>
<hr />
<p>Comparison of Sequential, Marginal, and Partial SS</p>
<table>
<colgroup>
<col width="7%" />
<col width="31%" />
<col width="33%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Sequential SS (Type I)</th>
<th>Marginal SS (Type II)</th>
<th>Partial SS (Type III)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dependency</td>
<td>Depends on the order in which predictors are entered.</td>
<td>Independent of order; adjusts for non-collinear predictors.</td>
<td>Independent of order; evaluates unique contributions.</td>
</tr>
<tr class="even">
<td>Usage</td>
<td>Default in software functions like <code>anova()</code> (Type I SS).</td>
<td>Models without interactions or hierarchical dependencies.</td>
<td>Commonly used for hypothesis testing.</td>
</tr>
<tr class="odd">
<td>Interpretation</td>
<td>Measures the additional contribution of predictors in sequence.</td>
<td>Measures the contribution of a predictor, ignoring collinear terms.</td>
<td>Measures the unique contribution of each predictor.</td>
</tr>
<tr class="even">
<td>Uniqueness</td>
<td>Not unique; changes with predictor order.</td>
<td>Unique for a given model without interactions.</td>
<td>Unique for a given model.</td>
</tr>
</tbody>
</table>
<hr />
<p>Practical Notes</p>
<ul>
<li>Use Type III SS (Partial SS) when:
<ul>
<li>The focus is on individual predictor contributions while accounting for all others.</li>
<li>Conducting hypothesis tests on predictors in complex models with interactions or hierarchical structures.</li>
</ul></li>
<li>Use Type II SS (Marginal SS) when:
<ul>
<li>Working with balanced datasets or models without interaction terms.</li>
<li>Ignoring interactions and focusing on independent effects.</li>
</ul></li>
<li>Use Type I SS (Sequential SS) when:
<ul>
<li>Interested in understanding the incremental contribution of predictors based on a specific order of entry (e.g., stepwise regression).</li>
</ul></li>
</ul>
<div id="ols-assumptions" class="section level4 hasAnchor" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> OLS Assumptions<a href="ordinary-least-squares.html#ols-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a></li>
<li><a href="ordinary-least-squares.html#a2-full-rank">A2 Full Rank</a></li>
<li><a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a></li>
<li><a href="ordinary-least-squares.html#a4-homoskedasticity">A4 Homoskedasticity</a></li>
<li><a href="ordinary-least-squares.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></li>
<li><a href="ordinary-least-squares.html#a6-normal-distribution">A6 Normal Distribution</a></li>
</ul>
<hr />
<div id="a1-linearity" class="section level5 hasAnchor" number="5.1.2.1.1">
<h5><span class="header-section-number">5.1.2.1.1</span> A1 Linearity<a href="ordinary-least-squares.html#a1-linearity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The linear regression model is expressed as:</p>
<p><span class="math display">\[
A1: y = \mathbf{x}\beta + \epsilon
\]</span></p>
<p>This assumption is not restrictive since <span class="math inline">\(x\)</span> can include nonlinear transformations (e.g., interactions, natural logarithms, quadratic terms).</p>
<p>However, when combined with A3 (Exogeneity of Independent Variables), linearity can become restrictive.</p>
<hr />
<div id="log-model-variants" class="section level6 hasAnchor" number="5.1.2.1.1.1">
<h6><span class="header-section-number">5.1.2.1.1.1</span> Log Model Variants<a href="ordinary-least-squares.html#log-model-variants" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Logarithmic transformations of variables allow for flexible modeling of nonlinear relationships. Common log model forms include:</p>
<table>
<colgroup>
<col width="7%" />
<col width="28%" />
<col width="22%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Form</th>
<th>Interpretation of <span class="math inline">\(\beta\)</span></th>
<th>In Words</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Level-Level</td>
<td><span class="math inline">\(y = \beta_0 + \beta_1x + \epsilon\)</span></td>
<td><span class="math inline">\(\Delta y = \beta_1 \Delta x\)</span></td>
<td>A unit change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\(\beta_1\)</span> unit change in <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="even">
<td>Log-Level</td>
<td><span class="math inline">\(\ln(y) = \beta_0 + \beta_1x + \epsilon\)</span></td>
<td><span class="math inline">\(\% \Delta y = 100 \beta_1 \Delta x\)</span></td>
<td>A unit change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\(100 \beta_1 \%\)</span> change in <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="odd">
<td>Level-Log</td>
<td><span class="math inline">\(y = \beta_0 + \beta_1 \ln(x) + \epsilon\)</span></td>
<td><span class="math inline">\(\Delta y = (\beta_1/100)\% \Delta x\)</span></td>
<td>A 1% change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\((\beta_1 / 100)\)</span> unit change in <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="even">
<td>Log-Log</td>
<td><span class="math inline">\(\ln(y) = \beta_0 + \beta_1 \ln(x) + \epsilon\)</span></td>
<td><span class="math inline">\(\% \Delta y = \beta_1 \% \Delta x\)</span></td>
<td>A 1% change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\(\beta_1 \%\)</span> change in <span class="math inline">\(y\)</span>.</td>
</tr>
</tbody>
</table>
<hr />
</div>
<div id="higher-order-models" class="section level6 hasAnchor" number="5.1.2.1.1.2">
<h6><span class="header-section-number">5.1.2.1.1.2</span> Higher-Order Models<a href="ordinary-least-squares.html#higher-order-models" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Higher-order terms allow the effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> to depend on the level of <span class="math inline">\(x_1\)</span>. For example:</p>
<p><span class="math display">\[
y = \beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon
\]</span></p>
<p>The partial derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(x_1\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial y}{\partial x_1} = \beta_1 + 2x_1\beta_2
\]</span></p>
<ul>
<li>The effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> depends on the value of <span class="math inline">\(x_1\)</span>.</li>
<li>Partial Effect at the Average: <span class="math inline">\(\beta_1 + 2E(x_1)\beta_2\)</span>.</li>
<li>Average Partial Effect: <span class="math inline">\(E(\beta_1 + 2x_1\beta_2)\)</span>.</li>
</ul>
<hr />
</div>
<div id="interaction-terms" class="section level6 hasAnchor" number="5.1.2.1.1.3">
<h6><span class="header-section-number">5.1.2.1.1.3</span> Interaction Terms<a href="ordinary-least-squares.html#interaction-terms" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Interactions capture the joint effect of two variables. For example:</p>
<p><span class="math display">\[
y = \beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon
\]</span></p>
<ul>
<li><span class="math inline">\(\beta_1\)</span> is the average effect of a unit change in <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> when <span class="math inline">\(x_2 = 0\)</span>.</li>
<li>The partial effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span>, which depends on the level of <span class="math inline">\(x_2\)</span>, is:</li>
</ul>
<p><span class="math display">\[
\beta_1 + x_2\beta_3.
\]</span></p>
<hr />
</div>
</div>
<div id="a2-full-rank" class="section level5 hasAnchor" number="5.1.2.1.2">
<h5><span class="header-section-number">5.1.2.1.2</span> A2 Full Rank<a href="ordinary-least-squares.html#a2-full-rank" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The full rank assumption ensures the uniqueness and existence of the parameter estimates in the population regression equation. It is expressed as:</p>
<p><span class="math display">\[
A2: \text{rank}(E(\mathbf{x&#39;x})) = k
\]</span></p>
<p>This assumption is also known as the identification condition.</p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>No Perfect Multicollinearity:
<ul>
<li>The columns of <span class="math inline">\(\mathbf{x}\)</span> (the matrix of predictors) must be linearly independent.</li>
<li>No column in <span class="math inline">\(\mathbf{x}\)</span> can be written as a linear combination of other columns.</li>
</ul></li>
<li>Implications:
<ul>
<li>Ensures that each parameter in the regression equation is identifiable and unique.</li>
<li>Prevents computational issues, such as the inability to invert <span class="math inline">\(\mathbf{x&#39;x}\)</span>, which is required for estimating <span class="math inline">\(\hat{\beta}\)</span>.</li>
</ul></li>
</ol>
<p>Example of Violation</p>
<p>If two predictors, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, are perfectly correlated (e.g., <span class="math inline">\(x_2 = 2x_1\)</span>), the rank of <span class="math inline">\(\mathbf{x}\)</span> is reduced, and <span class="math inline">\(\mathbf{x&#39;x}\)</span> becomes singular. In such cases:</p>
<ul>
<li><p>The regression coefficients cannot be uniquely estimated.</p></li>
<li><p>The model fails to satisfy the full rank assumption.</p></li>
</ul>
<hr />
</div>
<div id="a3-exogeneity-of-independent-variables" class="section level5 hasAnchor" number="5.1.2.1.3">
<h5><span class="header-section-number">5.1.2.1.3</span> A3 Exogeneity of Independent Variables<a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The exogeneity assumption ensures that the independent variables (<span class="math inline">\(\mathbf{x}\)</span>) are not systematically related to the error term (<span class="math inline">\(\epsilon\)</span>). It is expressed as:</p>
<p><span class="math display">\[
A3: E[\epsilon | x_1, x_2, \dots, x_k] = E[\epsilon | \mathbf{x}] = 0
\]</span></p>
<p>This assumption is often referred to as strict exogeneity or mean independence (see <a href="probability-theory.html#correlation-and-independence">Correlation and Independence</a>.</p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>Strict Exogeneity:
<ul>
<li>Independent variables carry no information about the error term <span class="math inline">\(\epsilon\)</span>.</li>
<li>By the [Law of Iterated Expectations], <span class="math inline">\(E(\epsilon) = 0\)</span>, which can be satisfied by always including an intercept in the regression model.</li>
</ul></li>
<li>Implication:
<ul>
<li>A3 implies: <span class="math display">\[
E(y | \mathbf{x}) = \mathbf{x}\beta,
\]</span> meaning the conditional mean function is a linear function of <span class="math inline">\(\mathbf{x}\)</span>. This aligns with <a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a>.</li>
</ul></li>
<li>Relationship with Independence:
<ul>
<li>Also referred to as mean independence, which is a weaker condition than full independence (see <a href="probability-theory.html#correlation-and-independence">Correlation and Independence</a>).</li>
</ul></li>
</ol>
<hr />
<div id="a3a-weak-exogeneity" class="section level6 hasAnchor" number="5.1.2.1.3.1">
<h6><span class="header-section-number">5.1.2.1.3.1</span> A3a: Weaker Exogeneity Assumption<a href="ordinary-least-squares.html#a3a-weak-exogeneity" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>A weaker version of the exogeneity assumption is:</p>
<p><span class="math display">\[
A3a: E(\mathbf{x_i&#39;}\epsilon_i) = 0
\]</span></p>
<p>This implies:</p>
<ul>
<li><p>The independent variables (<span class="math inline">\(\mathbf{x}_i\)</span>) are uncorrelated with the error term (<span class="math inline">\(\epsilon_i\)</span>).</p></li>
<li><p>Weaker than mean independence in A3.</p></li>
</ul>
<hr />
<p>Comparison Between A3 and A3a</p>
<table>
<colgroup>
<col width="13%" />
<col width="46%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>A3 (Strict Exogeneity)</th>
<th>A3a (Weaker Exogeneity)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Definition</td>
<td><span class="math inline">\(E(\epsilon | \mathbf{x}) = 0\)</span>.</td>
<td><span class="math inline">\(E(\mathbf{x}_i&#39;\epsilon_i) = 0\)</span>.</td>
</tr>
<tr class="even">
<td>Strength</td>
<td>Stronger assumption; implies A3a.</td>
<td>Weaker assumption; does not imply A3.</td>
</tr>
<tr class="odd">
<td>Interpretation</td>
<td>Predictors provide no information about <span class="math inline">\(\epsilon\)</span>.</td>
<td>Predictors are uncorrelated with <span class="math inline">\(\epsilon\)</span>.</td>
</tr>
<tr class="even">
<td>Causality</td>
<td>Enables causal interpretation.</td>
<td>Does not allow causal interpretations.</td>
</tr>
</tbody>
</table>
<hr />
<p>Notes on Practical Relevance</p>
<ul>
<li>Checking for Exogeneity:
<ul>
<li>Strict exogeneity cannot be tested directly, but violations can manifest as omitted variable bias, endogeneity, or measurement error.</li>
<li>Including all relevant predictors and ensuring accurate measurement can help satisfy this assumption.</li>
</ul></li>
<li>Violations of Exogeneity:
<ul>
<li>If A3 is violated, standard OLS estimates are biased and inconsistent.</li>
<li>In such cases, instrumental variable (IV) methods or other approaches may be required to correct for endogeneity.</li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="a4-homoskedasticity" class="section level5 hasAnchor" number="5.1.2.1.4">
<h5><span class="header-section-number">5.1.2.1.4</span> A4 Homoskedasticity<a href="ordinary-least-squares.html#a4-homoskedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The homoskedasticity assumption ensures that the variance of the error term (<span class="math inline">\(\epsilon\)</span>) is constant across all levels of the independent variables (<span class="math inline">\(\mathbf{x}\)</span>). It is expressed as:</p>
<p><span class="math display">\[
A4: \text{Var}(\epsilon | \mathbf{x}) = \text{Var}(\epsilon) = \sigma^2
\]</span></p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>Definition:
<ul>
<li>The variance of the disturbance term <span class="math inline">\(\epsilon\)</span> is the same for all observations, regardless of the values of the predictors <span class="math inline">\(\mathbf{x}\)</span>.</li>
</ul></li>
<li>Practical Implication:
<ul>
<li>Homoskedasticity ensures that the errors do not systematically vary with the predictors.</li>
<li>This is critical for valid inference, as the standard errors of the coefficients rely on this assumption.</li>
</ul></li>
<li>Violation (Heteroskedasticity):
<ul>
<li>If the variance of <span class="math inline">\(\epsilon\)</span> depends on <span class="math inline">\(\mathbf{x}\)</span>, the assumption is violated.</li>
<li>Common signs include funnel-shaped patterns in residual plots or varying error sizes.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="a5-data-generation-random-sampling" class="section level5 hasAnchor" number="5.1.2.1.5">
<h5><span class="header-section-number">5.1.2.1.5</span> A5 Data Generation (Random Sampling)<a href="ordinary-least-squares.html#a5-data-generation-random-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The random sampling assumption ensures that the observations <span class="math inline">\((y_i, x_{i1}, \dots, x_{ik-1})\)</span> are drawn independently and identically distributed (iid) from the joint distribution of <span class="math inline">\((y, \mathbf{x})\)</span>. It is expressed as:</p>
<p><span class="math display">\[
A5: \{y_i, x_{i1}, \dots, x_{ik-1} : i = 1, \dots, n\}
\]</span></p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>Random Sampling:
<ul>
<li>The dataset is assumed to be a random sample from the population.</li>
<li>Each observation is independent of others and follows the same probability distribution.</li>
</ul></li>
<li>Implications:
<ul>
<li>With <a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3 (Exogeneity of Independent Variables)</a> and <a href="ordinary-least-squares.html#a4-homoskedasticity">A4 (Homoskedasticity)</a>, random sampling implies:
<ul>
<li>Strict Exogeneity: <span class="math display">\[
E(\epsilon_i | x_1, \dots, x_n) = 0
\]</span> Independent variables do not contain information for predicting <span class="math inline">\(\epsilon\)</span>.</li>
<li>Non-Autocorrelation: <span class="math display">\[
E(\epsilon_i \epsilon_j | x_1, \dots, x_n) = 0 \quad \text{for } i \neq j
\]</span> The error terms are uncorrelated across observations, conditional on the independent variables.</li>
<li>Variance of Errors: <span class="math display">\[
\text{Var}(\epsilon | \mathbf{X}) = \text{Var}(\epsilon) = \sigma^2 \mathbf{I}_n
\]</span></li>
</ul></li>
</ul></li>
<li>When A5 May Not Hold:
<ul>
<li>In time series data, where observations are often autocorrelated.</li>
<li>In spatial data, where neighboring observations may not be independent.</li>
</ul></li>
</ol>
<hr />
<p>Practical Considerations</p>
<ul>
<li>Time Series Data:
<ul>
<li>Use methods such as autoregressive models or generalized least squares (GLS) to address dependency in observations.</li>
</ul></li>
<li>Spatial Data:
<ul>
<li>Spatial econometric models may be required to handle correlation across geographic locations.</li>
</ul></li>
<li>Checking Random Sampling:
<ul>
<li>While true randomness cannot always be verified, exploratory analysis of the residuals (e.g., for patterns or autocorrelation) can help detect violations.</li>
</ul></li>
</ul>
<div id="A5a-stationarity-in-stochastic-processes" class="section level6 hasAnchor" number="5.1.2.1.5.1">
<h6><span class="header-section-number">5.1.2.1.5.1</span> A5a: Stationarity in Stochastic Processes<a href="ordinary-least-squares.html#A5a-stationarity-in-stochastic-processes" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>A stochastic process <span class="math inline">\(\{x_t\}_{t=1}^T\)</span> is stationary if, for every collection of time indices <span class="math inline">\(\{t_1, t_2, \dots, t_m\}\)</span>, the joint distribution of:</p>
<p><span class="math display">\[
x_{t_1}, x_{t_2}, \dots, x_{t_m}
\]</span></p>
<p>is the same as the joint distribution of:</p>
<p><span class="math display">\[
x_{t_1+h}, x_{t_2+h}, \dots, x_{t_m+h}
\]</span></p>
<p>for any <span class="math inline">\(h \geq 1\)</span>.</p>
<p>Key Points on Stationarity</p>
<ol style="list-style-type: decimal">
<li>Definition:
<ul>
<li>A stationary process has statistical properties (mean, variance, and covariance) that are invariant over time.</li>
<li>For example, the joint distribution for the first ten observations is identical to the joint distribution for the next ten observations, regardless of their position in time.</li>
</ul></li>
<li>Implication:
<ul>
<li>Stationarity ensures that the relationships observed in the data remain consistent over time.</li>
<li>Independent draws automatically satisfy the stationarity condition.</li>
</ul></li>
</ol>
<hr />
<p>Weak Stationarity</p>
<p>A stochastic process <span class="math inline">\(\{x_t\}_{t=1}^T\)</span> is weakly stationary if:</p>
<ul>
<li>The covariance between <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_{t+h}\)</span> depends only on the lag <span class="math inline">\(h\)</span> and not on <span class="math inline">\(t\)</span>.</li>
<li>As <span class="math inline">\(h \to \infty\)</span>, the covariance diminishes, meaning <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_{t+h}\)</span> become “almost independent.”</li>
</ul>
<hr />
<p>Differences Between Stationarity and Weak Stationarity</p>
<table>
<colgroup>
<col width="15%" />
<col width="40%" />
<col width="43%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Stationarity</th>
<th>Weak Stationarity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Joint Distribution</td>
<td>Entire joint distribution remains unchanged over time.</td>
<td>Focuses only on the first two moments: mean and covariance.</td>
</tr>
<tr class="even">
<td>Dependence Over Time</td>
<td>Observations at all lags are equally distributed.</td>
<td>Observations far apart are “almost independent.”</td>
</tr>
<tr class="odd">
<td>Application</td>
<td>Ensures strong consistency in time-series processes.</td>
<td>More practical for many time-series applications.</td>
</tr>
</tbody>
</table>
<p>Weak stationarity is often sufficient for many time-series analyses, especially when focusing on correlations and trends rather than full distributions.</p>
<hr />
<p>Common Weakly Dependent Processes</p>
<p>1. Moving Average Process of Order 1 (MA(1))</p>
<p>An MA(1) process models the dependent variable <span class="math inline">\(y_t\)</span> as a function of the current and one-period lagged stochastic error term:</p>
<p><span class="math display">\[
y_t = u_t + \alpha_1 u_{t-1},
\]</span></p>
<p>where <span class="math inline">\(u_t\)</span> is white noise, independently and identically distributed (iid) with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Key Properties</p>
<ol style="list-style-type: decimal">
<li><p>Mean: <span class="math display">\[
E(y_t) = E(u_t) + \alpha_1E(u_{t-1}) = 0
\]</span></p></li>
<li><p>Variance: <span class="math display">\[
\begin{aligned}
\text{Var}(y_t) &amp;= \text{Var}(u_t) + \alpha_1^2 \text{Var}(u_{t-1}) \\
&amp;= \sigma^2 + \alpha_1^2 \sigma^2 \\
&amp;= \sigma^2 (1 + \alpha_1^2)
\end{aligned}
\]</span> An increase in the absolute value of <span class="math inline">\(\alpha_1\)</span> increases the variance.</p></li>
<li><p>Autocovariance:</p>
<ul>
<li>For lag 1: <span class="math display">\[
\text{Cov}(y_t, y_{t-1}) = \alpha_1 \text{Var}(u_{t-1}) = \alpha_1 \sigma^2.
\]</span></li>
<li>For lag 2 or greater: <span class="math display">\[
\text{Cov}(y_t, y_{t-2}) = 0.
\]</span></li>
</ul></li>
</ol>
<p>The MA(1) process is invertible if <span class="math inline">\(|\alpha_1| &lt; 1\)</span>, allowing it to be rewritten as an autoregressive (AR) representation:</p>
<p><span class="math display">\[
u_t = y_t - \alpha_1 u_{t-1}.
\]</span></p>
<p>Invertibility implies that we can express the current observation in terms of past observations.</p>
<hr />
<p>An MA(q) process generalizes the MA(1) process to include <span class="math inline">\(q\)</span> lags:</p>
<p><span class="math display">\[
y_t = u_t + \alpha_1 u_{t-1} + \dots + \alpha_q u_{t-q},
\]</span></p>
<p>where <span class="math inline">\(u_t \sim WN(0, \sigma^2)\)</span> (white noise with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>).</p>
<p>Key Characteristics:</p>
<ol style="list-style-type: decimal">
<li>Covariance Stationary:
<ul>
<li>An MA(q) process is covariance stationary irrespective of the parameter values.</li>
</ul></li>
<li>Invertibility:
<ul>
<li>An MA(q) process is invertible if the parameters satisfy certain conditions (e.g., <span class="math inline">\(|\alpha_i| &lt; 1\)</span>).</li>
</ul></li>
<li>Autocorrelations:
<ul>
<li>The autocorrelations are nonzero for lags up to <span class="math inline">\(q\)</span> but are 0 for lags beyond <span class="math inline">\(q\)</span>.</li>
</ul></li>
<li>Conditional Mean:
<ul>
<li>The conditional mean of <span class="math inline">\(y_t\)</span> depends on the <span class="math inline">\(q\)</span> lags, indicating “long-term memory.”</li>
</ul></li>
</ol>
<p>Example: Autocovariance of an MA(1)</p>
<ol style="list-style-type: decimal">
<li><p>Lag 1: <span class="math display">\[
\begin{aligned}
\text{Cov}(y_t, y_{t-1}) &amp;= \text{Cov}(u_t + \alpha_1 u_{t-1}, u_{t-1} + \alpha_1 u_{t-2}) \\
&amp;= \alpha_1 \text{Var}(u_{t-1}) \\
&amp;= \alpha_1 \sigma^2.
\end{aligned}
\]</span></p></li>
<li><p>Lag 2: <span class="math display">\[
\begin{aligned}
\text{Cov}(y_t, y_{t-2}) &amp;= \text{Cov}(u_t + \alpha_1 u_{t-1}, u_{t-2} + \alpha_1 u_{t-3}) \\
&amp;= 0.
\end{aligned}
\]</span></p></li>
</ol>
<p>An MA process captures a linear relationship between the dependent variable <span class="math inline">\(y_t\)</span> and the current and past values of a stochastic error term <span class="math inline">\(u_t\)</span>. Its properties make it useful for modeling time-series data with limited memory and short-term dependencies.</p>
<ol start="2" style="list-style-type: decimal">
<li>Auto-Regressive Process of Order 1 (AR(1))</li>
</ol>
<p>An auto-regressive process of order 1 (AR(1)) is defined as:</p>
<p><span class="math display">\[
y_t = \rho y_{t-1} + u_t, \quad |\rho| &lt; 1
\]</span></p>
<p>where <span class="math inline">\(u_t\)</span> represents independent and identically distributed (i.i.d.) random noise over <span class="math inline">\(t\)</span> with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Covariance at lag 1:</p>
<p><span class="math display">\[
\begin{aligned}
Cov(y_t, y_{t-1}) &amp;= Cov(\rho y_{t-1} + u_t, y_{t-1}) \\
&amp;= \rho Var(y_{t-1}) \\
&amp;= \rho \frac{\sigma^2}{1-\rho^2}.
\end{aligned}
\]</span></p>
<p>Covariance at lag <span class="math inline">\(h\)</span>:</p>
<p><span class="math display">\[
Cov(y_t, y_{t-h}) = \rho^h \frac{\sigma^2}{1-\rho^2}.
\]</span></p>
<p>Stationarity implies that the distribution of <span class="math inline">\(y_t\)</span> does not change over time, requiring constant mean and variance. For this process:</p>
<ol style="list-style-type: decimal">
<li><p>Mean Stationarity:<br />
Assuming <span class="math inline">\(E(y_t) = 0\)</span>, we have: <span class="math display">\[
y_t = \rho^t y_0 + \rho^{t-1}u_1 + \rho^{t-2}u_2 + \dots + \rho u_{t-1} + u_t.
\]</span></p>
<p>If the initial observation <span class="math inline">\(y_0 = 0\)</span>, then <span class="math inline">\(y_t\)</span> is simply a weighted sum of the random shocks <span class="math inline">\(u_t\)</span> from all prior time periods. Thus, <span class="math inline">\(E(y_t) = 0\)</span> for all <span class="math inline">\(t\)</span>.</p></li>
<li><p>Variance Stationarity:<br />
The variance is computed as: <span class="math display">\[
Var(y_t) = Var(\rho y_{t-1} + u_t).
\]</span></p>
<p>Expanding and simplifying: <span class="math display">\[
Var(y_t) = \rho^2 Var(y_{t-1}) + Var(u_t) + 2\rho Cov(y_{t-1}, u_t).
\]</span> Since <span class="math inline">\(u_t\)</span> is independent of <span class="math inline">\(y_{t-1}\)</span>, <span class="math inline">\(Cov(y_{t-1}, u_t) = 0\)</span>, giving: <span class="math display">\[
Var(y_t) = \rho^2 Var(y_{t-1}) + \sigma^2.
\]</span></p>
<p>Solving recursively, we find: <span class="math display">\[
Var(y_t) = \frac{\sigma^2}{1-\rho^2}.
\]</span></p></li>
</ol>
<p>For the variance to remain constant over time, it is required that <span class="math inline">\(|\rho| &lt; 1\)</span> and <span class="math inline">\(p \notin \{1,-1\}\)</span>.</p>
<p>Key Insights on Stationarity</p>
<ol style="list-style-type: decimal">
<li><p>Stationarity Requirement:<br />
The condition <span class="math inline">\(|\rho| &lt; 1\)</span> ensures stationarity, as this guarantees that both the mean and variance of the process are constant over time.</p></li>
<li><p>Weak Dependence:<br />
As <span class="math inline">\(|\rho| &lt; 1\)</span>, the dependency between observations diminishes with increasing lag <span class="math inline">\(h\)</span>, as seen from the covariance: <span class="math display">\[
Cov(y_t, y_{t-h}) = \rho^h \frac{\sigma^2}{1-\rho^2}.
\]</span></p></li>
</ol>
<p>To estimate an AR(1) process, we utilize the Yule-Walker equations, which relate the autocovariances of the process to its parameters.</p>
<p>Starting with the AR(1) process: <span class="math display">\[
y_t = \epsilon_t + \phi y_{t-1},
\]</span> multiplying both sides by <span class="math inline">\(y_{t-\tau}\)</span> and taking expectations, we get: <span class="math display">\[
y_t y_{t-\tau} = \epsilon_t y_{t-\tau} + \phi y_{t-1} y_{t-\tau}.
\]</span></p>
<p>For <span class="math inline">\(\tau \geq 1\)</span>, the autocovariance <span class="math inline">\(\gamma(\tau)\)</span> satisfies: <span class="math display">\[
\gamma(\tau) = \phi \gamma(\tau - 1).
\]</span></p>
<p>Dividing through by the variance <span class="math inline">\(\gamma(0)\)</span>, we obtain the autocorrelation: <span class="math display">\[
\rho_\tau = \phi^\tau.
\]</span></p>
<p>Thus, the autocorrelations decay geometrically as <span class="math inline">\(\phi^\tau\)</span>, where <span class="math inline">\(|\phi| &lt; 1\)</span> ensures stationarity and decay over time.</p>
<hr />
<p>Generalizing to AR(p)</p>
<p>An AR(p) process extends the AR(1) structure to include <span class="math inline">\(p\)</span> lags: <span class="math display">\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t.
\]</span></p>
<p>Here, <span class="math inline">\(\epsilon_t\)</span> is white noise with <span class="math inline">\(E(\epsilon_t) = 0\)</span> and <span class="math inline">\(Var(\epsilon_t) = \sigma^2\)</span>.</p>
<p>The AR(p) process is covariance stationary if the roots of the characteristic equation lie outside the unit circle: <span class="math display">\[
1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0.
\]</span></p>
<p>For the AR(p) process, the autocorrelations <span class="math inline">\(\rho_\tau\)</span> decay more complexly compared to the AR(1). However, they still diminish over time, ensuring weak dependence among distant observations.</p>
<hr />
<p>The Yule-Walker equations for an AR(p) process provide a system of linear equations to estimate the parameters <span class="math inline">\(\phi_1, \phi_2, \dots, \phi_p\)</span>: <span class="math display">\[
\gamma(\tau) = \phi_1 \gamma(\tau - 1) + \phi_2 \gamma(\tau - 2) + \dots + \phi_p \gamma(\tau - p), \quad \tau \geq 1.
\]</span></p>
<p>This system can be written in matrix form for <span class="math inline">\(\tau = 1, \dots, p\)</span> as:</p>
<p><span class="math display">\[
\begin{bmatrix}
\gamma(1) \\
\gamma(2) \\
\vdots \\
\gamma(p)
\end{bmatrix}
=
\begin{bmatrix}
\gamma(0) &amp; \gamma(1) &amp; \dots &amp; \gamma(p-1) \\
\gamma(1) &amp; \gamma(0) &amp; \dots &amp; \gamma(p-2) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\gamma(p-1) &amp; \gamma(p-2) &amp; \dots &amp; \gamma(0)
\end{bmatrix}
\begin{bmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_p
\end{bmatrix}.
\]</span></p>
<p>This system is solved to estimate the coefficients <span class="math inline">\(\phi_1, \phi_2, \dots, \phi_p\)</span>.</p>
<hr />
<ol start="3" style="list-style-type: decimal">
<li>ARMA(p, q) Process</li>
</ol>
<p>An ARMA(p, q) process combines autoregressive (AR) and moving average (MA) components to model time series data effectively. The general form is:</p>
<p><span class="math display">\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t + \alpha_1 \epsilon_{t-1} + \alpha_2 \epsilon_{t-2} + \dots + \alpha_q \epsilon_{t-q}.
\]</span></p>
<p>A simple case of ARMA(1, 1) is given by: <span class="math display">\[
y_t = \phi y_{t-1} + \epsilon_t + \alpha \epsilon_{t-1},
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(\phi\)</span> captures the autoregressive behavior,</p></li>
<li><p><span class="math inline">\(\alpha\)</span> controls the moving average component,</p></li>
<li><p><span class="math inline">\(\epsilon_t\)</span> represents white noise.</p></li>
</ul>
<p>ARMA processes can capture seasonality and more complex dependencies than pure AR or MA models.</p>
<hr />
<ol start="4" style="list-style-type: decimal">
<li><strong>Random Walk Process</strong></li>
</ol>
<p>A <strong>random walk</strong> is a non-stationary process defined as: <span class="math display">\[
y_t = y_0 + \sum_{s=1}^t u_s,
\]</span> where <span class="math inline">\(u_s\)</span> are i.i.d. random variables.</p>
<p>Properties:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Non-Stationarity</strong>:</p>
<ul>
<li>If <span class="math inline">\(y_0 = 0\)</span>, then <span class="math inline">\(E(y_t) = 0\)</span>, but the variance increases over time: <span class="math display">\[
Var(y_t) = t \sigma^2.
\]</span></li>
</ul></li>
<li><p><strong>Not Weakly Dependent</strong>:</p>
<ul>
<li>The covariance of the process does not diminish with increasing lag <span class="math inline">\(h\)</span>: <span class="math display">\[
Cov\left(\sum_{s=1}^t u_s, \sum_{s=1}^{t-h} u_s\right) = (t-h)\sigma^2.
\]</span></li>
</ul>
<p>As <span class="math inline">\(h\)</span> increases, the covariance remains large, violating the condition for weak dependence.</p></li>
</ol>
<hr />
</div>
<div id="A5a-stationarity-and-weak-dependence-in-time-series" class="section level6 hasAnchor" number="5.1.2.1.5.2">
<h6><span class="header-section-number">5.1.2.1.5.2</span> A5a: Stationarity and Weak Dependence in Time Series<a href="ordinary-least-squares.html#A5a-stationarity-and-weak-dependence-in-time-series" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>For time series data, the set <span class="math inline">\(\{y_t, x_{t1}, \dots, x_{tk-1}\}\)</span>, where <span class="math inline">\(t = 1, \dots, T\)</span>, must satisfy the conditions of <strong>stationarity</strong> and <strong>weak dependence</strong>. These properties are essential to ensure the consistency and efficiency of estimators in time-series regression models.</p>
<ol style="list-style-type: decimal">
<li><strong>Stationarity</strong>: A stationary process has statistical properties (e.g., mean, variance, autocovariance) that remain constant over time. This ensures that the relationships in the data do not change as <span class="math inline">\(t\)</span> progresses, making it possible to draw meaningful inferences.</li>
<li><strong>Weak Dependence</strong>: Weak dependence implies that observations far apart in time are “almost independent.” While there may be short-term correlations, these diminish as the time lag increases. This property ensures that the sample averages are representative of the population mean.</li>
</ol>
<p>The <a href="general-math.html#weak-law-of-large-numbers">Weak Law of Large Numbers</a> provides a foundation for the consistency of sample means. If <span class="math inline">\(\{z_t\}\)</span> is a weakly dependent, stationary process with <span class="math inline">\(E(|z_t|) &lt; \infty\)</span> and <span class="math inline">\(E(z_t) = \mu\)</span>, then:</p>
<p><span class="math display">\[
\frac{1}{T} \sum_{t=1}^T z_t \xrightarrow{p} \mu.
\]</span></p>
<p>Interpretation:</p>
<ul>
<li><p>As the sample size <span class="math inline">\(T \to \infty\)</span>, the sample mean <span class="math inline">\(\bar{z} = \frac{1}{T} \sum_{t=1}^T z_t\)</span> converges in probability to the true mean <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>This ensures the <strong>consistency</strong> of estimators based on time-series data.</p></li>
</ul>
<p>The <a href="probability-theory.html#central-limit-theorem">Central Limit Theorem</a> extends the WLLN by describing the distribution of the sample mean. Under additional regularity conditions (e.g., finite variance) <span class="citation">(<a href="#ref-greene1990gamma">Greene 1990</a>)</span>, the sample mean <span class="math inline">\(\bar{z}\)</span> satisfies:</p>
<p><span class="math display">\[
\sqrt{T}(\bar{z} - \mu) \xrightarrow{d} N(0, B),
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
B = \text{Var}(z_t) + 2\sum_{h=1}^\infty \text{Cov}(z_t, z_{t-h}).
\]</span></p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><p>The sample mean <span class="math inline">\(\bar{z}\)</span> is approximately normally distributed for large <span class="math inline">\(T\)</span>.</p></li>
<li><p>The variance of the limiting distribution, <span class="math inline">\(B\)</span>, depends not only on the variance of <span class="math inline">\(z_t\)</span> but also on the covariances between <span class="math inline">\(z_t\)</span> and its past values.</p></li>
</ul>
</div>
</div>
<div id="a6-normal-distribution" class="section level5 hasAnchor" number="5.1.2.1.6">
<h5><span class="header-section-number">5.1.2.1.6</span> A6 Normal Distribution<a href="ordinary-least-squares.html#a6-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A6: <span class="math inline">\(\epsilon|\mathbf{x} \sim N(0, \sigma^2 I_n)\)</span></p>
<p>The assumption here implies that the error term <span class="math inline">\(\epsilon\)</span> is normally distributed with mean zero and variance <span class="math inline">\(\sigma^2 I_n\)</span>. This assumption is fundamental for statistical inference in linear regression models.</p>
<p>Using assumptions <a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a>, <a href="ordinary-least-squares.html#a2-full-rank">A2 Full Rank</a>, and <a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a>, we derive the identification (or orthogonality condition) for the population parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
y &amp;= x\beta + \epsilon &amp;&amp; \text{(A1: Model Specification)} \\
x&#39;y &amp;= x&#39;x\beta + x&#39;\epsilon &amp;&amp; \text{(Multiply both sides by $x&#39;$)} \\
E(x&#39;y) &amp;= E(x&#39;x)\beta + E(x&#39;\epsilon) &amp;&amp; \text{(Taking expectation)} \\
E(x&#39;y) &amp;= E(x&#39;x)\beta &amp;&amp; \text{(A3: Exogeneity, $E(x&#39;\epsilon) = 0$)} \\
[E(x&#39;x)]^{-1}E(x&#39;y) &amp;= [E(x&#39;x)]^{-1}E(x&#39;x)\beta &amp;&amp; \text{(Invertibility of $E(x&#39;x)$, A2)} \\
[E(x&#39;x)]^{-1}E(x&#39;y) &amp;= \beta &amp;&amp; \text{(Simplified solution for $\beta$)}
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\beta\)</span> is identified as the vector of parameters that minimizes the expected squared error.</p>
<p>To find <span class="math inline">\(\beta\)</span>, we minimize the expected value of the squared error:</p>
<p><span class="math display">\[
\underset{\gamma}{\operatorname{argmin}} \ E\big((y - x\gamma)^2\big)
\]</span></p>
<p>The first-order condition is derived by taking the derivative of the objective function with respect to <span class="math inline">\(\gamma\)</span> and setting it to zero:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial E\big((y - x\gamma)^2\big)}{\partial \gamma} &amp;= 0 \\
-2E(x&#39;(y - x\gamma)) &amp;= 0 \\
E(x&#39;y) - E(x&#39;x\gamma) &amp;= 0 \\
E(x&#39;y) &amp;= E(x&#39;x)\gamma \\
(E(x&#39;x))^{-1}E(x&#39;y) &amp;= \gamma
\end{aligned}
\]</span></p>
<p>This confirms that <span class="math inline">\(\gamma = \beta\)</span>.</p>
<p>The second-order condition ensures that the solution minimizes the objective function. Taking the second derivative:</p>
<p><span class="math display">\[
\frac{\partial^2 E\big((y - x\gamma)^2\big)}{\partial \gamma&#39;^2} = 0 = 2E(x&#39;x)
\]</span></p>
<p>If assumption <a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a> holds, <span class="math inline">\(E(x&#39;x)\)</span> is positive semi-definite (PSD). Thus, <span class="math inline">\(2E(x&#39;x)\)</span> is also PSD, ensuring a minimum.</p>
</div>
<div id="hierarchy-of-ols-assumptions" class="section level5 hasAnchor" number="5.1.2.1.7">
<h5><span class="header-section-number">5.1.2.1.7</span> Hierarchy of OLS Assumptions<a href="ordinary-least-squares.html#hierarchy-of-ols-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>This table summarizes the hierarchical nature of assumptions required to derive different properties of the OLS estimator.</p>
<table style="width:99%;">
<caption>Usage of Assumptions</caption>
<colgroup>
<col width="27%" />
<col width="12%" />
<col width="10%" />
<col width="30%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th>Assumption</th>
<th><strong>Identification Data Description</strong></th>
<th><strong>Unbiasedness Consistency</strong></th>
<th><a href="ordinary-least-squares.html#gauss-markov-theorem"><strong>Gauss-Markov</strong></a> <strong>(BLUE) Asymptotic Inference (z and Chi-squared)</strong></th>
<th><strong>Classical LM (BUE) Small-sample Inference (t and F)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><a href="ordinary-least-squares.html#a2-full-rank">A2 Full Rank</a></p>
<p>Variation in <span class="math inline">\(\mathbf{X}\)</span></p></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td><p><a href="ordinary-least-squares.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></p>
<p>Random Sampling</p></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td><p><a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a></p>
<p>Linearity in Parameters</p></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td><p><a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a></p>
<p>Zero Conditional Mean</p></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td><p><a href="ordinary-least-squares.html#a4-homoskedasticity">A4 Homoskedasticity</a></p>
<p><span class="math inline">\(\mathbf{H}\)</span> homoskedasticity</p></td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td><p><a href="ordinary-least-squares.html#a6-normal-distribution">A6 Normal Distribution</a></p>
<p>Normality of Errors</p></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Identification Data Description:</strong> Ensures the model is identifiable and coefficients can be estimated.</li>
<li><strong>Unbiasedness Consistency:</strong> Guarantees that OLS estimates are unbiased and converge to the true parameter values as the sample size increases.</li>
<li><strong>Gauss-Markov (BLUE) and Asymptotic Inference:</strong> Requires additional assumptions (e.g., homoskedasticity) to ensure minimum variance of estimators and valid inference using large-sample tests (z and chi-squared).</li>
<li><strong>Classical LM (BUE) Small-sample Inference:</strong> Builds on all previous assumptions and adds normality of errors for valid t and F tests in finite samples.</li>
</ul>
<hr />
</div>
</div>
<div id="theorems" class="section level4 hasAnchor" number="5.1.2.2">
<h4><span class="header-section-number">5.1.2.2</span> Theorems<a href="ordinary-least-squares.html#theorems" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="frischwaughlovell-theorem-frischwaughlovell-theorem" class="section level5 hasAnchor" number="5.1.2.2.1">
<h5><span class="header-section-number">5.1.2.2.1</span> Frisch–Waugh–Lovell Theorem {#Frisch–Waugh–Lovell Theorem}<a href="ordinary-least-squares.html#frischwaughlovell-theorem-frischwaughlovell-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The Frisch–Waugh–Lovell (FWL) Theorem is a fundamental result in linear regression that allows for a deeper understanding of how coefficients are computed in a multiple regression setting <span class="citation">(<a href="#ref-lovell2008simple">Lovell 2008</a>)</span>. Informally, it states:</p>
<blockquote>
<p>When estimating the effect of a subset of variables (<span class="math inline">\(X_1\)</span>) on <span class="math inline">\(y\)</span> in the presence of other variables (<span class="math inline">\(X_2\)</span>), you can “partial out” the influence of <span class="math inline">\(X_2\)</span> from both <span class="math inline">\(y\)</span> and <span class="math inline">\(X_1\)</span>. Then, regressing the residuals of <span class="math inline">\(y\)</span> on the residuals of <span class="math inline">\(X_1\)</span> produces coefficients for <span class="math inline">\(X_1\)</span> that are identical to those obtained from the full multiple regression.</p>
</blockquote>
<p>Consider the multiple linear regression model:</p>
<p><span class="math display">\[ \mathbf{y = X\beta + \epsilon = X_1\beta_1 + X_2\beta_2 + \epsilon} \]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of the dependent variable.</li>
<li><span class="math inline">\(X_1\)</span> is an <span class="math inline">\(n \times k_1\)</span> matrix of regressors of interest.</li>
<li><span class="math inline">\(X_2\)</span> is an <span class="math inline">\(n \times k_2\)</span> matrix of additional regressors.</li>
<li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are coefficient vectors of sizes <span class="math inline">\(k_1 \times 1\)</span> and <span class="math inline">\(k_2 \times 1\)</span>, respectively.</li>
<li><span class="math inline">\(\epsilon\)</span> is an <span class="math inline">\(n \times 1\)</span> error term vector.</li>
</ul>
<p>This can be equivalently represented in partitioned matrix form as:</p>
<p><span class="math display">\[ \left( \begin{array}{cc} X_1&#39;X_1 &amp; X_1&#39;X_2 \\ X_2&#39;X_1 &amp; X_2&#39;X_2 \end{array} \right) \left( \begin{array}{c} \hat{\beta_1} \\ \hat{\beta_2} \end{array} \right) = \left( \begin{array}{c} X_1&#39;y \\ X_2&#39;y \end{array} \right) \]</span></p>
<p>The OLS estimator for the vector <span class="math inline">\(\begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}\)</span> is:</p>
<p><span class="math display">\[
\begin{pmatrix}
\hat{\beta}_1 \\
\hat{\beta}_2
\end{pmatrix}
=
\begin{pmatrix}
X_1&#39;X_1 &amp; X_1&#39;X_2 \\
X_2&#39;X_1 &amp; X_2&#39;X_2
\end{pmatrix}^{-1}
\begin{pmatrix}
X_1&#39;y \\
X_2&#39;y
\end{pmatrix}.
\]</span></p>
<p>If we only want the coefficients on <span class="math inline">\(X_1\)</span>, a known result from partitioned-inversion gives:</p>
<p><span class="math display">\[
\hat{\beta}_1
=
\left(X_1&#39; M_2\, X_1\right)^{-1}
\,X_1&#39; M_2\, y,
\]</span></p>
<p>where</p>
<p><span class="math display">\[
M_2
=
I
-
X_2 \left(X_2&#39;X_2\right)^{-1} X_2&#39;.
\]</span></p>
<p>The matrix <span class="math inline">\(M_2\)</span> is often called the <strong>residual-maker</strong> or <strong>annihilator</strong> matrix for <span class="math inline">\(X_2\)</span>. It is an <span class="math inline">\(n \times n\)</span> symmetric, idempotent projection matrix that projects any vector in <span class="math inline">\(\mathbb{R}^n\)</span> onto the orthogonal complement of the column space of <span class="math inline">\(X_2\)</span>. <span class="math inline">\(M_2\)</span> satisfies <span class="math inline">\(M_2^2 = M_2\)</span>, and <span class="math inline">\(M_2 = M_2&#39;\)</span>.</p>
<p>Intuitively, <span class="math inline">\(M_2\)</span> captures the part of <span class="math inline">\(y\)</span> (and any other vector) that is orthogonal to the columns of <span class="math inline">\(X_2\)</span>. This “partialling out” of <span class="math inline">\(X_2\)</span> from both <span class="math inline">\(y\)</span> and <span class="math inline">\(X_1\)</span> lets us isolate <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>Equivalently, we can also represent <span class="math inline">\(\hat{\beta_1}\)</span> as:</p>
<p><span class="math display">\[ \mathbf{\hat{\beta_1} = (X_1&#39;X_1)^{-1}X_1&#39;y - (X_1&#39;X_1)^{-1}X_1&#39;X_2\hat{\beta_2}} \]</span></p>
<p>From this equation, we can see that</p>
<ol style="list-style-type: decimal">
<li><strong>Betas from Multiple vs. Simple Regressions:</strong>
<ul>
<li>The coefficients (<span class="math inline">\(\beta\)</span>) from a multiple regression are generally <strong>not the same</strong> as the coefficients from separate individual simple regressions.</li>
</ul></li>
<li><strong>Impact of Additional Variables (</strong><span class="math inline">\(X_2\)</span>):
<ul>
<li>The inclusion of different sets of explanatory variables (<span class="math inline">\(X_2\)</span>) affects all coefficient estimates, even for those in <span class="math inline">\(X_1\)</span>.</li>
</ul></li>
<li><strong>Special Cases:</strong>
<ul>
<li>If <span class="math inline">\(X_1&#39;X_2 = 0\)</span> (orthogonality between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>) or <span class="math inline">\(\hat{\beta_2} = 0\)</span>, the above points (1 and 2) do not hold. In such cases, there is no interaction between the coefficients in <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, making the coefficients in <span class="math inline">\(X_1\)</span> unaffected by <span class="math inline">\(X_2\)</span>.</li>
</ul></li>
</ol>
<p><strong>Steps in FWL:</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Partial Out</strong> <span class="math inline">\(X_2\)</span> from <span class="math inline">\(y\)</span>: Regress <span class="math inline">\(y\)</span> on <span class="math inline">\(X_2\)</span> to obtain residuals:</p>
<p><span class="math display">\[
\tilde{y} = M_2y.
\]</span></p></li>
<li><p><strong>Partial Out</strong> <span class="math inline">\(X_2\)</span> from <span class="math inline">\(X_1\)</span>: For each column of <span class="math inline">\(X_1\)</span>, regress it on <span class="math inline">\(X_2\)</span> to obtain residuals:</p>
<p><span class="math display">\[
\tilde{X}_1 = M_2X_1.
\]</span></p></li>
<li><p><strong>Regression of Residuals:</strong> Regress <span class="math inline">\(\tilde{y}\)</span> on <span class="math inline">\(\tilde{X}_1\)</span>:</p>
<p><span class="math display">\[
\tilde{y} = \tilde{X}_1\beta_1 + \text{error}.
\]</span></p></li>
</ol>
<p>The coefficients <span class="math inline">\(\beta_1\)</span> obtained here are identical to those from the full model regression:</p>
<p><span class="math display">\[
y = X_1\beta_1 + X_2\beta_2 + \epsilon.
\]</span></p>
<p>Why It Matters</p>
<ol style="list-style-type: decimal">
<li><p><strong>Interpretation of Partial Effects:</strong> The FWL Theorem provides a way to interpret <span class="math inline">\(\beta_1\)</span> as the effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(y\)</span> after removing any linear dependence on <span class="math inline">\(X_2\)</span>.</p></li>
<li><p><strong>Computational Simplicity:</strong> It allows the decomposition of a large regression problem into smaller, computationally simpler pieces.</p></li>
</ol>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="ordinary-least-squares.html#cb170-1" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb170-2"><a href="ordinary-least-squares.html#cb170-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb170-3"><a href="ordinary-least-squares.html#cb170-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb170-4"><a href="ordinary-least-squares.html#cb170-4" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> <span class="dv">2</span>), n, <span class="dv">2</span>)  <span class="co"># Two regressors of interest</span></span>
<span id="cb170-5"><a href="ordinary-least-squares.html#cb170-5" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> <span class="dv">2</span>), n, <span class="dv">2</span>)  <span class="co"># Two additional regressors</span></span>
<span id="cb170-6"><a href="ordinary-least-squares.html#cb170-6" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="sc">-</span><span class="dv">1</span>)  <span class="co"># Coefficients for X1</span></span>
<span id="cb170-7"><a href="ordinary-least-squares.html#cb170-7" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>)  <span class="co"># Coefficients for X2undefined</span></span>
<span id="cb170-8"><a href="ordinary-least-squares.html#cb170-8" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)  <span class="co"># Error term</span></span>
<span id="cb170-9"><a href="ordinary-least-squares.html#cb170-9" tabindex="-1"></a>y <span class="ot">&lt;-</span> X1 <span class="sc">%*%</span> beta1 <span class="sc">+</span> X2 <span class="sc">%*%</span> beta2 <span class="sc">+</span> u  <span class="co"># Generate dependent variable</span></span>
<span id="cb170-10"><a href="ordinary-least-squares.html#cb170-10" tabindex="-1"></a></span>
<span id="cb170-11"><a href="ordinary-least-squares.html#cb170-11" tabindex="-1"></a><span class="co"># Full regression</span></span>
<span id="cb170-12"><a href="ordinary-least-squares.html#cb170-12" tabindex="-1"></a>full_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">+</span> X2)</span>
<span id="cb170-13"><a href="ordinary-least-squares.html#cb170-13" tabindex="-1"></a><span class="fu">summary</span>(full_model)</span>
<span id="cb170-14"><a href="ordinary-least-squares.html#cb170-14" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-15"><a href="ordinary-least-squares.html#cb170-15" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb170-16"><a href="ordinary-least-squares.html#cb170-16" tabindex="-1"></a><span class="co">#&gt; lm(formula = y ~ X1 + X2)</span></span>
<span id="cb170-17"><a href="ordinary-least-squares.html#cb170-17" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-18"><a href="ordinary-least-squares.html#cb170-18" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb170-19"><a href="ordinary-least-squares.html#cb170-19" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb170-20"><a href="ordinary-least-squares.html#cb170-20" tabindex="-1"></a><span class="co">#&gt; -2.47336 -0.58010  0.07461  0.68778  2.46552 </span></span>
<span id="cb170-21"><a href="ordinary-least-squares.html#cb170-21" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-22"><a href="ordinary-least-squares.html#cb170-22" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb170-23"><a href="ordinary-least-squares.html#cb170-23" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb170-24"><a href="ordinary-least-squares.html#cb170-24" tabindex="-1"></a><span class="co">#&gt; (Intercept)  0.11614    0.10000   1.161    0.248    </span></span>
<span id="cb170-25"><a href="ordinary-least-squares.html#cb170-25" tabindex="-1"></a><span class="co">#&gt; X11          1.77575    0.10899  16.293  &lt; 2e-16 ***</span></span>
<span id="cb170-26"><a href="ordinary-least-squares.html#cb170-26" tabindex="-1"></a><span class="co">#&gt; X12         -1.14151    0.10204 -11.187  &lt; 2e-16 ***</span></span>
<span id="cb170-27"><a href="ordinary-least-squares.html#cb170-27" tabindex="-1"></a><span class="co">#&gt; X21          0.94954    0.10468   9.071 1.60e-14 ***</span></span>
<span id="cb170-28"><a href="ordinary-least-squares.html#cb170-28" tabindex="-1"></a><span class="co">#&gt; X22          0.47667    0.09506   5.014 2.47e-06 ***</span></span>
<span id="cb170-29"><a href="ordinary-least-squares.html#cb170-29" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb170-30"><a href="ordinary-least-squares.html#cb170-30" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb170-31"><a href="ordinary-least-squares.html#cb170-31" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-32"><a href="ordinary-least-squares.html#cb170-32" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.9794 on 95 degrees of freedom</span></span>
<span id="cb170-33"><a href="ordinary-least-squares.html#cb170-33" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8297, Adjusted R-squared:  0.8225 </span></span>
<span id="cb170-34"><a href="ordinary-least-squares.html#cb170-34" tabindex="-1"></a><span class="co">#&gt; F-statistic: 115.7 on 4 and 95 DF,  p-value: &lt; 2.2e-16</span></span>
<span id="cb170-35"><a href="ordinary-least-squares.html#cb170-35" tabindex="-1"></a></span>
<span id="cb170-36"><a href="ordinary-least-squares.html#cb170-36" tabindex="-1"></a><span class="co"># Step 1: Partial out X2 from y</span></span>
<span id="cb170-37"><a href="ordinary-least-squares.html#cb170-37" tabindex="-1"></a>y_residual <span class="ot">&lt;-</span> <span class="fu">residuals</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X2))</span>
<span id="cb170-38"><a href="ordinary-least-squares.html#cb170-38" tabindex="-1"></a></span>
<span id="cb170-39"><a href="ordinary-least-squares.html#cb170-39" tabindex="-1"></a><span class="co"># Step 2: Partial out X2 from X1</span></span>
<span id="cb170-40"><a href="ordinary-least-squares.html#cb170-40" tabindex="-1"></a>X1_residual <span class="ot">&lt;-</span> <span class="fu">residuals</span>(<span class="fu">lm</span>(X1 <span class="sc">~</span> X2))</span>
<span id="cb170-41"><a href="ordinary-least-squares.html#cb170-41" tabindex="-1"></a></span>
<span id="cb170-42"><a href="ordinary-least-squares.html#cb170-42" tabindex="-1"></a><span class="co"># Step 3: Regress residuals</span></span>
<span id="cb170-43"><a href="ordinary-least-squares.html#cb170-43" tabindex="-1"></a>fwl_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y_residual <span class="sc">~</span> X1_residual <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb170-44"><a href="ordinary-least-squares.html#cb170-44" tabindex="-1"></a><span class="fu">summary</span>(fwl_model)</span>
<span id="cb170-45"><a href="ordinary-least-squares.html#cb170-45" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-46"><a href="ordinary-least-squares.html#cb170-46" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb170-47"><a href="ordinary-least-squares.html#cb170-47" tabindex="-1"></a><span class="co">#&gt; lm(formula = y_residual ~ X1_residual - 1)</span></span>
<span id="cb170-48"><a href="ordinary-least-squares.html#cb170-48" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-49"><a href="ordinary-least-squares.html#cb170-49" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb170-50"><a href="ordinary-least-squares.html#cb170-50" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb170-51"><a href="ordinary-least-squares.html#cb170-51" tabindex="-1"></a><span class="co">#&gt; -2.47336 -0.58010  0.07461  0.68778  2.46552 </span></span>
<span id="cb170-52"><a href="ordinary-least-squares.html#cb170-52" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-53"><a href="ordinary-least-squares.html#cb170-53" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb170-54"><a href="ordinary-least-squares.html#cb170-54" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb170-55"><a href="ordinary-least-squares.html#cb170-55" tabindex="-1"></a><span class="co">#&gt; X1_residual1   1.7758     0.1073   16.55   &lt;2e-16 ***</span></span>
<span id="cb170-56"><a href="ordinary-least-squares.html#cb170-56" tabindex="-1"></a><span class="co">#&gt; X1_residual2  -1.1415     0.1005  -11.36   &lt;2e-16 ***</span></span>
<span id="cb170-57"><a href="ordinary-least-squares.html#cb170-57" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb170-58"><a href="ordinary-least-squares.html#cb170-58" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb170-59"><a href="ordinary-least-squares.html#cb170-59" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb170-60"><a href="ordinary-least-squares.html#cb170-60" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.9643 on 98 degrees of freedom</span></span>
<span id="cb170-61"><a href="ordinary-least-squares.html#cb170-61" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8109, Adjusted R-squared:  0.807 </span></span>
<span id="cb170-62"><a href="ordinary-least-squares.html#cb170-62" tabindex="-1"></a><span class="co">#&gt; F-statistic: 210.1 on 2 and 98 DF,  p-value: &lt; 2.2e-16</span></span>
<span id="cb170-63"><a href="ordinary-least-squares.html#cb170-63" tabindex="-1"></a></span>
<span id="cb170-64"><a href="ordinary-least-squares.html#cb170-64" tabindex="-1"></a><span class="co"># Comparison of coefficients</span></span>
<span id="cb170-65"><a href="ordinary-least-squares.html#cb170-65" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Full model coefficients (X1):&quot;</span>, <span class="fu">coef</span>(full_model)[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>], <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb170-66"><a href="ordinary-least-squares.html#cb170-66" tabindex="-1"></a><span class="co">#&gt; Full model coefficients (X1): 1.775754 -1.141514</span></span>
<span id="cb170-67"><a href="ordinary-least-squares.html#cb170-67" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;FWL model coefficients:&quot;</span>, <span class="fu">coef</span>(fwl_model), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb170-68"><a href="ordinary-least-squares.html#cb170-68" tabindex="-1"></a><span class="co">#&gt; FWL model coefficients: 1.775754 -1.141514</span></span></code></pre></div>
</div>
<div id="gauss-markov-theorem" class="section level5 hasAnchor" number="5.1.2.2.2">
<h5><span class="header-section-number">5.1.2.2.2</span> Gauss-Markov Theorem<a href="ordinary-least-squares.html#gauss-markov-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>For a linear regression model:</p>
<p><span class="math display">\[
\mathbf{y = X\beta + \epsilon},
\]</span></p>
<p>under the assumptions:</p>
<ol style="list-style-type: decimal">
<li><a href="ordinary-least-squares.html#a1-linearity"><strong>A1</strong></a>: Linearity of the model.</li>
<li><a href="ordinary-least-squares.html#a2-full-rank"><strong>A2</strong></a>: Full rank of <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li><a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables"><strong>A3</strong></a>: Exogeneity of <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li><a href="ordinary-least-squares.html#a4-homoskedasticity"><strong>A4</strong></a>: Homoskedasticity of <span class="math inline">\(\epsilon\)</span>.</li>
</ol>
<p>The <a href="ordinary-least-squares.html#ordinary-least-squares">Ordinary Least Squares</a> estimator:</p>
<p><span class="math display">\[
\hat{\beta} = \mathbf{(X&#39;X)^{-1}X&#39;y},
\]</span></p>
<p>is the <strong>Best Linear Unbiased Estimator (BLUE)</strong>. This means that <span class="math inline">\(\hat{\beta}\)</span> has the minimum variance among all linear unbiased estimators of <span class="math inline">\(\beta\)</span>.</p>
<p><strong>1. Unbiasedness</strong></p>
<p>Suppose we consider <strong>any</strong> linear estimator of <span class="math inline">\(\beta\)</span> of the form:</p>
<p><span class="math display">\[
\tilde{\beta} = \mathbf{C\,y},
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(n \times 1\)</span> vector of observations,</li>
<li><span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(k \times n\)</span> matrix (with <span class="math inline">\(k\)</span> the dimension of <span class="math inline">\(\beta\)</span>) that depends <strong>only</strong> on the design matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul>
<p>Our regression model is</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X}\beta + \mathbf{\epsilon},
\quad
E[\mathbf{\epsilon} \mid \mathbf{X}] = \mathbf{0},
\quad
\mathrm{Var}(\mathbf{\epsilon} \mid \mathbf{X}) = \sigma^2 \mathbf{I}.
\]</span></p>
<p>We say <span class="math inline">\(\tilde{\beta}\)</span> is <strong>unbiased</strong> if its conditional expectation (given <span class="math inline">\(\mathbf{X}\)</span>) equals the true parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
E(\tilde{\beta} \mid \mathbf{X})
=
E(\mathbf{C\,y} \mid \mathbf{X})
=
\beta.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Substitute <span class="math inline">\(\mathbf{y} = \mathbf{X}\beta + \mathbf{\epsilon}\)</span>:</li>
</ol>
<p><span class="math display">\[
E(\mathbf{C\,y} \mid \mathbf{X})
=
E\left(\mathbf{C}(\mathbf{X}\beta + \mathbf{\epsilon}) \mid \mathbf{X}\right)
=
\mathbf{C\,X}\,\beta
+
\mathbf{C}\,E(\mathbf{\epsilon} \mid \mathbf{X})
=
\mathbf{C\,X}\,\beta.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>For this to hold for <em>all</em> <span class="math inline">\(\beta\)</span>, we require</li>
</ol>
<p><span class="math display">\[
\mathbf{C\,X} = \mathbf{I}.
\]</span></p>
<p>In other words, <span class="math inline">\(\mathbf{C}\)</span> must be a “right-inverse” of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>On the other hand, the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is given by</p>
<p><span class="math display">\[
\hat{\beta}
=
(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\,\mathbf{y}.
\]</span></p>
<p>You can verify:</p>
<ul>
<li>Let <span class="math inline">\(\mathbf{C}_{\text{OLS}} = (\mathbf{X}&#39;\mathbf{X})^{-1} \mathbf{X}&#39;\)</span>.</li>
<li>Then <span class="math display">\[
\mathbf{C}_{\text{OLS}}\,\mathbf{X}
=
(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\,\mathbf{X}
=
\mathbf{I}.
\]</span></li>
<li>By the argument above, this makes <span class="math inline">\(\hat{\beta}\)</span> unbiased.</li>
</ul>
<p>Hence, any linear estimator <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> that is unbiased must satisfy <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>.</p>
<p><strong>2. Minimum Variance (Gauss–Markov Part)</strong></p>
<p>Among all estimators of the form <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> that are unbiased (so <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>), <strong>OLS</strong> achieves the smallest covariance matrix.</p>
<ul>
<li>Variance of a General Unbiased Estimator</li>
</ul>
<p>If <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> with <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>, then:</p>
<p><span class="math display">\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
=
\mathrm{Var}(\mathbf{C\,y} \mid \mathbf{X})
=
\mathbf{C}\,\mathrm{Var}(\mathbf{y} \mid \mathbf{X})\,\mathbf{C}&#39;
=
\mathbf{C}\left(\sigma^2 \mathbf{I}\right)\mathbf{C}&#39;
=
\sigma^2\,\mathbf{C}\,\mathbf{C}&#39;.
\]</span></p>
<ul>
<li>Variance of the OLS Estimator</li>
</ul>
<p>For <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a>, <span class="math inline">\(\hat{\beta} = (\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\,\mathbf{y}\)</span>. Thus,</p>
<p><span class="math display">\[
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
=
\sigma^2\,(\mathbf{X}&#39;\mathbf{X})^{-1}.
\]</span></p>
<ul>
<li>Comparing <span class="math inline">\(\mathrm{Var}(\tilde{\beta})\)</span> to <span class="math inline">\(\mathrm{Var}(\hat{\beta})\)</span></li>
</ul>
<p>We want to show:</p>
<p><span class="math display">\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
-
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
\;\;\text{is positive semi-definite.}
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Since both <span class="math inline">\(\tilde{\beta}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are unbiased, we know: <span class="math display">\[
\mathbf{C\,X} = \mathbf{I},
\quad
(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\,\mathbf{X} = \mathbf{I}.
\]</span></p></li>
<li><p>One can show algebraically (as in the proof provided above) that <span class="math display">\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
-
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
=
\sigma^2 \left[\mathbf{C}\mathbf{C}&#39; - (\mathbf{X}&#39;\mathbf{X})^{-1}\right].
\]</span> Under the condition <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>, the difference <span class="math inline">\(\mathbf{C}\mathbf{C}&#39; - (\mathbf{X}&#39;\mathbf{X})^{-1}\)</span> is positive semi-definite.</p></li>
<li><p>Positive semi-definite difference means</p></li>
</ol>
<p><span class="math display">\[
\mathbf{v}&#39; \left(\mathbf{C}\mathbf{C}&#39;
-
(\mathbf{X}&#39;\mathbf{X})^{-1}\right)\mathbf{v}
\ge
0
\quad
\text{for all vectors } \mathbf{v}.
\]</span></p>
<p>Hence, <span class="math inline">\(\hat{\beta}\)</span> has the smallest variance (in the sense of covariance matrices) among all linear unbiased estimators <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span>.</p>
<p><strong>Summary of the Key Points</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Unbiasedness:</strong><br />
A linear estimator <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> is unbiased if <span class="math inline">\(E(\tilde{\beta}\mid \mathbf{X}) = \beta\)</span>.<br />
This forces <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>.</p></li>
<li><p><strong>OLS is Unbiased:</strong><br />
The OLS estimator <span class="math inline">\(\hat{\beta} = (X&#39;X)^{-1} X&#39; \, y\)</span> satisfies <span class="math inline">\((X&#39;X)^{-1} X&#39; \, X = I\)</span>, hence is unbiased.</p></li>
<li><p><strong>OLS has Minimum Variance:</strong><br />
Among all <span class="math inline">\(\mathbf{C}\)</span> that satisfy <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>, the matrix <span class="math inline">\((\mathbf{X}&#39;\mathbf{X})^{-1}\)</span> gives the smallest possible <span class="math inline">\(\mathrm{Var}(\tilde{\beta})\)</span>.<br />
In matrix form, <span class="math inline">\(\mathrm{Var}(\tilde{\beta}) - \mathrm{Var}(\hat{\beta})\)</span> is positive semi-definite, showing OLS is optimal (the <strong>Best Linear Unbiased Estimator</strong>, BLUE).</p></li>
</ol>
</div>
</div>
<div id="finite-sample-properties" class="section level4 hasAnchor" number="5.1.2.3">
<h4><span class="header-section-number">5.1.2.3</span> Finite Sample Properties<a href="ordinary-least-squares.html#finite-sample-properties" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The finite sample properties of an estimator are considered when the sample size <span class="math inline">\(n\)</span> is fixed (not asymptotically large). Key properties include <strong>bias</strong>, <strong>distribution</strong>, and <strong>standard deviation</strong> of the estimator.</p>
<hr />
<p><strong>Bias</strong> measures how close an estimator is, on average, to the true parameter value <span class="math inline">\(\beta\)</span>. It is defined as:</p>
<p><span class="math display">\[
\text{Bias} = E(\hat{\beta}) - \beta
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\beta\)</span>: True parameter value.</li>
<li><span class="math inline">\(\hat{\beta}\)</span>: Estimator for <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p><strong>Unbiased Estimator</strong>: An estimator is unbiased if:</p>
<p><span class="math display">\[
\text{Bias} = E(\hat{\beta}) - \beta = 0 \quad \text{or equivalently} \quad E(\hat{\beta}) = \beta
\]</span></p>
<p>This means the estimator will produce estimates that are, on average, equal to the value it is trying to estimate.</p>
<hr />
<p>An estimator is a function of random variables (data). Its <strong>distribution</strong> describes how the estimates vary across repeated samples. Key aspects include:</p>
<ul>
<li><strong>Center</strong>: Mean of the distribution, which relates to bias.</li>
<li><strong>Spread</strong>: Variability of the estimator, captured by its standard deviation or variance.</li>
</ul>
<hr />
<p>The <strong>standard deviation</strong> of an estimator measures the spread of its sampling distribution. It indicates the variability of the estimator across different samples.</p>
<hr />
<div id="ordinary-least-squares-properties" class="section level5 hasAnchor" number="5.1.2.3.1">
<h5><span class="header-section-number">5.1.2.3.1</span> Ordinary Least Squares Properties<a href="ordinary-least-squares.html#ordinary-least-squares-properties" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Under the standard assumptions for OLS:</p>
<ol style="list-style-type: decimal">
<li><a href="ordinary-least-squares.html#a1-linearity">A1</a>: The relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li><a href="ordinary-least-squares.html#a2-full-rank">A2</a>: The matrix <span class="math inline">\(\mathbf{X&#39;X}\)</span> is invertible.</li>
<li><a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3</a>: <span class="math inline">\(E(\epsilon|X) = 0\)</span> (errors are uncorrelated with predictors).</li>
</ol>
<p><strong>OLS is unbiased</strong> under these assumptions. The proof is as follows:</p>
<p><span class="math display">\[
\begin{aligned}
E(\hat{\beta}) &amp;= E(\mathbf{(X&#39;X)^{-1}X&#39;y}) &amp;&amp; \text{A2}\\
               &amp;= E(\mathbf{(X&#39;X)^{-1}X&#39;(X\beta + \epsilon)}) &amp;&amp; \text{A1}\\
               &amp;= E(\mathbf{(X&#39;X)^{-1}X&#39;X\beta + (X&#39;X)^{-1}X&#39;\epsilon}) \\
               &amp;= E(\beta + \mathbf{(X&#39;X)^{-1}X&#39;\epsilon}) \\
               &amp;= \beta + E(\mathbf{(X&#39;X)^{-1}X&#39;\epsilon}) \\
               &amp;= \beta + E(E(\mathbf{(X&#39;X)^{-1}X&#39;\epsilon}|X)) &amp;&amp; \text{LIE (Law of Iterated Expectation)} \\
               &amp;= \beta + E(\mathbf{(X&#39;X)^{-1}X&#39;}E(\epsilon|X)) \\
               &amp;= \beta + E(\mathbf{(X&#39;X)^{-1}X&#39;} \cdot 0) &amp;&amp; \text{A3}\\
               &amp;= \beta
\end{aligned}
\]</span></p>
<p><strong>Key Points</strong>:</p>
<ol style="list-style-type: decimal">
<li><strong>Linearity of Expectation</strong>: Used to separate terms involving <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\epsilon\)</span>.</li>
<li><a href="probability-theory.html#law-of-iterated-expectation">Law of Iterated Expectation</a> <strong>(LIE)</strong>: Simplifies nested expectations.</li>
<li><strong>Exogeneity of Errors (A3)</strong>: Ensures <span class="math inline">\(E(\epsilon|X) = 0\)</span>, eliminating bias.</li>
</ol>
<hr />
<p>Implications of Unbiasedness</p>
<ul>
<li>OLS estimators are centered around the true value <span class="math inline">\(\beta\)</span> across repeated samples.</li>
<li>In small samples, OLS estimators may exhibit variability, but their expected value remains <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p>If the assumption of exogeneity (<a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3</a>) is violated, the OLS estimator becomes <strong>biased</strong>. Specifically, omitted variables or endogeneity can introduce systematic errors into the estimation.</p>
<p>From the <strong>Frisch-Waugh-Lovell Theorem</strong>:</p>
<ul>
<li>If an omitted variable <span class="math inline">\(\hat{\beta}_2 \neq 0\)</span> (non-zero effect) and the omitted variable is correlated with the included predictors (<span class="math inline">\(\mathbf{X_1&#39;X_2} \neq 0\)</span>), then the OLS estimator will be biased.</li>
<li>This bias arises because the omitted variable contributes to the variation in the dependent variable, but its effect is incorrectly attributed to other predictors.</li>
</ul>
<hr />
</div>
<div id="conditional-variance-of-ols-estimator" class="section level5 hasAnchor" number="5.1.2.3.2">
<h5><span class="header-section-number">5.1.2.3.2</span> Conditional Variance of OLS Estimator<a href="ordinary-least-squares.html#conditional-variance-of-ols-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Under assumptions <a href="ordinary-least-squares.html#a1-linearity">A1</a>, <a href="ordinary-least-squares.html#a2-full-rank">A2</a>, <a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3</a>, and <a href="ordinary-least-squares.html#a4-homoskedasticity">A4</a>, the <strong>conditional variance of the OLS estimator</strong> is:</p>
<p><span class="math display">\[
\begin{aligned}
Var(\hat{\beta}|\mathbf{X}) &amp;= Var(\beta + \mathbf{(X&#39;X)^{-1}X&#39;\epsilon|X}) &amp;&amp; \text{A1-A2} \\
    &amp;= Var((\mathbf{X&#39;X)^{-1}X&#39;\epsilon|X}) \\
    &amp;= \mathbf{(X&#39;X)^{-1}X&#39;} Var(\epsilon|\mathbf{X})\mathbf{X(X&#39;X)^{-1}} \\
    &amp;= \mathbf{(X&#39;X)^{-1}X&#39;} \sigma^2 I \mathbf{X(X&#39;X)^{-1}} &amp;&amp; \text{A4} \\
    &amp;= \sigma^2 \mathbf{(X&#39;X)^{-1}}
\end{aligned}
\]</span></p>
<p>This result shows that the variance of <span class="math inline">\(\hat{\beta}\)</span> depends on:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sigma^2\)</span>: The variance of the errors.</li>
<li><span class="math inline">\(\mathbf{X&#39;X}\)</span>: The information content in the design matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ol>
<hr />
</div>
<div id="sources-of-variation-in-ols-estimator" class="section level5 hasAnchor" number="5.1.2.3.3">
<h5><span class="header-section-number">5.1.2.3.3</span> Sources of Variation in OLS Estimator<a href="ordinary-least-squares.html#sources-of-variation-in-ols-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li><strong>Unexplained Variation in the Dependent Variable</strong>: <span class="math inline">\(\sigma^2 = Var(\epsilon_i|\mathbf{X})\)</span></li>
</ol>
<ul>
<li>Large <span class="math inline">\(\sigma^2\)</span> indicates that the amount of unexplained variation (noise) is high relative to the explained variation (<span class="math inline">\(\mathbf{x_i \beta}\)</span>).</li>
<li>This increases the variance of the OLS estimator.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Small Variation in Predictor Variables</strong></li>
</ol>
<ul>
<li>If the variance of predictors (<span class="math inline">\(Var(x_{i1}), Var(x_{i2}), \dots\)</span>) is small, the design matrix <span class="math inline">\(\mathbf{X}\)</span> lacks information, leading to:
<ul>
<li>High variability in <span class="math inline">\(\hat{\beta}\)</span>.</li>
<li>Potential issues in estimating coefficients accurately.</li>
</ul></li>
<li><strong>Small sample size</strong> exacerbates this issue, as fewer observations reduce the robustness of parameter estimates.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Correlation Between Explanatory Variables (Collinearity)</strong></li>
</ol>
<ul>
<li><strong>Strong correlation</strong> among explanatory variables creates problems:
<ul>
<li><span class="math inline">\(x_{i1}\)</span> being highly correlated with a linear combination of <span class="math inline">\(1, x_{i2}, x_{i3}, \dots\)</span> contributes to inflated standard errors for <span class="math inline">\(\hat{\beta}_1\)</span>.</li>
<li>Including many irrelevant variables exacerbates this issue.</li>
</ul></li>
<li><strong>Perfect Collinearity</strong>:
<ul>
<li>If <span class="math inline">\(x_1\)</span> is perfectly determined by a linear combination of other predictors, the matrix <span class="math inline">\(\mathbf{X&#39;X}\)</span> becomes singular.</li>
<li>This violates <a href="ordinary-least-squares.html#a2-full-rank">A2</a>, making OLS impossible to compute.</li>
</ul></li>
<li><strong>Multicollinearity</strong>:
<ul>
<li>If <span class="math inline">\(x_1\)</span> is highly correlated (but not perfectly) with a linear combination of other variables, the variance of <span class="math inline">\(\hat{\beta}_1\)</span> increases.</li>
<li>Multicollinearity does not violate OLS assumptions but weakens inference by inflating standard errors.</li>
</ul></li>
</ul>
<hr />
</div>
<div id="standard-errors" class="section level5 hasAnchor" number="5.1.2.3.4">
<h5><span class="header-section-number">5.1.2.3.4</span> Standard Errors<a href="ordinary-least-squares.html#standard-errors" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Standard errors measure the variability of an estimator, specifically the standard deviation of <span class="math inline">\(\hat{\beta}\)</span>. They are crucial for inference, as they quantify the uncertainty associated with parameter estimates.</p>
<hr />
<p>The variance of the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is:</p>
<p><span class="math display">\[
Var(\hat{\beta}|\mathbf{X}) = \sigma^2 \mathbf{(X&#39;X)^{-1}}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\sigma^2\)</span>: Variance of the error terms.</li>
<li><span class="math inline">\(\mathbf{(X&#39;X)^{-1}}\)</span>: Inverse of the design matrix product, capturing the geometry of the predictors.</li>
</ul>
<hr />
<p><strong>Estimation of</strong> <span class="math inline">\(\sigma^2\)</span></p>
<p>Under assumptions <a href="ordinary-least-squares.html#a1-linearity">A1</a> through <a href="#a5-homoskedasticity">A5</a>, we can estimate <span class="math inline">\(\sigma^2\)</span> as:</p>
<p><span class="math display">\[
s^2 = \frac{1}{n-k} \sum_{i=1}^{n} e_i^2 = \frac{1}{n-k} SSR
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(n\)</span>: Number of observations.</li>
<li><span class="math inline">\(k\)</span>: Number of predictors, including the intercept.</li>
<li><span class="math inline">\(e_i\)</span>: Residuals from the regression model (<span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>).</li>
<li><span class="math inline">\(SSR\)</span>: Sum of squared residuals (<span class="math inline">\(\sum e_i^2\)</span>).</li>
</ul>
<p>The degrees of freedom adjustment (<span class="math inline">\(n-k\)</span>) accounts for the fact that residuals <span class="math inline">\(e_i\)</span> are not true errors <span class="math inline">\(\epsilon_i\)</span>. Since the regression model uses <span class="math inline">\(k\)</span> parameters, we lose <span class="math inline">\(k\)</span> degrees of freedom in estimating variance.</p>
<p>The standard error for <span class="math inline">\(\sigma\)</span> is:</p>
<p><span class="math display">\[
s = \sqrt{s^2}
\]</span></p>
<p>However, <span class="math inline">\(s\)</span> is a biased estimator of <span class="math inline">\(\sigma\)</span> due to <a href="probability-theory.html#jensens-inequality">Jensen’s Inequality</a>.</p>
<hr />
<p>The standard error of each regression coefficient <span class="math inline">\(\hat{\beta}_{j-1}\)</span> is:</p>
<p><span class="math display">\[
SE(\hat{\beta}_{j-1}) = s \sqrt{[(\mathbf{X&#39;X})^{-1}]_{jj}}
\]</span></p>
<p>Alternatively, it can be expressed in terms of <span class="math inline">\(SST_{j-1}\)</span> and <span class="math inline">\(R_{j-1}^2\)</span>:</p>
<p><span class="math display">\[
SE(\hat{\beta}_{j-1}) = \frac{s}{\sqrt{SST_{j-1}(1 - R_{j-1}^2)}}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(SST_{j-1}\)</span>: Total sum of squares for <span class="math inline">\(x_{j-1}\)</span> from a regression of <span class="math inline">\(x_{j-1}\)</span> on all other predictors.</li>
<li><span class="math inline">\(R_{j-1}^2\)</span>: Coefficient of determination for the same regression.</li>
</ul>
<p>This formulation highlights the role of multicollinearity, as <span class="math inline">\(R_{j-1}^2\)</span> reflects the correlation between <span class="math inline">\(x_{j-1}\)</span> and other predictors.</p>
<hr />
</div>
<div id="summary-of-finite-sample-properties-of-ols-under-different-assumptions" class="section level5 hasAnchor" number="5.1.2.3.5">
<h5><span class="header-section-number">5.1.2.3.5</span> Summary of Finite Sample Properties of OLS Under Different Assumptions<a href="ordinary-least-squares.html#summary-of-finite-sample-properties-of-ols-under-different-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li><p><strong>Under <a href="ordinary-least-squares.html#a1-linearity">A1-A3</a></strong>: <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> is unbiased. <span class="math display">\[ E(\hat{\beta}) = \beta \]</span></p></li>
<li><p><strong>Under <a href="ordinary-least-squares.html#a4-homoskedasticity">A1-A4</a></strong>: The variance of the OLS estimator is: <span class="math display">\[ Var(\hat{\beta}|\mathbf{X}) = \sigma^2 \mathbf{(X&#39;X)^{-1}} \]</span></p></li>
<li><p><strong>Under <a href="#a6-normality-of-errors">A1-A4, A6</a></strong>: The OLS estimator is normally distributed: <span class="math display">\[ \hat{\beta} \sim N(\beta, \sigma^2 \mathbf{(X&#39;X)^{-1}}) \]</span></p></li>
<li><p><strong>Under <a href="ordinary-least-squares.html#gauss-markov-theorem">A1-A4, Gauss-Markov Theorem</a></strong>: OLS is BLUE (Best Linear Unbiased Estimator).</p></li>
<li><p><strong>Under <a href="#a5-homoskedasticity">A1-A5</a></strong>: The standard errors for <span class="math inline">\(\hat{\beta}\)</span> are unbiased estimators of the standard deviation of <span class="math inline">\(\hat{\beta}\)</span>.</p></li>
</ol>
<hr />
</div>
</div>
<div id="large-sample-properties" class="section level4 hasAnchor" number="5.1.2.4">
<h4><span class="header-section-number">5.1.2.4</span> Large Sample Properties<a href="ordinary-least-squares.html#large-sample-properties" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Large sample properties provide a framework to evaluate the quality of estimators when finite sample properties are either uninformative or computationally infeasible. This perspective becomes crucial in modern data analysis, especially for methods like GLS or MLE, where assumptions for finite sample analysis may not hold.</p>
<table>
<caption>When to Use Finite vs. Large Sample Analysis</caption>
<colgroup>
<col width="16%" />
<col width="43%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Finite Sample Properties</strong></th>
<th><strong>Large Sample Properties</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Applicability</strong></td>
<td>Limited to fixed sample sizes</td>
<td>Relevant for <span class="math inline">\(n \to \infty\)</span></td>
</tr>
<tr class="even">
<td><strong>Exactness</strong></td>
<td>Exact results (e.g., distributions, unbiasedness)</td>
<td>Approximate results</td>
</tr>
<tr class="odd">
<td><strong>Assumptions</strong></td>
<td>May require stronger assumptions (e.g., normality, independence)</td>
<td>Relies on asymptotic approximations (e.g., CLT, LLN)</td>
</tr>
<tr class="even">
<td><strong>Estimator Behavior</strong></td>
<td>Performance may vary significantly</td>
<td>Estimators stabilize and improve in accuracy</td>
</tr>
<tr class="odd">
<td><strong>Ease of Use</strong></td>
<td>Often complex due to reliance on exact distributions</td>
<td>Simplifies analysis by leveraging asymptotic approximations</td>
</tr>
<tr class="even">
<td><strong>Real-World Relevance</strong></td>
<td>More realistic for small datasets</td>
<td>More relevant for large datasets</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>Finite Sample Analysis</strong>:</p>
<ul>
<li><p>Small sample sizes (e.g., <span class="math inline">\(n &lt; 30\)</span>).</p></li>
<li><p>Critical for studies where exact results are needed.</p></li>
<li><p>Useful in experimental designs and case studies.</p></li>
</ul></li>
<li><p><strong>Large Sample Analysis</strong>:</p>
<ul>
<li><p>Large datasets (e.g., <span class="math inline">\(n &gt; 100\)</span>).</p></li>
<li><p>Necessary when asymptotic approximations improve computational simplicity.</p></li>
</ul></li>
</ul>
<hr />
<p><strong>Key Concepts:</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Consistency</strong>:
<ul>
<li>Consistency ensures that an estimator converges in probability to the true parameter value as the sample size increases.</li>
<li>Mathematically, an estimator <span class="math inline">\(\hat{\theta}\)</span> is consistent for <span class="math inline">\(\theta\)</span> if: <span class="math display">\[
\hat{\theta}_n \to^p \theta \quad \text{as } n \to \infty.
\]</span></li>
<li>Consistency does not imply unbiasedness, and unbiasedness does not guarantee consistency.</li>
</ul></li>
<li><strong>Asymptotic Distribution</strong>:
<ul>
<li>The limiting distribution describes the shape of the scaled estimator as <span class="math inline">\(n \to \infty\)</span>.</li>
<li>Asymptotic distributions often follow normality due to the <a href="probability-theory.html#central-limit-theorem">Central Limit Theorem</a>, which underpins much of inferential statistics.</li>
</ul></li>
<li><strong>Asymptotic Variance</strong>:
<ul>
<li>Represents the spread of the estimator with respect to its limiting distribution.</li>
<li>Smaller asymptotic variance implies greater precision in large samples.</li>
</ul></li>
</ol>
<hr />
<p><strong>Motivation</strong></p>
<p><a href="ordinary-least-squares.html#finite-sample-properties">Finite Sample Properties</a>, such as unbiasedness, rely on strong assumptions like:</p>
<ul>
<li><a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a></li>
<li><a href="ordinary-least-squares.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a></li>
<li><a href="ordinary-least-squares.html#a4-homoskedasticity">A4 Homoskedasticity</a></li>
<li><a href="ordinary-least-squares.html#a6-normal-distribution">A6 Normal Distribution</a></li>
</ul>
<p>When these assumptions are violated or impractical to verify, finite sample properties lose relevance. In such cases, <a href="ordinary-least-squares.html#large-sample-properties">Large Sample Properties</a>serve as an essential alternative for evaluating estimators.</p>
<p>For example, let the conditional expectation function (CEF) be: <span class="math display">\[
\mu(\mathbf{X}) = E(y | \mathbf{X}),
\]</span> which represents the <strong>minimum mean squared predictor</strong> over all possible functions <span class="math inline">\(f(\mathbf{X})\)</span>: <span class="math display">\[
\min_f E((y - f(\mathbf{X}))^2).
\]</span></p>
<p>Under the assumptions A1 and A3, the CEF simplifies to: <span class="math display">\[
\mu(\mathbf{X}) = \mathbf{X}\beta.
\]</span></p>
<p>The <strong>linear projection</strong> is given by: <span class="math display">\[
L(y | 1, \mathbf{X}) = \gamma_0 + \mathbf{X}\text{Var}(\mathbf{X})^{-1}\text{Cov}(\mathbf{X}, y),
\]</span> where: <span class="math display">\[
\gamma = \mathbf{X}\text{Var}(\mathbf{X})^{-1}\text{Cov}(\mathbf{X}, y).
\]</span></p>
<p>This linear projection minimizes the mean squared error: <span class="math display">\[
(\gamma_0, \gamma) = \arg\min_{(a, b)} E\left[\left(E(y|\mathbf{X}) - \left(a + \mathbf{X}b\right)\right)^2\right].
\]</span></p>
<p>Implications for <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a></p>
<ul>
<li><strong>Consistency</strong>: OLS is always consistent for the linear projection, ensuring convergence to the true parameter value as <span class="math inline">\(n \to \infty\)</span>.</li>
<li><strong>Causal Interpretation</strong>: The linear projection has no inherent causal interpretation—it approximates the conditional mean function.</li>
<li><strong>Assumption Independence</strong>: Unlike the CEF, the linear projection does not depend on assumptions A1 and A3.</li>
</ul>
<hr />
<p><strong>Evaluating Estimators via Large Sample Properties</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Consistency</strong>:
<ul>
<li>Measures the estimator’s centrality to the true value.</li>
<li>A consistent estimator ensures that with larger samples, estimates become arbitrarily close to the population parameter.</li>
</ul></li>
<li><strong>Limiting Distribution</strong>:
<ul>
<li>Helps infer the sampling behavior of the estimator as <span class="math inline">\(n\)</span> grows.</li>
<li>Often approximated by a normal distribution for practical use in hypothesis testing and confidence interval construction.</li>
</ul></li>
<li><strong>Asymptotic Variance</strong>:
<ul>
<li>Quantifies the dispersion of the estimator around its limiting distribution.</li>
<li>Smaller variance is desirable for greater reliability.</li>
</ul></li>
</ol>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is <strong>consistent</strong> for a parameter <span class="math inline">\(\theta\)</span> if, as the sample size <span class="math inline">\(n\)</span> increases, <span class="math inline">\(\hat{\theta}\)</span> converges in probability to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\hat{\theta}_n \to^p \theta \quad \text{as } n \to \infty.
\]</span></p>
<ol style="list-style-type: decimal">
<li><p><strong>Convergence in Probability</strong>:</p>
<ul>
<li>The probability that <span class="math inline">\(\hat{\theta}\)</span> deviates from <span class="math inline">\(\theta\)</span> by more than a small margin (no matter how small) approaches zero as <span class="math inline">\(n\)</span> increases.</li>
</ul>
<p>Formally: <span class="math display">\[
\forall \epsilon &gt; 0, \quad P(|\hat{\theta}_n - \theta| &gt; \epsilon) \to 0 \quad \text{as } n \to \infty.
\]</span></p></li>
<li><p><strong>Interpretation</strong>: Consistency ensures that the estimator becomes arbitrarily close to the true population parameter <span class="math inline">\(\theta\)</span> as the sample size grows.</p></li>
<li><p><strong>Asymptotic Behavior</strong>: Large sample properties rely on consistency to provide valid approximations of an estimator’s behavior in finite samples.</p></li>
</ol>
<hr />
<p><strong>Relationship Between Consistency and Unbiasedness</strong></p>
<ul>
<li><strong>Unbiasedness</strong>:
<ul>
<li>An estimator <span class="math inline">\(\hat{\theta}\)</span> is unbiased if its expected value equals the true parameter: <span class="math display">\[
E(\hat{\theta}) = \theta.
\]</span></li>
<li>Unbiasedness is a finite-sample property and does not depend on the sample size.</li>
</ul></li>
<li><strong>Consistency</strong>:
<ul>
<li>Consistency is a large-sample property and requires <span class="math inline">\(\hat{\theta}\)</span> to converge to <span class="math inline">\(\theta\)</span> as <span class="math inline">\(n \to \infty\)</span>.</li>
</ul></li>
</ul>
<hr />
<p><strong>Important Distinctions</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Unbiasedness Does Not Imply Consistency</strong>:
<ul>
<li>Example: Consider an unbiased estimator with extremely high variance that does not diminish as <span class="math inline">\(n\)</span> increases. Such an estimator does not converge to <span class="math inline">\(\theta\)</span> in probability.</li>
</ul></li>
<li><strong>Consistency Does Not Imply Unbiasedness</strong>:
<ul>
<li>Example: <span class="math inline">\(\hat{\theta}_n = \frac{n-1}{n}\theta\)</span> is biased for all finite <span class="math inline">\(n\)</span>, but as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\hat{\theta}_n \to^p \theta\)</span>, making it consistent.</li>
</ul></li>
</ol>
<hr />
<p>From the OLS formula: <span class="math display">\[
\hat{\beta} = \mathbf{(X&#39;X)^{-1}X&#39;y},
\]</span> we can expand as: <span class="math display">\[
\hat{\beta} = \mathbf{(\sum_{i=1}^{n}x_i&#39;x_i)^{-1} \sum_{i=1}^{n}x_i&#39;y_i},
\]</span> or equivalently: <span class="math display">\[
\hat{\beta} = (n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;y_i}.
\]</span></p>
<p>Taking the probability limit under the assumptions <a href="ordinary-least-squares.html#a2-full-rank">A2</a> and <a href="ordinary-least-squares.html#a5-data-generation-random-sampling">A5</a>, we apply the <a href="general-math.html#weak-law-of-large-numbers">Weak Law of Large Numbers</a> (for a random sample, averages converge to expectations as <span class="math inline">\(n \to \infty\)</span>): <span class="math display">\[
plim(\hat{\beta}) = plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;y_i}),
\]</span> which simplifies to: <span class="math display">\[
\begin{aligned}
plim(\hat{\beta})
  &amp;= (E(\mathbf{x_i&#39;x_i}))^{-1}E(\mathbf{x_i&#39;y_i}) &amp; \text{A1}\\
  &amp;= (E(\mathbf{x_i&#39;x_i}))^{-1} \left(E(\mathbf{x_i&#39;x_i} \,\beta) + E(\mathbf{x_i\,\epsilon_i})\right) &amp; (A3a) \\
  &amp;= (E(\mathbf{x_i&#39; x_i}))^{-1}E(\mathbf{x_i&#39; x_i})\, \beta &amp; (A2)\\
  &amp;= \beta
\end{aligned}
\]</span></p>
<p><strong>Proof:</strong></p>
<p>For a model of <span class="math inline">\(y_i = x_i \beta + \epsilon_i\)</span>, where <span class="math inline">\(\beta\)</span> is the true parameter vector, <span class="math inline">\(\epsilon_i\)</span> is the random error:</p>
<p>Expanding <span class="math inline">\(E(x_i y_i)\)</span>:</p>
<p><span class="math display">\[
E(x_i&#39;y_i) = E(x_i&#39;(x_i\beta + \epsilon_i))
\]</span></p>
<p>By the linearity of expectation:</p>
<p><span class="math display">\[
E(x_i&#39;y_i) = E(x_i&#39;x_i \beta) + E(x_i \epsilon_i)
\]</span></p>
<p>The second term <span class="math inline">\(E(x_i \epsilon_i) = 0\)</span> under assumption <a href="#a3a">A3a</a>.</p>
<p>Thus, <span class="math display">\[
E(x_i&#39;y_i) = E(x_i&#39;x_i)\beta.
\]</span></p>
<hr />
<p><strong>Consistency of OLS</strong></p>
<p>Hence, in short, under the assumptions:</p>
<ul>
<li><a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a></li>
<li><a href="ordinary-least-squares.html#a2-full-rank">A2 Full Rank</a></li>
<li><a href="ordinary-least-squares.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a></li>
<li><a href="ordinary-least-squares.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></li>
</ul>
<p>the term <span class="math inline">\(E(\mathbf{x_i&#39;\epsilon_i}) = 0\)</span>, and the OLS estimator is <strong>consistent</strong>:</p>
<p><span class="math display">\[
plim(\hat{\beta}) = \beta.
\]</span></p>
<p>However, <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a> consistency does not guarantee unbiasedness in small samples.</p>
<hr />
<div id="asymptotic-distribution-of-ols" class="section level5 hasAnchor" number="5.1.2.4.1">
<h5><span class="header-section-number">5.1.2.4.1</span> Asymptotic Distribution of OLS<a href="ordinary-least-squares.html#asymptotic-distribution-of-ols" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Under the same assumptions :</p>
<ul>
<li><a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a></li>
<li><a href="ordinary-least-squares.html#a2-full-rank">A2 Full Rank</a></li>
<li><a href="ordinary-least-squares.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a></li>
<li><a href="ordinary-least-squares.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></li>
</ul>
<p>and if <span class="math inline">\(\mathbf{x_i&#39;x_i}\)</span> has finite first and second moments (required for the <a href="probability-theory.html#central-limit-theorem">Central Limit Theorem</a>), we have:</p>
<ol style="list-style-type: decimal">
<li><p>Convergence of <span class="math inline">\(n^{-1}\sum_{i=1}^n \mathbf{x_i&#39;x_i}\)</span>: <span class="math display">\[
n^{-1}\sum_{i=1}^n \mathbf{x_i&#39;x_i} \to^p E(\mathbf{x_i&#39;x_i}).
\]</span></p></li>
<li><p>Asymptotic normality of <span class="math inline">\(\sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i&#39;\epsilon_i})\)</span>: <span class="math display">\[
\sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i&#39;\epsilon_i}) \to^d N(0, \mathbf{B}),
\]</span> where <span class="math inline">\(\mathbf{B} = Var(\mathbf{x_i&#39;\epsilon_i})\)</span>.</p></li>
</ol>
<p>From these results, the scaled difference between <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\beta\)</span> follows: <span class="math display">\[
\sqrt{n}(\hat{\beta} - \beta) = (n^{-1}\sum_{i=1}^n \mathbf{x_i&#39;x_i})^{-1} \sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i&#39;\epsilon_i}).
\]</span></p>
<p>By the <a href="probability-theory.html#central-limit-theorem">Central Limit Theorem</a>: <span class="math display">\[
\sqrt{n}(\hat{\beta} - \beta) \to^d N(0, \Sigma),
\]</span> where: <span class="math display">\[
\Sigma = (E(\mathbf{x_i&#39;x_i}))^{-1} \mathbf{B} (E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p>
<p>The sandwich form is <span class="math inline">\(\Sigma\)</span> is standard.</p>
<p><strong>Implications for Homoskedasticity (A4) vs. Heteroskedasticity</strong></p>
<ol style="list-style-type: decimal">
<li><p>No Homoskedasticity (<a href="ordinary-least-squares.html#a4-homoskedasticity">A4 Homoskedasticity</a>) needed:</p>
<ul>
<li>the CLT and the large-sample distribution of <span class="math inline">\(\hat{\beta}\)</span> do <em>not</em> require homoskedasticity. The only place homoskedasticity would simplify things is that <span class="math display">\[
\mathbf{B} = Var(\mathbf{x_i&#39;\epsilon_i}) = \sigma^2 E(\mathbf{x_i&#39;x_i}),
\]</span></li>
</ul>
<p>only if <span class="math inline">\(Var(\epsilon_i | \mathbf{x}_i) \sigma^2\)</span></p>
<p>Then <span class="math display">\[
\Sigma = \sigma^2 (E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p></li>
<li><p>Adjusting for Heteroskedasticity:</p>
<ul>
<li>In practice, <span class="math inline">\(\sigma_i^2\)</span> can vary with <span class="math inline">\(\mathbf{x}_i\)</span>​, leading to heteroskedasticity.</li>
<li>The standard OLS formula for <span class="math inline">\(Var(\hat{\beta})\)</span> is inconsistent under heteroskedasticity, so one uses <em>robust (White) standard errors</em>.</li>
<li>Heteroskedasticity can arise from (but not limited to):
<ul>
<li>Limited dependent variables.</li>
<li>Dependent variables with large or skewed ranges.</li>
</ul></li>
</ul></li>
</ol>
<hr />
</div>
<div id="derivation-of-asymptotic-variance" class="section level5 hasAnchor" number="5.1.2.4.2">
<h5><span class="header-section-number">5.1.2.4.2</span> Derivation of Asymptotic Variance<a href="ordinary-least-squares.html#derivation-of-asymptotic-variance" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The asymptotic variance of the OLS estimator is derived as follows:</p>
<p><span class="math display">\[
\Sigma = (E(\mathbf{x_i&#39;x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Substituting <span class="math inline">\(\mathbf{B} = Var(\mathbf{x_i&#39;}\epsilon_i)\)</span>: <span class="math display">\[
\Sigma = (E(\mathbf{x_i&#39;x_i}))^{-1}Var(\mathbf{x_i&#39;}\epsilon_i)(E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p></li>
<li><p>Using the definition of variance: <span class="math display">\[
\Sigma = (E(\mathbf{x_i&#39;x_i}))^{-1}E[(\mathbf{x_i&#39;}\epsilon_i - 0)(\mathbf{x_i&#39;}\epsilon_i - 0)&#39;](E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p></li>
<li><p>By the [Law of Iterated Expectations] and <a href="ordinary-least-squares.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a>, we have: <span class="math display">\[
E[(\mathbf{x_i&#39;}\epsilon_i)(\mathbf{x_i&#39;}\epsilon_i)&#39;] = E[E(\epsilon_i^2|\mathbf{x_i})\mathbf{x_i&#39;x_i}],
\]</span></p></li>
<li><p>Assuming homoskedasticity (<a href="ordinary-least-squares.html#a4-homoskedasticity">A4 Homoskedasticity</a>), <span class="math inline">\(E(\epsilon_i^2|\mathbf{x_i}) = \sigma^2\)</span>, so: <span class="math display">\[
\Sigma = (E(\mathbf{x_i&#39;x_i}))^{-1}\sigma^2E(\mathbf{x_i&#39;x_i})(E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p></li>
<li><p>Simplifying: <span class="math display">\[
\Sigma = \sigma^2(E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p></li>
</ol>
<p>Hence, under the assumptions:</p>
<ul>
<li><p><a href="ordinary-least-squares.html#a1-linearity">A1 Linearity</a></p></li>
<li><p><a href="ordinary-least-squares.html#a2-full-rank">A2 Full Rank</a></p></li>
<li><p><a href="ordinary-least-squares.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a></p></li>
<li><p><a href="ordinary-least-squares.html#a4-homoskedasticity">A4 Homoskedasticity</a></p></li>
<li><p><a href="ordinary-least-squares.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></p></li>
</ul>
<p>we have<span class="math display">\[
\sqrt{n}(\hat{\beta} - \beta) \to^d N(0, \sigma^2(E(\mathbf{x_i&#39;x_i}))^{-1}).
\]</span></p>
<p>The asymptotic variance provides an approximation of the scaled estimator’s variance for large <span class="math inline">\(n\)</span>. This leads to:</p>
<p><span class="math display">\[
Avar(\sqrt{n}(\hat{\beta} - \beta)) = \sigma^2(E(\mathbf{x_i&#39;x_i}))^{-1}.
\]</span></p>
<p>The finite sample variance of an estimator can be approximated using the asymptotic variance for large sample sizes:</p>
<p><span class="math display">\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta)) &amp;\approx Var(\sqrt{n}(\hat{\beta}-\beta)) \\
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &amp;\approx Var(\sqrt{n}(\hat{\beta}-\beta))/n = Var(\hat{\beta})
\end{aligned}
\]</span></p>
<p>However, it is critical to note that <strong>asymptotic variance</strong> (<span class="math inline">\(Avar(.)\)</span>) does not behave in the same manner as finite sample variance (<span class="math inline">\(Var(.)\)</span>). This distinction is evident in the following expressions:</p>
<p><span class="math display">\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &amp;\neq Avar(\sqrt{n}(\hat{\beta}-\beta)/\sqrt{n}) \\
&amp;\neq Avar(\hat{\beta})
\end{aligned}
\]</span></p>
<p>Thus, while <span class="math inline">\(Avar(.)\)</span> provides a useful approximation for large samples, its conceptual properties differ from finite sample variance.</p>
<hr />
<p>In <a href="ordinary-least-squares.html#finite-sample-properties">Finite Sample Properties</a>, the standard errors are calculated as estimates of the conditional standard deviation:</p>
<p><span class="math display">\[
SE_{fs}(\hat{\beta}_{j-1}) = \sqrt{\hat{Var}(\hat{\beta}_{j-1}|\mathbf{X})} = \sqrt{s^2 \cdot [\mathbf{(X&#39;X)}^{-1}]_{jj}},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(s^2\)</span> is the estimator of the error variance,</p></li>
<li><p><span class="math inline">\([\mathbf{(X&#39;X)}^{-1}]_{jj}\)</span> represents the <span class="math inline">\(j\)</span>th diagonal element of the inverse design matrix.</p></li>
</ul>
<p>In contrast, in <a href="ordinary-least-squares.html#large-sample-properties">Large Sample Properties</a>, the standard errors are calculated as estimates of the square root of the asymptotic variance:</p>
<p><span class="math display">\[
SE_{ls}(\hat{\beta}_{j-1}) = \sqrt{\hat{Avar}(\sqrt{n} \hat{\beta}_{j-1})/n} = \sqrt{s^2 \cdot [\mathbf{(X&#39;X)}^{-1}]_{jj}}.
\]</span></p>
<p>Interestingly, the standard error estimator is <strong>identical</strong> for both finite and large samples:</p>
<ul>
<li>The expressions for <span class="math inline">\(SE_{fs}\)</span> and <span class="math inline">\(SE_{ls}\)</span> are mathematically the same.</li>
<li>However, they are conceptually estimating two <strong>different quantities</strong>:
<ul>
<li><strong>Finite Sample Standard Error</strong>: An estimate of the conditional standard deviation of <span class="math inline">\(\hat{\beta}_{j-1}\)</span> given <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li><strong>Large Sample Standard Error</strong>: An estimate of the square root of the asymptotic variance of <span class="math inline">\(\hat{\beta}_{j-1}\)</span>.</li>
</ul></li>
</ul>
<p>The assumptions required for these estimators to be valid differ in their stringency:</p>
<ol style="list-style-type: decimal">
<li><strong>Finite Sample Variance (Conditional Variance):</strong>
<ul>
<li>Requires stronger assumptions (A1-A5).</li>
</ul></li>
<li><strong>Asymptotic Variance:</strong>
<ul>
<li>Valid under weaker assumptions (A1, A2, A3a, A4, A5).</li>
</ul></li>
</ol>
<p>This distinction highlights the utility of asymptotic properties in providing robust approximations when finite sample assumptions may not hold.</p>
</div>
</div>
<div id="diagnostics" class="section level4 hasAnchor" number="5.1.2.5">
<h4><span class="header-section-number">5.1.2.5</span> Diagnostics<a href="ordinary-least-squares.html#diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="normality-of-errors" class="section level5 hasAnchor" number="5.1.2.5.1">
<h5><span class="header-section-number">5.1.2.5.1</span> Normality of Errors<a href="ordinary-least-squares.html#normality-of-errors" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Ensuring the normality of errors is a critical assumption in many regression models. Deviations from this assumption can impact inference and model interpretation. For diagnoses assessing normality, see <a href="normality-assessment.html#normality-assessment">Normality Assessment</a>.</p>
<p>Plots are invaluable for visual inspection of normality. One common approach is the Q-Q plot, which compares the quantiles of the residuals against those of a standard normal distribution:</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="ordinary-least-squares.html#cb171-1" tabindex="-1"></a><span class="co"># Example Q-Q plot</span></span>
<span id="cb171-2"><a href="ordinary-least-squares.html#cb171-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># For reproducibility</span></span>
<span id="cb171-3"><a href="ordinary-least-squares.html#cb171-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb171-4"><a href="ordinary-least-squares.html#cb171-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>) <span class="co"># Generating random normal data</span></span>
<span id="cb171-5"><a href="ordinary-least-squares.html#cb171-5" tabindex="-1"></a><span class="fu">qqplot</span>(x,</span>
<span id="cb171-6"><a href="ordinary-least-squares.html#cb171-6" tabindex="-1"></a>       y,</span>
<span id="cb171-7"><a href="ordinary-least-squares.html#cb171-7" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Q-Q Plot&quot;</span>,</span>
<span id="cb171-8"><a href="ordinary-least-squares.html#cb171-8" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Theoretical Quantiles&quot;</span>,</span>
<span id="cb171-9"><a href="ordinary-least-squares.html#cb171-9" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;Sample Quantiles&quot;</span>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-5-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="influential-observations-and-outliers" class="section level5 hasAnchor" number="5.1.2.5.2">
<h5><span class="header-section-number">5.1.2.5.2</span> Influential Observations and Outliers<a href="ordinary-least-squares.html#influential-observations-and-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Identifying influential observations or outliers is essential for robust regression modeling. The <strong>hat matrix</strong> (<span class="math inline">\(\mathbf{H}\)</span>) plays a key role in diagnosing influence.</p>
<div id="hat-matrix-outliers-in-x-space" class="section level6 hasAnchor" number="5.1.2.5.2.1">
<h6><span class="header-section-number">5.1.2.5.2.1</span> Hat Matrix: Outliers in X-Space<a href="ordinary-least-squares.html#hat-matrix-outliers-in-x-space" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>The hat matrix is primarily concerned with <strong>leverage</strong>, which reflects how far an observation’s predictor values (<span class="math inline">\(X\)</span>-space) are from the centroid of the predictor space.</p>
<ul>
<li><p><strong>What it measures:</strong> The diagonal elements of the hat matrix quantify <strong>leverage</strong>, not residual size or model fit. High leverage suggests that an observation has an unusual predictor configuration and might disproportionately influence the regression line, irrespective of the response variable.</p></li>
<li><p><strong>What it doesn’t measure:</strong> It doesn’t directly account for outliers in <span class="math inline">\(Y\)</span>-space or residuals.</p></li>
</ul>
<p>The hat matrix, defined as:</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X}(\mathbf{X}&#39;\mathbf{X})^{-1}
\]</span></p>
<p>has the following properties:</p>
<ul>
<li><p><strong>Fitted Values</strong>: <span class="math inline">\(\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}\)</span>.</p></li>
<li><p><strong>Residuals</strong>: <span class="math inline">\(\mathbf{e} = (\mathbf{I} - \mathbf{H})\mathbf{Y}\)</span>.</p></li>
<li><p><strong>Variance of Residuals</strong>: <span class="math inline">\(\text{var}(\mathbf{e}) = \sigma^2 (\mathbf{I} - \mathbf{H})\)</span>.</p></li>
</ul>
<p>Diagonal Elements of the Hat Matrix (<span class="math inline">\(h_{ii}\)</span>)</p>
<ul>
<li><p><span class="math inline">\(h_{ii}\)</span> is the <span class="math inline">\(i\)</span>-th element on the main diagonal of <span class="math inline">\(\mathbf{H}\)</span>. It must satisfy <span class="math inline">\(0 \leq h_{ii} \leq 1\)</span>.</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^{n} h_{ii} = p\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters (including the intercept).</p></li>
<li><p>The variance of residuals for observation <span class="math inline">\(i\)</span> is given by <span class="math inline">\(\sigma^2(e_i) = \sigma^2 (1 - h_{ii})\)</span>.</p></li>
<li><p>The covariance between residuals for observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> (<span class="math inline">\(i \neq j\)</span>) is <span class="math inline">\(-h_{ij}\sigma^2\)</span>.</p></li>
</ul>
<p>For large datasets, the off-diagonal elements (<span class="math inline">\(h_{ij}\)</span>) tend to have small covariance if model assumptions hold.</p>
<p>Estimations Using MSE</p>
<ul>
<li><p>Variance of residuals: <span class="math inline">\(s^2(e_i) = MSE (1 - h_{ii})\)</span>, where <span class="math inline">\(MSE\)</span> is the mean squared error.</p></li>
<li><p>Covariance of residuals: <span class="math inline">\(\hat{\text{cov}}(e_i, e_j) = -h_{ij}(MSE)\)</span>.</p></li>
</ul>
<p>Interpretation of <span class="math inline">\(h_{ii}\)</span></p>
<p>If <span class="math inline">\(\mathbf{x}_i = [1, X_{i,1}, \ldots, X_{i,p-1}]&#39;\)</span> represents the vector of predictor values for observation <span class="math inline">\(i\)</span>, then:</p>
<p><span class="math display">\[
h_{ii} = \mathbf{x}_i&#39; (\mathbf{X}&#39;\mathbf{X})^{-1} \mathbf{x}_i
\]</span></p>
<p>The value of <span class="math inline">\(h_{ii}\)</span> depends on the relative positions of the design points <span class="math inline">\(X_{i,1}, \ldots, X_{i,p-1}\)</span>. Observations with high <span class="math inline">\(h_{ii}\)</span> are more influential and warrant closer inspection for leverage or outlier behavior.</p>
</div>
<div id="studentized-residuals-outliers-in-y-space" class="section level6 hasAnchor" number="5.1.2.5.2.2">
<h6><span class="header-section-number">5.1.2.5.2.2</span> Studentized Residuals: Outliers in Y-Space<a href="ordinary-least-squares.html#studentized-residuals-outliers-in-y-space" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Residuals focus on discrepancies between observed (<span class="math inline">\(Y\)</span>) and predicted (<span class="math inline">\(\hat{Y}\)</span>) values, helping to identify outliers in <span class="math inline">\(Y\)</span>-space.</p>
<ul>
<li><p><strong>What they measure:</strong></p>
<ul>
<li><p>Standardized or studentized residuals assess how far an observation’s response is from the regression line, adjusted for variance.</p></li>
<li><p><strong>Externally studentized residuals</strong> are more robust because they exclude the <span class="math inline">\(i\)</span>-th observation when estimating variance.</p></li>
<li><p>Large studentized residuals (e.g., <span class="math inline">\(&gt;2\)</span> or <span class="math inline">\(&gt;3\)</span>) indicate observations that are unusual in <span class="math inline">\(Y\)</span>-space.</p></li>
</ul></li>
<li><p><strong>What they don’t measure:</strong> They do not consider leverage or the <span class="math inline">\(X\)</span>-space configuration. A point with a large residual could have low leverage, making it less influential overall.</p></li>
</ul>
<p>Studentized residuals, also known as standardized residuals, adjust for the variance of residuals by dividing the residuals by their standard error:</p>
<p><span class="math display">\[
\begin{aligned}
r_i &amp;= \frac{e_i}{s(e_i)} \\
r_i &amp;\sim N(0,1),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(s(e_i) = \sqrt{MSE(1-h_{ii})}\)</span>, and <span class="math inline">\(r_i\)</span> accounts for the varying variances of residuals. These residuals allow for a better comparison of model fit across observations.</p>
<ul>
<li><strong>Semi-Studentized Residuals</strong>: In contrast, the semi-studentized residuals are defined as:</li>
</ul>
<p><span class="math display">\[
e_i^* = \frac{e_i}{\sqrt{MSE}}
\]</span></p>
<p>This approach does not adjust for the heterogeneity in variances of residuals, as <span class="math inline">\(e_i^*\)</span> assumes equal variance for all residuals.</p>
<hr />
<p>To assess the influence of individual observations, we consider the model without a particular value. When the <span class="math inline">\(i\)</span>-th observation is removed, the <strong>deleted residual</strong> is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
d_i &amp;= Y_i - \hat{Y}_{i(i)} \\
    &amp;= \frac{e_i}{1-h_{ii}},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the actual observation, and <span class="math inline">\(\hat{Y}_{i(i)}\)</span> is the predicted value for the <span class="math inline">\(i\)</span>-th observation, computed using the regression model fitted to the remaining <span class="math inline">\(n-1\)</span> observations. Importantly, we do not need to refit the regression model for each observation to compute <span class="math inline">\(d_i\)</span>.</p>
<ul>
<li>As <span class="math inline">\(h_{ii}\)</span> (leverage) increases, <span class="math inline">\(d_i\)</span> also increases, indicating higher influence of the observation.</li>
</ul>
<p>The variance of the deleted residual is given by:</p>
<p><span class="math display">\[
s^2(d_i) = \frac{MSE_{(i)}}{1-h_{ii}},
\]</span></p>
<p>where <span class="math inline">\(MSE_{(i)}\)</span> is the mean squared error when the <span class="math inline">\(i\)</span>-th case is omitted.</p>
<hr />
<p>The <strong>studentized deleted residual</strong> accounts for variability and follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-p-1\)</span> degrees of freedom:</p>
<p><span class="math display">\[
t_i = \frac{d_i}{s(d_i)} = \frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}},
\]</span></p>
<p>where <span class="math inline">\(t_i\)</span> helps identify outliers more effectively.</p>
<p>We can compute <span class="math inline">\(t_i\)</span> without fitting the regression model multiple times. Using the relationship:</p>
<p><span class="math display">\[
(n-p)MSE = (n-p-1)MSE_{(i)} + \frac{e_i^2}{1-h_{ii}},
\]</span></p>
<p>we derive:</p>
<p><span class="math display">\[
t_i = e_i \sqrt{\frac{n-p-1}{SSE(1-h_{ii}) - e_i^2}}.
\]</span></p>
<p>This formulation avoids the need for recalculating <span class="math inline">\(MSE_{(i)}\)</span> explicitly for each case.</p>
<hr />
<p>Outlying <span class="math inline">\(Y\)</span>-observations are those with large studentized deleted residuals in absolute value. To handle multiple testing when there are many residuals, we use a Bonferroni-adjusted critical value:</p>
<p><span class="math display">\[
t_{1-\alpha/2n; n-p-1},
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the desired significance level, <span class="math inline">\(n\)</span> is the sample size, and <span class="math inline">\(p\)</span> is the number of parameters in the model. Observations exceeding this threshold are flagged as potential outliers.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="ordinary-least-squares.html#cb172-1" tabindex="-1"></a><span class="co"># Example R Code for Demonstrating Residual Diagnostics</span></span>
<span id="cb172-2"><a href="ordinary-least-squares.html#cb172-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># For reproducibility</span></span>
<span id="cb172-3"><a href="ordinary-least-squares.html#cb172-3" tabindex="-1"></a></span>
<span id="cb172-4"><a href="ordinary-least-squares.html#cb172-4" tabindex="-1"></a><span class="co"># Simulate some data</span></span>
<span id="cb172-5"><a href="ordinary-least-squares.html#cb172-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb172-6"><a href="ordinary-least-squares.html#cb172-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">3</span>)</span>
<span id="cb172-7"><a href="ordinary-least-squares.html#cb172-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb172-8"><a href="ordinary-least-squares.html#cb172-8" tabindex="-1"></a></span>
<span id="cb172-9"><a href="ordinary-least-squares.html#cb172-9" tabindex="-1"></a><span class="co"># Fit a linear regression model</span></span>
<span id="cb172-10"><a href="ordinary-least-squares.html#cb172-10" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb172-11"><a href="ordinary-least-squares.html#cb172-11" tabindex="-1"></a></span>
<span id="cb172-12"><a href="ordinary-least-squares.html#cb172-12" tabindex="-1"></a></span>
<span id="cb172-13"><a href="ordinary-least-squares.html#cb172-13" tabindex="-1"></a><span class="co"># Extract residuals, fitted values, and hat values</span></span>
<span id="cb172-14"><a href="ordinary-least-squares.html#cb172-14" tabindex="-1"></a>residuals <span class="ot">&lt;-</span> <span class="fu">resid</span>(model)</span>
<span id="cb172-15"><a href="ordinary-least-squares.html#cb172-15" tabindex="-1"></a>hat_values <span class="ot">&lt;-</span> <span class="fu">lm.influence</span>(model)<span class="sc">$</span>hat</span>
<span id="cb172-16"><a href="ordinary-least-squares.html#cb172-16" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>(residuals <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb172-17"><a href="ordinary-least-squares.html#cb172-17" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(residuals)</span>
<span id="cb172-18"><a href="ordinary-least-squares.html#cb172-18" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">coefficients</span>(model)) <span class="co"># Number of parameters</span></span>
<span id="cb172-19"><a href="ordinary-least-squares.html#cb172-19" tabindex="-1"></a></span>
<span id="cb172-20"><a href="ordinary-least-squares.html#cb172-20" tabindex="-1"></a><span class="co"># Compute studentized residuals</span></span>
<span id="cb172-21"><a href="ordinary-least-squares.html#cb172-21" tabindex="-1"></a>studentized_residuals <span class="ot">&lt;-</span> residuals <span class="sc">/</span> <span class="fu">sqrt</span>(mse <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> hat_values))</span>
<span id="cb172-22"><a href="ordinary-least-squares.html#cb172-22" tabindex="-1"></a></span>
<span id="cb172-23"><a href="ordinary-least-squares.html#cb172-23" tabindex="-1"></a><span class="co"># Compute deleted residuals</span></span>
<span id="cb172-24"><a href="ordinary-least-squares.html#cb172-24" tabindex="-1"></a>deleted_residuals <span class="ot">&lt;-</span> residuals <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> hat_values)</span>
<span id="cb172-25"><a href="ordinary-least-squares.html#cb172-25" tabindex="-1"></a></span>
<span id="cb172-26"><a href="ordinary-least-squares.html#cb172-26" tabindex="-1"></a><span class="co"># Compute studentized deleted residuals</span></span>
<span id="cb172-27"><a href="ordinary-least-squares.html#cb172-27" tabindex="-1"></a>studentized_deleted_residuals <span class="ot">&lt;-</span></span>
<span id="cb172-28"><a href="ordinary-least-squares.html#cb172-28" tabindex="-1"></a>    residuals <span class="sc">*</span> <span class="fu">sqrt</span>((n <span class="sc">-</span> p <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb172-29"><a href="ordinary-least-squares.html#cb172-29" tabindex="-1"></a>                     <span class="sc">/</span> (<span class="fu">sum</span>(residuals <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> hat_values) <span class="sc">-</span> residuals <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb172-30"><a href="ordinary-least-squares.html#cb172-30" tabindex="-1"></a></span>
<span id="cb172-31"><a href="ordinary-least-squares.html#cb172-31" tabindex="-1"></a><span class="co"># Flag potential outliers using Bonferroni-adjusted critical value</span></span>
<span id="cb172-32"><a href="ordinary-least-squares.html#cb172-32" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb172-33"><a href="ordinary-least-squares.html#cb172-33" tabindex="-1"></a>bonferroni_threshold <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="dv">1</span> <span class="sc">-</span> alpha <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> n), <span class="at">df =</span> n <span class="sc">-</span> p <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb172-34"><a href="ordinary-least-squares.html#cb172-34" tabindex="-1"></a>outliers <span class="ot">&lt;-</span></span>
<span id="cb172-35"><a href="ordinary-least-squares.html#cb172-35" tabindex="-1"></a>    <span class="fu">abs</span>(studentized_deleted_residuals) <span class="sc">&gt;</span> bonferroni_threshold</span>
<span id="cb172-36"><a href="ordinary-least-squares.html#cb172-36" tabindex="-1"></a></span>
<span id="cb172-37"><a href="ordinary-least-squares.html#cb172-37" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb172-38"><a href="ordinary-least-squares.html#cb172-38" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb172-39"><a href="ordinary-least-squares.html#cb172-39" tabindex="-1"></a>    <span class="at">Residuals                     =</span> residuals,</span>
<span id="cb172-40"><a href="ordinary-least-squares.html#cb172-40" tabindex="-1"></a>    <span class="at">Hat_Values                    =</span> hat_values,</span>
<span id="cb172-41"><a href="ordinary-least-squares.html#cb172-41" tabindex="-1"></a>    <span class="at">Studentized_Residuals         =</span> studentized_residuals,</span>
<span id="cb172-42"><a href="ordinary-least-squares.html#cb172-42" tabindex="-1"></a>    <span class="at">Deleted_Residuals             =</span> deleted_residuals,</span>
<span id="cb172-43"><a href="ordinary-least-squares.html#cb172-43" tabindex="-1"></a>    <span class="at">Studentized_Deleted_Residuals =</span> studentized_deleted_residuals,</span>
<span id="cb172-44"><a href="ordinary-least-squares.html#cb172-44" tabindex="-1"></a>    <span class="at">Outlier                       =</span> outliers</span>
<span id="cb172-45"><a href="ordinary-least-squares.html#cb172-45" tabindex="-1"></a>)</span>
<span id="cb172-46"><a href="ordinary-least-squares.html#cb172-46" tabindex="-1"></a></span>
<span id="cb172-47"><a href="ordinary-least-squares.html#cb172-47" tabindex="-1"></a>causalverse<span class="sc">::</span><span class="fu">nice_tab</span>(<span class="fu">head</span>(results))</span>
<span id="cb172-48"><a href="ordinary-least-squares.html#cb172-48" tabindex="-1"></a><span class="co">#&gt;   Residuals Hat_Values Studentized_Residuals Deleted_Residuals</span></span>
<span id="cb172-49"><a href="ordinary-least-squares.html#cb172-49" tabindex="-1"></a><span class="co">#&gt; 1     -1.27       0.02                 -0.67             -1.29</span></span>
<span id="cb172-50"><a href="ordinary-least-squares.html#cb172-50" tabindex="-1"></a><span class="co">#&gt; 2      0.70       0.01                  0.36              0.70</span></span>
<span id="cb172-51"><a href="ordinary-least-squares.html#cb172-51" tabindex="-1"></a><span class="co">#&gt; 3     -0.12       0.04                 -0.07             -0.13</span></span>
<span id="cb172-52"><a href="ordinary-least-squares.html#cb172-52" tabindex="-1"></a><span class="co">#&gt; 4     -0.48       0.01                 -0.25             -0.49</span></span>
<span id="cb172-53"><a href="ordinary-least-squares.html#cb172-53" tabindex="-1"></a><span class="co">#&gt; 5     -1.68       0.01                 -0.88             -1.70</span></span>
<span id="cb172-54"><a href="ordinary-least-squares.html#cb172-54" tabindex="-1"></a><span class="co">#&gt; 6      0.30       0.04                  0.16              0.31</span></span>
<span id="cb172-55"><a href="ordinary-least-squares.html#cb172-55" tabindex="-1"></a><span class="co">#&gt;   Studentized_Deleted_Residuals Outlier</span></span>
<span id="cb172-56"><a href="ordinary-least-squares.html#cb172-56" tabindex="-1"></a><span class="co">#&gt; 1                         -0.66   FALSE</span></span>
<span id="cb172-57"><a href="ordinary-least-squares.html#cb172-57" tabindex="-1"></a><span class="co">#&gt; 2                          0.36   FALSE</span></span>
<span id="cb172-58"><a href="ordinary-least-squares.html#cb172-58" tabindex="-1"></a><span class="co">#&gt; 3                         -0.06   FALSE</span></span>
<span id="cb172-59"><a href="ordinary-least-squares.html#cb172-59" tabindex="-1"></a><span class="co">#&gt; 4                         -0.25   FALSE</span></span>
<span id="cb172-60"><a href="ordinary-least-squares.html#cb172-60" tabindex="-1"></a><span class="co">#&gt; 5                         -0.87   FALSE</span></span>
<span id="cb172-61"><a href="ordinary-least-squares.html#cb172-61" tabindex="-1"></a><span class="co">#&gt; 6                          0.15   FALSE</span></span>
<span id="cb172-62"><a href="ordinary-least-squares.html#cb172-62" tabindex="-1"></a></span>
<span id="cb172-63"><a href="ordinary-least-squares.html#cb172-63" tabindex="-1"></a><span class="co"># Plot studentized deleted residuals for visualization</span></span>
<span id="cb172-64"><a href="ordinary-least-squares.html#cb172-64" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb172-65"><a href="ordinary-least-squares.html#cb172-65" tabindex="-1"></a>    studentized_deleted_residuals,</span>
<span id="cb172-66"><a href="ordinary-least-squares.html#cb172-66" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;Studentized Deleted Residuals&quot;</span>,</span>
<span id="cb172-67"><a href="ordinary-least-squares.html#cb172-67" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;Observation&quot;</span>,</span>
<span id="cb172-68"><a href="ordinary-least-squares.html#cb172-68" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Studentized Deleted Residuals&quot;</span>,</span>
<span id="cb172-69"><a href="ordinary-least-squares.html#cb172-69" tabindex="-1"></a>    <span class="at">pch =</span> <span class="dv">16</span>,</span>
<span id="cb172-70"><a href="ordinary-least-squares.html#cb172-70" tabindex="-1"></a>    <span class="at">col =</span> <span class="fu">ifelse</span>(outliers, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>)</span>
<span id="cb172-71"><a href="ordinary-least-squares.html#cb172-71" tabindex="-1"></a>)</span>
<span id="cb172-72"><a href="ordinary-least-squares.html#cb172-72" tabindex="-1"></a><span class="fu">abline</span>(</span>
<span id="cb172-73"><a href="ordinary-least-squares.html#cb172-73" tabindex="-1"></a>    <span class="at">h =</span> <span class="fu">c</span>(<span class="sc">-</span>bonferroni_threshold, bonferroni_threshold),</span>
<span id="cb172-74"><a href="ordinary-least-squares.html#cb172-74" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb172-75"><a href="ordinary-least-squares.html#cb172-75" tabindex="-1"></a>    <span class="at">lty =</span> <span class="dv">2</span></span>
<span id="cb172-76"><a href="ordinary-least-squares.html#cb172-76" tabindex="-1"></a>)</span>
<span id="cb172-77"><a href="ordinary-least-squares.html#cb172-77" tabindex="-1"></a><span class="fu">legend</span>(</span>
<span id="cb172-78"><a href="ordinary-least-squares.html#cb172-78" tabindex="-1"></a>    <span class="st">&quot;topright&quot;</span>,</span>
<span id="cb172-79"><a href="ordinary-least-squares.html#cb172-79" tabindex="-1"></a>    <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Potential Outliers&quot;</span>, <span class="st">&quot;Threshold&quot;</span>),</span>
<span id="cb172-80"><a href="ordinary-least-squares.html#cb172-80" tabindex="-1"></a>    <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>),</span>
<span id="cb172-81"><a href="ordinary-least-squares.html#cb172-81" tabindex="-1"></a>    <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="cn">NA</span>),</span>
<span id="cb172-82"><a href="ordinary-least-squares.html#cb172-82" tabindex="-1"></a>    <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">2</span>)</span>
<span id="cb172-83"><a href="ordinary-least-squares.html#cb172-83" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="identifying-influential-cases" class="section level5 hasAnchor" number="5.1.2.5.3">
<h5><span class="header-section-number">5.1.2.5.3</span> Identifying Influential Cases<a href="ordinary-least-squares.html#identifying-influential-cases" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>By <strong>influential</strong>, we refer to observations whose exclusion causes major changes in the fitted regression model. Note that not all outliers are influential.</p>
<p>Types of Influence Measures</p>
<ul>
<li><strong>Influence on Single Fitted Values</strong>: <a href="ordinary-least-squares.html#dffits">DFFITS</a></li>
<li><strong>Influence on All Fitted Values</strong>: <a href="ordinary-least-squares.html#cooks-d">Cook’s D</a></li>
<li><strong>Influence on the Regression Coefficients</strong>: <a href="ordinary-least-squares.html#dfbetas">DFBETAS</a></li>
</ul>
<p>Measures like Cook’s D, DFFITS, and DFBETAS combine leverage (from the hat matrix) and residual size (from studentized residuals) to assess the <strong>influence</strong> of an observation on the model as a whole. Hence, these effectively combine impact of <span class="math inline">\(X\)</span>-space and <span class="math inline">\(Y\)</span>-space.</p>
<div id="dffits" class="section level6 hasAnchor" number="5.1.2.5.3.1">
<h6><span class="header-section-number">5.1.2.5.3.1</span> DFFITS<a href="ordinary-least-squares.html#dffits" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>DFFITS measures the <strong>influence on single fitted values</strong>. It is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
(DFFITS)_i &amp;= \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)}h_{ii}}} \\
&amp;= t_i \left(\frac{h_{ii}}{1-h_{ii}}\right)^{1/2}
\end{aligned}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\hat{Y}_i\)</span> = fitted value for observation <span class="math inline">\(i\)</span> using all data.</li>
<li><span class="math inline">\(\hat{Y}_{i(i)}\)</span> = fitted value for observation <span class="math inline">\(i\)</span> with the <span class="math inline">\(i\)</span>th case removed.</li>
<li><span class="math inline">\(MSE_{(i)}\)</span> = mean squared error with observation <span class="math inline">\(i\)</span> excluded.</li>
<li><span class="math inline">\(h_{ii}\)</span> = leverage of the <span class="math inline">\(i\)</span>th observation.</li>
<li><span class="math inline">\(t_i\)</span> = studentized deleted residual.</li>
</ul>
<p>High DFFITS values occur when leverage and residuals are jointly significant.</p>
<p>DFFITS captures the <strong>standardized difference between the fitted value for observation</strong> <span class="math inline">\(i\)</span> with and without the <span class="math inline">\(i\)</span>th case in the model. It is a product of:</p>
<ol style="list-style-type: decimal">
<li>The <strong>studentized deleted residual</strong>.</li>
<li>A scaling factor based on the leverage of the <span class="math inline">\(i\)</span>th observation, <span class="math inline">\(h_{ii}\)</span>.</li>
</ol>
<p>An observation is considered influential based on DFFITS if:</p>
<ul>
<li><strong>Small to medium data sets</strong>: <span class="math inline">\(|DFFITS| &gt; 1\)</span></li>
<li><strong>Large data sets</strong>: <span class="math inline">\(|DFFITS| &gt; 2 \sqrt{p/n}\)</span></li>
</ul>
<p>Where:</p>
<ul>
<li><span class="math inline">\(p\)</span> = number of predictors (including the intercept).</li>
<li><span class="math inline">\(n\)</span> = total number of observations.</li>
</ul>
<p>This provides a practical threshold for detecting influential observations in different dataset sizes.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="ordinary-least-squares.html#cb173-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb173-2"><a href="ordinary-least-squares.html#cb173-2" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb173-3"><a href="ordinary-least-squares.html#cb173-3" tabindex="-1"></a></span>
<span id="cb173-4"><a href="ordinary-least-squares.html#cb173-4" tabindex="-1"></a><span class="co"># Fit a linear model</span></span>
<span id="cb173-5"><a href="ordinary-least-squares.html#cb173-5" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb173-6"><a href="ordinary-least-squares.html#cb173-6" tabindex="-1"></a></span>
<span id="cb173-7"><a href="ordinary-least-squares.html#cb173-7" tabindex="-1"></a><span class="co"># Compute DFFITS</span></span>
<span id="cb173-8"><a href="ordinary-least-squares.html#cb173-8" tabindex="-1"></a>dffits_values <span class="ot">&lt;-</span> <span class="fu">dffits</span>(model)</span>
<span id="cb173-9"><a href="ordinary-least-squares.html#cb173-9" tabindex="-1"></a></span>
<span id="cb173-10"><a href="ordinary-least-squares.html#cb173-10" tabindex="-1"></a><span class="co"># Display influential observations based on the threshold for a large dataset</span></span>
<span id="cb173-11"><a href="ordinary-least-squares.html#cb173-11" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="fu">length</span>(<span class="fu">coefficients</span>(model)) <span class="sc">/</span> <span class="fu">nrow</span>(mtcars))</span>
<span id="cb173-12"><a href="ordinary-least-squares.html#cb173-12" tabindex="-1"></a>influential_obs <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">abs</span>(dffits_values) <span class="sc">&gt;</span> threshold)</span>
<span id="cb173-13"><a href="ordinary-least-squares.html#cb173-13" tabindex="-1"></a></span>
<span id="cb173-14"><a href="ordinary-least-squares.html#cb173-14" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb173-15"><a href="ordinary-least-squares.html#cb173-15" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb173-16"><a href="ordinary-least-squares.html#cb173-16" tabindex="-1"></a>    <span class="at">DFFITS                   =</span> dffits_values,</span>
<span id="cb173-17"><a href="ordinary-least-squares.html#cb173-17" tabindex="-1"></a>    <span class="at">Threshold                =</span> threshold,</span>
<span id="cb173-18"><a href="ordinary-least-squares.html#cb173-18" tabindex="-1"></a>    <span class="at">Influential_Observations =</span> influential_obs</span>
<span id="cb173-19"><a href="ordinary-least-squares.html#cb173-19" tabindex="-1"></a>)</span>
<span id="cb173-20"><a href="ordinary-least-squares.html#cb173-20" tabindex="-1"></a><span class="co">#&gt; $DFFITS</span></span>
<span id="cb173-21"><a href="ordinary-least-squares.html#cb173-21" tabindex="-1"></a><span class="co">#&gt;           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive </span></span>
<span id="cb173-22"><a href="ordinary-least-squares.html#cb173-22" tabindex="-1"></a><span class="co">#&gt;        -0.218494101        -0.126664789        -0.249103400         0.011699160 </span></span>
<span id="cb173-23"><a href="ordinary-least-squares.html#cb173-23" tabindex="-1"></a><span class="co">#&gt;   Hornet Sportabout             Valiant          Duster 360           Merc 240D </span></span>
<span id="cb173-24"><a href="ordinary-least-squares.html#cb173-24" tabindex="-1"></a><span class="co">#&gt;         0.028162679        -0.253806124        -0.191618944         0.221917842 </span></span>
<span id="cb173-25"><a href="ordinary-least-squares.html#cb173-25" tabindex="-1"></a><span class="co">#&gt;            Merc 230            Merc 280           Merc 280C          Merc 450SE </span></span>
<span id="cb173-26"><a href="ordinary-least-squares.html#cb173-26" tabindex="-1"></a><span class="co">#&gt;         0.079763706        -0.067222732        -0.190099538         0.064280875 </span></span>
<span id="cb173-27"><a href="ordinary-least-squares.html#cb173-27" tabindex="-1"></a><span class="co">#&gt;          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental </span></span>
<span id="cb173-28"><a href="ordinary-least-squares.html#cb173-28" tabindex="-1"></a><span class="co">#&gt;         0.020560728        -0.135714533         0.008984366         0.227919348 </span></span>
<span id="cb173-29"><a href="ordinary-least-squares.html#cb173-29" tabindex="-1"></a><span class="co">#&gt;   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla </span></span>
<span id="cb173-30"><a href="ordinary-least-squares.html#cb173-30" tabindex="-1"></a><span class="co">#&gt;         1.231668760         0.749153703         0.165329646         0.865985851 </span></span>
<span id="cb173-31"><a href="ordinary-least-squares.html#cb173-31" tabindex="-1"></a><span class="co">#&gt;       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 </span></span>
<span id="cb173-32"><a href="ordinary-least-squares.html#cb173-32" tabindex="-1"></a><span class="co">#&gt;        -0.292008465        -0.253389811        -0.294709853        -0.170476763 </span></span>
<span id="cb173-33"><a href="ordinary-least-squares.html#cb173-33" tabindex="-1"></a><span class="co">#&gt;    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa </span></span>
<span id="cb173-34"><a href="ordinary-least-squares.html#cb173-34" tabindex="-1"></a><span class="co">#&gt;         0.207813200        -0.041423665        -0.004054382         0.471518032 </span></span>
<span id="cb173-35"><a href="ordinary-least-squares.html#cb173-35" tabindex="-1"></a><span class="co">#&gt;      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E </span></span>
<span id="cb173-36"><a href="ordinary-least-squares.html#cb173-36" tabindex="-1"></a><span class="co">#&gt;        -0.161026362        -0.129395315         0.907521354        -0.128232538 </span></span>
<span id="cb173-37"><a href="ordinary-least-squares.html#cb173-37" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb173-38"><a href="ordinary-least-squares.html#cb173-38" tabindex="-1"></a><span class="co">#&gt; $Threshold</span></span>
<span id="cb173-39"><a href="ordinary-least-squares.html#cb173-39" tabindex="-1"></a><span class="co">#&gt; [1] 0.6123724</span></span>
<span id="cb173-40"><a href="ordinary-least-squares.html#cb173-40" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb173-41"><a href="ordinary-least-squares.html#cb173-41" tabindex="-1"></a><span class="co">#&gt; $Influential_Observations</span></span>
<span id="cb173-42"><a href="ordinary-least-squares.html#cb173-42" tabindex="-1"></a><span class="co">#&gt; Chrysler Imperial          Fiat 128    Toyota Corolla     Maserati Bora </span></span>
<span id="cb173-43"><a href="ordinary-least-squares.html#cb173-43" tabindex="-1"></a><span class="co">#&gt;                17                18                20                31</span></span></code></pre></div>
</div>
<div id="cooks-d" class="section level6 hasAnchor" number="5.1.2.5.3.2">
<h6><span class="header-section-number">5.1.2.5.3.2</span> Cook’s D<a href="ordinary-least-squares.html#cooks-d" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><strong>Cook’s D</strong> measures the <strong>influence of the</strong> <span class="math inline">\(i\)</span>th case on all fitted values in a regression model. It is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
D_i &amp;= \frac{\sum_{j=1}^{n}(\hat{Y}_j - \hat{Y}_{j(i)})^2}{p(MSE)} \\
&amp;= \frac{e^2_i}{p(MSE)}\left(\frac{h_{ii}}{(1-h_{ii})^2}\right)
\end{aligned}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\hat{Y}_j\)</span> = fitted value for observation <span class="math inline">\(j\)</span> using all data.</li>
<li><span class="math inline">\(\hat{Y}_{j(i)}\)</span> = fitted value for observation <span class="math inline">\(j\)</span> with the <span class="math inline">\(i\)</span>th case removed.</li>
<li><span class="math inline">\(e_i\)</span> = residual for observation <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(h_{ii}\)</span> = leverage of the <span class="math inline">\(i\)</span>th observation.</li>
<li><span class="math inline">\(p\)</span> = number of predictors (including the intercept).</li>
<li><span class="math inline">\(MSE\)</span> = mean squared error of the model.</li>
</ul>
<p>Key Insights</p>
<ul>
<li><strong>Cook’s D quantifies the overall influence</strong> of the <span class="math inline">\(i\)</span>th observation on the entire set of fitted values.</li>
<li>If either the residual <span class="math inline">\(e_i\)</span> increases or the leverage <span class="math inline">\(h_{ii}\)</span> increases, then <span class="math inline">\(D_i\)</span> also increases, indicating higher influence.</li>
<li>Observations with both high leverage and large residuals are flagged as influential.</li>
</ul>
<p>Threshold for Influence</p>
<ul>
<li><span class="math inline">\(D_i\)</span> can be interpreted as a percentile of an <span class="math inline">\(F_{(p,n-p)}\)</span> distribution.</li>
<li>Practical thresholds:
<ul>
<li><strong>If</strong> <span class="math inline">\(D_i &gt; 4/n\)</span>, the <span class="math inline">\(i\)</span>th case has major influence.</li>
<li>Alternatively, cases where <span class="math inline">\(D_i\)</span> exceeds the 50th percentile of the <span class="math inline">\(F\)</span>-distribution may also be considered influential.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="ordinary-least-squares.html#cb174-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb174-2"><a href="ordinary-least-squares.html#cb174-2" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb174-3"><a href="ordinary-least-squares.html#cb174-3" tabindex="-1"></a></span>
<span id="cb174-4"><a href="ordinary-least-squares.html#cb174-4" tabindex="-1"></a><span class="co"># Fit a linear model</span></span>
<span id="cb174-5"><a href="ordinary-least-squares.html#cb174-5" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb174-6"><a href="ordinary-least-squares.html#cb174-6" tabindex="-1"></a></span>
<span id="cb174-7"><a href="ordinary-least-squares.html#cb174-7" tabindex="-1"></a><span class="co"># Compute Cook&#39;s D</span></span>
<span id="cb174-8"><a href="ordinary-least-squares.html#cb174-8" tabindex="-1"></a>cooks_d_values <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(model)</span>
<span id="cb174-9"><a href="ordinary-least-squares.html#cb174-9" tabindex="-1"></a></span>
<span id="cb174-10"><a href="ordinary-least-squares.html#cb174-10" tabindex="-1"></a><span class="co"># Display influential observations based on the threshold</span></span>
<span id="cb174-11"><a href="ordinary-least-squares.html#cb174-11" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="dv">4</span> <span class="sc">/</span> <span class="fu">nrow</span>(mtcars)</span>
<span id="cb174-12"><a href="ordinary-least-squares.html#cb174-12" tabindex="-1"></a>influential_obs <span class="ot">&lt;-</span> <span class="fu">which</span>(cooks_d_values <span class="sc">&gt;</span> threshold)</span>
<span id="cb174-13"><a href="ordinary-least-squares.html#cb174-13" tabindex="-1"></a></span>
<span id="cb174-14"><a href="ordinary-least-squares.html#cb174-14" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb174-15"><a href="ordinary-least-squares.html#cb174-15" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb174-16"><a href="ordinary-least-squares.html#cb174-16" tabindex="-1"></a>  <span class="at">Cooks_D                  =</span> cooks_d_values,</span>
<span id="cb174-17"><a href="ordinary-least-squares.html#cb174-17" tabindex="-1"></a>  <span class="at">Threshold                =</span> threshold,</span>
<span id="cb174-18"><a href="ordinary-least-squares.html#cb174-18" tabindex="-1"></a>  <span class="at">Influential_Observations =</span> influential_obs</span>
<span id="cb174-19"><a href="ordinary-least-squares.html#cb174-19" tabindex="-1"></a>)</span>
<span id="cb174-20"><a href="ordinary-least-squares.html#cb174-20" tabindex="-1"></a><span class="co">#&gt; $Cooks_D</span></span>
<span id="cb174-21"><a href="ordinary-least-squares.html#cb174-21" tabindex="-1"></a><span class="co">#&gt;           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive </span></span>
<span id="cb174-22"><a href="ordinary-least-squares.html#cb174-22" tabindex="-1"></a><span class="co">#&gt;        1.589652e-02        5.464779e-03        2.070651e-02        4.724822e-05 </span></span>
<span id="cb174-23"><a href="ordinary-least-squares.html#cb174-23" tabindex="-1"></a><span class="co">#&gt;   Hornet Sportabout             Valiant          Duster 360           Merc 240D </span></span>
<span id="cb174-24"><a href="ordinary-least-squares.html#cb174-24" tabindex="-1"></a><span class="co">#&gt;        2.736184e-04        2.155064e-02        1.255218e-02        1.677650e-02 </span></span>
<span id="cb174-25"><a href="ordinary-least-squares.html#cb174-25" tabindex="-1"></a><span class="co">#&gt;            Merc 230            Merc 280           Merc 280C          Merc 450SE </span></span>
<span id="cb174-26"><a href="ordinary-least-squares.html#cb174-26" tabindex="-1"></a><span class="co">#&gt;        2.188702e-03        1.554996e-03        1.215737e-02        1.423008e-03 </span></span>
<span id="cb174-27"><a href="ordinary-least-squares.html#cb174-27" tabindex="-1"></a><span class="co">#&gt;          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental </span></span>
<span id="cb174-28"><a href="ordinary-least-squares.html#cb174-28" tabindex="-1"></a><span class="co">#&gt;        1.458960e-04        6.266049e-03        2.786686e-05        1.780910e-02 </span></span>
<span id="cb174-29"><a href="ordinary-least-squares.html#cb174-29" tabindex="-1"></a><span class="co">#&gt;   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla </span></span>
<span id="cb174-30"><a href="ordinary-least-squares.html#cb174-30" tabindex="-1"></a><span class="co">#&gt;        4.236109e-01        1.574263e-01        9.371446e-03        2.083933e-01 </span></span>
<span id="cb174-31"><a href="ordinary-least-squares.html#cb174-31" tabindex="-1"></a><span class="co">#&gt;       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 </span></span>
<span id="cb174-32"><a href="ordinary-least-squares.html#cb174-32" tabindex="-1"></a><span class="co">#&gt;        2.791982e-02        2.087419e-02        2.751510e-02        9.943527e-03 </span></span>
<span id="cb174-33"><a href="ordinary-least-squares.html#cb174-33" tabindex="-1"></a><span class="co">#&gt;    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa </span></span>
<span id="cb174-34"><a href="ordinary-least-squares.html#cb174-34" tabindex="-1"></a><span class="co">#&gt;        1.443199e-02        5.920440e-04        5.674986e-06        7.353985e-02 </span></span>
<span id="cb174-35"><a href="ordinary-least-squares.html#cb174-35" tabindex="-1"></a><span class="co">#&gt;      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E </span></span>
<span id="cb174-36"><a href="ordinary-least-squares.html#cb174-36" tabindex="-1"></a><span class="co">#&gt;        8.919701e-03        5.732672e-03        2.720397e-01        5.600804e-03 </span></span>
<span id="cb174-37"><a href="ordinary-least-squares.html#cb174-37" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb174-38"><a href="ordinary-least-squares.html#cb174-38" tabindex="-1"></a><span class="co">#&gt; $Threshold</span></span>
<span id="cb174-39"><a href="ordinary-least-squares.html#cb174-39" tabindex="-1"></a><span class="co">#&gt; [1] 0.125</span></span>
<span id="cb174-40"><a href="ordinary-least-squares.html#cb174-40" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb174-41"><a href="ordinary-least-squares.html#cb174-41" tabindex="-1"></a><span class="co">#&gt; $Influential_Observations</span></span>
<span id="cb174-42"><a href="ordinary-least-squares.html#cb174-42" tabindex="-1"></a><span class="co">#&gt; Chrysler Imperial          Fiat 128    Toyota Corolla     Maserati Bora </span></span>
<span id="cb174-43"><a href="ordinary-least-squares.html#cb174-43" tabindex="-1"></a><span class="co">#&gt;                17                18                20                31</span></span></code></pre></div>
</div>
<div id="dfbetas" class="section level6 hasAnchor" number="5.1.2.5.3.3">
<h6><span class="header-section-number">5.1.2.5.3.3</span> DFBETAS<a href="ordinary-least-squares.html#dfbetas" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><strong>DFBETAS</strong> measures the <strong>influence of the</strong> <span class="math inline">\(i\)</span>th observation on each regression coefficient in a regression model. It is defined as:</p>
<p><span class="math display">\[
(DFBETAS)_{k(i)} = \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)}c_{kk}}}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(b_k\)</span> = regression coefficient for the <span class="math inline">\(k\)</span>th predictor using all observations.</li>
<li><span class="math inline">\(b_{k(i)}\)</span> = regression coefficient for the <span class="math inline">\(k\)</span>th predictor with the <span class="math inline">\(i\)</span>th observation omitted.</li>
<li><span class="math inline">\(MSE_{(i)}\)</span> = mean squared error with the <span class="math inline">\(i\)</span>th observation excluded.</li>
<li><span class="math inline">\(c_{kk}\)</span> = <span class="math inline">\(k\)</span>th diagonal element of <span class="math inline">\(\mathbf{X&#39;X}^{-1}\)</span>, representing the variance of <span class="math inline">\(b_k\)</span>.</li>
</ul>
<p>Key Insights</p>
<ul>
<li><strong>DFBETAS quantifies the change in each regression coefficient (</strong><span class="math inline">\(b_k\)</span>) caused by omitting the <span class="math inline">\(i\)</span>th observation.</li>
<li>The sign of DFBETAS indicates whether the inclusion of an observation increases or decreases the regression coefficient.
<ul>
<li>Positive DFBETAS: Inclusion increases <span class="math inline">\(b_k\)</span>.</li>
<li>Negative DFBETAS: Inclusion decreases <span class="math inline">\(b_k\)</span>.</li>
</ul></li>
<li>High DFBETAS indicate that a single observation disproportionately affects one or more predictors.</li>
</ul>
<p>The thresholds for identifying influential observations based on DFBETAS are:</p>
<ul>
<li><strong>Small data sets</strong>: <span class="math inline">\(|DFBETAS| &gt; 1\)</span></li>
<li><strong>Large data sets</strong>: <span class="math inline">\(|DFBETAS| &gt; 2 / \sqrt{n}\)</span></li>
</ul>
<p>Where <span class="math inline">\(n\)</span> is the total number of observations.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="ordinary-least-squares.html#cb175-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb175-2"><a href="ordinary-least-squares.html#cb175-2" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb175-3"><a href="ordinary-least-squares.html#cb175-3" tabindex="-1"></a></span>
<span id="cb175-4"><a href="ordinary-least-squares.html#cb175-4" tabindex="-1"></a><span class="co"># Fit a linear model</span></span>
<span id="cb175-5"><a href="ordinary-least-squares.html#cb175-5" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb175-6"><a href="ordinary-least-squares.html#cb175-6" tabindex="-1"></a></span>
<span id="cb175-7"><a href="ordinary-least-squares.html#cb175-7" tabindex="-1"></a><span class="co"># Compute DFBETAS</span></span>
<span id="cb175-8"><a href="ordinary-least-squares.html#cb175-8" tabindex="-1"></a>dfbetas_values <span class="ot">&lt;-</span> <span class="fu">dfbetas</span>(model)</span>
<span id="cb175-9"><a href="ordinary-least-squares.html#cb175-9" tabindex="-1"></a></span>
<span id="cb175-10"><a href="ordinary-least-squares.html#cb175-10" tabindex="-1"></a><span class="co"># Display influential observations based on the threshold for each predictor</span></span>
<span id="cb175-11"><a href="ordinary-least-squares.html#cb175-11" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">nrow</span>(mtcars))</span>
<span id="cb175-12"><a href="ordinary-least-squares.html#cb175-12" tabindex="-1"></a>influential_obs <span class="ot">&lt;-</span></span>
<span id="cb175-13"><a href="ordinary-least-squares.html#cb175-13" tabindex="-1"></a>    <span class="fu">apply</span>(dfbetas_values, <span class="dv">2</span>, <span class="cf">function</span>(x)</span>
<span id="cb175-14"><a href="ordinary-least-squares.html#cb175-14" tabindex="-1"></a>        <span class="fu">which</span>(<span class="fu">abs</span>(x) <span class="sc">&gt;</span> threshold))</span>
<span id="cb175-15"><a href="ordinary-least-squares.html#cb175-15" tabindex="-1"></a></span>
<span id="cb175-16"><a href="ordinary-least-squares.html#cb175-16" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb175-17"><a href="ordinary-least-squares.html#cb175-17" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb175-18"><a href="ordinary-least-squares.html#cb175-18" tabindex="-1"></a>    <span class="at">DFBETAS                  =</span> dfbetas_values,</span>
<span id="cb175-19"><a href="ordinary-least-squares.html#cb175-19" tabindex="-1"></a>    <span class="at">Threshold                =</span> threshold,</span>
<span id="cb175-20"><a href="ordinary-least-squares.html#cb175-20" tabindex="-1"></a>    <span class="at">Influential_Observations =</span> influential_obs</span>
<span id="cb175-21"><a href="ordinary-least-squares.html#cb175-21" tabindex="-1"></a>)</span>
<span id="cb175-22"><a href="ordinary-least-squares.html#cb175-22" tabindex="-1"></a><span class="co">#&gt; $DFBETAS</span></span>
<span id="cb175-23"><a href="ordinary-least-squares.html#cb175-23" tabindex="-1"></a><span class="co">#&gt;                      (Intercept)           hp            wt</span></span>
<span id="cb175-24"><a href="ordinary-least-squares.html#cb175-24" tabindex="-1"></a><span class="co">#&gt; Mazda RX4           -0.161347204  0.032966471  0.0639304305</span></span>
<span id="cb175-25"><a href="ordinary-least-squares.html#cb175-25" tabindex="-1"></a><span class="co">#&gt; Mazda RX4 Wag       -0.069324050  0.045785122 -0.0004066495</span></span>
<span id="cb175-26"><a href="ordinary-least-squares.html#cb175-26" tabindex="-1"></a><span class="co">#&gt; Datsun 710          -0.211199646  0.043374926  0.0972314374</span></span>
<span id="cb175-27"><a href="ordinary-least-squares.html#cb175-27" tabindex="-1"></a><span class="co">#&gt; Hornet 4 Drive       0.002672687 -0.006839301  0.0044886906</span></span>
<span id="cb175-28"><a href="ordinary-least-squares.html#cb175-28" tabindex="-1"></a><span class="co">#&gt; Hornet Sportabout    0.001784844  0.009208434 -0.0015536931</span></span>
<span id="cb175-29"><a href="ordinary-least-squares.html#cb175-29" tabindex="-1"></a><span class="co">#&gt; Valiant             -0.005985946  0.180374447 -0.1516565139</span></span>
<span id="cb175-30"><a href="ordinary-least-squares.html#cb175-30" tabindex="-1"></a><span class="co">#&gt; Duster 360           0.004705177 -0.159988770  0.0781031774</span></span>
<span id="cb175-31"><a href="ordinary-least-squares.html#cb175-31" tabindex="-1"></a><span class="co">#&gt; Merc 240D            0.034255292 -0.189552940  0.1224118752</span></span>
<span id="cb175-32"><a href="ordinary-least-squares.html#cb175-32" tabindex="-1"></a><span class="co">#&gt; Merc 230             0.019788247 -0.055075623  0.0332570461</span></span>
<span id="cb175-33"><a href="ordinary-least-squares.html#cb175-33" tabindex="-1"></a><span class="co">#&gt; Merc 280            -0.003198686  0.036709039 -0.0337297820</span></span>
<span id="cb175-34"><a href="ordinary-least-squares.html#cb175-34" tabindex="-1"></a><span class="co">#&gt; Merc 280C           -0.009045583  0.103809696 -0.0953846390</span></span>
<span id="cb175-35"><a href="ordinary-least-squares.html#cb175-35" tabindex="-1"></a><span class="co">#&gt; Merc 450SE          -0.026973686 -0.005712458  0.0356973740</span></span>
<span id="cb175-36"><a href="ordinary-least-squares.html#cb175-36" tabindex="-1"></a><span class="co">#&gt; Merc 450SL          -0.003961562  0.003399822  0.0049302300</span></span>
<span id="cb175-37"><a href="ordinary-least-squares.html#cb175-37" tabindex="-1"></a><span class="co">#&gt; Merc 450SLC          0.031572445 -0.016800308 -0.0400515832</span></span>
<span id="cb175-38"><a href="ordinary-least-squares.html#cb175-38" tabindex="-1"></a><span class="co">#&gt; Cadillac Fleetwood  -0.006420656 -0.002577897  0.0075499557</span></span>
<span id="cb175-39"><a href="ordinary-least-squares.html#cb175-39" tabindex="-1"></a><span class="co">#&gt; Lincoln Continental -0.168791258 -0.058242601  0.1903129995</span></span>
<span id="cb175-40"><a href="ordinary-least-squares.html#cb175-40" tabindex="-1"></a><span class="co">#&gt; Chrysler Imperial   -0.924056752 -0.148009806  0.9355996760</span></span>
<span id="cb175-41"><a href="ordinary-least-squares.html#cb175-41" tabindex="-1"></a><span class="co">#&gt; Fiat 128             0.605181396 -0.311246566 -0.1672758566</span></span>
<span id="cb175-42"><a href="ordinary-least-squares.html#cb175-42" tabindex="-1"></a><span class="co">#&gt; Honda Civic          0.156388333 -0.034026915 -0.0819144214</span></span>
<span id="cb175-43"><a href="ordinary-least-squares.html#cb175-43" tabindex="-1"></a><span class="co">#&gt; Toyota Corolla       0.804669969 -0.170934240 -0.4114605894</span></span>
<span id="cb175-44"><a href="ordinary-least-squares.html#cb175-44" tabindex="-1"></a><span class="co">#&gt; Toyota Corona       -0.231328587  0.066064464  0.0882138248</span></span>
<span id="cb175-45"><a href="ordinary-least-squares.html#cb175-45" tabindex="-1"></a><span class="co">#&gt; Dodge Challenger     0.003923967  0.049775308 -0.0888481611</span></span>
<span id="cb175-46"><a href="ordinary-least-squares.html#cb175-46" tabindex="-1"></a><span class="co">#&gt; AMC Javelin         -0.019610048  0.037837437 -0.0734203131</span></span>
<span id="cb175-47"><a href="ordinary-least-squares.html#cb175-47" tabindex="-1"></a><span class="co">#&gt; Camaro Z28           0.029920076 -0.128670440  0.0390740055</span></span>
<span id="cb175-48"><a href="ordinary-least-squares.html#cb175-48" tabindex="-1"></a><span class="co">#&gt; Pontiac Firebird    -0.058806962 -0.002278294  0.0868742949</span></span>
<span id="cb175-49"><a href="ordinary-least-squares.html#cb175-49" tabindex="-1"></a><span class="co">#&gt; Fiat X1-9           -0.037559007  0.010208853  0.0174261386</span></span>
<span id="cb175-50"><a href="ordinary-least-squares.html#cb175-50" tabindex="-1"></a><span class="co">#&gt; Porsche 914-2       -0.003655931  0.000316321  0.0020588013</span></span>
<span id="cb175-51"><a href="ordinary-least-squares.html#cb175-51" tabindex="-1"></a><span class="co">#&gt; Lotus Europa         0.423409344  0.188396749 -0.4072338373</span></span>
<span id="cb175-52"><a href="ordinary-least-squares.html#cb175-52" tabindex="-1"></a><span class="co">#&gt; Ford Pantera L      -0.022536462 -0.148176049  0.0999346699</span></span>
<span id="cb175-53"><a href="ordinary-least-squares.html#cb175-53" tabindex="-1"></a><span class="co">#&gt; Ferrari Dino        -0.065508308 -0.085182962  0.0869804902</span></span>
<span id="cb175-54"><a href="ordinary-least-squares.html#cb175-54" tabindex="-1"></a><span class="co">#&gt; Maserati Bora       -0.007482815  0.865763737 -0.4999048760</span></span>
<span id="cb175-55"><a href="ordinary-least-squares.html#cb175-55" tabindex="-1"></a><span class="co">#&gt; Volvo 142E          -0.080001907  0.038406565  0.0127537553</span></span>
<span id="cb175-56"><a href="ordinary-least-squares.html#cb175-56" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb175-57"><a href="ordinary-least-squares.html#cb175-57" tabindex="-1"></a><span class="co">#&gt; $Threshold</span></span>
<span id="cb175-58"><a href="ordinary-least-squares.html#cb175-58" tabindex="-1"></a><span class="co">#&gt; [1] 0.3535534</span></span>
<span id="cb175-59"><a href="ordinary-least-squares.html#cb175-59" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb175-60"><a href="ordinary-least-squares.html#cb175-60" tabindex="-1"></a><span class="co">#&gt; $Influential_Observations</span></span>
<span id="cb175-61"><a href="ordinary-least-squares.html#cb175-61" tabindex="-1"></a><span class="co">#&gt; $Influential_Observations$`(Intercept)`</span></span>
<span id="cb175-62"><a href="ordinary-least-squares.html#cb175-62" tabindex="-1"></a><span class="co">#&gt; Chrysler Imperial          Fiat 128    Toyota Corolla      Lotus Europa </span></span>
<span id="cb175-63"><a href="ordinary-least-squares.html#cb175-63" tabindex="-1"></a><span class="co">#&gt;                17                18                20                28 </span></span>
<span id="cb175-64"><a href="ordinary-least-squares.html#cb175-64" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb175-65"><a href="ordinary-least-squares.html#cb175-65" tabindex="-1"></a><span class="co">#&gt; $Influential_Observations$hp</span></span>
<span id="cb175-66"><a href="ordinary-least-squares.html#cb175-66" tabindex="-1"></a><span class="co">#&gt; Maserati Bora </span></span>
<span id="cb175-67"><a href="ordinary-least-squares.html#cb175-67" tabindex="-1"></a><span class="co">#&gt;            31 </span></span>
<span id="cb175-68"><a href="ordinary-least-squares.html#cb175-68" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb175-69"><a href="ordinary-least-squares.html#cb175-69" tabindex="-1"></a><span class="co">#&gt; $Influential_Observations$wt</span></span>
<span id="cb175-70"><a href="ordinary-least-squares.html#cb175-70" tabindex="-1"></a><span class="co">#&gt; Chrysler Imperial    Toyota Corolla      Lotus Europa     Maserati Bora </span></span>
<span id="cb175-71"><a href="ordinary-least-squares.html#cb175-71" tabindex="-1"></a><span class="co">#&gt;                17                20                28                31</span></span></code></pre></div>
</div>
</div>
<div id="collinearity" class="section level5 hasAnchor" number="5.1.2.5.4">
<h5><span class="header-section-number">5.1.2.5.4</span> Collinearity<a href="ordinary-least-squares.html#collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><strong>Collinearity</strong> (or multicollinearity) refers to the correlation among explanatory variables in a regression model. It can lead to various issues, including:</p>
<ul>
<li>Large changes in the estimated regression coefficients when a predictor variable is added or removed, or when observations are altered.</li>
<li>Non-significant results for individual tests on regression coefficients of important predictor variables.</li>
<li>Regression coefficients with signs opposite to theoretical expectations or prior experience.</li>
<li>Large coefficients of simple correlation between pairs of predictor variables in the correlation matrix.</li>
<li>Wide confidence intervals for the regression coefficients representing important predictor variables.</li>
</ul>
<p>When some <span class="math inline">\(X\)</span> variables are highly correlated, the inverse <span class="math inline">\((X&#39;X)^{-1}\)</span> either does not exist or is computationally unstable. This can result in:</p>
<ol style="list-style-type: decimal">
<li><strong>Non-interpretability of parameters</strong>: <span class="math display">\[\mathbf{b = (X&#39;X)^{-1}X&#39;y}\]</span></li>
<li><strong>Infinite sampling variability</strong>: <span class="math display">\[\mathbf{s^2(b) = MSE (X&#39;X)^{-1}}\]</span></li>
</ol>
<p>If some predictor variables (<span class="math inline">\(X\)</span>) are “perfectly” correlated, the system becomes undetermined, leading to an infinite number of models that fit the data. Specifically:</p>
<ul>
<li>If <span class="math inline">\(X&#39;X\)</span> is singular, then <span class="math inline">\((X&#39;X)^{-1}\)</span> does not exist.</li>
<li>This results in poor parameter estimation and invalid statistical inference.</li>
</ul>
<hr />
<div id="variance-inflation-factors-vifs" class="section level6 hasAnchor" number="5.1.2.5.4.1">
<h6><span class="header-section-number">5.1.2.5.4.1</span> Variance Inflation Factors (VIFs)<a href="ordinary-least-squares.html#variance-inflation-factors-vifs" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>The <strong>Variance Inflation Factor (VIF)</strong> quantifies how much the variance of a regression coefficient is inflated due to multicollinearity. It is defined as:</p>
<p><span class="math display">\[
VIF_k = \frac{1}{1-R^2_k}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(R^2_k\)</span> is the coefficient of multiple determination when <span class="math inline">\(X_k\)</span> is regressed on the other <span class="math inline">\(p-2\)</span> predictor variables in the model.</li>
</ul>
<hr />
<p><strong>Interpretation of VIFs</strong></p>
<ul>
<li>Large <span class="math inline">\(VIF_k\)</span> values indicate that near collinearity is inflating the variance of <span class="math inline">\(b_k\)</span>. The relationship is given by: <span class="math display">\[ var(b_k) \propto \sigma^2 (VIF_k) \]</span></li>
<li><strong>Thresholds</strong>:
<ul>
<li><span class="math inline">\(VIF &gt; 4\)</span>: Investigate the cause of multicollinearity.</li>
<li><span class="math inline">\(VIF_k &gt; 10\)</span>: Serious multicollinearity problem that can lead to poor parameter estimates.</li>
</ul></li>
<li>The <strong>mean VIF</strong> provides an estimate of the degree of multicollinearity:
<ul>
<li>If <span class="math inline">\(avg(VIF) &gt;&gt; 1\)</span>, serious multicollinearity is present.</li>
</ul></li>
<li><strong>Multicollinearity and VIF:</strong>
<ul>
<li><p>High VIFs with indicator variables are normal and not problematic.</p></li>
<li><p>VIF is generally not useful for detecting multicollinearity concerns in models with fixed effects.</p></li>
</ul></li>
<li><strong>Overemphasis on Multicollinearity:</strong>
<ul>
<li><p>Multicollinearity inflates standard errors and widens confidence intervals but does not bias results.</p></li>
<li><p>If key variables have narrow confidence intervals, multicollinearity is not an issue.</p></li>
</ul></li>
<li><strong>Goldberger’s Insight</strong> <span class="citation">(<a href="#ref-goldberger1991course">Goldberger 1991</a>)</span><strong>:</strong>
<ul>
<li><p>Multicollinearity is akin to small sample size (“micronumerosity”).</p></li>
<li><p>Large standard errors are expected with highly correlated independent variables.</p></li>
</ul></li>
<li><strong>Practical Implications:</strong>
<ul>
<li><p>Evaluate whether confidence intervals for key variables are sufficiently narrow.</p></li>
<li><p>If not, the study is inconclusive, and a larger dataset or redesigned study is needed.</p></li>
</ul></li>
</ul>
<hr />
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="ordinary-least-squares.html#cb176-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb176-2"><a href="ordinary-least-squares.html#cb176-2" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb176-3"><a href="ordinary-least-squares.html#cb176-3" tabindex="-1"></a></span>
<span id="cb176-4"><a href="ordinary-least-squares.html#cb176-4" tabindex="-1"></a><span class="co"># Fit a regression model</span></span>
<span id="cb176-5"><a href="ordinary-least-squares.html#cb176-5" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt <span class="sc">+</span> disp, <span class="at">data =</span> mtcars)</span>
<span id="cb176-6"><a href="ordinary-least-squares.html#cb176-6" tabindex="-1"></a></span>
<span id="cb176-7"><a href="ordinary-least-squares.html#cb176-7" tabindex="-1"></a><span class="co"># Compute Variance Inflation Factors</span></span>
<span id="cb176-8"><a href="ordinary-least-squares.html#cb176-8" tabindex="-1"></a>vif_values <span class="ot">&lt;-</span> <span class="fu">vif</span>(model)</span>
<span id="cb176-9"><a href="ordinary-least-squares.html#cb176-9" tabindex="-1"></a></span>
<span id="cb176-10"><a href="ordinary-least-squares.html#cb176-10" tabindex="-1"></a><span class="co"># Check for high multicollinearity</span></span>
<span id="cb176-11"><a href="ordinary-least-squares.html#cb176-11" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb176-12"><a href="ordinary-least-squares.html#cb176-12" tabindex="-1"></a>high_vif <span class="ot">&lt;-</span> <span class="fu">which</span>(vif_values <span class="sc">&gt;</span> threshold)</span>
<span id="cb176-13"><a href="ordinary-least-squares.html#cb176-13" tabindex="-1"></a></span>
<span id="cb176-14"><a href="ordinary-least-squares.html#cb176-14" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb176-15"><a href="ordinary-least-squares.html#cb176-15" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb176-16"><a href="ordinary-least-squares.html#cb176-16" tabindex="-1"></a>  <span class="at">VIFs =</span> vif_values,</span>
<span id="cb176-17"><a href="ordinary-least-squares.html#cb176-17" tabindex="-1"></a>  <span class="at">High_VIF_Threshold =</span> threshold,</span>
<span id="cb176-18"><a href="ordinary-least-squares.html#cb176-18" tabindex="-1"></a>  <span class="at">High_VIF_Indices =</span> high_vif</span>
<span id="cb176-19"><a href="ordinary-least-squares.html#cb176-19" tabindex="-1"></a>)</span>
<span id="cb176-20"><a href="ordinary-least-squares.html#cb176-20" tabindex="-1"></a><span class="co">#&gt; $VIFs</span></span>
<span id="cb176-21"><a href="ordinary-least-squares.html#cb176-21" tabindex="-1"></a><span class="co">#&gt;       hp       wt     disp </span></span>
<span id="cb176-22"><a href="ordinary-least-squares.html#cb176-22" tabindex="-1"></a><span class="co">#&gt; 2.736633 4.844618 7.324517 </span></span>
<span id="cb176-23"><a href="ordinary-least-squares.html#cb176-23" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb176-24"><a href="ordinary-least-squares.html#cb176-24" tabindex="-1"></a><span class="co">#&gt; $High_VIF_Threshold</span></span>
<span id="cb176-25"><a href="ordinary-least-squares.html#cb176-25" tabindex="-1"></a><span class="co">#&gt; [1] 10</span></span>
<span id="cb176-26"><a href="ordinary-least-squares.html#cb176-26" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb176-27"><a href="ordinary-least-squares.html#cb176-27" tabindex="-1"></a><span class="co">#&gt; $High_VIF_Indices</span></span>
<span id="cb176-28"><a href="ordinary-least-squares.html#cb176-28" tabindex="-1"></a><span class="co">#&gt; named integer(0)</span></span></code></pre></div>
</div>
<div id="condition-number" class="section level6 hasAnchor" number="5.1.2.5.4.2">
<h6><span class="header-section-number">5.1.2.5.4.2</span> Condition Number<a href="ordinary-least-squares.html#condition-number" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p><strong>Condition Number</strong> is a diagnostic measure for detecting multicollinearity, derived from the spectral decomposition of the matrix <span class="math inline">\(\mathbf{X&#39;X}\)</span>.</p>
<p>The spectral decomposition of <span class="math inline">\(\mathbf{X&#39;X}\)</span> is:</p>
<p><span class="math display">\[
\mathbf{X&#39;X}= \sum_{i=1}^{p} \lambda_i \mathbf{u_i u_i&#39;}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\lambda_i\)</span>: Eigenvalue associated with the <span class="math inline">\(i\)</span>th eigenvector.</li>
<li><span class="math inline">\(\mathbf{u}_i\)</span>: Eigenvector associated with <span class="math inline">\(\lambda_i\)</span>.</li>
<li><span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \dots &gt; \lambda_p\)</span> (ordered eigenvalues).</li>
<li>The eigenvectors are orthogonal: <span class="math display">\[
\begin{cases}
\mathbf{u_i&#39;u_j} = 0 &amp; \text{for } i \neq j \\
\mathbf{u_i&#39;u_j} = 1 &amp; \text{for } i = j
\end{cases}
\]</span></li>
</ul>
<hr />
<p>Definition of the Condition Number</p>
<p>The <strong>Condition Number</strong> is defined as:</p>
<p><span class="math display">\[
k = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\lambda_{\text{max}}\)</span>: Largest eigenvalue.</li>
<li><span class="math inline">\(\lambda_{\text{min}}\)</span>: Smallest eigenvalue.</li>
</ul>
<p>Interpretation</p>
<ul>
<li><span class="math inline">\(k &gt; 30\)</span>: Cause for concern.</li>
<li><span class="math inline">\(30 &lt; k &lt; 100\)</span>: Moderate dependencies among predictors.</li>
<li><span class="math inline">\(k &gt; 100\)</span>: Strong collinearity, indicating serious multicollinearity.</li>
</ul>
<hr />
<p>The <strong>Condition Index</strong> for the <span class="math inline">\(i\)</span>th eigenvalue is defined as:</p>
<p><span class="math display">\[
\delta_i = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_i}}
\]</span></p>
<p>Where <span class="math inline">\(i = 1, \dots, p\)</span>.</p>
<hr />
<p><strong>Variance proportions</strong> can be used to identify collinearity issues. The proportion of total variance associated with the <span class="math inline">\(k\)</span>th regression coefficient and the <span class="math inline">\(i\)</span>th eigen mode is given by:</p>
<p><span class="math display">\[
\frac{u_{ik}^2/\lambda_i}{\sum_j \left(u^2_{jk}/\lambda_j\right)}
\]</span></p>
<p><strong>Key Indicators</strong>:</p>
<ul>
<li>A <strong>large condition index</strong> <span class="math inline">\(\delta_i\)</span> suggests potential collinearity.</li>
<li>Variance proportions <span class="math inline">\(&gt; 0.5\)</span> for at least two regression coefficients indicate serious collinearity problems.</li>
</ul>
<hr />
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="ordinary-least-squares.html#cb177-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb177-2"><a href="ordinary-least-squares.html#cb177-2" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb177-3"><a href="ordinary-least-squares.html#cb177-3" tabindex="-1"></a></span>
<span id="cb177-4"><a href="ordinary-least-squares.html#cb177-4" tabindex="-1"></a><span class="co"># Fit a regression model</span></span>
<span id="cb177-5"><a href="ordinary-least-squares.html#cb177-5" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt <span class="sc">+</span> disp, <span class="at">data =</span> mtcars)</span>
<span id="cb177-6"><a href="ordinary-least-squares.html#cb177-6" tabindex="-1"></a></span>
<span id="cb177-7"><a href="ordinary-least-squares.html#cb177-7" tabindex="-1"></a><span class="co"># Compute eigenvalues and eigenvectors of the correlation matrix of predictors</span></span>
<span id="cb177-8"><a href="ordinary-least-squares.html#cb177-8" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(mtcars[, <span class="fu">c</span>(<span class="st">&quot;hp&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;disp&quot;</span>)])</span>
<span id="cb177-9"><a href="ordinary-least-squares.html#cb177-9" tabindex="-1"></a>eigen_decomp <span class="ot">&lt;-</span> <span class="fu">eigen</span>(cor_matrix)</span>
<span id="cb177-10"><a href="ordinary-least-squares.html#cb177-10" tabindex="-1"></a></span>
<span id="cb177-11"><a href="ordinary-least-squares.html#cb177-11" tabindex="-1"></a><span class="co"># Extract eigenvalues and compute the Condition Number</span></span>
<span id="cb177-12"><a href="ordinary-least-squares.html#cb177-12" tabindex="-1"></a>eigenvalues <span class="ot">&lt;-</span> eigen_decomp<span class="sc">$</span>values</span>
<span id="cb177-13"><a href="ordinary-least-squares.html#cb177-13" tabindex="-1"></a>condition_number <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">max</span>(eigenvalues) <span class="sc">/</span> <span class="fu">min</span>(eigenvalues))</span>
<span id="cb177-14"><a href="ordinary-least-squares.html#cb177-14" tabindex="-1"></a></span>
<span id="cb177-15"><a href="ordinary-least-squares.html#cb177-15" tabindex="-1"></a><span class="co"># Compute Condition Indices</span></span>
<span id="cb177-16"><a href="ordinary-least-squares.html#cb177-16" tabindex="-1"></a>condition_indices <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">max</span>(eigenvalues) <span class="sc">/</span> eigenvalues)</span>
<span id="cb177-17"><a href="ordinary-least-squares.html#cb177-17" tabindex="-1"></a></span>
<span id="cb177-18"><a href="ordinary-least-squares.html#cb177-18" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb177-19"><a href="ordinary-least-squares.html#cb177-19" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb177-20"><a href="ordinary-least-squares.html#cb177-20" tabindex="-1"></a>  <span class="at">Condition_Number =</span> condition_number,</span>
<span id="cb177-21"><a href="ordinary-least-squares.html#cb177-21" tabindex="-1"></a>  <span class="at">Condition_Indices =</span> condition_indices</span>
<span id="cb177-22"><a href="ordinary-least-squares.html#cb177-22" tabindex="-1"></a>)</span>
<span id="cb177-23"><a href="ordinary-least-squares.html#cb177-23" tabindex="-1"></a><span class="co">#&gt; $Condition_Number</span></span>
<span id="cb177-24"><a href="ordinary-least-squares.html#cb177-24" tabindex="-1"></a><span class="co">#&gt; [1] 5.469549</span></span>
<span id="cb177-25"><a href="ordinary-least-squares.html#cb177-25" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb177-26"><a href="ordinary-least-squares.html#cb177-26" tabindex="-1"></a><span class="co">#&gt; $Condition_Indices</span></span>
<span id="cb177-27"><a href="ordinary-least-squares.html#cb177-27" tabindex="-1"></a><span class="co">#&gt; [1] 1.000000 2.697266 5.469549</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li><strong>Condition Number</strong>: A single value indicating the degree of multicollinearity.</li>
<li><strong>Condition Indices</strong>: A vector showing the relative dependency associated with each eigenvalue.</li>
</ol>
</div>
</div>
<div id="constancy-of-error-variance" class="section level5 hasAnchor" number="5.1.2.5.5">
<h5><span class="header-section-number">5.1.2.5.5</span> Constancy of Error Variance<a href="ordinary-least-squares.html#constancy-of-error-variance" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Testing for the constancy of error variance (homoscedasticity) ensures that the assumptions of linear regression are met.</p>
<p>Below are two commonly used tests to assess error variance.</p>
<hr />
<div id="brown-forsythe-test-modified-levene-test" class="section level6 hasAnchor" number="5.1.2.5.5.1">
<h6><span class="header-section-number">5.1.2.5.5.1</span> Brown-Forsythe Test (Modified Levene Test)<a href="ordinary-least-squares.html#brown-forsythe-test-modified-levene-test" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>The <strong>Brown-Forsythe Test</strong> does not depend on the normality of errors and is suitable when error variance increases or decreases with <span class="math inline">\(X\)</span>.</p>
<p>Procedure</p>
<ol style="list-style-type: decimal">
<li><p>Split the residuals into two groups: <span class="math display">\[ e_{i1}, i = 1, \dots, n_1 \quad \text{and} \quad e_{j2}, j = 1, \dots, n_2 \]</span></p></li>
<li><p>Compute absolute deviations from the group medians: <span class="math display">\[ d_{i1} = |e_{i1} - \tilde{e}_1| \quad \text{and} \quad d_{j2} = |e_{j2} - \tilde{e}_2| \]</span> where <span class="math inline">\(\tilde{e}_1\)</span> and <span class="math inline">\(\tilde{e}_2\)</span> are the medians of groups 1 and 2, respectively.</p></li>
<li><p>Perform a two-sample t-test on <span class="math inline">\(d_{i1}\)</span> and <span class="math inline">\(d_{j2}\)</span>: <span class="math display">\[
t_L = \frac{\bar{d}_1 - \bar{d}_2}{s\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
\]</span> where <span class="math display">\[
s^2 = \frac{\sum_i (d_{i1} - \bar{d}_1)^2 + \sum_j (d_{j2} - \bar{d}_2)^2}{n - 2}
\]</span></p></li>
<li><p>Reject the null hypothesis of constant error variance if: <span class="math display">\[ |t_L| &gt; t_{1-\alpha/2; n-2} \]</span></p></li>
</ol>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="ordinary-least-squares.html#cb178-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb178-2"><a href="ordinary-least-squares.html#cb178-2" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb178-3"><a href="ordinary-least-squares.html#cb178-3" tabindex="-1"></a></span>
<span id="cb178-4"><a href="ordinary-least-squares.html#cb178-4" tabindex="-1"></a><span class="co"># Fit a linear model</span></span>
<span id="cb178-5"><a href="ordinary-least-squares.html#cb178-5" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb178-6"><a href="ordinary-least-squares.html#cb178-6" tabindex="-1"></a></span>
<span id="cb178-7"><a href="ordinary-least-squares.html#cb178-7" tabindex="-1"></a><span class="co"># Perform the Brown-Forsythe Test</span></span>
<span id="cb178-8"><a href="ordinary-least-squares.html#cb178-8" tabindex="-1"></a>levene_test <span class="ot">&lt;-</span> <span class="fu">leveneTest</span>(model<span class="sc">$</span>residuals <span class="sc">~</span> <span class="fu">cut</span>(mtcars<span class="sc">$</span>hp, <span class="dv">2</span>))  <span class="co"># Split HP into 2 groups</span></span>
<span id="cb178-9"><a href="ordinary-least-squares.html#cb178-9" tabindex="-1"></a></span>
<span id="cb178-10"><a href="ordinary-least-squares.html#cb178-10" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb178-11"><a href="ordinary-least-squares.html#cb178-11" tabindex="-1"></a>levene_test</span>
<span id="cb178-12"><a href="ordinary-least-squares.html#cb178-12" tabindex="-1"></a><span class="co">#&gt; Levene&#39;s Test for Homogeneity of Variance (center = median)</span></span>
<span id="cb178-13"><a href="ordinary-least-squares.html#cb178-13" tabindex="-1"></a><span class="co">#&gt;       Df F value Pr(&gt;F)</span></span>
<span id="cb178-14"><a href="ordinary-least-squares.html#cb178-14" tabindex="-1"></a><span class="co">#&gt; group  1  0.0851 0.7724</span></span>
<span id="cb178-15"><a href="ordinary-least-squares.html#cb178-15" tabindex="-1"></a><span class="co">#&gt;       30</span></span></code></pre></div>
<p>The p-value determines whether to reject the null hypothesis of constant variance.</p>
<hr />
</div>
<div id="breusch-pagan-test-cook-weisberg-test" class="section level6 hasAnchor" number="5.1.2.5.5.2">
<h6><span class="header-section-number">5.1.2.5.5.2</span> Breusch-Pagan Test (Cook-Weisberg Test)<a href="ordinary-least-squares.html#breusch-pagan-test-cook-weisberg-test" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>The <strong>Breusch-Pagan Test</strong> assumes independent and normally distributed errors. It tests the hypothesis:</p>
<ul>
<li><span class="math inline">\(H_0: \gamma_1 = 0\)</span> (Constant variance).</li>
<li><span class="math inline">\(H_1: \gamma_1 \neq 0\)</span> (Non-constant variance).</li>
</ul>
<p>Procedure</p>
<ol style="list-style-type: decimal">
<li><p>Assume the variance of the error terms is modeled as: <span class="math display">\[ \sigma^2_i = \gamma_0 + \gamma_1 X_i \]</span></p></li>
<li><p>Regress the squared residuals (<span class="math inline">\(e_i^2\)</span>) on <span class="math inline">\(X_i\)</span>:</p>
<ul>
<li>Obtain the <strong>regression sum of squares</strong> (<span class="math inline">\(SSR^*\)</span>).</li>
</ul></li>
<li><p>Compute the Breusch-Pagan statistic: <span class="math display">\[
X^2_{BP} = \frac{SSR^*/2}{\left(\frac{SSE}{n}\right)^2}
\]</span> where <span class="math inline">\(SSE\)</span> is the error sum of squares from the regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p></li>
<li><p>Compare <span class="math inline">\(X^2_{BP}\)</span> to the critical value from the <span class="math inline">\(\chi^2\)</span> distribution with 1 degree of freedom:</p>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> (homogeneous variance) if: <span class="math display">\[ X^2_{BP} &gt; \chi^2_{1-\alpha;1} \]</span></li>
</ul></li>
</ol>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="ordinary-least-squares.html#cb179-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb179-2"><a href="ordinary-least-squares.html#cb179-2" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb179-3"><a href="ordinary-least-squares.html#cb179-3" tabindex="-1"></a></span>
<span id="cb179-4"><a href="ordinary-least-squares.html#cb179-4" tabindex="-1"></a><span class="co"># Perform the Breusch-Pagan Test</span></span>
<span id="cb179-5"><a href="ordinary-least-squares.html#cb179-5" tabindex="-1"></a>bp_test <span class="ot">&lt;-</span> <span class="fu">bptest</span>(model)</span>
<span id="cb179-6"><a href="ordinary-least-squares.html#cb179-6" tabindex="-1"></a></span>
<span id="cb179-7"><a href="ordinary-least-squares.html#cb179-7" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb179-8"><a href="ordinary-least-squares.html#cb179-8" tabindex="-1"></a>bp_test</span>
<span id="cb179-9"><a href="ordinary-least-squares.html#cb179-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb179-10"><a href="ordinary-least-squares.html#cb179-10" tabindex="-1"></a><span class="co">#&gt;  studentized Breusch-Pagan test</span></span>
<span id="cb179-11"><a href="ordinary-least-squares.html#cb179-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb179-12"><a href="ordinary-least-squares.html#cb179-12" tabindex="-1"></a><span class="co">#&gt; data:  model</span></span>
<span id="cb179-13"><a href="ordinary-least-squares.html#cb179-13" tabindex="-1"></a><span class="co">#&gt; BP = 0.88072, df = 2, p-value = 0.6438</span></span></code></pre></div>
<p>If the p-value is below the chosen significance level (e.g., 0.05), reject $H_0$ and conclude non-constant variance.</p>
</div>
</div>
<div id="independence" class="section level5 hasAnchor" number="5.1.2.5.6">
<h5><span class="header-section-number">5.1.2.5.6</span> Independence<a href="ordinary-least-squares.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Testing for independence ensures that the residuals of a regression model are uncorrelated. Violation of this assumption may lead to biased or inefficient parameter estimates. Below, we discuss three primary methods for diagnosing dependence: plots, the Durbin-Watson test, and specific methods for time-series and spatial data.</p>
<hr />
<div id="plots" class="section level6 hasAnchor" number="5.1.2.5.6.1">
<h6><span class="header-section-number">5.1.2.5.6.1</span> Plots<a href="ordinary-least-squares.html#plots" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>A residual plot can help detect dependence in the residuals:</p>
<ol style="list-style-type: decimal">
<li>Plot residuals (<span class="math inline">\(e_i\)</span>) versus the predicted values (<span class="math inline">\(\hat{Y}_i\)</span>).</li>
<li>Patterns such as systematic waves, trends, or clusters indicate possible dependence.</li>
<li>Independence is suggested if residuals are randomly scattered without clear patterns.</li>
</ol>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="ordinary-least-squares.html#cb180-1" tabindex="-1"></a><span class="co"># Fit a regression model</span></span>
<span id="cb180-2"><a href="ordinary-least-squares.html#cb180-2" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb180-3"><a href="ordinary-least-squares.html#cb180-3" tabindex="-1"></a></span>
<span id="cb180-4"><a href="ordinary-least-squares.html#cb180-4" tabindex="-1"></a><span class="co"># Residual plot</span></span>
<span id="cb180-5"><a href="ordinary-least-squares.html#cb180-5" tabindex="-1"></a><span class="fu">plot</span>(model<span class="sc">$</span>fitted.values, model<span class="sc">$</span>residuals,</span>
<span id="cb180-6"><a href="ordinary-least-squares.html#cb180-6" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>,</span>
<span id="cb180-7"><a href="ordinary-least-squares.html#cb180-7" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb180-8"><a href="ordinary-least-squares.html#cb180-8" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Residual Plot&quot;</span>)</span>
<span id="cb180-9"><a href="ordinary-least-squares.html#cb180-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-14-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="durbin-watson-test" class="section level6 hasAnchor" number="5.1.2.5.6.2">
<h6><span class="header-section-number">5.1.2.5.6.2</span> Durbin-Watson Test<a href="ordinary-least-squares.html#durbin-watson-test" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>The <strong>Durbin-Watson test</strong> specifically detects autocorrelation in residuals, especially in time-series data.</p>
<ul>
<li><p><strong>Hypotheses</strong>:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Residuals are uncorrelated.</p></li>
<li><p><span class="math inline">\(H_1\)</span>: Residuals are autocorrelated.</p></li>
</ul></li>
<li><p>The Durbin-Watson statistic (<span class="math inline">\(d\)</span>) is calculated as: <span class="math inline">\(d = \frac{\sum_{t=2}^n (e_t - e_{t-1})^2}{\sum_{t=1}^n e_t^2}\)</span></p>
<ul>
<li><p><span class="math inline">\(d \approx 2\)</span>: No autocorrelation.</p></li>
<li><p><span class="math inline">\(d &lt; 2\)</span>: Positive autocorrelation.</p></li>
<li><p><span class="math inline">\(d &gt; 2\)</span>: Negative autocorrelation.</p></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="ordinary-least-squares.html#cb181-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb181-2"><a href="ordinary-least-squares.html#cb181-2" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb181-3"><a href="ordinary-least-squares.html#cb181-3" tabindex="-1"></a></span>
<span id="cb181-4"><a href="ordinary-least-squares.html#cb181-4" tabindex="-1"></a><span class="co"># Perform the Durbin-Watson test</span></span>
<span id="cb181-5"><a href="ordinary-least-squares.html#cb181-5" tabindex="-1"></a>dw_test <span class="ot">&lt;-</span> <span class="fu">dwtest</span>(model)</span>
<span id="cb181-6"><a href="ordinary-least-squares.html#cb181-6" tabindex="-1"></a></span>
<span id="cb181-7"><a href="ordinary-least-squares.html#cb181-7" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb181-8"><a href="ordinary-least-squares.html#cb181-8" tabindex="-1"></a>dw_test</span>
<span id="cb181-9"><a href="ordinary-least-squares.html#cb181-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb181-10"><a href="ordinary-least-squares.html#cb181-10" tabindex="-1"></a><span class="co">#&gt;  Durbin-Watson test</span></span>
<span id="cb181-11"><a href="ordinary-least-squares.html#cb181-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb181-12"><a href="ordinary-least-squares.html#cb181-12" tabindex="-1"></a><span class="co">#&gt; data:  model</span></span>
<span id="cb181-13"><a href="ordinary-least-squares.html#cb181-13" tabindex="-1"></a><span class="co">#&gt; DW = 1.3624, p-value = 0.02061</span></span>
<span id="cb181-14"><a href="ordinary-least-squares.html#cb181-14" tabindex="-1"></a><span class="co">#&gt; alternative hypothesis: true autocorrelation is greater than 0</span></span></code></pre></div>
</div>
<div id="time-series-autocorrelation" class="section level6 hasAnchor" number="5.1.2.5.6.3">
<h6><span class="header-section-number">5.1.2.5.6.3</span> Time-Series Autocorrelation<a href="ordinary-least-squares.html#time-series-autocorrelation" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>For time-series data, autocorrelation often occurs due to the temporal structure. Key diagnostics include:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Autocorrelation Function (ACF)</strong>:</p>
<ul>
<li><p>Shows the correlation of residuals with their lagged values.</p></li>
<li><p>Significant spikes in the ACF plot indicate autocorrelation.</p></li>
</ul></li>
<li><p><strong>Partial Autocorrelation Function (PACF)</strong>:</p>
<ul>
<li>Identifies the correlation of residuals after removing the influence of intermediate lags.</li>
</ul></li>
</ol>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="ordinary-least-squares.html#cb182-1" tabindex="-1"></a><span class="co"># Load necessary package</span></span>
<span id="cb182-2"><a href="ordinary-least-squares.html#cb182-2" tabindex="-1"></a><span class="fu">library</span>(forecast)</span>
<span id="cb182-3"><a href="ordinary-least-squares.html#cb182-3" tabindex="-1"></a></span>
<span id="cb182-4"><a href="ordinary-least-squares.html#cb182-4" tabindex="-1"></a><span class="co"># Residuals from a time-series model</span></span>
<span id="cb182-5"><a href="ordinary-least-squares.html#cb182-5" tabindex="-1"></a>time_series_res <span class="ot">&lt;-</span> <span class="fu">ts</span>(model<span class="sc">$</span>residuals)</span>
<span id="cb182-6"><a href="ordinary-least-squares.html#cb182-6" tabindex="-1"></a></span>
<span id="cb182-7"><a href="ordinary-least-squares.html#cb182-7" tabindex="-1"></a><span class="co"># Plot ACF and PACF</span></span>
<span id="cb182-8"><a href="ordinary-least-squares.html#cb182-8" tabindex="-1"></a><span class="fu">acf</span>(time_series_res, <span class="at">main =</span> <span class="st">&quot;ACF of Residuals&quot;</span>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-16-1.png" width="90%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="ordinary-least-squares.html#cb183-1" tabindex="-1"></a><span class="fu">pacf</span>(time_series_res, <span class="at">main =</span> <span class="st">&quot;PACF of Residuals&quot;</span>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-16-2.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="spatial-statistics" class="section level6 hasAnchor" number="5.1.2.5.6.4">
<h6><span class="header-section-number">5.1.2.5.6.4</span> Spatial Statistics<a href="ordinary-least-squares.html#spatial-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Spatial dependence occurs when residuals are correlated across geographical or spatial locations. Two primary tests are used to diagnose spatial autocorrelation:</p>
<hr />
<p><strong>Moran’s I</strong> measures global spatial autocorrelation. It determines whether similar values cluster spatially. The statistic is defined as:</p>
<p><span class="math display">\[
I = \frac{n}{W} \cdot \frac{\sum_i \sum_j w_{ij} (x_i - \bar{x})(x_j - \bar{x})}{\sum_i (x_i - \bar{x})^2}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(n\)</span>: Number of observations.</li>
<li><span class="math inline">\(W\)</span>: Sum of all spatial weights <span class="math inline">\(w_{ij}\)</span>.</li>
<li><span class="math inline">\(w_{ij}\)</span>: Spatial weight between locations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(x_i\)</span>: Residual value at location <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\bar{x}\)</span>: Mean of the residuals.</li>
</ul>
<p>Spatial Weight Matrix (<span class="math inline">\(W\)</span>): <span class="math inline">\(W\)</span> defines the spatial relationship between observations. It is often derived from:</p>
<ul>
<li><strong>Distance-based methods</strong>: E.g., k-nearest neighbors or distance bands.</li>
<li><strong>Adjacency methods</strong>: Based on shared boundaries in geographic data.</li>
</ul>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><p><span class="math inline">\(I &gt; 0\)</span>: Positive spatial autocorrelation (similar values cluster together).</p></li>
<li><p><span class="math inline">\(I &lt; 0\)</span>: Negative spatial autocorrelation (dissimilar values are neighbors).</p></li>
<li><p><span class="math inline">\(I \approx 0\)</span>: Random spatial distribution.</p></li>
</ul>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="ordinary-least-squares.html#cb184-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb184-2"><a href="ordinary-least-squares.html#cb184-2" tabindex="-1"></a><span class="fu">library</span>(sf)</span>
<span id="cb184-3"><a href="ordinary-least-squares.html#cb184-3" tabindex="-1"></a><span class="fu">library</span>(spdep)</span>
<span id="cb184-4"><a href="ordinary-least-squares.html#cb184-4" tabindex="-1"></a></span>
<span id="cb184-5"><a href="ordinary-least-squares.html#cb184-5" tabindex="-1"></a><span class="co"># Simulate spatial data (example with mtcars dataset)</span></span>
<span id="cb184-6"><a href="ordinary-least-squares.html#cb184-6" tabindex="-1"></a>coords <span class="ot">&lt;-</span> <span class="fu">cbind</span>(mtcars<span class="sc">$</span>hp, mtcars<span class="sc">$</span>wt)  <span class="co"># Coordinates based on two variables</span></span>
<span id="cb184-7"><a href="ordinary-least-squares.html#cb184-7" tabindex="-1"></a></span>
<span id="cb184-8"><a href="ordinary-least-squares.html#cb184-8" tabindex="-1"></a><span class="co"># Add small jitter to avoid duplicate coordinates</span></span>
<span id="cb184-9"><a href="ordinary-least-squares.html#cb184-9" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb184-10"><a href="ordinary-least-squares.html#cb184-10" tabindex="-1"></a>coords_jittered <span class="ot">&lt;-</span></span>
<span id="cb184-11"><a href="ordinary-least-squares.html#cb184-11" tabindex="-1"></a>    coords <span class="sc">+</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="fu">length</span>(coords),<span class="sc">-</span><span class="fl">0.01</span>, <span class="fl">0.01</span>), <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb184-12"><a href="ordinary-least-squares.html#cb184-12" tabindex="-1"></a></span>
<span id="cb184-13"><a href="ordinary-least-squares.html#cb184-13" tabindex="-1"></a><span class="co"># Find nearest neighbors</span></span>
<span id="cb184-14"><a href="ordinary-least-squares.html#cb184-14" tabindex="-1"></a>neighbors <span class="ot">&lt;-</span> <span class="fu">knn2nb</span>(<span class="fu">knearneigh</span>(coords_jittered, <span class="at">k =</span> <span class="dv">3</span>))</span>
<span id="cb184-15"><a href="ordinary-least-squares.html#cb184-15" tabindex="-1"></a></span>
<span id="cb184-16"><a href="ordinary-least-squares.html#cb184-16" tabindex="-1"></a><span class="co"># Create spatial weights matrix</span></span>
<span id="cb184-17"><a href="ordinary-least-squares.html#cb184-17" tabindex="-1"></a>weights <span class="ot">&lt;-</span> <span class="fu">nb2listw</span>(neighbors, <span class="at">style =</span> <span class="st">&quot;W&quot;</span>)  <span class="co"># Row-standardized weights</span></span>
<span id="cb184-18"><a href="ordinary-least-squares.html#cb184-18" tabindex="-1"></a></span>
<span id="cb184-19"><a href="ordinary-least-squares.html#cb184-19" tabindex="-1"></a><span class="co"># Fit the linear model</span></span>
<span id="cb184-20"><a href="ordinary-least-squares.html#cb184-20" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp <span class="sc">+</span> wt, <span class="at">data =</span> mtcars)</span>
<span id="cb184-21"><a href="ordinary-least-squares.html#cb184-21" tabindex="-1"></a></span>
<span id="cb184-22"><a href="ordinary-least-squares.html#cb184-22" tabindex="-1"></a><span class="co"># Check lengths of residuals and weights</span></span>
<span id="cb184-23"><a href="ordinary-least-squares.html#cb184-23" tabindex="-1"></a><span class="fu">length</span>(model<span class="sc">$</span>residuals)  <span class="co"># Should be 32</span></span>
<span id="cb184-24"><a href="ordinary-least-squares.html#cb184-24" tabindex="-1"></a><span class="co">#&gt; [1] 32</span></span>
<span id="cb184-25"><a href="ordinary-least-squares.html#cb184-25" tabindex="-1"></a><span class="fu">length</span>(weights<span class="sc">$</span>neighbours)  <span class="co"># Should also be 32</span></span>
<span id="cb184-26"><a href="ordinary-least-squares.html#cb184-26" tabindex="-1"></a><span class="co">#&gt; [1] 32</span></span>
<span id="cb184-27"><a href="ordinary-least-squares.html#cb184-27" tabindex="-1"></a></span>
<span id="cb184-28"><a href="ordinary-least-squares.html#cb184-28" tabindex="-1"></a><span class="co"># Compute Moran&#39;s I for residuals</span></span>
<span id="cb184-29"><a href="ordinary-least-squares.html#cb184-29" tabindex="-1"></a>moran_test <span class="ot">&lt;-</span> <span class="fu">moran.test</span>(model<span class="sc">$</span>residuals, weights)</span>
<span id="cb184-30"><a href="ordinary-least-squares.html#cb184-30" tabindex="-1"></a><span class="fu">print</span>(moran_test)</span>
<span id="cb184-31"><a href="ordinary-least-squares.html#cb184-31" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb184-32"><a href="ordinary-least-squares.html#cb184-32" tabindex="-1"></a><span class="co">#&gt;  Moran I test under randomisation</span></span>
<span id="cb184-33"><a href="ordinary-least-squares.html#cb184-33" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb184-34"><a href="ordinary-least-squares.html#cb184-34" tabindex="-1"></a><span class="co">#&gt; data:  model$residuals  </span></span>
<span id="cb184-35"><a href="ordinary-least-squares.html#cb184-35" tabindex="-1"></a><span class="co">#&gt; weights: weights    </span></span>
<span id="cb184-36"><a href="ordinary-least-squares.html#cb184-36" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb184-37"><a href="ordinary-least-squares.html#cb184-37" tabindex="-1"></a><span class="co">#&gt; Moran I statistic standard deviate = 1.7679, p-value = 0.03854</span></span>
<span id="cb184-38"><a href="ordinary-least-squares.html#cb184-38" tabindex="-1"></a><span class="co">#&gt; alternative hypothesis: greater</span></span>
<span id="cb184-39"><a href="ordinary-least-squares.html#cb184-39" tabindex="-1"></a><span class="co">#&gt; sample estimates:</span></span>
<span id="cb184-40"><a href="ordinary-least-squares.html#cb184-40" tabindex="-1"></a><span class="co">#&gt; Moran I statistic       Expectation          Variance </span></span>
<span id="cb184-41"><a href="ordinary-least-squares.html#cb184-41" tabindex="-1"></a><span class="co">#&gt;        0.18544790       -0.03225806        0.01516371</span></span>
<span id="cb184-42"><a href="ordinary-least-squares.html#cb184-42" tabindex="-1"></a></span>
<span id="cb184-43"><a href="ordinary-least-squares.html#cb184-43" tabindex="-1"></a><span class="co"># Moran&#39;s scatterplot</span></span>
<span id="cb184-44"><a href="ordinary-least-squares.html#cb184-44" tabindex="-1"></a><span class="fu">moran.plot</span>(model<span class="sc">$</span>residuals, weights, <span class="at">main =</span> <span class="st">&quot;Moran&#39;s Scatterplot&quot;</span>)</span></code></pre></div>
<p><img src="05-linear-regression_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Significant Moran’s I: Indicates global clustering of similar residuals, suggesting spatial dependence.</p>
<hr />
<p><strong>Geary’s C</strong> measures spatial autocorrelation at a local level, emphasizing differences between neighboring observations. The statistic is defined as:</p>
<p><span class="math display">\[
C = \frac{n - 1}{2W} \cdot \frac{\sum_i \sum_j w_{ij} (x_i - x_j)^2}{\sum_i (x_i - \bar{x})^2}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(C\)</span> ranges from 0 to 2:
<ul>
<li><span class="math inline">\(C \approx 0\)</span>: High positive spatial autocorrelation.</li>
<li><span class="math inline">\(C \approx 1\)</span>: Spatial randomness.</li>
<li><span class="math inline">\(C \approx 2\)</span>: High negative spatial autocorrelation.</li>
</ul></li>
</ul>
<p><strong>Comparison of Moran’s I and Geary’s C</strong>:</p>
<ul>
<li><p>Moran’s I is more global and measures the overall pattern of autocorrelation.</p></li>
<li><p>Geary’s C is more sensitive to local spatial autocorrelation, detecting small-scale patterns.</p></li>
</ul>
<hr />
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="ordinary-least-squares.html#cb185-1" tabindex="-1"></a><span class="co"># Compute Geary&#39;s C for residuals</span></span>
<span id="cb185-2"><a href="ordinary-least-squares.html#cb185-2" tabindex="-1"></a>geary_test <span class="ot">&lt;-</span> <span class="fu">geary.test</span>(model<span class="sc">$</span>residuals, weights)</span>
<span id="cb185-3"><a href="ordinary-least-squares.html#cb185-3" tabindex="-1"></a></span>
<span id="cb185-4"><a href="ordinary-least-squares.html#cb185-4" tabindex="-1"></a><span class="co"># Results</span></span>
<span id="cb185-5"><a href="ordinary-least-squares.html#cb185-5" tabindex="-1"></a>geary_test</span>
<span id="cb185-6"><a href="ordinary-least-squares.html#cb185-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb185-7"><a href="ordinary-least-squares.html#cb185-7" tabindex="-1"></a><span class="co">#&gt;  Geary C test under randomisation</span></span>
<span id="cb185-8"><a href="ordinary-least-squares.html#cb185-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb185-9"><a href="ordinary-least-squares.html#cb185-9" tabindex="-1"></a><span class="co">#&gt; data:  model$residuals </span></span>
<span id="cb185-10"><a href="ordinary-least-squares.html#cb185-10" tabindex="-1"></a><span class="co">#&gt; weights: weights   </span></span>
<span id="cb185-11"><a href="ordinary-least-squares.html#cb185-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb185-12"><a href="ordinary-least-squares.html#cb185-12" tabindex="-1"></a><span class="co">#&gt; Geary C statistic standard deviate = 1.0606, p-value = 0.1444</span></span>
<span id="cb185-13"><a href="ordinary-least-squares.html#cb185-13" tabindex="-1"></a><span class="co">#&gt; alternative hypothesis: Expectation greater than statistic</span></span>
<span id="cb185-14"><a href="ordinary-least-squares.html#cb185-14" tabindex="-1"></a><span class="co">#&gt; sample estimates:</span></span>
<span id="cb185-15"><a href="ordinary-least-squares.html#cb185-15" tabindex="-1"></a><span class="co">#&gt; Geary C statistic       Expectation          Variance </span></span>
<span id="cb185-16"><a href="ordinary-least-squares.html#cb185-16" tabindex="-1"></a><span class="co">#&gt;        0.84515868        1.00000000        0.02131399</span></span></code></pre></div>
<p>Significant Geary’s C: Highlights local spatial autocorrelation, useful for identifying specific regions or groups of observations where dependence is strong.</p>
</div>
</div>
</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-chen2024logs" class="csl-entry">
Chen, Jiafeng, and Jonathan Roth. 2024. <span>“Logs with Zeros? Some Problems and Solutions.”</span> <em>The Quarterly Journal of Economics</em> 139 (2): 891–936.
</div>
<div id="ref-goldberger1991course" class="csl-entry">
Goldberger, Arthur Stanley. 1991. <em>A Course in Econometrics</em>. Harvard University Press.
</div>
<div id="ref-greene1990gamma" class="csl-entry">
Greene, William H. 1990. <span>“A Gamma-Distributed Stochastic Frontier Model.”</span> <em>Journal of Econometrics</em> 46 (1-2): 141–63.
</div>
<div id="ref-lovell2008simple" class="csl-entry">
Lovell, Michael C. 2008. <span>“A Simple Proof of the FWL Theorem.”</span> <em>The Journal of Economic Education</em> 39 (1): 88–91.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-least-squares.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/mikenguyen13/data_analysis/edit/main/05-linear-regression.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/mikenguyen13/data_analysis/blob/main/05-linear-regression.Rmd",
    "text": null
  },
  "download": ["data_analysis.pdf", "data_analysis.epub", "data_analysis.mobi"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true,
    "sharing": {
      "facebook": true,
      "github": true,
      "twitter": true,
      "linkedin": true
    },
    "info": true,
    "edit": "https://github.com/mikenguyen13/data_analysis/edit/main/%s"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
