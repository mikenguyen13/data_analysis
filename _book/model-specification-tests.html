<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 14 Model Specification Tests | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Model specification tests are critical in econometric analysis to verify whether the assumptions underlying a model hold true. These tests help determine if the model is correctly specified,...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="Chapter 14 Model Specification Tests | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/model-specification-tests.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Model specification tests are critical in econometric analysis to verify whether the assumptions underlying a model hold true. These tests help determine if the model is correctly specified,...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 14 Model Specification Tests | A Guide on Data Analysis">
<meta name="twitter:description" content="Model specification tests are critical in econometric analysis to verify whether the assumptions underlying a model hold true. These tests help determine if the model is correctly specified,...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="active" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">39</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="model-specification-tests" class="section level1" number="14">
<h1>
<span class="header-section-number">14</span> Model Specification Tests<a class="anchor" aria-label="anchor" href="#model-specification-tests"><i class="fas fa-link"></i></a>
</h1>
<p>Model specification tests are critical in econometric analysis to verify whether the assumptions underlying a model hold true. These tests help determine if the model is correctly specified, ensuring that the estimators are both reliable and efficient. A mis-specified model can lead to biased, inconsistent, or inefficient estimates, which undermines the validity of inferences drawn from the analysis.</p>
<p>This chapter addresses various model specification tests, including tests for:</p>
<ul>
<li><a href="model-specification-tests.html#nested-model-tests">Nested Model Tests</a></li>
<li><a href="model-specification-tests.html#non-nested-model-tests">Non-Nested Model Tests</a></li>
<li><a href="model-specification-tests.html#heteroskedasticity-tests">Heteroskedasticity Tests</a></li>
<li><a href="model-specification-tests.html#functional-form-tests">Functional Form Tests</a></li>
<li><a href="model-specification-tests.html#autocorrelation-tests">Autocorrelation Tests</a></li>
<li><a href="model-specification-tests.html#multicollinearity-diagnostics">Multicollinearity Diagnostics</a></li>
</ul>
<p>Understanding these tests allows researchers to evaluate the robustness of their models and make necessary adjustments to improve model performance.</p>
<hr>
<div id="nested-model-tests" class="section level2" number="14.1">
<h2>
<span class="header-section-number">14.1</span> Nested Model Tests<a class="anchor" aria-label="anchor" href="#nested-model-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Nested models are those where the <strong>restricted model</strong> is a special case of the <strong>unrestricted model</strong>. In other words, the restricted model can be derived from the unrestricted model by imposing constraints on certain parameters, typically setting them equal to zero. This structure allows us to formally test whether the additional variables in the unrestricted model significantly improve the model’s explanatory power. The following tests help compare these models:</p>
<ul>
<li>
<a href="model-specification-tests.html#sec-wald-test-nested">Wald Test</a>: Assesses the significance of individual coefficients or groups of coefficients.</li>
<li>
<a href="model-specification-tests.html#sec-likelihood-ratio-test-nested">Likelihood Ratio Test</a>: Compares the goodness-of-fit between restricted and unrestricted models.</li>
<li>
<a href="model-specification-tests.html#sec-f-test-for-linear-regression-nested">F-Test</a>: Evaluates the joint significance of multiple coefficients.</li>
<li>
<a href="model-specification-tests.html#sec-chow-test">Chow Test</a>: Evaluates whether the coefficients of a regression model are the same across different groups or time periods.</li>
</ul>
<p>Consider the following models:</p>
<p><span class="math display">\[
\begin{aligned}
\textbf{Unrestricted Model:} \quad &amp; y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon \\
\textbf{Restricted Model:} \quad &amp; y = \beta_0 + \beta_1 x_1 + \epsilon
\end{aligned}
\]</span></p>
<ul>
<li>The <strong>unrestricted model</strong> includes all potential explanatory variables: <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>.</li>
<li>The <strong>restricted model</strong> is nested within the unrestricted model, containing a subset of variables (in this case, excluding <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>).</li>
</ul>
<p>Our goal is to test the null hypothesis that the restrictions are valid:</p>
<p><span class="math display">\[
H_0: \beta_2 = \beta_3 = 0 \quad \text{(restrictions are valid)}
\]</span></p>
<p>against the alternative hypothesis:</p>
<p><span class="math display">\[
H_1: \text{At least one of } \beta_2, \beta_3 \neq 0 \quad \text{(restrictions are invalid)}
\]</span></p>
<p>To conduct this test, we use the following methods:</p>
<hr>
<div id="sec-wald-test-nested" class="section level3" number="14.1.1">
<h3>
<span class="header-section-number">14.1.1</span> Wald Test<a class="anchor" aria-label="anchor" href="#sec-wald-test-nested"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Wald Test</strong> assesses whether certain linear restrictions on the parameters of the model are valid. It is commonly used when testing the joint significance of multiple coefficients.</p>
<p>Consider a set of linear restrictions expressed as:</p>
<p><span class="math display">\[
H_0: R\boldsymbol{\beta} = r
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(R\)</span> is a <span class="math inline">\(q \times k\)</span> restriction matrix,</p></li>
<li><p><span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span class="math inline">\(k \times 1\)</span> vector of parameters,</p></li>
<li><p><span class="math inline">\(r\)</span> is a <span class="math inline">\(q \times 1\)</span> vector representing the hypothesized values (often zeros),</p></li>
<li><p><span class="math inline">\(q\)</span> is the number of restrictions being tested.</p></li>
</ul>
<p>For example, if we want to test <span class="math inline">\(H_0: \beta_2 = \beta_3 = 0\)</span>, then:</p>
<p><span class="math display">\[
R = \begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}, \quad r = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
\]</span></p>
<p>The Wald statistic is calculated as:</p>
<p><span class="math display">\[
W = (R\hat{\boldsymbol{\beta}} - r)' \left[ R \, \hat{\text{Var}}(\hat{\boldsymbol{\beta}}) \, R' \right]^{-1} (R\hat{\boldsymbol{\beta}} - r)
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the vector of estimated coefficients from the unrestricted model,</p></li>
<li><p><span class="math inline">\(\hat{\text{Var}}(\hat{\boldsymbol{\beta}})\)</span> is the estimated covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p></li>
</ul>
<p>Distribution and Decision Rule</p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span>, the Wald statistic follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(q\)</span> degrees of freedom:</li>
</ul>
<p><span class="math display">\[
W \sim \chi^2_q
\]</span></p>
<ul>
<li>
<strong>Decision Rule:</strong>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(W &gt; \chi^2_{q,\alpha}\)</span>, where <span class="math inline">\(\alpha\)</span> is the significance level.</li>
<li>A large Wald statistic indicates that the restrictions are invalid.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="sec-likelihood-ratio-test-nested" class="section level3" number="14.1.2">
<h3>
<span class="header-section-number">14.1.2</span> Likelihood Ratio Test<a class="anchor" aria-label="anchor" href="#sec-likelihood-ratio-test-nested"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Likelihood Ratio Test</strong> compares the goodness-of-fit between the restricted and unrestricted models. It evaluates whether the additional parameters in the unrestricted model significantly improve the likelihood of observing the data.</p>
<p>Same as before:</p>
<p><span class="math display">\[
H_0: \beta_2 = \beta_3 = 0 \quad \text{vs.} \quad H_1: \text{At least one of } \beta_2, \beta_3 \neq 0
\]</span></p>
<p>The LR statistic is calculated as:</p>
<p><span class="math display">\[
LR = -2 \left( \ln L_R - \ln L_U \right)
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(L_R\)</span> is the maximized likelihood of the restricted model,</p></li>
<li><p><span class="math inline">\(L_U\)</span> is the maximized likelihood of the unrestricted model.</p></li>
</ul>
<p>Distribution and Decision Rule</p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span>, the LR statistic follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(q\)</span> degrees of freedom:</li>
</ul>
<p><span class="math display">\[
LR \sim \chi^2_q
\]</span></p>
<ul>
<li>
<strong>Decision Rule:</strong>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(LR &gt; \chi^2_{q,\alpha}\)</span>.</li>
<li>A large LR statistic suggests that the unrestricted model provides a significantly better fit.</li>
</ul>
</li>
</ul>
<p><strong>Connection to <a href="linear-regression.html#ordinary-least-squares">OLS</a></strong></p>
<p>In the case of linear regression with normally distributed errors, the LR statistic can be expressed in terms of the sum of squared residuals (SSR):</p>
<p><span class="math display">\[
LR = n \ln \left( \frac{SSR_R}{SSR_U} \right)
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the sample size.</p>
<hr>
</div>
<div id="sec-f-test-for-linear-regression-nested" class="section level3" number="14.1.3">
<h3>
<span class="header-section-number">14.1.3</span> F-Test (for Linear Regression)<a class="anchor" aria-label="anchor" href="#sec-f-test-for-linear-regression-nested"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>F-Test</strong> is commonly used in linear regression to evaluate the joint significance of multiple coefficients. It compares the fit of the restricted and unrestricted models using their sum of squared residuals.</p>
<p>Again:</p>
<p><span class="math display">\[
H_0: \beta_2 = \beta_3 = 0 \quad \text{vs.} \quad H_1: \text{At least one of } \beta_2, \beta_3 \neq 0
\]</span></p>
<p>The F-statistic is calculated as:</p>
<p><span class="math display">\[
F = \frac{(SSR_R - SSR_U) / q}{SSR_U / (n - k)}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(SSR_R\)</span> = Sum of Squared Residuals from the restricted model,</li>
<li>
<span class="math inline">\(SSR_U\)</span> = Sum of Squared Residuals from the unrestricted model,</li>
<li>
<span class="math inline">\(q\)</span> = Number of restrictions (here, 2),</li>
<li>
<span class="math inline">\(n\)</span> = Sample size,</li>
<li>
<span class="math inline">\(k\)</span> = Number of parameters in the unrestricted model.</li>
</ul>
<p>Distribution and Decision Rule</p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span>, the F-statistic follows an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\((q, n - k)\)</span> degrees of freedom:</li>
</ul>
<p><span class="math display">\[
F \sim F_{q, n - k}
\]</span></p>
<ul>
<li>
<strong>Decision Rule:</strong>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F &gt; F_{q, n - k, \alpha}\)</span>.</li>
<li>A large F-statistic indicates that the restricted model fits significantly worse, suggesting the excluded variables are important.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="sec-chow-test" class="section level3" number="14.1.4">
<h3>
<span class="header-section-number">14.1.4</span> Chow Test<a class="anchor" aria-label="anchor" href="#sec-chow-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Chow Test</strong> evaluates whether the coefficients of a regression model are the same across different groups or time periods. It is often used to detect <strong>structural breaks</strong> in the data.</p>
<p><strong>Key Question:</strong><br><em>Should we run two different regressions for two groups, or can we pool the data and use a single regression?</em></p>
<p><strong>Chow Test Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>Estimate the regression model for the <strong>pooled data</strong> (all observations).</li>
<li>Estimate the model separately for <strong>Group 1</strong> and <strong>Group 2</strong>.</li>
<li>Compare the <strong>sum of squared residuals (SSR)</strong> from these models.</li>
</ol>
<p>The test statistic follows an F-distribution:</p>
<p><span class="math display">\[
F = \frac{(SSR_P - (SSR_1 + SSR_2)) / k}{(SSR_1 + SSR_2) / (n_1 + n_2 - 2k)}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(SSR_P\)</span> = Sum of Squared Residuals for the pooled model</li>
<li>
<span class="math inline">\(SSR_1\)</span>, <span class="math inline">\(SSR_2\)</span> = SSRs for Group 1 and Group 2 models</li>
<li>
<span class="math inline">\(k\)</span> = Number of parameters</li>
<li>
<span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2\)</span> = Number of observations in each group</li>
</ul>
<p><strong>Interpretation:</strong></p>
<ul>
<li><p>A <strong>significant F-statistic</strong> suggests structural differences between groups, implying separate regressions are more appropriate.</p></li>
<li><p>A <strong>non-significant F-statistic</strong> indicates no structural break, supporting the use of a pooled model.</p></li>
</ul>
<div class="sourceCode" id="cb541"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span>        <span class="co"># For Wald Test</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span>     <span class="co"># For Likelihood Ratio Test</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">strucchange</span><span class="op">)</span>  <span class="co"># For Chow Test</span></span>
<span></span>
<span><span class="co"># Simulated dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">x3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">x2</span> <span class="op">-</span> <span class="fl">0.7</span> <span class="op">*</span> <span class="va">x3</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span></span>
<span><span class="co"># Creating a group variable (simulating a structural break)</span></span>
<span><span class="va">group</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, each <span class="op">=</span> <span class="va">n</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># Group 0 and Group 1</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Wald Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="va">unrestricted_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span><span class="op">)</span>    <span class="co"># Unrestricted model</span></span>
<span><span class="va">restricted_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span><span class="op">)</span>                <span class="co"># Restricted model</span></span>
<span></span>
<span><span class="va">wald_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">unrestricted_model</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x2 = 0"</span>, <span class="st">"x3 = 0"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">wald_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Linear hypothesis test:</span></span>
<span><span class="co">#&gt; x2 = 0</span></span>
<span><span class="co">#&gt; x3 = 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 1: restricted model</span></span>
<span><span class="co">#&gt; Model 2: y ~ x1 + x2 + x3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; 1     98 182.26                                  </span></span>
<span><span class="co">#&gt; 2     96 106.14  2    76.117 34.421 5.368e-12 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Likelihood Ratio Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="va">lr_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/lrtest.html">lrtest</a></span><span class="op">(</span><span class="va">unrestricted_model</span>, <span class="va">restricted_model</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">lr_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; Likelihood ratio test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 1: y ~ x1 + x2 + x3</span></span>
<span><span class="co">#&gt; Model 2: y ~ x1</span></span>
<span><span class="co">#&gt;   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    </span></span>
<span><span class="co">#&gt; 1   5 -144.88                         </span></span>
<span><span class="co">#&gt; 2   3 -171.91 -2 54.064  1.821e-12 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># F-Test (for Linear Regression)</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="va">SSR_U</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">unrestricted_model</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>  <span class="co"># SSR for unrestricted model</span></span>
<span><span class="va">SSR_R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">restricted_model</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>    <span class="co"># SSR for restricted model</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">2</span>                                        <span class="co"># Number of restrictions</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>                                <span class="co"># Sample size</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">unrestricted_model</span><span class="op">)</span><span class="op">)</span>         <span class="co"># Number of parameters in unrestricted model</span></span>
<span></span>
<span><span class="co"># F-statistic formula</span></span>
<span><span class="va">F_statistic</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="op">(</span><span class="va">SSR_R</span> <span class="op">-</span> <span class="va">SSR_U</span><span class="op">)</span> <span class="op">/</span> <span class="va">q</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">SSR_U</span> <span class="op">/</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">k</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">p_value_F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">pf</a></span><span class="op">(</span><span class="va">F_statistic</span>, df1 <span class="op">=</span> <span class="va">q</span>, df2 <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="va">k</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"F-statistic:"</span>, <span class="va">F_statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; F-statistic: 34.42083</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"P-value:"</span>, <span class="va">p_value_F</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; P-value: 5.367912e-12</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Chow Test (Proper Use of the Group Variable)</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Pooled model (all data)</span></span>
<span><span class="va">pooled_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Separate models for Group 0 and Group 1</span></span>
<span><span class="va">model_group0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">0</span><span class="op">]</span> <span class="op">~</span> <span class="va">x1</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">0</span><span class="op">]</span> <span class="op">+</span> <span class="va">x2</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">0</span><span class="op">]</span> <span class="op">+</span> <span class="va">x3</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">0</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">model_group1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">~</span> <span class="va">x1</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">x2</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">x3</span><span class="op">[</span><span class="va">group</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculating SSRs</span></span>
<span><span class="va">SSR_pooled</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">pooled_model</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">SSR_group0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model_group0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">SSR_group1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model_group1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Chow Test formula</span></span>
<span><span class="va">k_chow</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">pooled_model</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Number of parameters (including intercept)</span></span>
<span><span class="va">n0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span>                 <span class="co"># Sample size for Group 0</span></span>
<span><span class="va">n1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">group</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>                 <span class="co"># Sample size for Group 1</span></span>
<span></span>
<span><span class="va">F_chow</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="op">(</span><span class="va">SSR_pooled</span> <span class="op">-</span> <span class="op">(</span><span class="va">SSR_group0</span> <span class="op">+</span> <span class="va">SSR_group1</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">k_chow</span><span class="op">)</span> <span class="op">/</span></span>
<span>          <span class="op">(</span><span class="op">(</span><span class="va">SSR_group0</span> <span class="op">+</span> <span class="va">SSR_group1</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">n0</span> <span class="op">+</span> <span class="va">n1</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">k_chow</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Corresponding p-value</span></span>
<span><span class="va">p_value_chow</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">pf</a></span><span class="op">(</span></span>
<span>        <span class="va">F_chow</span>,</span>
<span>        df1 <span class="op">=</span> <span class="va">k_chow</span>,</span>
<span>        df2 <span class="op">=</span> <span class="op">(</span><span class="va">n0</span> <span class="op">+</span> <span class="va">n1</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">k_chow</span><span class="op">)</span>,</span>
<span>        lower.tail <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Chow Test F-statistic:"</span>, <span class="va">F_chow</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Chow Test F-statistic: 0.3551197</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"P-value:"</span>, <span class="va">p_value_chow</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; P-value: 0.8398657</span></span></code></pre></div>
<p><strong>Interpretation of the Results</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Wald Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math inline">\(\beta_2 = \beta_3 = 0\)</span> (the coefficients for <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are zero).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> &lt; 0.05: <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are jointly significant.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> ≥ 0.05: <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> do not significantly contribute to the model.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Likelihood Ratio Test (LR Test)</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The restricted model fits the data as well as the unrestricted model.</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> &lt; 0.05: The unrestricted model fits better, indicating <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> improve the model.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> ≥ 0.05: Adding <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> doesn’t improve the model significantly.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>F-Test (for Linear Regression)</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math inline">\(\beta_2 = \beta_3 = 0\)</span> (similar to the Wald Test).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> &lt; 0.05: The excluded variables are significant.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> ≥ 0.05: The excluded variables are not significant.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Chow Test (Using the <code>group</code> Variable)</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): No structural break exists; the regression coefficients are the same across <strong>Group 0</strong> and <strong>Group 1</strong>.</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> &lt; 0.05: A structural break exists—model coefficients differ significantly between the groups.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> ≥ 0.05: No significant structural break detected; the model coefficients are stable across both groups.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="non-nested-model-tests" class="section level2" number="14.2">
<h2>
<span class="header-section-number">14.2</span> Non-Nested Model Tests<a class="anchor" aria-label="anchor" href="#non-nested-model-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Comparing models is essential to identify which specification best explains the data. While nested model comparisons rely on one model being a restricted version of another, <strong>non-nested models</strong> do not share such a hierarchical structure. This situation commonly arises when comparing models with:</p>
<ul>
<li>Different functional forms (e.g., linear vs. logarithmic relationships),</li>
<li>Different sets of explanatory variables,</li>
<li>Competing theoretical frameworks.</li>
</ul>
<p>To compare non-nested models, we rely on specialized statistical tests designed to handle these complexities. The two most widely used approaches are:</p>
<ul>
<li><p><a href="model-specification-tests.html#sec-vuong-test">Vuong Test</a> used to compare the fit of two non-nested models to determine which model better explains the data.</p></li>
<li><p><a href="#sec-davidson%E2%80%93mackinnon-j-test">Davidson–MacKinnon J-Test</a> is a regression-based approach for comparing non-nested models. It evaluates which model better fits the data by incorporating the predicted values from the competing model as an additional regressor.</p></li>
</ul>
<hr>
<p>Consider two competing models:</p>
<ul>
<li>
<strong>Model A:</strong><br><span class="math display">\[
y = \alpha_0 + \alpha_1 f(X) + \epsilon_A
\]</span>
</li>
<li>
<strong>Model B:</strong><br><span class="math display">\[
y = \beta_0 + \beta_1 g(Z) + \epsilon_B
\]</span>
</li>
</ul>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(f(X)\)</span> and <span class="math inline">\(g(Z)\)</span> represent different functional forms or different sets of explanatory variables.</p></li>
<li><p>The models are <strong>non-nested</strong> because one cannot be obtained from the other by restricting parameters.</p></li>
</ul>
<p>Our goal is to determine which model better explains the data.</p>
<hr>
<div id="sec-vuong-test" class="section level3" number="14.2.1">
<h3>
<span class="header-section-number">14.2.1</span> Vuong Test<a class="anchor" aria-label="anchor" href="#sec-vuong-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Vuong Test</strong> is a likelihood-ratio-based approach for comparing non-nested models <span class="citation">(<a href="references.html#ref-vuong1989likelihood">Vuong 1989</a>)</span>. It is particularly useful when both models are estimated via <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> Estimation.</p>
<p>Hypotheses</p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): Both models are equally close to the true data-generating process (i.e., the models have equal predictive power).</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>):
<ul>
<li>Positive Test Statistic (<span class="math inline">\(V &gt; 0\)</span>): Model A is preferred.</li>
<li>Negative Test Statistic (<span class="math inline">\(V &lt; 0\)</span>): Model B is preferred.</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Vuong Test Statistic</strong></p>
<p>The Vuong test is based on the difference in the log-likelihood contributions of each observation under the two models. Let:</p>
<ul>
<li>
<span class="math inline">\(\ell_{A,i}\)</span> = log-likelihood of observation <span class="math inline">\(i\)</span> under Model A,</li>
<li>
<span class="math inline">\(\ell_{B,i}\)</span> = log-likelihood of observation <span class="math inline">\(i\)</span> under Model B.</li>
</ul>
<p>Define the difference in log-likelihoods:</p>
<p><span class="math display">\[
m_i = \ell_{A,i} - \ell_{B,i}
\]</span></p>
<p>The Vuong test statistic is:</p>
<p><span class="math display">\[
V = \frac{\sqrt{n} \, \bar{m}}{s_m}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\bar{m} = \frac{1}{n} \sum_{i=1}^n m_i\)</span> is the sample mean of the log-likelihood differences,</p></li>
<li><p><span class="math inline">\(s_m = \sqrt{\frac{1}{n} \sum_{i=1}^n (m_i - \bar{m})^2}\)</span> is the sample standard deviation of <span class="math inline">\(m_i\)</span>,</p></li>
<li><p><span class="math inline">\(n\)</span> is the sample size.</p></li>
</ul>
<hr>
<p><strong>Distribution and Decision Rule</strong></p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span>, the Vuong statistic asymptotically follows a <strong>standard normal distribution</strong>:</li>
</ul>
<p><span class="math display">\[
V \sim N(0, 1)
\]</span></p>
<ul>
<li>
<strong>Decision Rule:</strong>
<ul>
<li>If <span class="math inline">\(|V| &gt; z_{\alpha/2}\)</span> (critical value from the standard normal distribution), <strong>reject</strong> <span class="math inline">\(H_0\)</span>.
<ul>
<li>If <span class="math inline">\(V &gt; 0\)</span>: Prefer <strong>Model A</strong>.</li>
<li>If <span class="math inline">\(V &lt; 0\)</span>: Prefer <strong>Model B</strong>.</li>
</ul>
</li>
<li>If <span class="math inline">\(|V| \leq z_{\alpha/2}\)</span>: <strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span>; no significant difference between models.</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Corrections for Model Complexity</strong></p>
<p>When comparing models with different numbers of parameters, a <strong>penalized version</strong> of the Vuong test can be used, similar to adjusting for model complexity in criteria like AIC or BIC. The corrected statistic is:</p>
<p><span class="math display">\[
V_{\text{adjusted}} = V - \frac{(k_A - k_B) \ln(n)}{2 s_m \sqrt{n}}
\]</span></p>
<p>Where <span class="math inline">\(k_A\)</span> and <span class="math inline">\(k_B\)</span> are the number of parameters in Models A and B, respectively.</p>
<hr>
<p><strong>Limitations of the Vuong Test</strong></p>
<ul>
<li>Requires models to be estimated via <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a>.</li>
<li>Sensitive to <strong>model misspecification</strong> and <strong>heteroskedasticity</strong>.</li>
<li>Assumes <strong>independent and identically distributed (i.i.d.)</strong> errors.</li>
</ul>
<hr>
</div>
<div id="sec-davidson--mackinnon-j-test" class="section level3" number="14.2.2">
<h3>
<span class="header-section-number">14.2.2</span> Davidson–MacKinnon J-Test<a class="anchor" aria-label="anchor" href="#sec-davidson--mackinnon-j-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Davidson–MacKinnon J-Test</strong> provides a flexible, regression-based approach for comparing non-nested models <span class="citation">(<a href="references.html#ref-davidson1981several">Davidson and MacKinnon 1981</a>)</span>. It evaluates whether the predictions from one model contain information not captured by the competing model. This test can be thought of as comparing models with transformed independent variables, as opposed to the next section, <a href="model-specification-tests.html#comparing-models-with-transformed-dependent-variables">Comparing Models with Transformed Dependent Variables</a>.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): The competing model does not provide additional explanatory power beyond the current model.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): The competing model provides additional explanatory power.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<p>Consider two competing models:</p>
<ul>
<li>
<strong>Model A:</strong><br><span class="math display">\[
y = \alpha_0 + \alpha_1 x + \epsilon_A
\]</span>
</li>
<li>
<strong>Model B:</strong><br><span class="math display">\[
y = \beta_0 + \beta_1 \ln(x) + \epsilon_B
\]</span>
</li>
</ul>
<p><strong>Step 1: Testing Model A Against Model B</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Estimate Model B</strong> and obtain predicted values <span class="math inline">\(\hat{y}_B\)</span>.</li>
<li><strong>Run the auxiliary regression:</strong></li>
</ol>
<p><span class="math display">\[
y = \alpha_0 + \alpha_1 x + \gamma \hat{y}_B + u
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Test the null hypothesis:</strong></li>
</ol>
<p><span class="math display">\[
H_0: \gamma = 0
\]</span></p>
<ul>
<li>If <span class="math inline">\(\gamma\)</span> is significant, Model B adds explanatory power beyond Model A.</li>
<li>If <span class="math inline">\(\gamma\)</span> is not significant, Model A sufficiently explains the data.</li>
</ul>
<hr>
<p><strong>Step 2: Testing Model B Against Model A</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Estimate Model A</strong> and obtain predicted values <span class="math inline">\(\hat{y}_A\)</span>.</li>
<li><strong>Run the auxiliary regression:</strong></li>
</ol>
<p><span class="math display">\[
y = \beta_0 + \beta_1 \ln(x) + \gamma \hat{y}_A + u
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Test the null hypothesis:</strong></li>
</ol>
<p><span class="math display">\[
H_0: \gamma = 0
\]</span></p>
<hr>
<p><strong>Decision Rules</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> in Step 1, Fail to Reject in Step 2: Prefer <strong>Model B</strong>.</li>
<li>
<strong>Fail to Reject</strong> <span class="math inline">\(H_0\)</span> in Step 1, Reject in Step 2: Prefer <strong>Model A</strong>.</li>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> in Both Steps: Neither model is adequate; reconsider the functional form.</li>
<li>
<strong>Fail to Reject</strong> <span class="math inline">\(H_0\)</span> in Both Steps: No strong evidence to prefer one model; rely on other criteria (e.g., theory, simplicity). Alternatively, <span class="math inline">\(R^2_{adj}\)</span> can also be used to choose between the two.</li>
</ul>
<p><strong>Adjusted</strong> <span class="math inline">\(R^2\)</span></p>
<ul>
<li>
<span class="math inline">\(R^2\)</span> will always increase with more variables included</li>
<li>Adjusted <span class="math inline">\(R^2\)</span> tries to correct by penalizing inclusion of unnecessary variables.</li>
</ul>
<p><span class="math display">\[ \begin{aligned} {R}^2 &amp;= 1 - \frac{SSR/n}{SST/n} \\ {R}^2_{adj} &amp;= 1 - \frac{SSR/(n-k)}{SST/(n-1)} \\ &amp;= 1 - \frac{(n-1)(1-R^2)}{(n-k)} \end{aligned} \]</span></p>
<ul>
<li>
<span class="math inline">\({R}^2_{adj}\)</span> increases if and only if the t-statistic on the additional variable is greater than 1 in absolute value.</li>
<li>
<span class="math inline">\({R}^2_{adj}\)</span> is valid in models where there is no heteroskedasticity</li>
<li>there fore it <strong>should not</strong> be used in determining which variables should be included in the model (the t or F-tests are more appropriate)</li>
</ul>
</div>
<div id="adjusted-r2" class="section level3" number="14.2.3">
<h3>
<span class="header-section-number">14.2.3</span> Adjusted <span class="math inline">\(R^2\)</span><a class="anchor" aria-label="anchor" href="#adjusted-r2"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>coefficient of determination</strong> (<span class="math inline">\(R^2\)</span>) measures the proportion of the variance in the dependent variable that is explained by the model. However, a key limitation of <span class="math inline">\(R^2\)</span> is that it <strong>always increases</strong> (or at least stays the same) when additional explanatory variables are added to the model, even if those variables are not statistically significant.</p>
<p>To address this issue, the <strong>adjusted</strong> <span class="math inline">\(R^2\)</span> introduces a penalty for including unnecessary variables, making it a more reliable measure when comparing models with different numbers of predictors.</p>
<hr>
<p><strong>Formulas</strong></p>
<p><strong>Unadjusted</strong> <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{SSR}{SST}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(SSR\)</span> = <strong>Sum of Squared Residuals</strong> (measures unexplained variance),</p></li>
<li><p><span class="math inline">\(SST\)</span> = <strong>Total Sum of Squares</strong> (measures total variance in the dependent variable).</p></li>
</ul>
<p><strong>Adjusted</strong> <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[
R^2_{\text{adj}} = 1 - \frac{SSR / (n - k)}{SST / (n - 1)}
\]</span></p>
<p>Alternatively, it can be expressed in terms of <span class="math inline">\(R^2\)</span> as:</p>
<p><span class="math display">\[
R^2_{\text{adj}} = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - k} \right)
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> = Number of observations,</p></li>
<li><p><span class="math inline">\(k\)</span> = Number of estimated parameters in the model (including the intercept).</p></li>
</ul>
<hr>
<p><strong>Key Insights</strong></p>
<ul>
<li>
<strong>Penalty for Complexity:</strong> Unlike <span class="math inline">\(R^2\)</span>, the adjusted <span class="math inline">\(R^2\)</span> <strong>can decrease</strong> when irrelevant variables are added to the model because it adjusts for the number of predictors relative to the sample size.</li>
<li>
<strong>Interpretation:</strong> It represents the proportion of variance explained <strong>after accounting for model complexity</strong>.</li>
<li>
<strong>Comparison Across Models:</strong> Adjusted <span class="math inline">\(R^2\)</span> is useful for comparing models with different numbers of predictors, as it discourages overfitting.</li>
</ul>
<hr>
<p><strong>When Does Adjusted</strong> <span class="math inline">\(R^2\)</span> Increase?</p>
<ul>
<li>The adjusted <span class="math inline">\(R^2\)</span> will <strong>increase</strong> if and only if the inclusion of a new variable <strong>improves the model more than expected by chance</strong>.</li>
<li>
<strong>Mathematically:</strong> This typically occurs when the <strong>absolute value of the</strong> <span class="math inline">\(t\)</span>-statistic for the new variable is <strong>greater than 1</strong> (assuming large samples and standard model assumptions).</li>
</ul>
<hr>
<p><strong>Limitations of Adjusted</strong> <span class="math inline">\(R^2\)</span></p>
<ul>
<li>
<strong>Sensitive to Assumptions:</strong> Adjusted <span class="math inline">\(R^2\)</span> assumes <strong>homoskedasticity</strong> (constant variance of errors) and <strong>no autocorrelation</strong>. In the presence of heteroskedasticity, its interpretation may be misleading.</li>
<li>
<strong>Not a Substitute for Hypothesis Testing:</strong> It <strong>should not</strong> be the primary criterion for deciding which variables to include in a model.
<ul>
<li>Use <span class="math inline">\(t\)</span>-tests to evaluate the significance of individual coefficients.</li>
<li>Use <span class="math inline">\(F\)</span>-tests for assessing the joint significance of multiple variables.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="comparing-models-with-transformed-dependent-variables" class="section level3" number="14.2.4">
<h3>
<span class="header-section-number">14.2.4</span> Comparing Models with Transformed Dependent Variables<a class="anchor" aria-label="anchor" href="#comparing-models-with-transformed-dependent-variables"><i class="fas fa-link"></i></a>
</h3>
<p>When comparing regression models with different transformations of the dependent variable, such as level and log-linear models, direct comparisons using traditional goodness-of-fit metrics like <span class="math inline">\(R^2\)</span> or adjusted <span class="math inline">\(R^2\)</span> are invalid. This is because the transformation changes the scale of the dependent variable, affecting the calculation of the Total Sum of Squares (SST), which is the denominator in <span class="math inline">\(R^2\)</span> calculations.</p>
<hr>
<p><strong>Model Specifications</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Level Model (Linear):</strong></li>
</ol>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \epsilon
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Log-Linear Model:</strong></li>
</ol>
<p><span class="math display">\[
\ln(y) = \beta_0 + \beta_1 x_1 + \epsilon
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(y\)</span> is the dependent variable,</p></li>
<li><p><span class="math inline">\(x_1\)</span> is an independent variable,</p></li>
<li><p><span class="math inline">\(\epsilon\)</span> represents the error term.</p></li>
</ul>
<hr>
<p><strong>Interpretation of Coefficients</strong></p>
<ul>
<li>
<p><strong>In the Level Model:</strong><br>
The effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> is <strong>constant</strong>, regardless of the magnitude of <span class="math inline">\(y\)</span>. Specifically, a <strong>one-unit increase</strong> in <span class="math inline">\(x_1\)</span> results in a change of <span class="math inline">\(\beta_1\)</span> units in <span class="math inline">\(y\)</span>. This implies:</p>
<p><span class="math display">\[
\Delta y = \beta_1 \cdot \Delta x_1
\]</span></p>
</li>
<li>
<p><strong>In the Log Model:</strong><br>
The effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> is <strong>proportional</strong> to the current level of <span class="math inline">\(y\)</span>. A <strong>one-unit increase</strong> in <span class="math inline">\(x_1\)</span> leads to a <strong>percentage change</strong> in <span class="math inline">\(y\)</span>, approximately equal to <span class="math inline">\(100 \times \beta_1\%\)</span>. Specifically:</p>
<p><span class="math display">\[
\Delta \ln(y) = \beta_1 \cdot \Delta x_1 \quad \Rightarrow \quad \% \Delta y \approx 100 \times \beta_1
\]</span></p>
<ul>
<li>For small values of <span class="math inline">\(y\)</span>, the absolute change is small.</li>
<li>For large values of <span class="math inline">\(y\)</span>, the absolute change is larger, reflecting the multiplicative nature of the model.</li>
</ul>
</li>
</ul>
<hr>
<p>Why We Cannot Compare <span class="math inline">\(R^2\)</span> or Adjusted <span class="math inline">\(R^2\)</span> Directly</p>
<ul>
<li>The level model explains variance in the original scale of <span class="math inline">\(y\)</span>, while the log model explains variance in the logarithmic scale of <span class="math inline">\(y\)</span>.</li>
<li>The SST (Total Sum of Squares) differs across the models because the dependent variable is transformed, making direct comparisons of <span class="math inline">\(R^2\)</span> invalid.</li>
<li>Adjusted <span class="math inline">\(R^2\)</span> does not resolve this issue because it also depends on the scale of the dependent variable.</li>
</ul>
<hr>
<p><strong>Approach to Compare Model Fit Across Transformations</strong></p>
<p>To compare models on the <strong>same scale</strong> as the original dependent variable (<span class="math inline">\(y\)</span>), we need to <strong>“un-transform”</strong> the predictions from the log model. Here’s the step-by-step procedure:</p>
<hr>
<p><strong>Step-by-Step Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Estimate the Log Model</strong><br>
Fit the log-linear model and obtain the predicted values:</p>
<p><span class="math display">\[
\widehat{\ln(y)} = \hat{\beta}_0 + \hat{\beta}_1 x_1
\]</span></p>
</li>
<li>
<p><strong>Un-Transform the Predictions</strong><br>
Convert the predicted values back to the original scale of <span class="math inline">\(y\)</span> using the exponential function:</p>
<p><span class="math display">\[
\hat{m} = \exp(\widehat{\ln(y)})
\]</span></p>
<ul>
<li>This transformation assumes that the errors are homoskedastic in the log model.</li>
<li>
<strong>Note:</strong> To correct for potential bias due to Jensen’s inequality, a <strong>smearing estimator</strong> can be applied, but for simplicity, we use the basic exponential transformation here.</li>
</ul>
</li>
<li>
<p><strong>Fit a Regression Without an Intercept</strong><br>
Regress the actual <span class="math inline">\(y\)</span> on the un-transformed predictions <span class="math inline">\(\hat{m}\)</span> <strong>without an intercept</strong>:</p>
<p><span class="math display">\[
y = \alpha \hat{m} + u
\]</span></p>
<ul>
<li>The coefficient <span class="math inline">\(\alpha\)</span> adjusts for any scaling differences between the predicted and actual values.</li>
<li>The residual term <span class="math inline">\(u\)</span> captures the unexplained variance.</li>
</ul>
</li>
<li>
<p><strong>Compute the Scaled</strong> <span class="math inline">\(R^2\)</span><br>
Calculate the <strong>squared correlation</strong> between the observed <span class="math inline">\(y\)</span> and the predicted values <span class="math inline">\(\hat{y}\)</span> from the above regression:</p>
<p><span class="math display">\[
R^2_{\text{scaled}} = \left( \text{Corr}(y, \hat{y}) \right)^2
\]</span></p>
<ul>
<li>This <strong>scaled</strong> <span class="math inline">\(R^2\)</span> represents how well the log-transformed model predicts the original <span class="math inline">\(y\)</span> on its natural scale.</li>
<li>Now, you can <strong>compare</strong> <span class="math inline">\(R^2_{\text{scaled}}\)</span> from the log model with the <strong>regular</strong> <span class="math inline">\(R^2\)</span> from the level model.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Key Insights</strong></p>
<ul>
<li>
<strong>If</strong> <span class="math inline">\(R^2_{\text{scaled}}\)</span> (from the log model) &gt; <span class="math inline">\(R^2\)</span> (from the level model): The log model fits the data better.</li>
<li>
<strong>If</strong> <span class="math inline">\(R^2_{\text{scaled}}\)</span> &lt; <span class="math inline">\(R^2\)</span> (from the level model): The level model provides a better fit.</li>
<li>
<strong>If both are similar:</strong> Consider other model diagnostics, theoretical justification, or model simplicity.</li>
</ul>
<hr>
<p><strong>Caveats and Considerations</strong></p>
<ul>
<li><p><strong>Heteroskedasticity:</strong> If heteroskedasticity is present, the un-transformation may introduce bias.</p></li>
<li><p><strong>Error Distribution:</strong> Log-transformed models assume multiplicative errors, which may not be appropriate in all contexts.</p></li>
<li>
<p><strong>Smearing Estimator (Advanced Correction):</strong> To adjust for bias in the back-transformation, apply the smearing estimator:</p>
<p><span class="math display">\[
\hat{y} = \exp(\widehat{\ln(y)}) \cdot \hat{S}
\]</span></p>
<p>Where <span class="math inline">\(\hat{S}\)</span> is the <strong>mean of the exponentiated residuals</strong> from the log model.</p>
</li>
</ul>
<div class="sourceCode" id="cb542"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install and load necessary libraries</span></span>
<span><span class="co"># install.packages("nonnest2")  # Uncomment if not already installed</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/qpsy/nonnest2">nonnest2</a></span><span class="op">)</span>    <span class="co"># For Vuong Test</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span>      <span class="co"># For J-Test</span></span>
<span></span>
<span><span class="co"># Simulated dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">50</span>, sd <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">100</span>, sd <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Competing models (non-nested)</span></span>
<span><span class="co"># Model A: Linear relationship with x</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span><span class="va">model_A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model B: Log-linear relationship with z</span></span>
<span><span class="va">model_B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">z</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Vuong Test (Correct Function)</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="va">vuong_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/nonnest2/man/vuongtest.html">vuongtest</a></span><span class="op">(</span><span class="va">model_A</span>, <span class="va">model_B</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">vuong_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 1 </span></span>
<span><span class="co">#&gt;  Class: lm </span></span>
<span><span class="co">#&gt;  Call: lm(formula = y ~ x)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 2 </span></span>
<span><span class="co">#&gt;  Class: lm </span></span>
<span><span class="co">#&gt;  Call: lm(formula = y ~ log(z))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Variance test </span></span>
<span><span class="co">#&gt;   H0: Model 1 and Model 2 are indistinguishable </span></span>
<span><span class="co">#&gt;   H1: Model 1 and Model 2 are distinguishable </span></span>
<span><span class="co">#&gt;     w2 = 0.681,   p = 2.35e-08</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Non-nested likelihood ratio test </span></span>
<span><span class="co">#&gt;   H0: Model fits are equal for the focal population </span></span>
<span><span class="co">#&gt;   H1A: Model 1 fits better than Model 2 </span></span>
<span><span class="co">#&gt;     z = 13.108,   p = &lt;2e-16</span></span>
<span><span class="co">#&gt;   H1B: Model 2 fits better than Model 1 </span></span>
<span><span class="co">#&gt;     z = 13.108,   p = 1</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Davidson–MacKinnon J-Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span></span>
<span><span class="co"># Step 1: Testing Model A against Model B</span></span>
<span><span class="co"># Obtain fitted values from Model B</span></span>
<span><span class="va">fitted_B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">model_B</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Auxiliary regression: Add fitted_B to Model A</span></span>
<span><span class="va">j_test_A_vs_B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span> <span class="op">+</span> <span class="va">fitted_B</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">j_test_A_vs_B</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ x + fitted_B)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.8717 -0.6573 -0.1223  0.6154  2.0952 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 14.70881   25.98307   0.566    0.573    </span></span>
<span><span class="co">#&gt; x            0.28671    0.01048  27.358   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; fitted_B    -0.43702    1.27500  -0.343    0.733    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.951 on 97 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8854, Adjusted R-squared:  0.883 </span></span>
<span><span class="co">#&gt; F-statistic: 374.5 on 2 and 97 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="co"># Step 2: Testing Model B against Model A</span></span>
<span><span class="co"># Obtain fitted values from Model A</span></span>
<span><span class="va">fitted_A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">model_A</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Auxiliary regression: Add fitted_A to Model B</span></span>
<span><span class="va">j_test_B_vs_A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">z</span><span class="op">)</span> <span class="op">+</span> <span class="va">fitted_A</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">j_test_B_vs_A</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ log(z) + fitted_A)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.8717 -0.6573 -0.1223  0.6154  2.0952 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.77868    2.39275  -0.325    0.746    </span></span>
<span><span class="co">#&gt; log(z)       0.16829    0.49097   0.343    0.733    </span></span>
<span><span class="co">#&gt; fitted_A     1.00052    0.03657  27.358   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.951 on 97 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8854, Adjusted R-squared:  0.883 </span></span>
<span><span class="co">#&gt; F-statistic: 374.5 on 2 and 97 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<hr>
</div>
</div>
<div id="heteroskedasticity-tests" class="section level2" number="14.3">
<h2>
<span class="header-section-number">14.3</span> Heteroskedasticity Tests<a class="anchor" aria-label="anchor" href="#heteroskedasticity-tests"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Heteroskedasticity</strong> occurs when the variance of the error terms (<span class="math inline">\(\epsilon_i\)</span>) in a regression model is <strong>not constant</strong> across observations. This violates the Classical <a href="linear-regression.html#ols-assumptions">OLS Assumption</a>, specifically the assumption of <strong>homoskedasticity</strong> (Assumption <a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a> in the <a href="linear-regression.html#gauss-markov-theorem">Gauss-Markov Theorem</a>), which states:</p>
<p><span class="math display">\[
\text{Var}(\epsilon_i) = \sigma^2 \quad \forall \, i
\]</span></p>
<p>When heteroskedasticity is present:</p>
<ul>
<li><p><a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> estimators remain <strong>unbiased</strong> but become <strong>inefficient</strong> (i.e., no longer Best Linear Unbiased Estimators—BLUE).</p></li>
<li><p>The <strong>standard errors</strong> of the estimates are biased, leading to unreliable <strong>hypothesis tests</strong> (e.g., <span class="math inline">\(t\)</span>-tests and <span class="math inline">\(F\)</span>-tests).</p></li>
</ul>
<p>Detecting heteroskedasticity is crucial for ensuring the validity of regression results. This section covers key tests used to identify heteroskedasticity:</p>
<ul>
<li><a href="#sec-breusch%E2%80%93pagan-test">Breusch–Pagan Test</a></li>
<li><a href="model-specification-tests.html#sec-white-test-hetero">White Test</a></li>
<li><a href="#sec-goldfeld%E2%80%93quandt-test">Goldfeld–Quandt Test</a></li>
<li><a href="model-specification-tests.html#sec-park-test">Park Test</a></li>
<li><a href="model-specification-tests.html#sec-glejser-test">Glejser Test</a></li>
</ul>
<hr>
<div id="sec-breusch--pagan-test" class="section level3" number="14.3.1">
<h3>
<span class="header-section-number">14.3.1</span> Breusch–Pagan Test<a class="anchor" aria-label="anchor" href="#sec-breusch--pagan-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Breusch–Pagan (BP) Test</strong> is one of the most widely used tests for detecting heteroskedasticity <span class="citation">(<a href="references.html#ref-breusch1979simple">Breusch and Pagan 1979</a>)</span>. It examines whether the variance of the residuals depends on the independent variables.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): Homoskedasticity (<span class="math inline">\(\text{Var}(\epsilon_i) = \sigma^2\)</span> is constant).</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Heteroskedasticity exists; the variance of <span class="math inline">\(\epsilon_i\)</span> depends on the independent variables.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Estimate the original regression model:</strong></li>
</ol>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + \epsilon_i
\]</span></p>
<p>Obtain the residuals <span class="math inline">\(\hat{\epsilon}_i\)</span> from this regression.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Compute the squared residuals:</strong></li>
</ol>
<p><span class="math display">\[
\hat{\epsilon}_i^2
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>Auxiliary Regression:</strong> Regress the squared residuals on the independent variables:</li>
</ol>
<p><span class="math display">\[
\hat{\epsilon}_i^2 = \alpha_0 + \alpha_1 x_{1i} + \alpha_2 x_{2i} + \dots + \alpha_k x_{ki} + u_i
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Calculate the Test Statistic:</strong></li>
</ol>
<p>The BP test statistic is:</p>
<p><span class="math display">\[
\text{BP} = n \cdot R^2_{\text{aux}}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> is the sample size,</p></li>
<li><p><span class="math inline">\(R^2_{\text{aux}}\)</span> is the <span class="math inline">\(R^2\)</span> from the auxiliary regression.</p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><strong>Decision Rule:</strong></li>
</ol>
<ul>
<li>Under <span class="math inline">\(H_0\)</span>, the BP statistic follows a <strong>chi-squared distribution</strong> with <span class="math inline">\(k\)</span> degrees of freedom (where <span class="math inline">\(k\)</span> is the number of independent variables):</li>
</ul>
<p><span class="math display">\[
\text{BP} \sim \chi^2_k
\]</span></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if the BP statistic exceeds the critical value from the chi-squared distribution.</li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Simple to implement; directly tests the relationship between residual variance and regressors.</li>
<li>
<strong>Limitation:</strong> Sensitive to non-normality; less effective when heteroskedasticity is not linearly related to independent variables.</li>
</ul>
<hr>
</div>
<div id="sec-white-test-hetero" class="section level3" number="14.3.2">
<h3>
<span class="header-section-number">14.3.2</span> White Test<a class="anchor" aria-label="anchor" href="#sec-white-test-hetero"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>White Test</strong> is a more general heteroskedasticity test that does not require specifying the form of heteroskedasticity <span class="citation">(<a href="references.html#ref-white1980heteroskedasticity">White 1980</a>)</span>. It can detect both <strong>linear</strong> and <strong>nonlinear</strong> forms.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): Homoskedasticity.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Heteroskedasticity (of any form).</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Estimate the original regression model</strong> and obtain residuals <span class="math inline">\(\hat{\epsilon}_i\)</span>.</p></li>
<li>
<p><strong>Auxiliary Regression:</strong> Regress the squared residuals on:</p>
<ul>
<li>The original independent variables (<span class="math inline">\(x_{1i}, x_{2i}, \dots, x_{ki}\)</span>),</li>
<li>Their <strong>squares</strong> (<span class="math inline">\(x_{1i}^2, x_{2i}^2, \dots, x_{ki}^2\)</span>),</li>
<li>Their <strong>cross-products</strong> (e.g., <span class="math inline">\(x_{1i} x_{2i}\)</span>).</li>
</ul>
<p>The auxiliary regression is:</p>
<p><span class="math display">\[
\hat{\epsilon}_i^2 = \alpha_0 + \alpha_1 x_{1i} + \alpha_2 x_{2i} + \dots + \alpha_k x_{ki} + \alpha_{k+1} x_{1i}^2 + \dots + \alpha_{2k} x_{ki}^2 + \alpha_{2k+1} (x_{1i} x_{2i}) + u_i
\]</span></p>
</li>
<li>
<p><strong>Calculate the Test Statistic:</strong></p>
<p><span class="math display">\[
\text{White} = n \cdot R^2_{\text{aux}}
\]</span></p>
</li>
<li><p><strong>Decision Rule:</strong></p></li>
</ol>
<ul>
<li>
<p>Under <span class="math inline">\(H_0\)</span>, the statistic follows a <strong>chi-squared distribution</strong> with degrees of freedom equal to the number of auxiliary regressors:</p>
<p><span class="math display">\[
\text{White} \sim \chi^2_{\text{df}}
\]</span></p>
</li>
<li><p><strong>Reject</strong> <span class="math inline">\(H_0\)</span> if the statistic exceeds the critical chi-squared value.</p></li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Can detect a wide range of heteroskedasticity patterns.</li>
<li>
<strong>Limitation:</strong> May suffer from overfitting in small samples due to many auxiliary regressors.</li>
</ul>
<hr>
</div>
<div id="sec-goldfeld--quandt-test" class="section level3" number="14.3.3">
<h3>
<span class="header-section-number">14.3.3</span> Goldfeld–Quandt Test<a class="anchor" aria-label="anchor" href="#sec-goldfeld--quandt-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Goldfeld–Quandt Test</strong> is a simple test that detects heteroskedasticity by comparing the variance of residuals in two different subsets of the data <span class="citation">(<a href="references.html#ref-goldfeld1965some">Goldfeld and Quandt 1965</a>)</span>.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): Homoskedasticity.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Heteroskedasticity; variances differ between groups.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Sort the data</strong> based on an independent variable suspected to cause heteroskedasticity.</p></li>
<li>
<p><strong>Split the data</strong> into three groups:</p>
<ul>
<li>
<strong>Group 1:</strong> Lower values,</li>
<li>
<strong>Group 2:</strong> Middle values (often omitted),</li>
<li>
<strong>Group 3:</strong> Higher values.</li>
</ul>
</li>
<li><p><strong>Estimate the regression model</strong> separately for Groups 1 and 3. Obtain the residual sum of squares (<span class="math inline">\(SSR_1\)</span> and <span class="math inline">\(SSR_2\)</span>).</p></li>
<li>
<p><strong>Calculate the Test Statistic:</strong></p>
<p><span class="math display">\[
F = \frac{SSR_2 / (n_2 - k)}{SSR_1 / (n_1 - k)}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are the number of observations in Groups 1 and 3, respectively,</li>
<li>
<span class="math inline">\(k\)</span> is the number of estimated parameters.</li>
</ul>
</li>
<li><p><strong>Decision Rule:</strong></p></li>
</ol>
<ul>
<li>
<p>Under <span class="math inline">\(H_0\)</span>, the test statistic follows an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\((n_2 - k, n_1 - k)\)</span> degrees of freedom:</p>
<p><span class="math display">\[
F \sim F_{(n_2 - k, n_1 - k)}
\]</span></p>
</li>
<li><p><strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F\)</span> exceeds the critical value.</p></li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Simple to apply when heteroskedasticity is suspected to vary systematically with an independent variable.</li>
<li>
<strong>Limitation:</strong> Requires arbitrary splitting of data and assumes the error variance changes abruptly between groups.</li>
</ul>
<hr>
</div>
<div id="sec-park-test" class="section level3" number="14.3.4">
<h3>
<span class="header-section-number">14.3.4</span> Park Test<a class="anchor" aria-label="anchor" href="#sec-park-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Park Test</strong> identifies heteroskedasticity by modeling the error variance as a function of an independent variable <span class="citation">(<a href="references.html#ref-park1966estimation">R. E. Park 1966</a>)</span>.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): Homoskedasticity.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Heteroskedasticity; variance depends on an independent variable.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Estimate the original regression</strong> and obtain residuals <span class="math inline">\(\hat{\epsilon}_i\)</span>.</p></li>
<li>
<p><strong>Transform the residuals:</strong> Take the natural logarithm of the squared residuals:</p>
<p><span class="math display">\[
\ln(\hat{\epsilon}_i^2)
\]</span></p>
</li>
<li>
<p><strong>Auxiliary Regression:</strong> Regress <span class="math inline">\(\ln(\hat{\epsilon}_i^2)\)</span> on the independent variable(s):</p>
<p><span class="math display">\[
\ln(\hat{\epsilon}_i^2) = \alpha_0 + \alpha_1 \ln(x_i) + u_i
\]</span></p>
</li>
<li><p><strong>Decision Rule:</strong></p></li>
</ol>
<ul>
<li>Test whether <span class="math inline">\(\alpha_1 = 0\)</span> using a <span class="math inline">\(t\)</span>-test.</li>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\alpha_1\)</span> is statistically significant, indicating heteroskedasticity.</li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Simple to implement; works well when the variance follows a log-linear relationship.</li>
<li>
<strong>Limitation:</strong> Assumes a specific functional form for the variance, which may not hold in practice.</li>
</ul>
<hr>
</div>
<div id="sec-glejser-test" class="section level3" number="14.3.5">
<h3>
<span class="header-section-number">14.3.5</span> Glejser Test<a class="anchor" aria-label="anchor" href="#sec-glejser-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Glejser Test</strong> detects heteroskedasticity by regressing the <strong>absolute value of residuals</strong> on the independent variables <span class="citation">(<a href="references.html#ref-glejser1969new">Glejser 1969</a>)</span>.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): Homoskedasticity.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Heteroskedasticity exists.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Estimate the original regression</strong> and obtain residuals <span class="math inline">\(\hat{\epsilon}_i\)</span>.</p></li>
<li>
<p><strong>Auxiliary Regression:</strong> Regress the absolute residuals on the independent variables:</p>
<p><span class="math display">\[
|\hat{\epsilon}_i| = \alpha_0 + \alpha_1 x_{1i} + \alpha_2 x_{2i} + \dots + \alpha_k x_{ki} + u_i
\]</span></p>
</li>
<li><p><strong>Decision Rule:</strong></p></li>
</ol>
<ul>
<li>Test the significance of the coefficients (<span class="math inline">\(\alpha_1, \alpha_2, \dots\)</span>) using <span class="math inline">\(t\)</span>-tests.</li>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if any coefficient is statistically significant, indicating heteroskedasticity.</li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Flexible; can detect various forms of heteroskedasticity.</li>
<li>
<strong>Limitation:</strong> Sensitive to outliers since it relies on absolute residuals.</li>
</ul>
<hr>
</div>
<div id="summary-of-heteroskedasticity-tests" class="section level3" number="14.3.6">
<h3>
<span class="header-section-number">14.3.6</span> Summary of Heteroskedasticity Tests<a class="anchor" aria-label="anchor" href="#summary-of-heteroskedasticity-tests"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th><strong>Test</strong></th>
<th><strong>Type</strong></th>
<th><strong>Assumptions</strong></th>
<th><strong>Key Statistic</strong></th>
<th><strong>When to Use</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="#sec-breusch%E2%80%93pagan-test">Breusch–Pagan</a></td>
<td>Parametric</td>
<td>Linear relationship with predictors</td>
<td><span class="math inline">\(\chi^2\)</span></td>
<td>General-purpose test</td>
</tr>
<tr class="even">
<td><a href="model-specification-tests.html#sec-white-test-hetero">White</a></td>
<td>General (non-parametric)</td>
<td>No functional form assumption</td>
<td><span class="math inline">\(\chi^2\)</span></td>
<td>Detects both linear &amp; nonlinear forms</td>
</tr>
<tr class="odd">
<td><a href="#sec-goldfeld%E2%80%93quandt-test">Goldfeld–Quandt</a></td>
<td>Group comparison</td>
<td>Assumes known ordering of variance</td>
<td>
<span class="math inline">\(F\)</span>-distribution</td>
<td>When heteroskedasticity varies by groups</td>
</tr>
<tr class="even">
<td><a href="model-specification-tests.html#sec-park-test">Park</a></td>
<td>Parametric (log-linear)</td>
<td>Assumes log-linear variance</td>
<td>
<span class="math inline">\(t\)</span>-test</td>
<td>When variance depends on predictors</td>
</tr>
<tr class="odd">
<td><a href="model-specification-tests.html#sec-glejser-test">Glejser</a></td>
<td>Parametric</td>
<td>Based on absolute residuals</td>
<td>
<span class="math inline">\(t\)</span>-test</td>
<td>Simple test for variance dependence</td>
</tr>
</tbody>
</table></div>
<p>Detecting heteroskedasticity is critical for ensuring the reliability of regression models. While each test has strengths and limitations, combining multiple tests can provide robust insights. Once heteroskedasticity is detected, consider using <strong>robust standard errors</strong> or alternative estimation techniques (e.g., <a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a> or <a href="linear-regression.html#weighted-least-squares">Weighted Least Squares</a>) to address the issue.</p>
<div class="sourceCode" id="cb543"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install and load necessary libraries</span></span>
<span><span class="co"># install.packages("lmtest")      # For Breusch–Pagan Test</span></span>
<span><span class="co"># install.packages("car")         # For additional regression diagnostics</span></span>
<span><span class="co"># install.packages("sandwich")    # For robust covariance estimation</span></span>
<span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/">sandwich</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">50</span>, sd <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">30</span>, sd <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="va">x1</span> <span class="op">*</span> <span class="fl">0.1</span><span class="op">)</span>  <span class="co"># Heteroskedastic errors increasing with x1</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">-</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span></span>
<span><span class="co"># Original regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 1. Breusch–Pagan Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: Homoskedasticity</span></span>
<span><span class="va">bp_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/bptest.html">bptest</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">bp_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  studentized Breusch-Pagan test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; BP = 7.8141, df = 2, p-value = 0.0201</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 2. White Test (using Breusch–Pagan framework with squares &amp; interactions)</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Create squared and interaction terms</span></span>
<span><span class="va">model_white</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">x1</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">x2</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">x1</span> <span class="op">*</span> <span class="va">x2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">white_statistic</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_white</span><span class="op">)</span><span class="op">$</span><span class="va">r.squared</span> <span class="op">*</span> <span class="va">n</span>  <span class="co"># White Test Statistic</span></span>
<span><span class="va">df_white</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">model_white</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span>              <span class="co"># Degrees of freedom</span></span>
<span><span class="va">p_value_white</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">white_statistic</span>, <span class="va">df_white</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display White Test result</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"White Test Statistic:"</span>, <span class="va">white_statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; White Test Statistic: 11.85132</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Degrees of Freedom:"</span>, <span class="va">df_white</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Degrees of Freedom: 5</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"P-value:"</span>, <span class="va">p_value_white</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; P-value: 0.0368828</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 3. Goldfeld–Quandt Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: Homoskedasticity</span></span>
<span><span class="co"># Sort data by x1 (suspected source of heteroskedasticity)</span></span>
<span><span class="va">gq_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/gqtest.html">gqtest</a></span><span class="op">(</span><span class="va">model</span>, order.by <span class="op">=</span> <span class="op">~</span> <span class="va">x1</span>, fraction <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span>  <span class="co"># Omit middle 20% of data</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">gq_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Goldfeld-Quandt test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; GQ = 1.8352, df1 = 37, df2 = 37, p-value = 0.03434</span></span>
<span><span class="co">#&gt; alternative hypothesis: variance increases from segment 1 to 2</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 4. Park Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Step 1: Get residuals and square them</span></span>
<span><span class="va">residuals_squared</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span></span>
<span></span>
<span><span class="co"># Step 2: Log-transform squared residuals</span></span>
<span><span class="va">log_residuals_squared</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">residuals_squared</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 3: Regress log(residuals^2) on log(x1) (assuming variance depends on x1)</span></span>
<span><span class="va">park_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">log_residuals_squared</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">park_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log_residuals_squared ~ log(x1))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -9.3633 -1.3424  0.4218  1.6089  3.0697 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span><span class="co">#&gt; (Intercept)  -1.6319     4.5982  -0.355    0.723</span></span>
<span><span class="co">#&gt; log(x1)       0.8903     1.1737   0.759    0.450</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 2.171 on 98 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.005837,   Adjusted R-squared:  -0.004308 </span></span>
<span><span class="co">#&gt; F-statistic: 0.5754 on 1 and 98 DF,  p-value: 0.4499</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 5. Glejser Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Step 1: Absolute value of residuals</span></span>
<span><span class="va">abs_residuals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: Regress absolute residuals on independent variables</span></span>
<span><span class="va">glejser_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">abs_residuals</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">glejser_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = abs_residuals ~ x1 + x2)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -4.3096 -2.2680 -0.4564  1.9554  8.3921 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)  </span></span>
<span><span class="co">#&gt; (Intercept)  0.755846   2.554842   0.296   0.7680  </span></span>
<span><span class="co">#&gt; x1           0.064896   0.032852   1.975   0.0511 .</span></span>
<span><span class="co">#&gt; x2          -0.008495   0.062023  -0.137   0.8913  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 2.98 on 97 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.0392, Adjusted R-squared:  0.01939 </span></span>
<span><span class="co">#&gt; F-statistic: 1.979 on 2 and 97 DF,  p-value: 0.1438</span></span></code></pre></div>
<p><strong>Interpretation of the Results</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Breusch–Pagan Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): Homoskedasticity (constant error variance).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Heteroskedasticity exists (error variance depends on predictors).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span> → Evidence of heteroskedasticity.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span> → No strong evidence of heteroskedasticity.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>White Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): Homoskedasticity.</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Heteroskedasticity (of any form, linear or nonlinear).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span> → Presence of heteroskedasticity.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span> → Homoskedasticity likely holds.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Goldfeld–Quandt Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): Homoskedasticity (equal variances across groups).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Heteroskedasticity (unequal variances between groups).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span> → Variances differ between groups, indicating heteroskedasticity.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span> → No significant evidence of heteroskedasticity.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Park Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): No relationship between the variance of errors and predictor(s) (homoskedasticity).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Variance of errors depends on predictor(s).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if the <em>coefficient of</em> <span class="math inline">\(\log(x_1)\)</span> <em>is statistically significant</em> (<em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span>).</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Glejser Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): Homoskedasticity (no relationship between absolute residuals and predictors).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Heteroskedasticity exists.</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if any predictor is statistically significant (<em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span>).</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="functional-form-tests" class="section level2" number="14.4">
<h2>
<span class="header-section-number">14.4</span> Functional Form Tests<a class="anchor" aria-label="anchor" href="#functional-form-tests"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Functional form misspecification</strong> occurs when the chosen regression model does not correctly represent the true relationship between the dependent and independent variables. This can happen due to:</p>
<ul>
<li>
<strong>Omitted variables</strong> (important predictors not included),</li>
<li>
<strong>Incorrect transformations</strong> of variables (e.g., missing nonlinear relationships),</li>
<li>
<strong>Incorrect interaction terms</strong> (missing interaction effects between variables),</li>
<li>
<strong>Inappropriate linearity assumptions</strong>.</li>
</ul>
<p>Functional form errors can lead to <strong>biased and inconsistent</strong> estimators, undermining the validity of statistical inferences. To detect such issues, several diagnostic tests are available.</p>
<p><strong>Key Functional Form Tests:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="model-specification-tests.html#sec-ramsey-reset-test">Ramsey RESET Test (Regression Equation Specification Error Test)</a></li>
<li><a href="#sec-harvey%E2%80%93collier-test">Harvey–Collier Test</a></li>
<li><a href="model-specification-tests.html#sec-rainbow-test">Rainbow Test</a></li>
</ol>
<p>Each test focuses on identifying different aspects of potential model misspecification.</p>
<hr>
<div id="sec-ramsey-reset-test" class="section level3" number="14.4.1">
<h3>
<span class="header-section-number">14.4.1</span> Ramsey RESET Test (Regression Equation Specification Error Test)<a class="anchor" aria-label="anchor" href="#sec-ramsey-reset-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Ramsey RESET Test</strong> is one of the most widely used tests to detect functional form misspecification <span class="citation">(<a href="references.html#ref-ramsey1969tests">Ramsey 1969</a>)</span>. It examines whether adding <strong>nonlinear transformations</strong> of the fitted values (or regressors) improves the model fit.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): The model is correctly specified.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): The model suffers from omitted variables, incorrect functional form, or other specification errors.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Estimate the original regression model:</strong></p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + \epsilon_i
\]</span></p>
</li>
<li>
<p><strong>Obtain the fitted values:</strong></p>
<p><span class="math display">\[
\hat{y}_i
\]</span></p>
</li>
<li>
<p><strong>Augment the model with powers of the fitted values (squared, cubed, etc.):</strong></p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} + \gamma_1 \hat{y}_i^2 + \gamma_2 \hat{y}_i^3 + u_i
\]</span></p>
</li>
<li>
<p><strong>Test the joint significance</strong> of the added terms:</p>
<p><span class="math display">\[
H_0: \gamma_1 = \gamma_2 = 0
\]</span></p>
</li>
<li>
<p><strong>Compute the F-statistic:</strong></p>
<p><span class="math display">\[
F = \frac{(SSR_{\text{restricted}} - SSR_{\text{unrestricted}}) / q}{SSR_{\text{unrestricted}} / (n - k - q - 1)}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(SSR_{\text{restricted}}\)</span> = Sum of Squared Residuals from the original model,</li>
<li>
<span class="math inline">\(SSR_{\text{unrestricted}}\)</span> = SSR from the augmented model,</li>
<li>
<span class="math inline">\(q\)</span> = Number of additional terms (e.g., 2 if adding <span class="math inline">\(\hat{y}^2\)</span> and <span class="math inline">\(\hat{y}^3\)</span>),</li>
<li>
<span class="math inline">\(n\)</span> = Sample size,</li>
<li>
<span class="math inline">\(k\)</span> = Number of predictors in the original model.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Decision Rule</strong></p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span>, the F-statistic follows an <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\((q, n - k - q - 1)\)</span> degrees of freedom.</li>
<li>Reject <span class="math inline">\(H_0\)</span> if the F-statistic exceeds the critical value, indicating functional form misspecification.</li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Simple to implement; detects omitted variables and incorrect functional forms.</li>
<li>
<strong>Limitation:</strong> Does not identify which variable or functional form is incorrect—only indicates the presence of an issue.</li>
</ul>
<hr>
</div>
<div id="sec-harvey--collier-test" class="section level3" number="14.4.2">
<h3>
<span class="header-section-number">14.4.2</span> Harvey–Collier Test<a class="anchor" aria-label="anchor" href="#sec-harvey--collier-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Harvey–Collier Test</strong> evaluates whether the model’s residuals display systematic patterns, which would indicate functional form misspecification <span class="citation">(<a href="references.html#ref-harvey1977testing">Harvey and Collier 1977</a>)</span>. It is based on testing for a <strong>non-zero mean</strong> in the residuals after projection onto specific components.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): The model is correctly specified (residuals are random noise with zero mean).</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): The model is misspecified (residuals contain systematic patterns).</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Estimate the original regression model</strong> and obtain residuals <span class="math inline">\(\hat{\epsilon}_i\)</span>.</p></li>
<li><p><strong>Project the residuals</strong> onto the space spanned by a specially constructed test vector (often derived from the inverse of the design matrix in linear regression).</p></li>
<li>
<p><strong>Calculate the Harvey–Collier statistic:</strong></p>
<p><span class="math display">\[
t = \frac{\bar{\epsilon}}{\text{SE}(\bar{\epsilon})}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\bar{\epsilon}\)</span> is the mean of the projected residuals,</li>
<li>
<span class="math inline">\(\text{SE}(\bar{\epsilon})\)</span> is the standard error of the mean residual.</li>
</ul>
</li>
</ol>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>The test statistic follows a <span class="math inline">\(t\)</span>-distribution under <span class="math inline">\(H_0\)</span>.</li>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if the <span class="math inline">\(t\)</span>-statistic is significantly different from zero.</li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Simple to apply and interpret; good for detecting subtle misspecifications.</li>
<li>
<strong>Limitation:</strong> Sensitive to outliers; may have reduced power in small samples.</li>
</ul>
<hr>
</div>
<div id="sec-rainbow-test" class="section level3" number="14.4.3">
<h3>
<span class="header-section-number">14.4.3</span> Rainbow Test<a class="anchor" aria-label="anchor" href="#sec-rainbow-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Rainbow Test</strong> is a general-purpose diagnostic tool for functional form misspecification <span class="citation">(<a href="references.html#ref-utts1982rainbow">Utts 1982</a>)</span>. It compares the performance of the model on the <strong>full sample</strong> versus a <strong>central subsample</strong>, where the central subsample contains observations near the median of the independent variables.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): The model is correctly specified.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): The model is misspecified.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Estimate the regression model</strong> on the full dataset and record the residuals.</p></li>
<li><p><strong>Identify a central subsample</strong> (e.g., observations near the median of key predictors).</p></li>
<li><p><strong>Estimate the model again</strong> on the central subsample.</p></li>
<li>
<p><strong>Compare the predictive accuracy</strong> between the full sample and subsample using an F-statistic:</p>
<p><span class="math display">\[
F = \frac{(SSR_{\text{full}} - SSR_{\text{subsample}}) / q}{SSR_{\text{subsample}} / (n - k - q)}
\]</span></p>
<p>Where <span class="math inline">\(q\)</span> is the number of restrictions implied by using the subsample.</p>
</li>
</ol>
<hr>
<p><strong>Decision Rule</strong></p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span>, the test statistic follows an <span class="math inline">\(F\)</span>-distribution.</li>
<li>Reject <span class="math inline">\(H_0\)</span> if the F-statistic is significant, indicating model misspecification.</li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Robust to various forms of misspecification.</li>
<li>
<strong>Limitation:</strong> Choice of subsample may influence results; less informative about the specific nature of the misspecification.</li>
</ul>
<hr>
</div>
<div id="summary-of-functional-form-tests" class="section level3" number="14.4.4">
<h3>
<span class="header-section-number">14.4.4</span> Summary of Functional Form Tests<a class="anchor" aria-label="anchor" href="#summary-of-functional-form-tests"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th><strong>Test</strong></th>
<th><strong>Type</strong></th>
<th><strong>Key Statistic</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>When to Use</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="model-specification-tests.html#sec-ramsey-reset-test">Ramsey RESET Test</a></td>
<td>Augmented regression</td>
<td>
<span class="math inline">\(F\)</span>-test</td>
<td>Detects omitted variables, nonlinearities</td>
<td>General model specification testing</td>
</tr>
<tr class="even">
<td><a href="#sec-harvey%E2%80%93collier-test">Harvey–Collier Test</a></td>
<td>Residual-based</td>
<td>
<span class="math inline">\(t\)</span>-test</td>
<td>Detects systematic patterns in residuals</td>
<td>Subtle misspecifications in linear models</td>
</tr>
<tr class="odd">
<td><a href="model-specification-tests.html#sec-rainbow-test">Rainbow Test</a></td>
<td>Subsample comparison</td>
<td>
<span class="math inline">\(F\)</span>-test</td>
<td>Tests model stability across subsamples</td>
<td>Comparing central vs. full sample</td>
</tr>
</tbody>
</table></div>
<p>Functional form misspecification can severely distort regression results, leading to biased estimates and invalid inferences. While no single test can detect all types of misspecification, using a combination of tests provides a robust framework for model diagnostics.</p>
<div class="sourceCode" id="cb544"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install and load necessary libraries</span></span>
<span><span class="co"># install.packages("lmtest")      # For RESET and Harvey–Collier Test</span></span>
<span><span class="co"># install.packages("car")         # For diagnostic tests</span></span>
<span><span class="co"># install.packages("strucchange") # For Rainbow Test</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">strucchange</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">50</span>, sd <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">30</span>, sd <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">-</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span></span>
<span><span class="co"># Original regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 1. Ramsey RESET Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: The model is correctly specified</span></span>
<span><span class="va">reset_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/resettest.html">resettest</a></span><span class="op">(</span><span class="va">model</span>, power <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">3</span>, type <span class="op">=</span> <span class="st">"fitted"</span><span class="op">)</span>  <span class="co"># Adds ŷ² and ŷ³</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">reset_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  RESET test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; RESET = 0.1921, df1 = 2, df2 = 95, p-value = 0.8255</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 2. Harvey–Collier Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: The model is correctly specified (residuals have zero mean)</span></span>
<span><span class="va">hc_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/harvtest.html">harvtest</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">hc_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Harvey-Collier test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; HC = 0.041264, df = 96, p-value = 0.9672</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 3. Rainbow Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: The model is correctly specified</span></span>
<span><span class="va">rainbow_test</span> <span class="op">&lt;-</span> <span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/raintest.html">raintest</a></span> <span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">rainbow_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Rainbow test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; Rain = 1.1857, df1 = 50, df2 = 47, p-value = 0.279</span></span></code></pre></div>
<p><strong>Interpretation of the Results</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Ramsey RESET Test (Regression Equation Specification Error Test)</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The model is correctly specified.</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): The model suffers from omitted variables, incorrect functional form, or other specification errors.</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span> → Evidence of model misspecification (e.g., missing nonlinear terms).</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span> → No strong evidence of misspecification.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Harvey–Collier Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The model is correctly specified (residuals are random noise with zero mean).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): The model is misspecified (residuals contain systematic patterns).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span> → Model misspecification detected (non-random residual patterns).</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span> → No evidence of misspecification.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Rainbow Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The model is correctly specified.</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): The model is misspecified.</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span> → Evidence of model misspecification (model performs differently on subsets).</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span> → Model specification appears valid.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="autocorrelation-tests" class="section level2" number="14.5">
<h2>
<span class="header-section-number">14.5</span> Autocorrelation Tests<a class="anchor" aria-label="anchor" href="#autocorrelation-tests"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Autocorrelation</strong> (also known as <strong>serial correlation</strong>) occurs when the error terms (<span class="math inline">\(\epsilon_t\)</span>) in a regression model are <strong>correlated across observations</strong>, violating the assumption of <strong>independence</strong> in the classical linear regression model. This issue is particularly common in <strong>time-series data</strong>, where observations are ordered over time.</p>
<p><strong>Consequences of Autocorrelation:</strong></p>
<ul>
<li>
<a href="%7B#ordinary-least-squares%7D">OLS</a> estimators remain unbiased but become <strong>inefficient</strong>, meaning they do not have the minimum variance among all linear unbiased estimators.</li>
<li>
<strong>Standard errors</strong> are biased, leading to unreliable hypothesis tests (e.g., <span class="math inline">\(t\)</span>-tests and <span class="math inline">\(F\)</span>-tests).</li>
<li>Potential <strong>underestimation of standard errors</strong>, increasing the risk of Type I errors (false positives).</li>
</ul>
<hr>
<div id="sec-durbin--watson-test" class="section level3" number="14.5.1">
<h3>
<span class="header-section-number">14.5.1</span> Durbin–Watson Test<a class="anchor" aria-label="anchor" href="#sec-durbin--watson-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Durbin–Watson (DW) Test</strong> is the most widely used test for detecting <strong>first-order autocorrelation</strong>, where the current error term is correlated with the previous one:</p>
<p><span class="math display">\[
\epsilon_t = \rho \, \epsilon_{t-1} + u_t
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\rho\)</span> is the autocorrelation coefficient,</p></li>
<li><p><span class="math inline">\(u_t\)</span> is a white noise error term.</p></li>
</ul>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): No first-order autocorrelation (<span class="math inline">\(\rho = 0\)</span>).</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): First-order autocorrelation exists (<span class="math inline">\(\rho \neq 0\)</span>).</li>
</ul>
<hr>
<p><strong>Durbin–Watson Test Statistic</strong></p>
<p>The DW statistic is calculated as:</p>
<p><span class="math display">\[
DW = \frac{\sum_{t=2}^{n} (\hat{\epsilon}_t - \hat{\epsilon}_{t-1})^2}{\sum_{t=1}^{n} \hat{\epsilon}_t^2}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\hat{\epsilon}_t\)</span> are the residuals from the regression,</p></li>
<li><p><span class="math inline">\(n\)</span> is the number of observations.</p></li>
</ul>
<hr>
<p><strong>Decision Rule</strong></p>
<ul>
<li>The DW statistic ranges from <strong>0 to 4</strong>:
<ul>
<li>
<strong>DW ≈ 2</strong>: No autocorrelation.</li>
<li>
<strong>DW &lt; 2</strong>: Positive autocorrelation.</li>
<li>
<strong>DW &gt; 2</strong>: Negative autocorrelation.</li>
</ul>
</li>
</ul>
<p>For more precise interpretation:</p>
<ul>
<li><p>Use <strong>Durbin–Watson critical values</strong> (<span class="math inline">\(d_L\)</span> and <span class="math inline">\(d_U\)</span>) to form decision boundaries.</p></li>
<li><p>If the test statistic falls into an <strong>inconclusive range</strong>, consider alternative tests like the <a href="#sec-breusch%E2%80%93godfrey-test">Breusch–Godfrey test</a>.</p></li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Simple to compute; specifically designed for detecting first-order autocorrelation.</li>
<li>
<strong>Limitation:</strong> Inconclusive in some cases; <strong>invalid</strong> when lagged dependent variables are included in the model.</li>
</ul>
<hr>
</div>
<div id="sec-breusch--godfrey-test" class="section level3" number="14.5.2">
<h3>
<span class="header-section-number">14.5.2</span> Breusch–Godfrey Test<a class="anchor" aria-label="anchor" href="#sec-breusch--godfrey-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Breusch–Godfrey (BG) Test</strong> is a more general approach that can detect <strong>higher-order autocorrelation</strong> (e.g., second-order, third-order) and is valid even when <strong>lagged dependent variables</strong> are included in the model <span class="citation">(<a href="references.html#ref-breusch1978testing">Breusch 1978</a>; <a href="references.html#ref-godfrey1978testing">Godfrey 1978</a>)</span>.</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): No autocorrelation of any order (up to lag <span class="math inline">\(p\)</span>).</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Autocorrelation exists at some lag(s).</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Estimate the original regression model</strong> and obtain residuals <span class="math inline">\(\hat{\epsilon}_t\)</span>:</p>
<p><span class="math display">\[
y_t = \beta_0 + \beta_1 x_{1t} + \dots + \beta_k x_{kt} + \epsilon_t
\]</span></p>
</li>
<li>
<p><strong>Run an auxiliary regression</strong> by regressing the residuals on the original regressors plus <span class="math inline">\(p\)</span> lagged residuals:</p>
<p><span class="math display">\[
\hat{\epsilon}_t = \alpha_0 + \alpha_1 x_{1t} + \dots + \alpha_k x_{kt} + \rho_1 \hat{\epsilon}_{t-1} + \dots + \rho_p \hat{\epsilon}_{t-p} + u_t
\]</span></p>
</li>
<li>
<p><strong>Calculate the test statistic:</strong></p>
<p><span class="math display">\[
\text{BG} = n \cdot R^2_{\text{aux}}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span> is the sample size,</li>
<li>
<span class="math inline">\(R^2_{\text{aux}}\)</span> is the <span class="math inline">\(R^2\)</span> from the auxiliary regression.</li>
</ul>
</li>
</ol>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<p>Under <span class="math inline">\(H_0\)</span>, the BG statistic follows a <strong>chi-squared distribution</strong> with <span class="math inline">\(p\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\text{BG} \sim \chi^2_p
\]</span></p>
</li>
<li><p><strong>Reject</strong> <span class="math inline">\(H_0\)</span> if the statistic exceeds the critical chi-squared value, indicating the presence of autocorrelation.</p></li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Can detect <strong>higher-order autocorrelation</strong>; valid with lagged dependent variables.</li>
<li>
<strong>Limitation:</strong> More computationally intensive than the <a href="#sec-durbin%E2%80%93watson-test">Durbin–Watson test</a>.</li>
</ul>
<hr>
</div>
<div id="sec-ljung--box-test" class="section level3" number="14.5.3">
<h3>
<span class="header-section-number">14.5.3</span> Ljung–Box Test (or Box–Pierce Test)<a class="anchor" aria-label="anchor" href="#sec-ljung--box-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Ljung–Box Test</strong> is a <strong>portmanteau test</strong> designed to detect <strong>autocorrelation at multiple lags</strong> simultaneously <span class="citation">(<a href="references.html#ref-box1970distribution">Box and Pierce 1970</a>; <a href="references.html#ref-ljung1978measure">Ljung and Box 1978</a>)</span>. It is commonly used in <strong>time-series analysis</strong> to check residual autocorrelation after model estimation (e.g., in ARIMA models).</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): No autocorrelation up to lag <span class="math inline">\(h\)</span>.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Autocorrelation exists at one or more lags.</li>
</ul>
<hr>
<p><strong>Ljung–Box Test Statistic</strong></p>
<p>The Ljung–Box statistic is calculated as:</p>
<p><span class="math display">\[
Q = n(n + 2) \sum_{k=1}^{h} \frac{\hat{\rho}_k^2}{n - k}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span> = Sample size,</li>
<li>
<span class="math inline">\(h\)</span> = Number of lags tested,</li>
<li>
<span class="math inline">\(\hat{\rho}_k\)</span> = Sample autocorrelation at lag <span class="math inline">\(k\)</span>.</li>
</ul>
<hr>
<p><strong>Decision Rule</strong></p>
<ul>
<li>
<p>Under <span class="math inline">\(H_0\)</span>, the <span class="math inline">\(Q\)</span> statistic follows a <strong>chi-squared distribution</strong> with <span class="math inline">\(h\)</span> degrees of freedom:</p>
<p><span class="math display">\[
Q \sim \chi^2_h
\]</span></p>
</li>
<li><p><strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <span class="math inline">\(Q\)</span> exceeds the critical value, indicating significant autocorrelation.</p></li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Detects autocorrelation across multiple lags simultaneously.</li>
<li>
<strong>Limitation:</strong> Less powerful for detecting specific lag structures; sensitive to model misspecification.</li>
</ul>
<hr>
</div>
<div id="sec-runs-test" class="section level3" number="14.5.4">
<h3>
<span class="header-section-number">14.5.4</span> Runs Test<a class="anchor" aria-label="anchor" href="#sec-runs-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Runs Test</strong> is a <strong>non-parametric test</strong> that examines the randomness of residuals. It is based on the number of <strong>runs</strong>—sequences of consecutive residuals with the same sign.</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): Residuals are randomly distributed (no autocorrelation).</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_1\)</span>): Residuals exhibit non-random patterns (indicating autocorrelation).</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Classify residuals</strong> as positive or negative.</p></li>
<li><p><strong>Count the number of runs:</strong> A <strong>run</strong> is a sequence of consecutive positive or negative residuals.</p></li>
<li>
<p><strong>Calculate the expected number of runs under randomness:</strong></p>
<p><span class="math display">\[
E(R) = \frac{2 n_+ n_-}{n} + 1
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(n_+\)</span> = Number of positive residuals,</li>
<li>
<span class="math inline">\(n_-\)</span> = Number of negative residuals,</li>
<li>
<span class="math inline">\(n = n_+ + n_-\)</span>.</li>
</ul>
</li>
<li>
<p><strong>Compute the test statistic (Z-score):</strong></p>
<p><span class="math display">\[
Z = \frac{R - E(R)}{\sqrt{\text{Var}(R)}}
\]</span></p>
<p>Where <span class="math inline">\(\text{Var}(R)\)</span> is the variance of the number of runs under the null hypothesis.</p>
</li>
</ol>
<hr>
<p><strong>Decision Rule</strong></p>
<ul>
<li>
<p>Under <span class="math inline">\(H_0\)</span>, the <span class="math inline">\(Z\)</span>-statistic follows a <strong>standard normal distribution</strong>:</p>
<p><span class="math display">\[
Z \sim N(0, 1)
\]</span></p>
</li>
<li><p><strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|Z|\)</span> exceeds the critical value from the standard normal distribution.</p></li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Non-parametric; does not assume normality or linearity.</li>
<li>
<strong>Limitation:</strong> Less powerful than parametric tests; primarily useful as a supplementary diagnostic.</li>
</ul>
<hr>
</div>
<div id="summary-of-autocorrelation-tests" class="section level3" number="14.5.5">
<h3>
<span class="header-section-number">14.5.5</span> Summary of Autocorrelation Tests<a class="anchor" aria-label="anchor" href="#summary-of-autocorrelation-tests"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th><strong>Test</strong></th>
<th><strong>Type</strong></th>
<th><strong>Key Statistic</strong></th>
<th><strong>Detects</strong></th>
<th><strong>When to Use</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="#sec-durbin%E2%80%93watson-test">Durbin–Watson</a></td>
<td>Parametric</td>
<td><span class="math inline">\(DW\)</span></td>
<td>First-order autocorrelation</td>
<td>Simple linear models without lagged dependent variables</td>
</tr>
<tr class="even">
<td><a href="#sec-breusch%E2%80%93godfrey-test">Breusch–Godfrey</a></td>
<td>Parametric (general)</td>
<td><span class="math inline">\(\chi^2\)</span></td>
<td>Higher-order autocorrelation</td>
<td>Models with lagged dependent variables</td>
</tr>
<tr class="odd">
<td><a href="#sec-ljung%E2%80%93box-test">Ljung–Box</a></td>
<td>Portmanteau (global test)</td>
<td><span class="math inline">\(\chi^2\)</span></td>
<td>Autocorrelation at multiple lags</td>
<td>Time-series models (e.g., ARIMA)</td>
</tr>
<tr class="even">
<td><a href="model-specification-tests.html#sec-runs-test">Runs Test</a></td>
<td>Non-parametric</td>
<td>
<span class="math inline">\(Z\)</span>-statistic</td>
<td>Non-random patterns in residuals</td>
<td>Supplementary diagnostic for randomness</td>
</tr>
</tbody>
</table></div>
<p>Detecting autocorrelation is crucial for ensuring the efficiency and reliability of regression models, especially in time-series analysis. While the <a href="#sec-durbin%E2%80%93watson-test">Durbin–Watson Test</a> is suitable for detecting first-order autocorrelation, the <a href="#sec-breusch%E2%80%93godfrey-test">Breusch–Godfrey Test</a> and <a href="#sec-ljung%E2%80%93box-test-or-box%E2%80%93pierce-test">Ljung–Box Test</a> offer more flexibility for higher-order and multi-lag dependencies. Non-parametric tests like the <a href="model-specification-tests.html#sec-runs-test">Runs Test</a> serve as useful supplementary diagnostics.</p>
<div class="sourceCode" id="cb545"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install and load necessary libraries</span></span>
<span><span class="co"># install.packages("lmtest")  # For Durbin–Watson and Breusch–Godfrey Tests</span></span>
<span><span class="co"># install.packages("tseries") # For Runs Test</span></span>
<span><span class="co"># install.packages("forecast")# For Ljung–Box Test</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">tseries</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://pkg.robjhyndman.com/forecast/">forecast</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated time-series dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">time</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">50</span>, sd <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">30</span>, sd <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Introducing autocorrelation in errors</span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.sim.html">arima.sim</a></span><span class="op">(</span>model <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span>ar <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span>, n <span class="op">=</span> <span class="va">n</span><span class="op">)</span> </span>
<span><span class="co"># AR(1) process with ρ = 0.6</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">-</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span></span>
<span><span class="co"># Original regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 1. Durbin–Watson Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: No first-order autocorrelation</span></span>
<span><span class="va">dw_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/dwtest.html">dwtest</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">dw_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Durbin-Watson test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; DW = 0.77291, p-value = 3.323e-10</span></span>
<span><span class="co">#&gt; alternative hypothesis: true autocorrelation is greater than 0</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 2. Breusch–Godfrey Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: No autocorrelation up to lag 2</span></span>
<span><span class="va">bg_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/bgtest.html">bgtest</a></span><span class="op">(</span><span class="va">model</span>, order <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># Testing for autocorrelation up to lag 2</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">bg_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Breusch-Godfrey test for serial correlation of order up to 2</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; LM test = 40.314, df = 2, p-value = 1.762e-09</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 3. Ljung–Box Test</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: No autocorrelation up to lag 10</span></span>
<span><span class="va">ljung_box_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/box.test.html">Box.test</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span>, lag <span class="op">=</span> <span class="fl">10</span>, type <span class="op">=</span> <span class="st">"Ljung-Box"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">ljung_box_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Box-Ljung test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  residuals(model)</span></span>
<span><span class="co">#&gt; X-squared = 50.123, df = 10, p-value = 2.534e-07</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 4. Runs Test (Non-parametric)</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: Residuals are randomly distributed</span></span>
<span><span class="va">runs_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tseries/man/runs.test.html">runs.test</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sign.html">sign</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">runs_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Runs Test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  as.factor(sign(residuals(model)))</span></span>
<span><span class="co">#&gt; Standard Normal = -4.2214, p-value = 2.428e-05</span></span>
<span><span class="co">#&gt; alternative hypothesis: two.sided</span></span></code></pre></div>
<p><strong>Interpretation of the Results</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Durbin–Watson Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): No first-order autocorrelation (<span class="math inline">\(\rho = 0\)</span>).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): First-order autocorrelation exists (<span class="math inline">\(\rho \neq 0\)</span>).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>DW</em> <span class="math inline">\(&lt; 1.5\)</span> (positive autocorrelation) or <em>DW</em> <span class="math inline">\(&gt; 2.5\)</span> (negative autocorrelation).</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>DW</em> <span class="math inline">\(\approx 2\)</span>, suggesting no significant autocorrelation.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Breusch–Godfrey Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): No autocorrelation up to lag <span class="math inline">\(p\)</span> (here, <span class="math inline">\(p = 2\)</span>).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Autocorrelation exists at one or more lags.</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span>, indicating significant autocorrelation.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span>, suggesting no evidence of autocorrelation.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Ljung–Box Test</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): No autocorrelation up to lag <span class="math inline">\(h\)</span> (here, <span class="math inline">\(h = 10\)</span>).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Autocorrelation exists at one or more lags.</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span>, indicating significant autocorrelation.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span>, suggesting no evidence of autocorrelation.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Runs Test (Non-parametric)</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): Residuals are randomly distributed (no autocorrelation).</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): Residuals exhibit non-random patterns (indicating autocorrelation).</p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(&lt; 0.05\)</span>, indicating non-randomness and potential autocorrelation.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span> if <em>p-value</em> <span class="math inline">\(\ge 0.05\)</span>, suggesting randomness in residuals.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="multicollinearity-diagnostics" class="section level2" number="14.6">
<h2>
<span class="header-section-number">14.6</span> Multicollinearity Diagnostics<a class="anchor" aria-label="anchor" href="#multicollinearity-diagnostics"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Multicollinearity</strong> occurs when two or more independent variables in a regression model are <strong>highly correlated</strong>, leading to several issues:</p>
<ul>
<li>
<strong>Unstable coefficient estimates:</strong> Small changes in the data can cause large fluctuations in parameter estimates.</li>
<li>
<strong>Inflated standard errors:</strong> Reduces the precision of estimated coefficients, making it difficult to determine the significance of predictors.</li>
<li>
<strong>Difficulty in assessing variable importance:</strong> It becomes challenging to isolate the effect of individual predictors on the dependent variable.</li>
</ul>
<p>Multicollinearity does not affect the overall fit of the model (e.g., <span class="math inline">\(R^2\)</span> remains high), but it distorts the reliability of individual coefficient estimates.</p>
<p><strong>Key Multicollinearity Diagnostics:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="model-specification-tests.html#sec-variance-inflation-factor">Variance Inflation Factor</a></li>
<li><a href="model-specification-tests.html#sec-tolerance-statistic">Tolerance Statistic</a></li>
<li><a href="model-specification-tests.html#sec-condition-index-and-eigenvalue-decomposition">Condition Index and Eigenvalue Decomposition</a></li>
<li><a href="model-specification-tests.html#sec-pairwise-correlation-matrix">Pairwise Correlation Matrix</a></li>
<li><a href="model-specification-tests.html#sec-determinant-of-the-correlation-matrix">Determinant of the Correlation Matrix</a></li>
</ol>
<hr>
<div id="sec-variance-inflation-factor" class="section level3" number="14.6.1">
<h3>
<span class="header-section-number">14.6.1</span> Variance Inflation Factor<a class="anchor" aria-label="anchor" href="#sec-variance-inflation-factor"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Variance Inflation Factor (VIF)</strong> is the most commonly used diagnostic for detecting multicollinearity. It measures how much the <strong>variance of an estimated regression coefficient</strong> is inflated due to multicollinearity compared to when the predictors are uncorrelated.</p>
<hr>
<p>For each predictor <span class="math inline">\(X_j\)</span>, the VIF is defined as:</p>
<p><span class="math display">\[
\text{VIF}_j = \frac{1}{1 - R_j^2}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(R_j^2\)</span> is the <strong>coefficient of determination</strong> obtained by regressing <span class="math inline">\(X_j\)</span> on all other independent variables in the model.</li>
</ul>
<hr>
<p><strong>Interpretation of VIF</strong></p>
<ul>
<li>
<strong>VIF = 1:</strong> No multicollinearity (perfect independence).</li>
<li>
<strong>1 &lt; VIF &lt; 5:</strong> Moderate correlation, typically not problematic.</li>
<li>
<strong>VIF ≥ 5:</strong> High correlation; consider investigating further.</li>
<li>
<strong>VIF ≥ 10:</strong> Severe multicollinearity; corrective action is recommended.</li>
</ul>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>Regress each independent variable (<span class="math inline">\(X_j\)</span>) on the remaining predictors.</li>
<li>Compute <span class="math inline">\(R_j^2\)</span> for each regression.</li>
<li>Calculate <span class="math inline">\(\text{VIF}_j = 1 / (1 - R_j^2)\)</span>.</li>
<li>Analyze VIF values to identify problematic predictors.</li>
</ol>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Easy to compute and interpret.</li>
<li>
<strong>Limitation:</strong> Detects only <strong>linear</strong> relationships; may not capture complex multicollinearity patterns involving multiple variables simultaneously.</li>
</ul>
<hr>
</div>
<div id="sec-tolerance-statistic" class="section level3" number="14.6.2">
<h3>
<span class="header-section-number">14.6.2</span> Tolerance Statistic<a class="anchor" aria-label="anchor" href="#sec-tolerance-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Tolerance Statistic</strong> is the <strong>reciprocal of the VIF</strong> and measures the proportion of variance in an independent variable <strong>not explained</strong> by the other predictors.</p>
<p><span class="math display">\[
\text{Tolerance}_j = 1 - R_j^2
\]</span></p>
<p>Where <span class="math inline">\(R_j^2\)</span> is defined as in the VIF calculation.</p>
<hr>
<p><strong>Interpretation of Tolerance</strong></p>
<ul>
<li>
<strong>Tolerance close to 1:</strong> Low multicollinearity.</li>
<li>
<strong>Tolerance &lt; 0.2:</strong> Potential multicollinearity problem.</li>
<li>
<strong>Tolerance &lt; 0.1:</strong> Severe multicollinearity.</li>
</ul>
<p>Since <strong>low tolerance</strong> implies <strong>high VIF</strong>, both metrics provide consistent information.</p>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Provides an intuitive measure of how much variance is “free” from multicollinearity.</li>
<li>
<strong>Limitation:</strong> Similar to VIF, focuses on linear dependencies.</li>
</ul>
<hr>
</div>
<div id="sec-condition-index-and-eigenvalue-decomposition" class="section level3" number="14.6.3">
<h3>
<span class="header-section-number">14.6.3</span> Condition Index and Eigenvalue Decomposition<a class="anchor" aria-label="anchor" href="#sec-condition-index-and-eigenvalue-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Condition Index</strong> is a more advanced diagnostic that detects <strong>multicollinearity involving multiple variables</strong> simultaneously. It is based on the <strong>eigenvalues</strong> of the scaled independent variable matrix.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Compute the scaled design matrix</strong> <span class="math inline">\(X'X\)</span>, where <span class="math inline">\(X\)</span> is the matrix of independent variables.</p></li>
<li><p><strong>Perform eigenvalue decomposition</strong> to obtain the eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_k\)</span>.</p></li>
<li>
<p><strong>Calculate the Condition Index:</strong></p>
<p><span class="math display">\[
\text{CI}_j = \sqrt{\frac{\lambda_{\max}}{\lambda_j}}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\lambda_{\max}\)</span> is the largest eigenvalue,</li>
<li>
<span class="math inline">\(\lambda_j\)</span> is the <span class="math inline">\(j\)</span>-th eigenvalue.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Interpretation of Condition Index</strong></p>
<ul>
<li>
<strong>CI &lt; 10:</strong> No serious multicollinearity.</li>
<li>
<strong>10 ≤ CI &lt; 30:</strong> Moderate to strong multicollinearity.</li>
<li>
<strong>CI ≥ 30:</strong> Severe multicollinearity.</li>
</ul>
<p>A <strong>high condition index</strong> indicates near-linear dependence among variables.</p>
<hr>
<p><strong>Variance Decomposition Proportions</strong></p>
<p>To identify which variables contribute to multicollinearity:</p>
<ul>
<li><p>Compute the <strong>Variance Decomposition Proportions (VDP)</strong> for each coefficient across eigenvalues.</p></li>
<li><p>If <strong>two or more variables</strong> have high VDPs (e.g., &gt; 0.5) associated with a <strong>high condition index</strong>, this indicates severe multicollinearity.</p></li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Detects multicollinearity involving <strong>multiple variables</strong>, which VIF may miss.</li>
<li>
<strong>Limitation:</strong> Requires matrix algebra knowledge; less intuitive than VIF or tolerance.</li>
</ul>
<hr>
</div>
<div id="sec-pairwise-correlation-matrix" class="section level3" number="14.6.4">
<h3>
<span class="header-section-number">14.6.4</span> Pairwise Correlation Matrix<a class="anchor" aria-label="anchor" href="#sec-pairwise-correlation-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>Pairwise Correlation Matrix</strong> provides a simple diagnostic by computing the correlation coefficients between each pair of independent variables.</p>
<hr>
<p>For variables <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>, the <strong>correlation coefficient</strong> is:</p>
<p><span class="math display">\[
\rho_{ij} = \frac{\text{Cov}(X_i, X_j)}{\sigma_{X_i} \sigma_{X_j}}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\text{Cov}(X_i, X_j)\)</span> is the covariance,</p></li>
<li><p><span class="math inline">\(\sigma_{X_i}\)</span> and <span class="math inline">\(\sigma_{X_j}\)</span> are standard deviations.</p></li>
</ul>
<hr>
<p><strong>Interpretation of Correlation Coefficients</strong></p>
<ul>
<li>
<span class="math inline">\(|\rho| &lt; 0.5\)</span>: Weak correlation (unlikely to cause multicollinearity).</li>
<li>
<span class="math inline">\(0.5 \leq |\rho| &lt; 0.8\)</span>: Moderate correlation; monitor carefully.</li>
<li>
<span class="math inline">\(|\rho| ≥ 0.8\)</span>: Strong correlation; potential multicollinearity issue.</li>
</ul>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Quick and easy to compute; useful for initial screening.</li>
<li>
<strong>Limitation:</strong> Detects <strong>only pairwise relationships</strong>; may miss multicollinearity involving more than two variables.</li>
</ul>
<hr>
</div>
<div id="sec-determinant-of-the-correlation-matrix" class="section level3" number="14.6.5">
<h3>
<span class="header-section-number">14.6.5</span> Determinant of the Correlation Matrix<a class="anchor" aria-label="anchor" href="#sec-determinant-of-the-correlation-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Determinant of the Correlation Matrix</strong> provides a global measure of multicollinearity. A <strong>small determinant</strong> indicates high multicollinearity.</p>
<ol style="list-style-type: decimal">
<li><p>Form the <strong>correlation matrix</strong> <span class="math inline">\(R\)</span> of the independent variables.</p></li>
<li>
<p>Compute the <strong>determinant</strong>:</p>
<p><span class="math display">\[
\det(R)
\]</span></p>
</li>
</ol>
<hr>
<p><strong>Interpretation</strong></p>
<ul>
<li>
<span class="math inline">\(\det(R) \approx 1\)</span>: No multicollinearity (perfect independence).</li>
<li>
<span class="math inline">\(\det(R) \approx 0\)</span>: Severe multicollinearity.</li>
</ul>
<p>A determinant <strong>close to zero</strong> suggests that the correlation matrix is nearly singular, indicating strong multicollinearity.</p>
<hr>
<p><strong>Advantages and Limitations</strong></p>
<ul>
<li>
<strong>Advantage:</strong> Provides a single summary statistic for overall multicollinearity.</li>
<li>
<strong>Limitation:</strong> Does not indicate <strong>which</strong> variables are causing the problem.</li>
</ul>
<hr>
</div>
<div id="summary-of-multicollinearity-diagnostics" class="section level3" number="14.6.6">
<h3>
<span class="header-section-number">14.6.6</span> Summary of Multicollinearity Diagnostics<a class="anchor" aria-label="anchor" href="#summary-of-multicollinearity-diagnostics"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th><strong>Diagnostic</strong></th>
<th><strong>Type</strong></th>
<th><strong>Key Metric</strong></th>
<th><strong>Threshold for Concern</strong></th>
<th><strong>When to Use</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="model-specification-tests.html#sec-variance-inflation-factor">Variance Inflation Factor</a></td>
<td>Parametric</td>
<td><span class="math inline">\(VIF = \frac{1}{1 - R_j^2}\)</span></td>
<td>
<span class="math inline">\(VIF \geq 5\)</span> (moderate), <span class="math inline">\(VIF \geq 10\)</span> (severe)</td>
<td>General-purpose detection</td>
</tr>
<tr class="even">
<td><a href="model-specification-tests.html#sec-tolerance-statistic">Tolerance Statistic</a></td>
<td>Parametric</td>
<td><span class="math inline">\(1 - R_j^2\)</span></td>
<td>
<span class="math inline">\(&lt; 0.2\)</span> (moderate), <span class="math inline">\(&lt; 0.1\)</span> (severe)</td>
<td>Reciprocal of VIF for variance interpretation</td>
</tr>
<tr class="odd">
<td><a href="model-specification-tests.html#sec-condition-index-and-eigenvalue-decomposition">Condition Index</a></td>
<td>Eigenvalue-based</td>
<td><span class="math inline">\(\sqrt{\frac{\lambda_{\max}}{\lambda_j}}\)</span></td>
<td>
<span class="math inline">\(&gt; 10\)</span> (moderate), <span class="math inline">\(&gt; 30\)</span> (severe)</td>
<td>Detects multicollinearity among multiple variables</td>
</tr>
<tr class="even">
<td><a href="model-specification-tests.html#sec-pairwise-correlation-matrix">Pairwise Correlation Matrix</a></td>
<td>Correlation-based</td>
<td>Pearson correlation (<span class="math inline">\(\rho\)</span>)</td>
<td><span class="math inline">\(|\rho| \geq 0.8\)</span></td>
<td>Initial screening for bivariate correlations</td>
</tr>
<tr class="odd">
<td><a href="model-specification-tests.html#sec-determinant-of-the-correlation-matrix">Determinant of Correlation Matrix</a></td>
<td>Global diagnostic</td>
<td><span class="math inline">\(\det(R)\)</span></td>
<td>
<span class="math inline">\(\approx 0\)</span> indicates severe multicollinearity</td>
<td>Overall assessment of multicollinearity</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="addressing-multicollinearity" class="section level3" number="14.6.7">
<h3>
<span class="header-section-number">14.6.7</span> Addressing Multicollinearity<a class="anchor" aria-label="anchor" href="#addressing-multicollinearity"><i class="fas fa-link"></i></a>
</h3>
<p>If multicollinearity is detected, consider the following solutions:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Remove or combine correlated variables:</strong> Drop one of the correlated predictors or create an index/aggregate.</li>
<li>
<strong>Principal Component Analysis:</strong> Reduce dimensionality by transforming correlated variables into uncorrelated components.</li>
<li>
<a href="linear-regression.html#ridge-regression">Ridge Regression</a> <strong>(L2 regularization):</strong> Introduces a penalty term to stabilize coefficient estimates in the presence of multicollinearity.</li>
<li>
<strong>Centering variables:</strong> Mean-centering can help reduce multicollinearity, especially in interaction terms.</li>
</ol>
<hr>
<p>Multicollinearity can significantly distort regression estimates, leading to misleading interpretations. While <a href="model-specification-tests.html#sec-variance-inflation-factor">VIF</a> and <a href="model-specification-tests.html#sec-tolerance-statistic">Tolerance</a> are commonly used diagnostics, advanced techniques like the <a href="model-specification-tests.html#sec-condition-index-and-eigenvalue-decomposition">Condition Index</a> and Eigenvalue Decomposition provide deeper insights, especially when dealing with complex datasets.</p>
<div class="sourceCode" id="cb546"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install and load necessary libraries</span></span>
<span><span class="co"># install.packages("car")        # For VIF calculation</span></span>
<span><span class="co"># install.packages("corpcor")    # For determinant of correlation matrix</span></span>
<span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://strimmerlab.github.io/software/corpcor/">corpcor</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated dataset with multicollinearity</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">50</span>, sd <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fl">0.8</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>   <span class="co"># Highly correlated with x1</span></span>
<span><span class="va">x3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">30</span>, sd <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">-</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x2</span> <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">x3</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Original regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 1. Variance Inflation Factor (VIF)</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Null Hypothesis: No multicollinearity (VIF = 1)</span></span>
<span><span class="va">vif_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">vif_values</span><span class="op">)</span></span>
<span><span class="co">#&gt;        x1        x2        x3 </span></span>
<span><span class="co">#&gt; 14.969143 14.929013  1.017576</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 2. Tolerance Statistic (Reciprocal of VIF)</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="va">tolerance_values</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">vif_values</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">tolerance_values</span><span class="op">)</span></span>
<span><span class="co">#&gt;         x1         x2         x3 </span></span>
<span><span class="co">#&gt; 0.06680409 0.06698366 0.98272742</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 3. Condition Index and Eigenvalue Decomposition</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># Scaling the independent variables</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">[</span>,<span class="op">-</span><span class="fl">1</span><span class="op">]</span>  <span class="co"># Removing intercept</span></span>
<span><span class="va">eigen_decomp</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>   <span class="co"># Eigenvalue decomposition of the correlation matrix</span></span>
<span></span>
<span><span class="co"># Condition Index</span></span>
<span><span class="va">condition_index</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">eigen_decomp</span><span class="op">$</span><span class="va">values</span><span class="op">)</span> <span class="op">/</span> <span class="va">eigen_decomp</span><span class="op">$</span><span class="va">values</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">condition_index</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.000000 1.435255 7.659566</span></span>
<span></span>
<span><span class="co"># Variance Decomposition Proportions (VDP)</span></span>
<span><span class="co"># Proportions calculated based on the squared coefficients</span></span>
<span><span class="va">loadings</span> <span class="op">&lt;-</span> <span class="va">eigen_decomp</span><span class="op">$</span><span class="va">vectors</span></span>
<span><span class="va">vdp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">loadings</span> <span class="op">^</span> <span class="fl">2</span>, <span class="fl">2</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">vdp</span><span class="op">)</span></span>
<span><span class="co">#&gt;            [,1]       [,2]         [,3]</span></span>
<span><span class="co">#&gt; [1,] 0.48567837 0.01363318 5.006885e-01</span></span>
<span><span class="co">#&gt; [2,] 0.48436754 0.01638399 4.992485e-01</span></span>
<span><span class="co">#&gt; [3,] 0.02995409 0.96998283 6.307954e-05</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 4. Pairwise Correlation Matrix</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="va">correlation_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">correlation_matrix</span><span class="op">)</span></span>
<span><span class="co">#&gt;           x1         x2         x3</span></span>
<span><span class="co">#&gt; x1  1.000000  0.9659070 -0.1291760</span></span>
<span><span class="co">#&gt; x2  0.965907  1.0000000 -0.1185042</span></span>
<span><span class="co">#&gt; x3 -0.129176 -0.1185042  1.0000000</span></span>
<span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="co"># 5. Determinant of the Correlation Matrix</span></span>
<span><span class="co"># ----------------------------------------------------------------------</span></span>
<span><span class="va">determinant_corr_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/Matrix-class.html">det</a></span><span class="op">(</span><span class="va">correlation_matrix</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Determinant of the Correlation Matrix:"</span>,</span>
<span>    <span class="va">determinant_corr_matrix</span>,</span>
<span>    <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Determinant of the Correlation Matrix: 0.06586594</span></span></code></pre></div>
<p><strong>Interpretation of the Results</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Variance Inflation Factor (VIF)</strong></p>
<ul>
<li><p><strong>Formula:</strong> <span class="math inline">\(\text{VIF}_j = \frac{1}{1 - R_j^2}\)</span></p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>VIF</strong> <span class="math inline">\(\approx 1\)</span>: No multicollinearity.</li>
<li>
<span class="math inline">\(1 &lt; \text{VIF} &lt; 5\)</span>: Moderate correlation, usually acceptable.</li>
<li>
<span class="math inline">\(\text{VIF} \ge 5\)</span>: High correlation; investigate further.</li>
<li>
<span class="math inline">\(\text{VIF} \ge 10\)</span>: Severe multicollinearity; corrective action recommended.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Tolerance Statistic</strong></p>
<ul>
<li><p><strong>Formula:</strong> <span class="math inline">\(\text{Tolerance}_j = 1 - R_j^2 = \frac{1}{\text{VIF}_j}\)</span></p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>Tolerance</strong> <span class="math inline">\(&gt; 0.2\)</span>: Low risk of multicollinearity.</li>
<li>
<strong>Tolerance</strong> <span class="math inline">\(&lt; 0.2\)</span>: Possible multicollinearity problem.</li>
<li>
<strong>Tolerance</strong> <span class="math inline">\(&lt; 0.1\)</span>: Severe multicollinearity.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Condition Index and Eigenvalue Decomposition</strong></p>
<ul>
<li><p><strong>Formula:</strong> <span class="math inline">\(\text{CI}_j = \sqrt{\frac{\lambda_{\max}}{\lambda_j}}\)</span></p></li>
<li>
<p><strong>Decision Rule:</strong></p>
<ul>
<li>
<strong>CI</strong> <span class="math inline">\(&lt; 10\)</span>: No significant multicollinearity.</li>
<li>
<span class="math inline">\(10 \le \text{CI} &lt; 30\)</span>: Moderate to strong multicollinearity.</li>
<li>
<strong>CI</strong> <span class="math inline">\(\ge 30\)</span>: Severe multicollinearity.</li>
</ul>
</li>
<li>
<p><strong>Variance Decomposition Proportions (VDP):</strong></p>
<ul>
<li>High VDP (<span class="math inline">\(&gt; 0.5\)</span>) associated with high CI indicates problematic variables.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Pairwise Correlation Matrix</strong></p>
<ul>
<li>
<strong>Decision Rule:</strong>
<ul>
<li>
<span class="math inline">\(|\rho| &lt; 0.5\)</span>: Weak correlation.</li>
<li>
<span class="math inline">\(0.5 \le |\rho| &lt; 0.8\)</span>: Moderate correlation; monitor.</li>
<li>
<span class="math inline">\(|\rho| \ge 0.8\)</span>: Strong correlation; potential multicollinearity issue.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Determinant of the Correlation Matrix</strong></p>
<ul>
<li>
<strong>Decision Rule:</strong>
<ul>
<li>
<span class="math inline">\(\det(R) \approx 1\)</span>: No multicollinearity.</li>
<li>
<span class="math inline">\(\det(R) \approx 0\)</span>: Severe multicollinearity (near-singular matrix).</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<p>Model specification tests are essential for diagnosing and validating econometric models. They ensure that the model assumptions hold true, thereby improving the accuracy and reliability of the estimations. By systematically applying these tests, researchers can identify issues related to nested and non-nested models, heteroskedasticity, functional form, endogeneity, autocorrelation, and multicollinearity, leading to more robust and credible econometric analyses.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></div>
<div class="next"><a href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#model-specification-tests"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li>
<a class="nav-link" href="#nested-model-tests"><span class="header-section-number">14.1</span> Nested Model Tests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-wald-test-nested"><span class="header-section-number">14.1.1</span> Wald Test</a></li>
<li><a class="nav-link" href="#sec-likelihood-ratio-test-nested"><span class="header-section-number">14.1.2</span> Likelihood Ratio Test</a></li>
<li><a class="nav-link" href="#sec-f-test-for-linear-regression-nested"><span class="header-section-number">14.1.3</span> F-Test (for Linear Regression)</a></li>
<li><a class="nav-link" href="#sec-chow-test"><span class="header-section-number">14.1.4</span> Chow Test</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#non-nested-model-tests"><span class="header-section-number">14.2</span> Non-Nested Model Tests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-vuong-test"><span class="header-section-number">14.2.1</span> Vuong Test</a></li>
<li><a class="nav-link" href="#sec-davidson--mackinnon-j-test"><span class="header-section-number">14.2.2</span> Davidson–MacKinnon J-Test</a></li>
<li><a class="nav-link" href="#adjusted-r2"><span class="header-section-number">14.2.3</span> Adjusted \(R^2\)</a></li>
<li><a class="nav-link" href="#comparing-models-with-transformed-dependent-variables"><span class="header-section-number">14.2.4</span> Comparing Models with Transformed Dependent Variables</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#heteroskedasticity-tests"><span class="header-section-number">14.3</span> Heteroskedasticity Tests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-breusch--pagan-test"><span class="header-section-number">14.3.1</span> Breusch–Pagan Test</a></li>
<li><a class="nav-link" href="#sec-white-test-hetero"><span class="header-section-number">14.3.2</span> White Test</a></li>
<li><a class="nav-link" href="#sec-goldfeld--quandt-test"><span class="header-section-number">14.3.3</span> Goldfeld–Quandt Test</a></li>
<li><a class="nav-link" href="#sec-park-test"><span class="header-section-number">14.3.4</span> Park Test</a></li>
<li><a class="nav-link" href="#sec-glejser-test"><span class="header-section-number">14.3.5</span> Glejser Test</a></li>
<li><a class="nav-link" href="#summary-of-heteroskedasticity-tests"><span class="header-section-number">14.3.6</span> Summary of Heteroskedasticity Tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#functional-form-tests"><span class="header-section-number">14.4</span> Functional Form Tests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-ramsey-reset-test"><span class="header-section-number">14.4.1</span> Ramsey RESET Test (Regression Equation Specification Error Test)</a></li>
<li><a class="nav-link" href="#sec-harvey--collier-test"><span class="header-section-number">14.4.2</span> Harvey–Collier Test</a></li>
<li><a class="nav-link" href="#sec-rainbow-test"><span class="header-section-number">14.4.3</span> Rainbow Test</a></li>
<li><a class="nav-link" href="#summary-of-functional-form-tests"><span class="header-section-number">14.4.4</span> Summary of Functional Form Tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#autocorrelation-tests"><span class="header-section-number">14.5</span> Autocorrelation Tests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-durbin--watson-test"><span class="header-section-number">14.5.1</span> Durbin–Watson Test</a></li>
<li><a class="nav-link" href="#sec-breusch--godfrey-test"><span class="header-section-number">14.5.2</span> Breusch–Godfrey Test</a></li>
<li><a class="nav-link" href="#sec-ljung--box-test"><span class="header-section-number">14.5.3</span> Ljung–Box Test (or Box–Pierce Test)</a></li>
<li><a class="nav-link" href="#sec-runs-test"><span class="header-section-number">14.5.4</span> Runs Test</a></li>
<li><a class="nav-link" href="#summary-of-autocorrelation-tests"><span class="header-section-number">14.5.5</span> Summary of Autocorrelation Tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#multicollinearity-diagnostics"><span class="header-section-number">14.6</span> Multicollinearity Diagnostics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-variance-inflation-factor"><span class="header-section-number">14.6.1</span> Variance Inflation Factor</a></li>
<li><a class="nav-link" href="#sec-tolerance-statistic"><span class="header-section-number">14.6.2</span> Tolerance Statistic</a></li>
<li><a class="nav-link" href="#sec-condition-index-and-eigenvalue-decomposition"><span class="header-section-number">14.6.3</span> Condition Index and Eigenvalue Decomposition</a></li>
<li><a class="nav-link" href="#sec-pairwise-correlation-matrix"><span class="header-section-number">14.6.4</span> Pairwise Correlation Matrix</a></li>
<li><a class="nav-link" href="#sec-determinant-of-the-correlation-matrix"><span class="header-section-number">14.6.5</span> Determinant of the Correlation Matrix</a></li>
<li><a class="nav-link" href="#summary-of-multicollinearity-diagnostics"><span class="header-section-number">14.6.6</span> Summary of Multicollinearity Diagnostics</a></li>
<li><a class="nav-link" href="#addressing-multicollinearity"><span class="header-section-number">14.6.7</span> Addressing Multicollinearity</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/14-model_specification.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/14-model_specification.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-04-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
