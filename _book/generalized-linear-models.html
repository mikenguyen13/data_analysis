<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Generalized Linear Models | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Generalized Linear Models (GLMs) extend the traditional linear regression framework to accommodate response variables that do not necessarily follow a normal distribution. They provide a flexible...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 7 Generalized Linear Models | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/generalized-linear-models.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Generalized Linear Models (GLMs) extend the traditional linear regression framework to accommodate response variables that do not necessarily follow a normal distribution. They provide a flexible...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 7 Generalized Linear Models | A Guide on Data Analysis">
<meta name="twitter:description" content="Generalized Linear Models (GLMs) extend the traditional linear regression framework to accommodate response variables that do not necessarily follow a normal distribution. They provide a flexible...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="active" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="sec-endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="sec-directed-acyclic-graphs.html"><span class="header-section-number">38</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="sec-controls.html"><span class="header-section-number">39</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="generalized-linear-models" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Generalized Linear Models<a class="anchor" aria-label="anchor" href="#generalized-linear-models"><i class="fas fa-link"></i></a>
</h1>
<p>Generalized Linear Models (GLMs) extend the traditional linear regression framework to accommodate response variables that do not necessarily follow a normal distribution. They provide a <strong>flexible approach</strong> to modeling relationships between a set of predictors and various types of dependent variables.</p>
<p>While <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> regression assumes that the response variable is continuous and normally distributed, GLMs allow for response variables that follow distributions from the <strong>exponential family</strong>, such as <strong>binomial, Poisson, and gamma distributions</strong>. This flexibility makes them particularly useful in a wide range of business and research applications.</p>
<p>A GLM consists of three key components:</p>
<ol style="list-style-type: decimal">
<li>
<strong>A random component</strong>: The response variable <span class="math inline">\(Y_i\)</span> follows a distribution from the exponential family (e.g., binomial, Poisson, gamma).</li>
<li>
<strong>A systematic component</strong>: A linear predictor <span class="math inline">\(\eta_i = \mathbf{x'_i} \beta\)</span>, where <span class="math inline">\(\mathbf{x'_i}\)</span> is a vector of observed covariates (predictor variables) and <span class="math inline">\(\beta\)</span> is a vector of parameters to be estimated.</li>
<li>
<strong>A link function</strong>: A function <span class="math inline">\(g(\cdot)\)</span> that relates the expected value of the response variable, <span class="math inline">\(\mu_i = E(Y_i)\)</span>, to the linear predictor (i.e., <span class="math inline">\(\eta_i = g(\mu_i)\)</span>).</li>
</ol>
<p>Although the relationship between the predictors and the outcome <em>may appear nonlinear</em> on the original outcome scale (due to the link function), a GLM is still considered “linear” in the statistical sense because it remains linear in the parameters <span class="math inline">\(\beta\)</span>. Consequently, GLMs are <strong>not</strong> generally classified as nonlinear regression models. They “generalize” the traditional linear model by allowing for a broader range of response variable distributions and link functions, but retain linearity in their parameters.</p>
<p>The choice of <strong>distribution</strong> and <strong>link function</strong> depends on the nature of the response variable. In the following sections, we will explore several important GLM variants:</p>
<ul>
<li>
<a href="generalized-linear-models.html#sec-logistic-regression">Logistic Regression</a>: Used for binary response variables (e.g., customer churn, loan defaults).</li>
<li>
<a href="generalized-linear-models.html#sec-probit-regression">Probit Regression</a>: Similar to logistic regression but assumes a normal distribution for the underlying probability.</li>
<li>
<a href="generalized-linear-models.html#sec-poisson-regression">Poisson Regression</a>: Used for modeling count data (e.g., number of purchases, call center inquiries).</li>
<li>
<a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial Regression</a>: An extension of Poisson regression that accounts for overdispersion in count data.</li>
<li>
<a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson Regression</a>: A variation of Poisson regression that adjusts for overdispersion by allowing the variance to be a linear function of the mean.</li>
<li>
<a href="generalized-linear-models.html#sec-multinomial-logistic-regression">Multinomial Logistic Regression</a>: A generalization of logistic regression for categorical response variables with more than two outcomes.</li>
<li>
<a href="generalized-linear-models.html#sec-generalization-of-generalized-linear-models">Generalization of Generalized Linear Model</a>: A flexible generalization of ordinary linear regression that allows for response variables with different distributions (e.g., normal, binomial, Poisson).</li>
</ul>
<div id="sec-logistic-regression" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#sec-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Logistic regression is a widely used <a href="generalized-linear-models.html#generalized-linear-models">Generalized Linear Model</a> designed for modeling binary response variables. It is particularly useful in applications such as credit scoring, medical diagnosis, and customer churn prediction.</p>
<div id="logistic-model" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">7.1.1</span> Logistic Model<a class="anchor" aria-label="anchor" href="#logistic-model"><i class="fas fa-link"></i></a>
</h3>
<p>Given a set of predictor variables <span class="math inline">\(\mathbf{x}_i\)</span>, the probability of a positive outcome (e.g., success, event occurring) is modeled as:</p>
<p><span class="math display">\[
p_i = f(\mathbf{x}_i ; \beta) = \frac{\exp(\mathbf{x_i'\beta})}{1 + \exp(\mathbf{x_i'\beta})}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(p_i = \mathbb{E}[Y_i]\)</span> is the probability of success for observation <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{x_i}\)</span> is the vector of predictor variables.</li>
<li>
<span class="math inline">\(\beta\)</span> is the vector of model coefficients.</li>
</ul>
<div id="sec-logit-transformation" class="section level4" number="7.1.1.1">
<h4>
<span class="header-section-number">7.1.1.1</span> Logit Transformation<a class="anchor" aria-label="anchor" href="#sec-logit-transformation"><i class="fas fa-link"></i></a>
</h4>
<p>The logistic function can be rewritten in terms of the <strong>log-odds</strong>, also known as the <strong>logit function</strong>:</p>
<p><span class="math display">\[
\text{logit}(p_i) = \log \left(\frac{p_i}{1 - p_i} \right) = \mathbf{x_i'\beta}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\frac{p_i}{1 - p_i}\)</span> represents the <strong>odds</strong> of success (the ratio of the probability of success to the probability of failure).</li>
<li>The logit function <strong>ensures linearity</strong> in the parameters, which aligns with the GLM framework.</li>
</ul>
<p>Thus, logistic regression belongs to the family of <strong>Generalized Linear Models</strong> because <strong>a function of the mean response (logit) is linear in the predictors</strong>.</p>
</div>
</div>
<div id="sec-likelihood-function-logistic" class="section level3" number="7.1.2">
<h3>
<span class="header-section-number">7.1.2</span> Likelihood Function<a class="anchor" aria-label="anchor" href="#sec-likelihood-function-logistic"><i class="fas fa-link"></i></a>
</h3>
<p>Since <span class="math inline">\(Y_i\)</span> follows a <strong>Bernoulli distribution</strong> with probability <span class="math inline">\(p_i\)</span>, the likelihood function for <span class="math inline">\(n\)</span> independent observations is:</p>
<p><span class="math display">\[
L(p_i) = \prod_{i=1}^{n} p_i^{Y_i} (1 - p_i)^{1 - Y_i}
\]</span></p>
<p>By substituting the logistic function for <span class="math inline">\(p_i\)</span>:</p>
<p><span class="math display">\[
p_i = \frac{\exp(\mathbf{x'_i \beta})}{1+\exp(\mathbf{x'_i \beta})}, \quad 1 - p_i = \frac{1}{1+\exp(\mathbf{x'_i \beta})}
\]</span></p>
<p>we obtain:</p>
<p><span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \left( \frac{\exp(\mathbf{x'_i \beta})}{1+\exp(\mathbf{x'_i \beta})} \right)^{Y_i} \left( \frac{1}{1+\exp(\mathbf{x'_i \beta})} \right)^{1 - Y_i}
\]</span></p>
<p>Taking the natural logarithm of the likelihood function gives the <strong>log-likelihood function</strong>:</p>
<p><span class="math display">\[
Q(\beta) = \log L(\beta) = \sum_{i=1}^n Y_i \mathbf{x'_i \beta} - \sum_{i=1}^n \log(1 + \exp(\mathbf{x'_i \beta}))
\]</span></p>
<p>Since this function is <strong>concave</strong>, we can maximize it numerically using <strong>iterative optimization techniques</strong>, such as:</p>
<ul>
<li><strong>Newton-Raphson Method</strong></li>
<li><strong>Fisher Scoring Algorithm</strong></li>
</ul>
<p>These methods allow us to obtain the <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> Estimates of the parameters, <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>Under standard regularity conditions, the <strong>MLEs of logistic regression parameters are asymptotically normal</strong>:</p>
<p><span class="math display">\[
\hat{\beta} \dot{\sim} AN(\beta, [\mathbf{I}(\beta)]^{-1})
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{I}(\beta)\)</span> is the <a href="generalized-linear-models.html#fisher-information-matrix">Fisher Information Matrix</a>, which determines the variance-covariance structure of <span class="math inline">\(\hat{\beta}\)</span>.</li>
</ul>
</div>
<div id="fisher-information-matrix" class="section level3" number="7.1.3">
<h3>
<span class="header-section-number">7.1.3</span> Fisher Information Matrix<a class="anchor" aria-label="anchor" href="#fisher-information-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Fisher Information Matrix</strong> quantifies the amount of information that an observable random variable carries about the <strong>unknown parameter</strong> <span class="math inline">\(\beta\)</span>. It is crucial in estimating the <strong>variance-covariance matrix</strong> of the estimated coefficients in logistic regression.</p>
<p>Mathematically, the Fisher Information Matrix is defined as:</p>
<p><span class="math display">\[
\mathbf{I}(\beta) = E\left[ \frac{\partial \log L(\beta)}{\partial \beta} \frac{\partial \log L(\beta)}{\partial \beta'} \right]
\]</span></p>
<p>which expands to:</p>
<p><span class="math display">\[
\mathbf{I}(\beta) = E\left[ \left(\frac{\partial \log L(\beta)}{\partial \beta_i} \frac{\partial \log L(\beta)}{\partial \beta_j} \right)_{ij} \right]
\]</span></p>
<p>Under <strong>regularity conditions</strong>, the Fisher Information Matrix is equivalent to the <strong>negative expected Hessian matrix</strong>:</p>
<p><span class="math display">\[
\mathbf{I}(\beta) = -E\left[ \frac{\partial^2 \log L(\beta)}{\partial \beta \partial \beta'} \right]
\]</span></p>
<p>which further expands to:</p>
<p><span class="math display">\[
\mathbf{I}(\beta) = -E \left[ \left( \frac{\partial^2 \log L(\beta)}{\partial \beta_i \partial \beta_j} \right)_{ij} \right]
\]</span></p>
<p>This representation is particularly useful because it allows us to compute the Fisher Information Matrix directly from the Hessian of the log-likelihood function.</p>
<hr>
<p>Example: Fisher Information Matrix in Logistic Regression</p>
<p>Consider a <strong>simple logistic regression model</strong> with one predictor:</p>
<p><span class="math display">\[
x_i' \beta = \beta_0 + \beta_1 x_i
\]</span></p>
<p>From the log-<a href="generalized-linear-models.html#sec-likelihood-function-logistic">likelihood function</a>, the second-order partial derivatives are:</p>
<p><span class="math display">\[
\begin{aligned}
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_0} &amp;= \sum_{i=1}^n \frac{\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - \left[\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}\right]^2 &amp; \text{Intercept} \\
&amp;= \sum_{i=1}^n p_i (1-p_i)  \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_1} &amp;= \sum_{i=1}^n \frac{x_i^2\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - \left[\frac{x_i\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}\right]^2 &amp; \text{Slope}\\
&amp;= \sum_{i=1}^n x_i^2p_i (1-p_i)  \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta_0 \partial \beta_1} &amp;= \sum_{i=1}^n \frac{x_i\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - x_i\left[\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}\right]^2 &amp; \text{Cross-derivative}\\
&amp;= \sum_{i=1}^n x_ip_i (1-p_i)
\end{aligned}
\]</span></p>
<p>Combining these elements, the <strong>Fisher Information Matrix</strong> for the logistic regression model is:</p>
<p><span class="math display">\[
\mathbf{I} (\beta) =
\begin{bmatrix}
\sum_{i=1}^{n} p_i(1 - p_i) &amp; \sum_{i=1}^{n} x_i p_i(1 - p_i) \\
\sum_{i=1}^{n} x_i p_i(1 - p_i) &amp; \sum_{i=1}^{n} x_i^2 p_i(1 - p_i)
\end{bmatrix}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(p_i = \frac{\exp(x_i' \beta)}{1+\exp(x_i' \beta)}\)</span> represents the predicted probability.</li>
<li>
<span class="math inline">\(p_i (1 - p_i)\)</span> is the <strong>variance of the Bernoulli response variable</strong>.</li>
<li>The diagonal elements represent the variances of the estimated coefficients.</li>
<li>The off-diagonal elements represent the covariances between <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
</ul>
<p>The inverse of the Fisher Information Matrix provides the <strong>variance-covariance matrix</strong> of the estimated coefficients:</p>
<p><span class="math display">\[
\mathbf{Var}(\hat{\beta}) = \mathbf{I}(\hat{\beta})^{-1}
\]</span></p>
<p>This matrix is essential for:</p>
<ul>
<li>
<strong>Estimating standard errors</strong> of the logistic regression coefficients.</li>
<li>
<strong>Constructing confidence intervals</strong> for <span class="math inline">\(\beta\)</span>.</li>
<li>
<strong>Performing hypothesis tests</strong> (e.g., Wald Test).</li>
</ul>
<hr>
<div class="sourceCode" id="cb258"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary library</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stats</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">1.2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit logistic regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract the Fisher Information Matrix (Negative Hessian)</span></span>
<span><span class="va">fisher_info</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">cov.unscaled</span></span>
<span></span>
<span><span class="co"># Display the Fisher Information Matrix</span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="va">fisher_info</span><span class="op">)</span></span>
<span><span class="co">#&gt;             (Intercept)          x</span></span>
<span><span class="co">#&gt; (Intercept)  0.05718171 0.01564322</span></span>
<span><span class="co">#&gt; x            0.01564322 0.10302992</span></span></code></pre></div>
</div>
<div id="inference-in-logistic-regression" class="section level3" number="7.1.4">
<h3>
<span class="header-section-number">7.1.4</span> Inference in Logistic Regression<a class="anchor" aria-label="anchor" href="#inference-in-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Once we estimate the model parameters <span class="math inline">\(\hat{\beta}\)</span> using <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> Estimation, we can conduct inference to assess the significance of predictors, construct confidence intervals, and perform hypothesis testing. The two most common inference approaches in logistic regression are:</p>
<ol style="list-style-type: decimal">
<li><a href="generalized-linear-models.html#sec-likelihood-ratio-test-logistic">Likelihood Ratio Test</a></li>
<li><a href="generalized-linear-models.html#sec-wald-test-logistic">Wald Statistics</a></li>
</ol>
<p>These tests rely on the <strong>asymptotic normality</strong> of MLEs and the properties of the <a href="generalized-linear-models.html#fisher-information-matrix">Fisher Information Matrix</a>.</p>
<hr>
<div id="sec-likelihood-ratio-test-logistic" class="section level4" number="7.1.4.1">
<h4>
<span class="header-section-number">7.1.4.1</span> Likelihood Ratio Test<a class="anchor" aria-label="anchor" href="#sec-likelihood-ratio-test-logistic"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Likelihood Ratio Test</strong> compares two models:</p>
<ul>
<li>
<strong>Restricted Model</strong>: A simpler model where some parameters are constrained to specific values.</li>
<li>
<strong>Unrestricted Model</strong>: The full model without constraints.</li>
</ul>
<p>To test a hypothesis about a subset of parameters <span class="math inline">\(\beta_1\)</span>, we leave <span class="math inline">\(\beta_2\)</span> (nuisance parameters) unspecified.</p>
<p>Hypothesis Setup:</p>
<p><span class="math display">\[
H_0: \beta_1 = \beta_{1,0}
\]</span></p>
<p>where <span class="math inline">\(\beta_{1,0}\)</span> is a specified value (often zero). Let:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}_{2,0}\)</span> be the <strong>MLE of</strong> <span class="math inline">\(\beta_2\)</span> under the constraint <span class="math inline">\(\beta_1 = \beta_{1,0}\)</span>.</li>
<li>
<span class="math inline">\(\hat{\beta}_1, \hat{\beta}_2\)</span> be the <strong>MLEs under the full model</strong>.</li>
</ul>
<p>The <strong>likelihood ratio test statistic</strong> is:</p>
<p><span class="math display">\[
-2\log\Lambda = -2[\log L(\beta_{1,0}, \hat{\beta}_{2,0}) - \log L(\hat{\beta}_1, \hat{\beta}_2)]
\]</span></p>
<p>where:</p>
<ul>
<li>The first term is the <strong>log-likelihood of the restricted model</strong>.</li>
<li>The second term is the <strong>log-likelihood of the unrestricted model</strong>.</li>
</ul>
<p>Under the null hypothesis:</p>
<p><span class="math display">\[
-2 \log \Lambda \sim \chi^2_{\upsilon}
\]</span></p>
<p>where <span class="math inline">\(\upsilon\)</span> is the number of restricted parameters. We <strong>reject</strong> <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
-2\log \Lambda &gt; \chi^2_{\upsilon,1-\alpha}
\]</span></p>
<p><strong>Interpretation</strong>: If the likelihood ratio test statistic is large, this suggests that the restricted model (under <span class="math inline">\(H_0\)</span>) fits significantly worse than the full model, leading us to reject the null hypothesis.</p>
<hr>
</div>
<div id="sec-wald-test-logistic" class="section level4" number="7.1.4.2">
<h4>
<span class="header-section-number">7.1.4.2</span> Wald Test<a class="anchor" aria-label="anchor" href="#sec-wald-test-logistic"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Wald test</strong> is based on the asymptotic normality of MLEs:</p>
<p><span class="math display">\[
\hat{\beta} \sim AN (\beta, [\mathbf{I}(\beta)]^{-1})
\]</span></p>
<p>We test:</p>
<p><span class="math display">\[
H_0: \mathbf{L} \hat{\beta} = 0
\]</span></p>
<p>where <span class="math inline">\(\mathbf{L}\)</span> is a <span class="math inline">\(q \times p\)</span> matrix with <span class="math inline">\(q\)</span> linearly independent rows (often used to test multiple coefficients simultaneously). The <strong>Wald test statistic</strong> is:</p>
<p><span class="math display">\[
W = (\mathbf{L\hat{\beta}})'(\mathbf{L[I(\hat{\beta})]^{-1}L'})^{-1}(\mathbf{L\hat{\beta}})
\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
W \sim \chi^2_q
\]</span></p>
<p><strong>Interpretation</strong>: If <span class="math inline">\(W\)</span> is large, the null hypothesis is rejected, suggesting that at least one of the tested coefficients is significantly different from zero.</p>
<hr>
<p><strong>Comparing Likelihood Ratio and Wald Tests</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="36%">
<col width="63%">
</colgroup>
<thead><tr class="header">
<th>Test</th>
<th>Best Used When…</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="generalized-linear-models.html#sec-likelihood-ratio-test-logistic">Likelihood Ratio Test</a></td>
<td>More accurate in small samples, providing better control of error rates. Recommended when sample sizes are small.</td>
</tr>
<tr class="even">
<td><a href="generalized-linear-models.html#sec-wald-test-logistic">Wald Test</a></td>
<td>Easier to compute but may be inaccurate in small samples. Recommended when computational efficiency is a priority.</td>
</tr>
</tbody>
</table></div>
<hr>
<div class="sourceCode" id="cb259"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary library</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stats</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate some binary outcome data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">1.2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit logistic regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display model summary (includes Wald tests)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = y ~ x, family = binomial)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)   0.7223     0.2391   3.020 0.002524 ** </span></span>
<span><span class="co">#&gt; x             1.2271     0.3210   3.823 0.000132 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 128.21  on 99  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 108.29  on 98  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 112.29</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span></span>
<span><span class="co"># Perform likelihood ratio test using anova()</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">model</span>, test<span class="op">=</span><span class="st">"Chisq"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Analysis of Deviance Table</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model: binomial, link: logit</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: y</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Terms added sequentially (first to last)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    </span></span>
<span><span class="co">#&gt; NULL                    99     128.21              </span></span>
<span><span class="co">#&gt; x     1   19.913        98     108.29 8.105e-06 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
</div>
<div id="confidence-intervals-for-coefficients" class="section level4" number="7.1.4.3">
<h4>
<span class="header-section-number">7.1.4.3</span> Confidence Intervals for Coefficients<a class="anchor" aria-label="anchor" href="#confidence-intervals-for-coefficients"><i class="fas fa-link"></i></a>
</h4>
<p>A <strong>95% confidence interval</strong> for a logistic regression coefficient <span class="math inline">\(\beta_i\)</span> is given by:</p>
<p><span class="math display">\[
\hat{\beta}_i \pm 1.96 \hat{s}_{ii}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}_i\)</span> is the estimated coefficient.</li>
<li>
<span class="math inline">\(\hat{s}_{ii}\)</span> is the standard error (square root of the diagonal element of <span class="math inline">\(\mathbf{[I(\hat{\beta})]}^{-1}\)</span>).</li>
</ul>
<p>This confidence interval provides a <strong>range of plausible values</strong> for <span class="math inline">\(\beta_i\)</span>. If the interval does not include <strong>zero</strong>, we conclude that <span class="math inline">\(\beta_i\)</span> is statistically significant.</p>
<ul>
<li>
<strong>For large sample sizes</strong>, the <a href="generalized-linear-models.html#sec-likelihood-ratio-test-logistic">Likelihood Ratio Test</a> and <a href="generalized-linear-models.html#sec-wald-test-logistic">Wald Test</a> yield similar results.</li>
<li>
<strong>For small sample sizes</strong>, the <a href="generalized-linear-models.html#sec-likelihood-ratio-test-logistic">Likelihood Ratio Test</a> is preferred because the <a href="generalized-linear-models.html#sec-wald-test-logistic">Wald test</a> can be less reliable.</li>
</ul>
<hr>
</div>
<div id="interpretation-of-logistic-regression-coefficients" class="section level4" number="7.1.4.4">
<h4>
<span class="header-section-number">7.1.4.4</span> Interpretation of Logistic Regression Coefficients<a class="anchor" aria-label="anchor" href="#interpretation-of-logistic-regression-coefficients"><i class="fas fa-link"></i></a>
</h4>
<p>For a <strong>single predictor variable</strong>, the logistic regression model is:</p>
<p><span class="math display">\[
\text{logit}(\hat{p}_i) = \log\left(\frac{\hat{p}_i}{1 - \hat{p}_i} \right) = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{p}_i\)</span> is the predicted probability of success at <span class="math inline">\(x_i\)</span>.</li>
<li>
<span class="math inline">\(\hat{\beta}_1\)</span> represents the <strong>log odds change</strong> for a one-unit increase in <span class="math inline">\(x\)</span>.</li>
</ul>
<p><strong>Interpreting</strong> <span class="math inline">\(\beta_1\)</span> <strong>in Terms of Odds</strong></p>
<p>When the predictor variable increases by <strong>one unit</strong>, the logit of the probability changes by <span class="math inline">\(\hat{\beta}_1\)</span>:</p>
<p><span class="math display">\[
\text{logit}(\hat{p}_{x_i +1}) = \hat{\beta}_0 + \hat{\beta}_1 (x_i + 1) = \text{logit}(\hat{p}_{x_i}) + \hat{\beta}_1
\]</span></p>
<p>Thus, the difference in log odds is:</p>
<p><span class="math display">\[
\begin{aligned}
\text{logit}(\hat{p}_{x_i +1}) - \text{logit}(\hat{p}_{x_i})
&amp;= \log ( \text{odds}(\hat{p}_{x_i + 1})) - \log (\text{odds}(\hat{p}_{x_i}) )\\
&amp;= \log\left( \frac{\text{odds}(\hat{p}_{x_i + 1})}{\text{odds}(\hat{p}_{x_i})} \right) \\
&amp;= \hat{\beta}_1
\end{aligned}
\]</span></p>
<p>Exponentiating both sides:</p>
<p><span class="math display">\[
\exp(\hat{\beta}_1) = \frac{\text{odds}(\hat{p}_{x_i + 1})}{\text{odds}(\hat{p}_{x_i})}
\]</span></p>
<p>This quantity, <span class="math inline">\(\exp(\hat{\beta}_1)\)</span>, is the <strong>odds ratio</strong>, which quantifies the effect of a one-unit increase in <span class="math inline">\(x\)</span> on the odds of success.</p>
<hr>
<p><strong>Generalization: Odds Ratio for Any Change in</strong> <span class="math inline">\(x\)</span></p>
<p>For a difference of <span class="math inline">\(c\)</span> units in the predictor <span class="math inline">\(x\)</span>, the estimated odds ratio is:</p>
<p><span class="math display">\[
\exp(c\hat{\beta}_1)
\]</span></p>
<p>For multiple predictors, <span class="math inline">\(\exp(\hat{\beta}_k)\)</span> represents the odds ratio for <span class="math inline">\(x_k\)</span>, holding all other variables constant.</p>
<hr>
</div>
<div id="inference-on-the-mean-response" class="section level4" number="7.1.4.5">
<h4>
<span class="header-section-number">7.1.4.5</span> Inference on the Mean Response<a class="anchor" aria-label="anchor" href="#inference-on-the-mean-response"><i class="fas fa-link"></i></a>
</h4>
<p>For a given set of predictor values <span class="math inline">\(x_h = (1, x_{h1}, ..., x_{h,p-1})'\)</span>, the estimated <strong>mean response</strong> (probability of success) is:</p>
<p><span class="math display">\[
\hat{p}_h = \frac{\exp(\mathbf{x'_h \hat{\beta}})}{1 + \exp(\mathbf{x'_h \hat{\beta}})}
\]</span></p>
<p>The <strong>variance of the estimated probability</strong> is:</p>
<p><span class="math display">\[
s^2(\hat{p}_h) = \mathbf{x'_h[I(\hat{\beta})]^{-1}x_h}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{I}(\hat{\beta})^{-1}\)</span> is the <strong>variance-covariance matrix</strong> of <span class="math inline">\(\hat{\beta}\)</span>.</li>
<li>
<span class="math inline">\(s^2(\hat{p}_h)\)</span> provides an estimate of <strong>uncertainty</strong> in <span class="math inline">\(\hat{p}_h\)</span>.</li>
</ul>
<p>In many applications, logistic regression is used for <strong>classification</strong>, where we predict whether an observation belongs to <strong>category 0 or 1</strong>. A commonly used decision rule is:</p>
<ul>
<li>Assign <span class="math inline">\(y = 1\)</span> if <span class="math inline">\(\hat{p}_h \geq \tau\)</span>
</li>
<li>Assign <span class="math inline">\(y = 0\)</span> if <span class="math inline">\(\hat{p}_h &lt; \tau\)</span>
</li>
</ul>
<p>where <span class="math inline">\(\tau\)</span> is a chosen cutoff threshold (typically <span class="math inline">\(\tau = 0.5\)</span>).</p>
<hr>
<div class="sourceCode" id="cb260"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary library</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stats</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">1.2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit logistic regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = y ~ x, family = binomial)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)   0.7223     0.2391   3.020 0.002524 ** </span></span>
<span><span class="co">#&gt; x             1.2271     0.3210   3.823 0.000132 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 128.21  on 99  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 108.29  on 98  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 112.29</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span></span>
<span><span class="co"># Extract coefficients and standard errors</span></span>
<span><span class="va">coef_estimates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">beta_hat</span> <span class="op">&lt;-</span> <span class="va">coef_estimates</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>   <span class="co"># Estimated coefficients</span></span>
<span><span class="va">se_beta</span>  <span class="op">&lt;-</span> <span class="va">coef_estimates</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>   <span class="co"># Standard errors</span></span>
<span></span>
<span><span class="co"># Compute 95% confidence intervals for coefficients</span></span>
<span><span class="va">conf_intervals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span></span>
<span>  <span class="va">beta_hat</span> <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">se_beta</span>, </span>
<span>  <span class="va">beta_hat</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">se_beta</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Odds Ratios</span></span>
<span><span class="va">odds_ratios</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">beta_hat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="st">"Confidence Intervals for Coefficients:"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Confidence Intervals for Coefficients:"</span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="va">conf_intervals</span><span class="op">)</span></span>
<span><span class="co">#&gt;                  [,1]     [,2]</span></span>
<span><span class="co">#&gt; (Intercept) 0.2535704 1.190948</span></span>
<span><span class="co">#&gt; x           0.5979658 1.856218</span></span>
<span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="st">"Odds Ratios:"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Odds Ratios:"</span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="va">odds_ratios</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)           x </span></span>
<span><span class="co">#&gt;    2.059080    3.411295</span></span>
<span></span>
<span><span class="co"># Predict probability for a new observation (e.g., x = 1)</span></span>
<span><span class="va">new_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">predicted_prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model</span>, newdata <span class="op">=</span> <span class="va">new_x</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="st">"Predicted Probability for x = 1:"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Predicted Probability for x = 1:"</span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="va">predicted_prob</span><span class="op">)</span></span>
<span><span class="co">#&gt;         1 </span></span>
<span><span class="co">#&gt; 0.8753759</span></span></code></pre></div>
</div>
</div>
<div id="application-logistic-regression" class="section level3" number="7.1.5">
<h3>
<span class="header-section-number">7.1.5</span> Application: Logistic Regression<a class="anchor" aria-label="anchor" href="#application-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p>In this section, we demonstrate the application of <strong>logistic regression</strong> using simulated data. We explore model fitting, inference, residual analysis, and goodness-of-fit testing.</p>
<p><strong>1. Load Required Libraries</strong></p>
<div class="sourceCode" id="cb261"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haozhu233.github.io/kableExtra/">kableExtra</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/atahk/pscl">pscl</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/julianfaraway/faraway">faraway</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://kwstat.github.io/agridat/">agridat</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/lbbe-software/nlstools">nlstools</a></span><span class="op">)</span></span></code></pre></div>
<p><strong>2. Data Generation</strong></p>
<p>We generate a dataset where the predictor variable <span class="math inline">\(X\)</span> follows a <strong>uniform distribution</strong>:</p>
<p><span class="math display">\[
x \sim Unif(-0.5,2.5)
\]</span></p>
<p>The <strong>linear predictor</strong> is given by:</p>
<p><span class="math display">\[
\eta = 0.5 + 0.75 x
\]</span></p>
<p>Passing <span class="math inline">\(\eta\)</span> into the inverse-logit function, we obtain:</p>
<p><span class="math display">\[
p = \frac{\exp(\eta)}{1+ \exp(\eta)}
\]</span></p>
<p>which ensures that <span class="math inline">\(p \in [0,1]\)</span>. We then generate the binary response variable:</p>
<p><span class="math display">\[
y \sim Bernoulli(p)
\]</span></p>
<div class="sourceCode" id="cb262"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">23</span><span class="op">)</span> <span class="co"># Set seed for reproducibility</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, max <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span>  <span class="co"># Generate X values</span></span>
<span><span class="va">eta1</span> <span class="op">&lt;-</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.75</span> <span class="op">*</span> <span class="va">x</span>                   <span class="co"># Compute linear predictor</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">eta1</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">eta1</span><span class="op">)</span><span class="op">)</span>          <span class="co"># Compute probabilities</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">1</span>, <span class="va">p</span><span class="op">)</span>                   <span class="co"># Generate binary response</span></span>
<span><span class="va">BinData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">x</span>, Y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>       <span class="co"># Create data frame</span></span></code></pre></div>
<p><strong>3. Model Fitting</strong></p>
<p>We fit a <strong>logistic regression model</strong> to the simulated data:</p>
<p><span class="math display">\[
\text{logit}(p) = \beta_0 + \beta_1 X
\]</span></p>
<div class="sourceCode" id="cb263"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Logistic_Model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">Y</span> <span class="op">~</span> <span class="va">X</span>,</span>
<span>                      <span class="co"># Specifies the response distribution</span></span>
<span>                      family <span class="op">=</span> <span class="va">binomial</span>,</span>
<span>                      data <span class="op">=</span> <span class="va">BinData</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Logistic_Model</span><span class="op">)</span> <span class="co"># Model summary</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Y ~ X, family = binomial, data = BinData)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.46205    0.10201   4.530 5.91e-06 ***</span></span>
<span><span class="co">#&gt; X            0.78527    0.09296   8.447  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1106.7  on 999  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1027.4  on 998  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 1031.4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span><span class="fu">nlstools</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/nlstools/man/confint2.html">confint2</a></span><span class="op">(</span><span class="va">Logistic_Model</span><span class="op">)</span> <span class="co"># Confidence intervals</span></span>
<span><span class="co">#&gt;                 2.5 %    97.5 %</span></span>
<span><span class="co">#&gt; (Intercept) 0.2618709 0.6622204</span></span>
<span><span class="co">#&gt; X           0.6028433 0.9676934</span></span>
<span></span>
<span><span class="co"># Compute odds ratios</span></span>
<span><span class="va">OddsRatio</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">Logistic_Model</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">exp</span></span>
<span><span class="va">OddsRatio</span></span>
<span><span class="co">#&gt; (Intercept)           X </span></span>
<span><span class="co">#&gt;    1.587318    2.192995</span></span></code></pre></div>
<p>Interpretation of the Odds Ratio</p>
<ul>
<li><p>When <span class="math inline">\(x = 0\)</span>, the <strong>odds of success</strong> are <strong>1.59</strong>.</p></li>
<li><p>When <span class="math inline">\(x = 1\)</span>, the <strong>odds of success increase by a factor of 2.19</strong>, indicating a <strong>119.29% increase</strong>.</p></li>
</ul>
<p><strong>4. Deviance Test</strong></p>
<p>We assess the model’s significance using the <strong>deviance test</strong>, which compares:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: No predictors are related to the response (intercept-only model).</p></li>
<li><p><span class="math inline">\(H_1\)</span>: At least one predictor is related to the response.</p></li>
</ul>
<p>The <strong>test statistic</strong> is:</p>
<p><span class="math display">\[
D = \text{Null Deviance} - \text{Residual Deviance}
\]</span></p>
<div class="sourceCode" id="cb264"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Test_Dev</span> <span class="op">&lt;-</span> <span class="va">Logistic_Model</span><span class="op">$</span><span class="va">null.deviance</span> <span class="op">-</span> <span class="va">Logistic_Model</span><span class="op">$</span><span class="va">deviance</span></span>
<span><span class="va">p_val_dev</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span>q <span class="op">=</span> <span class="va">Test_Dev</span>, df <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">p_val_dev</span></span>
<span><span class="co">#&gt; [1] 0</span></span></code></pre></div>
<p><strong>Conclusion:</strong></p>
<p>Since the <strong>p-value is approximately 0</strong>, we <strong>reject</strong> <span class="math inline">\(H_0\)</span>, confirming that <span class="math inline">\(X\)</span> is significantly related to <span class="math inline">\(Y\)</span>.</p>
<p><strong>5. Residual Analysis</strong></p>
<p>We compute <strong>deviance residuals</strong> and plot them against <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode" id="cb265"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Logistic_Resids</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">Logistic_Model</span>, type <span class="op">=</span> <span class="st">"deviance"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    y <span class="op">=</span> <span class="va">Logistic_Resids</span>,</span>
<span>    x <span class="op">=</span> <span class="va">BinData</span><span class="op">$</span><span class="va">X</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">'X'</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">'Deviance Residuals'</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>This plot is not very informative. A more insightful approach is <strong>binned residual plots</strong>.</p>
<p><strong>6. Binned Residual Plot</strong></p>
<p>We group residuals into <strong>bins</strong> based on predicted values.</p>
<div class="sourceCode" id="cb266"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">plot_bin</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">Y</span>,</span>
<span>                     <span class="va">X</span>,</span>
<span>                     <span class="va">bins</span> <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                     <span class="va">return.DF</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">Y_Name</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html">deparse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substitute.html">substitute</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">X_Name</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html">deparse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substitute.html">substitute</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">Binned_Plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Plot_Y <span class="op">=</span> <span class="va">Y</span>, Plot_X <span class="op">=</span> <span class="va">X</span><span class="op">)</span></span>
<span>  <span class="va">Binned_Plot</span><span class="op">$</span><span class="va">bin</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span><span class="op">(</span><span class="va">Binned_Plot</span><span class="op">$</span><span class="va">Plot_X</span>, breaks <span class="op">=</span> <span class="va">bins</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.numeric</span></span>
<span>  </span>
<span>  <span class="va">Binned_Plot_summary</span> <span class="op">&lt;-</span> <span class="va">Binned_Plot</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">bin</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span></span>
<span>      Y_ave <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Plot_Y</span><span class="op">)</span>,</span>
<span>      X_ave <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Plot_X</span><span class="op">)</span>,</span>
<span>      Count <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.data.frame</span></span>
<span>  </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    y <span class="op">=</span> <span class="va">Binned_Plot_summary</span><span class="op">$</span><span class="va">Y_ave</span>,</span>
<span>    x <span class="op">=</span> <span class="va">Binned_Plot_summary</span><span class="op">$</span><span class="va">X_ave</span>,</span>
<span>    ylab <span class="op">=</span> <span class="va">Y_Name</span>,</span>
<span>    xlab <span class="op">=</span> <span class="va">X_Name</span></span>
<span>  <span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">return.DF</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">Binned_Plot_summary</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu">plot_bin</span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Logistic_Resids</span>, X <span class="op">=</span> <span class="va">BinData</span><span class="op">$</span><span class="va">X</span>, bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-8-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>We also examine <strong>predicted values vs residuals</strong>:</p>
<div class="sourceCode" id="cb267"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Logistic_Predictions</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Logistic_Model</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="fu">plot_bin</span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Logistic_Resids</span>, X <span class="op">=</span> <span class="va">Logistic_Predictions</span>, bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Finally, we compare <strong>predicted probabilities</strong> to actual outcomes:</p>
<div class="sourceCode" id="cb268"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">NumBins</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">Binned_Data</span> <span class="op">&lt;-</span> <span class="fu">plot_bin</span><span class="op">(</span></span>
<span>    Y <span class="op">=</span> <span class="va">BinData</span><span class="op">$</span><span class="va">Y</span>,</span>
<span>    X <span class="op">=</span> <span class="va">Logistic_Predictions</span>,</span>
<span>    bins <span class="op">=</span> <span class="va">NumBins</span>,</span>
<span>    return.DF <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">Binned_Data</span></span>
<span><span class="co">#&gt;    bin     Y_ave     X_ave Count</span></span>
<span><span class="co">#&gt; 1    1 0.5833333 0.5382095    72</span></span>
<span><span class="co">#&gt; 2    2 0.5200000 0.5795887    75</span></span>
<span><span class="co">#&gt; 3    3 0.6567164 0.6156540    67</span></span>
<span><span class="co">#&gt; 4    4 0.7014925 0.6579674    67</span></span>
<span><span class="co">#&gt; 5    5 0.6373626 0.6984765    91</span></span>
<span><span class="co">#&gt; 6    6 0.7500000 0.7373341    72</span></span>
<span><span class="co">#&gt; 7    7 0.7096774 0.7786747    93</span></span>
<span><span class="co">#&gt; 8    8 0.8503937 0.8203819   127</span></span>
<span><span class="co">#&gt; 9    9 0.8947368 0.8601232   133</span></span>
<span><span class="co">#&gt; 10  10 0.8916256 0.9004734   203</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, lty <span class="op">=</span> <span class="fl">2</span>, col <span class="op">=</span> <span class="st">'blue'</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-10-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>7. Model Goodness-of-Fit: Hosmer-Lemeshow Test</strong></p>
<p>The <strong>Hosmer-Lemeshow test</strong> evaluates whether the model fits the data well. The test statistic is: <span class="math display">\[
X^2_{HL} = \sum_{j=1}^{J} \frac{(y_j - m_j \hat{p}_j)^2}{m_j \hat{p}_j(1-\hat{p}_j)}
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(y_j\)</span> is the observed number of successes in bin <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(m_j\)</span> is the number of observations in bin <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{p}_j\)</span> is the predicted probability in bin <span class="math inline">\(j\)</span>.</p></li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>, we assume:</p>
<p><span class="math display">\[
X^2_{HL} \sim \chi^2_{J-1}
\]</span></p>
<div class="sourceCode" id="cb269"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">HL_BinVals</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">Binned_Data</span><span class="op">$</span><span class="va">Count</span> <span class="op">*</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">Y_ave</span> <span class="op">-</span> </span>
<span>               <span class="va">Binned_Data</span><span class="op">$</span><span class="va">Count</span> <span class="op">*</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">X_ave</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span>   </span>
<span>               <span class="op">(</span><span class="va">Binned_Data</span><span class="op">$</span><span class="va">Count</span> <span class="op">*</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">X_ave</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">X_ave</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">HLpval</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span>q <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">HL_BinVals</span><span class="op">)</span>,</span>
<span>                 df <span class="op">=</span> <span class="va">NumBins</span> <span class="op">-</span> <span class="fl">1</span>,</span>
<span>                 lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">HLpval</span></span>
<span><span class="co">#&gt; [1] 0.4150004</span></span></code></pre></div>
<p><strong>Conclusion:</strong></p>
<ul>
<li><p>Since <span class="math inline">\(p\)</span>-value = 0.99, we <strong>fail to reject</strong> <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>This indicates that <strong>the model fits the data well</strong>.</p></li>
</ul>
<hr>
</div>
</div>
<div id="sec-probit-regression" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Probit Regression<a class="anchor" aria-label="anchor" href="#sec-probit-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Probit regression is a type of <a href="generalized-linear-models.html#generalized-linear-models">Generalized Linear Models</a> used for binary outcome variables. Unlike <a href="generalized-linear-models.html#sec-logistic-regression">logistic regression</a>, which uses the <a href="generalized-linear-models.html#sec-logit-transformation">logit function</a>, probit regression assumes that the probability of success is determined by an <strong>underlying normally distributed latent variable</strong>.</p>
<div id="probit-model" class="section level3" number="7.2.1">
<h3>
<span class="header-section-number">7.2.1</span> Probit Model<a class="anchor" aria-label="anchor" href="#probit-model"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(Y_i\)</span> be a binary response variable:</p>
<p><span class="math display">\[
Y_i =
\begin{cases}
1, &amp; \text{if success occurs} \\
0, &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>We assume that <span class="math inline">\(Y_i\)</span> follows a <strong>Bernoulli distribution</strong>:</p>
<p><span class="math display">\[
Y_i \sim \text{Bernoulli}(p_i), \quad \text{where } p_i = P(Y_i = 1 | \mathbf{x_i})
\]</span></p>
<p>Instead of the <a href="generalized-linear-models.html#sec-logit-transformation">logit function</a> in <a href="generalized-linear-models.html#sec-logistic-regression">logistic regression</a>:</p>
<p><span class="math display">\[
\text{logit}(p_i) = \log\left( \frac{p_i}{1 - p_i} \right) = \mathbf{x_i'\beta}
\]</span></p>
<p>Probit regression uses the <strong>inverse standard normal CDF</strong>:</p>
<p><span class="math display">\[
\Phi^{-1}(p_i) = \mathbf{x_i'\theta}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\Phi(\cdot)\)</span> is the <strong>CDF of the standard normal distribution</strong>.</p></li>
<li><p><span class="math inline">\(\mathbf{x_i}\)</span> is the vector of predictors.</p></li>
<li><p><span class="math inline">\(\theta\)</span> is the vector of regression coefficients.</p></li>
</ul>
<p>Thus, the probability of success is:</p>
<p><span class="math display">\[
p_i = P(Y_i = 1 | \mathbf{x_i}) = \Phi(\mathbf{x_i'\theta})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\Phi(z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt
\]</span></p>
<hr>
</div>
<div id="application-probit-regression" class="section level3" number="7.2.2">
<h3>
<span class="header-section-number">7.2.2</span> Application: Probit Regression<a class="anchor" aria-label="anchor" href="#application-probit-regression"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb270"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary library</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate data</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">latent</span> <span class="op">&lt;-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">+</span> <span class="fl">0.7</span> <span class="op">*</span> <span class="va">x2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>  <span class="co"># Linear combination</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">latent</span> <span class="op">&gt;</span> <span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>  <span class="co"># Binary outcome</span></span>
<span></span>
<span><span class="co"># Create dataframe</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x1</span>, <span class="va">x2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit Probit model</span></span>
<span><span class="va">probit_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"probit"</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">probit_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = y ~ x1 + x2, family = binomial(link = "probit"), </span></span>
<span><span class="co">#&gt;     data = data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.09781    0.04499  -2.174   0.0297 *  </span></span>
<span><span class="co">#&gt; x1           0.43838    0.04891   8.963   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x2           0.75538    0.05306  14.235   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1385.1  on 999  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1045.3  on 997  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 1051.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span>
<span></span>
<span><span class="co"># Fit Logit model</span></span>
<span><span class="va">logit_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"logit"</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">logit_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = y ~ x1 + x2, family = binomial(link = "logit"), </span></span>
<span><span class="co">#&gt;     data = data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.16562    0.07600  -2.179   0.0293 *  </span></span>
<span><span class="co">#&gt; x1           0.73234    0.08507   8.608   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x2           1.25220    0.09486  13.201   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1385.1  on 999  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1048.4  on 997  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 1054.4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span></span>
<span><span class="co"># Compare Coefficients</span></span>
<span><span class="va">coef_comparison</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Variable <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">probit_model</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    Probit_Coef <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">probit_model</span><span class="op">)</span>,</span>
<span>    Logit_Coef <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">logit_model</span><span class="op">)</span>,</span>
<span>    Logit_Probit_Ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">logit_model</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">probit_model</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="va">coef_comparison</span><span class="op">)</span></span>
<span><span class="co">#&gt;                Variable Probit_Coef Logit_Coef Logit_Probit_Ratio</span></span>
<span><span class="co">#&gt; (Intercept) (Intercept) -0.09780689 -0.1656216           1.693353</span></span>
<span><span class="co">#&gt; x1                   x1  0.43837627  0.7323392           1.670572</span></span>
<span><span class="co">#&gt; x2                   x2  0.75538259  1.2522008           1.657704</span></span>
<span></span>
<span><span class="co"># Compute predicted probabilities</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">probit_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">probit_model</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">logit_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">logit_model</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot Probit vs Logit predictions</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">probit_pred</span>, y <span class="op">=</span> <span class="va">logit_pred</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_abline</a></span><span class="op">(</span>slope <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                intercept <span class="op">=</span> <span class="fl">0</span>,</span>
<span>                col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Comparison of Predicted Probabilities"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Probit Predictions"</span>, y <span class="op">=</span> <span class="st">"Logit Predictions"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-12-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb271"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Classification Accuracy</span></span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">probit_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">probit_pred</span> <span class="op">&gt;</span> <span class="va">threshold</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">logit_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">logit_pred</span> <span class="op">&gt;</span> <span class="va">threshold</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="va">probit_acc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">probit_class</span> <span class="op">==</span> <span class="va">data</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">logit_acc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">logit_class</span> <span class="op">==</span> <span class="va">data</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Probit Accuracy:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">probit_acc</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Probit Accuracy: 0.71"</span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Logit Accuracy:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">logit_acc</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Logit Accuracy: 0.71"</span></span></code></pre></div>
</div>
</div>
<div id="sec-binomial-regression" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Binomial Regression<a class="anchor" aria-label="anchor" href="#sec-binomial-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In previous sections, we introduced <strong>binomial regression models</strong>, including both <a href="generalized-linear-models.html#sec-logistic-regression">Logistic Regression</a> and <a href="An%20alternative%20to%20logistic%20regression,%20commonly%20used%20in%20decision%20modeling.">probit regression</a>, and discussed their theoretical foundations. Now, we apply these methods to real-world data using the <code>esoph</code> dataset, which examines the relationship between esophageal cancer and potential risk factors such as <strong>alcohol consumption</strong> and <strong>age group</strong>.</p>
<div id="dataset-overview" class="section level3" number="7.3.1">
<h3>
<span class="header-section-number">7.3.1</span> Dataset Overview<a class="anchor" aria-label="anchor" href="#dataset-overview"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>esoph</code> dataset consists of:</p>
<ul>
<li><p><strong>Successes (<code>ncases</code>)</strong>: The number of individuals diagnosed with esophageal cancer.</p></li>
<li><p><strong>Failures (<code>ncontrols</code>)</strong>: The number of individuals in the control group (without cancer).</p></li>
<li>
<p><strong>Predictors</strong>:</p>
<ul>
<li><p><code>agegp</code>: Age group of individuals.</p></li>
<li><p><code>alcgp</code>: Alcohol consumption category.</p></li>
<li><p><code>tobgp</code>: Tobacco consumption category.</p></li>
</ul>
</li>
</ul>
<p>Before fitting our models, let’s inspect the dataset and visualize some key relationships.</p>
<div class="sourceCode" id="cb272"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load and inspect the dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"esoph"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">esoph</span>, n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;   agegp     alcgp    tobgp ncases ncontrols</span></span>
<span><span class="co">#&gt; 1 25-34 0-39g/day 0-9g/day      0        40</span></span>
<span><span class="co">#&gt; 2 25-34 0-39g/day    10-19      0        10</span></span>
<span><span class="co">#&gt; 3 25-34 0-39g/day    20-29      0         6</span></span>
<span></span>
<span><span class="co"># Visualizing the proportion of cancer cases by alcohol consumption</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>  <span class="va">esoph</span><span class="op">$</span><span class="va">ncases</span> <span class="op">/</span> <span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">ncases</span> <span class="op">+</span> <span class="va">esoph</span><span class="op">$</span><span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">esoph</span><span class="op">$</span><span class="va">alcgp</span>,</span>
<span>  ylab <span class="op">=</span> <span class="st">"Proportion of Cancer Cases"</span>,</span>
<span>  xlab <span class="op">=</span> <span class="st">"Alcohol Consumption Group"</span>,</span>
<span>  main <span class="op">=</span> <span class="st">"Esophageal Cancer Data"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-13-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb273"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Ensure categorical variables are treated as factors</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">agegp</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"factor"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">alcgp</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"factor"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">tobgp</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"factor"</span></span></code></pre></div>
</div>
<div id="apply-logistic-model" class="section level3" number="7.3.2">
<h3>
<span class="header-section-number">7.3.2</span> Apply Logistic Model<a class="anchor" aria-label="anchor" href="#apply-logistic-model"><i class="fas fa-link"></i></a>
</h3>
<p>We first fit a <strong>logistic regression model</strong>, where the response variable is the proportion of cancer cases (<code>ncases</code>) relative to total observations (<code>ncases + ncontrols</code>).</p>
<div class="sourceCode" id="cb274"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Logistic regression using alcohol consumption as a predictor</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ncases</span>, <span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">alcgp</span>,</span>
<span>        data <span class="op">=</span> <span class="va">esoph</span>,</span>
<span>        family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = cbind(ncases, ncontrols) ~ alcgp, family = binomial, </span></span>
<span><span class="co">#&gt;     data = esoph)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -2.5885     0.1925 -13.444  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; alcgp40-79    1.2712     0.2323   5.472 4.46e-08 ***</span></span>
<span><span class="co">#&gt; alcgp80-119   2.0545     0.2611   7.868 3.59e-15 ***</span></span>
<span><span class="co">#&gt; alcgp120+     3.3042     0.3237  10.209  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 367.95  on 87  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 221.46  on 84  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 344.51</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>Interpretation</p>
<ul>
<li><p>The coefficients represent the <strong>log-odds</strong> of having esophageal cancer relative to the baseline alcohol consumption group.</p></li>
<li><p><strong>P-values</strong> indicate whether alcohol consumption levels significantly influence cancer risk.</p></li>
</ul>
<p>Model Diagnostics</p>
<div class="sourceCode" id="cb275"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Convert coefficients to odds ratios</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)  alcgp40-79 alcgp80-119   alcgp120+ </span></span>
<span><span class="co">#&gt;  0.07512953  3.56527094  7.80261593 27.22570533</span></span>
<span></span>
<span><span class="co"># Model goodness-of-fit measures</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/df.residual.html">df.residual</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span>  <span class="co"># Closer to 1 suggests a better fit</span></span>
<span><span class="co">#&gt; [1] 2.63638</span></span>
<span><span class="va">model</span><span class="op">$</span><span class="va">aic</span>  <span class="co"># Lower AIC is preferable for model comparison</span></span>
<span><span class="co">#&gt; [1] 344.5109</span></span></code></pre></div>
<p>To improve our model, we include <strong>age group (<code>agegp</code>)</strong> as an additional predictor.</p>
<div class="sourceCode" id="cb276"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Logistic regression with alcohol consumption and age</span></span>
<span><span class="va">better_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ncases</span>, <span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">agegp</span> <span class="op">+</span> <span class="va">alcgp</span>,</span>
<span>    data <span class="op">=</span> <span class="va">esoph</span>,</span>
<span>    family <span class="op">=</span> <span class="va">binomial</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the improved model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">better_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial, </span></span>
<span><span class="co">#&gt;     data = esoph)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -6.1472     1.0419  -5.900 3.63e-09 ***</span></span>
<span><span class="co">#&gt; agegp35-44    1.6311     1.0800   1.510 0.130973    </span></span>
<span><span class="co">#&gt; agegp45-54    3.4258     1.0389   3.297 0.000976 ***</span></span>
<span><span class="co">#&gt; agegp55-64    3.9435     1.0346   3.811 0.000138 ***</span></span>
<span><span class="co">#&gt; agegp65-74    4.3568     1.0413   4.184 2.87e-05 ***</span></span>
<span><span class="co">#&gt; agegp75+      4.4242     1.0914   4.054 5.04e-05 ***</span></span>
<span><span class="co">#&gt; alcgp40-79    1.4343     0.2448   5.859 4.64e-09 ***</span></span>
<span><span class="co">#&gt; alcgp80-119   2.0071     0.2776   7.230 4.84e-13 ***</span></span>
<span><span class="co">#&gt; alcgp120+     3.6800     0.3763   9.778  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 367.95  on 87  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 105.88  on 79  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 238.94</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 6</span></span>
<span></span>
<span><span class="co"># Model evaluation</span></span>
<span><span class="va">better_model</span><span class="op">$</span><span class="va">aic</span> <span class="co"># Lower AIC is better</span></span>
<span><span class="co">#&gt; [1] 238.9361</span></span>
<span></span>
<span><span class="co"># Convert coefficients to odds ratios</span></span>
<span><span class="co"># exp(coefficients(better_model))</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>`Odds Ratios` <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">better_model</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;              Odds.Ratios</span></span>
<span><span class="co">#&gt; (Intercept)  0.002139482</span></span>
<span><span class="co">#&gt; agegp35-44   5.109601844</span></span>
<span><span class="co">#&gt; agegp45-54  30.748594216</span></span>
<span><span class="co">#&gt; agegp55-64  51.596634690</span></span>
<span><span class="co">#&gt; agegp65-74  78.005283850</span></span>
<span><span class="co">#&gt; agegp75+    83.448437749</span></span>
<span><span class="co">#&gt; alcgp40-79   4.196747169</span></span>
<span><span class="co">#&gt; alcgp80-119  7.441782227</span></span>
<span><span class="co">#&gt; alcgp120+   39.646885126</span></span>
<span></span>
<span><span class="co"># Compare models using likelihood ratio test (Chi-square test)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span></span>
<span>    q <span class="op">=</span> <span class="va">model</span><span class="op">$</span><span class="va">deviance</span> <span class="op">-</span> <span class="va">better_model</span><span class="op">$</span><span class="va">deviance</span>,</span>
<span>    df <span class="op">=</span> <span class="va">model</span><span class="op">$</span><span class="va">df.residual</span> <span class="op">-</span> <span class="va">better_model</span><span class="op">$</span><span class="va">df.residual</span>,</span>
<span>    lower.tail <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.713923e-23</span></span></code></pre></div>
<p>Key Takeaways</p>
<ul>
<li><p><strong>AIC Reduction</strong>: A lower AIC suggests that adding age as a predictor improves the model.</p></li>
<li><p><strong>Likelihood Ratio Test</strong>: This test compares the two models and determines whether the improvement is statistically significant.</p></li>
</ul>
</div>
<div id="apply-probit-model" class="section level3" number="7.3.3">
<h3>
<span class="header-section-number">7.3.3</span> Apply Probit Model<a class="anchor" aria-label="anchor" href="#apply-probit-model"><i class="fas fa-link"></i></a>
</h3>
<p>As discussed earlier, the <strong>probit</strong> model is an alternative to logistic regression, using a cumulative normal distribution instead of the logistic function.</p>
<div class="sourceCode" id="cb277"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Probit regression model</span></span>
<span><span class="va">Prob_better_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ncases</span>, <span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">agegp</span> <span class="op">+</span> <span class="va">alcgp</span>,</span>
<span>    data <span class="op">=</span> <span class="va">esoph</span>,</span>
<span>    family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="va">probit</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the probit model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Prob_better_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial(link = probit), </span></span>
<span><span class="co">#&gt;     data = esoph)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -3.3741     0.4922  -6.855 7.13e-12 ***</span></span>
<span><span class="co">#&gt; agegp35-44    0.8562     0.5081   1.685 0.092003 .  </span></span>
<span><span class="co">#&gt; agegp45-54    1.7829     0.4904   3.636 0.000277 ***</span></span>
<span><span class="co">#&gt; agegp55-64    2.1034     0.4876   4.314 1.61e-05 ***</span></span>
<span><span class="co">#&gt; agegp65-74    2.3374     0.4930   4.741 2.13e-06 ***</span></span>
<span><span class="co">#&gt; agegp75+      2.3694     0.5275   4.491 7.08e-06 ***</span></span>
<span><span class="co">#&gt; alcgp40-79    0.8080     0.1330   6.076 1.23e-09 ***</span></span>
<span><span class="co">#&gt; alcgp80-119   1.1399     0.1558   7.318 2.52e-13 ***</span></span>
<span><span class="co">#&gt; alcgp120+     2.1204     0.2060  10.295  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 367.95  on 87  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 104.48  on 79  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 237.53</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 6</span></span></code></pre></div>
<p>Why Consider a Probit Model?</p>
<ul>
<li><p>Like logistic regression, probit regression estimates probabilities, but it assumes a <strong>normal distribution of the latent variable</strong>.</p></li>
<li><p>While the <strong>interpretation of coefficients differs</strong>, model comparisons can still be made using <strong>AIC</strong>.</p></li>
</ul>
</div>
</div>
<div id="sec-poisson-regression" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Poisson Regression<a class="anchor" aria-label="anchor" href="#sec-poisson-regression"><i class="fas fa-link"></i></a>
</h2>
<div id="the-poisson-distribution" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">7.4.1</span> The Poisson Distribution<a class="anchor" aria-label="anchor" href="#the-poisson-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Poisson regression is used for modeling <strong>count data</strong>, where the response variable represents the number of occurrences of an event within a fixed period, space, or other unit. The Poisson distribution is defined as:</p>
<p><span class="math display">\[
\begin{aligned} f(Y_i) &amp;= \frac{\mu_i^{Y_i} \exp(-\mu_i)}{Y_i!}, \quad Y_i = 0,1,2, \dots \\ E(Y_i) &amp;= \mu_i \\ \text{Var}(Y_i) &amp;= \mu_i \end{aligned}
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(Y_i\)</span> is the count variable.</p></li>
<li><p><span class="math inline">\(\mu_i\)</span> is the expected count for the <span class="math inline">\(i\)</span>-th observation.</p></li>
<li><p>The <strong>mean and variance are equal</strong> <span class="math inline">\(E(Y_i) = \text{Var}(Y_i)\)</span>, making Poisson regression suitable when variance follows this property.</p></li>
</ul>
<p>However, real-world count data often exhibit <strong>overdispersion</strong>, where the variance exceeds the mean. We will discuss remedies such as <a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson</a> and <a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial Regression</a> later.</p>
</div>
<div id="poisson-model" class="section level3" number="7.4.2">
<h3>
<span class="header-section-number">7.4.2</span> Poisson Model<a class="anchor" aria-label="anchor" href="#poisson-model"><i class="fas fa-link"></i></a>
</h3>
<p>We model the expected count <span class="math inline">\(\mu_i\)</span> as a function of predictors <span class="math inline">\(\mathbf{x_i}\)</span> and parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>:</p>
<p><span class="math display">\[
\mu_i = f(\mathbf{x_i; \theta})
\]</span></p>
</div>
<div id="link-function-choices" class="section level3" number="7.4.3">
<h3>
<span class="header-section-number">7.4.3</span> Link Function Choices<a class="anchor" aria-label="anchor" href="#link-function-choices"><i class="fas fa-link"></i></a>
</h3>
<p>Since <span class="math inline">\(\mu_i\)</span> must be positive, we often use a <strong>log-link function</strong>:</p>
<p><span class="math display">\[
θ\log(\mu_i) = \mathbf{x_i' \theta}
\]</span></p>
<p>This ensures that the predicted counts are always non-negative. This is analogous to <a href="generalized-linear-models.html#sec-logistic-regression">logistic regression</a>, where we use the logit link for binary outcomes.</p>
<p>Rewriting:</p>
<p><span class="math display">\[
\mu_i = \exp(\mathbf{x_i' \theta})
\]</span> which ensures <span class="math inline">\(\mu_i &gt; 0\)</span> for all parameter values.</p>
</div>
<div id="application-poisson-regression" class="section level3" number="7.4.4">
<h3>
<span class="header-section-number">7.4.4</span> Application: Poisson Regression<a class="anchor" aria-label="anchor" href="#application-poisson-regression"><i class="fas fa-link"></i></a>
</h3>
<p>We apply Poisson regression to the <strong>bioChemists</strong> dataset (from the <code>pscl</code> package), which contains information on academic productivity in terms of published articles.</p>
<div id="dataset-overview-1" class="section level4" number="7.4.4.1">
<h4>
<span class="header-section-number">7.4.4.1</span> Dataset Overview<a class="anchor" aria-label="anchor" href="#dataset-overview-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb278"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="co"># Load dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">bioChemists</span>, package <span class="op">=</span> <span class="st">"pscl"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Rename columns for clarity</span></span>
<span><span class="va">bioChemists</span> <span class="op">&lt;-</span> <span class="va">bioChemists</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span></span>
<span>    Num_Article <span class="op">=</span> <span class="va">art</span>,</span>
<span>    <span class="co"># Number of articles in last 3 years</span></span>
<span>    Sex <span class="op">=</span> <span class="va">fem</span>,</span>
<span>    <span class="co"># 1 if female, 0 if male</span></span>
<span>    Married <span class="op">=</span> <span class="va">mar</span>,</span>
<span>    <span class="co"># 1 if married, 0 otherwise</span></span>
<span>    Num_Kid5 <span class="op">=</span> <span class="va">kid5</span>,</span>
<span>    <span class="co"># Number of children under age 6</span></span>
<span>    PhD_Quality <span class="op">=</span> <span class="va">phd</span>,</span>
<span>    <span class="co"># Prestige of PhD program</span></span>
<span>    Num_MentArticle <span class="op">=</span> <span class="va">ment</span>   <span class="co"># Number of articles by mentor in last 3 years</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualize response variable distribution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">bioChemists</span><span class="op">$</span><span class="va">Num_Article</span>, </span>
<span>     breaks <span class="op">=</span> <span class="fl">25</span>, </span>
<span>     main <span class="op">=</span> <span class="st">"Number of Articles Published"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-18-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>The <strong>distribution of the number of articles</strong> is right-skewed, which suggests a Poisson model may be appropriate.</p>
</div>
<div id="fitting-a-poisson-regression-model" class="section level4" number="7.4.4.2">
<h4>
<span class="header-section-number">7.4.4.2</span> Fitting a Poisson Regression Model<a class="anchor" aria-label="anchor" href="#fitting-a-poisson-regression-model"><i class="fas fa-link"></i></a>
</h4>
<p>We model the number of articles published (<code>Num_Article</code>) as a function of various predictors.</p>
<div class="sourceCode" id="cb279"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Poisson regression model</span></span>
<span><span class="va">Poisson_Mod</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>      family <span class="op">=</span> <span class="va">poisson</span>,</span>
<span>      data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Poisson_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.304617   0.102981   2.958   0.0031 ** </span></span>
<span><span class="co">#&gt; SexWomen        -0.224594   0.054613  -4.112 3.92e-05 ***</span></span>
<span><span class="co">#&gt; MarriedMarried   0.155243   0.061374   2.529   0.0114 *  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.184883   0.040127  -4.607 4.08e-06 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.012823   0.026397   0.486   0.6271    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.025543   0.002006  12.733  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for poisson family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1817.4  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1634.4  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3314.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>Interpretation:</p>
<ul>
<li><p>Coefficients are on the log scale, meaning they represent log-rate ratios.</p></li>
<li><p>Exponentiating the coefficients gives the rate ratios.</p></li>
<li><p>Statistical significance tells us whether each variable has a meaningful impact on publication count.</p></li>
</ul>
</div>
<div id="model-diagnostics-goodness-of-fit" class="section level4" number="7.4.4.3">
<h4>
<span class="header-section-number">7.4.4.3</span> Model Diagnostics: Goodness of Fit<a class="anchor" aria-label="anchor" href="#model-diagnostics-goodness-of-fit"><i class="fas fa-link"></i></a>
</h4>
<div id="pearsons-chi-square-test-for-overdispersion" class="section level5" number="7.4.4.3.1">
<h5>
<span class="header-section-number">7.4.4.3.1</span> Pearson’s Chi-Square Test for Overdispersion<a class="anchor" aria-label="anchor" href="#pearsons-chi-square-test-for-overdispersion"><i class="fas fa-link"></i></a>
</h5>
<p>We compute the <strong>Pearson chi-square statistic</strong> to check whether the variance significantly exceeds the mean. <span class="math display">\[
X^2 = \sum \frac{(Y_i - \hat{\mu}_i)^2}{\hat{\mu}_i}
\]</span></p>
<div class="sourceCode" id="cb280"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute predicted means</span></span>
<span><span class="va">Predicted_Means</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Poisson_Mod</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Pearson chi-square test</span></span>
<span><span class="va">X2</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">bioChemists</span><span class="op">$</span><span class="va">Num_Article</span> <span class="op">-</span> <span class="va">Predicted_Means</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span> <span class="va">Predicted_Means</span><span class="op">)</span></span>
<span><span class="va">X2</span></span>
<span><span class="co">#&gt; [1] 1662.547</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">X2</span>, <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 7.849882e-47</span></span></code></pre></div>
<ul>
<li><p>If p-value is small, overdispersion is present.</p></li>
<li><p>Large X² statistic suggests the model may not adequately capture variability.</p></li>
</ul>
</div>
<div id="overdispersion-check-ratio-of-deviance-to-degrees-of-freedom" class="section level5" number="7.4.4.3.2">
<h5>
<span class="header-section-number">7.4.4.3.2</span> Overdispersion Check: Ratio of Deviance to Degrees of Freedom<a class="anchor" aria-label="anchor" href="#overdispersion-check-ratio-of-deviance-to-degrees-of-freedom"><i class="fas fa-link"></i></a>
</h5>
<p>We compute: <span class="math display">\[
\hat{\phi} = \frac{\text{deviance}}{\text{degrees of freedom}}
\]</span></p>
<div class="sourceCode" id="cb281"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Overdispersion check</span></span>
<span><span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">deviance</span> <span class="op">/</span> <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span></span>
<span><span class="co">#&gt; [1] 1.797988</span></span></code></pre></div>
<ul>
<li><p>If <span class="math inline">\(\hat{\phi} &gt; 1\)</span>, overdispersion is likely present.</p></li>
<li><p>A value significantly above 1 suggests the need for an alternative model.</p></li>
</ul>
</div>
</div>
<div id="addressing-overdispersion" class="section level4" number="7.4.4.4">
<h4>
<span class="header-section-number">7.4.4.4</span> Addressing Overdispersion<a class="anchor" aria-label="anchor" href="#addressing-overdispersion"><i class="fas fa-link"></i></a>
</h4>
<div id="including-interaction-terms" class="section level5" number="7.4.4.4.1">
<h5>
<span class="header-section-number">7.4.4.4.1</span> Including Interaction Terms<a class="anchor" aria-label="anchor" href="#including-interaction-terms"><i class="fas fa-link"></i></a>
</h5>
<p>One possible remedy is to incorporate <strong>interaction terms</strong>, capturing complex relationships between predictors.</p>
<div class="sourceCode" id="cb282"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Adding two-way and three-way interaction terms</span></span>
<span><span class="va">Poisson_Mod_All2way</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span> <span class="op">^</span> <span class="fl">2</span>, family <span class="op">=</span> <span class="va">poisson</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span><span class="va">Poisson_Mod_All3way</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span> <span class="op">^</span> <span class="fl">3</span>, family <span class="op">=</span> <span class="va">poisson</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span></code></pre></div>
<p>This may improve model fit, but can lead to overfitting.</p>
</div>
<div id="quasi-poisson-model-adjusting-for-overdispersion" class="section level5" number="7.4.4.4.2">
<h5>
<span class="header-section-number">7.4.4.4.2</span> Quasi-Poisson Model (Adjusting for Overdispersion)<a class="anchor" aria-label="anchor" href="#quasi-poisson-model-adjusting-for-overdispersion"><i class="fas fa-link"></i></a>
</h5>
<p>A quick fix is to allow the variance to scale by introducing <span class="math inline">\(\hat{\phi}\)</span>:</p>
<p><span class="math display">\[
\text{Var}(Y_i) = \hat{\phi} \mu_i
\]</span></p>
<div class="sourceCode" id="cb283"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Estimate dispersion parameter</span></span>
<span><span class="va">phi_hat</span> <span class="op">=</span> <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">deviance</span> <span class="op">/</span> <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span></span>
<span></span>
<span><span class="co"># Adjusting Poisson model to account for overdispersion</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Poisson_Mod</span>, dispersion <span class="op">=</span> <span class="va">phi_hat</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.30462    0.13809   2.206  0.02739 *  </span></span>
<span><span class="co">#&gt; SexWomen        -0.22459    0.07323  -3.067  0.00216 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.15524    0.08230   1.886  0.05924 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.18488    0.05381  -3.436  0.00059 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.01282    0.03540   0.362  0.71715    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.02554    0.00269   9.496  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for poisson family taken to be 1.797988)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1817.4  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1634.4  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3314.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>Alternatively, we refit using a <a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson model</a>, which adjusts standard errors:</p>
<div class="sourceCode" id="cb284"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Quasi-Poisson model</span></span>
<span><span class="va">quasiPoisson_Mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, family <span class="op">=</span> <span class="va">quasipoisson</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">quasiPoisson_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Num_Article ~ ., family = quasipoisson, data = bioChemists)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.304617   0.139273   2.187 0.028983 *  </span></span>
<span><span class="co">#&gt; SexWomen        -0.224594   0.073860  -3.041 0.002427 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.155243   0.083003   1.870 0.061759 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.184883   0.054268  -3.407 0.000686 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.012823   0.035700   0.359 0.719544    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.025543   0.002713   9.415  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for quasipoisson family taken to be 1.829006)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1817.4  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1634.4  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: NA</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>While <a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson</a> corrects standard errors, it does not introduce an extra parameter for overdispersion.</p>
</div>
<div id="negative-binomial-regression-preferred-approach" class="section level5" number="7.4.4.4.3">
<h5>
<span class="header-section-number">7.4.4.4.3</span> Negative Binomial Regression (Preferred Approach)<a class="anchor" aria-label="anchor" href="#negative-binomial-regression-preferred-approach"><i class="fas fa-link"></i></a>
</h5>
<p>A <a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial Regression</a> explicitly models overdispersion by introducing a dispersion parameter <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\text{Var}(Y_i) = \mu_i + \theta \mu_i^2
\]</span></p>
<p>This extends Poisson regression by allowing the variance to grow quadratically rather than linearly.</p>
<div class="sourceCode" id="cb285"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load MASS package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit Negative Binomial regression</span></span>
<span><span class="va">NegBin_Mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/glm.nb.html">glm.nb</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">NegBin_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, </span></span>
<span><span class="co">#&gt;     link = log)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.256144   0.137348   1.865 0.062191 .  </span></span>
<span><span class="co">#&gt; SexWomen        -0.216418   0.072636  -2.979 0.002887 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.150489   0.082097   1.833 0.066791 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.015271   0.035873   0.426 0.670326    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.029082   0.003214   9.048  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1109.0  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1004.3  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3135.9</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;               Theta:  2.264 </span></span>
<span><span class="co">#&gt;           Std. Err.:  0.271 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  2 x log-likelihood:  -3121.917</span></span></code></pre></div>
<p>This model is generally <strong>preferred over Quasi-Poisson</strong>, as it explicitly accounts for <strong>heterogeneity</strong> in the data.</p>
</div>
</div>
</div>
</div>
<div id="sec-negative-binomial-regression" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Negative Binomial Regression<a class="anchor" aria-label="anchor" href="#sec-negative-binomial-regression"><i class="fas fa-link"></i></a>
</h2>
<p>When modeling <strong>count data</strong>, <a href="generalized-linear-models.html#sec-poisson-regression">Poisson regression</a> assumes that the <strong>mean and variance are equal</strong>:</p>
<p><span class="math display">\[
\text{Var}(Y_i) = E(Y_i) = \mu_i
\]</span> However, in many real-world datasets, the variance exceeds the mean—a phenomenon known as <strong>overdispersion</strong>. When overdispersion is present, the Poisson model underestimates the variance, leading to:</p>
<ul>
<li><p>Inflated test statistics (small p-values).</p></li>
<li><p>Overconfident predictions.</p></li>
<li><p>Poor model fit.</p></li>
</ul>
<div id="negative-binomial-distribution" class="section level3" number="7.5.1">
<h3>
<span class="header-section-number">7.5.1</span> Negative Binomial Distribution<a class="anchor" aria-label="anchor" href="#negative-binomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>To address overdispersion, <strong>Negative Binomial regression</strong> introduces an extra <strong>dispersion parameter</strong> <span class="math inline">\(\theta\)</span> to allow variance to be greater than the mean: <span class="math display">\[
\text{Var}(Y_i) = \mu_i + \theta \mu_i^2
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(\mu_i = \exp(\mathbf{x_i' \theta})\)</span> is the expected count.</p></li>
<li><p><span class="math inline">\(\theta\)</span> is the dispersion parameter.</p></li>
<li><p>When <span class="math inline">\(\theta \to 0\)</span>, the NB model reduces to the <a href="generalized-linear-models.html#poisson-model">Poisson model</a>.</p></li>
</ul>
<p>Thus, Negative Binomial regression is a <strong>generalization of Poisson regression</strong> that accounts for overdispersion.</p>
</div>
<div id="application-negative-binomial-regression" class="section level3" number="7.5.2">
<h3>
<span class="header-section-number">7.5.2</span> Application: Negative Binomial Regression<a class="anchor" aria-label="anchor" href="#application-negative-binomial-regression"><i class="fas fa-link"></i></a>
</h3>
<p>We apply <strong>Negative Binomial regression</strong> to the <code>bioChemists</code> dataset to model the number of research articles (<code>Num_Article</code>) as a function of several predictors.</p>
<div id="fitting-the-negative-binomial-model" class="section level4" number="7.5.2.1">
<h4>
<span class="header-section-number">7.5.2.1</span> Fitting the Negative Binomial Model<a class="anchor" aria-label="anchor" href="#fitting-the-negative-binomial-model"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb286"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit Negative Binomial model</span></span>
<span><span class="va">NegBinom_Mod</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/glm.nb.html">glm.nb</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">NegBinom_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; MASS::glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, </span></span>
<span><span class="co">#&gt;     link = log)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.256144   0.137348   1.865 0.062191 .  </span></span>
<span><span class="co">#&gt; SexWomen        -0.216418   0.072636  -2.979 0.002887 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.150489   0.082097   1.833 0.066791 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.015271   0.035873   0.426 0.670326    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.029082   0.003214   9.048  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1109.0  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1004.3  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3135.9</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;               Theta:  2.264 </span></span>
<span><span class="co">#&gt;           Std. Err.:  0.271 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  2 x log-likelihood:  -3121.917</span></span></code></pre></div>
<p>Interpretation:</p>
<ul>
<li><p>The coefficients are on the log scale.</p></li>
<li><p>The dispersion parameter <span class="math inline">\(\theta\)</span> (also called size parameter in some contexts) is estimated as 2.264 with a standard error of 0.271. Check <a href="generalized-linear-models.html#over-dispersion">Over-Dispersion</a> for more detail.</p></li>
<li><p>Since <span class="math inline">\(\theta\)</span> is significantly different from 1, this confirms overdispersion, validating the choice of the Negative Binomial model over Poisson regression.</p></li>
</ul>
</div>
<div id="model-comparison-poisson-vs.-negative-binomial" class="section level4" number="7.5.2.2">
<h4>
<span class="header-section-number">7.5.2.2</span> Model Comparison: Poisson vs. Negative Binomial<a class="anchor" aria-label="anchor" href="#model-comparison-poisson-vs.-negative-binomial"><i class="fas fa-link"></i></a>
</h4>
<div id="checking-overdispersion-in-poisson-model" class="section level5" number="7.5.2.2.1">
<h5>
<span class="header-section-number">7.5.2.2.1</span> Checking Overdispersion in Poisson Model<a class="anchor" aria-label="anchor" href="#checking-overdispersion-in-poisson-model"><i class="fas fa-link"></i></a>
</h5>
<p>Before using NB regression, we confirm <strong>overdispersion</strong> by computing:</p>
<p><span class="math display">\[
\hat{\phi} = \frac{\text{deviance}}{\text{degrees of freedom}}
\]</span></p>
<div class="sourceCode" id="cb287"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Overdispersion check for Poisson model</span></span>
<span><span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">deviance</span> <span class="op">/</span> <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span></span>
<span><span class="co">#&gt; [1] 1.797988</span></span></code></pre></div>
<ul>
<li><p>If <span class="math inline">\(\hat{\phi} &gt; 1\)</span>, overdispersion is present.</p></li>
<li><p>A large value suggests that Poisson regression underestimates variance.</p></li>
</ul>
</div>
<div id="likelihood-ratio-test-poisson-vs.-negative-binomial" class="section level5" number="7.5.2.2.2">
<h5>
<span class="header-section-number">7.5.2.2.2</span> Likelihood Ratio Test: Poisson vs. Negative Binomial<a class="anchor" aria-label="anchor" href="#likelihood-ratio-test-poisson-vs.-negative-binomial"><i class="fas fa-link"></i></a>
</h5>
<p>We compare the Poisson and Negative Binomial models using a <strong>likelihood ratio test</strong>, where: <span class="math display">\[
G^2 = 2 \times ( \log L_{NB} - \log L_{Poisson})
\]</span> with <span class="math inline">\(\text{df} = 1\)</span></p>
<div class="sourceCode" id="cb288"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Likelihood ratio test between Poisson and Negative Binomial</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">NegBinom_Mod</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">Poisson_Mod</span><span class="op">)</span><span class="op">)</span>,</span>
<span>       df <span class="op">=</span> <span class="fl">1</span>,</span>
<span>       lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'log Lik.' 4.391728e-41 (df=7)</span></span></code></pre></div>
<ul>
<li><p>Small p-value (&lt; 0.05) → Negative Binomial model is significantly better.</p></li>
<li><p>Large p-value (&gt; 0.05) → Poisson model is adequate.</p></li>
</ul>
<p>Since overdispersion is confirmed, the Negative Binomial model is preferred.</p>
</div>
</div>
<div id="model-diagnostics-and-evaluation" class="section level4" number="7.5.2.3">
<h4>
<span class="header-section-number">7.5.2.3</span> Model Diagnostics and Evaluation<a class="anchor" aria-label="anchor" href="#model-diagnostics-and-evaluation"><i class="fas fa-link"></i></a>
</h4>
<div id="checking-dispersion-parameter-theta" class="section level5" number="7.5.2.3.1">
<h5>
<span class="header-section-number">7.5.2.3.1</span> Checking Dispersion Parameter <span class="math inline">\(\theta\)</span><a class="anchor" aria-label="anchor" href="#checking-dispersion-parameter-theta"><i class="fas fa-link"></i></a>
</h5>
<p>The Negative Binomial dispersion parameter <span class="math inline">\(\theta\)</span> can be retrieved:</p>
<div class="sourceCode" id="cb289"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract dispersion parameter estimate</span></span>
<span><span class="va">NegBinom_Mod</span><span class="op">$</span><span class="va">theta</span></span>
<span><span class="co">#&gt; [1] 2.264388</span></span></code></pre></div>
<ul>
<li>A large <span class="math inline">\(\theta\)</span> suggests that overdispersion is <strong>not extreme</strong>, while a small <span class="math inline">\(\theta\)</span> (close to 0) would indicate the Poisson model is reasonable.</li>
</ul>
</div>
</div>
<div id="predictions-and-rate-ratios" class="section level4" number="7.5.2.4">
<h4>
<span class="header-section-number">7.5.2.4</span> Predictions and Rate Ratios<a class="anchor" aria-label="anchor" href="#predictions-and-rate-ratios"><i class="fas fa-link"></i></a>
</h4>
<p>In Negative Binomial regression, exponentiating the coefficients gives rate ratios:</p>
<div class="sourceCode" id="cb290"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Convert coefficients to rate ratios</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>`Odds Ratios` <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">NegBinom_Mod</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 Odds.Ratios</span></span>
<span><span class="co">#&gt; (Intercept)       1.2919388</span></span>
<span><span class="co">#&gt; SexWomen          0.8053982</span></span>
<span><span class="co">#&gt; MarriedMarried    1.1624030</span></span>
<span><span class="co">#&gt; Num_Kid5          0.8382698</span></span>
<span><span class="co">#&gt; PhD_Quality       1.0153884</span></span>
<span><span class="co">#&gt; Num_MentArticle   1.0295094</span></span></code></pre></div>
<p>A rate ratio of:</p>
<ul>
<li><p><strong>&gt;</strong> 1 <span class="math inline">\(\to\)</span> Increases expected article count.</p></li>
<li><p>&lt; 1 <span class="math inline">\(\to\)</span> Decreases expected article count.</p></li>
<li><p>= 1 <span class="math inline">\(\to\)</span> No effect.</p></li>
</ul>
<p>For example:</p>
<ul>
<li><p>If <code>PhD_Quality</code> has an exponentiated coefficient of 1.5, individuals from higher-quality PhD programs are expected to publish 50% more articles.</p></li>
<li><p>If <code>Sex</code> has an exponentiated coefficient of 0.8, females publish 20% fewer articles than males, all else equal.</p></li>
</ul>
</div>
<div id="alternative-approach-zero-inflated-models" class="section level4" number="7.5.2.5">
<h4>
<span class="header-section-number">7.5.2.5</span> Alternative Approach: Zero-Inflated Models<a class="anchor" aria-label="anchor" href="#alternative-approach-zero-inflated-models"><i class="fas fa-link"></i></a>
</h4>
<p>If a dataset has excess zeros (many individuals publish no articles), <strong>Zero-Inflated Negative Binomial (ZINB) models</strong> may be required.</p>
<p><span class="math display">\[
\text{P}(Y_i = 0) = p + (1 - p) f(Y_i = 0 | \mu, \theta)
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(p\)</span> is the probability of always being a zero (e.g., inactive researchers).</p></li>
<li><p><span class="math inline">\(f(Y_i)\)</span> follows the Negative Binomial distribution.</p></li>
</ul>
</div>
</div>
<div id="fitting-a-zero-inflated-negative-binomial-model" class="section level3" number="7.5.3">
<h3>
<span class="header-section-number">7.5.3</span> Fitting a Zero-Inflated Negative Binomial Model<a class="anchor" aria-label="anchor" href="#fitting-a-zero-inflated-negative-binomial-model"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb291"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load package for zero-inflated models</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/atahk/pscl">pscl</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit ZINB model</span></span>
<span><span class="va">ZINB_Mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/pscl/man/zeroinfl.html">zeroinfl</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">bioChemists</span>, dist <span class="op">=</span> <span class="st">"negbin"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ZINB_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; zeroinfl(formula = Num_Article ~ ., data = bioChemists, dist = "negbin")</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Pearson residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.2942 -0.7601 -0.2909  0.4448  6.4155 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Count model coefficients (negbin with log link):</span></span>
<span><span class="co">#&gt;                   Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.4167465  0.1435966   2.902  0.00371 ** </span></span>
<span><span class="co">#&gt; SexWomen        -0.1955068  0.0755926  -2.586  0.00970 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.0975826  0.0844520   1.155  0.24789    </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.1517325  0.0542061  -2.799  0.00512 ** </span></span>
<span><span class="co">#&gt; PhD_Quality     -0.0007001  0.0362697  -0.019  0.98460    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.0247862  0.0034927   7.097 1.28e-12 ***</span></span>
<span><span class="co">#&gt; Log(theta)       0.9763565  0.1354695   7.207 5.71e-13 ***</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Zero-inflation model coefficients (binomial with logit link):</span></span>
<span><span class="co">#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)   </span></span>
<span><span class="co">#&gt; (Intercept)     -0.19169    1.32282  -0.145  0.88478   </span></span>
<span><span class="co">#&gt; SexWomen         0.63593    0.84892   0.749  0.45379   </span></span>
<span><span class="co">#&gt; MarriedMarried  -1.49947    0.93867  -1.597  0.11017   </span></span>
<span><span class="co">#&gt; Num_Kid5         0.62843    0.44278   1.419  0.15582   </span></span>
<span><span class="co">#&gt; PhD_Quality     -0.03771    0.30801  -0.122  0.90254   </span></span>
<span><span class="co">#&gt; Num_MentArticle -0.88229    0.31623  -2.790  0.00527 **</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Theta = 2.6548 </span></span>
<span><span class="co">#&gt; Number of iterations in BFGS optimization: 43 </span></span>
<span><span class="co">#&gt; Log-likelihood: -1550 on 13 Df</span></span></code></pre></div>
<p>This model accounts for:</p>
<ul>
<li><p>Structural zero inflation.</p></li>
<li><p>Overdispersion.</p></li>
</ul>
<p>ZINB is often preferred when many observations are zero. However, since ZINB does not fall under the GLM framework, we will discuss it further in <a href="sec-nonlinear-and-generalized-linear-mixed-models.html#sec-nonlinear-and-generalized-linear-mixed-models">Nonlinear and Generalized Linear Mixed Models</a>.</p>
<p><strong>Why ZINB is Not a GLM?</strong></p>
<ul>
<li>
<p>Unlike GLMs, which assume a single response distribution from the exponential family, ZINB is a mixture model with two components:</p>
<ul>
<li><p>Count model – A negative binomial regression for the main count process.</p></li>
<li><p>Inflation model – A logistic regression for excess zeros.</p></li>
</ul>
</li>
</ul>
<p>Because ZINB combines two distinct processes rather than using a single exponential family distribution, it does not fit within the standard GLM framework.</p>
<p><strong>What ZINB Belongs To</strong></p>
<p>ZINB is part of finite mixture models and is sometimes considered within generalized linear mixed models (GLMMs) or semi-parametric models.</p>
</div>
</div>
<div id="sec-quasi-poisson-regression" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> Quasi-Poisson Regression<a class="anchor" aria-label="anchor" href="#sec-quasi-poisson-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Poisson regression assumes that the <strong>mean and variance are equal</strong>:</p>
<p><span class="math display">\[
\text{Var}(Y_i) = E(Y_i) = \mu_i
\]</span></p>
<p>However, many real-world datasets exhibit <strong>overdispersion</strong>, where the variance exceeds the mean:</p>
<p><span class="math display">\[
\text{Var}(Y_i) = \phi \mu_i
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> (the <strong>dispersion parameter</strong>) allows the variance to scale beyond the Poisson assumption.</p>
<p>To correct for this, we use Quasi-Poisson regression, which:</p>
<ul>
<li><p>Follows the <a href="generalized-linear-models.html#generalized-linear-models">Generalized Linear Models</a> structure but is not a strict GLM.</p></li>
<li><p>Uses a variance function proportional to the mean: <span class="math inline">\(\text{Var}(Y_i) = \phi \mu_i\)</span>.</p></li>
<li><p>Does not assume a specific probability distribution, unlike Poisson or Negative Binomial models.</p></li>
</ul>
<div id="is-quasi-poisson-regression-a-generalized-linear-model" class="section level3" number="7.6.1">
<h3>
<span class="header-section-number">7.6.1</span> Is Quasi-Poisson Regression a <a href="generalized-linear-models.html#generalized-linear-models">Generalized Linear Model</a>?<a class="anchor" aria-label="anchor" href="#is-quasi-poisson-regression-a-generalized-linear-model"><i class="fas fa-link"></i></a>
</h3>
<p>✅ Yes, Quasi-Poisson is GLM-like:</p>
<ul>
<li><p><strong>Linear Predictor:</strong> Like Poisson regression, it models the log of the expected count as a function of predictors: <span class="math display">\[
\log(E(Y)) = X\beta
\]</span></p></li>
<li><p><strong>Canonical Link Function:</strong> It typically uses a log link function, just like standard Poisson regression.</p></li>
<li><p><strong>Variance Structure:</strong> Unlike standard Poisson, which assumes <span class="math inline">\(\text{Var}(Y) = E(Y)\)</span>, Quasi-Poisson allows for <strong>overdispersion</strong>: <span class="math display">\[
\text{Var}(Y) = \phi E(Y)
\]</span> where <span class="math inline">\(\phi\)</span> is estimated rather than assumed to be 1.</p></li>
</ul>
<p>❌ No, Quasi-Poisson is not a strict GLM because:</p>
<ul>
<li>
<p>GLMs require a full probability distribution from the exponential family.</p>
<ul>
<li><p>Standard Poisson regression assumes a Poisson distribution (which belongs to the exponential family).</p></li>
<li><p>Quasi-Poisson does not assume a full probability distribution, only a mean-variance relationship.</p></li>
</ul>
</li>
<li>
<p>It does not use <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> Estimation.</p>
<ul>
<li><p>Standard GLMs use MLE to estimate parameters.</p></li>
<li><p>Quasi-Poisson uses quasi-likelihood methods, which require specifying only the mean and variance, but not a full likelihood function.</p></li>
</ul>
</li>
<li>
<p>Likelihood-based inference is not valid.</p>
<ul>
<li>AIC, BIC, and Likelihood Ratio Tests cannot be used with Quasi-Poisson regression.</li>
</ul>
</li>
</ul>
<p><strong>When to Use Quasi-Poisson:</strong></p>
<ul>
<li><p>When <strong>data exhibit overdispersion</strong> (variance &gt; mean), making standard Poisson regression inappropriate.</p></li>
<li><p>When <a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial Regression</a> is not preferred, but an alternative is needed to handle overdispersion.</p></li>
<li><p>If overdispersion is present, <a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial Regression</a> is often a better alternative because it is a true GLM with a full likelihood function, whereas Quasi-Poisson is only a quasi-likelihood approach.</p></li>
</ul>
</div>
<div id="application-quasi-poisson-regression" class="section level3" number="7.6.2">
<h3>
<span class="header-section-number">7.6.2</span> Application: Quasi-Poisson Regression<a class="anchor" aria-label="anchor" href="#application-quasi-poisson-regression"><i class="fas fa-link"></i></a>
</h3>
<p>We analyze the <code>bioChemists</code> dataset, modeling the number of published articles (<code>Num_Article</code>) as a function of various predictors.</p>
<div id="checking-overdispersion-in-the-poisson-model" class="section level4" number="7.6.2.1">
<h4>
<span class="header-section-number">7.6.2.1</span> Checking Overdispersion in the Poisson Model<a class="anchor" aria-label="anchor" href="#checking-overdispersion-in-the-poisson-model"><i class="fas fa-link"></i></a>
</h4>
<p>We first fit a <a href="generalized-linear-models.html#sec-poisson-regression">Poisson regression model</a> and check for overdispersion using the deviance-to-degrees-of-freedom ratio:</p>
<div class="sourceCode" id="cb292"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit Poisson regression model</span></span>
<span><span class="va">Poisson_Mod</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, family <span class="op">=</span> <span class="va">poisson</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute dispersion parameter</span></span>
<span><span class="va">dispersion_estimate</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">deviance</span> <span class="op">/</span> <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span></span>
<span><span class="va">dispersion_estimate</span></span>
<span><span class="co">#&gt; [1] 1.797988</span></span></code></pre></div>
<ul>
<li><p>If <span class="math inline">\(\hat{\phi} &gt; 1\)</span>, the Poisson model underestimates variance.</p></li>
<li><p>A large value (&gt;&gt; 1) suggests that Poisson regression is not appropriate.</p></li>
</ul>
</div>
<div id="fitting-the-quasi-poisson-model" class="section level4" number="7.6.2.2">
<h4>
<span class="header-section-number">7.6.2.2</span> Fitting the Quasi-Poisson Model<a class="anchor" aria-label="anchor" href="#fitting-the-quasi-poisson-model"><i class="fas fa-link"></i></a>
</h4>
<p>Since overdispersion is present, we refit the model using <a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson regression</a>, which scales standard errors by <span class="math inline">\(\phi\)</span>.</p>
<div class="sourceCode" id="cb293"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit Quasi-Poisson regression model</span></span>
<span><span class="va">quasiPoisson_Mod</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, family <span class="op">=</span> <span class="va">quasipoisson</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">quasiPoisson_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Num_Article ~ ., family = quasipoisson, data = bioChemists)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.304617   0.139273   2.187 0.028983 *  </span></span>
<span><span class="co">#&gt; SexWomen        -0.224594   0.073860  -3.041 0.002427 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.155243   0.083003   1.870 0.061759 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.184883   0.054268  -3.407 0.000686 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.012823   0.035700   0.359 0.719544    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.025543   0.002713   9.415  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for quasipoisson family taken to be 1.829006)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1817.4  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1634.4  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: NA</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>Interpretation:</p>
<ul>
<li><p>The coefficients remain the same as in Poisson regression.</p></li>
<li><p>Standard errors are inflated to account for overdispersion.</p></li>
<li><p>P-values increase, leading to more conservative inference.</p></li>
</ul>
</div>
<div id="comparing-poisson-and-quasi-poisson" class="section level4" number="7.6.2.3">
<h4>
<span class="header-section-number">7.6.2.3</span> Comparing Poisson and Quasi-Poisson<a class="anchor" aria-label="anchor" href="#comparing-poisson-and-quasi-poisson"><i class="fas fa-link"></i></a>
</h4>
<p>To see the effect of using <a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson</a>, we compare standard errors:</p>
<div class="sourceCode" id="cb294"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract coefficients and standard errors</span></span>
<span><span class="va">poisson_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Poisson_Mod</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">quasi_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">quasiPoisson_Mod</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Compare standard errors</span></span>
<span><span class="va">se_comparison</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Poisson <span class="op">=</span> <span class="va">poisson_se</span>,</span>
<span>                            Quasi_Poisson <span class="op">=</span> <span class="va">quasi_se</span><span class="op">)</span></span>
<span><span class="va">se_comparison</span></span>
<span><span class="co">#&gt;                     Poisson Quasi_Poisson</span></span>
<span><span class="co">#&gt; (Intercept)     0.102981443   0.139272885</span></span>
<span><span class="co">#&gt; SexWomen        0.054613488   0.073859696</span></span>
<span><span class="co">#&gt; MarriedMarried  0.061374395   0.083003199</span></span>
<span><span class="co">#&gt; Num_Kid5        0.040126898   0.054267922</span></span>
<span><span class="co">#&gt; PhD_Quality     0.026397045   0.035699564</span></span>
<span><span class="co">#&gt; Num_MentArticle 0.002006073   0.002713028</span></span></code></pre></div>
<ul>
<li><p><a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson</a> has larger standard errors than <a href="generalized-linear-models.html#sec-poisson-regression">Poisson</a>.</p></li>
<li><p>This leads to wider confidence intervals, reducing the likelihood of false positives.</p></li>
</ul>
</div>
<div id="model-diagnostics-checking-residuals" class="section level4" number="7.6.2.4">
<h4>
<span class="header-section-number">7.6.2.4</span> Model Diagnostics: Checking Residuals<a class="anchor" aria-label="anchor" href="#model-diagnostics-checking-residuals"><i class="fas fa-link"></i></a>
</h4>
<p>We examine residuals to assess model fit:</p>
<div class="sourceCode" id="cb295"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Residual plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="va">quasiPoisson_Mod</span><span class="op">$</span><span class="va">fitted.values</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">quasiPoisson_Mod</span>, type <span class="op">=</span> <span class="st">"pearson"</span><span class="op">)</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Fitted Values"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Pearson Residuals"</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Residuals vs. Fitted Values (Quasi-Poisson)"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-35-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>If residuals show a pattern, additional predictors or transformations may be needed.</p></li>
<li><p>Random scatter around zero suggests a well-fitting model.</p></li>
</ul>
</div>
<div id="alternative-negative-binomial-vs.-quasi-poisson" class="section level4" number="7.6.2.5">
<h4>
<span class="header-section-number">7.6.2.5</span> Alternative: Negative Binomial vs. Quasi-Poisson<a class="anchor" aria-label="anchor" href="#alternative-negative-binomial-vs.-quasi-poisson"><i class="fas fa-link"></i></a>
</h4>
<p>If overdispersion is <strong>severe</strong>, <a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial regression</a> may be preferable because it explicitly models dispersion:</p>
<div class="sourceCode" id="cb296"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit Negative Binomial model</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="va">NegBinom_Mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/glm.nb.html">glm.nb</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">bioChemists</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model summaries</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">quasiPoisson_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Num_Article ~ ., family = quasipoisson, data = bioChemists)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.304617   0.139273   2.187 0.028983 *  </span></span>
<span><span class="co">#&gt; SexWomen        -0.224594   0.073860  -3.041 0.002427 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.155243   0.083003   1.870 0.061759 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.184883   0.054268  -3.407 0.000686 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.012823   0.035700   0.359 0.719544    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.025543   0.002713   9.415  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for quasipoisson family taken to be 1.829006)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1817.4  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1634.4  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: NA</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">NegBinom_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, </span></span>
<span><span class="co">#&gt;     link = log)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.256144   0.137348   1.865 0.062191 .  </span></span>
<span><span class="co">#&gt; SexWomen        -0.216418   0.072636  -2.979 0.002887 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.150489   0.082097   1.833 0.066791 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.015271   0.035873   0.426 0.670326    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.029082   0.003214   9.048  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1109.0  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1004.3  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3135.9</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;               Theta:  2.264 </span></span>
<span><span class="co">#&gt;           Std. Err.:  0.271 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  2 x log-likelihood:  -3121.917</span></span></code></pre></div>
</div>
<div id="key-differences-quasi-poisson-vs.-negative-binomial" class="section level4" number="7.6.2.6">
<h4>
<span class="header-section-number">7.6.2.6</span> Key Differences: Quasi-Poisson vs. Negative Binomial<a class="anchor" aria-label="anchor" href="#key-differences-quasi-poisson-vs.-negative-binomial"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="45%">
<col width="29%">
<col width="25%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Quasi-Poisson</th>
<th>Negative Binomial</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Handles Overdispersion?</td>
<td>✅ Yes</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td>Uses a Full Probability Distribution?</td>
<td>❌ No</td>
<td>✅ Yes</td>
</tr>
<tr class="odd">
<td>MLE-Based?</td>
<td>❌ No (quasi-likelihood)</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td>Can Use AIC/BIC for Model Selection?</td>
<td>❌ No</td>
<td>✅ Yes</td>
</tr>
<tr class="odd">
<td>Better for Model Interpretation?</td>
<td>✅ Yes</td>
<td>✅ Yes</td>
</tr>
<tr class="even">
<td>Best for Severe Overdispersion?</td>
<td>❌ No</td>
<td>✅ Yes</td>
</tr>
</tbody>
</table></div>
<p><strong>When to Choose:</strong></p>
<ul>
<li><p>Use <a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson</a> when you only need robust standard errors and do not require model selection via AIC/BIC.</p></li>
<li><p>Use <a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial</a> when overdispersion is large and you want a true likelihood-based model.</p></li>
</ul>
<p>While <a href="generalized-linear-models.html#sec-quasi-poisson-regression">Quasi-Poisson</a> is a quick fix, <a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial</a> is generally the better choice for modeling count data with overdispersion.</p>
</div>
</div>
</div>
<div id="sec-multinomial-logistic-regression" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> Multinomial Logistic Regression<a class="anchor" aria-label="anchor" href="#sec-multinomial-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>When dealing with categorical response variables with more than two possible outcomes, the <strong>multinomial logistic regression</strong> is a natural extension of the binary logistic model.</p>
<div id="the-multinomial-distribution" class="section level3" number="7.7.1">
<h3>
<span class="header-section-number">7.7.1</span> The Multinomial Distribution<a class="anchor" aria-label="anchor" href="#the-multinomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have a categorical response variable <span class="math inline">\(Y_i\)</span> that can take values in <span class="math inline">\(\{1, 2, \dots, J\}\)</span>. For each observation <span class="math inline">\(i\)</span>, the probability that it falls into category <span class="math inline">\(j\)</span> is given by:</p>
<p><span class="math display">\[
p_{ij} = P(Y_i = j), \quad \text{where} \quad \sum_{j=1}^{J} p_{ij} = 1.
\]</span></p>
<p>The response follows a <strong>multinomial distribution</strong>:</p>
<p><span class="math display">\[
Y_i \sim \text{Multinomial}(1; p_{i1}, p_{i2}, ..., p_{iJ}).
\]</span></p>
<p>This means that each observation belongs to exactly one of the <span class="math inline">\(J\)</span> categories.</p>
</div>
<div id="modeling-probabilities-using-log-odds" class="section level3" number="7.7.2">
<h3>
<span class="header-section-number">7.7.2</span> Modeling Probabilities Using Log-Odds<a class="anchor" aria-label="anchor" href="#modeling-probabilities-using-log-odds"><i class="fas fa-link"></i></a>
</h3>
<p>We cannot model the probabilities <span class="math inline">\(p_{ij}\)</span> directly because they must sum to 1. Instead, we use a <strong>logit transformation</strong>, comparing each category <span class="math inline">\(j\)</span> to a <strong>baseline category</strong> (typically the first category, <span class="math inline">\(j=1\)</span>):</p>
<p><span class="math display">\[
\eta_{ij} = \log \frac{p_{ij}}{p_{i1}}, \quad j = 2, \dots, J.
\]</span></p>
<p>Using a <strong>linear function of covariates</strong> <span class="math inline">\(\mathbf{x}_i\)</span>, we define:</p>
<p><span class="math display">\[
\eta_{ij} = \mathbf{x}_i' \beta_j = \beta_{j0} + \sum_{p=1}^{P} \beta_{jp} x_{ip}.
\]</span></p>
<p>Rearranging to express probabilities explicitly:</p>
<p><span class="math display">\[
p_{ij} = p_{i1} \exp(\mathbf{x}_i' \beta_j).
\]</span></p>
<p>Since all probabilities must sum to 1:</p>
<p><span class="math display">\[
p_{i1} + \sum_{j=2}^{J} p_{ij} = 1.
\]</span></p>
<p>Substituting for <span class="math inline">\(p_{ij}\)</span>:</p>
<p><span class="math display">\[
p_{i1} + \sum_{j=2}^{J} p_{i1} \exp(\mathbf{x}_i' \beta_j) = 1.
\]</span></p>
<p>Solving for <span class="math inline">\(p_{i1}\)</span>:</p>
<p><span class="math display">\[
p_{i1} = \frac{1}{1 + \sum_{j=2}^{J} \exp(\mathbf{x}_i' \beta_j)}.
\]</span></p>
<p>Thus, the probability for category <span class="math inline">\(j\)</span> is:</p>
<p><span class="math display">\[
p_{ij} = \frac{\exp(\mathbf{x}_i' \beta_j)}{1 + \sum_{l=2}^{J} \exp(\mathbf{x}_i' \beta_l)}, \quad j = 2, \dots, J.
\]</span></p>
<p>This formulation is known as the <strong>multinomial logit model</strong>.</p>
</div>
<div id="softmax-representation" class="section level3" number="7.7.3">
<h3>
<span class="header-section-number">7.7.3</span> Softmax Representation<a class="anchor" aria-label="anchor" href="#softmax-representation"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative formulation avoids choosing a baseline category and instead treats all <span class="math inline">\(J\)</span> categories <strong>symmetrically</strong> using the <strong>softmax function</strong>:</p>
<p><span class="math display">\[
P(Y_i = j | X_i = x) = \frac{\exp(\beta_{j0} + \sum_{p=1}^{P} \beta_{jp} x_p)}{\sum_{l=1}^{J} \exp(\beta_{l0} + \sum_{p=1}^{P} \beta_{lp} x_p)}.
\]</span></p>
<p>This representation is often used in <strong>neural networks</strong> and general machine learning models.</p>
</div>
<div id="log-odds-ratio-between-two-categories" class="section level3" number="7.7.4">
<h3>
<span class="header-section-number">7.7.4</span> Log-Odds Ratio Between Two Categories<a class="anchor" aria-label="anchor" href="#log-odds-ratio-between-two-categories"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>log-odds ratio</strong> between two categories <span class="math inline">\(k\)</span> and <span class="math inline">\(k'\)</span> is:</p>
<p><span class="math display">\[
\log \frac{P(Y = k | X = x)}{P(Y = k' | X = x)}
= (\beta_{k0} - \beta_{k'0}) + \sum_{p=1}^{P} (\beta_{kp} - \beta_{k'p}) x_p.
\]</span></p>
<p>This equation tells us that:</p>
<ul>
<li>If <span class="math inline">\(\beta_{kp} &gt; \beta_{k'p}\)</span>, then increasing <span class="math inline">\(x_p\)</span> increases the odds of choosing category <span class="math inline">\(k\)</span> over <span class="math inline">\(k'\)</span>.</li>
<li>If <span class="math inline">\(\beta_{kp} &lt; \beta_{k'p}\)</span>, then increasing <span class="math inline">\(x_p\)</span> decreases the odds of choosing <span class="math inline">\(k\)</span> over <span class="math inline">\(k'\)</span>.</li>
</ul>
</div>
<div id="estimation" class="section level3" number="7.7.5">
<h3>
<span class="header-section-number">7.7.5</span> Estimation<a class="anchor" aria-label="anchor" href="#estimation"><i class="fas fa-link"></i></a>
</h3>
<p>To estimate the parameters <span class="math inline">\(\beta_j\)</span>, we use <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> estimation.</p>
<p>Given <span class="math inline">\(n\)</span> independent observations <span class="math inline">\((Y_i, X_i)\)</span>, the likelihood function is:</p>
<p><span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \prod_{j=1}^{J} p_{ij}^{Y_{ij}}.
\]</span></p>
<p>Taking the log-likelihood:</p>
<p><span class="math display">\[
\log L(\beta) = \sum_{i=1}^{n} \sum_{j=1}^{J} Y_{ij} \log p_{ij}.
\]</span></p>
<p>Since there is no closed-form solution, numerical methods (see <a href="non-linear-regression.html#non-linear-least-squares-estimation">Non-linear Least Squares Estimation</a>) are used for estimation.</p>
</div>
<div id="interpretation-of-coefficients" class="section level3" number="7.7.6">
<h3>
<span class="header-section-number">7.7.6</span> Interpretation of Coefficients<a class="anchor" aria-label="anchor" href="#interpretation-of-coefficients"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Each <span class="math inline">\(\beta_{jp}\)</span> represents the effect of <span class="math inline">\(x_p\)</span> on the log-odds of category <span class="math inline">\(j\)</span> relative to the baseline.</li>
<li>Positive coefficients mean increasing <span class="math inline">\(x_p\)</span> makes category <span class="math inline">\(j\)</span> more likely relative to the baseline.</li>
<li>Negative coefficients mean increasing <span class="math inline">\(x_p\)</span> makes category <span class="math inline">\(j\)</span> less likely relative to the baseline.</li>
</ul>
</div>
<div id="application-multinomial-logistic-regression" class="section level3" number="7.7.7">
<h3>
<span class="header-section-number">7.7.7</span> Application: Multinomial Logistic Regression<a class="anchor" aria-label="anchor" href="#application-multinomial-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p><strong>1. Load Necessary Libraries and Data</strong></p>
<div class="sourceCode" id="cb297"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/julianfaraway/faraway">faraway</a></span><span class="op">)</span>  <span class="co"># For the dataset</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span>    <span class="co"># For data manipulation</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>  <span class="co"># For visualization</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span>     <span class="co"># For multinomial logistic regression</span></span>
<span></span>
<span><span class="co"># Load and inspect data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">nes96</span>, package<span class="op">=</span><span class="st">"faraway"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">nes96</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote</span></span>
<span><span class="co">#&gt; 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole</span></span>
<span><span class="co">#&gt; 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton</span></span>
<span><span class="co">#&gt; 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton</span></span></code></pre></div>
<p>The dataset <code>nes96</code> contains survey responses, including political party identification (<code>PID</code>), age (<code>age</code>), and education level (<code>educ</code>).</p>
<p><strong>2. Define Political Strength Categories</strong></p>
<p>We classify political strength into three categories:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Strong</strong>: Strong Democrat or Strong Republican</p></li>
<li><p><strong>Weak</strong>: Weak Democrat or Weak Republican</p></li>
<li><p><strong>Neutral</strong>: Independents and other affiliations</p></li>
</ol>
<div class="sourceCode" id="cb298"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Check distribution of political identity</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">nes96</span><span class="op">$</span><span class="va">PID</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  strDem weakDem  indDem  indind  indRep weakRep  strRep </span></span>
<span><span class="co">#&gt;     200     180     108      37      94     150     175</span></span>
<span></span>
<span><span class="co"># Define Political Strength variable</span></span>
<span><span class="va">nes96</span> <span class="op">&lt;-</span> <span class="va">nes96</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Political_Strength <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span></span>
<span>    <span class="va">PID</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"strDem"</span>, <span class="st">"strRep"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"Strong"</span>,</span>
<span>    <span class="va">PID</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"weakDem"</span>, <span class="st">"weakRep"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"Weak"</span>,</span>
<span>    <span class="va">PID</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"indDem"</span>, <span class="st">"indind"</span>, <span class="st">"indRep"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"Neutral"</span>,</span>
<span>    <span class="cn">TRUE</span> <span class="op">~</span> <span class="cn">NA_character_</span></span>
<span>  <span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summarize</span></span>
<span><span class="va">nes96</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Political_Strength</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>Count <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   Political_Strength Count</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;              &lt;int&gt;</span></span>
<span><span class="co">#&gt; 1 Neutral              239</span></span>
<span><span class="co">#&gt; 2 Strong               375</span></span>
<span><span class="co">#&gt; 3 Weak                 330</span></span></code></pre></div>
<p><strong>3. Visualizing Political Strength by Age</strong></p>
<p>We visualize the proportion of each political strength category across age groups.</p>
<div class="sourceCode" id="cb299"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Prepare data for visualization</span></span>
<span><span class="va">Plot_DF</span> <span class="op">&lt;-</span> <span class="va">nes96</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Age_Grp <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/cut_interval.html">cut_number</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Age_Grp</span>, <span class="va">Political_Strength</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>count <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span>, .groups <span class="op">=</span> <span class="st">'drop'</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Age_Grp</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>etotal <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span>, proportion <span class="op">=</span> <span class="va">count</span> <span class="op">/</span> <span class="va">etotal</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot age vs political strength</span></span>
<span><span class="va">Age_Plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    <span class="va">Plot_DF</span>,</span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>        x        <span class="op">=</span> <span class="va">Age_Grp</span>,</span>
<span>        y        <span class="op">=</span> <span class="va">proportion</span>,</span>
<span>        group    <span class="op">=</span> <span class="va">Political_Strength</span>,</span>
<span>        linetype <span class="op">=</span> <span class="va">Political_Strength</span>,</span>
<span>        color    <span class="op">=</span> <span class="va">Political_Strength</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Political Strength by Age Group"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Age Group"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Proportion"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display plot</span></span>
<span><span class="va">Age_Plot</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-39-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>4. Fit a Multinomial Logistic Model</strong></p>
<p>We model <strong>political strength</strong> as a function of <strong>age</strong> and <strong>education</strong>.</p>
<div class="sourceCode" id="cb300"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit multinomial logistic regression</span></span>
<span><span class="va">Multinomial_Model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/nnet/man/multinom.html">multinom</a></span><span class="op">(</span><span class="va">Political_Strength</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>             data <span class="op">=</span> <span class="va">nes96</span>,</span>
<span>             trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Multinomial_Model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; multinom(formula = Political_Strength ~ age + educ, data = nes96, </span></span>
<span><span class="co">#&gt;     trace = FALSE)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;        (Intercept)          age     educ.L     educ.Q     educ.C      educ^4</span></span>
<span><span class="co">#&gt; Strong -0.08788729  0.010700364 -0.1098951 -0.2016197 -0.1757739 -0.02116307</span></span>
<span><span class="co">#&gt; Weak    0.51976285 -0.004868771 -0.1431104 -0.2405395 -0.2411795  0.18353634</span></span>
<span><span class="co">#&gt;            educ^5     educ^6</span></span>
<span><span class="co">#&gt; Strong -0.1664377 -0.1359449</span></span>
<span><span class="co">#&gt; Weak   -0.1489030 -0.2173144</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Std. Errors:</span></span>
<span><span class="co">#&gt;        (Intercept)         age    educ.L    educ.Q    educ.C    educ^4</span></span>
<span><span class="co">#&gt; Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776</span></span>
<span><span class="co">#&gt; Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149</span></span>
<span><span class="co">#&gt;           educ^5    educ^6</span></span>
<span><span class="co">#&gt; Strong 0.2515012 0.2166774</span></span>
<span><span class="co">#&gt; Weak   0.2643747 0.2199186</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual Deviance: 2024.596 </span></span>
<span><span class="co">#&gt; AIC: 2056.596</span></span></code></pre></div>
<p><strong>5. Stepwise Model Selection Based on AIC</strong></p>
<p>We perform stepwise selection to find the best model.</p>
<div class="sourceCode" id="cb301"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Multinomial_Step</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span><span class="va">Multinomial_Model</span>, trace <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="co">#&gt; trying - age </span></span>
<span><span class="co">#&gt; trying - educ </span></span>
<span><span class="co">#&gt; trying - age</span></span>
<span><span class="va">Multinomial_Step</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; multinom(formula = Political_Strength ~ age, data = nes96, trace = FALSE)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;        (Intercept)          age</span></span>
<span><span class="co">#&gt; Strong -0.01988977  0.009832916</span></span>
<span><span class="co">#&gt; Weak    0.59497046 -0.005954348</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual Deviance: 2030.756 </span></span>
<span><span class="co">#&gt; AIC: 2038.756</span></span></code></pre></div>
<p>Compare the best model to the full model based on <strong>deviance</strong>:</p>
<div class="sourceCode" id="cb302"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span></span>
<span>    q <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">Multinomial_Step</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">Multinomial_Model</span><span class="op">)</span>,</span>
<span>    df <span class="op">=</span> <span class="va">Multinomial_Model</span><span class="op">$</span><span class="va">edf</span> <span class="op">-</span> <span class="va">Multinomial_Step</span><span class="op">$</span><span class="va">edf</span>,</span>
<span>    lower.tail <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.9078172</span></span></code></pre></div>
<p>A non-significant p-value suggests <strong>no major difference</strong> between the full and stepwise models.</p>
<p><strong>6. Predictions &amp; Visualization</strong></p>
<p>Predicting Political Strength Probabilities by Age</p>
<div class="sourceCode" id="cb303"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create data for prediction</span></span>
<span><span class="va">PlotData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">19</span>, to <span class="op">=</span> <span class="fl">91</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get predicted probabilities</span></span>
<span><span class="va">Preds</span> <span class="op">&lt;-</span> <span class="va">PlotData</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_cols.html">bind_cols</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Multinomial_Step</span>, </span>
<span>                                 <span class="va">PlotData</span>, </span>
<span>                                 type <span class="op">=</span> <span class="st">"probs"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot predicted probabilities across age</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">age</span>,</span>
<span>    y <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">Neutral</span>,</span>
<span>    type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>    ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.6</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"black"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Proportion"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Age"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">age</span>,</span>
<span>      y <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">Weak</span>,</span>
<span>      col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">age</span>,</span>
<span>      y <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">Strong</span>,</span>
<span>      col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>    <span class="st">"topleft"</span>,</span>
<span>    legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Neutral"</span>, <span class="st">"Weak"</span>, <span class="st">"Strong"</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span><span class="op">)</span>,</span>
<span>    lty <span class="op">=</span> <span class="fl">1</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-43-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Predict for Specific Ages</p>
<div class="sourceCode" id="cb304"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Predict class for a 34-year-old</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Multinomial_Step</span>, <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>age <span class="op">=</span> <span class="fl">34</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] Weak</span></span>
<span><span class="co">#&gt; Levels: Neutral Strong Weak</span></span>
<span></span>
<span><span class="co"># Predict probabilities for 34 and 35-year-olds</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Multinomial_Step</span>, <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">34</span>, <span class="fl">35</span><span class="op">)</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"probs"</span><span class="op">)</span></span>
<span><span class="co">#&gt;     Neutral    Strong      Weak</span></span>
<span><span class="co">#&gt; 1 0.2597275 0.3556910 0.3845815</span></span>
<span><span class="co">#&gt; 2 0.2594080 0.3587639 0.3818281</span></span></code></pre></div>
</div>
<div id="application-gamma-regression" class="section level3" number="7.7.8">
<h3>
<span class="header-section-number">7.7.8</span> Application: Gamma Regression<a class="anchor" aria-label="anchor" href="#application-gamma-regression"><i class="fas fa-link"></i></a>
</h3>
<p>When response variables are <strong>strictly positive</strong>, we use <strong>Gamma regression</strong>.</p>
<p><strong>1. Load and Prepare Data</strong></p>
<div class="sourceCode" id="cb305"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://kwstat.github.io/agridat/">agridat</a></span><span class="op">)</span>  <span class="co"># Agricultural dataset</span></span>
<span></span>
<span><span class="co"># Load and filter data</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu">agridat</span><span class="fu">::</span><span class="va"><a href="https://kwstat.github.io/agridat/reference/streibig.competition.html">streibig.competition</a></span></span>
<span><span class="va">gammaDat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">dat</span>, <span class="va">sseeds</span> <span class="op">&lt;</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Keep only barley</span></span>
<span><span class="va">gammaDat</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/transform.html">transform</a></span><span class="op">(</span><span class="va">gammaDat</span>,</span>
<span>              x <span class="op">=</span> <span class="va">bseeds</span>,</span>
<span>              y <span class="op">=</span> <span class="va">bdwt</span>,</span>
<span>              block <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">block</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><strong>2. Visualization of Inverse Yield</strong></p>
<div class="sourceCode" id="cb306"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">gammaDat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>color <span class="op">=</span> <span class="va">block</span>, shape <span class="op">=</span> <span class="va">block</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Inverse Yield vs Seeding Rate"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Seeding Rate"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Inverse Yield"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-46-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>3. Fit Gamma Regression Model</strong></p>
<p>Gamma regression models <strong>yield as a function of seeding rate</strong> using an inverse link: <span class="math display">\[
\eta_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \beta_2 x_{ij}^2, \quad Y_{ij} = \eta_{ij}^{-1}
\]</span></p>
<div class="sourceCode" id="cb307"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">block</span> <span class="op">+</span> <span class="va">block</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">block</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>,</span>
<span>          data <span class="op">=</span> <span class="va">gammaDat</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">Gamma</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"inverse"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = y ~ block + block * x + block * I(x^2), family = Gamma(link = "inverse"), </span></span>
<span><span class="co">#&gt;     data = gammaDat)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)     1.115e-01  2.870e-02   3.886 0.000854 ***</span></span>
<span><span class="co">#&gt; blockB2        -1.208e-02  3.880e-02  -0.311 0.758630    </span></span>
<span><span class="co">#&gt; blockB3        -2.386e-02  3.683e-02  -0.648 0.524029    </span></span>
<span><span class="co">#&gt; x              -2.075e-03  1.099e-03  -1.888 0.072884 .  </span></span>
<span><span class="co">#&gt; I(x^2)          1.372e-05  9.109e-06   1.506 0.146849    </span></span>
<span><span class="co">#&gt; blockB2:x       5.198e-04  1.468e-03   0.354 0.726814    </span></span>
<span><span class="co">#&gt; blockB3:x       7.475e-04  1.393e-03   0.537 0.597103    </span></span>
<span><span class="co">#&gt; blockB2:I(x^2) -5.076e-06  1.184e-05  -0.429 0.672475    </span></span>
<span><span class="co">#&gt; blockB3:I(x^2) -6.651e-06  1.123e-05  -0.592 0.560012    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for Gamma family taken to be 0.3232083)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 13.1677  on 29  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  7.8605  on 21  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 225.32</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p><strong>4. Predictions and Visualization</strong></p>
<div class="sourceCode" id="cb308"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate new data for prediction</span></span>
<span><span class="va">newdf</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">120</span>, length <span class="op">=</span> <span class="fl">50</span><span class="op">)</span>, </span>
<span>                block <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"B1"</span>, <span class="st">"B2"</span>, <span class="st">"B3"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predict responses</span></span>
<span><span class="va">newdf</span><span class="op">$</span><span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">m1</span>, newdata <span class="op">=</span> <span class="va">newdf</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot predictions</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">gammaDat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>color <span class="op">=</span> <span class="va">block</span>, shape <span class="op">=</span> <span class="va">block</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">newdf</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>        x <span class="op">=</span> <span class="va">x</span>,</span>
<span>        y <span class="op">=</span> <span class="va">pred</span>,</span>
<span>        color <span class="op">=</span> <span class="va">block</span>,</span>
<span>        linetype <span class="op">=</span> <span class="va">block</span></span>
<span>    <span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Predicted Yield by Seeding Rate"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Seeding Rate"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Yield"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-48-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="sec-generalization-of-generalized-linear-models" class="section level2" number="7.8">
<h2>
<span class="header-section-number">7.8</span> Generalization of Generalized Linear Models<a class="anchor" aria-label="anchor" href="#sec-generalization-of-generalized-linear-models"><i class="fas fa-link"></i></a>
</h2>
<p>We have seen that <a href="generalized-linear-models.html#sec-poisson-regression">Poisson regression</a> bears similarities to logistic regression. This insight leads us to a broader class of models known as <a href="generalized-linear-models.html#generalized-linear-models">Generalized Linear Models</a>, introduced by <span class="citation">Nelder and Wedderburn (<a href="references.html#ref-nelder1972generalized">1972</a>)</span>. These models provide a unified framework for handling different types of response variables while maintaining the fundamental principles of linear modeling.</p>
<div id="exponential-family" class="section level3" number="7.8.1">
<h3>
<span class="header-section-number">7.8.1</span> Exponential Family<a class="anchor" aria-label="anchor" href="#exponential-family"><i class="fas fa-link"></i></a>
</h3>
<p>The foundation of GLMs is built on the <strong>exponential family of distributions</strong>, which provides a flexible class of probability distributions that share a common form:</p>
<p><span class="math display">\[
f(y;\theta, \phi) = \exp\left(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi)\right)
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\theta\)</span> is the <strong>natural parameter</strong> (canonical parameter),</li>
<li>
<span class="math inline">\(\phi\)</span> is the <strong>dispersion parameter</strong>,</li>
<li>
<span class="math inline">\(a(\phi)\)</span>, <span class="math inline">\(b(\theta)\)</span>, and <span class="math inline">\(c(y, \phi)\)</span> are functions ensuring the proper distributional form.</li>
</ul>
<p><strong>Distributions in the Exponential Family that Can Be Used in GLMs:</strong></p>
<ol style="list-style-type: decimal">
<li><p><a href="prerequisites.html#normal-distribution">Normal Distribution</a></p></li>
<li><p><a href="prerequisites.html#binomial-distribution">Binomial Distribution</a></p></li>
<li><p><a href="prerequisites.html#poisson-distribution">Poisson Distribution</a></p></li>
<li><p><a href="prerequisites.html#gamma-distribution">Gamma Distribution</a></p></li>
<li><p>Inverse Gaussian Distribution</p></li>
<li><p>Negative Binomial Distribution (used in GLMs but requires overdispersion adjustments)</p></li>
<li><p>Multinomial Distribution (for categorical response)</p></li>
</ol>
<p><strong>Exponential Family Distributions Not Commonly Used in GLMs:</strong></p>
<ol style="list-style-type: decimal">
<li><p>Beta Distribution</p></li>
<li><p>Dirichlet Distribution</p></li>
<li><p>Wishart Distribution</p></li>
<li><p><a href="prerequisites.html#geometric-distribution">Geometric Distribution</a></p></li>
<li><p>Exponential Distribution (can be used indirectly through survival models)</p></li>
</ol>
<hr>
<p><strong>Example: Normal Distribution</strong></p>
<p>Consider a normally distributed response variable <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>. The probability density function (PDF) is:</p>
<p><span class="math display">\[
\begin{aligned}
f(y; \mu, \sigma^2) &amp;= \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) \\
&amp;= \exp\left(-\frac{y^2 - 2y\mu + \mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\right)
\end{aligned}
\]</span></p>
<p>Rewriting in exponential family form:</p>
<p><span class="math display">\[
\begin{aligned}
f(y; \mu, \sigma^2) &amp;= \exp\left(\frac{y \mu - \frac{\mu^2}{2}}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\right) \\
&amp;= \exp\left(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi)\right)
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<strong>Natural parameter</strong>: <span class="math inline">\(\theta = \mu\)</span>
</li>
<li>
<strong>Function</strong> <span class="math inline">\(b(\theta)\)</span>: <span class="math inline">\(b(\theta) = \frac{\mu^2}{2}\)</span>
</li>
<li>
<strong>Dispersion function</strong>: <span class="math inline">\(a(\phi) = \sigma^2 = \phi\)</span>
</li>
<li>
<strong>Function</strong> <span class="math inline">\(c(y, \phi)\)</span>: <span class="math inline">\(c(y, \phi) = -\frac{1}{2} \left(\frac{y^2}{\phi} + \log(2\pi \sigma^2)\right)\)</span>
</li>
</ul>
</div>
<div id="properties-of-glm-exponential-families" class="section level3" number="7.8.2">
<h3>
<span class="header-section-number">7.8.2</span> Properties of GLM Exponential Families<a class="anchor" aria-label="anchor" href="#properties-of-glm-exponential-families"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p><strong>Expected Value (Mean)</strong> <span class="math display">\[
E(Y) = b'(\theta)
\]</span> where <span class="math inline">\(b'(\theta) = \frac{\partial b(\theta)}{\partial \theta}\)</span>. (Note: <code>'</code> is “prime,” not transpose).</p></li>
<li>
<p><strong>Variance</strong> <span class="math display">\[
\text{Var}(Y) = a(\phi) b''(\theta) = a(\phi) V(\mu)
\]</span> where:</p>
<ul>
<li>
<span class="math inline">\(V(\mu) = b''(\theta)\)</span> is the <strong>variance function</strong>, though it only represents the variance when <span class="math inline">\(a(\phi) = 1\)</span>.</li>
</ul>
</li>
<li><p>If <span class="math inline">\(a(\phi)\)</span>, <span class="math inline">\(b(\theta)\)</span>, and <span class="math inline">\(c(y, \phi)\)</span> are identifiable, we can derive the expected value and variance of <span class="math inline">\(Y\)</span>.</p></li>
</ol>
<hr>
<p><strong>Examples of Exponential Family Distributions</strong></p>
<p><strong>1. Normal Distribution</strong></p>
<p>For a normal distribution <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>, the exponential family representation is:</p>
<p><span class="math display">\[
f(y; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
\]</span></p>
<p>which can be rewritten in exponential form:</p>
<p><span class="math display">\[
\exp \left( \frac{y\mu - \frac{1}{2} \mu^2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2) \right)
\]</span></p>
<p>From this, we identify:</p>
<ul>
<li><span class="math inline">\(\theta = \mu\)</span></li>
<li><span class="math inline">\(b(\theta) = \frac{\theta^2}{2}\)</span></li>
<li><span class="math inline">\(a(\phi) = \sigma^2\)</span></li>
</ul>
<p>Computing derivatives:</p>
<p><span class="math display">\[
b'(\theta) = \frac{\partial b(\theta)}{\partial \theta} = \mu, \quad V(\mu) = b''(\theta) = 1
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
E(Y) = \mu, \quad \text{Var}(Y) = a(\phi) V(\mu) = \sigma^2
\]</span></p>
<hr>
<p><strong>2. Poisson Distribution</strong></p>
<p>For a Poisson-distributed response <span class="math inline">\(Y \sim \text{Poisson}(\mu)\)</span>, the probability mass function is:</p>
<p><span class="math display">\[
f(y; \mu) = \frac{\mu^y e^{-\mu}}{y!}
\]</span></p>
<p>Rewriting in exponential form:</p>
<p><span class="math display">\[
\exp(y \log \mu - \mu - \log y!)
\]</span></p>
<p>Thus, we identify:</p>
<ul>
<li><span class="math inline">\(\theta = \log \mu\)</span></li>
<li><span class="math inline">\(b(\theta) = e^\theta\)</span></li>
<li><span class="math inline">\(a(\phi) = 1\)</span></li>
<li><span class="math inline">\(c(y, \phi) = \log y!\)</span></li>
</ul>
<p>Computing derivatives:</p>
<p><span class="math display">\[
E(Y) = b'(\theta) = e^\theta = \mu, \quad \text{Var}(Y) = b''(\theta) = \mu
\]</span></p>
<p>Since <span class="math inline">\(\mu = E(Y)\)</span>, we confirm the variance function:</p>
<p><span class="math display">\[
\text{Var}(Y) = V(\mu) = \mu
\]</span></p>
<hr>
</div>
<div id="structure-of-a-generalized-linear-model" class="section level3" number="7.8.3">
<h3>
<span class="header-section-number">7.8.3</span> Structure of a Generalized Linear Model<a class="anchor" aria-label="anchor" href="#structure-of-a-generalized-linear-model"><i class="fas fa-link"></i></a>
</h3>
<p>In a GLM, we model the mean <span class="math inline">\(\mu\)</span> through a <strong>link function</strong> that connects it to a linear predictor:</p>
<p><span class="math display">\[
g(\mu) = g(b'(\theta)) = \mathbf{x' \beta}
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\mu = g^{-1}(\mathbf{x' \beta})
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(g(\cdot)\)</span> is the <strong>link function</strong>, which ensures a transformation between the expected response <span class="math inline">\(E(Y) = \mu\)</span> and the linear predictor.</li>
<li>
<span class="math inline">\(\eta = \mathbf{x' \beta}\)</span> is called the <strong>linear predictor</strong>.</li>
</ul>
<hr>
</div>
<div id="components-of-a-glm" class="section level3" number="7.8.4">
<h3>
<span class="header-section-number">7.8.4</span> Components of a GLM<a class="anchor" aria-label="anchor" href="#components-of-a-glm"><i class="fas fa-link"></i></a>
</h3>
<p>A GLM consists of two main components:</p>
<div id="random-component" class="section level4" number="7.8.4.1">
<h4>
<span class="header-section-number">7.8.4.1</span> Random Component<a class="anchor" aria-label="anchor" href="#random-component"><i class="fas fa-link"></i></a>
</h4>
<p>This describes the <strong>distribution of the response variable</strong> <span class="math inline">\(Y_1, \dots, Y_n\)</span>. The response variables are assumed to follow a distribution from the <strong>exponential family</strong>, which can be written as:</p>
<p><span class="math display">\[
f(y_i ; \theta_i, \phi) = \exp \left( \frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right)
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(Y_i\)</span> are <strong>independent</strong> random variables.</p></li>
<li><p>The <strong>canonical parameter</strong> <span class="math inline">\(\theta_i\)</span> may differ for each observation.</p></li>
<li><p>The <strong>dispersion parameter</strong> <span class="math inline">\(\phi\)</span> is assumed to be constant across all <span class="math inline">\(i\)</span>.</p></li>
<li>
<p>The mean response is given by:</p>
<p><span class="math display">\[
\mu_i = E(Y_i)
\]</span></p>
</li>
</ul>
</div>
<div id="systematic-component" class="section level4" number="7.8.4.2">
<h4>
<span class="header-section-number">7.8.4.2</span> Systematic Component<a class="anchor" aria-label="anchor" href="#systematic-component"><i class="fas fa-link"></i></a>
</h4>
<p>This specifies how the mean response <span class="math inline">\(\mu\)</span> is related to the explanatory variables <span class="math inline">\(\mathbf{x}\)</span> through a <strong>linear predictor</strong> <span class="math inline">\(\eta\)</span>:</p>
<ul>
<li>
<p>The systematic component consists of:</p>
<ul>
<li>A <strong>link function</strong> <span class="math inline">\(g(\mu)\)</span>.</li>
<li>A <strong>linear predictor</strong> <span class="math inline">\(\eta = \mathbf{x' \beta}\)</span>.</li>
</ul>
</li>
<li>
<p><strong>Notation</strong>:</p>
<ul>
<li>
<p>We assume:</p>
<p><span class="math display">\[
g(\mu_i) = \mathbf{x' \beta} = \eta_i
\]</span></p>
</li>
<li><p>The parameter vector <span class="math inline">\(\mathbf{\beta} = (\beta_1, \dots, \beta_p)'\)</span> needs to be estimated.</p></li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="canonical-link" class="section level3" number="7.8.5">
<h3>
<span class="header-section-number">7.8.5</span> Canonical Link<a class="anchor" aria-label="anchor" href="#canonical-link"><i class="fas fa-link"></i></a>
</h3>
<p>In a GLM, a link function <span class="math inline">\(g(\cdot)\)</span> relates the mean <span class="math inline">\(\mu_i\)</span> of the response <span class="math inline">\(Y_i\)</span> to the linear predictor <span class="math inline">\(\eta_i\)</span> via</p>
<p><span class="math display">\[
\eta_i = g(\mu_i).
\]</span></p>
<p>A <strong>canonical link</strong> is a special case of <span class="math inline">\(g(\cdot)\)</span> where</p>
<p><span class="math display">\[
g(\mu_i) = \eta_i = \theta_i,
\]</span></p>
<p>and <span class="math inline">\(\theta_i\)</span> is the <strong>natural parameter</strong> of the exponential family. In other words, the link function directly equates the linear predictor <span class="math inline">\(\eta_i\)</span> with the distribution’s natural parameter <span class="math inline">\(\theta_i\)</span>. Hence, <span class="math inline">\(g(\mu)\)</span> is <strong>canonical</strong> if <span class="math inline">\(g(\mu) = \theta\)</span>.</p>
<p><strong>Exponential Family Components</strong></p>
<ul>
<li>
<span class="math inline">\(b(\theta)\)</span>: the <strong>cumulant (moment generating) function</strong>, which defines the variance function.</li>
<li>
<span class="math inline">\(g(\mu)\)</span>: the <strong>link function</strong>, which must be
<ul>
<li>Monotonically increasing</li>
<li>Continuously differentiable</li>
<li>Invertible</li>
</ul>
</li>
</ul>
<div class="float">
<img src="images/GLM.PNG" style="width:90.0%" alt="GLM Structure"><div class="figcaption">GLM Structure</div>
</div>
<p>For an exponential-family distribution, the function <span class="math inline">\(b(\theta)\)</span> is called the <em>cumulant moment generating function</em>, and it relates <span class="math inline">\(\theta\)</span> to the mean via its derivative:</p>
<p><span class="math display">\[
\mu = b'(\theta)
\quad\Longleftrightarrow\quad
\theta = b'^{-1}(\mu).
\]</span></p>
<p>By defining the link so that <span class="math inline">\(g(\mu) = \theta\)</span>, we impose <span class="math inline">\(\eta_i = \theta_i\)</span>, which is why <span class="math inline">\(g(\cdot)\)</span> is termed <strong>canonical</strong> in this setting.</p>
<p>When the link is canonical, an equivalent way to express this is</p>
<p><span class="math display">\[
\gamma^{-1} \circ g^{-1} = I,
\]</span></p>
<p>indicating that the inverse link <span class="math inline">\(g^{-1}(\cdot)\)</span> directly maps the linear predictor (now the natural parameter <span class="math inline">\(\theta\)</span>) back to <span class="math inline">\(\mu\)</span> in a way that respects the structure of the exponential family.</p>
<p>Choosing <span class="math inline">\(g(\cdot)\)</span> to be canonical often simplifies mathematical derivations and computations—especially for parameter estimation—because the linear predictor <span class="math inline">\(\eta\)</span> and the natural parameter <span class="math inline">\(\theta\)</span> coincide. Common examples of canonical links include:</p>
<ul>
<li>
<strong>Identity link</strong> for the normal (Gaussian) distribution</li>
<li>
<strong>Log link</strong> for the Poisson distribution</li>
<li>
<strong>Logit link</strong> for the Bernoulli (binomial) distribution</li>
</ul>
<p>In each case, setting <span class="math inline">\(\eta = \theta\)</span> streamlines the relationship between the mean and the linear predictor, making the model both elegant and practically convenient.</p>
<hr>
</div>
<div id="inverse-link-functions" class="section level3" number="7.8.6">
<h3>
<span class="header-section-number">7.8.6</span> Inverse Link Functions<a class="anchor" aria-label="anchor" href="#inverse-link-functions"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>inverse link function</strong> <span class="math inline">\(g^{-1}(\eta)\)</span> (also called the <strong>mean function</strong>) transforms the linear predictor <span class="math inline">\(\eta\)</span> (which can take any real value) into a <strong>valid mean response</strong> <span class="math inline">\(\mu\)</span>.</p>
<p>Example 1: Normal Distribution (Identity Link)</p>
<ul>
<li><p>Random Component: <span class="math inline">\(Y_i \sim N(\mu_i, \sigma^2)\)</span>.</p></li>
<li><p>Mean Response: <span class="math inline">\(\mu_i = \theta_i\)</span>.</p></li>
<li>
<p>Canonical Link Function:</p>
<p><span class="math display">\[
g(\mu_i) = \mu_i
\]</span></p>
<p>(i.e., the identity function).</p>
</li>
</ul>
<hr>
<p>Example 2: Binomial Distribution (Logit Link)</p>
<ul>
<li><p>Random Component: <span class="math inline">\(Y_i \sim \text{Binomial}(n_i, p_i)\)</span>.</p></li>
<li>
<p>Mean Response:</p>
<p><span class="math display">\[
\mu_i = \frac{n_i e^{\theta_i}}{1+e^{\theta_i}}
\]</span></p>
</li>
<li>
<p>Canonical Link Function:</p>
<p><span class="math display">\[
g(\mu_i) = \log \left( \frac{\mu_i}{n_i - \mu_i} \right)
\]</span></p>
<p>(Logit link function).</p>
</li>
</ul>
<hr>
<p>Example 3: Poisson Distribution (Log Link)</p>
<ul>
<li><p>Random Component: <span class="math inline">\(Y_i \sim \text{Poisson}(\mu_i)\)</span>.</p></li>
<li>
<p>Mean Response:</p>
<p><span class="math display">\[
\mu_i = e^{\theta_i}
\]</span></p>
</li>
<li>
<p>Canonical Link Function:</p>
<p><span class="math display">\[
g(\mu_i) = \log(\mu_i)
\]</span></p>
<p>(Log link function).</p>
</li>
</ul>
<hr>
<p>Example 4: Gamma Distribution (Inverse Link)</p>
<ul>
<li><p>Random Component: <span class="math inline">\(Y_i \sim \text{Gamma}(\alpha, \mu_i)\)</span>.</p></li>
<li>
<p>Mean Response:</p>
<p><span class="math display">\[
\mu_i = -\frac{1}{\theta_i}
\]</span></p>
</li>
<li>
<p>Canonical Link Function:</p>
<p><span class="math display">\[
g(\mu_i) = -\frac{1}{\mu_i}
\]</span></p>
<p>(Inverse link function).</p>
</li>
</ul>
<hr>
<p>The following table presents common <strong>GLM link functions</strong> and their corresponding <strong>inverse functions</strong>.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="23%">
<col width="39%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th>Link</th>
<th><span class="math inline">\(\eta_i = g(\mu_i)\)</span></th>
<th><span class="math inline">\(\mu_i = g^{-1}(\eta_i)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Identity</td>
<td><span class="math inline">\(\mu_i\)</span></td>
<td><span class="math inline">\(\eta_i\)</span></td>
</tr>
<tr class="even">
<td>Log</td>
<td><span class="math inline">\(\log_e \mu_i\)</span></td>
<td><span class="math inline">\(e^{\eta_i}\)</span></td>
</tr>
<tr class="odd">
<td>Inverse</td>
<td><span class="math inline">\(\mu_i^{-1}\)</span></td>
<td><span class="math inline">\(\eta_i^{-1}\)</span></td>
</tr>
<tr class="even">
<td>Inverse-square</td>
<td><span class="math inline">\(\mu_i^{-2}\)</span></td>
<td><span class="math inline">\(\eta_i^{-1/2}\)</span></td>
</tr>
<tr class="odd">
<td>Square-root</td>
<td><span class="math inline">\(\sqrt{\mu_i}\)</span></td>
<td><span class="math inline">\(\eta_i^2\)</span></td>
</tr>
<tr class="even">
<td>Logit</td>
<td><span class="math inline">\(\log_e \left( \frac{\mu_i}{1 - \mu_i} \right)\)</span></td>
<td><span class="math inline">\(\frac{1}{1 + e^{-\eta_i}}\)</span></td>
</tr>
<tr class="odd">
<td>Probit</td>
<td><span class="math inline">\(\Phi^{-1}(\mu_i)\)</span></td>
<td><span class="math inline">\(\Phi(\eta_i)\)</span></td>
</tr>
</tbody>
</table></div>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\mu_i\)</span> is the expected value of the response.</p></li>
<li><p><span class="math inline">\(\eta_i\)</span> is the linear predictor.</p></li>
<li><p><span class="math inline">\(\Phi(\cdot)\)</span> represents the CDF of the standard normal distribution.</p></li>
</ul>
<hr>
</div>
<div id="estimation-of-parameters-in-glms" class="section level3" number="7.8.7">
<h3>
<span class="header-section-number">7.8.7</span> Estimation of Parameters in GLMs<a class="anchor" aria-label="anchor" href="#estimation-of-parameters-in-glms"><i class="fas fa-link"></i></a>
</h3>
<p>The GLM framework extends <a href="linear-regression.html#linear-regression">Linear Regression</a> by allowing for response variables that follow <strong>exponential family distributions</strong>.</p>
<ul>
<li><p><a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> Estimation is used to estimate the parameters of the systematic component (<span class="math inline">\(\beta\)</span>), providing a consistent and efficient approach. The derivation and computation processes are unified, thanks to the exponential form of the model, which simplifies mathematical treatment and implementation.</p></li>
<li><p>However, this unification does not extend to the estimation of the dispersion parameter (<span class="math inline">\(\phi\)</span>), which requires separate treatment, often involving alternative estimation methods such as moment-based approaches or quasi-likelihood estimation.</p></li>
</ul>
<p>In GLMs, the response variable <span class="math inline">\(Y_i\)</span> follows an <strong>exponential family distribution</strong> characterized by the density function:</p>
<p><span class="math display">\[
f(y_i ; \theta_i, \phi) = \exp\left(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right)
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\theta_i\)</span> is the <strong>canonical parameter</strong>.</li>
<li>
<span class="math inline">\(\phi\)</span> is the <strong>dispersion parameter</strong> (which may be known or estimated separately).</li>
<li>
<span class="math inline">\(b(\theta_i)\)</span> determines the mean and variance of <span class="math inline">\(Y_i\)</span>.</li>
<li>
<span class="math inline">\(a(\phi)\)</span> scales the variance.</li>
<li>
<span class="math inline">\(c(y_i, \phi)\)</span> ensures proper normalization.</li>
</ul>
<p>For this family, we obtain:</p>
<ul>
<li>
<strong>Mean of</strong> <span class="math inline">\(Y_i\)</span>: <span class="math display">\[
E(Y_i) = \mu_i = b'(\theta_i)
\]</span>
</li>
<li>
<strong>Variance of</strong> <span class="math inline">\(Y_i\)</span>: <span class="math display">\[
\text{Var}(Y_i) = b''(\theta_i) a(\phi) = V(\mu_i)a(\phi)
\]</span> where <span class="math inline">\(V(\mu_i)\)</span> is the variance function.</li>
<li>
<strong>Systematic component (link function):</strong> <span class="math display">\[
g(\mu_i) = \eta_i = \mathbf{x}_i' \beta
\]</span>
</li>
</ul>
<p>The function <span class="math inline">\(g(\cdot)\)</span> is the link function, which connects the expected response <span class="math inline">\(\mu_i\)</span> to the linear predictor <span class="math inline">\(\mathbf{x}_i' \beta\)</span>.</p>
<hr>
<p>For a single observation <span class="math inline">\(Y_i\)</span>, the log-likelihood function is:</p>
<p><span class="math display">\[
l_i(\beta, \phi) = \frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi)
\]</span></p>
<p>For <span class="math inline">\(n\)</span> independent observations, the total log-likelihood is:</p>
<p><span class="math display">\[
l(\beta, \phi) = \sum_{i=1}^n l_i(\beta, \phi)
\]</span></p>
<p>Expanding this,</p>
<p><span class="math display">\[
l(\beta, \phi) = \sum_{i=1}^n \left( \frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right).
\]</span></p>
<p>To estimate <span class="math inline">\(\beta\)</span>, we maximize this log-likelihood function.</p>
<hr>
<div id="estimation-of-systematic-component-beta" class="section level4" number="7.8.7.1">
<h4>
<span class="header-section-number">7.8.7.1</span> Estimation of Systematic Component (<span class="math inline">\(\beta\)</span>)<a class="anchor" aria-label="anchor" href="#estimation-of-systematic-component-beta"><i class="fas fa-link"></i></a>
</h4>
<p>To differentiate <span class="math inline">\(l(\beta,\phi)\)</span> with respect to <span class="math inline">\(\beta_j\)</span>, we apply the chain rule:</p>
<p><span class="math display">\[
\frac{\partial l_i(\beta,\phi)}{\partial \beta_j} =
\underbrace{\frac{\partial l_i(\beta,\phi)}{\partial \theta_i}}_{\text{depends on }(y_i - \mu_i)}
\times
\underbrace{\frac{\partial \theta_i}{\partial \mu_i}}_{= 1/V(\mu_i)\text{ if canonical link}}
\times
\underbrace{\frac{\partial \mu_i}{\partial \eta_i}}_{\text{depends on the link}}
\times
\underbrace{\frac{\partial \eta_i}{\partial \beta_j}}_{= x_{ij}}.
\]</span></p>
<p>Let us see why these four pieces appear:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(l_i(\beta,\phi)\)</span> depends on <span class="math inline">\(\theta_i\)</span>. So we start by computing <span class="math inline">\(\frac{\partial l_i}{\partial \theta_i}\)</span>.</p></li>
<li>
<p><span class="math inline">\(\theta_i\)</span> (the “natural parameter” in the exponential family) may in turn be a function of <span class="math inline">\(\mu_i\)</span>.</p>
<ul>
<li><p>In <strong>canonical</strong>‐link GLMs, we often have <span class="math inline">\(\theta_i = \eta_i\)</span>.</p></li>
<li><p>In more <strong>general</strong>‐link GLMs,<span class="math inline">\(\theta_i\)</span> is still some function of <span class="math inline">\(\mu_i\)</span>.<br>
Hence we need <span class="math inline">\(\frac{\partial \theta_i}{\partial \mu_i}\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math inline">\(\mu_i\)</span> (the mean) is a function of the linear predictor <span class="math inline">\(\eta_i\)</span>. Typically, <span class="math inline">\(\eta_i = g(\mu_i)\)</span> implies <span class="math inline">\(\mu_i = g^{-1}(\eta_i)\)</span>. So we need <span class="math inline">\(\frac{\partial \mu_i}{\partial \eta_i}\)</span>.</p></li>
<li><p>Finally, <span class="math inline">\(\eta_i = \mathbf{x}_i^\prime \beta\)</span>. So the derivative <span class="math inline">\(\frac{\partial \eta_i}{\partial \beta_j}\)</span> is simply <span class="math inline">\(x_{ij}\)</span>, the <span class="math inline">\(j\)</span>‐th component of the covariate vector <span class="math inline">\(\mathbf{x}_i\)</span>.</p></li>
</ol>
<p>Let us look at each factor in turn.</p>
<p><strong>First term:</strong></p>
<p><span class="math display">\[
\displaystyle \frac{\partial l_i(\beta,\phi)}{\partial \theta_i}
\]</span></p>
<p>Recall</p>
<p><span class="math display">\[
l_i(\theta_i,\phi) = \frac{\theta_i \,y_i - b(\theta_i)}{a(\phi)} + c(y_i,\phi).
\]</span></p>
<ol style="list-style-type: decimal">
<li>Differentiate <span class="math inline">\(\theta_i y_i - b(\theta_i)\)</span> with respect to <span class="math inline">\(\theta_i\)</span>: <span class="math display">\[
\frac{\partial}{\partial \theta_i} \left[\theta_i\,y_i - b(\theta_i)\right] = y_i - b'(\theta_i).
\]</span> But by exponential‐family definitions,<span class="math inline">\(b'(\theta_i) = \mu_i\)</span>.<br>
So that is <span class="math inline">\(y_i - \mu_i\)</span>.</li>
<li>Since everything is divided by <span class="math inline">\(a(\phi)\)</span>, we get <span class="math display">\[
\frac{\partial l_i}{\partial \theta_i} = \frac{1}{a(\phi)}\,[\,y_i - \mu_i\,].
\]</span> Hence, <span class="math display">\[
\boxed{ \frac{\partial l_i(\beta,\phi)}{\partial \theta_i} = \frac{y_i - \mu_i}{a(\phi)}. }
\]</span>
</li>
</ol>
<p><strong>Second term:</strong> <span class="math display">\[
\displaystyle \frac{\partial \theta_i}{\partial \mu_i}
\]</span> 1. In an exponential family with <strong>canonical link</strong>, we have <span class="math display">\[
    \theta_i = \eta_i = \mathbf{x}_i^\prime \beta.
    \]</span> Then <span class="math inline">\(\theta_i\)</span> is actually the same as <span class="math inline">\(\eta_i\)</span>, which is the same as <span class="math inline">\(g(\mu_i)\)</span>. Recall also that if <span class="math inline">\(\mu_i = b'(\theta_i)\)</span>, then <span class="math inline">\(d\mu_i/d\theta_i = b''(\theta_i)\)</span>. But <span class="math inline">\(b''(\theta_i) = V(\mu_i)\)</span>. Hence <span class="math display">\[
    \frac{d \mu_i}{d \theta_i} = V(\mu_i) \quad\Longrightarrow\quad \frac{d \theta_i}{d \mu_i} = \frac{1}{V(\mu_i)}.
    \]</span> This identity is a well‐known property of <strong>canonical</strong> links in the exponential family.</p>
<ol start="2" style="list-style-type: decimal">
<li>In more general (non‐canonical) links, <span class="math inline">\(\theta_i\)</span> may be some other smooth function of <span class="math inline">\(\mu_i\)</span>. The key idea is: if <span class="math inline">\(\mu_i = b'(\theta_i)\)</span> but <span class="math inline">\(\eta_i \neq \theta_i\)</span>, you would have to carefully derive <span class="math inline">\(\partial \theta_i / \partial \mu_i\)</span>. Often, a canonical link is assumed to keep expressions simpler.</li>
</ol>
<p>If we <strong>assume</strong> a canonical link throughout, then</p>
<p><span class="math display">\[
\boxed{ \frac{\partial \theta_i}{\partial \mu_i} = \frac{1}{V(\mu_i)}. }
\]</span></p>
<p><strong>Third term:</strong></p>
<p><span class="math display">\[
\displaystyle \frac{\partial \mu_i}{\partial \eta_i}
\]</span></p>
<p>Here we consider the link function <span class="math inline">\(g(\cdot)\)</span>, defined by</p>
<p><span class="math display">\[
\eta_i = g(\mu_i) \quad\Longrightarrow\quad \mu_i = g^{-1}(\eta_i).
\]</span></p>
<p>For example,</p>
<ul>
<li>In a Bernoulli (logistic‐regression) model, <span class="math inline">\(μg(\mu) = \log\frac{\mu}{1-\mu}\)</span>. So <span class="math inline">\(\mu = g^{-1}(\eta) = \frac{1}{1+e^{-\eta}}\)</span>. Then <span class="math inline">\(\frac{d\mu}{d\eta} = \mu\,(1-\mu)\)</span>.</li>
<li>For a Poisson (log) link, <span class="math inline">\(g(\mu) = \log(\mu)\)</span>. So <span class="math inline">\(\mu = e^\eta\)</span>. Then <span class="math inline">\(\frac{d\mu}{d\eta} = e^\eta = \mu\)</span>.</li>
<li>For an identity link, <span class="math inline">\(g(\mu) = \mu\)</span>. Then <span class="math inline">\(\eta = \mu\)</span> and <span class="math inline">\(\frac{d\mu}{d\eta} = 1\)</span>.</li>
</ul>
<p>In general,</p>
<p><span class="math display">\[
\boxed{ \frac{\partial \mu_i}{\partial \eta_i} = \left.\frac{d}{d\eta}\left[g^{-1}(\eta)\right]\right|_{\eta=\eta_i} = \left(g^{-1}\right)'(\eta_i). }
\]</span></p>
<p>If the link is <em>also</em> canonical, then typically <span class="math inline">\(\frac{\partial \mu_i}{\partial \eta_i} = V(\mu_i)\)</span>. Indeed, that is consistent with the second term result.</p>
<p><strong>Fourth term:</strong></p>
<p><span class="math display">\[
\displaystyle \frac{\partial \eta_i}{\partial \beta_j}
\]</span></p>
<p>Finally, the linear predictor is</p>
<p><span class="math display">\[
\eta_i = \mathbf{x}_i^\prime \beta = \sum_{k=1}^p x_{ik}\,\beta_k.
\]</span></p>
<p>Hence, the derivative of <span class="math inline">\(\eta_i\)</span> with respect to <span class="math inline">\(\beta_j\)</span> is simply</p>
<p><span class="math display">\[
\boxed{ \frac{\partial \eta_i}{\partial \beta_j} = x_{ij}. }
\]</span></p>
<p>Therefore, for the <strong>entire</strong> log‐likelihood <span class="math inline">\(l(\beta, \phi) = \sum_{i=1}^n l_i(\beta,\phi)\)</span>, we sum over <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
\boxed{ \frac{\partial l(\beta,\phi)}{\partial \beta_j} = \sum_{i=1}^n \left[ \frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)} \times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij} \right] }
\]</span></p>
<hr>
<p>To simplify expressions, we define the weight:</p>
<p><span class="math display">\[
w_i = \left(\left(\frac{\partial \eta_i}{\partial \mu_i}\right)^2 V(\mu_i)\right)^{-1}.
\]</span></p>
<p>For canonical links, this often simplifies to <span class="math inline">\(w_i = V(\mu_i)\)</span>, such as:</p>
<ul>
<li>
<strong>Bernoulli (logit link)</strong>: <span class="math inline">\(w_i = p_i(1-p_i)\)</span>.</li>
<li>
<strong>Poisson (log link)</strong>: <span class="math inline">\(w_i = \mu_i\)</span>.</li>
</ul>
<p>Rewriting the score function in terms of <span class="math inline">\(w_i\)</span>:</p>
<p><span class="math display">\[
\frac{\partial l(\beta,\phi)}{\partial \beta_j} =
\sum_{i=1}^n
\left[
\frac{y_i - \mu_i}{a(\phi)}
\times
w_i
\times
\frac{\partial \eta_i}{\partial \mu_i}
\times
x_{ij}
\right].
\]</span></p>
<p>To use the <a href="non-linear-regression.html#newton-raphson">Newton-Raphson Algorithm</a> for estimating <span class="math inline">\(\beta\)</span>, we require the expected second derivative:</p>
<p><span class="math display">\[
- E\left(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}\right),
\]</span></p>
<p>which corresponds to the <span class="math inline">\((j,k)\)</span>-th element of the Fisher information matrix <span class="math inline">\(\mathbf{I}(\beta)\)</span>:</p>
<p><span class="math display">\[
\mathbf{I}_{jk}(\beta) = - E\left(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}\right) = \sum_{i=1}^n \frac{w_i}{a(\phi)}x_{ij}x_{ik}.
\]</span></p>
<hr>
<p><strong>Example: Bernoulli Model with Logit Link</strong></p>
<p>For a Bernoulli response with a logit link function (canonical link), we have:</p>
<p><span class="math display">\[
\begin{aligned}
b(\theta) &amp;= \log(1 + \exp(\theta)) = \log(1 + \exp(\mathbf{x'}\beta)), \\
a(\phi) &amp;= 1, \\
c(y_i, \phi) &amp;= 0.
\end{aligned}
\]</span></p>
<p>From the mean and link function:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y) = b'(\theta) &amp;= \frac{\exp(\theta)}{1 + \exp(\theta)} = \mu = p, \\
\eta = g(\mu) &amp;= \log\left(\frac{\mu}{1-\mu}\right) = \theta = \log\left(\frac{p}{1-p}\right) = \mathbf{x'}\beta.
\end{aligned}
\]</span></p>
<p>The log-likelihood for <span class="math inline">\(Y_i\)</span> is:</p>
<p><span class="math display">\[
l_i (\beta, \phi) = \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) = y_i \mathbf{x'}_i \beta - \log(1+ \exp(\mathbf{x'}\beta)).
\]</span></p>
<p>We also obtain:</p>
<p><span class="math display">\[
\begin{aligned}
V(\mu_i) &amp;= \mu_i(1-\mu_i) = p_i (1-p_i), \\
\frac{\partial \mu_i}{\partial \eta_i} &amp;= p_i(1-p_i).
\end{aligned}
\]</span></p>
<p>Thus, the first derivative simplifies as:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l(\beta, \phi)}{\partial \beta_j} &amp;= \sum_{i=1}^n \left[\frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)}\times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij} \right] \\
&amp;= \sum_{i=1}^n (y_i - p_i) \times \frac{1}{p_i(1-p_i)} \times p_i(1-p_i) \times x_{ij} \\
&amp;= \sum_{i=1}^n (y_i - p_i) x_{ij} \\
&amp;= \sum_{i=1}^n \left(y_i - \frac{\exp(\mathbf{x'}_i\beta)}{1+ \exp(\mathbf{x'}_i\beta)}\right)x_{ij}.
\end{aligned}
\]</span></p>
<p>The weight term in this case is:</p>
<p><span class="math display">\[
w_i = \left(\left(\frac{\partial \eta_i}{\partial \mu_i} \right)^2 V(\mu_i)\right)^{-1} = p_i (1-p_i).
\]</span></p>
<p>Thus, the Fisher information matrix has elements:</p>
<p><span class="math display">\[
\mathbf{I}_{jk}(\beta) = \sum_{i=1}^n \frac{w_i}{a(\phi)} x_{ij}x_{ik} = \sum_{i=1}^n p_i (1-p_i)x_{ij}x_{ik}.
\]</span>`</p>
<hr>
<p>The Fisher-Scoring algorithm for the <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> estimate of <span class="math inline">\(\mathbf{\beta}\)</span> is given by:</p>
<p><span class="math display">\[
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p \\
\end{array}
\right)^{(m+1)}
=
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p \\
\end{array}
\right)^{(m)} +
\mathbf{I}^{-1}(\mathbf{\beta})
\left(
\begin{array}
{c}
\frac{\partial l (\beta, \phi)}{\partial \beta_1} \\
\frac{\partial l (\beta, \phi)}{\partial \beta_2} \\
\vdots \\
\frac{\partial l (\beta, \phi)}{\partial \beta_p} \\
\end{array}
\right)\Bigg|_{\beta = \beta^{(m)}}
\]</span></p>
<p>This is similar to the <a href="non-linear-regression.html#newton-raphson">Newton-Raphson Algorithm</a>, except that we replace the observed matrix of second derivatives (Hessian) with its expected value.</p>
<p>In matrix representation, the score function (gradient of the log-likelihood) is:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l }{\partial \beta} &amp;= \frac{1}{a(\phi)}\mathbf{X'W\Delta(y - \mu)} \\
&amp;= \frac{1}{a(\phi)}\mathbf{F'V^{-1}(y - \mu)}
\end{aligned}
\]</span></p>
<p>The expected Fisher information matrix is:</p>
<p><span class="math display">\[
\mathbf{I}(\beta) = \frac{1}{a(\phi)}\mathbf{X'WX} = \frac{1}{a(\phi)}\mathbf{F'V^{-1}F}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix of covariates.</li>
<li>
<span class="math inline">\(\mathbf{W}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(w_i\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{\Delta}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(\frac{\partial \eta_i}{\partial \mu_i}\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{F} = \frac{\partial \mu}{\partial \beta}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix, where the <span class="math inline">\(i\)</span>-th row is given by <span class="math inline">\(\frac{\partial \mu_i}{\partial \beta} = (\frac{\partial \mu_i}{\partial \eta_i})\mathbf{x}'_i\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{V}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(V(\mu_i)\)</span>.</li>
</ul>
<p>Setting the derivative of the log-likelihood equal to zero gives the MLE equations:</p>
<p><span class="math display">\[
\mathbf{F'V^{-1}y} = \mathbf{F'V^{-1}\mu}
\]</span></p>
<p>Since all components of this equation (except for <span class="math inline">\(\mathbf{y}\)</span>) depend on <span class="math inline">\(\beta\)</span>, we solve iteratively.</p>
<hr>
<p><strong>Special Cases</strong></p>
<ol style="list-style-type: decimal">
<li>Canonical Link Function</li>
</ol>
<p>If a canonical link is used, the estimating equations simplify to:</p>
<p><span class="math display">\[
\mathbf{X'y} = \mathbf{X'\mu}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Identity Link Function</li>
</ol>
<p>If the identity link is used, the estimating equation reduces to:</p>
<p><span class="math display">\[
\mathbf{X'V^{-1}y} = \mathbf{X'V^{-1}X\hat{\beta}}
\]</span></p>
<p>which leads to the <a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a> estimator:</p>
<p><span class="math display">\[
\hat{\beta} = (\mathbf{X'V^{-1}X})^{-1} \mathbf{X'V^{-1}y}
\]</span></p>
<hr>
<p><strong>Fisher-Scoring Algorithm in General Form</strong></p>
<p>The iterative update formula for Fisher-scoring can be rewritten as:</p>
<p><span class="math display">\[
\beta^{(m+1)} = \beta^{(m)} + \mathbf{(\hat{F}'\hat{V}^{-1}\hat{F})^{-1}\hat{F}'\hat{V}^{-1}(y- \hat{\mu})}
\]</span></p>
<p>Since <span class="math inline">\(\hat{F}, \hat{V}, \hat{\mu}\)</span> depend on <span class="math inline">\(\beta\)</span>, we evaluate them at <span class="math inline">\(\beta^{(m)}\)</span>.</p>
<p>From an initial guess <span class="math inline">\(\beta^{(0)}\)</span>, we iterate until convergence.</p>
<p><strong>Notes</strong>:</p>
<ul>
<li>If <span class="math inline">\(a(\phi)\)</span> is a constant or takes the form <span class="math inline">\(m_i \phi\)</span> with known <span class="math inline">\(m_i\)</span>, then <span class="math inline">\(\phi\)</span> cancels from the equations, simplifying the estimation.</li>
</ul>
<hr>
</div>
<div id="estimation-of-dispersion-parameter-phi" class="section level4" number="7.8.7.2">
<h4>
<span class="header-section-number">7.8.7.2</span> Estimation of Dispersion Parameter (<span class="math inline">\(\phi\)</span>)<a class="anchor" aria-label="anchor" href="#estimation-of-dispersion-parameter-phi"><i class="fas fa-link"></i></a>
</h4>
<p>There are two common approaches to estimating <span class="math inline">\(\phi\)</span>:</p>
<ol style="list-style-type: decimal">
<li>
<a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> Estimation</li>
</ol>
<p>The derivative of the log-likelihood with respect to <span class="math inline">\(\phi\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial l_i}{\partial \phi} = \frac{(\theta_i y_i - b(\theta_i)a'(\phi))}{a^2(\phi)} + \frac{\partial c(y_i,\phi)}{\partial \phi}
\]</span></p>
<p>The MLE of <span class="math inline">\(\phi\)</span> satisfies the equation:</p>
<p><span class="math display">\[
\frac{a^2(\phi)}{a'(\phi)}\sum_{i=1}^n \frac{\partial c(y_i, \phi)}{\partial \phi} = \sum_{i=1}^n(\theta_i y_i - b(\theta_i))
\]</span></p>
<p><strong>Challenges:</strong></p>
<ul>
<li><p>For distributions other than the normal case, the expression for <span class="math inline">\(\frac{\partial c(y,\phi)}{\partial \phi}\)</span> is often complicated.</p></li>
<li><p>Even with a <strong>canonical link function</strong> and constant <span class="math inline">\(a(\phi)\)</span>, there is no simple general expression for the expected Fisher information:</p></li>
</ul>
<p><span class="math display">\[
  -E\left(\frac{\partial^2 l}{\partial \phi^2} \right)
  \]</span></p>
<p>This means that the unification GLMs provide for estimating <span class="math inline">\(\beta\)</span> does not extend as neatly to <span class="math inline">\(\phi\)</span>.</p>
<hr>
<ol start="2" style="list-style-type: decimal">
<li>Moment Estimation (Bias-Corrected <span class="math inline">\(\chi^2\)</span> Method)</li>
</ol>
<ul>
<li><p>The MLE is <strong>not</strong> the conventional approach for estimating <span class="math inline">\(\phi\)</span> in <a href="generalized-linear-models.html#generalized-linear-models">Generalized Linear Models</a>.</p></li>
<li>
<p>For an exponential family distribution, the variance function is:</p>
<p><span class="math display">\[
\text{Var}(Y) = V(\mu)a(\phi)
\]</span></p>
<p>This implies the following moment-based estimator:</p>
<p><span class="math display">\[
\begin{aligned}
a(\phi) &amp;= \frac{\text{Var}(Y)}{V(\mu)} = \frac{E(Y- \mu)^2}{V(\mu)} \\
a(\hat{\phi})  &amp;= \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i -\hat{\mu}_i)^2}{V(\hat{\mu}_i)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of parameters in <span class="math inline">\(\beta\)</span>.</p>
</li>
<li>
<p>For a <strong>GLM with a canonical link function</strong> <span class="math inline">\(g(.)= (b'(.))^{-1}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
g(\mu) &amp;= \theta = \eta = \mathbf{x'\beta} \\
\mu &amp;= g^{-1}(\eta)= b'(\eta)
\end{aligned}
\]</span></p>
<p>Using this, the moment estimator for <span class="math inline">\(a(\phi) = \phi\)</span> becomes:</p>
<p><span class="math display">\[
\hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i - g^{-1}(\hat{\eta}_i))^2}{V(g^{-1}(\hat{\eta}_i))}
\]</span></p>
</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="23%">
<col width="27%">
<col width="23%">
<col width="26%">
</colgroup>
<thead><tr class="header">
<th>Approach</th>
<th>Description</th>
<th>Pros</th>
<th>Cons</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>MLE</strong></td>
<td>Estimates <span class="math inline">\(\phi\)</span> by maximizing the likelihood function</td>
<td>Theoretically optimal</td>
<td>Computationally complex, lacks a general closed-form solution</td>
</tr>
<tr class="even">
<td><strong>Moment Estimation</strong></td>
<td>Uses a bias-corrected <span class="math inline">\(\chi^2\)</span> method based on residual variance</td>
<td>Simpler, widely used in GLMs</td>
<td>Not as efficient as MLE</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="inference-1" class="section level3" number="7.8.8">
<h3>
<span class="header-section-number">7.8.8</span> Inference<a class="anchor" aria-label="anchor" href="#inference-1"><i class="fas fa-link"></i></a>
</h3>
<p>The estimated variance of <span class="math inline">\(\hat{\beta}\)</span> is given by:</p>
<p><span class="math display">\[
\hat{\text{var}}(\beta) = a(\phi)(\mathbf{\hat{F}'\hat{V}^{-1}\hat{F}})^{-1}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{V}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with diagonal elements given by <span class="math inline">\(V(\mu_i)\)</span>.</p></li>
<li>
<p><span class="math inline">\(\mathbf{F}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix given by:</p>
<p><span class="math display">\[
\mathbf{F} = \frac{\partial \mu}{\partial \beta}
\]</span></p>
</li>
<li><p>Since <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{F}\)</span> depend on the mean <span class="math inline">\(\mu\)</span> (and thus on <span class="math inline">\(\beta\)</span>), their estimates <span class="math inline">\(\mathbf{\hat{V}}\)</span> and <span class="math inline">\(\mathbf{\hat{F}}\)</span> depend on <span class="math inline">\(\hat{\beta}\)</span>.</p></li>
</ul>
<hr>
<p>To test a general hypothesis:</p>
<p><span class="math display">\[
H_0: \mathbf{L\beta = d}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{L}\)</span> is a <span class="math inline">\(q \times p\)</span> matrix, we use the <strong>Wald test</strong>:</p>
<p><span class="math display">\[
W = \mathbf{(L \hat{\beta}-d)'(a(\phi)L(\hat{F}'\hat{V}^{-1}\hat{F})L')^{-1}(L \hat{\beta}-d)}
\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>, the Wald statistic follows a <strong>chi-square</strong> distribution:</p>
<p><span class="math display">\[
W \sim \chi^2_q
\]</span></p>
<p>where <span class="math inline">\(q\)</span> is the rank of <span class="math inline">\(\mathbf{L}\)</span>.</p>
<hr>
<p><strong>Special Case: Testing a Single Coefficient</strong></p>
<p>For a hypothesis of the form:</p>
<p><span class="math display">\[
H_0: \beta_j = 0
\]</span></p>
<p>the Wald test simplifies to:</p>
<p><span class="math display">\[
W = \frac{\hat{\beta}_j^2}{\hat{\text{var}}(\hat{\beta}_j)} \sim \chi^2_1
\]</span></p>
<p>asymptotically.</p>
<hr>
<p>Another common test is the likelihood ratio test, which compares the likelihoods of a <strong>full model</strong> and a <strong>reduced model</strong>:</p>
<p><span class="math display">\[
\Lambda = 2 \big(l(\hat{\beta}_f) - l(\hat{\beta}_r)\big) \sim \chi^2_q
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(l(\hat{\beta}_f)\)</span> is the log-likelihood of the <strong>full</strong> model.</li>
<li>
<span class="math inline">\(l(\hat{\beta}_r)\)</span> is the log-likelihood of the <strong>reduced</strong> model.</li>
<li>
<span class="math inline">\(q\)</span> is the number of constraints used in fitting the reduced model.</li>
</ul>
<hr>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="25%">
<col width="38%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th>Test</th>
<th>Pros</th>
<th>Cons</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Wald Test</strong></td>
<td>Easy to compute, does not require fitting two models</td>
<td>May perform poorly in small samples</td>
</tr>
<tr class="even">
<td><strong>Likelihood Ratio Test</strong></td>
<td>More accurate, especially for small samples</td>
<td>Requires fitting both full and reduced models</td>
</tr>
</tbody>
</table></div>
<p>While the Wald test is more convenient, the likelihood ratio test is often preferred when sample sizes are small, as it tends to have better statistical properties.</p>
</div>
<div id="deviance" class="section level3" number="7.8.9">
<h3>
<span class="header-section-number">7.8.9</span> Deviance<a class="anchor" aria-label="anchor" href="#deviance"><i class="fas fa-link"></i></a>
</h3>
<p><a href="generalized-linear-models.html#deviance">Deviance</a> plays a crucial role in:</p>
<ul>
<li>
<strong>Goodness-of-fit assessment</strong>: Checking how well a model explains the observed data.</li>
<li>
<strong>Statistical inference</strong>: Used in hypothesis testing, particularly likelihood ratio tests.</li>
<li>
<strong>Estimating dispersion parameters</strong>: Helps in refining variance estimates.</li>
<li>
<strong>Model comparison</strong>: Facilitates selection between competing models.</li>
</ul>
<p>Assuming the dispersion parameter <span class="math inline">\(\phi\)</span> is known, let:</p>
<ul>
<li><p><span class="math inline">\(\tilde{\theta}\)</span> be the <strong>maximum likelihood estimate (MLE)</strong> under the <strong>full model</strong>.</p></li>
<li><p><span class="math inline">\(\hat{\theta}\)</span> be the <strong>MLE</strong> under the <strong>reduced model</strong>.</p></li>
</ul>
<p>The <strong>likelihood ratio statistic</strong> (twice the difference in log-likelihoods) is:</p>
<p><span class="math display">\[
2\sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i- \hat{\theta}_i)-b(\tilde{\theta}_i) + b(\hat{\theta}_i)}{a_i(\phi)}
\]</span></p>
<p>For <strong>exponential family</strong> distributions, the mean parameter is:</p>
<p><span class="math display">\[
\mu = E(y) = b'(\theta)
\]</span></p>
<p>Thus, the <strong>natural parameter</strong> is a function of <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
\theta = \theta(\mu) = b'^{-1}(\mu)
\]</span></p>
<p>Rewriting the likelihood ratio statistic in terms of <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
2 \sum_{i=1}^n \frac{y_i\{\theta(\tilde{\mu}_i) - \theta(\hat{\mu}_i)\} - b(\theta(\tilde{\mu}_i)) + b(\theta(\hat{\mu}_i))}{a_i(\phi)}
\]</span></p>
<p>A <strong>saturated model</strong> provides the fullest possible fit, where each observation is perfectly predicted:</p>
<p><span class="math display">\[
\tilde{\mu}_i = y_i, \quad i = 1, \dots, n
\]</span></p>
<p>Setting <span class="math inline">\(\tilde{\theta}_i^* = \theta(y_i)\)</span> and <span class="math inline">\(\hat{\theta}_i^* = \theta (\hat{\mu}_i)\)</span>, the likelihood ratio simplifies to:</p>
<p><span class="math display">\[
2 \sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i^* - \hat{\theta}_i^*) - b(\tilde{\theta}_i^*) + b(\hat{\theta}_i^*)}{a_i(\phi)}
\]</span></p>
<p>Following <span class="citation">McCullagh (<a href="references.html#ref-McCullagh_2019">2019</a>)</span>, for <span class="math inline">\(a_i(\phi) = \phi\)</span>, we define the <strong>scaled deviance</strong> as:</p>
<p><span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = \frac{2}{\phi} \sum_{i=1}^n \{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*) - b(\tilde{\theta}_i^*) + b(\hat{\theta}_i^*)\}
\]</span></p>
<p>and the <strong>deviance</strong> as:</p>
<p><span class="math display">\[
D(\mathbf{y, \hat{\mu}}) = \phi D^*(\mathbf{y, \hat{\mu}})
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(D^*(\mathbf{y, \hat{\mu}})\)</span> is <strong>scaled deviance</strong>.</li>
<li>
<span class="math inline">\(D(\mathbf{y, \hat{\mu}})\)</span> is <strong>deviance</strong>.</li>
</ul>
<hr>
<p>In some models, <span class="math inline">\(a_i(\phi) = \phi m_i\)</span>, where <span class="math inline">\(m_i\)</span> is a known scalar that varies across observations. Then, the <strong>scaled deviance</strong> is:</p>
<p><span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = \sum_{i=1}^n \frac{2\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*) - b(\tilde{\theta}_i^*) + b(\hat{\theta}_i^*)\}}{\phi m_i}
\]</span></p>
<p>The deviance is often decomposed into <strong>deviance contributions</strong>:</p>
<p><span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = \sum_{i=1}^n d_i
\]</span></p>
<p>where <span class="math inline">\(d_i\)</span> is the <strong>deviance contribution</strong> from the <span class="math inline">\(i\)</span>-th observation.</p>
<p><strong>Uses of Deviance:</strong></p>
<ul>
<li><p><span class="math inline">\(D\)</span> is used for model selection.</p></li>
<li><p><span class="math inline">\(D^*\)</span> is used for goodness-of-fit tests, as it is a likelihood ratio statistic:</p></li>
</ul>
<p><span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = 2\{l(\mathbf{y,\tilde{\mu}})-l(\mathbf{y,\hat{\mu}})\}
\]</span></p>
<ul>
<li>The individual deviance contributions <span class="math inline">\(d_i\)</span> are used to form deviance residuals.</li>
</ul>
<hr>
<p><strong>Deviance for Normal Distribution</strong></p>
<p>For the normal distribution:</p>
<p><span class="math display">\[
\begin{aligned}
\theta &amp;= \mu, \\
\phi &amp;= \sigma^2, \\
b(\theta) &amp;= \frac{1}{2} \theta^2, \\
a(\phi) &amp;= \phi.
\end{aligned}
\]</span></p>
<p>The MLEs are:</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{\theta}_i &amp;= y_i, \\
\hat{\theta}_i &amp;= \hat{\mu}_i = g^{-1}(\hat{\eta}_i).
\end{aligned}
\]</span></p>
<p>The <strong>deviance</strong> simplifies to:</p>
<p><span class="math display">\[
\begin{aligned}
D &amp;= 2 \sum_{i=1}^n \left(y_i^2 - y_i \hat{\mu}_i - \frac{1}{2} y_i^2 + \frac{1}{2} \hat{\mu}_i^2 \right) \\
&amp;= \sum_{i=1}^n (y_i^2 - 2y_i \hat{\mu}_i + \hat{\mu}_i^2) \\
&amp;= \sum_{i=1}^n (y_i - \hat{\mu}_i)^2.
\end{aligned}
\]</span></p>
<p>Thus, for the normal model, the deviance corresponds to the residual sum of squares.</p>
<hr>
<p><strong>Deviance for Poisson Distribution</strong></p>
<p>For the Poisson distribution:</p>
<p><span class="math display">\[
\begin{aligned}
f(y) &amp;= \exp\{y\log(\mu) - \mu - \log(y!)\}, \\
\theta &amp;= \log(\mu), \\
b(\theta) &amp;= \exp(\theta), \\
a(\phi) &amp;= 1.
\end{aligned}
\]</span></p>
<p>MLEs:</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{\theta}_i &amp;= \log(y_i), \\
\hat{\theta}_i &amp;= \log(\hat{\mu}_i), \\
\hat{\mu}_i &amp;= g^{-1}(\hat{\eta}_i).
\end{aligned}
\]</span></p>
<p>The <strong>deviance</strong> simplifies to:</p>
<p><span class="math display">\[
\begin{aligned}
D &amp;= 2 \sum_{i = 1}^n \left(y_i \log(y_i) - y_i \log(\hat{\mu}_i) - y_i + \hat{\mu}_i \right) \\
&amp;= 2 \sum_{i = 1}^n \left[y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i) \right].
\end{aligned}
\]</span></p>
<p>The <strong>deviance contribution</strong> for each observation:</p>
<p><span class="math display">\[
d_i = 2 \left[y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i)\right].
\]</span></p>
<hr>
<div id="analysis-of-deviance" class="section level4" number="7.8.9.1">
<h4>
<span class="header-section-number">7.8.9.1</span> Analysis of Deviance<a class="anchor" aria-label="anchor" href="#analysis-of-deviance"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Analysis of Deviance</strong> is a likelihood-based approach for comparing nested models in <a href="generalized-linear-models.html#generalized-linear-models">GLM</a>. It is analogous to the [Analysis of Variance (ANOVA)] in linear models.</p>
<p>When comparing a <strong>reduced model</strong> (denoted by <span class="math inline">\(\hat{\mu}_r\)</span>) and a <strong>full model</strong> (denoted by <span class="math inline">\(\hat{\mu}_f\)</span>), the difference in <strong>scaled deviance</strong> follows an asymptotic chi-square distribution:</p>
<p><span class="math display">\[ D^*(\mathbf{y;\hat{\mu}_r}) - D^*(\mathbf{y;\hat{\mu}_f}) = 2\{l(\mathbf{y;\hat{\mu}_f})-l(\mathbf{y;\hat{\mu}_r})\} \sim \chi^2_q \]</span></p>
<p>where <span class="math inline">\(q\)</span> is the difference in the number of free parameters between the two models.</p>
<p>This test provides a means to assess whether the additional parameters in the full model significantly improve model fit.</p>
<p>The dispersion parameter <span class="math inline">\(\phi\)</span> is estimated as:</p>
<p><span class="math display">\[ \hat{\phi} = \frac{D(\mathbf{y, \hat{\mu}})}{n - p} \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(D(\mathbf{y, \hat{\mu}})\)</span> is the deviance of the fitted model.</li>
<li>
<span class="math inline">\(n\)</span> is the total number of observations.</li>
<li>
<span class="math inline">\(p\)</span> is the number of estimated parameters.</li>
</ul>
<p><strong>Caution:</strong> The frequent use of <span class="math inline">\(\chi^2\)</span> tests can be problematic due to their reliance on asymptotic approximations, especially for small samples or overdispersed data <span class="citation">(<a href="references.html#ref-McCullagh_2019">McCullagh 2019</a>)</span>.</p>
<hr>
</div>
<div id="deviance-residuals" class="section level4" number="7.8.9.2">
<h4>
<span class="header-section-number">7.8.9.2</span> Deviance Residuals<a class="anchor" aria-label="anchor" href="#deviance-residuals"><i class="fas fa-link"></i></a>
</h4>
<p>Since deviance plays a role in model evaluation, we define <strong>deviance residuals</strong> to examine individual data points. Given that the total deviance is:</p>
<p><span class="math display">\[ D = \sum_{i=1}^{n}d_i \]</span></p>
<p>the <strong>deviance residual</strong> for observation <span class="math inline">\(i\)</span> is:</p>
<p><span class="math display">\[ r_{D_i} = \text{sign}(y_i -\hat{\mu}_i)\sqrt{d_i} \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(d_i\)</span> is the deviance contribution from observation <span class="math inline">\(i\)</span>.</li>
<li>The <strong>sign function</strong> preserves the direction of the residual.</li>
</ul>
<hr>
<p>To account for varying leverage, we define the <strong>standardized deviance residual</strong>:</p>
<p><span class="math display">\[ r_{s,i} = \frac{y_i -\hat{\mu}_i}{\hat{\sigma}(1-h_{ii})^{1/2}} \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(h_{ii}\)</span> is the <strong>leverage</strong> of observation <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(\hat{\sigma}\)</span> is an estimate of the standard deviation.</li>
</ul>
<p>Alternatively, using the <strong>GLM hat matrix</strong>:</p>
<p><span class="math display">\[ \mathbf{H}^{GLM} = \mathbf{W}^{1/2}X(X'WX)^{-1}X'\mathbf{W}^{-1/2} \]</span></p>
<p>where <span class="math inline">\(\mathbf{W}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(w_i\)</span> (see <a href="generalized-linear-models.html#estimation-of-systematic-component-beta">Estimation of Systematic Component (<span class="math inline">\(\beta\)</span>)</a>), we express standardized deviance residuals as:</p>
<p><span class="math display">\[ r_{s, D_i} = \frac{r_{D_i}}{\{\hat{\phi}(1-h_{ii}^{glm})\}^{1/2}} \]</span></p>
<p>where <span class="math inline">\(h_{ii}^{glm}\)</span> is the <span class="math inline">\(i\)</span>-th diagonal element of <span class="math inline">\(\mathbf{H}^{GLM}\)</span>.</p>
<hr>
</div>
<div id="pearson-chi-square-residuals" class="section level4" number="7.8.9.3">
<h4>
<span class="header-section-number">7.8.9.3</span> Pearson Chi-Square Residuals<a class="anchor" aria-label="anchor" href="#pearson-chi-square-residuals"><i class="fas fa-link"></i></a>
</h4>
<p>Another goodness-of-fit statistic is the <strong>Pearson Chi-Square</strong> statistic, defined as:</p>
<p><span class="math display">\[ X^2 = \sum_{i=1}^{n} \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)} \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{\mu}_i\)</span> is the fitted mean response.</li>
<li>
<span class="math inline">\(V(\hat{\mu}_i)\)</span> is the variance function of the response.</li>
</ul>
<p>The <strong>Scaled Pearson</strong> <span class="math inline">\(\chi^2\)</span> statistic is:</p>
<p><span class="math display">\[ \frac{X^2}{\phi} \sim \chi^2_{n-p} \]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of estimated parameters.</p>
<p>The <strong>Pearson residuals</strong> are:</p>
<p><span class="math display">\[ X^2_i = \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)} \]</span></p>
<p>These residuals assess the difference between observed and fitted values, standardized by the variance.</p>
<p>If the assumptions hold:</p>
<ul>
<li><p><strong>Independent samples</strong></p></li>
<li>
<p><strong>No overdispersion</strong>: If <span class="math inline">\(\phi = 1\)</span>, we expect:</p>
<p><span class="math display">\[ \frac{D(\mathbf{y;\hat{\mu}})}{n-p} \approx 1, \quad \frac{X^2}{n-p} \approx 1 \]</span></p>
<p>A value <strong>substantially greater than 1</strong> suggests <strong>overdispersion</strong> or <strong>model misspecification</strong>.</p>
</li>
<li><p><strong>Multiple groups</strong>: If the model includes categorical predictors, separate calculations may be needed for each group.</p></li>
</ul>
<p>Under these assumptions, both:</p>
<p><span class="math display">\[ \frac{X^2}{\phi} \quad \text{and} \quad D^*(\mathbf{y; \hat{\mu}}) \]</span></p>
<p>follow a <span class="math inline">\(\chi^2_{n-p}\)</span> distribution.</p>
<hr>
<div class="sourceCode" id="cb309"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> </span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span> </span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> </span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="va">n</span>, lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>  </span>
<span></span>
<span><span class="co"># Poisson response  </span></span>
<span><span class="co"># Fit Poisson GLM </span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">poisson</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"log"</span><span class="op">)</span><span class="op">)</span>  </span>
<span></span>
<span><span class="co"># Extract deviance and Pearson residuals </span></span>
<span><span class="va">deviance_residuals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">fit</span>, type <span class="op">=</span> <span class="st">"deviance"</span><span class="op">)</span> </span>
<span><span class="va">pearson_residuals</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">fit</span>, type <span class="op">=</span> <span class="st">"pearson"</span><span class="op">)</span>  </span>
<span><span class="co"># Display residual summaries </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">deviance_residuals</span><span class="op">)</span> </span>
<span><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">#&gt; -2.1286 -1.0189 -0.1441 -0.1806  0.6171  1.8942</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">pearson_residuals</span><span class="op">)</span></span>
<span><span class="co">#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. </span></span>
<span><span class="co">#&gt; -1.505136 -0.837798 -0.140873  0.002003  0.658076  2.366618</span></span></code></pre></div>
<ul>
<li><p>The residuals suggest a reasonably <strong>well-fitted model</strong> with a <strong>slight underprediction</strong> tendency.</p></li>
<li><p>No extreme deviations or severe skewness.</p></li>
<li><p>The largest residuals indicate <strong>potential outliers</strong>, but they are not extreme enough to immediately suggest model inadequacy.</p></li>
</ul>
</div>
</div>
<div id="diagnostic-plots" class="section level3" number="7.8.10">
<h3>
<span class="header-section-number">7.8.10</span> Diagnostic Plots<a class="anchor" aria-label="anchor" href="#diagnostic-plots"><i class="fas fa-link"></i></a>
</h3>
<p>Standardized residual plots help diagnose potential issues with model specification, such as an incorrect link function or variance structure. Common residual plots include:</p>
<ul>
<li><p><strong>Plot of Standardized Deviance Residuals vs. Fitted Values</strong><br><span class="math display">\[ \text{plot}(r_{s, D_i}, \hat{\mu}_i) \]</span> or<br><span class="math display">\[ \text{plot}(r_{s, D_i}, T(\hat{\mu}_i)) \]</span> where <span class="math inline">\(T(\hat{\mu}_i)\)</span> is a <strong>constant information scale transformation</strong>, which adjusts <span class="math inline">\(\hat{\mu}_i\)</span> to maintain a stable variance across observations.</p></li>
<li><p><strong>Plot of Standardized Deviance Residuals vs. Linear Predictor</strong><br><span class="math display">\[ \text{plot}(r_{s, D_i}, \hat{\eta}_i) \]</span> where <span class="math inline">\(\hat{\eta}_i\)</span> represents the linear predictor before applying the link function.</p></li>
</ul>
<p>The following table summarizes the commonly used transformations <span class="math inline">\(T(\hat{\mu}_i)\)</span> for different random components:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center">Random Component</th>
<th align="center"><span class="math inline">\(T(\hat{\mu}_i)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">Normal</td>
<td align="center"><span class="math inline">\(\hat{\mu}\)</span></td>
</tr>
<tr class="even">
<td align="center">Poisson</td>
<td align="center"><span class="math inline">\(2\sqrt{\hat{\mu}}\)</span></td>
</tr>
<tr class="odd">
<td align="center">Binomial</td>
<td align="center"><span class="math inline">\(2 \sin^{-1}(\sqrt{\hat{\mu}})\)</span></td>
</tr>
<tr class="even">
<td align="center">Gamma</td>
<td align="center"><span class="math inline">\(2 \log(\hat{\mu})\)</span></td>
</tr>
<tr class="odd">
<td align="center">Inverse Gaussian</td>
<td align="center"><span class="math inline">\(-2\hat{\mu}^{-1/2}\)</span></td>
</tr>
</tbody>
</table></div>
<p>Interpretation of Residual Plots</p>
<ul>
<li>Trend in residuals: Indicates an incorrect link function or an inappropriate scale transformation.</li>
<li>Systematic change in residual range with <span class="math inline">\(T(\hat{\mu})\)</span>: Suggests an incorrect choice of random component—i.e., the assumed variance function does not match the data.</li>
<li>Plot of absolute residuals vs. fitted values:<br><span class="math display">\[ \text{plot}(|r_{D_i}|, \hat{\mu}_i) \]</span> This checks whether the variance function is correctly specified.</li>
</ul>
<hr>
</div>
<div id="goodness-of-fit" class="section level3" number="7.8.11">
<h3>
<span class="header-section-number">7.8.11</span> Goodness of Fit<a class="anchor" aria-label="anchor" href="#goodness-of-fit"><i class="fas fa-link"></i></a>
</h3>
<p>To assess how well the model fits the data, we use:</p>
<ul>
<li><a href="generalized-linear-models.html#deviance">Deviance</a></li>
<li><a href="generalized-linear-models.html#pearson-chi-square-residuals">Pearson Chi-square Residuals</a></li>
</ul>
<p>For <strong>nested models</strong>, likelihood-based information criteria provide a comparison metric:</p>
<p><span class="math display">\[
\begin{aligned}
AIC &amp;= -2l(\mathbf{\hat{\mu}}) + 2p \\
AICC &amp;= -2l(\mathbf{\hat{\mu}}) + 2p \left(\frac{n}{n-p-1} \right) \\
BIC &amp;= -2l(\mathbf{\hat{\mu}}) + p \log(n)
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(l(\hat{\mu})\)</span> is the log-likelihood at the estimated parameters.</li>
<li>
<span class="math inline">\(p\)</span> is the number of model parameters.</li>
<li>
<span class="math inline">\(n\)</span> is the number of observations.</li>
</ul>
<p><strong>Important Considerations:</strong></p>
<ul>
<li><p>The <strong>same data and model structure</strong> (i.e., same link function and assumed random component distribution) must be used for comparisons.</p></li>
<li><p>Models can differ in <strong>number of parameters</strong> but must remain consistent in other respects.</p></li>
</ul>
<p>While traditional <span class="math inline">\(R^2\)</span> is not directly applicable to GLMs, an analogous measure is:</p>
<p><span class="math display">\[
R^2_p = 1 - \frac{l(\hat{\mu})}{l(\hat{\mu}_0)}
\]</span></p>
<p>where <span class="math inline">\(l(\hat{\mu}_0)\)</span> is the log-likelihood of a model with only an intercept.</p>
<p>For binary response models, a <strong>rescaled generalized</strong> <span class="math inline">\(R^2\)</span> is often used:</p>
<p><span class="math display">\[
\bar{R}^2 = \frac{R^2_*}{\max(R^2_*)} = \frac{1-\exp\left(-\frac{2}{n}(l(\hat{\mu}) - l(\hat{\mu}_0))\right)}{1 - \exp\left(\frac{2}{n} l(\hat{\mu}_0) \right)}
\]</span></p>
<p>where the denominator ensures the maximum possible <span class="math inline">\(R^2\)</span> scaling.</p>
<hr>
</div>
<div id="over-dispersion" class="section level3" number="7.8.12">
<h3>
<span class="header-section-number">7.8.12</span> Over-Dispersion<a class="anchor" aria-label="anchor" href="#over-dispersion"><i class="fas fa-link"></i></a>
</h3>
<p>Over-dispersion occurs when the observed variance exceeds what the assumed model allows. This is common in <a href="generalized-linear-models.html#sec-poisson-regression">Poisson</a> and <a href="generalized-linear-models.html#sec-binomial-regression">Binomial</a> models.</p>
<p><strong>Variance Assumptions for Common Random Components</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="24%">
<col width="31%">
<col width="43%">
</colgroup>
<thead><tr class="header">
<th>Random Component</th>
<th>Standard Assumption (<span class="math inline">\(\text{var}(Y)\)</span>)</th>
<th>Alternative Model Allowing Over-Dispersion (<span class="math inline">\(V(\mu)\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Binomial</strong></td>
<td><span class="math inline">\(n \mu (1- \mu)\)</span></td>
<td>
<span class="math inline">\(\phi n \mu (1- \mu)\)</span>, where <span class="math inline">\(m_i = n\)</span>
</td>
</tr>
<tr class="even">
<td><strong>Poisson</strong></td>
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\phi \mu\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>
<strong>By default,</strong> <span class="math inline">\(\phi = 1\)</span>, meaning the variance follows the assumed model.</li>
<li>If <span class="math inline">\(\phi \neq 1\)</span>, the variance differs from the expectation:
<ul>
<li>
<span class="math inline">\(\phi &gt; 1\)</span>: <strong>Over-dispersion</strong> (greater variance than expected).</li>
<li>
<span class="math inline">\(\phi &lt; 1\)</span>: <strong>Under-dispersion</strong> (less variance than expected).</li>
</ul>
</li>
</ul>
<p>This discrepancy suggests the assumed <strong>random component distribution</strong> may be inappropriate.</p>
<p>To account for dispersion issues, we can:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Choose a more flexible random component distribution</strong>
<ul>
<li>
<a href="generalized-linear-models.html#sec-negative-binomial-regression">Negative Binomial</a> for over-dispersed <a href="generalized-linear-models.html#sec-poisson-regression">Poisson</a> data.</li>
<li>Conway-Maxwell Poisson for both over- and under-dispersed count data.</li>
</ul>
</li>
<li>
<strong>Use Mixed Models to account for random effects</strong>
<ul>
<li>
<a href="sec-nonlinear-and-generalized-linear-mixed-models.html#sec-nonlinear-and-generalized-linear-mixed-models">Nonlinear and Generalized Linear Mixed Models</a> introduce random effects, which can help capture additional variance.</li>
</ul>
</li>
</ol>
<hr>
<div class="sourceCode" id="cb310"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span>   <span class="co"># For negative binomial models</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">MuMIn</span><span class="op">)</span>  <span class="co"># For R^2 in GLMs</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/glmmTMB/glmmTMB">glmmTMB</a></span><span class="op">)</span> <span class="co"># For handling overdispersion</span></span>
<span></span>
<span><span class="co"># Generate Example Data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span>  <span class="co"># True mean function</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="va">n</span>, lambda <span class="op">=</span> <span class="va">mu</span><span class="op">)</span>  <span class="co"># Poisson outcome</span></span>
<span></span>
<span><span class="co"># Fit a Poisson GLM</span></span>
<span><span class="va">model_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">poisson</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"log"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute residuals</span></span>
<span><span class="va">resid_dev</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model_pois</span>, type <span class="op">=</span> <span class="st">"deviance"</span><span class="op">)</span></span>
<span><span class="va">fitted_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">model_pois</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Standardized Residual Plot: Residuals vs Fitted Values</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">fitted_vals</span>, <span class="va">resid_dev</span><span class="op">)</span>,</span>
<span>       <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">fitted_vals</span>, y <span class="op">=</span> <span class="va">resid_dev</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"loess"</span>,</span>
<span>                color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>                se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Standardized Deviance Residuals vs Fitted Values"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Fitted Values"</span>, y <span class="op">=</span> <span class="st">"Standardized Deviance Residuals"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-50-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb311"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Absolute Residuals vs Fitted Values (Variance Function Check)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">fitted_vals</span>, abs_resid <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">resid_dev</span><span class="op">)</span><span class="op">)</span>,</span>
<span>       <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">fitted_vals</span>, y <span class="op">=</span> <span class="va">abs_resid</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"loess"</span>,</span>
<span>                color <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>                se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Absolute Deviance Residuals vs Fitted Values"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Fitted Values"</span>, y <span class="op">=</span> <span class="st">"|Residuals|"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-50-2.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb312"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Goodness-of-Fit Metrics</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">model_pois</span><span class="op">)</span>    <span class="co"># Akaike Information Criterion</span></span>
<span><span class="co">#&gt; [1] 322.9552</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">BIC</a></span><span class="op">(</span><span class="va">model_pois</span><span class="op">)</span>    <span class="co"># Bayesian Information Criterion</span></span>
<span><span class="co">#&gt; [1] 328.1655</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">model_pois</span><span class="op">)</span> <span class="co"># Log-likelihood</span></span>
<span><span class="co">#&gt; 'log Lik.' -159.4776 (df=2)</span></span>
<span></span>
<span><span class="co"># R-squared for GLM</span></span>
<span><span class="va">r_squared</span> <span class="op">&lt;-</span></span>
<span>    <span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">model_pois</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fl">1</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">r_squared</span></span>
<span><span class="co">#&gt; 'log Lik.' 0.05192856 (df=2)</span></span>
<span></span>
<span><span class="co"># Overdispersion Check</span></span>
<span><span class="va">resid_pearson</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">model_pois</span>, type <span class="op">=</span> <span class="st">"pearson"</span><span class="op">)</span></span>
<span><span class="co"># Estimated dispersion parameter</span></span>
<span><span class="va">phi_hat</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">resid_pearson</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">model_pois</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>  </span>
<span><span class="va">phi_hat</span></span>
<span><span class="co">#&gt; [1] 1.055881</span></span>
<span></span>
<span><span class="co"># If phi &gt; 1, </span></span>
<span><span class="co"># fit a Negative Binomial Model to correct for overdispersion</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="va">phi_hat</span> <span class="op">&gt;</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">model_nb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/glm.nb.html">glm.nb</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_nb</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm.nb(formula = y ~ x, init.theta = 50.70707605, link = log)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.42913    0.08514   5.040 4.65e-07 ***</span></span>
<span><span class="co">#&gt; x            0.35262    0.08654   4.075 4.61e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for Negative Binomial(50.7071) family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 135.71  on 99  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 118.92  on 98  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 324.91</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;               Theta:  51 </span></span>
<span><span class="co">#&gt;           Std. Err.:  233 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  2 x log-likelihood:  -318.906</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li>Standardized Residuals vs. Fitted Values</li>
</ol>
<ul>
<li><p>If there is a clear trend in the residual plot (e.g., a curve), this suggests an incorrect link function.</p></li>
<li><p>If the residual spread increases with fitted values, the variance function may be misspecified.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Absolute Residuals vs. Fitted Values</li>
</ol>
<ul>
<li><p>A systematic pattern (e.g., funnel shape) suggests heteroscedasticity (i.e., changing variance).</p></li>
<li><p>If the residuals increase with fitted values, a different variance function (e.g., Negative Binomial instead of Poisson) may be needed.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Goodness-of-Fit Metrics</li>
</ol>
<ul>
<li><p>AIC/BIC: Lower values indicate a better model, but must be compared across nested models.</p></li>
<li><p>Log-likelihood: Higher values suggest a better-fitting model.</p></li>
<li><p><span class="math inline">\(R^2\)</span> for GLMs: Since traditional <span class="math inline">\(R^2\)</span> is not available, the likelihood-based <span class="math inline">\(R^2\)</span> is used to measure improvement over a null model.</p></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Overdispersion Check (<span class="math inline">\(\phi\)</span>)</li>
</ol>
<ul>
<li><p>If <span class="math inline">\(\phi \approx 1\)</span>, the Poisson assumption is valid.</p></li>
<li><p>If <span class="math inline">\(\phi &gt; 1\)</span>, there is overdispersion, meaning a Negative Binomial model may be more appropriate.</p></li>
<li><p>If <span class="math inline">\(\phi &lt; 1\)</span>, underdispersion is present, requiring alternative distributions like the Conway-Maxwell Poisson.</p></li>
</ul>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></div>
<div class="next"><a href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#generalized-linear-models"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li>
<a class="nav-link" href="#sec-logistic-regression"><span class="header-section-number">7.1</span> Logistic Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#logistic-model"><span class="header-section-number">7.1.1</span> Logistic Model</a></li>
<li><a class="nav-link" href="#sec-likelihood-function-logistic"><span class="header-section-number">7.1.2</span> Likelihood Function</a></li>
<li><a class="nav-link" href="#fisher-information-matrix"><span class="header-section-number">7.1.3</span> Fisher Information Matrix</a></li>
<li><a class="nav-link" href="#inference-in-logistic-regression"><span class="header-section-number">7.1.4</span> Inference in Logistic Regression</a></li>
<li><a class="nav-link" href="#application-logistic-regression"><span class="header-section-number">7.1.5</span> Application: Logistic Regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-probit-regression"><span class="header-section-number">7.2</span> Probit Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#probit-model"><span class="header-section-number">7.2.1</span> Probit Model</a></li>
<li><a class="nav-link" href="#application-probit-regression"><span class="header-section-number">7.2.2</span> Application: Probit Regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-binomial-regression"><span class="header-section-number">7.3</span> Binomial Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#dataset-overview"><span class="header-section-number">7.3.1</span> Dataset Overview</a></li>
<li><a class="nav-link" href="#apply-logistic-model"><span class="header-section-number">7.3.2</span> Apply Logistic Model</a></li>
<li><a class="nav-link" href="#apply-probit-model"><span class="header-section-number">7.3.3</span> Apply Probit Model</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-poisson-regression"><span class="header-section-number">7.4</span> Poisson Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-poisson-distribution"><span class="header-section-number">7.4.1</span> The Poisson Distribution</a></li>
<li><a class="nav-link" href="#poisson-model"><span class="header-section-number">7.4.2</span> Poisson Model</a></li>
<li><a class="nav-link" href="#link-function-choices"><span class="header-section-number">7.4.3</span> Link Function Choices</a></li>
<li><a class="nav-link" href="#application-poisson-regression"><span class="header-section-number">7.4.4</span> Application: Poisson Regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-negative-binomial-regression"><span class="header-section-number">7.5</span> Negative Binomial Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#negative-binomial-distribution"><span class="header-section-number">7.5.1</span> Negative Binomial Distribution</a></li>
<li><a class="nav-link" href="#application-negative-binomial-regression"><span class="header-section-number">7.5.2</span> Application: Negative Binomial Regression</a></li>
<li><a class="nav-link" href="#fitting-a-zero-inflated-negative-binomial-model"><span class="header-section-number">7.5.3</span> Fitting a Zero-Inflated Negative Binomial Model</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-quasi-poisson-regression"><span class="header-section-number">7.6</span> Quasi-Poisson Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#is-quasi-poisson-regression-a-generalized-linear-model"><span class="header-section-number">7.6.1</span> Is Quasi-Poisson Regression a Generalized Linear Model?</a></li>
<li><a class="nav-link" href="#application-quasi-poisson-regression"><span class="header-section-number">7.6.2</span> Application: Quasi-Poisson Regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-multinomial-logistic-regression"><span class="header-section-number">7.7</span> Multinomial Logistic Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-multinomial-distribution"><span class="header-section-number">7.7.1</span> The Multinomial Distribution</a></li>
<li><a class="nav-link" href="#modeling-probabilities-using-log-odds"><span class="header-section-number">7.7.2</span> Modeling Probabilities Using Log-Odds</a></li>
<li><a class="nav-link" href="#softmax-representation"><span class="header-section-number">7.7.3</span> Softmax Representation</a></li>
<li><a class="nav-link" href="#log-odds-ratio-between-two-categories"><span class="header-section-number">7.7.4</span> Log-Odds Ratio Between Two Categories</a></li>
<li><a class="nav-link" href="#estimation"><span class="header-section-number">7.7.5</span> Estimation</a></li>
<li><a class="nav-link" href="#interpretation-of-coefficients"><span class="header-section-number">7.7.6</span> Interpretation of Coefficients</a></li>
<li><a class="nav-link" href="#application-multinomial-logistic-regression"><span class="header-section-number">7.7.7</span> Application: Multinomial Logistic Regression</a></li>
<li><a class="nav-link" href="#application-gamma-regression"><span class="header-section-number">7.7.8</span> Application: Gamma Regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-generalization-of-generalized-linear-models"><span class="header-section-number">7.8</span> Generalization of Generalized Linear Models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#exponential-family"><span class="header-section-number">7.8.1</span> Exponential Family</a></li>
<li><a class="nav-link" href="#properties-of-glm-exponential-families"><span class="header-section-number">7.8.2</span> Properties of GLM Exponential Families</a></li>
<li><a class="nav-link" href="#structure-of-a-generalized-linear-model"><span class="header-section-number">7.8.3</span> Structure of a Generalized Linear Model</a></li>
<li><a class="nav-link" href="#components-of-a-glm"><span class="header-section-number">7.8.4</span> Components of a GLM</a></li>
<li><a class="nav-link" href="#canonical-link"><span class="header-section-number">7.8.5</span> Canonical Link</a></li>
<li><a class="nav-link" href="#inverse-link-functions"><span class="header-section-number">7.8.6</span> Inverse Link Functions</a></li>
<li><a class="nav-link" href="#estimation-of-parameters-in-glms"><span class="header-section-number">7.8.7</span> Estimation of Parameters in GLMs</a></li>
<li><a class="nav-link" href="#inference-1"><span class="header-section-number">7.8.8</span> Inference</a></li>
<li><a class="nav-link" href="#deviance"><span class="header-section-number">7.8.9</span> Deviance</a></li>
<li><a class="nav-link" href="#diagnostic-plots"><span class="header-section-number">7.8.10</span> Diagnostic Plots</a></li>
<li><a class="nav-link" href="#goodness-of-fit"><span class="header-section-number">7.8.11</span> Goodness of Fit</a></li>
<li><a class="nav-link" href="#over-dispersion"><span class="header-section-number">7.8.12</span> Over-Dispersion</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/07-generalized-linear-models.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/07-generalized-linear-models.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-05-14.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
