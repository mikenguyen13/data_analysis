<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Generalized Linear Models | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 7 Generalized Linear Models | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/generalized-linear-models.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 7 Generalized Linear Models | A Guide on Data Analysis">
<meta name="twitter:description" content="Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-stat.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-linear Regression</a></li>
<li><a class="active" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="" href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="synthetic-difference-in-differences.html"><span class="header-section-number">25</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">26</span> Difference-in-differences</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">27</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">28</span> Event Studies</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">29</span> Instrumental Variables</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">30</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">31</span> Interrupted Time Series</a></li>
<li><a class="" href="matching-methods-1.html"><span class="header-section-number">32</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">33</span> Endogeneity</a></li>
<li><a class="" href="interrupted-time-series-1.html"><span class="header-section-number">34</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="other-biases.html"><span class="header-section-number">35</span> Other Biases</a></li>
<li><a class="" href="endogeneity-1.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases-1.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="controls-1.html"><span class="header-section-number">39</span> Controls</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">40</span> Mediation</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">41</span> Directed Acyclic Graph</a></li>
<li><a class="" href="mediation-1.html"><span class="header-section-number">42</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph-1.html"><span class="header-section-number">43</span> Directed Acyclic Graph</a></li>
<li><a class="" href="report.html"><span class="header-section-number">44</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">45</span> Exploratory Data Analysis</a></li>
<li><a class="" href="report-1.html"><span class="header-section-number">46</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis-1.html"><span class="header-section-number">47</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">48</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">49</span> Replication and Synthetic Data</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check-1.html"><span class="header-section-number">50</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data-1.html"><span class="header-section-number">51</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="generalized-linear-models" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Generalized Linear Models<a class="anchor" aria-label="anchor" href="#generalized-linear-models"><i class="fas fa-link"></i></a>
</h1>
<p>Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model derived from the fact that we have <span class="math inline">\(\mathbf{x'_i \beta}\)</span> (which is linear form) in the model.</p>
<div id="logistic-regression-1" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#logistic-regression-1"><i class="fas fa-link"></i></a>
</h2>
<p><span class="math display">\[
p_i = f(\mathbf{x}_i ; \beta) = \frac{exp(\mathbf{x_i'\beta})}{1 + exp(\mathbf{x_i'\beta})}
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
logit(p_i) = log(\frac{p_i}{1+p_i}) = \mathbf{x_i'\beta}
\]</span></p>
<p>where <span class="math inline">\(\frac{p_i}{1+p_i}\)</span>is the <strong>odds</strong>.</p>
<p>In this form, the model is specified such that <strong>a function of the mean response is linear</strong>. Hence, <strong>Generalized Linear Models</strong></p>
<p>The likelihood function</p>
<p><span class="math display">\[
L(p_i) = \prod_{i=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}
\]</span></p>
<p>where <span class="math inline">\(p_i = \frac{\mathbf{x'_i \beta}}{1+\mathbf{x'_i \beta}}\)</span> and <span class="math inline">\(1-p_i = (1+ exp(\mathbf{x'_i \beta}))^{-1}\)</span></p>
<p>Hence, our objective function is</p>
<p><span class="math display">\[
Q(\beta) = log(L(\beta)) = \sum_{i=1}^n Y_i \mathbf{x'_i \beta} - \sum_{i=1}^n  log(1+ exp(\mathbf{x'_i \beta}))
\]</span></p>
<p>we could maximize this function numerically using the optimization method above, which allows us to find numerical MLE for <span class="math inline">\(\hat{\beta}\)</span>. Then we can use the standard asymptotic properties of MLEs to make inference.</p>
<p>Property of MLEs is that parameters are asymptotically unbiased with sample variance-covariance matrix given by the <strong>inverse Fisher information matrix</strong></p>
<p><span class="math display">\[
\hat{\beta} \dot{\sim} AN(\beta,[\mathbf{I}(\beta)]^{-1})
\]</span></p>
<p>where the <strong>Fisher Information matrix</strong>, <span class="math inline">\(\mathbf{I}(\beta)\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{I}(\beta) &amp;= E[\frac{\partial \log(L(\beta))}{\partial (\beta)}\frac{\partial \log(L(\beta))}{\partial \beta'}] \\
&amp;= E[(\frac{\partial \log(L(\beta))}{\partial \beta_i} \frac{\partial \log(L(\beta))}{\partial \beta_j})_{ij}]
\end{aligned}
\]</span></p>
<p>Under <strong>regularity conditions</strong>, this is equivalent to the negative of the expected value of the Hessian Matrix</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{I}(\beta) &amp;= -E[\frac{\partial^2 \log(L(\beta))}{\partial \beta \partial \beta'}] \\
&amp;= -E[(\frac{\partial^2 \log(L(\beta))}{\partial \beta_i \partial \beta_j})_{ij}]
\end{aligned}
\]</span></p>
<p>Example:</p>
<p><span class="math display">\[
x_i' \beta = \beta_0 + \beta_1 x_i
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_0} &amp;= \sum_{i=1}^n \frac{\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - [\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_1} &amp;= \sum_{i=1}^n \frac{x_i^2\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - [\frac{x_i\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n x_i^2p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta_0 \partial \beta_1} &amp;= \sum_{i=1}^n \frac{x_i\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - x_i[\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n x_ip_i (1-p_i)
\end{aligned}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\mathbf{I} (\beta) =
\left[
\begin{array}
{cc}
\sum_i p_i(1-p_i) &amp; \sum_i x_i p_i(1-p_i) \\
\sum_i x_i p_i(1-p_i) &amp; \sum_i x_i^2 p_i(1-p_i)
\end{array}
\right]
\]</span></p>
<p><strong>Inference</strong></p>
<p><strong>Likelihood Ratio Tests</strong></p>
<p>To formulate the test, let <span class="math inline">\(\beta = [\beta_1', \beta_2']'\)</span>. If you are interested in testing a hypothesis about <span class="math inline">\(\beta_1\)</span>, then we leave <span class="math inline">\(\beta_2\)</span> unspecified (called <strong>nuisance parameters</strong>). <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> can either a <strong>vector</strong> or <strong>scalar</strong>, or <span class="math inline">\(\beta_2\)</span> can be null.</p>
<p>Example: <span class="math inline">\(H_0: \beta_1 = \beta_{1,0}\)</span> (where <span class="math inline">\(\beta_{1,0}\)</span> is specified) and <span class="math inline">\(\hat{\beta}_{2,0}\)</span> be the MLE of <span class="math inline">\(\beta_2\)</span> under the restriction that <span class="math inline">\(\beta_1 = \beta_{1,0}\)</span>. The likelihood ratio test statistic is</p>
<p><span class="math display">\[
-2\log\Lambda = -2[\log(L(\beta_{1,0},\hat{\beta}_{2,0})) - \log(L(\hat{\beta}_1,\hat{\beta}_2))]
\]</span></p>
<p>where</p>
<ul>
<li>the first term is the value fo the likelihood for the fitted restricted model</li>
<li>the second term is the likelihood value of the fitted unrestricted model</li>
</ul>
<p>Under the null,</p>
<p><span class="math display">\[
-2 \log \Lambda \sim \chi^2_{\upsilon}
\]</span></p>
<p>where <span class="math inline">\(\upsilon\)</span> is the dimension of <span class="math inline">\(\beta_1\)</span></p>
<p>We reject the null when <span class="math inline">\(-2\log \Lambda &gt; \chi_{\upsilon,1-\alpha}^2\)</span></p>
<p><strong>Wald Statistics</strong></p>
<p>Based on</p>
<p><span class="math display">\[
\hat{\beta} \sim AN (\beta, [\mathbf{I}(\beta)^{-1}])
\]</span></p>
<p><span class="math display">\[
H_0: \mathbf{L}\hat{\beta} = 0
\]</span></p>
<p>where <span class="math inline">\(\mathbf{L}\)</span> is a <span class="math inline">\(q \times p\)</span> matrix with <span class="math inline">\(q\)</span> linearly independent rows. Then</p>
<p><span class="math display">\[
W = (\mathbf{L\hat{\beta}})'(\mathbf{L[I(\hat{\beta})]^{-1}L'})^{-1}(\mathbf{L\hat{\beta}})
\]</span></p>
<p>under the null hypothesis</p>
<p>Confidence interval</p>
<p><span class="math display">\[
\hat{\beta}_i \pm 1.96 \hat{s}_{ii}^2
\]</span></p>
<p>where <span class="math inline">\(\hat{s}_{ii}^2\)</span> is the i-th diagonal of <span class="math inline">\(\mathbf{[I(\hat{\beta})]}^{-1}\)</span></p>
<p>If you have</p>
<ul>
<li>large sample size, the likelihood ratio and Wald tests have similar results.</li>
<li>small sample size, the likelihood ratio test is better.</li>
</ul>
<p><strong>Logistic Regression: Interpretation of</strong> <span class="math inline">\(\beta\)</span></p>
<p>For single regressor, the model is</p>
<p><span class="math display">\[
logit\{\hat{p}_{x_i}\} \equiv logit (\hat{p}_i) = \log(\frac{\hat{p}_i}{1 - \hat{p}_i}) = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]</span></p>
<p>When <span class="math inline">\(x= x_i + 1\)</span></p>
<p><span class="math display">\[
logit\{\hat{p}_{x_i +1}\} = \hat{\beta}_0 + \hat{\beta}(x_i + 1) = logit\{\hat{p}_{x_i}\} + \hat{\beta}_1
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
\begin{aligned}
logit\{\hat{p}_{x_i +1}\} - logit\{\hat{p}_{x_i}\} &amp;= log\{odds[\hat{p}_{x_i +1}]\} - log\{odds[\hat{p}_{x_i}]\} \\
&amp;= log(\frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}) = \hat{\beta}_1
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
exp(\hat{\beta}_1) = \frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}
\]</span></p>
<p>the estimated <strong>odds ratio</strong></p>
<p>the estimated odds ratio, when there is a difference of c units in the regressor x, is <span class="math inline">\(exp(c\hat{\beta}_1)\)</span>. When there are multiple covariates, <span class="math inline">\(exp(\hat{\beta}_k)\)</span> is the estimated odds ratio for the variable <span class="math inline">\(x_k\)</span>, assuming that all of the other variables are held constant.</p>
<p><strong>Inference on the Mean Response</strong></p>
<p>Let <span class="math inline">\(x_h = (1, x_{h1}, ...,x_{h,p-1})'\)</span>. Then</p>
<p><span class="math display">\[
\hat{p}_h = \frac{exp(\mathbf{x'_h \hat{\beta}})}{1 + exp(\mathbf{x'_h \hat{\beta}})}
\]</span></p>
<p>and <span class="math inline">\(s^2(\hat{p}_h) = \mathbf{x'_h[I(\hat{\beta})]^{-1}x_h}\)</span></p>
<p>For new observation, we can have a cutoff point to decide whether y = 0 or 1.</p>
<div id="application-2" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">7.1.1</span> Application<a class="anchor" aria-label="anchor" href="#application-2"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://haozhu233.github.io/kableExtra/">kableExtra</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/atahk/pscl">pscl</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/julianfaraway/faraway">faraway</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://kwstat.github.io/agridat/">agridat</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/aursiber/nlstools">nlstools</a></span><span class="op">)</span></span></code></pre></div>
<p>Logistic Regression</p>
<p><span class="math inline">\(x \sim Unif(-0.5,2.5)\)</span>. Then <span class="math inline">\(\eta = 0.5 + 0.75 x\)</span></p>
<div class="sourceCode" id="cb133"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">23</span><span class="op">)</span> <span class="co">#set seed for reproducibility</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, max <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></span>
<span><span class="va">eta1</span> <span class="op">&lt;-</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.75</span> <span class="op">*</span> <span class="va">x</span></span></code></pre></div>
<p>Passing <span class="math inline">\(\eta\)</span>’s into the inverse-logit function, we get</p>
<p><span class="math display">\[
p = \frac{\exp(\eta)}{1+ \exp(\eta)}
\]</span></p>
<p>where <span class="math inline">\(p \in [0,1]\)</span></p>
<p>Then, we generate <span class="math inline">\(y \sim Bernoulli(p)\)</span></p>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">eta1</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">eta1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">1</span>, <span class="va">p</span><span class="op">)</span></span>
<span><span class="va">BinData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">x</span>, Y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span></code></pre></div>
<p><strong>Model Fit</strong></p>
<div class="sourceCode" id="cb135"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Logistic_Model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">Y</span> <span class="op">~</span> <span class="va">X</span>,</span>
<span>                      family <span class="op">=</span> <span class="va">binomial</span>, <span class="co"># family = specifies the response distribution</span></span>
<span>                      data <span class="op">=</span> <span class="va">BinData</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Logistic_Model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Y ~ X, family = binomial, data = BinData)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span><span class="co">#&gt; -2.2317   0.4153   0.5574   0.7922   1.1469  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.46205    0.10201   4.530 5.91e-06 ***</span></span>
<span><span class="co">#&gt; X            0.78527    0.09296   8.447  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1106.7  on 999  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1027.4  on 998  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 1031.4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span><span class="fu">nlstools</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/nlstools/man/confint2.html">confint2</a></span><span class="op">(</span><span class="va">Logistic_Model</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 2.5 %    97.5 %</span></span>
<span><span class="co">#&gt; (Intercept) 0.2618709 0.6622204</span></span>
<span><span class="co">#&gt; X           0.6028433 0.9676934</span></span>
<span><span class="va">OddsRatio</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">Logistic_Model</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">exp</span></span>
<span><span class="va">OddsRatio</span> </span>
<span><span class="co">#&gt; (Intercept)           X </span></span>
<span><span class="co">#&gt;    1.587318    2.192995</span></span></code></pre></div>
<p>Based on the odds ratio, when</p>
<ul>
<li>
<span class="math inline">\(x = 0\)</span> , the odds of success of 1.59</li>
<li>
<span class="math inline">\(x = 1\)</span>, the odds of success increase by a factor of 2.19 (i.e., 119.29% increase).</li>
</ul>
<p>Deviance Tests</p>
<ul>
<li>
<span class="math inline">\(H_0\)</span>: No variables are related to the response (i.e., model with just the intercept)</li>
<li>
<span class="math inline">\(H_1\)</span>: At least one variable is related to the response</li>
</ul>
<div class="sourceCode" id="cb136"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Test_Dev</span> <span class="op">&lt;-</span> <span class="va">Logistic_Model</span><span class="op">$</span><span class="va">null.deviance</span> <span class="op">-</span> <span class="va">Logistic_Model</span><span class="op">$</span><span class="va">deviance</span></span>
<span><span class="va">p_val_dev</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span>q <span class="op">=</span> <span class="va">Test_Dev</span>, df <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>Since we see the p-value of 0, we reject the null that no variables are related to the response</p>
<p><strong>Deviance residuals</strong></p>
<div class="sourceCode" id="cb137"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Logistic_Resids</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">Logistic_Model</span>, type <span class="op">=</span> <span class="st">"deviance"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    y <span class="op">=</span> <span class="va">Logistic_Resids</span>,</span>
<span>    x <span class="op">=</span> <span class="va">BinData</span><span class="op">$</span><span class="va">X</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">'X'</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">'Deviance Resids'</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>However, this plot is not informative. Hence, we can can see the residuals plots that are grouped into bins based on prediction values.</p>
<div class="sourceCode" id="cb138"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">plot_bin</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">Y</span>,</span>
<span>                     <span class="va">X</span>,</span>
<span>                     <span class="va">bins</span> <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                     <span class="va">return.DF</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">Y_Name</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html">deparse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substitute.html">substitute</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">X_Name</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html">deparse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substitute.html">substitute</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">Binned_Plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Plot_Y <span class="op">=</span> <span class="va">Y</span>, Plot_X <span class="op">=</span> <span class="va">X</span><span class="op">)</span></span>
<span>    <span class="va">Binned_Plot</span><span class="op">$</span><span class="va">bin</span> <span class="op">&lt;-</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span><span class="op">(</span><span class="va">Binned_Plot</span><span class="op">$</span><span class="va">Plot_X</span>, breaks <span class="op">=</span> <span class="va">bins</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.numeric</span></span>
<span>    <span class="va">Binned_Plot_summary</span> <span class="op">&lt;-</span> <span class="va">Binned_Plot</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>        <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">bin</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>        <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span></span>
<span>            Y_ave <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Plot_Y</span><span class="op">)</span>,</span>
<span>            X_ave <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Plot_X</span><span class="op">)</span>,</span>
<span>            Count <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span></span>
<span>        <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.data.frame</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>        y <span class="op">=</span> <span class="va">Binned_Plot_summary</span><span class="op">$</span><span class="va">Y_ave</span>,</span>
<span>        x <span class="op">=</span> <span class="va">Binned_Plot_summary</span><span class="op">$</span><span class="va">X_ave</span>,</span>
<span>        ylab <span class="op">=</span> <span class="va">Y_Name</span>,</span>
<span>        xlab <span class="op">=</span> <span class="va">X_Name</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">return.DF</span><span class="op">)</span></span>
<span>        <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">Binned_Plot_summary</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span></span>
<span><span class="fu">plot_bin</span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Logistic_Resids</span>,</span>
<span>         X <span class="op">=</span> <span class="va">BinData</span><span class="op">$</span><span class="va">X</span>,</span>
<span>         bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>We can also see the predicted value against the residuals.</p>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Logistic_Predictions</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Logistic_Model</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="fu">plot_bin</span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Logistic_Resids</span>, X <span class="op">=</span> <span class="va">Logistic_Predictions</span>, bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-8-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>We can also look at a binned plot of the logistic prediction versus the true category</p>
<div class="sourceCode" id="cb140"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">NumBins</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">Binned_Data</span> <span class="op">&lt;-</span> <span class="fu">plot_bin</span><span class="op">(</span></span>
<span>    Y <span class="op">=</span> <span class="va">BinData</span><span class="op">$</span><span class="va">Y</span>,</span>
<span>    X <span class="op">=</span> <span class="va">Logistic_Predictions</span>,</span>
<span>    bins <span class="op">=</span> <span class="va">NumBins</span>,</span>
<span>    return.DF <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">Binned_Data</span></span>
<span><span class="co">#&gt;    bin     Y_ave     X_ave Count</span></span>
<span><span class="co">#&gt; 1    1 0.5833333 0.5382095    72</span></span>
<span><span class="co">#&gt; 2    2 0.5200000 0.5795887    75</span></span>
<span><span class="co">#&gt; 3    3 0.6567164 0.6156540    67</span></span>
<span><span class="co">#&gt; 4    4 0.7014925 0.6579674    67</span></span>
<span><span class="co">#&gt; 5    5 0.6373626 0.6984765    91</span></span>
<span><span class="co">#&gt; 6    6 0.7500000 0.7373341    72</span></span>
<span><span class="co">#&gt; 7    7 0.7096774 0.7786747    93</span></span>
<span><span class="co">#&gt; 8    8 0.8503937 0.8203819   127</span></span>
<span><span class="co">#&gt; 9    9 0.8947368 0.8601232   133</span></span>
<span><span class="co">#&gt; 10  10 0.8916256 0.9004734   203</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, lty <span class="op">=</span> <span class="fl">2</span>, col <span class="op">=</span> <span class="st">'blue'</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>Formal deviance test</strong></p>
<p><strong>Hosmer-Lemeshow test</strong></p>
<p>Null hypothesis: the observed events match the expected evens</p>
<p><span class="math display">\[
X^2_{HL} = \sum_{j=1}^{J} \frac{(y_j - m_j \hat{p}_j)^2}{m_j \hat{p}_j(1-\hat{p}_j)}
\]</span></p>
<p>where</p>
<ul>
<li>within the j-th bin, <span class="math inline">\(y_j\)</span> is the number of successes</li>
<li>
<span class="math inline">\(m_j\)</span> = number of observations</li>
<li>
<span class="math inline">\(\hat{p}_j\)</span> = predicted probability</li>
</ul>
<p>Under the null hypothesis, <span class="math inline">\(X^2_{HLL} \sim \chi^2_{J-1}\)</span></p>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">HL_BinVals</span> <span class="op">&lt;-</span></span>
<span>    <span class="op">(</span><span class="va">Binned_Data</span><span class="op">$</span><span class="va">Count</span> <span class="op">*</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">Y_ave</span> <span class="op">-</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">Count</span> <span class="op">*</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">X_ave</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">/</span>   <span class="va">Binned_Data</span><span class="op">$</span><span class="va">Count</span> <span class="op">*</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">X_ave</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">Binned_Data</span><span class="op">$</span><span class="va">X_ave</span><span class="op">)</span></span>
<span><span class="va">HLpval</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span>q <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">HL_BinVals</span><span class="op">)</span>,</span>
<span>                 df <span class="op">=</span> <span class="va">NumBins</span>,</span>
<span>                 lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">HLpval</span></span>
<span><span class="co">#&gt; [1] 0.9999989</span></span></code></pre></div>
<p>Since <span class="math inline">\(p\)</span>-value = 0.99, we do not reject the null hypothesis (i.e., the model is fitting well).</p>
</div>
</div>
<div id="probit-regression" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Probit Regression<a class="anchor" aria-label="anchor" href="#probit-regression"><i class="fas fa-link"></i></a>
</h2>
<p><span class="math display">\[
E(Y_i) = p_i = \Phi(\mathbf{x_i'\theta})
\]</span></p>
<p>where <span class="math inline">\(\Phi()\)</span> is the CDF of a <span class="math inline">\(N(0,1)\)</span> random variable.</p>
<p>Other models (e..g, t–distribution; log-log; I complimentary log-log)</p>
<p>We let <span class="math inline">\(Y_i = 1\)</span> success, <span class="math inline">\(Y_i =0\)</span> no success.</p>
<ul>
<li><p>assume <span class="math inline">\(Y \sim Ber\)</span> and <span class="math inline">\(p_i = P(Y_i =1)\)</span>, the success probability.</p></li>
<li><p>consider a logistic regression with the response function <span class="math inline">\(logit(p_i) = x'_i \beta\)</span></p></li>
</ul>
<p><strong>Confusion matrix</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>Predicted</th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Truth</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr class="odd">
<td>0</td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table></div>
<p>Sensitivity: ability to identify positive results</p>
<p><span class="math display">\[
\text{Sensitivity} = \frac{TP}{TP + FN}
\]</span></p>
<p>Specificity: ability to identify negative results</p>
<p><span class="math display">\[
\text{Specificity} = \frac{TN}{TN + FP}
\]</span></p>
<p>False positive rate: Type I error (1- specificity)</p>
<p><span class="math display">\[
\text{ False Positive Rate} = \frac{FP}{TN+ FP}
\]</span></p>
<p>False Negative Rate: Type II error (1-sensitivity)</p>
<p><span class="math display">\[
\text{False Negative Rate} = \frac{FN}{TP + FN}
\]</span></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>Predicted</th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Truth</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>Sensitivity</td>
<td>False Negative Rate</td>
</tr>
<tr class="odd">
<td>0</td>
<td>False Positive Rate</td>
<td>Specificity</td>
</tr>
</tbody>
</table></div>
</div>
<div id="binomial-regression" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Binomial Regression<a class="anchor" aria-label="anchor" href="#binomial-regression"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Binomial</strong></p>
<p>Here, cancer case = successes, and control case = failures.</p>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"esoph"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">esoph</span>, n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;   agegp     alcgp    tobgp ncases ncontrols</span></span>
<span><span class="co">#&gt; 1 25-34 0-39g/day 0-9g/day      0        40</span></span>
<span><span class="co">#&gt; 2 25-34 0-39g/day    10-19      0        10</span></span>
<span><span class="co">#&gt; 3 25-34 0-39g/day    20-29      0         6</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>  <span class="va">esoph</span><span class="op">$</span><span class="va">ncases</span> <span class="op">/</span> <span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">ncases</span> <span class="op">+</span> <span class="va">esoph</span><span class="op">$</span><span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">esoph</span><span class="op">$</span><span class="va">alcgp</span>,</span>
<span>  ylab <span class="op">=</span> <span class="st">"Proportion"</span>,</span>
<span>  xlab <span class="op">=</span> <span class="st">'Alcohol consumption'</span>,</span>
<span>  main <span class="op">=</span> <span class="st">'Esophageal Cancer data'</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-11-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">agegp</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"factor"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">alcgp</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"factor"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">esoph</span><span class="op">$</span><span class="va">tobgp</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"factor"</span></span></code></pre></div>
<div class="sourceCode" id="cb144"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#  only the alcohol consumption as a predictor</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ncases</span>, <span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">alcgp</span>, data <span class="op">=</span> <span class="va">esoph</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = cbind(ncases, ncontrols) ~ alcgp, family = binomial, </span></span>
<span><span class="co">#&gt;     data = esoph)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span><span class="co">#&gt; -4.0759  -1.2037  -0.0183   1.0928   3.7336  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -2.5885     0.1925 -13.444  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; alcgp40-79    1.2712     0.2323   5.472 4.46e-08 ***</span></span>
<span><span class="co">#&gt; alcgp80-119   2.0545     0.2611   7.868 3.59e-15 ***</span></span>
<span><span class="co">#&gt; alcgp120+     3.3042     0.3237  10.209  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 367.95  on 87  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 221.46  on 84  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 344.51</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Coefficient Odds</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">exp</span></span>
<span><span class="co">#&gt; (Intercept)  alcgp40-79 alcgp80-119   alcgp120+ </span></span>
<span><span class="co">#&gt;  0.07512953  3.56527094  7.80261593 27.22570533</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/df.residual.html">df.residual</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.63638</span></span>
<span><span class="va">model</span><span class="op">$</span><span class="va">aic</span></span>
<span><span class="co">#&gt; [1] 344.5109</span></span></code></pre></div>
<div class="sourceCode" id="cb146"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># alcohol consumption and age as predictors</span></span>
<span><span class="va">better_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ncases</span>, <span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">agegp</span> <span class="op">+</span> <span class="va">alcgp</span>,</span>
<span>        data <span class="op">=</span> <span class="va">esoph</span>,</span>
<span>        family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">better_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial, </span></span>
<span><span class="co">#&gt;     data = esoph)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span><span class="co">#&gt; -2.2395  -0.7186  -0.2324   0.7930   3.3538  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -6.1472     1.0419  -5.900 3.63e-09 ***</span></span>
<span><span class="co">#&gt; agegp35-44    1.6311     1.0800   1.510 0.130973    </span></span>
<span><span class="co">#&gt; agegp45-54    3.4258     1.0389   3.297 0.000976 ***</span></span>
<span><span class="co">#&gt; agegp55-64    3.9435     1.0346   3.811 0.000138 ***</span></span>
<span><span class="co">#&gt; agegp65-74    4.3568     1.0413   4.184 2.87e-05 ***</span></span>
<span><span class="co">#&gt; agegp75+      4.4242     1.0914   4.054 5.04e-05 ***</span></span>
<span><span class="co">#&gt; alcgp40-79    1.4343     0.2448   5.859 4.64e-09 ***</span></span>
<span><span class="co">#&gt; alcgp80-119   2.0071     0.2776   7.230 4.84e-13 ***</span></span>
<span><span class="co">#&gt; alcgp120+     3.6800     0.3763   9.778  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 367.95  on 87  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 105.88  on 79  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 238.94</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 6</span></span></code></pre></div>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">better_model</span><span class="op">$</span><span class="va">aic</span> <span class="co">#smaller AIC is better</span></span>
<span><span class="co">#&gt; [1] 238.9361</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">better_model</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">exp</span></span>
<span><span class="co">#&gt;  (Intercept)   agegp35-44   agegp45-54   agegp55-64   agegp65-74     agegp75+ </span></span>
<span><span class="co">#&gt;  0.002139482  5.109601844 30.748594216 51.596634690 78.005283850 83.448437749 </span></span>
<span><span class="co">#&gt;   alcgp40-79  alcgp80-119    alcgp120+ </span></span>
<span><span class="co">#&gt;  4.196747169  7.441782227 39.646885126</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span></span>
<span>    q <span class="op">=</span> <span class="va">model</span><span class="op">$</span><span class="va">deviance</span> <span class="op">-</span> <span class="va">better_model</span><span class="op">$</span><span class="va">deviance</span>,</span>
<span>    df <span class="op">=</span> <span class="va">model</span><span class="op">$</span><span class="va">df.residual</span> <span class="op">-</span> <span class="va">better_model</span><span class="op">$</span><span class="va">df.residual</span>,</span>
<span>    lower <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.713923e-23</span></span></code></pre></div>
<div class="sourceCode" id="cb148"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># specify link function as probit</span></span>
<span><span class="va">Prob_better_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ncases</span>, <span class="va">ncontrols</span><span class="op">)</span> <span class="op">~</span> <span class="va">agegp</span> <span class="op">+</span> <span class="va">alcgp</span>,</span>
<span>    data <span class="op">=</span> <span class="va">esoph</span>,</span>
<span>    family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="va">probit</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Prob_better_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial(link = probit), </span></span>
<span><span class="co">#&gt;     data = esoph)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span><span class="co">#&gt; -2.1325  -0.6877  -0.1661   0.7654   3.3258  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -3.3741     0.4922  -6.855 7.13e-12 ***</span></span>
<span><span class="co">#&gt; agegp35-44    0.8562     0.5081   1.685 0.092003 .  </span></span>
<span><span class="co">#&gt; agegp45-54    1.7829     0.4904   3.636 0.000277 ***</span></span>
<span><span class="co">#&gt; agegp55-64    2.1034     0.4876   4.314 1.61e-05 ***</span></span>
<span><span class="co">#&gt; agegp65-74    2.3374     0.4930   4.741 2.13e-06 ***</span></span>
<span><span class="co">#&gt; agegp75+      2.3694     0.5275   4.491 7.08e-06 ***</span></span>
<span><span class="co">#&gt; alcgp40-79    0.8080     0.1330   6.076 1.23e-09 ***</span></span>
<span><span class="co">#&gt; alcgp80-119   1.1399     0.1558   7.318 2.52e-13 ***</span></span>
<span><span class="co">#&gt; alcgp120+     2.1204     0.2060  10.295  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 367.95  on 87  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 104.48  on 79  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 237.53</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 6</span></span></code></pre></div>
</div>
<div id="poisson-regression" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Poisson Regression<a class="anchor" aria-label="anchor" href="#poisson-regression"><i class="fas fa-link"></i></a>
</h2>
<p>From the Poisson distribution</p>
<p><span class="math display">\[
\begin{aligned}
f(Y_i) &amp;= \frac{\mu_i^{Y_i}exp(-\mu_i)}{Y_i!}, Y_i = 0,1,.. \\
E(Y_i) &amp;= \mu_i  \\
var(Y_i) &amp;= \mu_i
\end{aligned}
\]</span></p>
<p>which is a natural distribution for counts. We can see that the variance is a function of the mean. If we let <span class="math inline">\(\mu_i = f(\mathbf{x_i; \theta})\)</span>, it would be similar to <a href="descriptive-stat.html#logistic-regression">Logistic Regression</a> since we can choose <span class="math inline">\(f()\)</span> as <span class="math inline">\(\mu_i = \mathbf{x_i'\theta}, \mu_i = \exp(\mathbf{x_i'\theta}), \mu_i = \log(\mathbf{x_i'\theta})\)</span></p>
<div id="application-3" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">7.4.1</span> Application<a class="anchor" aria-label="anchor" href="#application-3"><i class="fas fa-link"></i></a>
</h3>
<p>Count Data and Poisson regression</p>
<div class="sourceCode" id="cb149"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">bioChemists</span>, package <span class="op">=</span> <span class="st">"pscl"</span><span class="op">)</span></span>
<span><span class="va">bioChemists</span> <span class="op">&lt;-</span> <span class="va">bioChemists</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span></span>
<span>        Num_Article <span class="op">=</span> <span class="va">art</span>, <span class="co">#articles in last 3 years of PhD</span></span>
<span>        Sex <span class="op">=</span> <span class="va">fem</span>, <span class="co">#coded 1 if female</span></span>
<span>        Married <span class="op">=</span> <span class="va">mar</span>, <span class="co">#coded 1 if married</span></span>
<span>        Num_Kid5 <span class="op">=</span> <span class="va">kid5</span>, <span class="co">#number of childeren under age 6</span></span>
<span>        PhD_Quality <span class="op">=</span> <span class="va">phd</span>, <span class="co">#prestige of PhD program</span></span>
<span>        Num_MentArticle <span class="op">=</span> <span class="va">ment</span> <span class="co">#articles by mentor in last 3 years</span></span>
<span>    <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">bioChemists</span><span class="op">$</span><span class="va">Num_Article</span>, breaks <span class="op">=</span> <span class="fl">25</span>, main <span class="op">=</span> <span class="st">'Number of Articles'</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb150"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Poisson_Mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, family<span class="op">=</span><span class="va">poisson</span>, <span class="va">bioChemists</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Poisson_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span><span class="co">#&gt; -3.5672  -1.5398  -0.3660   0.5722   5.4467  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.304617   0.102981   2.958   0.0031 ** </span></span>
<span><span class="co">#&gt; SexWomen        -0.224594   0.054613  -4.112 3.92e-05 ***</span></span>
<span><span class="co">#&gt; MarriedMarried   0.155243   0.061374   2.529   0.0114 *  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.184883   0.040127  -4.607 4.08e-06 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.012823   0.026397   0.486   0.6271    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.025543   0.002006  12.733  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for poisson family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1817.4  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1634.4  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3314.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>Residual of 1634 with 909 df isn’t great.</p>
<p>We see Pearson <span class="math inline">\(\chi^2\)</span></p>
<div class="sourceCode" id="cb151"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Predicted_Means</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Poisson_Mod</span>,type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="va">X2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">bioChemists</span><span class="op">$</span><span class="va">Num_Article</span> <span class="op">-</span> <span class="va">Predicted_Means</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="va">Predicted_Means</span><span class="op">)</span></span>
<span><span class="va">X2</span></span>
<span><span class="co">#&gt; [1] 1662.547</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">X2</span>,<span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 7.849882e-47</span></span></code></pre></div>
<p>With interaction terms, there are some improvements</p>
<div class="sourceCode" id="cb152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Poisson_Mod_All2way</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span><span class="op">^</span><span class="fl">2</span>, family<span class="op">=</span><span class="va">poisson</span>, <span class="va">bioChemists</span><span class="op">)</span></span>
<span><span class="va">Poisson_Mod_All3way</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span><span class="op">^</span><span class="fl">3</span>, family<span class="op">=</span><span class="va">poisson</span>, <span class="va">bioChemists</span><span class="op">)</span></span></code></pre></div>
<p>Consider the <span class="math inline">\(\hat{\phi} = \frac{\text{deviance}}{df}\)</span></p>
<div class="sourceCode" id="cb153"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">deviance</span> <span class="op">/</span> <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span></span>
<span><span class="co">#&gt; [1] 1.797988</span></span></code></pre></div>
<p>This is evidence for over-dispersion. Likely cause is missing variables. And remedies could either be to include more variables or consider random effects.</p>
<p>A quick fix is to force the Poisson Regression to include this value of <span class="math inline">\(\phi\)</span>, and this model is called “Quasi-Poisson”.</p>
<div class="sourceCode" id="cb154"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">phi_hat</span> <span class="op">=</span> <span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">deviance</span><span class="op">/</span><span class="va">Poisson_Mod</span><span class="op">$</span><span class="va">df.residual</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Poisson_Mod</span>,dispersion <span class="op">=</span> <span class="va">phi_hat</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span><span class="co">#&gt; -3.5672  -1.5398  -0.3660   0.5722   5.4467  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.30462    0.13809   2.206  0.02739 *  </span></span>
<span><span class="co">#&gt; SexWomen        -0.22459    0.07323  -3.067  0.00216 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.15524    0.08230   1.886  0.05924 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.18488    0.05381  -3.436  0.00059 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.01282    0.03540   0.362  0.71715    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.02554    0.00269   9.496  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for poisson family taken to be 1.797988)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1817.4  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1634.4  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3314.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>Or directly rerun the model as</p>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">quasiPoisson_Mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>, family<span class="op">=</span><span class="va">quasipoisson</span>, <span class="va">bioChemists</span><span class="op">)</span></span></code></pre></div>
<p>Quasi-Poisson is not recommended, but <a href="generalized-linear-models.html#negative-binomial-regression">Negative Binomial Regression</a> that has an extra parameter to account for over-dispersion is.</p>
</div>
</div>
<div id="negative-binomial-regression" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Negative Binomial Regression<a class="anchor" aria-label="anchor" href="#negative-binomial-regression"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="va">NegBinom_Mod</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/glm.nb.html">glm.nb</a></span><span class="op">(</span><span class="va">Num_Article</span> <span class="op">~</span> <span class="va">.</span>,<span class="va">bioChemists</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">NegBinom_Mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; MASS::glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, </span></span>
<span><span class="co">#&gt;     link = log)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;     Min       1Q   Median       3Q      Max  </span></span>
<span><span class="co">#&gt; -2.1678  -1.3617  -0.2806   0.4476   3.4524  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)      0.256144   0.137348   1.865 0.062191 .  </span></span>
<span><span class="co">#&gt; SexWomen        -0.216418   0.072636  -2.979 0.002887 ** </span></span>
<span><span class="co">#&gt; MarriedMarried   0.150489   0.082097   1.833 0.066791 .  </span></span>
<span><span class="co">#&gt; Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***</span></span>
<span><span class="co">#&gt; PhD_Quality      0.015271   0.035873   0.426 0.670326    </span></span>
<span><span class="co">#&gt; Num_MentArticle  0.029082   0.003214   9.048  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1109.0  on 914  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 1004.3  on 909  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 3135.9</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;               Theta:  2.264 </span></span>
<span><span class="co">#&gt;           Std. Err.:  0.271 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  2 x log-likelihood:  -3121.917</span></span></code></pre></div>
<p>We can see the dispersion is 2.264 with SE = 0.271, which is significantly different from 1, indicating over-dispersion. Check <a href="generalized-linear-models.html#over-dispersion">Over-Dispersion</a> for more detail</p>
</div>
<div id="multinomial" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> Multinomial<a class="anchor" aria-label="anchor" href="#multinomial"><i class="fas fa-link"></i></a>
</h2>
<p>If we have more than two categories or groups that we want to model relative to covariates (e.g., we have observations <span class="math inline">\(i = 1,…,n\)</span> and groups/ covariates <span class="math inline">\(j = 1,2,…,J\)</span>), multinomial is our candidate model</p>
<p>Let</p>
<ul>
<li>
<span class="math inline">\(p_{ij}\)</span> be the probability that the i-th observation belongs to the j-th group</li>
<li>
<span class="math inline">\(Y_{ij}\)</span> be the number of observations for individual i in group j; An individual will have observations <span class="math inline">\(Y_{i1},Y_{i2},…Y_{iJ}\)</span>
</li>
<li>assume the probability of observing this response is given by a multinomial distribution in terms of probabilities <span class="math inline">\(p_{ij}\)</span>, where <span class="math inline">\(\sum_{j = 1}^J p_{ij} = 1\)</span> . For interpretation, we have a baseline category <span class="math inline">\(p_{i1} = 1 - \sum_{j = 2}^J p_{ij}\)</span>
</li>
</ul>
<p>The link between the mean response (probability) <span class="math inline">\(p_{ij}\)</span> and a linear function of the covariates</p>
<p><span class="math display">\[
\eta_{ij} = \mathbf{x'_i \beta_j} = \log \frac{p_{ij}}{p_{i1}}, j = 2,..,J
\]</span></p>
<p>We compare <span class="math inline">\(p_{ij}\)</span> to the baseline <span class="math inline">\(p_{i1}\)</span>, suggesting</p>
<p><span class="math display">\[
p_{ij} = \frac{\exp(\eta_{ij})}{1 + \sum_{i=2}^J \exp(\eta_{ij})}
\]</span></p>
<p>which is known as <strong>multinomial logistic</strong> model.</p>
<p>Note:</p>
<ul>
<li>Softmax coding for multinomial logistic regression: rather than selecting a baseline class, we treat all <span class="math inline">\(K\)</span> class symmetrically - equally important (no baseline).</li>
</ul>
<p><span class="math display">\[
P(Y = k | X = x) = \frac{exp(\beta_{k1} + \dots + \beta_{k_p x_p})}{\sum_{l = 1}^K exp(\beta_{l0} + \dots + \beta_{l_p x_p})}
\]</span></p>
<p>then the log odds ratio between k-th and k’-th classes is</p>
<p><span class="math display">\[
\log (\frac{P(Y=k|X=x)}{P(Y = k' | X=x)}) = (\beta_{k0} - \beta_{k'0}) + \dots + (\beta_{kp} - \beta_{k'p}) x_p
\]</span></p>
<div class="sourceCode" id="cb157"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/julianfaraway/faraway">faraway</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">nes96</span>, package<span class="op">=</span><span class="st">"faraway"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">nes96</span>,<span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote</span></span>
<span><span class="co">#&gt; 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole</span></span>
<span><span class="co">#&gt; 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton</span></span>
<span><span class="co">#&gt; 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton</span></span></code></pre></div>
<p>We try to understand their political strength</p>
<div class="sourceCode" id="cb158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">nes96</span><span class="op">$</span><span class="va">PID</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  strDem weakDem  indDem  indind  indRep weakRep  strRep </span></span>
<span><span class="co">#&gt;     200     180     108      37      94     150     175</span></span>
<span><span class="va">nes96</span><span class="op">$</span><span class="va">Political_Strength</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span><span class="va">nes96</span><span class="op">$</span><span class="va">Political_Strength</span><span class="op">[</span><span class="va">nes96</span><span class="op">$</span><span class="va">PID</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"strDem"</span>, <span class="st">"strRep"</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span></span>
<span>    <span class="st">"Strong"</span></span>
<span><span class="va">nes96</span><span class="op">$</span><span class="va">Political_Strength</span><span class="op">[</span><span class="va">nes96</span><span class="op">$</span><span class="va">PID</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"weakDem"</span>, <span class="st">"weakRep"</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span></span>
<span>    <span class="st">"Weak"</span></span>
<span><span class="va">nes96</span><span class="op">$</span><span class="va">Political_Strength</span><span class="op">[</span><span class="va">nes96</span><span class="op">$</span><span class="va">PID</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"indDem"</span>, <span class="st">"indind"</span>, <span class="st">"indRep"</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span></span>
<span>    <span class="st">"Neutral"</span></span>
<span><span class="va">nes96</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Political_Strength</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>Count <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   Political_Strength Count</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;              &lt;int&gt;</span></span>
<span><span class="co">#&gt; 1 Neutral              239</span></span>
<span><span class="co">#&gt; 2 Strong               375</span></span>
<span><span class="co">#&gt; 3 Weak                 330</span></span></code></pre></div>
<p>visualize the political strength variable</p>
<div class="sourceCode" id="cb159"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="va">Plot_DF</span> <span class="op">&lt;-</span> <span class="va">nes96</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Age_Grp <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/cut_interval.html">cut_number</a></span><span class="op">(</span><span class="va">age</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Age_Grp</span>, <span class="va">Political_Strength</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>count <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">Age_Grp</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>etotal <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span>, proportion <span class="op">=</span> <span class="va">count</span> <span class="op">/</span> <span class="va">etotal</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Age_Plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span></span>
<span>    <span class="va">Plot_DF</span>,</span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>        x        <span class="op">=</span> <span class="va">Age_Grp</span>,</span>
<span>        y        <span class="op">=</span> <span class="va">proportion</span>,</span>
<span>        group    <span class="op">=</span> <span class="va">Political_Strength</span>,</span>
<span>        linetype <span class="op">=</span> <span class="va">Political_Strength</span>,</span>
<span>        color    <span class="op">=</span> <span class="va">Political_Strength</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">Age_Plot</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-27-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Fit the multinomial logistic model:</p>
<p>model political strength as a function of age and education</p>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span></span>
<span><span class="va">Multinomial_Model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/nnet/man/multinom.html">multinom</a></span><span class="op">(</span><span class="va">Political_Strength</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="va">educ</span>, <span class="va">nes96</span>, trace <span class="op">=</span> <span class="cn">F</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">Multinomial_Model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; multinom(formula = Political_Strength ~ age + educ, data = nes96, </span></span>
<span><span class="co">#&gt;     trace = F)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;        (Intercept)          age     educ.L     educ.Q     educ.C      educ^4</span></span>
<span><span class="co">#&gt; Strong -0.08788729  0.010700364 -0.1098951 -0.2016197 -0.1757739 -0.02116307</span></span>
<span><span class="co">#&gt; Weak    0.51976285 -0.004868771 -0.1431104 -0.2405395 -0.2411795  0.18353634</span></span>
<span><span class="co">#&gt;            educ^5     educ^6</span></span>
<span><span class="co">#&gt; Strong -0.1664377 -0.1359449</span></span>
<span><span class="co">#&gt; Weak   -0.1489030 -0.2173144</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Std. Errors:</span></span>
<span><span class="co">#&gt;        (Intercept)         age    educ.L    educ.Q    educ.C    educ^4</span></span>
<span><span class="co">#&gt; Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776</span></span>
<span><span class="co">#&gt; Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149</span></span>
<span><span class="co">#&gt;           educ^5    educ^6</span></span>
<span><span class="co">#&gt; Strong 0.2515012 0.2166774</span></span>
<span><span class="co">#&gt; Weak   0.2643747 0.2199186</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual Deviance: 2024.596 </span></span>
<span><span class="co">#&gt; AIC: 2056.596</span></span></code></pre></div>
<p>Alternatively, stepwise model selection based AIC</p>
<div class="sourceCode" id="cb161"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Multinomial_Step</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span><span class="va">Multinomial_Model</span>,trace <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="co">#&gt; trying - age </span></span>
<span><span class="co">#&gt; trying - educ </span></span>
<span><span class="co">#&gt; trying - age</span></span>
<span><span class="va">Multinomial_Step</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; multinom(formula = Political_Strength ~ age, data = nes96, trace = F)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;        (Intercept)          age</span></span>
<span><span class="co">#&gt; Strong -0.01988977  0.009832916</span></span>
<span><span class="co">#&gt; Weak    0.59497046 -0.005954348</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual Deviance: 2030.756 </span></span>
<span><span class="co">#&gt; AIC: 2038.756</span></span></code></pre></div>
<p>compare the best model to the full model based on deviance</p>
<div class="sourceCode" id="cb162"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span>q <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">Multinomial_Step</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/deviance.html">deviance</a></span><span class="op">(</span><span class="va">Multinomial_Model</span><span class="op">)</span>,</span>
<span>df <span class="op">=</span> <span class="va">Multinomial_Model</span><span class="op">$</span><span class="va">edf</span><span class="op">-</span><span class="va">Multinomial_Step</span><span class="op">$</span><span class="va">edf</span>,lower<span class="op">=</span><span class="cn">F</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.9078172</span></span></code></pre></div>
<p>We see no significant difference</p>
<p>Plot of the fitted model</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">PlotData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fl">19</span>, to <span class="op">=</span> <span class="fl">91</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Preds</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">PlotData</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_cols.html">bind_cols</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span></span>
<span>    object <span class="op">=</span> <span class="va">Multinomial_Step</span>,</span>
<span>    <span class="va">PlotData</span>, type <span class="op">=</span> <span class="st">"probs"</span></span>
<span>  <span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>  x       <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">age</span>,</span>
<span>  y       <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">Neutral</span>,</span>
<span>  type    <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>  ylim    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.6</span><span class="op">)</span>,</span>
<span>  col     <span class="op">=</span> <span class="st">"black"</span>,</span>
<span>  ylab    <span class="op">=</span> <span class="st">"Proportion"</span>,</span>
<span>  xlab    <span class="op">=</span> <span class="st">"Age"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>x   <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">age</span>,</span>
<span>      y   <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">Weak</span>,</span>
<span>      col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>x   <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">age</span>,</span>
<span>      y   <span class="op">=</span> <span class="va">Preds</span><span class="op">$</span><span class="va">Strong</span>,</span>
<span>      col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">'topleft'</span>,</span>
<span>  legend  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'Neutral'</span>, <span class="st">'Weak'</span>, <span class="st">'Strong'</span><span class="op">)</span>,</span>
<span>  col     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'black'</span>, <span class="st">'blue'</span>, <span class="st">'red'</span><span class="op">)</span>,</span>
<span>  lty     <span class="op">=</span> <span class="fl">1</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-31-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb164"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Multinomial_Step</span>,<span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>age <span class="op">=</span> <span class="fl">34</span><span class="op">)</span><span class="op">)</span> <span class="co"># predicted result (categoriy of political strength) of 34 year old</span></span>
<span><span class="co">#&gt; [1] Weak</span></span>
<span><span class="co">#&gt; Levels: Neutral Strong Weak</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Multinomial_Step</span>,<span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">34</span>,<span class="fl">35</span><span class="op">)</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"probs"</span><span class="op">)</span> <span class="co"># predicted result of the probabilities of each level of political strength for a 34 and 35</span></span>
<span><span class="co">#&gt;     Neutral    Strong      Weak</span></span>
<span><span class="co">#&gt; 1 0.2597275 0.3556910 0.3845815</span></span>
<span><span class="co">#&gt; 2 0.2594080 0.3587639 0.3818281</span></span></code></pre></div>
<p>If categories are ordered (i.e., ordinal data), we must use another approach (still multinomial, but use cumulative probabilities).</p>
<p>Another example</p>
<div class="sourceCode" id="cb165"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://kwstat.github.io/agridat/">agridat</a></span><span class="op">)</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu">agridat</span><span class="fu">::</span><span class="va"><a href="https://kwstat.github.io/agridat/reference/streibig.competition.html">streibig.competition</a></span></span>
<span><span class="co"># See Schaberger and Pierce, pages 370+</span></span>
<span><span class="co"># Consider only the mono-species barley data (no competition from Sinapis)</span></span>
<span><span class="va">gammaDat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">dat</span>, <span class="va">sseeds</span> <span class="op">&lt;</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">gammaDat</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/transform.html">transform</a></span><span class="op">(</span><span class="va">gammaDat</span>,</span>
<span>              x <span class="op">=</span> <span class="va">bseeds</span>,</span>
<span>              y <span class="op">=</span> <span class="va">bdwt</span>,</span>
<span>              block <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">block</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Inverse yield looks like it will be a good fit for Gamma's inverse link</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">gammaDat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>color <span class="op">=</span> <span class="va">block</span>, shape <span class="op">=</span> <span class="va">block</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">'Seeding Rate'</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">'Inverse yield'</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">'Streibig Competion - Barley only'</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-33-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><span class="math display">\[
Y \sim Gamma
\]</span></p>
<p>because Gamma is non-negative as opposed to Normal. The canonical Gamma link function is the inverse (or reciprocal) link</p>
<p><span class="math display">\[
\begin{aligned}
\eta_{ij} &amp;= \beta_{0j} + \beta_{1j}x_{ij} + \beta_2x_{ij}^2 \\
Y_{ij} &amp;= \eta_{ij}^{-1}
\end{aligned}
\]</span></p>
<p>The linear predictor is a quadratic model fit to each of the j-th blocks. A different model (not fitted) could be one with common slopes: <code>glm(y ~ x + I(x^2),…)</code></p>
<div class="sourceCode" id="cb166"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># linear predictor is quadratic, with separate intercept and slope per block</span></span>
<span><span class="va">m1</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">block</span> <span class="op">+</span> <span class="va">block</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="va">block</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">x</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>        data <span class="op">=</span> <span class="va">gammaDat</span>,</span>
<span>        family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">Gamma</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"inverse"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = y ~ block + block * x + block * I(x^2), family = Gamma(link = "inverse"), </span></span>
<span><span class="co">#&gt;     data = gammaDat)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Deviance Residuals: </span></span>
<span><span class="co">#&gt;      Min        1Q    Median        3Q       Max  </span></span>
<span><span class="co">#&gt; -1.21708  -0.44148   0.02479   0.17999   0.80745  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)     1.115e-01  2.870e-02   3.886 0.000854 ***</span></span>
<span><span class="co">#&gt; blockB2        -1.208e-02  3.880e-02  -0.311 0.758630    </span></span>
<span><span class="co">#&gt; blockB3        -2.386e-02  3.683e-02  -0.648 0.524029    </span></span>
<span><span class="co">#&gt; x              -2.075e-03  1.099e-03  -1.888 0.072884 .  </span></span>
<span><span class="co">#&gt; I(x^2)          1.372e-05  9.109e-06   1.506 0.146849    </span></span>
<span><span class="co">#&gt; blockB2:x       5.198e-04  1.468e-03   0.354 0.726814    </span></span>
<span><span class="co">#&gt; blockB3:x       7.475e-04  1.393e-03   0.537 0.597103    </span></span>
<span><span class="co">#&gt; blockB2:I(x^2) -5.076e-06  1.184e-05  -0.429 0.672475    </span></span>
<span><span class="co">#&gt; blockB3:I(x^2) -6.651e-06  1.123e-05  -0.592 0.560012    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for Gamma family taken to be 0.3232083)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 13.1677  on 29  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  7.8605  on 21  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 225.32</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 5</span></span></code></pre></div>
<p>For predict new value of <span class="math inline">\(x\)</span></p>
<div class="sourceCode" id="cb167"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">newdf</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">120</span>, length <span class="op">=</span> <span class="fl">50</span><span class="op">)</span>, block <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'B1'</span>, <span class="st">'B2'</span>, <span class="st">'B3'</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">newdf</span><span class="op">$</span><span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">m1</span>, new <span class="op">=</span> <span class="va">newdf</span>, type <span class="op">=</span> <span class="st">'response'</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">gammaDat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>color <span class="op">=</span> <span class="va">block</span>, shape <span class="op">=</span> <span class="va">block</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">'Seeding Rate'</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">'Inverse yield'</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">'Streibig Competion - Barley only Predictions'</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">newdf</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>        x <span class="op">=</span> <span class="va">x</span>,</span>
<span>        y <span class="op">=</span> <span class="va">pred</span>,</span>
<span>        color <span class="op">=</span> <span class="va">block</span>,</span>
<span>        linetype <span class="op">=</span> <span class="va">block</span></span>
<span>    <span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="07-generalized-linear-models_files/figure-html/unnamed-chunk-35-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="generalization" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> Generalization<a class="anchor" aria-label="anchor" href="#generalization"><i class="fas fa-link"></i></a>
</h2>
<p>We can see that Poisson regression looks similar to logistic regression. Hence, we can generalize to a class of modeling. Thanks to <span class="citation">Nelder and Wedderburn (<a href="references.html#ref-nelder1972generalized">1972</a>)</span>, we have the <strong>generalized linear models</strong> (GLMs). Estimation is generalize in these models.</p>
<p><strong>Exponential Family</strong><br>
The theory of GLMs is developed for data with distribution given y the <strong>exponential family</strong>.<br>
The form of the data distribution that is useful for GLMs is</p>
<p><span class="math display">\[
f(y;\theta, \phi) = \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi))
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\theta\)</span> is called the natural parameter</li>
<li>
<span class="math inline">\(\phi\)</span> is called the dispersion parameter</li>
</ul>
<p><strong>Note</strong>:</p>
<p>This family includes the <a href="prerequisites.html#gamma">Gamma</a>, <a href="prerequisites.html#normal">Normal</a>, <a href="prerequisites.html#poisson">Poisson</a>, and other. For all parameterization of the exponential family, check this <a href="https://www.stat.purdue.edu/~tlzhang/stat526/logistic.pdf">link</a></p>
<p><strong>Example</strong></p>
<p>if we have <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
f(y; \mu, \sigma^2) &amp;= \frac{1}{(2\pi \sigma^2)^{1/2}}\exp(-\frac{1}{2\sigma^2}(y- \mu)^2) \\
&amp;= \exp(-\frac{1}{2\sigma^2}(y^2 - 2y \mu +\mu^2)- \frac{1}{2}\log(2\pi \sigma^2)) \\
&amp;= \exp(\frac{y \mu - \mu^2/2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2}\log(2\pi \sigma^2)) \\
&amp;= \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y , \phi))
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\theta = \mu\)</span></li>
<li><span class="math inline">\(b(\theta) = \frac{\mu^2}{2}\)</span></li>
<li><span class="math inline">\(a(\phi) = \sigma^2 = \phi\)</span></li>
<li><span class="math inline">\(c(y , \phi) = - \frac{1}{2}(\frac{y^2}{\phi}+\log(2\pi \sigma^2))\)</span></li>
</ul>
<p><strong>Properties of GLM exponential families</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(E(Y) = b' (\theta)\)</span> where <span class="math inline">\(b'(\theta) = \frac{\partial b(\theta)}{\partial \theta}\)</span> (here <code>'</code> is “prime”, not transpose)</p></li>
<li>
<p><span class="math inline">\(var(Y) = a(\phi)b''(\theta)= a(\phi)V(\mu)\)</span>.</p>
<ul>
<li>
<span class="math inline">\(V(\mu)\)</span> is the <em>variance function</em>; however, it is only the variance in the case that <span class="math inline">\(a(\phi) =1\)</span>
</li>
</ul>
</li>
<li><p>If <span class="math inline">\(a(), b(), c()\)</span> are identifiable, we will derive expected value and variance of Y.</p></li>
</ol>
<p>Example</p>
<p>Normal distribution</p>
<p><span class="math display">\[
\begin{aligned}
b'(\theta) &amp;= \frac{\partial b(\mu^2/2)}{\partial \mu} = \mu \\
V(\mu) &amp;= \frac{\partial^2 (\mu^2/2)}{\partial \mu^2} = 1 \\
\to var(Y) &amp;= a(\phi) = \sigma^2
\end{aligned}
\]</span></p>
<p>Poisson distribution</p>
<p><span class="math display">\[
\begin{aligned}
f(y, \theta, \phi) &amp;= \frac{\mu^y \exp(-\mu)}{y!} \\
&amp;= \exp(y\log(\mu) - \mu - \log(y!)) \\
&amp;= \exp(y\theta - \exp(\theta) - \log(y!))
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\theta = \log(\mu)\)</span></li>
<li><span class="math inline">\(a(\phi) = 1\)</span></li>
<li><span class="math inline">\(b(\theta) = \exp(\theta)\)</span></li>
<li><span class="math inline">\(c(y, \phi) = \log(y!)\)</span></li>
</ul>
<p>Hence,</p>
<p><span class="math display">\[
\begin{aligned}
E(Y) = \frac{\partial b(\theta)}{\partial \theta} = \exp(\theta) &amp;= \mu \\
var(Y) = \frac{\partial^2 b(\theta)}{\partial \theta^2} &amp;= \mu
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\mu = E(Y) = b'(\theta)\)</span></p>
<p>In GLM, we take some monotone function (typically nonlinear) of <span class="math inline">\(\mu\)</span> to be linear in the set of covariates</p>
<p><span class="math display">\[
g(\mu) = g(b'(\theta)) = \mathbf{x'\beta}
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\mu = g^{-1}(\mathbf{x'\beta})
\]</span></p>
<p>where <span class="math inline">\(g(.)\)</span> is the <strong>link function</strong> since it links mean response (<span class="math inline">\(\mu = E(Y)\)</span>) and a linear expression of the covariates</p>
<p>Some people use <span class="math inline">\(\eta = \mathbf{x'\beta}\)</span> where <span class="math inline">\(\eta\)</span> = the “linear predictor”</p>
<p><strong>GLM is composed of 2 components</strong></p>
<p>The <strong>random component</strong>:</p>
<ul>
<li><p>is the distribution chosen to model the response variables <span class="math inline">\(Y_1,...,Y_n\)</span></p></li>
<li><p>is specified by the choice fo <span class="math inline">\(a(), b(), c()\)</span> in the exponential form</p></li>
<li>
<p>Notation:</p>
<ul>
<li>Assume that there are n <strong>independent</strong> response variables <span class="math inline">\(Y_1,...,Y_n\)</span> with densities<br><span class="math display">\[
f(y_i ; \theta_i, \phi) = \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
\]</span> notice each observation might have different densities</li>
<li>Assume that <span class="math inline">\(\phi\)</span> is constant for all <span class="math inline">\(i = 1,...,n\)</span>, but <span class="math inline">\(\theta_i\)</span> will vary. <span class="math inline">\(\mu_i = E(Y_i)\)</span> for all i.</li>
</ul>
</li>
</ul>
<p>The <strong>systematic component</strong></p>
<ul>
<li><p>is the portion of the model that gives the relation between <span class="math inline">\(\mu\)</span> and the covariates <span class="math inline">\(\mathbf{x}\)</span></p></li>
<li>
<p>consists of 2 parts:</p>
<ul>
<li>the <em>link</em> function, <span class="math inline">\(g(.)\)</span>
</li>
<li>the <em>linear predictor</em>, <span class="math inline">\(\eta = \mathbf{x'\beta}\)</span>
</li>
</ul>
</li>
<li>
<p>Notation:</p>
<ul>
<li>assume <span class="math inline">\(g(\mu_i) = \mathbf{x'\beta} = \eta_i\)</span> where <span class="math inline">\(\mathbf{\beta} = (\beta_1,..., \beta_p)'\)</span>
</li>
<li>The parameters to be estimated are <span class="math inline">\(\beta_1,...\beta_p , \phi\)</span>
</li>
</ul>
</li>
</ul>
<p><strong>The Canonical Link</strong></p>
<p>To choose <span class="math inline">\(g(.)\)</span>, we can use <strong>canonical link function</strong> (Remember: Canonical link is just a special case of the link function)</p>
<p>If the link function <span class="math inline">\(g(.)\)</span> is such <span class="math inline">\(g(\mu_i) = \eta_i = \theta_i\)</span>, the natural parameter, then <span class="math inline">\(g(.)\)</span> is the canonical link.</p>
<div class="inline-figure"><img src="images/GLM.PNG" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>
<span class="math inline">\(b(\theta)\)</span> = cumulant moment generating function</li>
<li>
<span class="math inline">\(g(\mu)\)</span> is the link function, which relates the linear predictor to the mean and is required to be monotone increasing, continuously differentiable and invertible.</li>
</ul>
<p>Equivalently, we can think of canonical link function as</p>
<p><span class="math display">\[
\gamma^{-1} \circ g^{-1} = I
\]</span> which is the identity. Hence,</p>
<p><span class="math display">\[
\theta = \eta
\]</span></p>
<p><strong>The inverse link</strong></p>
<p><span class="math inline">\(g^{-1}(.)\)</span> is also known as the mean function, take linear predictor output (ranging from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>) and transform it into a different scale.</p>
<ul>
<li><p><strong>Exponential</strong>: converts <span class="math inline">\(\mathbf{\beta X}\)</span> into a curve that is restricted between 0 and <span class="math inline">\(\infty\)</span> (which you can see that is useful in case you want to convert a linear predictor into a non-negative value). <span class="math inline">\(\lambda = \exp(y) = \mathbf{\beta X}\)</span></p></li>
<li>
<p><strong>Inverse Logit</strong> (also known as logistic): converts <span class="math inline">\(\mathbf{\beta X}\)</span> into a curve that is restricted between 0 and 1, which is useful in case you want to convert a linear predictor to a probability. <span class="math inline">\(\theta = \frac{1}{1 + \exp(-y)} = \frac{1}{1 + \exp(- \mathbf{\beta X})}\)</span></p>
<ul>
<li>
<span class="math inline">\(y\)</span> = linear predictor value</li>
<li>
<span class="math inline">\(\theta\)</span> = transformed value</li>
</ul>
</li>
</ul>
<p>The <strong>identity link</strong> is that</p>
<p><span class="math display">\[
\begin{aligned}
\eta_i &amp;= g(\mu_i) = \mu_i \\
\mu_i &amp;= g^{-1}(\eta_i) = \eta_i
\end{aligned}
\]</span></p>
<div class="inline-figure"><img src="images/2-Table15.1-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Table 15.1 Generalized Linear Models 15.1 the Structure of Generalized Linear Models</p>
<p>More example on the link functions and their inverses can be found on <a href="https://www.sagepub.com/sites/default/files/upm-binaries/21121_Chapter_15.pdf">page 380</a></p>
<p>Example</p>
<p>Normal random component</p>
<ul>
<li><p>Mean Response: <span class="math inline">\(\mu_i = \theta_i\)</span></p></li>
<li><p>Canonical Link: <span class="math inline">\(g( \mu_i) = \mu_i\)</span> (the identity link)</p></li>
</ul>
<p>Binomial random component</p>
<ul>
<li><p>Mean Response: <span class="math inline">\(\mu_i = \frac{n_i \exp( \theta)}{1+\exp (\theta_i)}\)</span> and <span class="math inline">\(\theta(\mu_i) = \log(\frac{p_i }{1-p_i}) = \log (\frac{\mu_i} {n_i - \mu_i})\)</span></p></li>
<li><p>Canonical link: <span class="math inline">\(g(\mu_i) = \log(\frac{\mu_i} {n_i - \mu_i})\)</span> (logit link)</p></li>
</ul>
<p>Poisson random component</p>
<ul>
<li><p>Mean Response: <span class="math inline">\(\mu_i = \exp(\theta_i)\)</span></p></li>
<li><p>Canonical Link: <span class="math inline">\(g(\mu_i) = \log(\mu_i)\)</span></p></li>
</ul>
<p>Gamma random component:</p>
<ul>
<li><p>Mean response: <span class="math inline">\(\mu_i = -\frac{1}{\theta_i}\)</span> and <span class="math inline">\(\theta(\mu_i) = - \mu_i^{-1}\)</span></p></li>
<li><p>Canonical Link: <span class="math inline">\(g(\mu\_i) = - \frac{1}{\mu_i}\)</span></p></li>
</ul>
<p>Inverse Gaussian random</p>
<ul>
<li>Canonical Link: <span class="math inline">\(g(\mu_i) = \frac{1}{\mu_i^2}\)</span>
</li>
</ul>
<div id="estimation-1" class="section level3" number="7.7.1">
<h3>
<span class="header-section-number">7.7.1</span> Estimation<a class="anchor" aria-label="anchor" href="#estimation-1"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>MLE for parameters of the <strong>systematic component (</strong><span class="math inline">\(\beta\)</span>)</li>
<li>Unification of derivation and computation (thanks to the exponential forms)</li>
<li>No unification for estimation of the dispersion parameter (<span class="math inline">\(\phi\)</span>)</li>
</ul>
<div id="estimation-of-beta" class="section level4" number="7.7.1.1">
<h4>
<span class="header-section-number">7.7.1.1</span> Estimation of <span class="math inline">\(\beta\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-beta"><i class="fas fa-link"></i></a>
</h4>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
f(y_i ; \theta_i, \phi) &amp;= \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi)) \\
E(Y_i) &amp;= \mu_i = b'(\theta) \\
var(Y_i) &amp;= b''(\theta)a(\phi) = V(\mu_i)a(\phi) \\
g(\mu_i) &amp;= \mathbf{x}_i'\beta = \eta_i
\end{aligned}
\]</span></p>
<p>If the log-likelihood for a single observation is <span class="math inline">\(l_i (\beta,\phi)\)</span>. The log-likelihood for all n observations is</p>
<p><span class="math display">\[
\begin{aligned}
l(\beta,\phi) &amp;= \sum_{i=1}^n l_i (\beta,\phi) \\
&amp;= \sum_{i=1}^n (\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
\end{aligned}
\]</span></p>
<p>Using MLE to find <span class="math inline">\(\beta\)</span>, we use the chain rule to get the derivatives</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} &amp;=  \frac{\partial l_i (\beta, \phi)}{\partial \theta_i} \times \frac{\partial \theta_i}{\partial \mu_i} \times \frac{\partial \mu_i}{\partial \eta_i}\times \frac{\partial \eta_i}{\partial \beta_j} \\
&amp;= \sum_{i=1}^{n}(\frac{ y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)} \times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij})
\end{aligned}
\]</span></p>
<p>If we let</p>
<p><span class="math display">\[
w_i \equiv ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1}
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} = \sum_{i=1}^n (\frac{y_i \mu_i}{a(\phi)} \times w_i \times \frac{\partial \eta_i}{\partial \mu_i} \times x_{ij})
\]</span></p>
<p>We can also get the second derivatives using the chain rule.</p>
<p>Example:</p>
<p>For the <span class="math display">\[Newton-Raphson\]</span> algorithm, we need</p>
<p><span class="math display">\[
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k})
\]</span></p>
<p>where <span class="math inline">\((j,k)\)</span>-th element of the <strong>Fisher information matrix</strong> <span class="math inline">\(\mathbf{I}(\beta)\)</span></p>
<p>Hence,</p>
<p><span class="math display">\[
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}) = \sum_{i=1}^n \frac{w_i}{a(\phi)}x_{ij}x_{ik}
\]</span></p>
<p>for the (j,k)th element</p>
<p>If Bernoulli model with logit link function (which is the canonical link)</p>
<p><span class="math display">\[
\begin{aligned}
b(\theta) &amp;= \log(1 + \exp(\theta)) = \log(1 + \exp(\mathbf{x'\beta})) \\
a(\phi) &amp;= 1  \\
c(y_i, \phi) &amp;= 0 \\
E(Y) = b'(\theta) &amp;= \frac{\exp(\theta)}{1 + \exp(\theta)} = \mu = p \\
\eta = g(\mu) &amp;= \log(\frac{\mu}{1-\mu}) = \theta = \log(\frac{p}{1-p}) = \mathbf{x'\beta}
\end{aligned}
\]</span></p>
<p>For <span class="math inline">\(Y_i\)</span>, i = 1,.., the log-likelihood is</p>
<p><span class="math display">\[
l_i (\beta, \phi) = \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) = y_i \mathbf{x}'_i \beta - \log(1+ \exp(\mathbf{x'\beta}))
\]</span></p>
<p>Additionally,</p>
<p><span class="math display">\[
\begin{aligned}
V(\mu_i) &amp;= \mu_i(1-\mu_i)= p_i (1-p_i) \\
\frac{\partial \mu_i}{\partial \eta_i} &amp;= p_i(1-p_i)
\end{aligned}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l(\beta, \phi)}{\partial \beta_j} &amp;= \sum_{i=1}^n[\frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)}\times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij}] \\
&amp;= \sum_{i=1}^n (y_i - p_i) \times \frac{1}{p_i(1-p_i)} \times p_i(1-p_i) \times x_{ij} \\
&amp;= \sum_{i=1}^n (y_i - p_i) x_{ij} \\
&amp;= \sum_{i=1}^n (y_i - \frac{\exp(\mathbf{x'_i\beta})}{1+ \exp(\mathbf{x'_i\beta})})x_{ij}
\end{aligned}
\]</span></p>
<p>then</p>
<p><span class="math display">\[
w_i = ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1} = p_i (1-p_i)
\]</span></p>
<p><span class="math display">\[
\mathbf{I}_{jk}(\mathbf{\beta}) = \sum_{i=1}^n \frac{w_i}{a(\phi)} x_{ij}x_{ik} = \sum_{i=1}^n p_i (1-p_i)x_{ij}x_{ik}
\]</span></p>
<p>The <strong>Fisher-scoring</strong> algorithm for the MLE of <span class="math inline">\(\mathbf{\beta}\)</span> is</p>
<p><span class="math display">\[
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m+1)}
=
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m)} +
\mathbf{I}^{-1}(\mathbf{\beta})
\left(
\begin{array}
{c}
\frac{\partial l (\beta, \phi)}{\partial \beta_1} \\
\frac{\partial l (\beta, \phi)}{\partial \beta_2} \\
. \\
. \\
. \\
\frac{\partial l (\beta, \phi)}{\partial \beta_p} \\
\end{array}
\right)|_{\beta = \beta^{(m)}}
\]</span></p>
<p>Similar to <span class="math display">\[Newton-Raphson\]</span> expect the matrix of second derivatives by the expected value of the second derivative matrix.</p>
<p>In matrix notation,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l }{\partial \beta} &amp;= \frac{1}{a(\phi)}\mathbf{X'W\Delta(y - \mu)} \\
&amp;= \frac{1}{a(\phi)}\mathbf{F'V^{-1}(y - \mu)} \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\mathbf{I}(\beta) = \frac{1}{a(\phi)}\mathbf{X'WX} = \frac{1}{a(\phi)}\mathbf{F'V^{-1}F}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix of covariates</li>
<li>
<span class="math inline">\(\mathbf{W}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(w_i\)</span>
</li>
<li>
<span class="math inline">\(\mathbf{\Delta}\)</span> an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(\frac{\partial \eta_i}{\partial \mu_i}\)</span>
</li>
<li>
<span class="math inline">\(\mathbf{F} = \mathbf{\frac{\partial \mu}{\partial \beta}}\)</span> an <span class="math inline">\(n \times p\)</span> matrix with <span class="math inline">\(i\)</span>-th row <span class="math inline">\(\frac{\partial \mu_i}{\partial \beta} = (\frac{\partial \mu_i}{\partial \eta_i})\mathbf{x}'_i\)</span>
</li>
<li>
<span class="math inline">\(\mathbf{V}\)</span> an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(V(\mu_i)\)</span>
</li>
</ul>
<p>Setting the derivative of the log-likelihood equal to 0, ML estimating equations are</p>
<p><span class="math display">\[
\mathbf{F'V^{-1}y= F'V^{-1}\mu}
\]</span></p>
<p>where all components of this equation expect y depends on the parameters <span class="math inline">\(\beta\)</span></p>
<p><strong>Special Cases</strong></p>
<p>If one has a canonical link, the estimating equations reduce to</p>
<p><span class="math display">\[
\mathbf{X'y= X'\mu}
\]</span></p>
<p>If one has an identity link, then</p>
<p><span class="math display">\[
\mathbf{X'V^{-1}y = X'V^{-1}X\hat{\beta}}
\]</span></p>
<p>which gives the generalized least squares estimator</p>
<p>Generally, we can rewrite the Fisher-scoring algorithm as</p>
<p><span class="math display">\[
\beta^{(m+1)} = \beta^{(m)} + \mathbf{(\hat{F}'\hat{V}^{-1}\hat{F})^{-1}\hat{F}'\hat{V}^{-1}(y- \hat{\mu})}
\]</span></p>
<p>Since <span class="math inline">\(\hat{F},\hat{V}, \hat{\mu}\)</span> depend on <span class="math inline">\(\beta\)</span>, we evaluate at <span class="math inline">\(\beta^{(m)}\)</span></p>
<p>From starting values <span class="math inline">\(\beta^{(0)}\)</span>, we can iterate until convergence.</p>
<p>Notes:</p>
<ul>
<li>if <span class="math inline">\(a(\phi)\)</span> is a constant or of the form <span class="math inline">\(m_i \phi\)</span> with known <span class="math inline">\(m_i\)</span>, then <span class="math inline">\(\phi\)</span> cancels.</li>
</ul>
</div>
<div id="estimation-of-phi" class="section level4" number="7.7.1.2">
<h4>
<span class="header-section-number">7.7.1.2</span> Estimation of <span class="math inline">\(\phi\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-phi"><i class="fas fa-link"></i></a>
</h4>
<p>2 approaches:</p>
<ol style="list-style-type: decimal">
<li>MLE</li>
</ol>
<p><span class="math display">\[
\frac{\partial l_i}{\partial \phi} = \frac{(\theta_i y_i - b(\theta_i)a'(\phi))}{a^2(\phi)} + \frac{\partial c(y_i,\phi)}{\partial \phi}
\]</span></p>
<p>the MLE of <span class="math inline">\(\phi\)</span> solves</p>
<p><span class="math display">\[
\frac{a^2(\phi)}{a'(\phi)}\sum_{i=1}^n \frac{\partial c(y_i, \phi)}{\partial \phi} = \sum_{i=1}^n(\theta_i y_i - b(\theta_i))
\]</span></p>
<ul>
<li><p>Situation others than normal error case, expression for <span class="math inline">\(\frac{\partial c(y,\phi)}{\partial \phi}\)</span> are not simple</p></li>
<li><p>Even for the canonical link and <span class="math inline">\(a(\phi)\)</span> constant, there is no nice general expression for <span class="math inline">\(-E(\frac{\partial^2 l}{\partial \phi^2})\)</span>, so the unification GLMs provide for estimation of <span class="math inline">\(\beta\)</span> breaks down for <span class="math inline">\(\phi\)</span></p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<p>Moment Estimation (“Bias Corrected <span class="math inline">\(\chi^2\)</span>”)</p>
<ul>
<li>The MLE is not conventional approach to estimation of <span class="math inline">\(\phi\)</span> in GLMS.</li>
<li>For the exponential family <span class="math inline">\(var(Y) =V(\mu)a(\phi)\)</span>. This implies<br><span class="math display">\[
\begin{aligned}
a(\phi) &amp;= \frac{var(Y)}{V(\mu)} = \frac{E(Y- \mu)^2}{V(\mu)} \\
a(\hat{\phi})  &amp;= \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i -\hat{\mu}_i)^2}{V(\hat{\mu})}
\end{aligned}
\]</span> where <span class="math inline">\(p\)</span> is the dimension of <span class="math inline">\(\beta\)</span>
</li>
<li>GLM with canonical link function <span class="math inline">\(g(.)= (b'(.))^{-1}\)</span><br><span class="math display">\[
\begin{aligned}
g(\mu) &amp;= \theta = \eta = \mathbf{x'\beta} \\
\mu &amp;= g^{-1}(\eta)= b'(\eta)
\end{aligned}
\]</span>
</li>
<li>so the method estimator for <span class="math inline">\(a(\phi)=\phi\)</span> is</li>
</ul>
</li>
</ol>
<p><span class="math display">\[
\hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i - g^{-1}(\hat{\eta}_i))^2}{V(g^{-1}(\hat{\eta}_i))}
\]</span></p>
</div>
</div>
<div id="inference-2" class="section level3" number="7.7.2">
<h3>
<span class="header-section-number">7.7.2</span> Inference<a class="anchor" aria-label="anchor" href="#inference-2"><i class="fas fa-link"></i></a>
</h3>
<p>We have</p>
<p><span class="math display">\[
\hat{var}(\beta) = a(\phi)(\mathbf{\hat{F}'\hat{V}\hat{F}})^{-1}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mathbf{V}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with diagonal elements given by <span class="math inline">\(V(\mu_i)\)</span>
</li>
<li>
<span class="math inline">\(\mathbf{F}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix given by <span class="math inline">\(\mathbf{F} = \frac{\partial \mu}{\partial \beta}\)</span>
</li>
<li>Both <span class="math inline">\(\mathbf{V,F}\)</span> are dependent on the mean <span class="math inline">\(\mu\)</span>, and thus <span class="math inline">\(\beta\)</span>. Hence, their estimates (<span class="math inline">\(\mathbf{\hat{V},\hat{F}}\)</span>) depend on <span class="math inline">\(\hat{\beta}\)</span>.</li>
</ul>
<p><span class="math display">\[
H_0: \mathbf{L\beta = d}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{L}\)</span> is a q x p matrix with a <strong>Wald</strong> test</p>
<p><span class="math display">\[
W = \mathbf{(L \hat{\beta}-d)'(a(\phi)L(\hat{F}'\hat{V}^{-1}\hat{F})L')^{-1}(L \hat{\beta}-d)}
\]</span></p>
<p>which follows <span class="math inline">\(\chi_q^2\)</span> distribution (asymptotically), where <span class="math inline">\(q\)</span> is the rank of <span class="math inline">\(\mathbf{L}\)</span></p>
<p>In the simple case <span class="math inline">\(H_0: \beta_j = 0\)</span> gives <span class="math inline">\(W = \frac{\hat{\beta}^2_j}{\hat{var}(\hat{\beta}_j)} \sim \chi^2_1\)</span> asymptotically</p>
<p>Likelihood ratio test</p>
<p><span class="math display">\[
\Lambda = 2 (l(\hat{\beta}_f)-l(\hat{\beta}_r)) \sim \chi^2_q
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(q\)</span> is the number of constraints used to fit the reduced model <span class="math inline">\(\hat{\beta}_r\)</span>, and <span class="math inline">\(\hat{\beta}_r\)</span> is the fit under the full model.</li>
</ul>
<p>Wald test is easier to implement, but likelihood ratio test is better (especially for small samples).</p>
</div>
<div id="deviance" class="section level3" number="7.7.3">
<h3>
<span class="header-section-number">7.7.3</span> Deviance<a class="anchor" aria-label="anchor" href="#deviance"><i class="fas fa-link"></i></a>
</h3>
<p><a href="generalized-linear-models.html#deviance">Deviance</a> is necessary for goodness of fit, inference and for alternative estimation of the dispersion parameter. We define and consider <a href="generalized-linear-models.html#deviance">Deviance</a> from a likelihood ratio perspective.</p>
<ul>
<li><p>Assume that <span class="math inline">\(\phi\)</span> is known. Let <span class="math inline">\(\tilde{\theta}\)</span> denote the full and <span class="math inline">\(\hat{\theta}\)</span> denote the reduced model MLEs. Then, the likelihood ratio (2 times the difference in log-likelihoods) is <span class="math display">\[
2\sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i- \hat{\theta}_i)-b(\tilde{\theta}_i) + b(\hat{\theta}_i)}{a_i(\phi)}
\]</span></p></li>
<li><p>For exponential families, <span class="math inline">\(\mu = E(y) = b'(\theta)\)</span>, so the natural parameter is a function of <span class="math inline">\(\mu: \theta = \theta(\mu) = b'^{-1}(\mu)\)</span>, and the likelihood ratio turns into<br><span class="math display">\[
2 \sum_{i=1}^m \frac{y_i\{\theta(\tilde{\mu}_i - \theta(\hat{\mu}_i)\} - b(\theta(\tilde{\mu}_i)) + b(\theta(\hat{\mu}_i))}{a_i(\phi)}
\]</span></p></li>
<li><p>Comparing a fitted model to “the fullest possible model”, which is the <strong>saturated model</strong>: <span class="math inline">\(\tilde{\mu}_i = y_i\)</span>, i = 1,..,n. If <span class="math inline">\(\tilde{\theta}_i^* = \theta(y_i), \hat{\theta}_i^* = \theta (\hat{\mu})\)</span>, the likelihood ratio is<br><span class="math display">\[
2 \sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i^* - \hat{\theta}_i^* + b(\hat{\theta}_i^*))}{a_i(\phi)}
\]</span></p></li>
<li><p><span class="citation">(<a href="references.html#ref-McCullagh_2019">McCullagh 2019</a>)</span> specify <span class="math inline">\(a(\phi) = \phi\)</span>, then the likelihood ratio can be written as<br><span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = \frac{2}{\phi}\sum_{i=1}^n\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*)- b(\tilde{\theta}_i^*) +b(\hat{\theta}_i^*)  \}  
\]</span> where</p></li>
<li><p><span class="math inline">\(D^*(\mathbf{y, \hat{\mu}})\)</span> = <strong>scaled deviance</strong></p></li>
<li><p><span class="math inline">\(D(\mathbf{y, \hat{\mu}}) = \phi D^*(\mathbf{y, \hat{\mu}})\)</span> = <strong>deviance</strong></p></li>
</ul>
<p><strong>Note</strong>:</p>
<ul>
<li>
<p>in some random component distributions, we can write <span class="math inline">\(a_i(\phi) = \phi m_i\)</span>, where</p>
<ul>
<li>
<span class="math inline">\(m_i\)</span> is some known scalar that may change with the observations. Then, the scaled deviance components are divided by <span class="math inline">\(m_i\)</span>:<br><span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) \equiv 2\sum_{i=1}^n\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*)- b(\tilde{\theta}_i^*) +b(\hat{\theta}_i^*)\} / (\phi m_i)  
\]</span>
</li>
</ul>
</li>
<li><p><span class="math inline">\(D^*(\mathbf{y, \hat{\mu}}) = \sum_{i=1}^n d_i\)</span>m where <span class="math inline">\(d_i\)</span> is the deviance contribution from the <span class="math inline">\(i\)</span>-th observation.</p></li>
<li><p><span class="math inline">\(D\)</span> is used in model selection</p></li>
<li><p><span class="math inline">\(D^*\)</span> is used in goodness of fit tests (as it is a likelihood ratio statistic). <span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = 2\{l(\mathbf{y,\tilde{\mu}})-l(\mathbf{y,\hat{\mu}})\}
\]</span></p></li>
<li><p><span class="math inline">\(d_i\)</span> are used to form <strong>deviance residuals</strong></p></li>
</ul>
<p><strong>Normal</strong></p>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
\theta &amp;= \mu \\
\phi &amp;= \sigma^2 \\
b(\theta) &amp;= \frac{1}{2} \theta^2 \\
a(\phi) &amp;= \phi
\end{aligned}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{\theta}_i &amp;= y_i \\
\hat{\theta}_i &amp;= \hat{\mu}_i = g^{-1}(\hat{\eta}_i)
\end{aligned}
\]</span></p>
<p>And</p>
<p><span class="math display">\[
\begin{aligned}
D &amp;= 2 \sum_{1=1}^n Y^2_i - y_i \hat{\mu}_i - \frac{1}{2}y^2_i + \frac{1}{2} \hat{\mu}_i^2 \\
&amp;= \sum_{i=1}^n y_i^2 - 2y_i \hat{\mu}_i + \hat{\mu}_i^2 \\
&amp;= \sum_{i=1}^n (y_i - \hat{\mu}_i)^2
\end{aligned}
\]</span></p>
<p>which is the <strong>residual sum of squares</strong></p>
<p><strong>Poisson</strong></p>
<p><span class="math display">\[
\begin{aligned}
f(y) &amp;= \exp\{y\log(\mu) - \mu - \log(y!)\} \\
\theta &amp;= \log(\mu) \\
b(\theta) &amp;= \exp(\theta) \\
a(\phi) &amp;= 1 \\
\tilde{\theta}_i &amp;= \log(y_i) \\
\hat{\theta}_i &amp;= \log(\hat{\mu}_i) \\
\hat{\mu}_i &amp;= g^{-1}(\hat{\eta}_i)
\end{aligned}
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
\begin{aligned}
D &amp;= 2 \sum_{i = 1}^n y_i \log(y_i) - y_i \log(\hat{\mu}_i) - y_i + \hat{\mu}_i \\
&amp;= 2 \sum_{i = 1}^n y_i \log(\frac{y_i}{\hat{\mu}_i}) - (y_i - \hat{\mu}_i)
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
d_i = 2\{y_i \log(\frac{y_i}{\hat{\mu}})- (y_i - \hat{\mu}_i)\}
\]</span></p>
<div id="analysis-of-deviance" class="section level4" number="7.7.3.1">
<h4>
<span class="header-section-number">7.7.3.1</span> Analysis of Deviance<a class="anchor" aria-label="anchor" href="#analysis-of-deviance"><i class="fas fa-link"></i></a>
</h4>
<p>The difference in deviance between a reduced and full model, where q is the difference in the number of free parameters, has an asymptotic <span class="math inline">\(\chi^2_q\)</span>. The likelihood ratio test</p>
<p><span class="math display">\[
D^*(\mathbf{y;\hat{\mu}_r}) - D^*(\mathbf{y;\hat{\mu}_f}) = 2\{l(\mathbf{y;\hat{\mu}_f})-l(\mathbf{y;\hat{\mu}_r})\}
\]</span></p>
<p>this comparison of models is <strong>Analysis of Deviance</strong>. <a href="generalized-linear-models.html#generalized-linear-models">GLM</a> uses this analysis for model selection.</p>
<p>An estimation of <span class="math inline">\(\phi\)</span> is</p>
<p><span class="math display">\[
\hat{\phi} = \frac{D(\mathbf{y, \hat{\mu}})}{n - p}
\]</span></p>
<p>where <span class="math inline">\(p\)</span> = number of parameters fit.</p>
<p>Excessive use of <span class="math inline">\(\chi^2\)</span> test could be problematic since it is asymptotic <span class="citation">(<a href="references.html#ref-McCullagh_2019">McCullagh 2019</a>)</span></p>
</div>
<div id="deviance-residuals" class="section level4" number="7.7.3.2">
<h4>
<span class="header-section-number">7.7.3.2</span> Deviance Residuals<a class="anchor" aria-label="anchor" href="#deviance-residuals"><i class="fas fa-link"></i></a>
</h4>
<p>We have <span class="math inline">\(D = \sum_{i=1}^{n}d_i\)</span>. Then, we define <strong>deviance residuals</strong></p>
<p><span class="math display">\[
r_{D_i} = \text{sign}(y_i -\hat{\mu}_i)\sqrt{d_i}
\]</span></p>
<p>Standardized version of deviance residuals is</p>
<p><span class="math display">\[
r_{s,i} = \frac{y_i -\hat{\mu}}{\hat{\sigma}(1-h_{ii})^{1/2}}
\]</span></p>
<p>Let <span class="math inline">\(\mathbf{H^{GLM} = W^{1/2}X(X'WX)^{-1}X'W^{-1/2}}\)</span>, where <span class="math inline">\(\mathbf{W}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix with <span class="math inline">\((i,i)\)</span>-th element given by <span class="math inline">\(w_i\)</span> (see <a href="generalized-linear-models.html#estimation-of-beta">Estimation of <span class="math inline">\(\beta\)</span></a>). Then Standardized deviance residuals is equivalently</p>
<p><span class="math display">\[
r_{s, D_i} = \frac{r_{D_i}}{\{\hat{\phi}(1-h_{ii}^{glm}\}^{1/2}}
\]</span></p>
<p>where <span class="math inline">\(h_{ii}^{glm}\)</span> is the <span class="math inline">\(i\)</span>-th diagonal of <span class="math inline">\(\mathbf{H}^{GLM}\)</span></p>
</div>
<div id="pearson-chi-square-residuals" class="section level4" number="7.7.3.3">
<h4>
<span class="header-section-number">7.7.3.3</span> Pearson Chi-square Residuals<a class="anchor" aria-label="anchor" href="#pearson-chi-square-residuals"><i class="fas fa-link"></i></a>
</h4>
<p>Another <span class="math inline">\(\chi^2\)</span> statistic is <strong>Pearson</strong> <span class="math inline">\(\chi^2\)</span> statistics: (assume <span class="math inline">\(m_i = 1\)</span>)</p>
<p><span class="math display">\[
X^2 = \sum_{i=1}^{n} \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)}
\]</span></p>
<p>where <span class="math inline">\(\hat{\mu}_i\)</span> is the fitted mean response fo the model of interest.</p>
<p>The <strong>Scaled Pearson</strong> <span class="math inline">\(\chi^2\)</span> statistic is given by <span class="math inline">\(\frac{X^2}{\phi} \sim \chi^2_{n-p}\)</span> where p is the number of parameters estimated. Hence, the <strong>Pearson</strong> <span class="math inline">\(\chi^2\)</span> residuals are</p>
<p><span class="math display">\[
X^2_i = \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)}
\]</span></p>
<p>If we have the following assumptions:</p>
<ul>
<li>Independent samples<br>
</li>
<li>No over-dispersion: If <span class="math inline">\(\phi = 1\)</span>, <span class="math inline">\(\frac{D(\mathbf{y;\hat{\mu}})}{n-p}\)</span> and <span class="math inline">\(\frac{X^2}{n-p}\)</span> have a value substantially larger 1 indicates <strong>improperly specified model</strong> or <strong>overdispersion</strong><br>
</li>
<li>Multiple groups</li>
</ul>
<p>then <span class="math inline">\(\frac{X^2}{\phi}\)</span> and <span class="math inline">\(D^*(\mathbf{y; \hat{\mu}})\)</span> both follow <span class="math inline">\(\chi^2_{n-p}\)</span></p>
</div>
</div>
<div id="diagnostic-plots" class="section level3" number="7.7.4">
<h3>
<span class="header-section-number">7.7.4</span> Diagnostic Plots<a class="anchor" aria-label="anchor" href="#diagnostic-plots"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<p>Standardized residual Plots:</p>
<ul>
<li>plot(<span class="math inline">\(r_{s, D_i}\)</span>, <span class="math inline">\(\hat{\mu}_i\)</span>) or plot(<span class="math inline">\(r_{s, D_i}\)</span>, <span class="math inline">\(T(\hat{\mu}_i)\)</span>) where <span class="math inline">\(T(\hat{\mu}_i)\)</span> is transformation(<span class="math inline">\(\hat{\mu}_i\)</span>) called <strong>constant information scale</strong>:<br>
</li>
<li>plot(<span class="math inline">\(r_{s, D_i}\)</span>, <span class="math inline">\(\hat{\eta}_i\)</span>)</li>
</ul>
</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center">Random Component</th>
<th align="center"><span class="math inline">\(T(\hat{\mu}_i)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">Normal</td>
<td align="center"><span class="math inline">\(\hat{\mu}\)</span></td>
</tr>
<tr class="even">
<td align="center">Poisson</td>
<td align="center"><span class="math inline">\(2\sqrt{\mu}\)</span></td>
</tr>
<tr class="odd">
<td align="center">Binomial</td>
<td align="center"><span class="math inline">\(2 \sin^{-1}(\sqrt{\hat{\mu}})\)</span></td>
</tr>
<tr class="even">
<td align="center">Gamma</td>
<td align="center"><span class="math inline">\(2 \log(\hat{\mu})\)</span></td>
</tr>
<tr class="odd">
<td align="center">Inverse Gaussian</td>
<td align="center"><span class="math inline">\(-2\hat{\mu}^{-1/2}\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>
<p>If we see:</p>
<ul>
<li>Trend, it means we might have a wrong link function, or choice of scale<br>
</li>
<li>Systematic change in range of residuals with a change in <span class="math inline">\(T(\hat{\mu})\)</span> (incorrect random component) (systematic <span class="math inline">\(\neq\)</span> random)</li>
</ul>
</li>
<li><p>plot(<span class="math inline">\(|r_{D_i}|,\hat{\mu}_i\)</span>) to check <strong>Variance Function</strong>.</p></li>
</ul>
</div>
<div id="goodness-of-fit" class="section level3" number="7.7.5">
<h3>
<span class="header-section-number">7.7.5</span> Goodness of Fit<a class="anchor" aria-label="anchor" href="#goodness-of-fit"><i class="fas fa-link"></i></a>
</h3>
<p>To assess goodness of fit, we can use</p>
<ul>
<li><a href="generalized-linear-models.html#deviance">Deviance</a></li>
<li><a href="generalized-linear-models.html#pearson-chi-square-residuals">Pearson Chi-square Residuals</a></li>
</ul>
<p>In nested model, we could use likelihood-based information measures:</p>
<p><span class="math display">\[
\begin{aligned}
AIC &amp;= -2l(\mathbf{\hat{\mu}}) + 2p \\
AICC &amp;= -2l(\mathbf{\hat{\mu}}) + 2p(\frac{n}{n-p-1}) \\
BIC &amp;= 2l(\hat{\mu}) + p \log(n)
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(l(\hat{\mu})\)</span> is the log-likelihood evaluated at the parameter estimates</li>
<li>
<span class="math inline">\(p\)</span> is the number of parameters</li>
<li>
<span class="math inline">\(n\)</span> is the number of observations.</li>
</ul>
<p>Note: you have to use the same data with the same model (i.e., same link function, same random underlying random distribution). but you can have different number of parameters.</p>
<p>Even though statisticians try to come up with measures that are similar to <span class="math inline">\(R^2\)</span>, in practice, it is not so appropriate. For example, they compare the log-likelihood of the fitted model against the that of a model with just the intercept:</p>
<p><span class="math display">\[
R^2_p = 1 - \frac{l(\hat{\mu})}{l(\hat{\mu}_0)}
\]</span></p>
<p>For certain specific random components such as binary response model, we have rescaled generalized <span class="math inline">\(R^2\)</span></p>
<p><span class="math display">\[
\bar{R}^2 = \frac{R^2_*}{\max(R^2_*)} = \frac{1-\exp\{-\frac{2}{n}(l(\hat{\mu}) - l(\hat{\mu}_0) \}}{1 - \exp\{\frac{2}{n}l(\hat{\mu}_0)\}}
\]</span></p>
</div>
<div id="over-dispersion" class="section level3" number="7.7.6">
<h3>
<span class="header-section-number">7.7.6</span> Over-Dispersion<a class="anchor" aria-label="anchor" href="#over-dispersion"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="23%">
<col width="28%">
<col width="47%">
</colgroup>
<thead><tr class="header">
<th>Random Components</th>
<th><span class="math inline">\(var(Y)\)</span></th>
<th><span class="math inline">\(V(\mu)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Binomial</td>
<td><span class="math inline">\(var(Y) = n \mu (1- \mu)\)</span></td>
<td>
<span class="math inline">\(V(\mu) = \phi n \mu(1- \mu)\)</span> where <span class="math inline">\(m_i =n\)</span>
</td>
</tr>
<tr class="even">
<td>Poisson</td>
<td><span class="math inline">\(var(Y) = \mu\)</span></td>
<td><span class="math inline">\(V(\mu) = \phi \mu\)</span></td>
</tr>
</tbody>
</table></div>
<p>In both cases <span class="math inline">\(\phi = 1\)</span>. Recall <span class="math inline">\(b''(\theta)= V(\mu)\)</span> check <a href="generalized-linear-models.html#estimation-of-phi">Estimation of <span class="math inline">\(\phi\)</span></a>.</p>
<p>If we find</p>
<ul>
<li>
<span class="math inline">\(\phi &gt;1\)</span>: over-dispersion (i.e., too much variation for an independent binomial or Poisson distribution).</li>
<li>
<span class="math inline">\(\phi&lt;1\)</span>: under-dispersion (i.e., too little variation for an independent binomial or Poisson distribution).</li>
</ul>
<p>If we have either over or under-dispersion, it means we might have unspecified random component, we could</p>
<ul>
<li>Select a different random component distribution that can accommodate over or under-dispersion (e.g., negative binomial, Conway-Maxwell Poisson)</li>
<li>use <a href="nonlinear-and-generalized-linear-mixed-models.html#nonlinear-and-generalized-linear-mixed-models">Nonlinear and Generalized Linear Mixed Models</a> to handle random effects in generalized linear models.</li>
</ul>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="non-linear-regression.html"><span class="header-section-number">6</span> Non-linear Regression</a></div>
<div class="next"><a href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#generalized-linear-models"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li>
<a class="nav-link" href="#logistic-regression-1"><span class="header-section-number">7.1</span> Logistic Regression</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#application-2"><span class="header-section-number">7.1.1</span> Application</a></li></ul>
</li>
<li><a class="nav-link" href="#probit-regression"><span class="header-section-number">7.2</span> Probit Regression</a></li>
<li><a class="nav-link" href="#binomial-regression"><span class="header-section-number">7.3</span> Binomial Regression</a></li>
<li>
<a class="nav-link" href="#poisson-regression"><span class="header-section-number">7.4</span> Poisson Regression</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#application-3"><span class="header-section-number">7.4.1</span> Application</a></li></ul>
</li>
<li><a class="nav-link" href="#negative-binomial-regression"><span class="header-section-number">7.5</span> Negative Binomial Regression</a></li>
<li><a class="nav-link" href="#multinomial"><span class="header-section-number">7.6</span> Multinomial</a></li>
<li>
<a class="nav-link" href="#generalization"><span class="header-section-number">7.7</span> Generalization</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimation-1"><span class="header-section-number">7.7.1</span> Estimation</a></li>
<li><a class="nav-link" href="#inference-2"><span class="header-section-number">7.7.2</span> Inference</a></li>
<li><a class="nav-link" href="#deviance"><span class="header-section-number">7.7.3</span> Deviance</a></li>
<li><a class="nav-link" href="#diagnostic-plots"><span class="header-section-number">7.7.4</span> Diagnostic Plots</a></li>
<li><a class="nav-link" href="#goodness-of-fit"><span class="header-section-number">7.7.5</span> Goodness of Fit</a></li>
<li><a class="nav-link" href="#over-dispersion"><span class="header-section-number">7.7.6</span> Over-Dispersion</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/07-generalized-linear-models.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/07-generalized-linear-models.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2024-07-31.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
