<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 36 Endogeneity | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="In applied research, it’s often tempting to treat regression coefficients as if they represent causal relationships. A positive coefficient on advertising spend, for example, might be interpreted...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 36 Endogeneity | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/sec-endogeneity.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="In applied research, it’s often tempting to treat regression coefficients as if they represent causal relationships. A positive coefficient on advertising spend, for example, might be interpreted...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 36 Endogeneity | A Guide on Data Analysis">
<meta name="twitter:description" content="In applied research, it’s often tempting to treat regression coefficients as if they represent causal relationships. A positive coefficient on advertising spend, for example, might be interpreted...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="active" href="sec-endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="sec-directed-acyclic-graphs.html"><span class="header-section-number">38</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="sec-controls.html"><span class="header-section-number">39</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="sec-endogeneity" class="section level1" number="36">
<h1>
<span class="header-section-number">36</span> Endogeneity<a class="anchor" aria-label="anchor" href="#sec-endogeneity"><i class="fas fa-link"></i></a>
</h1>
<p>In applied research, it’s often tempting to treat regression coefficients as if they represent <strong>causal relationships</strong>. A positive coefficient on advertising spend, for example, might be interpreted as evidence that increasing ad budgets will increase sales. But such interpretations rely on a critical assumption: that the independent variables we include in a model are <strong>exogenous</strong>.</p>
<p>This chapter explores the central threat to this assumption: <a href="sec-endogeneity.html#sec-endogeneity">endogeneity</a>.</p>
<p>Endogeneity refers to any situation where an explanatory variable is correlated with the error term in a regression model. When this happens, <strong>our coefficient estimates are biased and inconsistent</strong>, and any causal claims are invalid.</p>
<hr>
<p>To understand where endogeneity comes from, let’s begin with the familiar linear regression model:</p>
<p><span class="math display">\[
\mathbf{Y = X \beta + \epsilon}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{Y}\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of observed outcomes,</li>
<li>
<span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times k\)</span> matrix of explanatory variables (including a column of ones for the intercept, if present),</li>
<li>
<span class="math inline">\(\beta\)</span> is a <span class="math inline">\(k \times 1\)</span> vector of unknown parameters,</li>
<li>
<span class="math inline">\(\epsilon\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of unobserved error terms.</li>
</ul>
<p>The <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> estimator is:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta}_{OLS} &amp;= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{Y}) \\
&amp;= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'(\mathbf{X\beta + \epsilon})) \\
&amp;= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})\beta + (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\epsilon) \\
&amp;= \beta + (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\epsilon)
\end{aligned}
\]</span></p>
<p>This derivation makes it clear: OLS is <strong>unbiased</strong> only if the second term vanishes in expectation. That is:</p>
<p><span class="math display">\[
E[(\mathbf{X}'\mathbf{\epsilon})] = 0 \quad \text{or equivalently,} \quad Cov(\mathbf{X}, \epsilon) = 0
\]</span></p>
<p>To produce valid estimates, OLS requires two conditions:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Zero Conditional Mean</strong>:<br><span class="math display">\[
E[\epsilon \mid \mathbf{X}] = 0
\]</span> This implies that once we condition on the regressors, there is no systematic error left.</p></li>
<li><p><strong>No Correlation Between Regressors and Errors</strong>:<br><span class="math display">\[
Cov(\mathbf{X}, \epsilon) = 0
\]</span> This is a stronger requirement. If it fails, we have an endogeneity problem.</p></li>
</ol>
<p>The first condition is often satisfied by including an intercept or accounting for the distributional properties of errors. The second condition—lack of correlation between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\epsilon\)</span>—is much harder to satisfy, especially in observational data.</p>
<p>Endogeneity violates one of the core assumptions of regression, and it has serious consequences:</p>
<ul>
<li>
<strong>Coefficient bias</strong>: Estimates systematically differ from the true parameter values.</li>
<li>
<strong>Inconsistency</strong>: The bias does not vanish as the sample size increases.</li>
<li>
<strong>Incorrect inference</strong>: Hypothesis tests and confidence intervals become unreliable.</li>
<li>
<strong>Misleading decisions</strong>: In business and policy settings, this can lead to costly errors.</li>
</ul>
<hr>
<p>There are several common sources of endogeneity <span class="citation">(<a href="references.html#ref-hill2021endogeneity">Hill et al. 2021</a>)</span>. However, most problems fall into two broad categories:</p>
<ol style="list-style-type: decimal">
<li><a href="sec-endogeneity.html#sec-endogenous-treatment">Endogenous Treatment</a></li>
</ol>
<!-- --><ol style="list-style-type: lower-alpha">
<li><a href="sec-endogeneity.html#sec-omitted-variable-bias"><strong>Omitted Variable Bias (OVB)</strong></a></li>
</ol>
<p>This occurs when a relevant variable is:</p>
<ul>
<li>Left out of the model (i.e., omitted),</li>
<li>Correlated with both the explanatory variable(s) and the outcome.</li>
</ul>
<p>When is OVB a problem?</p>
<ul>
<li>The omitted variable is correlated with an included regressor.</li>
<li>It also affects the dependent variable.</li>
</ul>
<p>If either condition fails, there’s no bias.</p>
<p>Example (Economics): We want to estimate the effect of school on earnings, but typical unobservables (e.g., motivation, ability/talent, or self-selection) pose a threat to our identification strategy.</p>
<p>Example (Marketing): Suppose we regress sales on advertising spend, but omit product quality. If higher-quality products get more advertising and also generate more sales, the ad spend coefficient picks up some of the effect of quality—resulting in an upward bias.</p>
<p>Example (Finance): Regressing firm performance on executive compensation might omit executive ability. If more able executives both command higher compensation and deliver better results, OVB leads to biased inferences.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li><a href="sec-endogeneity.html#sec-simultaneity"><strong>Simultaneity (Feedback Effects)</strong></a></li>
</ol>
<p>Simultaneity arises when the dependent variable and an explanatory variable are determined <strong>jointly</strong>, in equilibrium.</p>
<p>Example (Economics): Price and quantity demanded are determined together in supply-and-demand models. A regression of quantity on price without modeling supply will yield a biased estimate of demand sensitivity.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li><a href="sec-endogeneity.html#sec-reverse-causality"><strong>Reverse Causality</strong></a></li>
</ol>
<p>A special case of simultaneity where the causation runs opposite to what the model assumes.</p>
<p>Example (Health Policy): A naive model might regress health outcomes on insurance coverage. But it’s plausible that people in poor health are more likely to purchase insurance, causing reverse causality.</p>
<p>Over longer time intervals (e.g., yearly business data), reverse causality can look just like simultaneity in terms of its effect on regression estimates.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><a href="sec-endogeneity.html#sec-measurement-error"><strong>Measurement Errors</strong></a></li>
</ol>
<p>Even if a relevant variable is included, <strong>imprecise measurement</strong> introduces bias.</p>
<p>Classical Measurement Error (in <span class="math inline">\(X\)</span>):</p>
<ul>
<li>Leads to <strong>attenuation bias</strong>—estimated coefficients are biased toward zero.</li>
<li>Occurs frequently in survey data, behavioral measures, and administrative records.</li>
</ul>
<p>Example (Digital Marketing): Click-through rates or exposure to ads may be tracked with browser cookies or device IDs, but such identifiers are imperfect. The resulting measurement error biases the estimated effect of advertising downward.</p>
<hr>
<ol start="2" style="list-style-type: decimal">
<li><a href="sec-endogeneity.html#sec-endogenous-sample-selection">Endogenous Sample Selection</a></li>
</ol>
<p>Sample selection becomes a source of endogeneity when inclusion in the sample is related to the outcome variable.</p>
<p>Example (Labor Economics): Estimating the effect of education on wages using only employed individuals excludes those not currently working. If employment is correlated with unobserved traits (e.g., motivation), the wage equation is biased.</p>
<hr>
<p>Summary Table: Types of Endogeneity</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="38%">
<col width="34%">
<col width="26%">
</colgroup>
<thead><tr class="header">
<th>Type</th>
<th>Mechanism</th>
<th>Example Context</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="sec-endogeneity.html#sec-omitted-variable-bias">Omitted Variable Bias</a></td>
<td>Omitted variable affects both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>
</td>
<td>Managerial talent in finance, brand quality</td>
</tr>
<tr class="even">
<td><a href="sec-endogeneity.html#sec-simultaneity">Simultaneity</a></td>
<td>
<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> determined jointly</td>
<td>Price <span class="math inline">\(\leftrightarrow\)</span> Demand</td>
</tr>
<tr class="odd">
<td><a href="sec-endogeneity.html#sec-reverse-causality">Reverse Causality</a></td>
<td>
<span class="math inline">\(Y\)</span> causes <span class="math inline">\(X\)</span> (opposite direction from model assumption)</td>
<td>
<p>Health <span class="math inline">\(\to\)</span> Insurance</p>
<p>Revenue <span class="math inline">\(\to\)</span> Ad Spend</p>
</td>
</tr>
<tr class="even">
<td><a href="sec-endogeneity.html#sec-measurement-error">Measurement Error</a></td>
<td>
<span class="math inline">\(X\)</span> is observed with error</td>
<td>Digital metrics, survey measures</td>
</tr>
<tr class="odd">
<td><a href="sec-endogeneity.html#sec-endogenous-sample-selection">Endogenous Sample Selection</a></td>
<td>Sample selection is correlated with outcome</td>
<td>Labor force participation, customer panels</td>
</tr>
</tbody>
</table></div>
<hr>
<p>Endogeneity is not always fatal—if we can identify it and adjust for it, we can still make credible inferences.</p>
<ol style="list-style-type: decimal">
<li><a href="sec-controls.html#sec-controls">Control Variables</a></li>
</ol>
<p>If you suspect an omitted variable but have data on it, you can include it as a control. This is called a “<a href="sec-matching-methods.html#sec-selection-on-observables">selection on observables</a>” approach.</p>
<p>However, this strategy is often insufficient because:</p>
<ul>
<li>Many important factors are <strong>unobserved</strong> (e.g., motivation, ability, expectations).</li>
<li>Measured variables may contain <strong>measurement error</strong>, creating new biases.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Toolbox for Endogeneity</li>
</ol>
<p>To address more complex cases, including those involving unobservables, we introduce more advanced methods (see <a href="sec-causal-inference.html#sec-causal-inference">Causal Inference Toolbox</a>)</p>
<hr>
<div id="sec-endogenous-treatment" class="section level2" number="36.1">
<h2>
<span class="header-section-number">36.1</span> Endogenous Treatment<a class="anchor" aria-label="anchor" href="#sec-endogenous-treatment"><i class="fas fa-link"></i></a>
</h2>
<p>Endogenous treatment occurs when the variable of interest (the “treatment”) is not randomly assigned and is correlated with unobserved determinants of the outcome. As discussed earlier, this can arise from omitted variables, simultaneity, or reverse causality. But even if the true variable is theoretically exogenous, <a href="sec-endogeneity.html#sec-measurement-error">measurement error</a> can make it endogenous in practice.</p>
<p>This section focuses on how <a href="sec-endogeneity.html#sec-measurement-error">measurement errors</a>, especially in explanatory variables, introduce bias—typically <strong>attenuation bias</strong>—and why they are a central concern in applied research.</p>
<hr>
<div id="sec-measurement-error" class="section level3" number="36.1.1">
<h3>
<span class="header-section-number">36.1.1</span> Measurement Errors<a class="anchor" aria-label="anchor" href="#sec-measurement-error"><i class="fas fa-link"></i></a>
</h3>
<p>Measurement error refers to the difference between the <strong>true value</strong> of a variable and its <strong>observed (measured) value</strong>.</p>
<ul>
<li>Sources of measurement error:
<ul>
<li>
<strong>Coding errors</strong>: Manual or software-induced data entry mistakes.</li>
<li>
<strong>Reporting errors</strong>: Self-report bias, recall issues, or strategic misreporting.</li>
</ul>
</li>
</ul>
<p>Two Broad Types of Measurement Error</p>
<ol style="list-style-type: decimal">
<li>
<strong>Random (Stochastic) Error</strong> — <a href="sec-endogeneity.html#sec-classical-measurement-error"><em>Classical Measurement Error</em></a>
<ul>
<li>Noise is unpredictable and averages out in expectation.</li>
<li>Error is <strong>uncorrelated</strong> with the true variable and the regression error.</li>
<li>Common in survey data, tracking errors.</li>
</ul>
</li>
<li>
<strong>Systematic (Non-classical) Error</strong> — <a href="sec-endogeneity.html#sec-non-classical-measurement-error"><em>Non-Random Bias</em></a>
<ul>
<li>Measurement error exhibits consistent patterns across observations.</li>
<li>Often arises from:
<ul>
<li>
<strong>Instrument error</strong>: e.g., faulty sensors, uncalibrated scales.</li>
<li>
<strong>Method error</strong>: poor sampling, survey design flaws.</li>
<li>
<strong>Human error</strong>: judgment errors, social desirability bias.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Key insight</strong>:</p>
<ul>
<li>
<em>Random error</em> adds <strong>noise</strong>, pushing estimates toward zero.</li>
<li>
<em>Systematic error</em> introduces <strong>bias</strong>, pushing estimates either upward or downward.</li>
</ul>
<hr>
<div id="sec-classical-measurement-error" class="section level4" number="36.1.1.1">
<h4>
<span class="header-section-number">36.1.1.1</span> Classical Measurement Error<a class="anchor" aria-label="anchor" href="#sec-classical-measurement-error"><i class="fas fa-link"></i></a>
</h4>
<div id="sec-right-hand-side-variable" class="section level5" number="36.1.1.1.1">
<h5>
<span class="header-section-number">36.1.1.1.1</span> Right-Hand Side Variable<a class="anchor" aria-label="anchor" href="#sec-right-hand-side-variable"><i class="fas fa-link"></i></a>
</h5>
<p>Let’s examine the most common and analytically tractable case: <strong>classical measurement error</strong> in an explanatory variable.</p>
<p>Suppose the true model is:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + u_i
\]</span></p>
<p>But we do not observe <span class="math inline">\(X_i\)</span> directly. Instead, we observe:</p>
<p><span class="math display">\[
\tilde{X}_i = X_i + e_i
\]</span></p>
<p>where <span class="math inline">\(e_i\)</span> is the <strong>measurement error</strong>, assumed classical:</p>
<ul>
<li><span class="math inline">\(E[e_i] = 0\)</span></li>
<li><span class="math inline">\(Cov(X_i, e_i) = 0\)</span></li>
<li><span class="math inline">\(Cov(e_i, u_i) = 0\)</span></li>
</ul>
<p>Now, substitute <span class="math inline">\(\tilde{X}_i\)</span> into the regression:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 ( \tilde{X}_i - e_i ) + u_i \\
&amp;= \beta_0 + \beta_1 \tilde{X}_i + (u_i - \beta_1 e_i) \\
&amp;= \beta_0 + \beta_1 \tilde{X}_i + v_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(v_i = u_i - \beta_1 e_i\)</span> is a <strong>composite error</strong> term.</p>
<p>Since <span class="math inline">\(\tilde{X}_i\)</span> contains <span class="math inline">\(e_i\)</span>, and <span class="math inline">\(v_i\)</span> contains <span class="math inline">\(e_i\)</span>, we now have:</p>
<p><span class="math display">\[
Cov(\tilde{X}_i, v_i) \neq 0
\]</span></p>
<p>This correlation violates the exogeneity assumption and introduces <a href="sec-endogeneity.html#sec-endogeneity">endogeneity</a>.</p>
<hr>
<p>We can derive the asymptotic bias:</p>
<p><span class="math display">\[
\begin{aligned}
E[\tilde{X}_i v_i] &amp;= E[(X_i + e_i)(u_i - \beta_1 e_i)] \\
&amp;= -\beta_1 Var(e_i) \\
&amp;\neq 0
\end{aligned}
\]</span></p>
<p>This implies:</p>
<ul>
<li>If <span class="math inline">\(\beta_1 &gt; 0\)</span>, then <span class="math inline">\(\hat{\beta}_1\)</span> is biased <strong>downward</strong>.</li>
<li>If <span class="math inline">\(\beta_1 &lt; 0\)</span>, then <span class="math inline">\(\hat{\beta}_1\)</span> is biased <strong>upward</strong>.</li>
</ul>
<p>This is called <strong>attenuation bias</strong>: the estimated effect is biased toward zero.</p>
<p>As the <strong>variance of the error</strong> <span class="math inline">\(Var(e_i)\)</span> increases or <span class="math inline">\(\frac{Var(e_i)}{Var(\tilde{X}_i)} \to 1\)</span>, this bias becomes more severe.</p>
<hr>
<p><strong>Attenuation Factor</strong></p>
<p>The OLS estimator based on the noisy regressor is</p>
<p><span class="math display">\[
\hat{\beta}_{OLS} = \frac{ \text{cov}(\tilde{X}, Y)}{\text{var}(\tilde{X})} = \frac{\text{cov}(X + e, \beta X + u)}{\text{var}(X + e)}.
\]</span></p>
<p>Using the assumptions of classical measurement error, it follows that:</p>
<p><span class="math display">\[
plim\ \hat{\beta}_{OLS} = \beta \cdot \frac{\sigma_X^2}{\sigma_X^2 + \sigma_e^2} = \beta \cdot \lambda,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\sigma_X^2\)</span> is the variance of the true regressor <span class="math inline">\(X\)</span>,</li>
<li>
<span class="math inline">\(\sigma_e^2\)</span> is the variance of the measurement error <span class="math inline">\(e\)</span>, and</li>
<li>
<span class="math inline">\(\lambda = \frac{\sigma_X^2}{\sigma_X^2 + \sigma_e^2}\)</span> is called the <strong>reliability ratio</strong>, <strong>signal-to-total variance ratio</strong>, or <strong>attenuation factor</strong>.</li>
</ul>
<p>Since <span class="math inline">\(\lambda \in (0, 1]\)</span>, the bias always attenuates the estimate toward zero. The degree of attenuation bias is:</p>
<p><span class="math display">\[
\hat{\beta}_{OLS} - \beta = - (1 - \lambda)\beta,
\]</span></p>
<p>which implies:</p>
<ul>
<li>If <span class="math inline">\(\lambda = 1\)</span>, then <span class="math inline">\(\hat{\beta}_{OLS} = \beta\)</span> — no bias (no measurement error).</li>
<li>If <span class="math inline">\(\lambda &lt; 1\)</span>, then <span class="math inline">\(\hat{\beta}_{OLS} &lt; \beta\)</span> — attenuation toward zero.</li>
</ul>
<hr>
<p>Important Notes on Measurement Error</p>
<ul>
<li>
<p><strong>Data transformations can magnify measurement error.</strong></p>
<p>Suppose the true model is nonlinear:</p>
<p><span class="math display">\[
y = \beta x + \gamma x^2 + \epsilon,
\]</span></p>
<p>and <span class="math inline">\(x\)</span> is measured with classical error. Then, the attenuation factor for <span class="math inline">\(\hat{\gamma}\)</span> is <strong>approximately the square</strong> of the attenuation factor for <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p><span class="math display">\[
\lambda_{\hat{\gamma}} \approx \lambda_{\hat{\beta}}^2.
\]</span></p>
<p>This shows how nonlinear transformations (e.g., squares, logs) can exacerbate measurement error problems.</p>
</li>
<li>
<p><strong>Including covariates can increase attenuation bias.</strong></p>
<p>Adding covariates that are correlated with the mismeasured variable can <strong>worsen</strong> bias in the coefficient of interest, especially if the measurement error is not accounted for in those covariates.</p>
</li>
</ul>
<hr>
<p><strong>Remedies for Measurement Error</strong></p>
<p>To address attenuation bias caused by classical measurement error, consider the following strategies:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Use validation data or survey information</strong> to estimate <span class="math inline">\(\sigma_X^2\)</span>, <span class="math inline">\(\sigma_e^2\)</span>, or <span class="math inline">\(\lambda\)</span> and apply correction methods (e.g., SIMEX, regression calibration).</li>
<li>
<a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental Variables Approach</a><br>
Use an instrument <span class="math inline">\(Z\)</span> that:
<ul>
<li>Is correlated with the true variable <span class="math inline">\(X\)</span>,</li>
<li>Is uncorrelated with the regression error <span class="math inline">\(\epsilon\)</span>, and</li>
<li>Is uncorrelated with the measurement error <span class="math inline">\(e\)</span>.</li>
</ul>
</li>
<li>
<strong>Abandon your project</strong><br>
If no good instruments or validation data exist, and the attenuation bias is too severe, it may be prudent to reconsider the analysis or research question. (Said with love and academic humility.)</li>
</ol>
<hr>
</div>
<div id="sec-left-hand-side-variable" class="section level5" number="36.1.1.1.2">
<h5>
<span class="header-section-number">36.1.1.1.2</span> Left-Hand Side Variable<a class="anchor" aria-label="anchor" href="#sec-left-hand-side-variable"><i class="fas fa-link"></i></a>
</h5>
<p>Measurement error in the <strong>dependent variable</strong> (i.e., the response or outcome) is fundamentally different from measurement error in explanatory variables. Its consequences are often <strong>less problematic</strong> for consistent estimation of regression coefficients (e.g., the zero conditional mean assumption is not violated), but <strong>not necessarily for statistical inference</strong> (e.g., higher standard errors) or model fit.</p>
<hr>
<p>Suppose we are interested in the standard linear regression model:</p>
<p><span class="math display">\[
Y_i = \beta X_i + u_i,
\]</span></p>
<p>but we do not observe <span class="math inline">\(Y_i\)</span> directly. Instead, we observe:</p>
<p><span class="math display">\[
\tilde{Y}_i = Y_i + v_i,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(v_i\)</span> is measurement error in the dependent variable,</li>
<li>
<span class="math inline">\(E[v_i] = 0\)</span> (mean-zero),</li>
<li>
<span class="math inline">\(v_i\)</span> is uncorrelated with <span class="math inline">\(X_i\)</span> and <span class="math inline">\(u_i\)</span>,</li>
<li>
<span class="math inline">\(v_i\)</span> is <strong>homoskedastic</strong> and independent across observations.</li>
</ul>
<blockquote>
<p><strong>Be extra careful here!</strong></p>
<p>These are classical‐error assumptions:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Mean zero:</strong> <span class="math inline">\(\mathbb{E}[v\mid X]=0\)</span>.</li>
<li>
<strong>Exogeneity:</strong> <span class="math inline">\(v\)</span> is uncorrelated with each regressor <strong>and</strong> with the structural disturbance <span class="math inline">\(\epsilon\)</span> (i.e., <span class="math inline">\(\operatorname{Cov}(X,v)=\operatorname{Cov}(\epsilon,v)=0\)</span>).</li>
<li>
<strong>Homoskedasticity / finite moments</strong> for the law‑of‑large‑numbers to apply.</li>
</ol>
</blockquote>
<hr>
<p>The regression we actually estimate is:</p>
<p><span class="math display">\[
\tilde{Y}_i = \beta X_i + u_i + v_i.
\]</span></p>
<p>We can define a composite error term:</p>
<p><span class="math display">\[
\tilde{u}_i = u_i + v_i,
\]</span></p>
<p>so that the model becomes:</p>
<p><span class="math display">\[
\tilde{Y}_i = \beta X_i + \tilde{u}_i.
\]</span></p>
<p>Under the classical-error assumptions, the extra noise simply enlarges the composite error term <span class="math inline">\(\tilde{u}_i\)</span>, leaving</p>
<p><span class="math display">\[
\hat\beta^{\text{OLS}}    =\beta + ( X' X)^{-1} X'(u+v)    \;\xrightarrow{p} \beta ,
\]</span></p>
<p>so the estimator remains <strong>consistent</strong> and only its variance rises.</p>
<hr>
<p><strong>Key Insights</strong></p>
<ul>
<li>
<p><strong>Unbiasedness and Consistency of</strong> <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p>As long as <span class="math inline">\(E[\tilde{u}_i \mid X_i] = 0\)</span>, which holds under the classical assumptions (i.e., <span class="math inline">\(E[u_i \mid X_i] = 0\)</span> and <span class="math inline">\(E[v_i \mid X_i] = 0\)</span>), the OLS estimator of <span class="math inline">\(\beta\)</span> remains <strong>unbiased</strong> and <strong>consistent</strong>.</p>
<p>This is because measurement error in the <a href="sec-endogeneity.html#sec-left-hand-side-variable">left-hand side</a> does <strong>not</strong> induce endogeneity. The zero conditional mean assumption is preserved.</p>
</li>
<li>
<p><strong>Interpretation (Why Econometricians Don’t Panic)</strong>:</p>
<p>Econometricians and causal researchers often focus on <strong>consistent estimation</strong> of causal effects under strict exogeneity. Since <span class="math inline">\(v_i\)</span> just adds noise to the outcome and doesn’t systematically relate to <span class="math inline">\(X_i\)</span>, the slope estimate <span class="math inline">\(\hat{\beta}\)</span> remains a valid estimate of the causal effect <span class="math inline">\(\beta\)</span>.</p>
</li>
<li>
<p><strong>Statistical Implications (Why Statisticians Might Worry)</strong>:</p>
<p>Although <span class="math inline">\(\hat{\beta}\)</span> is consistent, the variance of the error term increases due to the added noise <span class="math inline">\(v_i\)</span>. Specifically:</p>
<p><span class="math display">\[
\text{Var}(\tilde{u}_i) = \text{Var}(u_i) + \text{Var}(v_i) = \sigma_u^2 + \sigma_v^2.
\]</span></p>
<p>This leads to:</p>
<ul>
<li>
<strong>Higher residual variance</strong> <span class="math inline">\(\Rightarrow\)</span> lower <span class="math inline">\(R^2\)</span>
</li>
<li>
<strong>Higher standard errors</strong> for coefficient estimates</li>
<li>
<strong>Wider confidence intervals</strong>, reducing the precision of inference</li>
</ul>
<p>Thus, even though the point estimate is valid, <strong>inference becomes weaker</strong>: hypothesis tests are less powerful, and conclusions less precise.</p>
</li>
</ul>
<hr>
<p>Practical Illustration</p>
<ul>
<li>Suppose <span class="math inline">\(X\)</span> is a marketing investment and <span class="math inline">\(Y\)</span> is sales revenue.</li>
<li>If sales are measured with noise (e.g., misrecorded sales data, rounding, reporting delays), the coefficient on marketing is still consistently estimated.</li>
<li>However, uncertainty around the estimate grows: wider confidence intervals might make it harder to detect statistically significant effects, especially in small samples.</li>
</ul>
<hr>
<p><strong>Summary Table: Measurement Error Consequences</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="27%">
<col width="20%">
<col width="11%">
<col width="17%">
<col width="23%">
</colgroup>
<thead><tr class="header">
<th>Location of Measurement Error</th>
<th>Bias in <span class="math inline">\(\hat{\beta}\)</span>
</th>
<th>Consistency</th>
<th>Affects Inference?</th>
<th>Typical Concern</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Regressor (<span class="math inline">\(X\)</span>)</td>
<td>Yes (attenuation)</td>
<td>No</td>
<td>Yes</td>
<td>Econometric &amp; statistical</td>
</tr>
<tr class="even">
<td>Outcome (<span class="math inline">\(Y\)</span>)</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>Mainly statistical</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="sec-non-classical-measurement-error" class="section level4" number="36.1.1.2">
<h4>
<span class="header-section-number">36.1.1.2</span> Non-Classical Measurement Error<a class="anchor" aria-label="anchor" href="#sec-non-classical-measurement-error"><i class="fas fa-link"></i></a>
</h4>
<p>In the classical measurement error model, we assume that the measurement error <span class="math inline">\(\epsilon\)</span> is <strong>independent</strong> of the true variable <span class="math inline">\(X\)</span> and of the regression disturbance <span class="math inline">\(u\)</span>. However, in many realistic data scenarios, this assumption does not hold. <a href="sec-endogeneity.html#sec-non-classical-measurement-error">Non-classical measurement error</a> refers to cases where:</p>
<ul>
<li>
<span class="math inline">\(\epsilon\)</span> is <strong>correlated</strong> with <span class="math inline">\(X\)</span>,</li>
<li>or possibly even <strong>correlated</strong> with <span class="math inline">\(u\)</span>.</li>
</ul>
<p>Violating the classical assumptions introduces additional and potentially complex biases in OLS estimation.</p>
<hr>
<p>Recall that in the <a href="sec-endogeneity.html#sec-classical-measurement-error">classical measurement error model</a>, we observe:</p>
<p><span class="math display">\[
\tilde{X} = X + \epsilon,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\epsilon\)</span> is independent of <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span>,</li>
<li>
<span class="math inline">\(E[\epsilon] = 0\)</span>.</li>
</ul>
<p>The true model is:</p>
<p><span class="math display">\[
Y = \beta X + u.
\]</span></p>
<p>Then, OLS based on the mismeasured regressor gives:</p>
<p><span class="math display">\[
\hat{\beta}_{OLS} = \frac{\text{cov}(\tilde{X}, Y)}{\text{var}(\tilde{X})} = \frac{\text{cov}(X + \epsilon, \beta X + u)}{\text{var}(X + \epsilon)}.
\]</span></p>
<p>With classical assumptions, this simplifies to:</p>
<p><span class="math display">\[
plim\ \hat{\beta}_{OLS} = \beta \cdot \frac{\sigma_X^2}{\sigma_X^2 + \sigma_\epsilon^2} = \beta \cdot \lambda,
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the <strong>reliability ratio</strong>, which attenuates <span class="math inline">\(\hat{\beta}\)</span> toward zero.</p>
<hr>
<p>Let us now relax the independence assumption and allow for correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(\epsilon\)</span>. In particular, suppose:</p>
<ul>
<li>
<span class="math inline">\(\text{cov}(X, \epsilon) = \sigma_{X\epsilon} \ne 0\)</span>.</li>
</ul>
<p>Then the probability limit of the OLS estimator becomes:</p>
<p><span class="math display">\[
\begin{aligned}
plim\ \hat{\beta}
&amp;= \frac{\text{cov}(X + \epsilon, \beta X + u)}{\text{var}(X + \epsilon)} \\
&amp;= \frac{\beta (\sigma_X^2 + \sigma_{X\epsilon})}{\sigma_X^2 + \sigma_\epsilon^2 + 2 \sigma_{X\epsilon}}.
\end{aligned}
\]</span></p>
<p>We can rewrite this as:</p>
<p><span class="math display">\[
\begin{aligned}
plim\ \hat{\beta}
&amp;= \beta \left(1 - \frac{\sigma_\epsilon^2 + \sigma_{X\epsilon}}{\sigma_X^2 + \sigma_\epsilon^2 + 2 \sigma_{X\epsilon}} \right) \\
&amp;= \beta (1 - b_{\epsilon \tilde{X}}),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(b_{\epsilon \tilde{X}}\)</span> is the <strong>regression coefficient of</strong> <span class="math inline">\(\epsilon\)</span> on <span class="math inline">\(\tilde{X}\)</span>, or more precisely:</p>
<p><span class="math display">\[
b_{\epsilon \tilde{X}} = \frac{\text{cov}(\epsilon, \tilde{X})}{\text{var}(\tilde{X})}.
\]</span></p>
<p>This makes clear that the bias in <span class="math inline">\(\hat{\beta}\)</span> depends on how strongly the measurement error is correlated with the observed regressor <span class="math inline">\(\tilde{X}\)</span>. This general formulation nests the <a href="sec-endogeneity.html#sec-classical-measurement-error">classical case</a> as a special case:</p>
<ul>
<li>In classical error: <span class="math inline">\(\sigma_{X\epsilon} = 0 \Rightarrow b_{\epsilon \tilde{X}} = \frac{\sigma^2_\epsilon}{\sigma^2_X + \sigma^2_\epsilon} = 1 - \lambda\)</span>.</li>
</ul>
<hr>
<p><strong>Implications of Non-Classical Measurement Error</strong></p>
<ul>
<li>When <span class="math inline">\(\sigma_{X\epsilon} &gt; 0\)</span>, the <strong>attenuation bias can increase or decrease</strong> depending on the balance of variances.</li>
<li>In particular:
<ul>
<li>If more than <strong>half of the variance in</strong> <span class="math inline">\(\tilde{X}\)</span> is due to measurement error, increasing <span class="math inline">\(\sigma_{X\epsilon}\)</span> increases attenuation.</li>
<li>If less than half is due to measurement error, it can actually <strong>reduce</strong> attenuation.</li>
</ul>
</li>
<li>This phenomenon is sometimes called <strong>mean-reverting measurement error</strong>: the measurement error pulls observed values toward the mean, distorting estimates <span class="citation">Bound, Brown, and Mathiowetz (<a href="references.html#ref-bound2001measurement">2001</a>)</span>.</li>
</ul>
<hr>
<div id="a-general-framework-for-non-classical-measurement-error" class="section level5" number="36.1.1.2.1">
<h5>
<span class="header-section-number">36.1.1.2.1</span> A General Framework for Non-Classical Measurement Error<a class="anchor" aria-label="anchor" href="#a-general-framework-for-non-classical-measurement-error"><i class="fas fa-link"></i></a>
</h5>
<p><span class="citation">Bound, Brown, and Mathiowetz (<a href="references.html#ref-bound2001measurement">2001</a>)</span> offer a unified matrix framework that accommodates measurement error in both the independent and dependent variables.</p>
<p>Let the true model be:</p>
<p><span class="math display">\[
\mathbf{Y = X \beta + \epsilon},
\]</span></p>
<p>but we observe <span class="math inline">\(\tilde{X} = X + U\)</span> and <span class="math inline">\(\tilde{Y} = Y + v\)</span>, where:</p>
<ul>
<li>
<span class="math inline">\(U\)</span> is a matrix of measurement error in <span class="math inline">\(X\)</span>,</li>
<li>
<span class="math inline">\(v\)</span> is a vector of measurement error in <span class="math inline">\(Y\)</span>.</li>
</ul>
<p>Then, the observed model becomes:</p>
<p><span class="math display">\[
\hat{\beta} = (\tilde{X}' \tilde{X})^{-1} \tilde{X}' \tilde{Y}.
\]</span></p>
<p>Substituting the observed quantities:</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{Y} &amp;= Y + v = X \beta + \epsilon + v, \\
&amp;= \tilde{X} \beta - U \beta + v + \epsilon.
\end{aligned}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\hat{\beta} = (\tilde{X}' \tilde{X})^{-1} \tilde{X}' (\tilde{X} \beta - U \beta + v + \epsilon),
\]</span></p>
<p>which simplifies to:</p>
<p><span class="math display">\[
\hat{\beta} = \beta + (\tilde{X}' \tilde{X})^{-1} \tilde{X}' (-U \beta + v + \epsilon).
\]</span></p>
<p>Taking the probability limit:</p>
<p><span class="math display">\[
plim\ \hat{\beta} = \beta + plim\ [(\tilde{X}' \tilde{X})^{-1} \tilde{X}' (-U \beta + v)],
\]</span></p>
<p>Now define:</p>
<p><span class="math display">\[
W = [U \quad v],
\]</span></p>
<p>and we can express the bias compactly as:</p>
<p><span class="math display">\[
plim\ \hat{\beta} = \beta + plim\ [(\tilde{X}' \tilde{X})^{-1} \tilde{X}' W
\begin{bmatrix}
- \beta \\
1
\end{bmatrix}
].
\]</span></p>
<p>This formulation highlights a powerful insight:</p>
<blockquote>
<p>Bias in <span class="math inline">\(\hat{\beta}\)</span> arises from the linear projection of the measurement errors onto the observed <span class="math inline">\(\tilde{X}\)</span>.</p>
</blockquote>
<p>This expression <strong>does not assert</strong> that <span class="math inline">\(v\)</span> <em>necessarily</em> biases <span class="math inline">\(\hat\beta\)</span>; it simply makes explicit that bias arises whenever the <em>linear projection</em> of <span class="math inline">\((U\beta-v)\)</span> onto <span class="math inline">\(\tilde X\)</span> is non‑zero. Three cases illustrate the point:</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="43%">
<col width="22%">
<col width="33%">
</colgroup>
<tbody>
<tr class="odd">
<td>Case</td>
<td>Key correlation</td>
<td>Consequence for <span class="math inline">\(\hat\beta\)</span>
</td>
</tr>
<tr class="even">
<td>
<p><a href="sec-endogeneity.html#sec-left-hand-side-variable"><strong>Classical Y‑error only</strong></a></p>
<p><span class="math inline">\(U\equiv0,\; \operatorname{Cov}(\tilde X,v)=0\)</span></p>
</td>
<td>projection term vanishes</td>
<td>
<strong>Consistent</strong>; larger standard errors</td>
</tr>
<tr class="odd">
<td>
<p><strong>Correlated Y‑error</strong></p>
<p><span class="math inline">\(U\equiv0,\; \operatorname{Cov}(\tilde X,v)\neq0\)</span></p>
</td>
<td>projection picks up <span class="math inline">\(v\)</span>
</td>
<td>
<strong>Biased</strong> (attenuation or sign reversal possible)</td>
</tr>
<tr class="even">
<td>
<p><strong>Both X‑ and Y‑error, independent</strong></p>
<p><span class="math inline">\(\operatorname{Cov}(X,U)\neq0,\; \operatorname{Cov}(\tilde X,v)=0\)</span></p>
</td>
<td>
<span class="math inline">\(U\beta\)</span> projects onto <span class="math inline">\(\tilde X\)</span>
</td>
<td>
<strong>Biased</strong> because of <span class="math inline">\(U\)</span>, <strong>not</strong> <span class="math inline">\(v\)</span>
</td>
</tr>
</tbody>
</table></div>
<p>Hence, <a href="sec-endogeneity.html#sec-left-hand-side-variable">your usual</a> “harmless <span class="math inline">\(Y\)</span>-noise” result is the special case in the first row.</p>
<hr>
<p>Practical implications</p>
<ol style="list-style-type: decimal">
<li><p><strong>Check assumptions explicitly.</strong> If the dataset was generated by self‑reports, simultaneous proxies, or modelled outcomes, it is rarely safe to assume <span class="math inline">\(\operatorname{Cov}(X,v)=0\)</span>.</p></li>
<li>
<p><strong>Correlated errors in</strong> <span class="math inline">\(Y\)</span> can creep in through:</p>
<ul>
<li>
<strong>Common data‑generating mechanisms</strong> (e.g., same survey module records both earnings (<span class="math inline">\(Y\)</span>) and hours worked (<span class="math inline">\(X\)</span>)).</li>
<li>
<strong>Prediction‑generated variables</strong> where <span class="math inline">\(v\)</span> inherits correlation with the features used to build <span class="math inline">\(\tilde Y\)</span>.</li>
</ul>
</li>
<li><p><strong>Joint mis‑measurement</strong> (<span class="math inline">\(U\)</span> and <span class="math inline">\(v\)</span> correlated) is common in administrative or sensor data; here, even “classical” <span class="math inline">\(v\)</span> with respect to <span class="math inline">\(X\)</span> can correlate with <span class="math inline">\(\tilde X=X+U\)</span>.</p></li>
</ol>
<blockquote>
<p><strong>Measurement error in</strong> <span class="math inline">\(Y\)</span> is benign <em>only</em> under strong exogeneity and independence conditions. The Bound–Brown–Mathiowetz matrix form <span class="citation">(<a href="references.html#ref-bound2001measurement">Bound, Brown, and Mathiowetz 2001</a>)</span> simply shows that once those conditions fail—or once <span class="math inline">\(X\)</span> itself is mis‑measured—the same projection logic that produces attenuation bias for <span class="math inline">\(X\)</span> can also transmit bias from <span class="math inline">\(v\)</span> to <span class="math inline">\(\hat\beta\)</span>.</p>
</blockquote>
<p>So the rule of thumb you learned is true in its narrow, classical setting, but <span class="citation">Bound, Brown, and Mathiowetz (<a href="references.html#ref-bound2001measurement">2001</a>)</span> remind us that empirical work often strays outside that safe harbor.</p>
<hr>
<p><strong>Consequences and Correction</strong></p>
<ul>
<li>Non-classical error can lead to <strong>over- or underestimation</strong>, unlike the always-attenuating classical case.</li>
<li>The direction and magnitude of bias depend on the correlation structure of <span class="math inline">\(X\)</span>, <span class="math inline">\(\epsilon\)</span>, and <span class="math inline">\(v\)</span>.</li>
<li>This poses serious problems in many survey and administrative data settings where systematic misreporting occurs.</li>
</ul>
<hr>
<p><strong>Practical Solutions</strong></p>
<ol style="list-style-type: decimal">
<li><p><a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental Variables</a><br>
Use an instrument <span class="math inline">\(Z\)</span> that is correlated with the true variable <span class="math inline">\(X\)</span>, but uncorrelated with both measurement error and the regression disturbance. IV can help eliminate both <a href="sec-endogeneity.html#sec-classical-measurement-error">classical</a> and <a href="sec-endogeneity.html#sec-non-classical-measurement-error">non-classical</a> error-induced biases.</p></li>
<li><p><strong>Validation Studies</strong><br>
Use a subset of the data with accurate measures to estimate the structure of measurement error and correct estimates via techniques such as regression calibration, multiple imputation, or SIMEX.</p></li>
<li><p><strong>Modeling the Error Process</strong><br>
Explicitly model the measurement error process, especially in longitudinal or panel data (e.g., via state-space models or Bayesian approaches).</p></li>
<li><p><strong>Binary/Dummy Variable Case</strong><br>
Non-classical error in binary regressors (e.g., misclassification) also leads to bias, but IV methods still apply. For example, if education level is misreported in survey data, a valid instrument (e.g., policy-based variation) can correct for misclassification bias.</p></li>
</ol>
<hr>
<p><strong>Summary</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="36%">
<col width="24%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Classical Error</th>
<th>Non-Classical Error</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\text{Cov}(X, \epsilon)\)</span></td>
<td>0</td>
<td><span class="math inline">\(\ne 0\)</span></td>
</tr>
<tr class="even">
<td>Bias in <span class="math inline">\(\hat{\beta}\)</span>
</td>
<td>Always attenuation</td>
<td>Can attenuate or inflate</td>
</tr>
<tr class="odd">
<td>Consistency of OLS</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td>Effect of Variance Structure</td>
<td>Predictable</td>
<td>Depends on <span class="math inline">\(\sigma_{X\epsilon}\)</span>
</td>
</tr>
<tr class="odd">
<td>Fixable with IV</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table></div>
<blockquote>
<p>In short, <strong>non-classical measurement error breaks the comforting regularity of attenuation bias</strong>. It can produce arbitrary biases depending on the nature and structure of the error. <a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental variables</a> and validation studies are often the only reliable tools for addressing this complex problem.</p>
</blockquote>
<hr>
</div>
</div>
<div id="solution-to-measurement-errors-in-correlation-estimation" class="section level4" number="36.1.1.3">
<h4>
<span class="header-section-number">36.1.1.3</span> Solution to Measurement Errors in Correlation Estimation<a class="anchor" aria-label="anchor" href="#solution-to-measurement-errors-in-correlation-estimation"><i class="fas fa-link"></i></a>
</h4>
<div id="bayesian-correction-for-correlation-coefficient" class="section level5" number="36.1.1.3.1">
<h5>
<span class="header-section-number">36.1.1.3.1</span> Bayesian Correction for Correlation Coefficient<a class="anchor" aria-label="anchor" href="#bayesian-correction-for-correlation-coefficient"><i class="fas fa-link"></i></a>
</h5>
<p>We begin by expressing the Bayesian posterior for a correlation coefficient <span class="math inline">\(\rho\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
P(\rho \mid \text{data}) &amp;= \frac{P(\text{data} \mid \rho) P(\rho)}{P(\text{data})} \\
\text{Posterior Probability} &amp;\propto \text{Likelihood} \times \text{Prior Probability}
\end{aligned}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\rho\)</span> is the true population correlation coefficient</li>
<li>
<span class="math inline">\(P(\text{data} \mid \rho)\)</span> is the likelihood function</li>
<li>
<span class="math inline">\(P(\rho)\)</span> is the prior density of <span class="math inline">\(\rho\)</span>
</li>
<li>
<span class="math inline">\(P(\text{data})\)</span> is the marginal likelihood (a normalizing constant)</li>
</ul>
<p>With sample correlation coefficient <span class="math inline">\(r\)</span>:</p>
<p><span class="math display">\[
r = \frac{S_{xy}}{\sqrt{S_{xx} S_{yy}}}
\]</span></p>
<p>According to <span class="citation">Schisterman et al. (<a href="references.html#ref-schisterman2003estimation">2003</a>)</span>, pp. 3, the posterior density of <span class="math inline">\(\rho\)</span> can be approximated as:</p>
<p><span class="math display">\[
P(\rho \mid x, y) \propto P(\rho) \cdot \frac{(1 - \rho^2)^{(n - 1)/2}}{(1 - \rho r)^{n - 3/2}}
\]</span></p>
<p>This approximation leads to a posterior that can be modeled via the Fisher transformation:</p>
<ul>
<li>Let <span class="math inline">\(\rho = \tanh(\xi)\)</span>, where <span class="math inline">\(\xi \sim N(z, 1/n)\)</span>
</li>
<li>
<span class="math inline">\(r = \tanh(z)\)</span> is the Fisher-transformed correlation</li>
</ul>
<p>Using conjugate normal approximations, we derive the posterior for the transformed correlation <span class="math inline">\(\xi\)</span> as:</p>
<ul>
<li><strong>Posterior Variance:</strong></li>
</ul>
<p><span class="math display">\[
\sigma^2_{\text{posterior}} = \frac{1}{n_{\text{prior}} + n_{\text{likelihood}}}
\]</span></p>
<ul>
<li><strong>Posterior Mean:</strong></li>
</ul>
<p><span class="math display">\[
\mu_{\text{posterior}} = \sigma^2_{\text{posterior}} \left(n_{\text{prior}} \cdot \tanh^{-1}(r_{\text{prior}}) + n_{\text{likelihood}} \cdot \tanh^{-1}(r_{\text{likelihood}})\right)
\]</span></p>
<p>To simplify the mathematics, we may assume a prior of the form:</p>
<p><span class="math display">\[
P(\rho) \propto (1 - \rho^2)^c
\]</span></p>
<p>where <span class="math inline">\(c\)</span> controls the strength of the prior. If no prior information is available, we can set <span class="math inline">\(c = 0\)</span> so that <span class="math inline">\(P(\rho) \propto 1\)</span>.</p>
<hr>
<p><strong>Example: Combining Estimates from Two Studies</strong></p>
<p>Let:</p>
<ul>
<li>Current study: <span class="math inline">\(r_{\text{likelihood}} = 0.5\)</span>, <span class="math inline">\(n_{\text{likelihood}} = 200\)</span>
</li>
<li>Prior study: <span class="math inline">\(r_{\text{prior}} = 0.2765\)</span>, <span class="math inline">\(n_{\text{prior}} = 50205\)</span>
</li>
</ul>
<p><strong>Step 1: Posterior Variance</strong></p>
<p><span class="math display">\[
\sigma^2_{\text{posterior}} = \frac{1}{50205 + 200} = 0.0000198393
\]</span></p>
<p><strong>Step 2: Posterior Mean</strong></p>
<p>Apply Fisher transformation:</p>
<ul>
<li><span class="math inline">\(\tanh^{-1}(0.2765) \approx 0.2841\)</span></li>
<li><span class="math inline">\(\tanh^{-1}(0.5) = 0.5493\)</span></li>
</ul>
<p>Then:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_{\text{posterior}} &amp;= 0.0000198393 \times (50205 \times 0.2841 + 200 \times 0.5493) \\
&amp;= 0.0000198393 \times (14260.7 + 109.86) \\
&amp;= 0.0000198393 \times 14370.56 = 0.2850
\end{aligned}
\]</span></p>
<p>Thus, the posterior distribution of <span class="math inline">\(\xi = \tanh^{-1}(\rho)\)</span> is:</p>
<p><span class="math display">\[
\xi \sim N(0.2850, 0.0000198393)
\]</span></p>
<p>Transforming back:</p>
<ul>
<li>Posterior mean correlation: <span class="math inline">\(\rho = \tanh(0.2850) = 0.2776\)</span>
</li>
<li>95% CI for <span class="math inline">\(\xi\)</span>: <span class="math inline">\(0.2850 \pm 1.96 \cdot \sqrt{0.0000198393} = (0.2762, 0.2937)\)</span>
</li>
<li>Transforming endpoints: <span class="math inline">\(\tanh(0.2762) = 0.2694\)</span>, <span class="math inline">\(\tanh(0.2937) = 0.2855\)</span>
</li>
</ul>
<p>The Bayesian posterior distribution for the correlation coefficient is:</p>
<ul>
<li>Mean: <span class="math inline">\(\hat{\rho}_{\text{posterior}} = 0.2776\)</span>
</li>
<li>95% CI: <span class="math inline">\((0.2694,\ 0.2855)\)</span>
</li>
</ul>
<hr>
<p>This Bayesian adjustment is especially useful when:</p>
<ol style="list-style-type: decimal">
<li>There is high sampling variation due to small sample sizes</li>
<li>Measurement error attenuates the observed correlation</li>
<li>Combining evidence from multiple studies (meta-analytic context)</li>
</ol>
<p>By leveraging prior information and applying the Fisher transformation, researchers can obtain a more stable and accurate estimate of the true underlying correlation.</p>
<div class="sourceCode" id="cb954"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define inputs</span></span>
<span><span class="va">n_new</span>  <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">r_new</span>  <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">alpha</span>  <span class="op">&lt;-</span> <span class="fl">0.05</span></span>
<span></span>
<span><span class="co"># Bayesian update function for correlation coefficient</span></span>
<span><span class="va">update_correlation</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n_new</span>, <span class="va">r_new</span>, <span class="va">alpha</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Prior (meta-analysis study)</span></span>
<span>  <span class="va">n_meta</span> <span class="op">&lt;-</span> <span class="fl">50205</span></span>
<span>  <span class="va">r_meta</span> <span class="op">&lt;-</span> <span class="fl">0.2765</span></span>
<span>  </span>
<span>  <span class="co"># Step 1: Posterior variance (in Fisher-z space)</span></span>
<span>  <span class="va">var_xi</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="va">n_new</span> <span class="op">+</span> <span class="va">n_meta</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Step 2: Posterior mean (in Fisher-z space)</span></span>
<span>  <span class="va">mu_xi</span> <span class="op">&lt;-</span> <span class="va">var_xi</span> <span class="op">*</span> <span class="op">(</span><span class="va">n_meta</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">atanh</a></span><span class="op">(</span><span class="va">r_meta</span><span class="op">)</span> <span class="op">+</span> <span class="va">n_new</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">atanh</a></span><span class="op">(</span><span class="va">r_new</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Step 3: Confidence interval in Fisher-z space</span></span>
<span>  <span class="va">z_crit</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">upper_xi</span>  <span class="op">&lt;-</span> <span class="va">mu_xi</span> <span class="op">+</span> <span class="va">z_crit</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_xi</span><span class="op">)</span></span>
<span>  <span class="va">lower_xi</span>  <span class="op">&lt;-</span> <span class="va">mu_xi</span> <span class="op">-</span> <span class="va">z_crit</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_xi</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Step 4: Transform back to correlation scale</span></span>
<span>  <span class="va">mean_rho</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">tanh</a></span><span class="op">(</span><span class="va">mu_xi</span><span class="op">)</span></span>
<span>  <span class="va">upper_rho</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">tanh</a></span><span class="op">(</span><span class="va">upper_xi</span><span class="op">)</span></span>
<span>  <span class="va">lower_rho</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Hyperbolic.html">tanh</a></span><span class="op">(</span><span class="va">lower_xi</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Return all values as a list</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>    mu_xi     <span class="op">=</span> <span class="va">mu_xi</span>,</span>
<span>    var_xi    <span class="op">=</span> <span class="va">var_xi</span>,</span>
<span>    upper_xi  <span class="op">=</span> <span class="va">upper_xi</span>,</span>
<span>    lower_xi  <span class="op">=</span> <span class="va">lower_xi</span>,</span>
<span>    mean_rho  <span class="op">=</span> <span class="va">mean_rho</span>,</span>
<span>    upper_rho <span class="op">=</span> <span class="va">upper_rho</span>,</span>
<span>    lower_rho <span class="op">=</span> <span class="va">lower_rho</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span></span>
<span><span class="co"># Run update</span></span>
<span><span class="va">updated</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">update_correlation</span><span class="op">(</span>n_new <span class="op">=</span> <span class="va">n_new</span>,</span>
<span>                       r_new <span class="op">=</span> <span class="va">r_new</span>,</span>
<span>                       alpha <span class="op">=</span> <span class="va">alpha</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display updated posterior mean and confidence interval</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Posterior mean of rho:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">updated</span><span class="op">$</span><span class="va">mean_rho</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Posterior mean of rho: 0.2775</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span></span>
<span>    <span class="st">"95% CI for rho: ("</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">updated</span><span class="op">$</span><span class="va">lower_rho</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>    <span class="st">","</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">updated</span><span class="op">$</span><span class="va">upper_rho</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>    <span class="st">")\n"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; 95% CI for rho: ( 0.2694 , 0.2855 )</span></span>
<span></span>
<span><span class="co"># For comparison: Classical (frequentist) confidence interval around r_new</span></span>
<span><span class="va">se_r</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">n_new</span><span class="op">)</span></span>
<span><span class="va">z_r</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="va">se_r</span></span>
<span><span class="va">ci_lo</span> <span class="op">&lt;-</span> <span class="va">r_new</span> <span class="op">-</span> <span class="va">z_r</span></span>
<span><span class="va">ci_hi</span> <span class="op">&lt;-</span> <span class="va">r_new</span> <span class="op">+</span> <span class="va">z_r</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Frequentist 95% CI for r:"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">ci_lo</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>    <span class="st">"to"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">ci_hi</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>    <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Frequentist 95% CI for r: 0.3614 to 0.6386</span></span></code></pre></div>
<hr>
</div>
</div>
</div>
<div id="sec-simultaneity" class="section level3" number="36.1.2">
<h3>
<span class="header-section-number">36.1.2</span> Simultaneity<a class="anchor" aria-label="anchor" href="#sec-simultaneity"><i class="fas fa-link"></i></a>
</h3>
<p>Simultaneity arises when at least one of the explanatory variables in a regression model is <strong>jointly determined</strong> with the dependent variable, violating a critical assumption for causal inference: <strong>temporal precedence</strong>.</p>
<p>Why Simultaneity Matters</p>
<ul>
<li>In classical regression, we assume that regressors are determined <strong>exogenously</strong>—they are not influenced by the dependent variable.</li>
<li>Simultaneity introduces <a href="sec-endogeneity.html#sec-endogeneity">endogeneity</a>, where regressors are correlated with the error term, rendering OLS <strong>estimators biased and inconsistent</strong>.</li>
<li>This has major implications in fields like economics, marketing, finance, and social sciences, where feedback mechanisms or equilibrium processes are common.</li>
</ul>
<p>Real-World Examples</p>
<ul>
<li>
<strong>Demand and supply</strong>: Price and quantity are determined together in market equilibrium.</li>
<li>
<strong>Sales and advertising</strong>: Advertising influences sales, but firms also adjust advertising based on current or anticipated sales.</li>
<li>
<strong>Productivity and investment</strong>: Higher productivity may attract investment, but investment can improve productivity.</li>
</ul>
<hr>
<div id="simultaneous-equation-system" class="section level4" number="36.1.2.1">
<h4>
<span class="header-section-number">36.1.2.1</span> Simultaneous Equation System<a class="anchor" aria-label="anchor" href="#simultaneous-equation-system"><i class="fas fa-link"></i></a>
</h4>
<p>We begin with a basic two-equation structural model:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 X_i + u_i \quad \text{(Structural equation for } Y) \\
X_i &amp;= \alpha_0 + \alpha_1 Y_i + v_i \quad \text{(Structural equation for } X)
\end{aligned}
\]</span></p>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> are <a href="sec-endogeneity.html#sec-endogeneity">endogenous variables</a> — both determined within the system.</li>
<li>
<span class="math inline">\(u_i\)</span> and <span class="math inline">\(v_i\)</span> are structural error terms, assumed to be uncorrelated with the exogenous variables (if any).</li>
</ul>
<p>The equations form a <strong>simultaneous system</strong> because each endogenous variable appears on the right-hand side of the other’s equation.</p>
<hr>
<p>To uncover the statistical properties of these equations, we solve for <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> as functions of the error terms only:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp;= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 v_i + u_i}{1 - \alpha_1 \beta_1} \\
X_i &amp;= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}
\end{aligned}
\]</span></p>
<p>These are the <strong>reduced-form equations</strong>, expressing the endogenous variables as functions of exogenous factors and disturbances.</p>
<hr>
</div>
<div id="simultaneity-bias-in-ols" class="section level4" number="36.1.2.2">
<h4>
<span class="header-section-number">36.1.2.2</span> Simultaneity Bias in OLS<a class="anchor" aria-label="anchor" href="#simultaneity-bias-in-ols"><i class="fas fa-link"></i></a>
</h4>
<p>If we naïvely estimate the first equation using OLS, assuming <span class="math inline">\(X_i\)</span> is exogenous, we get:</p>
<p><span class="math display">\[
\text{Bias: } \quad Cov(X_i, u_i) = Cov\left(\frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}, u_i\right) = \frac{\alpha_1}{1 - \alpha_1 \beta_1} \cdot Var(u_i)
\]</span></p>
<p>This violates the <a href="linear-regression.html#gauss-markov-theorem">Gauss-Markov Theorem</a> that regressors be uncorrelated with the error term. The OLS estimator for <span class="math inline">\(\beta_1\)</span> is <strong>biased and inconsistent</strong>.</p>
<hr>
<p>To allow for identification and estimation, we introduce <strong>exogenous variables</strong>:</p>
<p><span class="math display">\[
\begin{cases}
Y_i = \beta_0 + \beta_1 X_i + \beta_2 T_i + u_i \\
X_i = \alpha_0 + \alpha_1 Y_i + \alpha_2 Z_i + v_i
\end{cases}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(X_i\)</span>, <span class="math inline">\(Y_i\)</span> — <a href="sec-endogeneity.html#sec-endogeneity">endogenous</a> variables</li>
<li>
<span class="math inline">\(T_i\)</span>, <span class="math inline">\(Z_i\)</span> — <strong>exogenous</strong> variables, not influenced by any variable in the system</li>
</ul>
<hr>
<p>Solving this system algebraically yields the reduced form model:</p>
<p><span class="math display">\[
\begin{cases}\begin{aligned}Y_i &amp;= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 \alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{u}_i \\&amp;= B_0 + B_1 Z_i + B_2 T_i + \tilde{u}_i\end{aligned}\\\begin{aligned}X_i &amp;= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{\alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\alpha_1\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{v}_i \\&amp;= A_0 + A_1 Z_i + A_2 T_i + \tilde{v}_i\end{aligned}\end{cases}
\]</span></p>
<p>The reduced form expresses <strong>endogenous variables as functions of exogenous instruments</strong>, which we can estimate using OLS.</p>
<hr>
<p>Using reduced-form estimates <span class="math inline">\((A_1, A_2, B_1, B_2)\)</span>, we can identify (recover) the structural coefficients:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_1 &amp;= \frac{B_1}{A_1} \\
\beta_2 &amp;= B_2 \left(1 - \frac{B_1 A_2}{A_1 B_2}\right) \\
\alpha_1 &amp;= \frac{A_2}{B_2} \\
\alpha_2 &amp;= A_1 \left(1 - \frac{B_1 A_2}{A_1 B_2} \right)
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="identification-conditions" class="section level4" number="36.1.2.3">
<h4>
<span class="header-section-number">36.1.2.3</span> Identification Conditions<a class="anchor" aria-label="anchor" href="#identification-conditions"><i class="fas fa-link"></i></a>
</h4>
<p>Estimation of structural parameters is only possible if the model is <strong>identified</strong>.</p>
<p><strong>Order Condition (Necessary but Not Sufficient)</strong></p>
<p>A structural equation is <strong>identified</strong> if:</p>
<p><span class="math display">\[
K - k \ge m - 1
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(M\)</span> = total number of endogenous variables in the system</p></li>
<li><p><span class="math inline">\(m\)</span> = number of endogenous variables in the given equation</p></li>
<li><p><span class="math inline">\(K\)</span> = number of total exogenous variables in the system</p></li>
<li><p><span class="math inline">\(k\)</span> = number of exogenous variables appearing in the given equation</p></li>
<li><p><strong>Just-identified</strong>: <span class="math inline">\(K - k = m - 1\)</span> (exact identification)</p></li>
<li><p><strong>Over-identified</strong>: <span class="math inline">\(K - k &gt; m - 1\)</span> (more instruments than necessary)</p></li>
<li><p><strong>Under-identified</strong>: <span class="math inline">\(K - k &lt; m - 1\)</span> (cannot be estimated)</p></li>
</ul>
<blockquote>
<p><strong>Note:</strong> The order condition is necessary but not sufficient. The <strong>rank condition</strong> must also be satisfied for full identification, which we cover in <a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental Variables</a>.</p>
</blockquote>
<p>This simultaneous equations framework provides the foundation for <a href="sec-instrumental-variables.html#sec-instrumental-variables">instrumental variable</a> estimation, where:</p>
<ul>
<li>Exogenous variables not appearing in a structural equation serve as <strong>instruments</strong>.</li>
<li>These instruments allow consistent estimation of endogenous regressors’ effects.</li>
</ul>
<p>The reduced-form equations are often used to generate fitted values of endogenous regressors, which are then used in a Two-Stage Least Squares estimation process</p>
<hr>
</div>
</div>
<div id="sec-reverse-causality" class="section level3" number="36.1.3">
<h3>
<span class="header-section-number">36.1.3</span> Reverse Causality<a class="anchor" aria-label="anchor" href="#sec-reverse-causality"><i class="fas fa-link"></i></a>
</h3>
<p>Reverse causality refers to a situation in which the <strong>direction of causation</strong> is opposite to what is presumed. Specifically, we may model a relationship where variable <span class="math inline">\(X\)</span> is assumed to cause <span class="math inline">\(Y\)</span>, but in reality, <span class="math inline">\(Y\)</span> causes <span class="math inline">\(X\)</span>, or both influence each other in a feedback loop.</p>
<p>This violates a fundamental assumption for causal inference: <strong>temporal precedence</strong> — the cause must come before the effect. In the presence of reverse causality, the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> becomes ambiguous, and statistical estimators such as OLS become <strong>biased and inconsistent</strong>.</p>
<hr>
<p>In a standard linear regression model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + u_i
\]</span></p>
<p>We interpret <span class="math inline">\(\beta_1\)</span> as the causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. However, this interpretation implicitly assumes that:</p>
<ul>
<li>
<span class="math inline">\(X_i\)</span> is exogenous (uncorrelated with <span class="math inline">\(u_i\)</span>)</li>
<li>Changes in <span class="math inline">\(X_i\)</span> occur prior to or independently of changes in <span class="math inline">\(Y_i\)</span>
</li>
</ul>
<p>If <span class="math inline">\(Y_i\)</span> also affects <span class="math inline">\(X_i\)</span>, then <span class="math inline">\(X_i\)</span> is not exogenous — it is <a href="sec-endogeneity.html#sec-endogeneity">endogenous</a>, because it is correlated with <span class="math inline">\(u_i\)</span> via the reverse causal path.</p>
<hr>
<p>Reverse causality is especially problematic in observational data where interventions are not randomly assigned. Some key examples include:</p>
<ul>
<li>
<strong>Health and income</strong>: Higher income may improve health outcomes, but healthier individuals may also earn more (e.g., due to better productivity or fewer sick days).</li>
<li>
<strong>Education and wages</strong>: Education raises wages, but higher-income individuals might afford better education — or individuals with higher innate ability (reflected in <span class="math inline">\(u\)</span>) pursue more education and also earn more.</li>
<li>
<strong>Crime and policing</strong>: Increased police presence is often assumed to reduce crime, but high-crime areas are also likely to receive more police resources.</li>
<li>
<strong>Advertising and sales</strong>: Firms advertise more to boost sales, but high sales may also lead to higher advertising budgets — especially when revenue is reinvested in marketing.</li>
</ul>
<hr>
<p>To model reverse causality explicitly, consider:</p>
<div id="system-of-equations" class="section level4" number="36.1.3.1">
<h4>
<span class="header-section-number">36.1.3.1</span> System of Equations<a class="anchor" aria-label="anchor" href="#system-of-equations"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 X_i + u_i \quad &amp;\text{(Y depends on X)} \\
X_i &amp;= \gamma_0 + \gamma_1 Y_i + v_i \quad &amp;\text{(X depends on Y)}
\end{aligned}
\]</span></p>
<p>This feedback loop represents a <strong>simultaneous system</strong>, but where the <strong>causality direction is unclear</strong>. The two equations indicate that <strong>both variables are endogenous</strong>.</p>
<p>Even if we estimate only the first equation using OLS, the bias becomes apparent:</p>
<p><span class="math display">\[
Cov(X_i, u_i) \ne 0 \quad \Rightarrow \quad \hat{\beta}_1 \text{ is biased}
\]</span></p>
<p>Why? Because <span class="math inline">\(X_i\)</span> is determined by <span class="math inline">\(Y_i\)</span>, which itself depends on <span class="math inline">\(u_i\)</span>. Thus, <span class="math inline">\(X_i\)</span> indirectly depends on <span class="math inline">\(u_i\)</span>.</p>
<hr>
<p>In causal diagram notation (Directed Acyclic Graphs, or DAGs), reverse causality violates the acyclicity assumption. Here’s an example:</p>
<ul>
<li>Intended model: <span class="math inline">\(X \rightarrow Y\)</span>
</li>
<li>Reality: <span class="math inline">\(X \leftrightarrow Y\)</span> (feedback loop)</li>
</ul>
<p>This <strong>non-directional causality</strong> prevents us from interpreting coefficients causally unless additional identification strategies are applied.</p>
<hr>
<p>OLS assumes:</p>
<p><span class="math display">\[
E[u_i \mid X_i] = 0
\]</span></p>
<p>Under reverse causality, this condition fails. The resulting estimator <span class="math inline">\(\hat{\beta}_1\)</span> captures both the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> and the feedback from <span class="math inline">\(Y\)</span> to <span class="math inline">\(X\)</span>, leading to:</p>
<ul>
<li>
<a href="sec-endogeneity.html#sec-omitted-variable-bias">Omitted variable bias</a>: <span class="math inline">\(X_i\)</span> captures unobserved information from <span class="math inline">\(Y_i\)</span>
</li>
<li>
<a href="sec-endogeneity.html#sec-simultaneity">Simultaneity bias</a>: caused by the endogenous nature of <span class="math inline">\(X_i\)</span>
</li>
</ul>
<hr>
</div>
<div id="distinction-from-simultaneity" class="section level4" number="36.1.3.2">
<h4>
<span class="header-section-number">36.1.3.2</span> Distinction from Simultaneity<a class="anchor" aria-label="anchor" href="#distinction-from-simultaneity"><i class="fas fa-link"></i></a>
</h4>
<p>Reverse causality is a <strong>special case</strong> of endogeneity, often manifesting as simultaneity. However, the key distinction is:</p>
<ul>
<li>
<a href="sec-endogeneity.html#sec-simultaneity">Simultaneity</a>: Variables are determined together (e.g., in equilibrium models), and both are modeled explicitly in a system.</li>
<li>
<a href="sec-endogeneity.html#sec-reverse-causality">Reverse causality</a>: Only one equation is estimated, and the true causal direction is unknown or opposite to what is modeled.</li>
</ul>
<p>Reverse causality may or may not involve a full simultaneous system — it’s often <strong>unrecognized</strong> or <strong>assumed away</strong>, making it especially dangerous in empirical research.</p>
<hr>
<p>There are no mechanical tests that definitively detect reverse causality, but researchers can:</p>
<ul>
<li>Use temporal data (lags): Estimate <span class="math inline">\(Y_{it} = \beta_0 + \beta_1 X_{i,t-1} + u_{it}\)</span> and examine the temporal precedence of variables.</li>
<li>Apply Granger causality tests in time series (not strictly causal, but helpful diagnostically).</li>
<li>Use theoretical reasoning to justify directionality.</li>
<li>Check robustness across different time frames or <a href="sec-instrumental-variables.html#sec-instrumental-variables">instrumental variables</a>.</li>
</ul>
<hr>
</div>
<div id="solutions-to-reverse-causality" class="section level4" number="36.1.3.3">
<h4>
<span class="header-section-number">36.1.3.3</span> Solutions to Reverse Causality<a class="anchor" aria-label="anchor" href="#solutions-to-reverse-causality"><i class="fas fa-link"></i></a>
</h4>
<p>The following methods can mitigate reverse causality:</p>
<ol style="list-style-type: decimal">
<li><a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental Variables</a></li>
</ol>
<ul>
<li>Find a variable <span class="math inline">\(Z\)</span> that affects <span class="math inline">\(X\)</span> but is not affected by <span class="math inline">\(Y\)</span>, nor correlated with <span class="math inline">\(u_i\)</span>.</li>
<li>First stage: <span class="math inline">\(X_i = \pi_0 + \pi_1 Z_i + e_i\)</span>
</li>
<li>Second stage: <span class="math inline">\(\hat{X}_i\)</span> from the first stage is used in the regression for <span class="math inline">\(Y\)</span>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<a href="sec-experimental-design.html#sec-the-gold-standard-randomized-controlled-trials">Randomized Controlled Trials</a> (RCTs)</li>
</ol>
<ul>
<li>In experiments, the treatment (e.g., <span class="math inline">\(X\)</span>) is assigned randomly and therefore exogenous by design.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Natural Experiments / <a href="sec-quasi-experimental.html#sec-quasi-experimental">Quasi-Experimental Designs</a>
</li>
</ol>
<ul>
<li>Use external shocks or policy changes that affect <span class="math inline">\(X\)</span> but not <span class="math inline">\(Y\)</span> directly (e.g., difference-in-differences, regression discontinuity).</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Panel Data Methods</li>
</ol>
<ul>
<li>Use fixed-effects or difference estimators to eliminate time-invariant confounders.</li>
<li>Lag independent variables to examine delayed effects and improve causal direction inference.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Structural Equation Modeling</li>
</ol>
<ul>
<li>Estimate a full system of equations to explicitly model feedback.</li>
</ul>
<hr>
</div>
</div>
<div id="sec-omitted-variable-bias" class="section level3" number="36.1.4">
<h3>
<span class="header-section-number">36.1.4</span> Omitted Variable Bias<a class="anchor" aria-label="anchor" href="#sec-omitted-variable-bias"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Omitted Variable Bias (OVB)</strong> arises when a relevant explanatory variable that influences the dependent variable is left out of the regression model, and the omitted variable is <strong>correlated</strong> with one or more included regressors. This violates the exogeneity assumption of OLS and leads to <strong>biased and inconsistent</strong> estimators.</p>
<p>Suppose we are interested in estimating the effect of an independent variable <span class="math inline">\(X\)</span> on an outcome <span class="math inline">\(Y\)</span>, and the true data-generating process is:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + u_i
\]</span></p>
<p>However, if we omit <span class="math inline">\(Z_i\)</span> and estimate the model:</p>
<p><span class="math display">\[
Y_i = \gamma_0 + \gamma_1 X_i + \varepsilon_i
\]</span></p>
<p>Then the estimate <span class="math inline">\(\hat{\gamma}_1\)</span> may be biased because <span class="math inline">\(X_i\)</span> may be correlated with <span class="math inline">\(Z_i\)</span>, and <span class="math inline">\(Z_i\)</span> influences <span class="math inline">\(Y_i\)</span>.</p>
<hr>
<p>Let us derive the bias formally.</p>
<p>True model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + u_i \quad \text{(1)}
\]</span></p>
<p>Estimated model (with <span class="math inline">\(Z_i\)</span> omitted):</p>
<p><span class="math display">\[
Y_i = \gamma_0 + \gamma_1 X_i + \varepsilon_i \quad \text{(2)}
\]</span></p>
<p>Now, substitute the true model into the omitted model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + u_i = \gamma_0 + \gamma_1 X_i + \varepsilon_i
\]</span></p>
<p>Comparing both models, the omitted variable becomes part of the new error term:</p>
<p><span class="math display">\[
\varepsilon_i = \beta_2 Z_i + u_i
\]</span></p>
<p>Now, consider the OLS assumption:</p>
<p><span class="math display">\[
E[\varepsilon_i \mid X_i] = 0 \quad \text{(OLS requirement)}
\]</span></p>
<p>But since <span class="math inline">\(\varepsilon_i = \beta_2 Z_i + u_i\)</span> and <span class="math inline">\(Z_i\)</span> is correlated with <span class="math inline">\(X_i\)</span>, we have:</p>
<p><span class="math display">\[
Cov(X_i, \varepsilon_i) = \beta_2 Cov(X_i, Z_i) \ne 0
\]</span></p>
<p>Therefore, OLS assumption fails, and <span class="math inline">\(\hat{\gamma}_1\)</span> is <strong>biased</strong>.</p>
<hr>
<p>Let us calculate the <strong>expected value</strong> of the OLS estimator <span class="math inline">\(\hat{\gamma}_1\)</span>.</p>
<p>From regression theory, when omitting <span class="math inline">\(Z_i\)</span>, the expected value of <span class="math inline">\(\hat{\gamma}_1\)</span> is:</p>
<p><span class="math display">\[
E[\hat{\gamma}_1] = \beta_1 + \beta_2 \cdot \frac{Cov(X_i, Z_i)}{Var(X_i)}
\]</span></p>
<p>This is the <strong>Omitted Variable Bias formula</strong>.</p>
<blockquote>
<p><strong>Interpretation</strong>:<br>
The bias in <span class="math inline">\(\hat{\gamma}_1\)</span> depends on: - <span class="math inline">\(\beta_2\)</span>: the true effect of the omitted variable on <span class="math inline">\(Y\)</span> - <span class="math inline">\(Cov(X_i, Z_i)\)</span>: the correlation between <span class="math inline">\(X\)</span> and the omitted variable <span class="math inline">\(Z\)</span></p>
</blockquote>
<hr>
<div id="direction-of-the-bias" class="section level4" number="36.1.4.1">
<h4>
<span class="header-section-number">36.1.4.1</span> Direction of the Bias<a class="anchor" aria-label="anchor" href="#direction-of-the-bias"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>If <span class="math inline">\(\beta_2 &gt; 0\)</span> and <span class="math inline">\(Cov(X_i, Z_i) &gt; 0\)</span>: <span class="math inline">\(\hat{\gamma}_1\)</span> is <strong>upward biased</strong>
</li>
<li>If <span class="math inline">\(\beta_2 &lt; 0\)</span> and <span class="math inline">\(Cov(X_i, Z_i) &gt; 0\)</span>: <span class="math inline">\(\hat{\gamma}_1\)</span> is <strong>downward biased</strong>
</li>
<li>If <span class="math inline">\(Cov(X_i, Z_i) = 0\)</span>: <strong>No bias</strong>, even if <span class="math inline">\(Z_i\)</span> is omitted</li>
</ul>
<blockquote>
<p><strong>Note</strong>: Uncorrelated omitted variables do <strong>not</strong> bias the OLS estimator, although they may reduce precision.</p>
</blockquote>
<hr>
</div>
<div id="practical-example-education-and-earnings" class="section level4" number="36.1.4.2">
<h4>
<span class="header-section-number">36.1.4.2</span> Practical Example: Education and Earnings<a class="anchor" aria-label="anchor" href="#practical-example-education-and-earnings"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose we model:</p>
<p><span class="math display">\[
\text{Earnings}_i = \gamma_0 + \gamma_1 \cdot \text{Education}_i + \varepsilon_i
\]</span></p>
<p>But the true model includes ability (<span class="math inline">\(Z_i\)</span>):</p>
<p><span class="math display">\[
\text{Earnings}_i = \beta_0 + \beta_1 \cdot \text{Education}_i + \beta_2 \cdot \text{Ability}_i + u_i
\]</span></p>
<p>Omitting “ability” — a determinant of both education and earnings — leads to bias in the estimated effect of education:</p>
<ul>
<li>If more able individuals pursue more education and ability raises earnings (<span class="math inline">\(\beta_2 &gt; 0\)</span>), then <span class="math inline">\(\hat{\gamma}_1\)</span> overstates the true return to education.</li>
</ul>
<hr>
</div>
<div id="generalization-to-multiple-regression" class="section level4" number="36.1.4.3">
<h4>
<span class="header-section-number">36.1.4.3</span> Generalization to Multiple Regression<a class="anchor" aria-label="anchor" href="#generalization-to-multiple-regression"><i class="fas fa-link"></i></a>
</h4>
<p>In models with multiple regressors, omitting a relevant variable that is correlated with at least one included regressor will bias all coefficients affected by the correlation structure.</p>
<p>For example:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i
\]</span></p>
<p>If <span class="math inline">\(X_2\)</span> is omitted, and <span class="math inline">\(Cov(X_1, X_2) \ne 0\)</span>, then:</p>
<p><span class="math display">\[
E[\hat{\gamma}_1] = \beta_1 + \beta_2 \cdot \frac{Cov(X_1, X_2)}{Var(X_1)}
\]</span></p>
<hr>
</div>
<div id="remedies-for-ovb" class="section level4" number="36.1.4.4">
<h4>
<span class="header-section-number">36.1.4.4</span> Remedies for OVB<a class="anchor" aria-label="anchor" href="#remedies-for-ovb"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>Include the omitted variable</li>
</ol>
<ul>
<li>If <span class="math inline">\(Z\)</span> is observed, include it in the regression model.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Use <a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental Variables</a>
</li>
</ol>
<ul>
<li>
<p>If <span class="math inline">\(Z\)</span> is unobserved but <span class="math inline">\(X\)</span> is endogenous, find an instrument <span class="math inline">\(W\)</span>:</p>
<ul>
<li>Relevance: <span class="math inline">\(Cov(W, X) \ne 0\)</span>
</li>
<li>Exogeneity: <span class="math inline">\(Cov(W, u) = 0\)</span>
</li>
</ul>
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Use Panel Data Methods</li>
</ol>
<ul>
<li>
<strong>Fixed Effects</strong>: eliminate time-invariant omitted variables.</li>
<li>
<strong>Difference-in-Differences</strong>: exploit temporal variation to isolate effects.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><a href="sec-experimental-design.html#sec-the-gold-standard-randomized-controlled-trials">Experimental Designs</a></li>
</ol>
<ul>
<li>
<strong>Randomization</strong> ensures omitted variables are orthogonal to treatment, avoiding bias.</li>
</ul>
<hr>
</div>
</div>
</div>
<div id="sec-endogenous-sample-selection" class="section level2" number="36.2">
<h2>
<span class="header-section-number">36.2</span> Endogenous Sample Selection<a class="anchor" aria-label="anchor" href="#sec-endogenous-sample-selection"><i class="fas fa-link"></i></a>
</h2>
<p>Endogenous sample selection arises in <strong>observational</strong> or <strong>non-experimental</strong> research whenever the inclusion of observations (or assignment to treatment) is <strong>not random</strong>, and the same unobservable factors influencing selection also affect the outcome of interest. This scenario leads to <strong>biased and inconsistent</strong> estimates of causal parameters (e.g., <a href="sec-causal-inference.html#sec-average-treatment-effect">Average Treatment Effect</a>) if not properly addressed.</p>
<p>This problem was first formalized in the econometric literature by <span class="citation">J. Heckman (<a href="references.html#ref-heckman1974shadow">1974</a>)</span>, <span class="citation">J. J. Heckman (<a href="references.html#ref-heckman1976common">1976b</a>)</span>, and <span class="citation">J. J. Heckman (<a href="references.html#ref-heckman1979sample">1979</a>)</span>, whose work addressed the issue in the context of labor force participation among women. Later, <span class="citation">Amemiya (<a href="references.html#ref-amemiya1984tobit">1984</a>)</span> generalize the method. Now, it has since been applied widely across social sciences, epidemiology, marketing, and finance.</p>
<p>Endogenous sample selection is often conflated with general selection bias, but it is important to understand that sample selection refers specifically to the inclusion of observations into the estimation sample, not just to assignment into treatment (i.e., selection bias).</p>
<p>This problem comes in many names such as self-selection problem, incidental truncation, or omitted variable (i.e., the omitted variable is how people were selected into the sample). Some disciplines consider nonresponse/selection bias as sample selection:</p>
<ul>
<li>When unobservable factors that affect who is in the sample are independent of unobservable factors that affect the outcome, the sample selection is not endogenous. Hence, the sample selection is ignorable and estimator that ignores sample selection is still consistent.</li>
<li>When the unobservable factors that affect who is included in the sample are correlated with the unobservable factors that affect the outcome, the sample selection is endogenous and not ignorable, because estimators that ignore endogenous sample selection are not consistent (we don’t know which part of the observable outcome is related to the causal relationship and which part is due to different people were selected for the treatment and control groups).</li>
</ul>
<p>Many evaluation studies use observational data, and in such data:</p>
<ul>
<li>Participants are not randomly assigned.</li>
<li>Treatment or exposure is determined by individual or institutional choices.</li>
<li>Counterfactual outcomes are not observed.</li>
<li>The treatment indicator is often endogenous.</li>
</ul>
<p>Some notable terminologies include:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Truncation</strong>: Occurs when data are collected only from a restricted subpopulation based on the value of a variable.
<ul>
<li>
<strong>Left truncation</strong>: Values below a threshold are excluded (e.g., only high-income individuals are surveyed).</li>
<li>
<strong>Right truncation</strong>: Values above a threshold are excluded.</li>
</ul>
</li>
<li>
<strong>Censoring</strong>: Occurs when the variable is <strong>observed but coarsened</strong> beyond a threshold.
<ul>
<li>E.g., incomes below a certain level are coded as zero; arrest counts above a threshold are top-coded.</li>
</ul>
</li>
<li>
<strong>Incidental Truncation</strong>: Refers to selection based on a latent variable (e.g., employment decisions), <strong>not directly observed</strong>. This is what makes Heckman’s model distinct.
<ul>
<li>Also called <strong>non-random sample selection</strong>.</li>
<li>The error in the outcome equation is correlated with the selection indicator.</li>
</ul>
</li>
</ol>
<p>Researchers often categorize self-selection into:</p>
<ul>
<li>
<strong>Negative (Mitigation-Based) Selection:</strong> Individuals select into a treatment or sample to address an existing problem, so they start off with worse potential outcomes.
<ul>
<li>Bias direction: Underestimates true treatment effects (makes the treatment look less effective than it is).</li>
<li>Individuals select into treatment to combat a problem they already face.</li>
<li>
<strong>Examples</strong>:
<ul>
<li>People at high risk of severe illness (e.g., elderly or immunocompromised individuals) are more likely to get vaccinated. If we compare vaccinated vs. unvaccinated individuals without adjusting for risk factors, we might mistakenly conclude that vaccines are ineffective simply because vaccinated individuals had worse initial health conditions.</li>
<li>Evaluating the effect of job training programs—unemployed individuals with the greatest difficulty finding jobs are most likely to enroll, leading to underestimated program benefits.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Positive (Preference-Based) Selection:</strong> Individuals select into a treatment or sample because they have advantageous traits, preferences, or resources. Hence, those who take treatment are systematically better off compared to those who do not.
<ul>
<li>Bias direction: Overestimates true treatment effects (makes the treatment look more effective than it is).</li>
<li>Individuals select into treatment because they inherently prefer it, rather than because of an underlying problem.</li>
<li>
<strong>Examples:</strong>
<ul>
<li>People who are health-conscious and physically active are more likely to join a fitness program. If we compare fitness program participants to non-participants, we might falsely attribute their better health outcomes to the program, when in reality, their pre-existing lifestyle contributed to their improved health.</li>
<li>Evaluating the effect of private school education—students who attend private schools often come from wealthier families with greater academic support, making it difficult to isolate the true impact of the school itself.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Both forms of selection reflect correlation between <strong>unobservables</strong> (driving selection) and <strong>potential outcomes</strong>—the hallmark of <strong>endogenous selection bias</strong>.</p>
<hr>
<p>Some seminal applied works in this area include:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Labor Force Participation</strong> <span class="citation">(<a href="references.html#ref-heckman1974shadow">J. Heckman 1974</a>)</span>
</li>
</ol>
<ul>
<li>Wages are observed <strong>only</strong> for women who choose to work.</li>
<li>Unobservable preferences (reservation wages) drive participation.</li>
<li>Ignoring this leads to <strong>biased estimates of the returns to education</strong>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Union Membership</strong> <span class="citation">(<a href="references.html#ref-lewis1986union">Lewis 1986</a>)</span>
</li>
</ol>
<ul>
<li>Wages differ between union and non-union workers.</li>
<li>But union membership is <strong>not exogenous</strong>—workers choose to join based on anticipated benefits.</li>
<li>Naïve OLS yields biased estimates of union premium.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>College Attendance</strong> <span class="citation">(<a href="references.html#ref-card1999causal">Card 1999</a>, <a href="references.html#ref-card2001estimating">2001</a>)</span>
</li>
</ol>
<ul>
<li>Comparing income of college graduates vs. non-graduates.</li>
<li>Attending college is a choice based on expected gains, ability, or family background.</li>
<li>A treatment effect model (described next) is more appropriate here.</li>
</ul>
<hr>
<div id="unifying-model-frameworks" class="section level3" number="36.2.1">
<h3>
<span class="header-section-number">36.2.1</span> Unifying Model Frameworks<a class="anchor" aria-label="anchor" href="#unifying-model-frameworks"><i class="fas fa-link"></i></a>
</h3>
<p>Though often conflated, there are several overlapping models to address endogenous selection:</p>
<ol style="list-style-type: decimal">
<li>
<a href="sec-endogeneity.html#sec-sample-selection-model">Sample Selection Model</a> <span class="citation">(<a href="references.html#ref-heckman1979sample">J. J. Heckman 1979</a>)</span>: Outcome is <em>unobserved</em> if an agent is not selected into the sample.</li>
<li>
<a href="sec-endogeneity.html#sec-treatment-effect-switching-model">Treatment Effect Model</a>: Outcome is observed for both groups (treated vs. untreated), but treatment assignment is endogenous.</li>
<li>
<a href="sec-endogeneity.html#sec-heckman-type-control-function">Heckman-Type / Control Function Approaches</a>: Decompose the endogenous regressor or incorporate a correction term (Inverse Mills Ratio or residual) to control for endogeneity.</li>
</ol>
<p>All revolve around the challenge: unobserved factors affect both who is included (or treated) and outcomes.</p>
<p>To formalize the problem, we consider the outcome and selection equations. Let:</p>
<ul>
<li>
<span class="math inline">\(y_i\)</span>: observed outcome (e.g., wage)</li>
<li>
<span class="math inline">\(x_i\)</span>: covariates affecting outcome</li>
<li>
<span class="math inline">\(z_i\)</span>: covariates affecting selection</li>
<li>
<span class="math inline">\(w_i\)</span>: binary indicator for selection into the sample (e.g., employment)</li>
</ul>
<div id="sec-sample-selection-model" class="section level4" number="36.2.1.1">
<h4>
<span class="header-section-number">36.2.1.1</span> Sample Selection Model<a class="anchor" aria-label="anchor" href="#sec-sample-selection-model"><i class="fas fa-link"></i></a>
</h4>
<p>We begin with an <strong>outcome equation</strong>, which describes the variable of interest <span class="math inline">\(y_i\)</span>. However, we only observe <span class="math inline">\(y_i\)</span> if a certain <strong>selection mechanism</strong> indicates it is part of the sample. That mechanism is captured by a binary indicator <span class="math inline">\(w_i = 1\)</span>. Formally, the observed outcome equation is:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;= x_i' \beta + \varepsilon_i, \quad &amp;\text{(Observed only if } w_i = 1\text{)}, \\
\varepsilon_i &amp;\sim N(0, \sigma_\varepsilon^2).
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(x_i\)</span> is a vector of explanatory variables (or covariates) that explain <span class="math inline">\(y_i\)</span>. The noise term <span class="math inline">\(\varepsilon_i\)</span> is assumed to be normally distributed with mean zero and variance <span class="math inline">\(\sigma_\varepsilon^2\)</span>. However, because we only see <span class="math inline">\(y_i\)</span> for those cases in which <span class="math inline">\(w_i = 1\)</span>, we must account for how the selection occurs.</p>
<p>Next, we specify the <strong>selection equation</strong> via a <strong>latent index model</strong>. Let <span class="math inline">\(w_i^*\)</span> be an unobserved latent variable:</p>
<p><span class="math display">\[
\begin{aligned}
w_i^* &amp;= z_i' \gamma + u_i, \\
w_i &amp;= \begin{cases}
1 &amp; \text{if } w_i^* &gt; 0, \\
0 &amp; \text{otherwise}.
\end{cases}
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(z_i\)</span> is a vector of variables that influence whether or not <span class="math inline">\(y_i\)</span> is observed. In practice, <span class="math inline">\(z_i\)</span> may overlap with <span class="math inline">\(x_i\)</span>, but can also include variables not in the outcome equation. For <strong>identification</strong>, we normalize <span class="math inline">\(\mathrm{Var}(u_i) = 1\)</span>. This is analogous to the probit model’s standard normalization.</p>
<p>Because <span class="math inline">\(w_i = 1\)</span> exactly when <span class="math inline">\(w_i^* &gt; 0\)</span>, this event occurs if <span class="math inline">\(u_i \ge -\,z_i' \gamma\)</span>. Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
P(w_i = 1)
&amp;= P\bigl(u_i \ge -z_i' \gamma\bigr),\\
&amp;= 1 - \Phi\bigl(-z_i'\gamma\bigr), \\
&amp;= \Phi\bigl(z_i'\gamma\bigr),
\end{aligned}
\]</span></p>
<p>where we use the symmetry of the standard normal distribution.</p>
<p>We assume <span class="math inline">\((\varepsilon_i, u_i)\)</span> are <strong>jointly normally distributed</strong> with correlation <span class="math inline">\(\rho\)</span>. In other words,</p>
<p><span class="math display">\[
\begin{pmatrix}
\varepsilon_i \\
u_i
\end{pmatrix}
\;\sim\; \mathcal{N} \!\Biggl(
\begin{pmatrix} 0 \\ 0 \end{pmatrix},
\begin{pmatrix} \sigma^2_\varepsilon &amp; \rho \,\sigma_\varepsilon \\
\rho \,\sigma_\varepsilon &amp; 1 \end{pmatrix}
\Biggr).
\]</span></p>
<ul>
<li>If <span class="math inline">\(\rho = 0\)</span>, the selection is exogenous: whether <span class="math inline">\(y_i\)</span> is observed is unrelated to unobserved determinants of <span class="math inline">\(y_i\)</span>.</li>
<li>If <span class="math inline">\(\rho \neq 0\)</span>, <strong>sample selection is endogenous</strong>. Failing to model this selection mechanism leads to biased estimates of <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p>Interpreting <span class="math inline">\(\rho\)</span></p>
<ul>
<li>
<span class="math inline">\(\rho &gt; 0\)</span>: Individuals with higher unobserved components in <span class="math inline">\(\varepsilon_i\)</span> (and thus typically larger <span class="math inline">\(y_i\)</span>) are <strong>more likely</strong> to appear in the sample. (Positive selection)</li>
<li>
<span class="math inline">\(\rho &lt; 0\)</span>: Individuals with higher unobserved components in <span class="math inline">\(\varepsilon_i\)</span> are <strong>less likely</strong> to appear. (Negative selection)</li>
<li>
<span class="math inline">\(\rho = 0\)</span>: No endogenous selection. Observed outcomes are effectively random with respect to the unobserved part of <span class="math inline">\(y_i\)</span>.</li>
</ul>
<p>In empirical practice, <span class="math inline">\(\rho\)</span> signals the direction of bias one might expect if the selection process is ignored.</p>
<p>Often, it is helpful to visualize how part of the distribution of <span class="math inline">\(u_i\)</span> (the error in the selection equation) is truncated based on the threshold <span class="math inline">\(w_i^*&gt;0\)</span>. Below is a notional R code snippet that draws a normal density and shades the region where <span class="math inline">\(u_i &gt; -z_i'\gamma\)</span>.</p>
<div class="sourceCode" id="cb955"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, <span class="fl">3</span>, length <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,</span>
<span>     <span class="va">y</span>,</span>
<span>     type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>     main <span class="op">=</span>  <span class="fu"><a href="https://rdrr.io/r/base/bquote.html">bquote</a></span><span class="op">(</span><span class="st">"Probabibility distribution of"</span> <span class="op">~</span> <span class="va">u</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x_shaded</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">3</span>, length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">y_shaded</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x_shaded</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/polygon.html">polygon</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="va">x_shaded</span>, <span class="fl">3</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">y_shaded</span>, <span class="fl">0</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.1</span>, <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://andrisignorell.github.io/DescTools/reference/CramerV.html">Phi</a></span><span class="op">(</span><span class="op">-</span><span class="va">z</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">*</span> <span class="va">gamma</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/arrows.html">arrows</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0</span>, length <span class="op">=</span> <span class="fl">0.15</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.12</span>, <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="op">-</span><span class="va">z</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">*</span> <span class="va">gamma</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topright"</span>,</span>
<span>       <span class="st">"Gray = Prob of Observed"</span>,</span>
<span>       pch <span class="op">=</span> <span class="fl">1</span>,</span>
<span>       inset <span class="op">=</span> <span class="fl">0.02</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="36-endogeneity_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>In this figure, the gray‐shaded area represents <span class="math inline">\(u_i &gt; -z_i'\gamma\)</span>. Observations in that range are included in the sample. If <span class="math inline">\(\rho\neq 0\)</span>, then the unobserved factors that drive <span class="math inline">\(u_i\)</span> also affect <span class="math inline">\(\varepsilon_i\)</span>, causing a non‐representative sample of <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p>A core insight of the Heckman model is the conditional expectation of <span class="math inline">\(y_i\)</span> given <span class="math inline">\(w_i=1\)</span>:</p>
<p><span class="math display">\[
E\bigl(y_i \mid w_i = 1\bigr)
\;=\;
E\bigl(y_i \mid w_i^*&gt;0\bigr)
\;=\;
E\bigl(x_i'\beta + \varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr).
\]</span></p>
<p>Since <span class="math inline">\(x_i'\beta\)</span> is nonrandom (conditional on <span class="math inline">\(x_i\)</span>), we get</p>
<p><span class="math display">\[
E\bigl(y_i \mid w_i=1\bigr)
= x_i'\beta + E\bigl(\varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr).
\]</span></p>
<p>From bivariate normal properties:</p>
<p><span class="math display">\[
\varepsilon_i \,\bigl\lvert\, u_i=a
\;\sim\;
N\!\Bigl(\rho\,\sigma_{\varepsilon}\cdot a,\; (1-\rho^2)\,\sigma_{\varepsilon}^2\Bigr).
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
E\bigl(\varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr)
=\;
\rho\,\sigma_{\varepsilon}\,
E\bigl(u_i \mid u_i &gt; -z_i'\gamma\bigr).
\]</span></p>
<p>If <span class="math inline">\(U\sim N(0,1)\)</span>, then</p>
<p><span class="math display">\[
E(U \mid U&gt;a)
= \frac{\phi(a)}{1-\Phi(a)}
= \frac{\phi(a)}{\Phi(-a)},
\]</span> where <span class="math inline">\(\phi\)</span> is the standard normal pdf, <span class="math inline">\(\Phi\)</span> is the cdf. By symmetry, <span class="math inline">\(\phi(-a)=\phi(a)\)</span> and <span class="math inline">\(\Phi(-a)=1-\Phi(a)\)</span>. Letting <span class="math inline">\(a = -\,z_i'\gamma\)</span> yields</p>
<p><span class="math display">\[
E\bigl(U \mid U &gt; -z_i'\gamma\bigr)
= \frac{\phi(-z_i'\gamma)}{1-\Phi(-z_i'\gamma)}
= \frac{\phi(z_i'\gamma)}{\Phi(z_i'\gamma)}.
\]</span></p>
<p>Define the <strong>Inverse Mills Ratio</strong> (IMR) as</p>
<p><span class="math display">\[
\lambda(x)
= \frac{\phi(x)}{\Phi(x)}.
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
E\bigl(\varepsilon_i \mid u_i &gt; -z_i'\gamma\bigr)
= \rho\,\sigma_{\varepsilon}\,\lambda\bigl(z_i'\gamma\bigr),
\]</span> and therefore</p>
<p><span class="math display">\[
\boxed{
E\bigl(y_i \mid w_i=1\bigr)
= x_i'\beta
\;+\;
\rho\,\sigma_{\varepsilon}\,
\frac{\phi\bigl(z_i'\gamma\bigr)}{\Phi\bigl(z_i'\gamma\bigr)}.
}
\]</span></p>
<p>This extra term is the so‐called <strong>Heckman correction</strong>.</p>
<p>The IMR appears in the two‐step procedure as a regressor for bias correction. It has useful derivatives:</p>
<p><span class="math display">\[
\frac{d}{dx}\Bigl[\text{IMR}(x)\Bigr]
= \frac{d}{dx}\Bigl[\frac{\phi(x)}{\Phi(x)}\Bigr]
= -x\,\text{IMR}(x)\;-\;\bigl[\text{IMR}(x)\bigr]^2.
\]</span></p>
<p>This arises from the quotient rule and the fact that <span class="math inline">\(\phi'(x)=-x\phi(x)\)</span>, <span class="math inline">\(\Phi'(x)=\phi(x)\)</span>. The derivative property also helps in interpreting marginal effects in selection models.</p>
<hr>
</div>
<div id="sec-treatment-effect-switching-model" class="section level4" number="36.2.1.2">
<h4>
<span class="header-section-number">36.2.1.2</span> Treatment Effect (Switching) Model<a class="anchor" aria-label="anchor" href="#sec-treatment-effect-switching-model"><i class="fas fa-link"></i></a>
</h4>
<p>While the sample selection model is used when outcome is only observed for one group (e.g., <span class="math inline">\(D = 1\)</span>), the treatment effect model is used when outcomes are observed for both groups, but treatment assignment is endogenous.</p>
<p>Treatment Effect Model Equations:</p>
<ul>
<li>Outcome: <span class="math display">\[ y_i = x_i' \beta + D_i \delta + \varepsilon_i \]</span>
</li>
<li>Selection: <span class="math display">\[ D_i^* = z_i' \gamma + u_i \\ D_i = 1 \text{ if } D_i^* &gt; 0 \]</span>
</li>
</ul>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(D_i\)</span> is the treatment indicator.</p></li>
<li><p><span class="math inline">\((\varepsilon_i, u_i)\)</span> are again bivariate normal with correlation <span class="math inline">\(\rho\)</span>.</p></li>
</ul>
<p>The treatment effect model is sometimes called a <strong>switching regression</strong>.</p>
</div>
<div id="sec-heckman-type-control-function" class="section level4" number="36.2.1.3">
<h4>
<span class="header-section-number">36.2.1.3</span> Heckman-Type vs. Control Function<a class="anchor" aria-label="anchor" href="#sec-heckman-type-control-function"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<strong>Heckman Sample Selection</strong>: Insert an Inverse Mills Ratio (IMR) to adjust the outcome equation for non-random truncation.</li>
<li>
<strong>Control Function</strong>: Residual-based or predicted-endogenous-variable approach that mirrors IV logic, but typically <em>still</em> requires an instrument or parametric assumption.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<caption>Differences between Heckman Sample Selection vs. Heckman-type correction</caption>
<colgroup>
<col width="7%">
<col width="36%">
<col width="56%">
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td><strong>Heckman Sample Selection Model</strong></td>
<td><strong>Heckman-Type Corrections</strong></td>
</tr>
<tr class="even">
<td>When</td>
<td>Only observes one sample (treated), addressing selection bias directly.</td>
<td>Two samples are observed (treated and untreated), known as the control function approach.</td>
</tr>
<tr class="odd">
<td>Model</td>
<td>Probit</td>
<td>OLS (even for dummy endogenous variable)</td>
</tr>
<tr class="even">
<td>Integration of 1st stage</td>
<td>Also include a term (called Inverse Mills ratio) besides the endogenous variable.</td>
<td>Decompose the endogenous variable to get the part that is uncorrelated with the error terms of the outcome equation. Either use the predicted endogenous variable directly or include the residual from the first-stage equation.</td>
</tr>
<tr class="odd">
<td>Advantages and Assumptions</td>
<td>Provides a direct test for endogeneity via the coefficient of the inverse Mills ratio but requires the assumption of joint normality of errors.</td>
<td>Does not require the assumption of joint normality, but can’t test for endogeneity directly.</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="estimation-methods-2" class="section level3" number="36.2.2">
<h3>
<span class="header-section-number">36.2.2</span> Estimation Methods<a class="anchor" aria-label="anchor" href="#estimation-methods-2"><i class="fas fa-link"></i></a>
</h3>
<div id="heckmans-two-step-estimator-heckit" class="section level4" number="36.2.2.1">
<h4>
<span class="header-section-number">36.2.2.1</span> Heckman’s Two-Step Estimator (Heckit)<a class="anchor" aria-label="anchor" href="#heckmans-two-step-estimator-heckit"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Step 1: Estimate Selection Equation with Probit</strong></p>
<p>We estimate the probability of being included in the sample: <span class="math display">\[ P(w_i = 1 \mid z_i) = \Phi(z_i' \gamma) \]</span></p>
<p>From the estimated model, we compute the <strong>Inverse Mills Ratio (IMR)</strong>: <span class="math display">\[ \lambda_i = \frac{\phi(z_i' \hat{\gamma})}{\Phi(z_i' \hat{\gamma})} \]</span></p>
<p>This term captures the expected value of the error in the outcome equation, conditional on selection.</p>
<p><strong>Step 2: Include IMR in Outcome Equation</strong></p>
<p>We then estimate the regression: <span class="math display">\[ y_i = x_i' \beta + \delta \lambda_i + \nu_i \]</span></p>
<ul>
<li>If <span class="math inline">\(\delta\)</span> is significantly different from 0, selection bias is present.</li>
<li>
<span class="math inline">\(\lambda_i\)</span> corrects for the non-random selection.</li>
<li>OLS on this augmented model yields consistent estimates of <span class="math inline">\(\beta\)</span> under the joint normality assumption.</li>
<li>Pros: Conceptually simple; widely used.</li>
<li>Cons: Relies heavily on the bivariate normal assumption for <span class="math inline">\((\varepsilon_i, u_i)\)</span>. If no good exclusion variable is available, identification rests on the functional form.</li>
</ul>
<p>Specifically, the model can be identified <strong>without an exclusion restriction</strong>, but in such cases, identification is driven purely by the <strong>non-linearity</strong> of the probit function and the normality assumption (through the IMR). This is <strong>fragile</strong>.</p>
<ul>
<li>With strong exclusion restriction for the covariate in the correction equation, the variation in this variable can help identify the control for selection.</li>
<li>With weak exclusion restriction, and the variable exists in both steps, it’s the assumed error structure that identifies the control for selection <span class="citation">(<a href="references.html#ref-heckman2004using">J. Heckman and Navarro-Lozano 2004</a>)</span>.</li>
<li>In management, <span class="citation">Wolfolds and Siegel (<a href="references.html#ref-wolfolds2019misaccounting">2019</a>)</span> found that papers should have valid exclusion conditions, because without these, simulations show that results using the Heckman method are less reliable than those obtained with OLS.</li>
</ul>
<p>For robust identification, we prefer an <strong>exclusion restriction</strong>:</p>
<ul>
<li>A variable that affects selection (through <span class="math inline">\(z_i\)</span>) but not the outcome.</li>
<li>Example: Distance to a training center might affect the probability of enrollment, but not post-training income.</li>
</ul>
<p>Without such a variable, the model relies solely on functional form.</p>
<p>The Heckman two-step estimation procedure is less efficient than FIML. One key limitation is that the two-step estimator does not fully exploit the joint distribution of the error terms across equations, leading to a loss of efficiency. Moreover, the two-step approach may introduce <strong>measurement error</strong> in the second stage. This arises because the inverse Mills ratio used in the second stage is itself an estimated regressor, which can lead to biased standard errors and inference.</p>
<div class="sourceCode" id="cb956"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">###########################</span></span>
<span><span class="co">#   SIM 1: Heckman 2-step #</span></span>
<span><span class="co">###########################</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">rho</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">beta_true</span> <span class="op">&lt;-</span> <span class="fl">2</span></span>
<span></span>
<span><span class="va">gamma_true</span> <span class="op">&lt;-</span> <span class="fl">1.0</span></span>
<span><span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">rho</span>, <span class="va">rho</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">errors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>, <span class="va">Sigma</span><span class="op">)</span></span>
<span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="va">errors</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">u</span>       <span class="op">&lt;-</span> <span class="va">errors</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">w</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Selection</span></span>
<span><span class="va">z_star</span> <span class="op">&lt;-</span> <span class="va">w</span><span class="op">*</span><span class="va">gamma_true</span> <span class="op">+</span> <span class="va">u</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">z_star</span><span class="op">&gt;</span><span class="fl">0</span>,<span class="fl">1</span>,<span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Outcome</span></span>
<span></span>
<span><span class="va">y_star</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">*</span> <span class="va">beta_true</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span><span class="co"># Observed only if z=1</span></span>
<span><span class="va">y_obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">z</span> <span class="op">==</span> <span class="fl">1</span>, <span class="va">y_star</span>, <span class="cn">NA</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 1: Probit</span></span>
<span><span class="va">sel_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">z</span> <span class="op">~</span> <span class="va">w</span>, family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"probit"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">z_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">sel_mod</span>, type <span class="op">=</span> <span class="st">"link"</span><span class="op">)</span></span>
<span><span class="va">lambda_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">z_hat</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">z_hat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: OLS on observed + IMR</span></span>
<span><span class="va">data_heck</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">y_obs</span>,</span>
<span>                        x <span class="op">=</span> <span class="va">x</span>,</span>
<span>                        imr <span class="op">=</span> <span class="va">lambda_vals</span>,</span>
<span>                        z <span class="op">=</span> <span class="va">z</span><span class="op">)</span></span>
<span><span class="va">observed_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/subset.html">subset</a></span><span class="op">(</span><span class="va">data_heck</span>, <span class="va">z</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">heck_lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span> <span class="op">+</span> <span class="va">imr</span>, data <span class="op">=</span> <span class="va">observed_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck_lm</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ x + imr, data = observed_data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -2.76657 -0.60099 -0.02776  0.56317  2.74797 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.01715    0.07068   0.243    0.808    </span></span>
<span><span class="co">#&gt; x            1.95925    0.03934  49.800  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; imr          0.41900    0.10063   4.164 3.69e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.8942 on 501 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8332, Adjusted R-squared:  0.8325 </span></span>
<span><span class="co">#&gt; F-statistic:  1251 on 2 and 501 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"True beta="</span>, <span class="va">beta_true</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; True beta= 2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Heckman 2-step estimated beta="</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">heck_lm</span><span class="op">)</span><span class="op">[</span><span class="st">"x"</span><span class="op">]</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Heckman 2-step estimated beta= 1.959249</span></span></code></pre></div>
</div>
<div id="full-information-maximum-likelihood" class="section level4" number="36.2.2.2">
<h4>
<span class="header-section-number">36.2.2.2</span> Full Information Maximum Likelihood<a class="anchor" aria-label="anchor" href="#full-information-maximum-likelihood"><i class="fas fa-link"></i></a>
</h4>
<p>Jointly estimates the selection and outcome equations via ML, assuming:</p>
<p><span class="math display">\[
\biggl(\varepsilon_i, u_i\biggr) \sim \mathcal{N}\biggl(\begin{pmatrix}0\\0\end{pmatrix},\begin{pmatrix}\sigma_{\varepsilon}^2 &amp; \rho\,\sigma_{\varepsilon}\\\rho\,\sigma_{\varepsilon} &amp; 1\end{pmatrix}\biggr).
\]</span></p>
<ul>
<li>Pros: More efficient if the distributional assumption is correct. Allows a direct test of <span class="math inline">\(\rho=0\)</span> (LR test).</li>
<li>Cons: More sensitive to specification errors (i.e., requires stronger distributional assumptions); potentially complex to implement.</li>
</ul>
<p>We can use the <strong><code>sampleSelection</code></strong> package in R to perform full maximum likelihood estimation for the same data:</p>
<div class="sourceCode" id="cb957"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#############################</span></span>
<span><span class="co"># SIM 2: 2-step vs. FIML    #</span></span>
<span><span class="co">#############################</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/message.html">suppressPackageStartupMessages</a></span><span class="op">(</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Using same data (z, x, y_obs) from above</span></span>
<span></span>
<span><span class="co"># 1) Heckman 2-step (built-in)</span></span>
<span><span class="va">heck2</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">z</span> <span class="op">~</span> <span class="va">w</span>, <span class="va">y_obs</span> <span class="op">~</span> <span class="va">x</span>, method <span class="op">=</span> <span class="st">"2step"</span>, data <span class="op">=</span> <span class="va">data_heck</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck2</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; 2-step Heckman / heckit estimation</span></span>
<span><span class="co">#&gt; 1000 observations (496 censored and 504 observed)</span></span>
<span><span class="co">#&gt; 7 free parameters (df = 994)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02053    0.04494   0.457    0.648    </span></span>
<span><span class="co">#&gt; w            0.94063    0.05911  15.913   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.01715    0.07289   0.235    0.814    </span></span>
<span><span class="co">#&gt; x            1.95925    0.03924  49.932   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Multiple R-Squared:0.8332,   Adjusted R-Squared:0.8325</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; invMillsRatio   0.4190     0.1018   4.116 4.18e-05 ***</span></span>
<span><span class="co">#&gt; sigma           0.9388         NA      NA       NA    </span></span>
<span><span class="co">#&gt; rho             0.4463         NA      NA       NA    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span></span>
<span><span class="co"># 2) FIML</span></span>
<span><span class="va">heckFIML</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">z</span> <span class="op">~</span> <span class="va">w</span>, <span class="va">y_obs</span> <span class="op">~</span> <span class="va">x</span>, method <span class="op">=</span> <span class="st">"ml"</span>, data <span class="op">=</span> <span class="va">data_heck</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heckFIML</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 2 iterations</span></span>
<span><span class="co">#&gt; Return code 8: successive function values within relative tolerance limit (reltol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -1174.233 </span></span>
<span><span class="co">#&gt; 1000 observations (496 censored and 504 observed)</span></span>
<span><span class="co">#&gt; 6 free parameters (df = 994)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02169    0.04488   0.483    0.629    </span></span>
<span><span class="co">#&gt; w            0.94203    0.05908  15.945   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 0.008601   0.071315   0.121    0.904    </span></span>
<span><span class="co">#&gt; x           1.959195   0.039124  50.077   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma  0.94118    0.03503  26.867  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; rho    0.46051    0.09411   4.893 1.16e-06 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>You can compare the coefficient estimates on <code>x</code> from <code>heck2</code> vs. <code>heckFIML</code>. In large samples, both should converge to the true <span class="math inline">\(\beta\)</span>. FIML is typically more efficient, but if the normality assumption is violated, both can be biased.</p>
</div>
<div id="cf-and-iv-approaches" class="section level4" number="36.2.2.3">
<h4>
<span class="header-section-number">36.2.2.3</span> CF and IV Approaches<a class="anchor" aria-label="anchor" href="#cf-and-iv-approaches"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><strong>Control Function</strong></li>
</ol>
<ul>
<li>Residual-based approach: Regress the selection (or treatment) variable on excluded instruments and included controls. Obtain the predicted residual. Include that residual in the main outcome regression.</li>
<li>If correlated residual is significant, that indicates endogeneity; adjusting for it can correct bias.</li>
<li>Often used in the context of treatment effect models or simultaneously with IV logic.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental Variables</a></li>
</ol>
<ul>
<li>In the pure treatment effect context, an IV must affect treatment assignment but not the outcome directly.</li>
<li>For sample selection, an exclusion restriction (“instrument”) must shift selection but not outcomes.</li>
<li>Example: Distance to a training center influences participation in a job program but not post-training earnings.</li>
</ul>
<hr>
</div>
</div>
<div id="theoretical-connections" class="section level3" number="36.2.3">
<h3>
<span class="header-section-number">36.2.3</span> Theoretical Connections<a class="anchor" aria-label="anchor" href="#theoretical-connections"><i class="fas fa-link"></i></a>
</h3>
<div id="conditional-expectation-from-truncated-distributions" class="section level4" number="36.2.3.1">
<h4>
<span class="header-section-number">36.2.3.1</span> Conditional Expectation from Truncated Distributions<a class="anchor" aria-label="anchor" href="#conditional-expectation-from-truncated-distributions"><i class="fas fa-link"></i></a>
</h4>
<p>In the sample selection scenario:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[y_i\mid w_i=1] &amp;= x_i\beta + \mathbb{E}[\varepsilon_i\mid w_i^*&gt;0],\\
&amp;= x_i\beta + \rho\,\sigma_{\varepsilon}\,\frac{\phi(z_i'\gamma)}{\Phi(z_i'\gamma)},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\rho\,\sigma_{\varepsilon}\)</span> is the covariance term and <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\Phi\)</span> are the standard normal PDF and CDF, respectively. This formula underpins the inverse Mills ratio correction.</p>
<ul>
<li>If <span class="math inline">\(\rho&gt;0\)</span>, then the same unobservables that increase the likelihood of selection also increase outcomes, implying positive selection.</li>
<li>If <span class="math inline">\(\rho&lt;0\)</span>, selection is negatively correlated with outcomes.</li>
<li>
<span class="math inline">\(\hat{\rho}\)</span>: Estimated correlation of error terms. If significantly different from 0, endogenous selection is present.</li>
<li>Wald or Likelihood Ratio Test: Used to test <span class="math inline">\(H_0: \rho = 0\)</span>.</li>
<li>Lambda (<span class="math inline">\(\hat{\lambda}\)</span>): Product of <span class="math inline">\(\hat{\rho} \hat{\sigma}_\varepsilon\)</span>—measures strength of selection bias.</li>
<li>Inverse Mills Ratio: Can be saved and inspected to understand sample inclusion probabilities.</li>
</ul>
</div>
<div id="relationship-among-models" class="section level4" number="36.2.3.2">
<h4>
<span class="header-section-number">36.2.3.2</span> Relationship Among Models<a class="anchor" aria-label="anchor" href="#relationship-among-models"><i class="fas fa-link"></i></a>
</h4>
<p>All the models in <a href="sec-endogeneity.html#unifying-model-frameworks">Unifying Model Frameworks</a> can be seen as special or generalized cases:</p>
<ul>
<li><p>If one only has data for the selected group, it’s a sample selection setup.</p></li>
<li><p>If data for both groups exist, but assignment is endogenous, it’s a treatment effect problem.</p></li>
<li><p>If there’s a valid instrument, one can do a control function or IV approach.</p></li>
<li><p>If the normality assumption holds and selection is truly parametric, Heckman or FIML correct for the bias.</p></li>
</ul>
<p>Summary Table of Methods</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="9%">
<col width="25%">
<col width="16%">
<col width="8%">
<col width="16%">
<col width="23%">
</colgroup>
<thead><tr class="header">
<th><strong>Method</strong></th>
<th><strong>Data Observed</strong></th>
<th><strong>Key Assumption</strong></th>
<th><strong>Exclusion?</strong></th>
<th><strong>Pros</strong></th>
<th><strong>Cons</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>OLS (Naive)</td>
<td>Full or partial, ignoring selection</td>
<td>No endogeneity in errors</td>
<td>Not required</td>
<td>Simple to implement</td>
<td>Biased if endogeneity is present</td>
</tr>
<tr class="even">
<td>Heckman 2-Step (Heckit)</td>
<td>Outcome only for selected group</td>
<td>Joint normal errors; linear functional</td>
<td>Strongly recommended</td>
<td>Intuitive, widely used</td>
<td>Sensitive to normality/functional form.</td>
</tr>
<tr class="odd">
<td>FIML (Full ML)</td>
<td>Same as Heckman (subset observed)</td>
<td>Joint normal errors</td>
<td>Strongly recommended</td>
<td>More efficient, direct test of <span class="math inline">\(\rho=0\)</span>
</td>
<td>Complex, more sensitive to misspecification</td>
</tr>
<tr class="even">
<td>Control Function</td>
<td>Observed data for both or one group (depending on setup)</td>
<td>Some form of valid instrument or exog.</td>
<td>Yes (instrument)</td>
<td>Extends easily to many models</td>
<td>Must find valid instrument, no direct test for endogeneity</td>
</tr>
<tr class="odd">
<td>Instrumental Variables</td>
<td>Observed data for both groups, or entire sample (for selection)</td>
<td>IV must affect selection but not outcome</td>
<td>Yes (instrument)</td>
<td>Common approach in program evaluation</td>
<td>Exclusion restriction validity is critical</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="concerns" class="section level4" number="36.2.3.3">
<h4>
<span class="header-section-number">36.2.3.3</span> Concerns<a class="anchor" aria-label="anchor" href="#concerns"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Small Samples</strong>: Two-step procedures can be unstable in smaller datasets.</li>
<li>
<strong>Exclusion Restrictions</strong>: Without a credible variable that predicts selection but not outcomes, identification depends purely on functional form (bivariate normal + nonlinearity of probit).</li>
<li>
<strong>Distributional Assumption</strong>: If normality is seriously violated, neither 2-step nor FIML may reliably remove bias.</li>
<li>
<strong>Measurement Error in IMR</strong>: The second-stage includes an <em>estimated</em> regressor <span class="math inline">\(\hat{\lambda}_i\)</span>, which can add noise.</li>
<li>
<strong>Connection to IV</strong>: If a strong instrument exists, one could proceed with a control function or standard IV in a treatment effect setup. But for sample selection (lack of data on unselected), the Heckman approach is more common.</li>
<li>
<strong>Presence of Correlation between the Error Terms</strong>: The Heckman treatment effect model outperforms OLS when <span class="math inline">\(\rho \neq 0\)</span> because it corrects for selection bias due to unobserved factors. However, when <span class="math inline">\(\rho = 0\)</span>, the correction is unnecessary and can introduce inefficiency, making simpler methods more accurate.</li>
</ol>
<hr>
</div>
</div>
<div id="tobit-2-heckmans-sample-selection-model" class="section level3" number="36.2.4">
<h3>
<span class="header-section-number">36.2.4</span> Tobit-2: Heckman’s Sample Selection Model<a class="anchor" aria-label="anchor" href="#tobit-2-heckmans-sample-selection-model"><i class="fas fa-link"></i></a>
</h3>
<p>The Tobit-2 model, also known as <strong>Heckman’s standard sample selection model</strong>, is designed to correct for sample selection bias. This arises when the outcome variable is only observed for a non-random subset of the population, and the selection process is correlated with the outcome of interest.</p>
<p>A key assumption of the model is the <strong>joint normality of the error terms</strong> in the selection and outcome equations.</p>
<div id="panel-study-of-income-dynamics" class="section level4" number="36.2.4.1">
<h4>
<span class="header-section-number">36.2.4.1</span> Panel Study of Income Dynamics<a class="anchor" aria-label="anchor" href="#panel-study-of-income-dynamics"><i class="fas fa-link"></i></a>
</h4>
<p>We demonstrate the model using the classic dataset from <span class="citation">Mroz (<a href="references.html#ref-mroz1984sensitivity">1984</a>)</span>, which provides data from the 1975 Panel Study of Income Dynamics on married women’s labor-force participation and wages.</p>
<p>We aim to estimate the log of hourly wages for married women, using:</p>
<ul>
<li>
<code>educ</code>: Years of education</li>
<li>
<code>exper</code>: Years of work experience</li>
<li>
<code>exper^2</code>: Experience squared (to capture non-linear effects)</li>
<li>
<code>city</code>: A dummy for residence in a big city</li>
</ul>
<p>However, wages are only observed for those who participated in the labor force, meaning an OLS regression using only this subsample would suffer from selection bias.</p>
<p>Because we also have data on non-participants, we can use Heckman’s two-step method to correct for this bias.</p>
<hr>
<ol style="list-style-type: decimal">
<li><strong>Load and Prepare Data</strong></li>
</ol>
<div class="sourceCode" id="cb958"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/hadley/reshape">reshape2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Mroz87"</span><span class="op">)</span>  <span class="co"># PSID data on married women in 1975</span></span>
<span><span class="va">Mroz87</span> <span class="op">=</span> <span class="va">Mroz87</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>kids <span class="op">=</span> <span class="va">kids5</span> <span class="op">+</span> <span class="va">kids618</span><span class="op">)</span>  <span class="co"># total number of children</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">Mroz87</span><span class="op">)</span></span>
<span><span class="co">#&gt;   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage</span></span>
<span><span class="co">#&gt; 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288</span></span>
<span><span class="co">#&gt; 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416</span></span>
<span><span class="co">#&gt; 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807</span></span>
<span><span class="co">#&gt; 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417</span></span>
<span><span class="co">#&gt; 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000</span></span>
<span><span class="co">#&gt; 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106</span></span>
<span><span class="co">#&gt;   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll</span></span>
<span><span class="co">#&gt; 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE</span></span>
<span><span class="co">#&gt; 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE</span></span>
<span><span class="co">#&gt; 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE</span></span>
<span><span class="co">#&gt;   kids</span></span>
<span><span class="co">#&gt; 1    1</span></span>
<span><span class="co">#&gt; 2    2</span></span>
<span><span class="co">#&gt; 3    4</span></span>
<span><span class="co">#&gt; 4    3</span></span>
<span><span class="co">#&gt; 5    3</span></span>
<span><span class="co">#&gt; 6    0</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li><strong>Model Overview</strong></li>
</ol>
<p>The two-step Heckman selection model proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Selection Equation (Probit)</strong>:<br>
Models the probability of labor force participation (<code>lfp = 1</code>) as a function of variables that affect the decision to work.</p></li>
<li><p><strong>Outcome Equation (Wage)</strong>:<br>
Models log wages conditional on working. A correction term, the IMR, is included to account for the non-random selection into work.</p></li>
</ol>
<p>Step 1: Naive OLS on Observed Wages</p>
<div class="sourceCode" id="cb959"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ols1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span>,</span>
<span>          data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/subset.html">subset</a></span><span class="op">(</span><span class="va">Mroz87</span>, <span class="va">lfp</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ols1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(wage) ~ educ + exper + I(exper^2) + city, data = subset(Mroz87, </span></span>
<span><span class="co">#&gt;     lfp == 1))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -3.10084 -0.32453  0.05292  0.36261  2.34806 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.5308476  0.1990253  -2.667  0.00794 ** </span></span>
<span><span class="co">#&gt; educ         0.1057097  0.0143280   7.378 8.58e-13 ***</span></span>
<span><span class="co">#&gt; exper        0.0410584  0.0131963   3.111  0.00199 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0007973  0.0003938  -2.025  0.04352 *  </span></span>
<span><span class="co">#&gt; city         0.0542225  0.0680903   0.796  0.42629    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.6667 on 423 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.1581, Adjusted R-squared:  0.1501 </span></span>
<span><span class="co">#&gt; F-statistic: 19.86 on 4 and 423 DF,  p-value: 5.389e-15</span></span></code></pre></div>
<p>This OLS is biased because it only includes women who chose to work.</p>
<p>Step 2: Heckman Two-Step Estimation</p>
<div class="sourceCode" id="cb960"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Heckman 2-step estimation</span></span>
<span><span class="va">heck1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">heckit</a></span><span class="op">(</span></span>
<span>    selection <span class="op">=</span> <span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>    outcome <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span>,</span>
<span>    data <span class="op">=</span> <span class="va">Mroz87</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stage 1: Selection equation (probit)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck1</span><span class="op">$</span><span class="va">probit</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Probit binary choice model/Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 4 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -482.8212 </span></span>
<span><span class="co">#&gt; Model: Y == '1' in contrary to '0'</span></span>
<span><span class="co">#&gt; 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)</span></span>
<span><span class="co">#&gt; Estimates:</span></span>
<span><span class="co">#&gt;                  Estimate  Std. error t value   Pr(&gt; t)    </span></span>
<span><span class="co">#&gt; XS(Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** </span></span>
<span><span class="co">#&gt; XSage          0.18608901  0.06517476  2.8552  0.004301 ** </span></span>
<span><span class="co">#&gt; XSI(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** </span></span>
<span><span class="co">#&gt; XSkids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***</span></span>
<span><span class="co">#&gt; XShuswage     -0.04303635  0.01220791 -3.5253  0.000423 ***</span></span>
<span><span class="co">#&gt; XSeduc         0.12502818  0.02277645  5.4894 4.034e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; Significance test:</span></span>
<span><span class="co">#&gt; chi2(5) = 64.10407 (p=1.719042e-12)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span></span>
<span><span class="co"># Stage 2: Wage equation with selection correction</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">heck1</span><span class="op">$</span><span class="va">lm</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = YO ~ -1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -3.09494 -0.30953  0.05341  0.36530  2.34770 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; XO(Intercept) -0.6143381  0.3768796  -1.630  0.10383    </span></span>
<span><span class="co">#&gt; XOeduc         0.1092363  0.0197062   5.543 5.24e-08 ***</span></span>
<span><span class="co">#&gt; XOexper        0.0419205  0.0136176   3.078  0.00222 ** </span></span>
<span><span class="co">#&gt; XOI(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  </span></span>
<span><span class="co">#&gt; XOcity         0.0510492  0.0692414   0.737  0.46137    </span></span>
<span><span class="co">#&gt; imrData$IMR1   0.0551177  0.2111916   0.261  0.79423    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.6674 on 422 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.7734, Adjusted R-squared:  0.7702 </span></span>
<span><span class="co">#&gt; F-statistic:   240 on 6 and 422 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The variable <code>kids</code> is used only in the selection equation. This follows good practice: at least one variable should appear only in the selection equation (serving as an instrument) to help identify the model.</p>
<div class="sourceCode" id="cb961"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># ML estimation of Heckman selection model</span></span>
<span><span class="va">ml1</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span></span>
<span>  selection <span class="op">=</span> <span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>  outcome <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Mroz87</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ml1</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 3 iterations</span></span>
<span><span class="co">#&gt; Return code 8: successive function values within relative tolerance limit (reltol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -914.0777 </span></span>
<span><span class="co">#&gt; 753 observations (325 censored and 428 observed)</span></span>
<span><span class="co">#&gt; 13 free parameters (df = 740)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -4.1484037  1.4109302  -2.940 0.003382 ** </span></span>
<span><span class="co">#&gt; age          0.1842132  0.0658041   2.799 0.005253 ** </span></span>
<span><span class="co">#&gt; I(age^2)    -0.0023925  0.0007664  -3.122 0.001868 ** </span></span>
<span><span class="co">#&gt; kids        -0.1488158  0.0384888  -3.866 0.000120 ***</span></span>
<span><span class="co">#&gt; huswage     -0.0434253  0.0123229  -3.524 0.000451 ***</span></span>
<span><span class="co">#&gt; educ         0.1255639  0.0229229   5.478 5.91e-08 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.5814781  0.3052031  -1.905  0.05714 .  </span></span>
<span><span class="co">#&gt; educ         0.1078481  0.0172998   6.234 7.63e-10 ***</span></span>
<span><span class="co">#&gt; exper        0.0415752  0.0133269   3.120  0.00188 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0008125  0.0003974  -2.044  0.04129 *  </span></span>
<span><span class="co">#&gt; city         0.0522990  0.0682652   0.766  0.44385    </span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma  0.66326    0.02309  28.729   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho    0.05048    0.23169   0.218    0.828    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>The MLE approach jointly estimates both equations, yielding consistent and asymptotically efficient estimates.</p>
<p>Manual Implementation: Constructing the IMR</p>
<div class="sourceCode" id="cb962"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 1: Probit model</span></span>
<span><span class="va">myprob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/binaryChoice.html">probit</a></span><span class="op">(</span><span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>                 data <span class="op">=</span> <span class="va">Mroz87</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">myprob</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Probit binary choice model/Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 4 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -482.8212 </span></span>
<span><span class="co">#&gt; Model: Y == '1' in contrary to '0'</span></span>
<span><span class="co">#&gt; 753 observations (325 'negative' and 428 'positive') and 6 free parameters (df = 747)</span></span>
<span><span class="co">#&gt; Estimates:</span></span>
<span><span class="co">#&gt;                Estimate  Std. error t value   Pr(&gt; t)    </span></span>
<span><span class="co">#&gt; (Intercept) -4.18146681  1.40241567 -2.9816  0.002867 ** </span></span>
<span><span class="co">#&gt; age          0.18608901  0.06517476  2.8552  0.004301 ** </span></span>
<span><span class="co">#&gt; I(age^2)    -0.00241491  0.00075857 -3.1835  0.001455 ** </span></span>
<span><span class="co">#&gt; kids        -0.14955977  0.03825079 -3.9100 9.230e-05 ***</span></span>
<span><span class="co">#&gt; huswage     -0.04303635  0.01220791 -3.5253  0.000423 ***</span></span>
<span><span class="co">#&gt; educ         0.12502818  0.02277645  5.4894 4.034e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; Significance test:</span></span>
<span><span class="co">#&gt; chi2(5) = 64.10407 (p=1.719042e-12)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span></span>
<span><span class="co"># Step 2: Compute IMR</span></span>
<span><span class="va">imr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/invMillsRatio.html">invMillsRatio</a></span><span class="op">(</span><span class="va">myprob</span><span class="op">)</span></span>
<span><span class="va">Mroz87</span><span class="op">$</span><span class="va">IMR1</span> <span class="op">&lt;-</span> <span class="va">imr</span><span class="op">$</span><span class="va">IMR1</span></span>
<span></span>
<span><span class="co"># Step 3: Wage regression including IMR</span></span>
<span><span class="va">manually_est</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span> <span class="op">+</span> <span class="va">IMR1</span>,</span>
<span>                   data <span class="op">=</span> <span class="va">Mroz87</span>,</span>
<span>                   subset <span class="op">=</span> <span class="op">(</span><span class="va">lfp</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">manually_est</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(wage) ~ educ + exper + I(exper^2) + city + IMR1, </span></span>
<span><span class="co">#&gt;     data = Mroz87, subset = (lfp == 1))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -3.09494 -0.30953  0.05341  0.36530  2.34770 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.6143381  0.3768796  -1.630  0.10383    </span></span>
<span><span class="co">#&gt; educ         0.1092363  0.0197062   5.543 5.24e-08 ***</span></span>
<span><span class="co">#&gt; exper        0.0419205  0.0136176   3.078  0.00222 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  </span></span>
<span><span class="co">#&gt; city         0.0510492  0.0692414   0.737  0.46137    </span></span>
<span><span class="co">#&gt; IMR1         0.0551177  0.2111916   0.261  0.79423    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.6674 on 422 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.1582, Adjusted R-squared:  0.1482 </span></span>
<span><span class="co">#&gt; F-statistic: 15.86 on 5 and 422 DF,  p-value: 2.505e-14</span></span></code></pre></div>
<p>Equivalent Method Using <code>glm</code> and Manual IMR Calculation</p>
<div class="sourceCode" id="cb963"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Probit via glm</span></span>
<span><span class="va">probit_selection</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="va">lfp</span> <span class="op">~</span> <span class="va">age</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">age</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">kids</span> <span class="op">+</span> <span class="va">huswage</span> <span class="op">+</span> <span class="va">educ</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Mroz87</span>,</span>
<span>  family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">'probit'</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute predicted latent index and IMR</span></span>
<span><span class="va">probit_lp</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">probit_selection</span><span class="op">)</span></span>
<span><span class="va">inv_mills</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">probit_lp</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">probit_lp</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Mroz87</span><span class="op">$</span><span class="va">inv_mills</span> <span class="op">&lt;-</span> <span class="va">inv_mills</span></span>
<span></span>
<span><span class="co"># Second stage: Wage regression with correction</span></span>
<span><span class="va">probit_outcome</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">wage</span><span class="op">)</span> <span class="op">~</span> <span class="va">educ</span> <span class="op">+</span> <span class="va">exper</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="va">exper</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">city</span> <span class="op">+</span> <span class="va">inv_mills</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Mroz87</span>,</span>
<span>  subset <span class="op">=</span> <span class="op">(</span><span class="va">lfp</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">probit_outcome</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = log(wage) ~ educ + exper + I(exper^2) + city + </span></span>
<span><span class="co">#&gt;     inv_mills, data = Mroz87, subset = (lfp == 1))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.6143383  0.3768798  -1.630  0.10383    </span></span>
<span><span class="co">#&gt; educ         0.1092363  0.0197062   5.543 5.24e-08 ***</span></span>
<span><span class="co">#&gt; exper        0.0419205  0.0136176   3.078  0.00222 ** </span></span>
<span><span class="co">#&gt; I(exper^2)  -0.0008226  0.0004059  -2.026  0.04335 *  </span></span>
<span><span class="co">#&gt; city         0.0510492  0.0692414   0.737  0.46137    </span></span>
<span><span class="co">#&gt; inv_mills    0.0551179  0.2111918   0.261  0.79423    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for gaussian family taken to be 0.4454809)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 223.33  on 427  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance: 187.99  on 422  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 876.49</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 2</span></span></code></pre></div>
<p>Comparing Models</p>
<div class="sourceCode" id="cb964"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stargazer</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://cran.r-project.org/package=plm">plm</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/">sandwich</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Custom robust SE function</span></span>
<span><span class="va">cse</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">reg</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/vcovHC.html">vcovHC</a></span><span class="op">(</span><span class="va">reg</span>, type <span class="op">=</span> <span class="st">"HC1"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Comparison table</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span></span>
<span>  <span class="va">ols1</span>, <span class="va">heck1</span>, <span class="va">ml1</span>, <span class="va">manually_est</span>,</span>
<span>  se <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="fu">cse</span><span class="op">(</span><span class="va">ols1</span><span class="op">)</span>, <span class="cn">NULL</span>, <span class="cn">NULL</span>, <span class="cn">NULL</span><span class="op">)</span>,</span>
<span>  title <span class="op">=</span> <span class="st">"Married Women's Wage Regressions: OLS vs Heckman Models"</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>  df <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  digits <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  selection.equation <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Married Women's Wage Regressions: OLS vs Heckman Models</span></span>
<span><span class="co">#&gt; =========================================================================</span></span>
<span><span class="co">#&gt;                                      Dependent variable:                 </span></span>
<span><span class="co">#&gt;                     -----------------------------------------------------</span></span>
<span><span class="co">#&gt;                     log(wage)                lfp               log(wage) </span></span>
<span><span class="co">#&gt;                        OLS         Heckman        selection       OLS    </span></span>
<span><span class="co">#&gt;                                   selection                              </span></span>
<span><span class="co">#&gt;                        (1)           (2)             (3)          (4)    </span></span>
<span><span class="co">#&gt; -------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; age                               0.1861***       0.1842***              </span></span>
<span><span class="co">#&gt;                                   (0.0652)        (0.0658)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; I(age2)                          -0.0024***      -0.0024***              </span></span>
<span><span class="co">#&gt;                                   (0.0008)        (0.0008)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; kids                             -0.1496***      -0.1488***              </span></span>
<span><span class="co">#&gt;                                   (0.0383)        (0.0385)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; huswage                          -0.0430***      -0.0434***              </span></span>
<span><span class="co">#&gt;                                   (0.0122)        (0.0123)               </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; educ                0.1057***     0.1250***       0.1256***    0.1092*** </span></span>
<span><span class="co">#&gt;                      (0.0130)     (0.0228)        (0.0229)      (0.0197) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; exper               0.0411***                                  0.0419*** </span></span>
<span><span class="co">#&gt;                      (0.0154)                                   (0.0136) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; I(exper2)            -0.0008*                                  -0.0008** </span></span>
<span><span class="co">#&gt;                      (0.0004)                                   (0.0004) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; city                  0.0542                                     0.0510  </span></span>
<span><span class="co">#&gt;                      (0.0653)                                   (0.0692) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; IMR1                                                             0.0551  </span></span>
<span><span class="co">#&gt;                                                                 (0.2112) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; Constant            -0.5308***   -4.1815***      -4.1484***     -0.6143  </span></span>
<span><span class="co">#&gt;                      (0.2032)     (1.4024)        (1.4109)      (0.3769) </span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; -------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; Observations           428           753             753          428    </span></span>
<span><span class="co">#&gt; R2                    0.1581       0.1582                        0.1582  </span></span>
<span><span class="co">#&gt; Adjusted R2           0.1501       0.1482                        0.1482  </span></span>
<span><span class="co">#&gt; Log Likelihood                                    -914.0777              </span></span>
<span><span class="co">#&gt; rho                                0.0830      0.0505 (0.2317)           </span></span>
<span><span class="co">#&gt; Inverse Mills Ratio            0.0551 (0.2099)                           </span></span>
<span><span class="co">#&gt; Residual Std. Error   0.6667                                     0.6674  </span></span>
<span><span class="co">#&gt; F Statistic         19.8561***                                 15.8635***</span></span>
<span><span class="co">#&gt; =========================================================================</span></span>
<span><span class="co">#&gt; Note:                                         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</span></span></code></pre></div>
<ul>
<li><p>IMR: If the coefficient on the IMR is statistically significant, it suggests selection bias is present and that OLS estimates are biased.</p></li>
<li><p><span class="math inline">\(\rho\)</span>: Represents the estimated correlation between the error terms of the selection and outcome equations. A significant <span class="math inline">\(\rho\)</span> implies non-random selection, justifying the Heckman correction.</p></li>
<li><p>In our case, if the IMR coefficient is not statistically different from zero, then selection bias may not be a serious concern.</p></li>
</ul>
</div>
<div id="the-role-of-exclusion-restrictions-in-heckmans-model" class="section level4" number="36.2.4.2">
<h4>
<span class="header-section-number">36.2.4.2</span> The Role of Exclusion Restrictions in Heckman’s Model<a class="anchor" aria-label="anchor" href="#the-role-of-exclusion-restrictions-in-heckmans-model"><i class="fas fa-link"></i></a>
</h4>
<p>This example, adapted from the <code>sampleSelection</code>, demonstrates the identification of the sample selection model using simulated data.</p>
<p>We compare two cases:</p>
<ul>
<li><p>One where an <strong>exclusion restriction</strong> is present (i.e., selection and outcome equations use different regressors).</p></li>
<li><p>One where the <strong>same regressor</strong> is used in both equations.</p></li>
</ul>
<hr>
<p><strong>Case 1: With Exclusion Restriction</strong></p>
<div class="sourceCode" id="cb965"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Simulate bivariate normal error terms with correlation -0.7</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span><span class="fl">500</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="op">-</span><span class="fl">0.7</span>,<span class="op">-</span><span class="fl">0.7</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Independent explanatory variable for selection equation</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Selection: Probit model (latent utility model)</span></span>
<span><span class="va">ys_latent</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">ys_latent</span> <span class="op">&gt;</span> <span class="fl">0</span>  <span class="co"># observed participation indicator (TRUE/FALSE)</span></span>
<span></span>
<span><span class="co"># Independent explanatory variable for outcome equation</span></span>
<span><span class="va">xo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Latent outcome variable</span></span>
<span><span class="va">yo_latent</span> <span class="op">&lt;-</span> <span class="va">xo</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Observed outcome: only when selected (ys == TRUE)</span></span>
<span><span class="va">yo</span> <span class="op">&lt;-</span> <span class="va">yo_latent</span> <span class="op">*</span> <span class="va">ys</span></span>
<span></span>
<span><span class="co"># Estimate Heckman's selection model</span></span>
<span><span class="va">model_with_exclusion</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span>selection <span class="op">=</span> <span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, outcome <span class="op">=</span> <span class="va">yo</span> <span class="op">~</span> <span class="va">xo</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_with_exclusion</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 5 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -712.3163 </span></span>
<span><span class="co">#&gt; 500 observations (172 censored and 328 observed)</span></span>
<span><span class="co">#&gt; 6 free parameters (df = 494)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -0.2228     0.1081  -2.061   0.0399 *  </span></span>
<span><span class="co">#&gt; xs            1.3377     0.2014   6.642 8.18e-11 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.0002265  0.1294178  -0.002    0.999    </span></span>
<span><span class="co">#&gt; xo           0.7299070  0.1635925   4.462 1.01e-05 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma   0.9190     0.0574  16.009  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; rho    -0.5392     0.1521  -3.544 0.000431 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p><strong>Key Observations:</strong></p>
<ul>
<li><p>The variables <code>xs</code> and <code>xo</code> are <strong>independent</strong>, fulfilling the <strong>exclusion restriction</strong>.</p></li>
<li><p>The outcome equation is identified not only by the non-linearity of the model but also by the presence of a regressor in the selection equation that is absent from the outcome equation.</p></li>
</ul>
<p>This mirrors realistic scenarios in applied business settings, such as:</p>
<ul>
<li><p>Participation in the labor force driven by family or geographic factors (<code>xs</code>), while wages depend on skills or education (<code>xo</code>).</p></li>
<li><p>Loan application driven by personal risk preferences, while interest rates depend on credit score or income.</p></li>
</ul>
<hr>
<p><strong>Case 2: Without Exclusion Restriction</strong></p>
<div class="sourceCode" id="cb966"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Now use the same regressor (xs) in both equations</span></span>
<span><span class="va">yo_latent2</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">yo_latent2</span> <span class="op">*</span> <span class="va">ys</span></span>
<span></span>
<span><span class="co"># Re-estimate model without exclusion restriction</span></span>
<span><span class="va">model_no_exclusion</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span>selection <span class="op">=</span> <span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, outcome <span class="op">=</span> <span class="va">yo2</span> <span class="op">~</span> <span class="va">xs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_no_exclusion</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 2 model (sample selection model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 14 iterations</span></span>
<span><span class="co">#&gt; Return code 8: successive function values within relative tolerance limit (reltol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -712.8298 </span></span>
<span><span class="co">#&gt; 500 observations (172 censored and 328 observed)</span></span>
<span><span class="co">#&gt; 6 free parameters (df = 494)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -0.1984     0.1114  -1.781   0.0756 .  </span></span>
<span><span class="co">#&gt; xs            1.2907     0.2085   6.191 1.25e-09 ***</span></span>
<span><span class="co">#&gt; Outcome equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   </span></span>
<span><span class="co">#&gt; (Intercept)  -0.5499     0.5644  -0.974  0.33038   </span></span>
<span><span class="co">#&gt; xs            1.3987     0.4482   3.120  0.00191 **</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma  0.85091    0.05352  15.899   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho   -0.13226    0.72684  -0.182    0.856    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p><strong>What changes?</strong></p>
<ul>
<li><p>The estimates remain approximately unbiased because the model is still identified by functional form (non-linear selection mechanism).</p></li>
<li><p>However, the standard errors are substantially larger, leading to less precise inference.</p></li>
</ul>
<p><strong>Why Exclusion Restrictions Matter</strong></p>
<p>The exclusion restriction improves identification by introducing additional variation that affects selection but not the outcome. This is a common recommendation in empirical work:</p>
<ul>
<li><p>In the first case, <code>xs</code> helps explain who is selected, while <code>xo</code> helps explain the outcome, enabling more precise estimates.</p></li>
<li><p>In the second case, all variation comes from <code>xs</code>, meaning the model relies solely on the distributional assumptions (e.g., joint normality of errors) for identification.</p></li>
</ul>
<p><strong>Best Practice (for applied researchers):</strong></p>
<blockquote>
<p>Always try to include at least one variable in the selection equation that does <strong>not appear in the outcome equation</strong>. This helps identify the model and improves estimation precision.</p>
</blockquote>
<hr>
</div>
</div>
<div id="tobit-5-switching-regression-model" class="section level3" number="36.2.5">
<h3>
<span class="header-section-number">36.2.5</span> Tobit-5: Switching Regression Model<a class="anchor" aria-label="anchor" href="#tobit-5-switching-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Tobit-5</strong> model, also known as the <strong>Switching Regression Model</strong>, generalizes Heckman’s sample selection model to allow for two separate outcome equations:</p>
<ul>
<li>One model for participants</li>
<li>A different model for non-participants</li>
</ul>
<p>Assumptions:</p>
<ul>
<li>The selection process determines which outcome equation is observed.</li>
<li>There is at least one variable in the selection equation that does not appear in the outcome equations (i.e., an exclusion restriction), which improves identification.</li>
<li>The errors from all three equations (selection, outcome1, outcome2) are assumed to be jointly normally distributed.</li>
</ul>
<p>This model is especially relevant when selection is endogenous and both groups (selected and unselected) have distinct data-generating processes.</p>
<hr>
<div id="simulated-example-with-exclusion-restriction" class="section level4" number="36.2.5.1">
<h4>
<span class="header-section-number">36.2.5.1</span> Simulated Example: With Exclusion Restriction<a class="anchor" aria-label="anchor" href="#simulated-example-with-exclusion-restriction"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb967"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sampleSelection.org">sampleSelection</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define a 3x3 covariance matrix with positive correlations</span></span>
<span><span class="va">vc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html">diag</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">vc</span><span class="op">[</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/lower.tri.html">lower.tri</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="va">vc</span><span class="op">[</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/lower.tri.html">upper.tri</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">[</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/lower.tri.html">upper.tri</a></span><span class="op">(</span><span class="va">vc</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Generate multivariate normal error terms</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span><span class="fl">500</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, sigma <span class="op">=</span> <span class="va">vc</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Selection equation regressor (xs), uniformly distributed</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Binary selection indicator: ys = 1 if selected</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span></span>
<span></span>
<span><span class="co"># Separate regressors for two regimes (fulfilling exclusion restriction)</span></span>
<span><span class="va">xo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span><span class="va">yo1</span> <span class="op">&lt;-</span> <span class="va">xo1</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>  <span class="co"># Outcome for selected group</span></span>
<span></span>
<span><span class="va">xo2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">xo2</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span>  <span class="co"># Outcome for non-selected group</span></span>
<span></span>
<span><span class="co"># Fit switching regression model</span></span>
<span><span class="va">model_switch</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">yo1</span> <span class="op">~</span> <span class="va">xo1</span>, <span class="va">yo2</span> <span class="op">~</span> <span class="va">xo2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_switch</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 5 model (switching regression model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 11 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -895.8201 </span></span>
<span><span class="co">#&gt; 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)</span></span>
<span><span class="co">#&gt; 10 free parameters (df = 490)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -0.1550     0.1051  -1.474    0.141    </span></span>
<span><span class="co">#&gt; xs            1.1408     0.1785   6.390 3.86e-10 ***</span></span>
<span><span class="co">#&gt; Outcome equation 1:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02708    0.16395   0.165    0.869    </span></span>
<span><span class="co">#&gt; xo1          0.83959    0.14968   5.609  3.4e-08 ***</span></span>
<span><span class="co">#&gt; Outcome equation 2:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   0.1583     0.1885   0.840    0.401    </span></span>
<span><span class="co">#&gt; xo2           0.8375     0.1707   4.908 1.26e-06 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;        Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma1  0.93191    0.09211  10.118   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; sigma2  0.90697    0.04434  20.455   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho1    0.88988    0.05353  16.623   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho2    0.17695    0.33139   0.534    0.594    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>The estimated coefficients are close to the true values (intercept = 0, slope = 1), and the model converges well due to correct specification and valid exclusion restriction.</p>
</div>
<div id="example-functional-form-misspecification" class="section level4" number="36.2.5.2">
<h4>
<span class="header-section-number">36.2.5.2</span> Example: Functional Form Misspecification<a class="anchor" aria-label="anchor" href="#example-functional-form-misspecification"><i class="fas fa-link"></i></a>
</h4>
<p>To demonstrate how <strong>non-normal errors</strong> or skewed distributions affect the model, consider the following:</p>
<div class="sourceCode" id="cb968"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span>, sigma <span class="op">=</span> <span class="va">vc</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Induce skewness: squared errors minus 1 to ensure mean zero</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="va">eps</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="fl">1</span></span>
<span></span>
<span><span class="co"># Generate xs on a skewed interval [-1, 0]</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span></span>
<span></span>
<span><span class="va">xo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">yo1</span> <span class="op">&lt;-</span> <span class="va">xo1</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span></span>
<span><span class="va">xo2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">xo2</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Attempt model estimation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">yo1</span> <span class="op">~</span> <span class="va">xo1</span>, <span class="va">yo2</span> <span class="op">~</span> <span class="va">xo2</span><span class="op">)</span>, iterlim <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 5 model (switching regression model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 12 iterations</span></span>
<span><span class="co">#&gt; Return code 3: Last step could not find a value above the current.</span></span>
<span><span class="co">#&gt; Boundary of parameter space?  </span></span>
<span><span class="co">#&gt; Consider switching to a more robust optimisation method temporarily.</span></span>
<span><span class="co">#&gt; Log-Likelihood: -1695.102 </span></span>
<span><span class="co">#&gt; 1000 observations: 782 selection 1 (FALSE) and 218 selection 2 (TRUE)</span></span>
<span><span class="co">#&gt; 10 free parameters (df = 990)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.660315   0.082477  -8.006  3.3e-15 ***</span></span>
<span><span class="co">#&gt; xs           0.007167   0.088630   0.081    0.936    </span></span>
<span><span class="co">#&gt; Outcome equation 1:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.31351    0.04868   -6.44 1.86e-10 ***</span></span>
<span><span class="co">#&gt; xo1          1.03862    0.08049   12.90  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation 2:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -2.6835     0.2043 -13.132  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; xo2           1.0230     0.1309   7.814 1.41e-14 ***</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;        Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma1  0.70172    0.02000   35.09   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; sigma2  2.49651        NaN     NaN      NaN    </span></span>
<span><span class="co">#&gt; rho1    0.51564    0.04216   12.23   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho2    1.00000        NaN     NaN      NaN    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>Even though the exclusion restriction is preserved, non-normal errors introduce bias in the intercept estimates, and convergence is less reliable. This illustrates how functional form misspecification (i.e., deviations from assumed distributional forms) impacts performance.</p>
</div>
<div id="example-no-exclusion-restriction" class="section level4" number="36.2.5.3">
<h4>
<span class="header-section-number">36.2.5.3</span> Example: No Exclusion Restriction<a class="anchor" aria-label="anchor" href="#example-no-exclusion-restriction"><i class="fas fa-link"></i></a>
</h4>
<p>Here we remove the exclusion restriction by using the same regressor (<code>xs</code>) in both outcome equations.</p>
<div class="sourceCode" id="cb969"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">xs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">ys</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span></span>
<span></span>
<span><span class="va">yo1</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">yo2</span> <span class="op">&lt;-</span> <span class="va">xs</span> <span class="op">+</span> <span class="va">eps</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Fit switching regression without exclusion restriction</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">tmp</span> <span class="op">&lt;-</span></span>
<span>            <span class="fu"><a href="https://rdrr.io/pkg/sampleSelection/man/selection.html">selection</a></span><span class="op">(</span><span class="va">ys</span> <span class="op">~</span> <span class="va">xs</span>, <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">yo1</span> <span class="op">~</span> <span class="va">xs</span>, <span class="va">yo2</span> <span class="op">~</span> <span class="va">xs</span><span class="op">)</span>, iterlim <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span>
<span><span class="co">#&gt; Tobit 5 model (switching regression model)</span></span>
<span><span class="co">#&gt; Maximum Likelihood estimation</span></span>
<span><span class="co">#&gt; Newton-Raphson maximisation, 16 iterations</span></span>
<span><span class="co">#&gt; Return code 1: gradient close to zero (gradtol)</span></span>
<span><span class="co">#&gt; Log-Likelihood: -1879.552 </span></span>
<span><span class="co">#&gt; 1000 observations: 615 selection 1 (FALSE) and 385 selection 2 (TRUE)</span></span>
<span><span class="co">#&gt; 10 free parameters (df = 990)</span></span>
<span><span class="co">#&gt; Probit selection equation:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.33425    0.04280   -7.81 1.46e-14 ***</span></span>
<span><span class="co">#&gt; xs           0.94762    0.07763   12.21  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation 1:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -0.49592    0.06800  -7.293 6.19e-13 ***</span></span>
<span><span class="co">#&gt; xs           0.84530    0.06789  12.450  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; Outcome equation 2:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  </span></span>
<span><span class="co">#&gt; (Intercept)   0.3861     0.4967   0.777   0.4371  </span></span>
<span><span class="co">#&gt; xs            0.6254     0.3322   1.882   0.0601 .</span></span>
<span><span class="co">#&gt;    Error terms:</span></span>
<span><span class="co">#&gt;        Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; sigma1  0.61693    0.02054  30.029   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; sigma2  1.59059    0.05745  27.687   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; rho1    0.19981    0.15863   1.260    0.208    </span></span>
<span><span class="co">#&gt; rho2   -0.01259    0.29339  -0.043    0.966    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; --------------------------------------------</span></span></code></pre></div>
<p>This model may fail to converge or produce biased estimates, especially for the intercepts. Even if it does converge, the reliability of the inference is questionable due to weak identification from using the same regressor across all equations.</p>
<p><strong>Notes on Estimation and Convergence</strong></p>
<p>The log-likelihood function of switching regression models is not globally concave, so the estimation process may:</p>
<ul>
<li><p>Fail to converge</p></li>
<li><p>Converge to a local maximum</p></li>
</ul>
<p>Practical Tips:</p>
<ul>
<li><p>Try different starting values or random seeds</p></li>
<li><p>Use alternative maximization algorithms (e.g., <code>optim</code> control)</p></li>
<li><p>Consider rescaling variables or centering predictors</p></li>
<li><p>Refer to <a href="non-linear-regression.html#non-linear-regression">Non-Linear Regression</a> for advanced diagnostics</p></li>
</ul>
<p>Model Comparison Summary</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="32%">
<col width="24%">
<col width="13%">
<col width="18%">
<col width="10%">
</colgroup>
<thead><tr class="header">
<th>Scenario</th>
<th>Exclusion Restriction</th>
<th>Convergence</th>
<th>Bias</th>
<th>Variance</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Well-specified with exclusion</td>
<td>Yes</td>
<td>Likely</td>
<td>No</td>
<td>Low</td>
</tr>
<tr class="even">
<td>Misspecified distribution</td>
<td>Yes</td>
<td>Risky</td>
<td>Yes (intercepts)</td>
<td>Moderate</td>
</tr>
<tr class="odd">
<td>No exclusion restriction</td>
<td>No</td>
<td>Often fails</td>
<td>Yes</td>
<td>High</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="sec-pattern-mixture-models" class="section level3" number="36.2.6">
<h3>
<span class="header-section-number">36.2.6</span> Pattern-Mixture Models<a class="anchor" aria-label="anchor" href="#sec-pattern-mixture-models"><i class="fas fa-link"></i></a>
</h3>
<p>In the context of endogenous sample selection, one of the central challenges is modeling the joint distribution of the outcome and the selection mechanism when data are <a href="imputation-missing-data.html#missing-not-at-random-mnar">Missing Not At Random (MNAR)</a>. In this framework, the probability that an outcome is observed may depend on unobserved values of that outcome, making the missingness mechanism <strong>nonignorable</strong>.</p>
<p>Previously, we discussed the <strong>selection model</strong> approach (e.g., Heckman’s Tobit-2 model), which factorizes the joint distribution as:</p>
<p><span class="math display">\[
\mathbb{P}(Y, R) = \mathbb{P}(Y) \cdot \mathbb{P}(R \mid Y),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> is the outcome of interest,</p></li>
<li><p><span class="math inline">\(R\)</span> is the response indicator (with <span class="math inline">\(R = 1\)</span> if <span class="math inline">\(Y\)</span> is observed, <span class="math inline">\(R = 0\)</span> otherwise).</p></li>
</ul>
<p>This approach models the selection process explicitly via <span class="math inline">\(\mathbb{P}(R \mid Y)\)</span>, often using a parametric model such as the probit.</p>
<p>However, the <strong>pattern-mixture model (PMM)</strong> offers an alternative and equally valid factorization:</p>
<p><span class="math display">\[
\mathbb{P}(Y, R) = \mathbb{P}(Y \mid R) \cdot \mathbb{P}(R),
\]</span></p>
<p>which decomposes the joint distribution by conditioning on the response pattern. This approach is particularly advantageous when the selection mechanism is complex, or when interest lies in modeling how the outcome distribution varies across response strata.</p>
<hr>
<div id="definition-of-the-pattern-mixture-model" class="section level4" number="36.2.6.1">
<h4>
<span class="header-section-number">36.2.6.1</span> Definition of the Pattern-Mixture Model<a class="anchor" aria-label="anchor" href="#definition-of-the-pattern-mixture-model"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\((Y_i, R_i)\)</span> for <span class="math inline">\(i = 1, \dots, n\)</span> denote the observed data, where:</p>
<ul>
<li>
<span class="math inline">\(Y_i \in \mathbb{R}^p\)</span> is the multivariate outcome of interest,</li>
<li>
<span class="math inline">\(R_i \in \{0,1\}^p\)</span> is a missingness pattern indicator vector, where <span class="math inline">\(R_{ij} = 1\)</span> indicates that <span class="math inline">\(Y_{ij}\)</span> is observed.</li>
</ul>
<p>Define <span class="math inline">\(\mathcal{R}\)</span> as the finite set of all possible response patterns. For each pattern <span class="math inline">\(r \in \mathcal{R}\)</span>, partition the outcome vector <span class="math inline">\(Y\)</span> into:</p>
<ul>
<li>
<span class="math inline">\(Y_{(r)}\)</span>: observed components of <span class="math inline">\(Y\)</span> under pattern <span class="math inline">\(r\)</span>,</li>
<li>
<span class="math inline">\(Y_{(\bar{r})}\)</span>: missing components of <span class="math inline">\(Y\)</span> under pattern <span class="math inline">\(r\)</span>.</li>
</ul>
<p>The joint distribution can then be factorized according to the pattern-mixture model as:</p>
<p><span class="math display">\[
f(Y, R) = f(Y \mid R = r) \cdot \mathbb{P}(R = r), \quad r \in \mathcal{R}.
\]</span></p>
<p>The marginal distribution of <span class="math inline">\(Y\)</span> is obtained by summing over all response patterns:</p>
<p><span class="math display">\[
f(Y) = \sum_{r \in \mathcal{R}} f(Y \mid R = r) \cdot \mathbb{P}(R = r).
\]</span></p>
<p>In the simplest case of binary response patterns (complete responders <span class="math inline">\(R=1\)</span> and complete nonresponders <span class="math inline">\(R=0\)</span>), this reduces to:</p>
<p><span class="math display">\[
\mathbb{P}(Y) = \mathbb{P}(Y \mid R = 1) \cdot \mathbb{P}(R = 1) + \mathbb{P}(Y \mid R = 0) \cdot \mathbb{P}(R = 0).
\]</span></p>
<p>Thus, the overall distribution of <span class="math inline">\(Y\)</span> is explicitly treated as a <strong>mixture</strong> of distributions for responders and nonresponders. While <span class="math inline">\(\mathbb{P}(Y \mid R = 1)\)</span> can be directly estimated from observed data, <span class="math inline">\(\mathbb{P}(Y \mid R = 0)\)</span> cannot be identified from data alone, necessitating additional assumptions or sensitivity parameters for its estimation.</p>
<p>Common approaches for addressing non-identifiability involve defining plausible deviations from <span class="math inline">\(\mathbb{P}(Y \mid R = 1)\)</span> using specific sensitivity parameters such as:</p>
<ul>
<li>
<strong>Shift Bias</strong>: Adjusting imputed values by a constant <span class="math inline">\(\delta\)</span> to reflect systematic differences in nonresponders.</li>
<li>
<strong>Scale Bias</strong>: Scaling imputed values to account for differing variability.</li>
<li>
<strong>Shape Bias</strong>: Reweighting imputations, for example, using methods like Selection-Indicator Reweighting (SIR), to capture distributional shape differences.</li>
</ul>
<p>In this discussion, primary focus is placed on the shift parameter <span class="math inline">\(\delta\)</span>, hypothesizing that nonresponse shifts the mean of <span class="math inline">\(Y\)</span> by <span class="math inline">\(\delta\)</span> units.</p>
<p>In practice, since only observed components <span class="math inline">\(Y_{(r)}\)</span> are available for individuals with missing data, modeling the full conditional distribution <span class="math inline">\(f(Y \mid R = r)\)</span> requires extrapolating from observed to unobserved components.</p>
<hr>
</div>
<div id="modeling-strategy-and-identifiability" class="section level4" number="36.2.6.2">
<h4>
<span class="header-section-number">36.2.6.2</span> Modeling Strategy and Identifiability<a class="anchor" aria-label="anchor" href="#modeling-strategy-and-identifiability"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose the full-data conditional distribution is parameterized by pattern-specific parameters <span class="math inline">\(\theta_r\)</span> for each response pattern <span class="math inline">\(r \in \mathcal{R}\)</span>:</p>
<p><span class="math display">\[
f(Y \mid R = r; \theta_r), \quad \text{with } \theta = \{\theta_r : r \in \mathcal{R}\}.
\]</span></p>
<p>Then, the marginal distribution of <span class="math inline">\(Y\)</span> can be expressed as:</p>
<p><span class="math display">\[
f(Y; \theta, \psi) = \sum_{r \in \mathcal{R}} f(Y \mid R = r; \theta_r) \cdot \mathbb{P}(R = r; \psi),
\]</span></p>
<p>where <span class="math inline">\(\psi\)</span> parameterizes the distribution of response patterns.</p>
<p>Important points about identifiability include:</p>
<ul>
<li>The model is inherently non-identifiable without explicit assumptions regarding the distribution of missing components <span class="math inline">\(Y_{(\bar{r})}\)</span>.</li>
<li>Estimating <span class="math inline">\(f(Y_{(\bar{r})} \mid Y_{(r)}, R = r)\)</span> requires external information, expert knowledge, or assumptions encapsulated in sensitivity parameters.</li>
</ul>
<p>The conditional density for each response pattern can be further decomposed as:</p>
<p><span class="math display">\[
f(Y \mid R = r) = f(Y_{(r)} \mid R = r) \cdot f(Y_{(\bar{r})} \mid Y_{(r)}, R = r).
\]</span></p>
<ul>
<li>The first term <span class="math inline">\(f(Y_{(r)} \mid R = r)\)</span> is estimable directly from observed data.</li>
<li>The second term <span class="math inline">\(f(Y_{(\bar{r})} \mid Y_{(r)}, R = r)\)</span> cannot be identified from observed data alone and must rely on assumptions or additional sensitivity analysis.</li>
</ul>
<hr>
</div>
<div id="location-shift-models" class="section level4" number="36.2.6.3">
<h4>
<span class="header-section-number">36.2.6.3</span> Location Shift Models<a class="anchor" aria-label="anchor" href="#location-shift-models"><i class="fas fa-link"></i></a>
</h4>
<p>A widely used strategy in PMMs is to introduce a <strong>location shift</strong> for the unobserved outcomes. Specifically, assume that:</p>
<p><span class="math display">\[
Y_{(\bar{r})} \mid Y_{(r)}, R = r \sim \mathcal{L}(Y_{(\bar{r})} \mid Y_{(r)}, R = r = r^*) + \delta_r,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(r^*\)</span> denotes a fully observed (reference) pattern (e.g., <span class="math inline">\(R = 1^p\)</span>),</p></li>
<li><p><span class="math inline">\(\delta_r\)</span> is a vector of <strong>sensitivity parameters</strong> that quantify the mean shift for missing outcomes under pattern <span class="math inline">\(r\)</span>.</p></li>
</ul>
<p>More formally, for each <span class="math inline">\(r \in \mathcal{R}\)</span> and unobserved component <span class="math inline">\(j \in \bar{r}\)</span>, we specify:</p>
<p><span class="math display">\[
\mathbb{E}[Y_j \mid Y_{(r)}, R = r] = \mathbb{E}[Y_j \mid Y_{(r)}, R = r^*] + \delta_{rj}.
\]</span></p>
<p>Under this framework:</p>
<ul>
<li>Setting <span class="math inline">\(\delta_r = 0\)</span> corresponds to the MAR assumption (i.e., ignorability).</li>
<li>Nonzero <span class="math inline">\(\delta_r\)</span> introduces controlled deviations from MAR and enables sensitivity analysis.</li>
</ul>
<hr>
</div>
<div id="sensitivity-analysis-in-pattern-mixture-models" class="section level4" number="36.2.6.4">
<h4>
<span class="header-section-number">36.2.6.4</span> Sensitivity Analysis in Pattern-Mixture Models<a class="anchor" aria-label="anchor" href="#sensitivity-analysis-in-pattern-mixture-models"><i class="fas fa-link"></i></a>
</h4>
<p>To evaluate the robustness of inferences to the missing data mechanism, we perform sensitivity analysis by varying <span class="math inline">\(\delta_r\)</span> over plausible ranges.</p>
<p><strong>Bias in Estimation</strong></p>
<p>Let <span class="math inline">\(\mu = \mathbb{E}[Y]\)</span> be the target estimand. Under the pattern-mixture decomposition:</p>
<p><span class="math display">\[
\mu = \sum_{r \in \mathcal{R}} \mathbb{E}[Y \mid R = r] \cdot \mathbb{P}(R = r),
\]</span></p>
<p>and under the shift model:</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid R = r] = \tilde{\mu}_r + \delta_r^{*},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\tilde{\mu}_r\)</span> is the mean computed using observed data (e.g., via imputation under MAR),</p></li>
<li><p><span class="math inline">\(\delta_r^{*}\)</span> is a vector with entries equal to the mean shifts <span class="math inline">\(\delta_{rj}\)</span> for missing components and 0 for observed components.</p></li>
</ul>
<p>Therefore, the overall bias induced by nonzero shifts is:</p>
<p><span class="math display">\[
\Delta = \sum_{r \in \mathcal{R}} \delta_r^{*} \cdot \mathbb{P}(R = r).
\]</span></p>
<p>This highlights how the overall expectation depends linearly on the sensitivity parameters and their associated pattern probabilities.</p>
<p>Practical Recommendations</p>
<ul>
<li>Use subject-matter knowledge to specify plausible ranges for <span class="math inline">\(\delta_r\)</span>.</li>
<li>Consider standard increments (e.g., <span class="math inline">\(\pm 0.2\)</span> SD units) or domain-specific metrics (e.g., <span class="math inline">\(\pm 5\)</span> P/E for valuation).</li>
<li>If the dataset is large and the number of missingness patterns is small (e.g., monotone dropout), more detailed pattern-specific modeling is feasible.</li>
</ul>
<hr>
</div>
<div id="generalization-to-multivariate-and-longitudinal-data" class="section level4" number="36.2.6.5">
<h4>
<span class="header-section-number">36.2.6.5</span> Generalization to Multivariate and Longitudinal Data<a class="anchor" aria-label="anchor" href="#generalization-to-multivariate-and-longitudinal-data"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose <span class="math inline">\(Y = (Y_1, \dots, Y_T)\)</span> is a longitudinal outcome. Let <span class="math inline">\(D \in \{1, \dots, T+1\}\)</span> denote the dropout time, where <span class="math inline">\(D = t\)</span> implies observation up to time <span class="math inline">\(t-1\)</span>.</p>
<p>Then the pattern-mixture factorization becomes:</p>
<p><span class="math display">\[
f(Y, D) = f(Y \mid D = t) \cdot \mathbb{P}(D = t), \quad t = 2, \dots, T+1.
\]</span></p>
<p>Assume a parametric model:</p>
<p><span class="math display">\[
Y_j \mid D = t \sim \mathcal{N}(\beta_{0t} + \beta_{1t} t_j, \sigma_t^2), \quad \text{for } j = 1, \dots, T.
\]</span></p>
<p>Then the overall mean trajectory becomes:</p>
<p><span class="math display">\[
\mathbb{E}[Y_j] = \sum_{t=2}^{T+1} (\beta_{0t} + \beta_{1t} t_j) \cdot \mathbb{P}(D = t).
\]</span></p>
<p>This model allows for dropout-pattern-specific trajectories and can flexibly account for deviations in the distributional shape due to dropout.</p>
<hr>
</div>
<div id="comparison-with-selection-models" class="section level4" number="36.2.6.6">
<h4>
<span class="header-section-number">36.2.6.6</span> Comparison with Selection Models<a class="anchor" aria-label="anchor" href="#comparison-with-selection-models"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="26%">
<col width="36%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Selection Model</th>
<th>Pattern-Mixture Model</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Factorization</td>
<td><span class="math inline">\(\mathbb{P}(Y) \cdot \mathbb{P}(R \mid Y)\)</span></td>
<td><span class="math inline">\(\mathbb{P}(Y \mid R) \cdot \mathbb{P}(R)\)</span></td>
</tr>
<tr class="even">
<td>Target of modeling</td>
<td>Missingness process <span class="math inline">\(\mathbb{P}(R \mid Y)\)</span>
</td>
<td>Distribution of <span class="math inline">\(Y\)</span> under <span class="math inline">\(R\)</span>
</td>
</tr>
<tr class="odd">
<td>Assumption for identifiability</td>
<td>MAR (often Probit)</td>
<td>Extrapolation (e.g., shift <span class="math inline">\(\delta\)</span>)</td>
</tr>
<tr class="even">
<td>Modeling burden</td>
<td>On <span class="math inline">\(\mathbb{P}(R \mid Y)\)</span>
</td>
<td>On <span class="math inline">\(f(Y \mid R)\)</span>
</td>
</tr>
<tr class="odd">
<td>Sensitivity analysis</td>
<td>Less interpretable</td>
<td>Easily parameterized (via <span class="math inline">\(\delta\)</span>)</td>
</tr>
</tbody>
</table></div>
<div class="sourceCode" id="cb970"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/amices/mice">mice</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li><strong>Simulate Longitudinal Dropout Data</strong></li>
</ol>
<p>We simulate a 3-time-point longitudinal outcome, where dropout depends on unobserved future outcomes, i.e., MNAR.</p>
<div class="sourceCode" id="cb971"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Parameters</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">time</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">5</span><span class="op">)</span>  <span class="co"># intercept and slope</span></span>
<span><span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span><span class="va">dropout_bias</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">10</span>  <span class="co"># MNAR: lower future Y -&gt; more dropout</span></span>
<span></span>
<span><span class="co"># Simulate full data (Y1, Y2, Y3)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="va">n</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">Y</span><span class="op">[</span>, <span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">beta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">time</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="va">sigma</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Dropout mechanism: higher chance to drop if Y3 is low</span></span>
<span><span class="va">prob_dropout</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="op">(</span><span class="va">dropout_bias</span> <span class="op">-</span> <span class="va">Y</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">drop</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, <span class="va">prob_dropout</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define dropout time: if drop == 1, then Y3 is missing</span></span>
<span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">1</span>, nrow <span class="op">=</span> <span class="va">n</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">R</span><span class="op">[</span><span class="va">drop</span> <span class="op">==</span> <span class="fl">1</span>, <span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span></span>
<span><span class="co"># Observed data</span></span>
<span><span class="va">Y_obs</span> <span class="op">&lt;-</span> <span class="va">Y</span></span>
<span><span class="va">Y_obs</span><span class="op">[</span><span class="va">R</span> <span class="op">==</span> <span class="fl">0</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span></span>
<span><span class="co"># Combine into a data frame</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>        id <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span>,</span>
<span>        Y1 <span class="op">=</span> <span class="va">Y_obs</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>,</span>
<span>        Y2 <span class="op">=</span> <span class="va">Y_obs</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>,</span>
<span>        Y3 <span class="op">=</span> <span class="va">Y_obs</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span>,</span>
<span>        R3 <span class="op">=</span> <span class="va">R</span><span class="op">[</span>, <span class="fl">3</span><span class="op">]</span></span>
<span>    <span class="op">)</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li><strong>Fit Pattern-Mixture Model with Delta Adjustment</strong></li>
</ol>
<p>We model <span class="math inline">\(Y_3\)</span> as a function of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>, using the observed cases (complete cases), and create multiple imputed datasets under various shift parameters <span class="math inline">\(\delta\)</span> to represent different MNAR scenarios.</p>
<div class="sourceCode" id="cb972"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Base linear model for Y3 ~ Y1 + Y2</span></span>
<span><span class="va">model_cc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y3</span> <span class="op">~</span> <span class="va">Y1</span> <span class="op">+</span> <span class="va">Y2</span>, data <span class="op">=</span> <span class="va">df</span>, subset <span class="op">=</span> <span class="va">R3</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predict for missing Y3s</span></span>
<span><span class="va">preds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model_cc</span>, newdata <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sensitivity values for delta</span></span>
<span><span class="va">delta_seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="op">-</span><span class="fl">2.5</span>, <span class="op">-</span><span class="fl">5</span>, <span class="op">-</span><span class="fl">7.5</span>, <span class="op">-</span><span class="fl">10</span><span class="op">)</span>  <span class="co"># in units of Y3</span></span>
<span></span>
<span><span class="co"># Perform delta adjustment</span></span>
<span><span class="va">imputed_dfs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/lapply.html">lapply</a></span><span class="op">(</span><span class="va">delta_seq</span>, <span class="kw">function</span><span class="op">(</span><span class="va">delta</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">df_new</span> <span class="op">&lt;-</span> <span class="va">df</span></span>
<span>  <span class="va">df_new</span><span class="op">$</span><span class="va">Y3_imp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">Y3</span><span class="op">)</span>,</span>
<span>                          <span class="va">preds</span> <span class="op">+</span> <span class="va">delta</span>,</span>
<span>                          <span class="va">df</span><span class="op">$</span><span class="va">Y3</span><span class="op">)</span></span>
<span>  <span class="va">df_new</span><span class="op">$</span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="va">delta</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">df_new</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stack all results</span></span>
<span><span class="va">df_imp_all</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span><span class="va">imputed_dfs</span><span class="op">)</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li><strong>Estimate Full-Data Mean and Trajectory under Each Delta</strong></li>
</ol>
<p>We now estimate the mean of <span class="math inline">\(Y_3\)</span> and the full outcome trajectory across delta values.</p>
<div class="sourceCode" id="cb973"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">estimates</span> <span class="op">&lt;-</span> <span class="va">df_imp_all</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span></span>
<span>        mu_Y1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y1</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>        mu_Y2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y2</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>        mu_Y3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y3_imp</span><span class="op">)</span>,</span>
<span>        .groups <span class="op">=</span> <span class="st">"drop"</span></span>
<span>    <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span>cols <span class="op">=</span> <span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"mu"</span><span class="op">)</span>,</span>
<span>                 names_to <span class="op">=</span> <span class="st">"Time"</span>,</span>
<span>                 values_to <span class="op">=</span> <span class="st">"Mean"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>Time <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span></span>
<span>        <span class="va">Time</span>,</span>
<span>        levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"mu_Y1"</span>, <span class="st">"mu_Y2"</span>, <span class="st">"mu_Y3"</span><span class="op">)</span>,</span>
<span>        labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Time 1"</span>, <span class="st">"Time 2"</span>, <span class="st">"Time 3"</span><span class="op">)</span></span>
<span>    <span class="op">)</span><span class="op">)</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li><strong>Visualization: Sensitivity Analysis</strong></li>
</ol>
<p>We visualize how the mean trajectory changes across different sensitivity parameters <span class="math inline">\(\delta\)</span>.</p>
<div class="sourceCode" id="cb974"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">estimates</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">Time</span>,</span>
<span>    y <span class="op">=</span> <span class="va">Mean</span>,</span>
<span>    color <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span>,</span>
<span>    group <span class="op">=</span> <span class="va">delta</span></span>
<span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_color_viridis_d</a></span><span class="op">(</span>name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Pattern-Mixture Model Sensitivity Analysis"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Mean of Y_t"</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"Time"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">causalverse</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/causalverse/man/ama_theme.html">ama_theme</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="36-endogeneity_files/figure-html/unnamed-chunk-21-1.png" width="90%" style="display: block; margin: auto;"></div>
<ol start="5" style="list-style-type: decimal">
<li><strong>Sensitivity Table for Reporting</strong></li>
</ol>
<p>This table quantifies the change in <span class="math inline">\(\mu_{Y3}\)</span> across sensitivity scenarios.</p>
<div class="sourceCode" id="cb975"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_summary</span> <span class="op">&lt;-</span> <span class="va">df_imp_all</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">delta</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span></span>
<span>        Mean_Y3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">Y3_imp</span><span class="op">)</span>,</span>
<span>        SD_Y3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">Y3_imp</span><span class="op">)</span>,</span>
<span>        N_Missing <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">Y3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        N_Observed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">Y3</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/kable.html">kable</a></span><span class="op">(</span><span class="va">df_summary</span>, digits <span class="op">=</span> <span class="fl">2</span>,</span>
<span>             caption <span class="op">=</span> <span class="st">"Sensitivity of Estimated Mean of Y3 to Delta Adjustments"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:unnamed-chunk-22">Table 36.1: </span>Sensitivity of Estimated Mean of Y3 to Delta Adjustments</caption>
<thead><tr class="header">
<th align="right">delta</th>
<th align="right">Mean_Y3</th>
<th align="right">SD_Y3</th>
<th align="right">N_Missing</th>
<th align="right">N_Observed</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">-10.0</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="even">
<td align="right">-7.5</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="odd">
<td align="right">-5.0</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="even">
<td align="right">-2.5</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">110.6</td>
<td align="right">4.75</td>
<td align="right">0</td>
<td align="right">100</td>
</tr>
</tbody>
</table></div>
<ol start="6" style="list-style-type: decimal">
<li><strong>Interpretation and Discussion</strong></li>
</ol>
<ul>
<li><p>The estimated mean of <span class="math inline">\(Y_3\)</span> decreases linearly with increasingly negative <span class="math inline">\(\delta\)</span>, as expected.</p></li>
<li><p>This illustrates the <strong>non-identifiability</strong> of the distribution of missing data without unverifiable assumptions.</p></li>
<li><p>The results provide a <strong>tipping point analysis</strong>: at what <span class="math inline">\(\delta\)</span> does inference about the mean (or treatment effect, in a causal study) substantially change?</p></li>
</ul>
</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></div>
<div class="next"><a href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-endogeneity"><span class="header-section-number">36</span> Endogeneity</a></li>
<li>
<a class="nav-link" href="#sec-endogenous-treatment"><span class="header-section-number">36.1</span> Endogenous Treatment</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-measurement-error"><span class="header-section-number">36.1.1</span> Measurement Errors</a></li>
<li><a class="nav-link" href="#sec-simultaneity"><span class="header-section-number">36.1.2</span> Simultaneity</a></li>
<li><a class="nav-link" href="#sec-reverse-causality"><span class="header-section-number">36.1.3</span> Reverse Causality</a></li>
<li><a class="nav-link" href="#sec-omitted-variable-bias"><span class="header-section-number">36.1.4</span> Omitted Variable Bias</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-endogenous-sample-selection"><span class="header-section-number">36.2</span> Endogenous Sample Selection</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#unifying-model-frameworks"><span class="header-section-number">36.2.1</span> Unifying Model Frameworks</a></li>
<li><a class="nav-link" href="#estimation-methods-2"><span class="header-section-number">36.2.2</span> Estimation Methods</a></li>
<li><a class="nav-link" href="#theoretical-connections"><span class="header-section-number">36.2.3</span> Theoretical Connections</a></li>
<li><a class="nav-link" href="#tobit-2-heckmans-sample-selection-model"><span class="header-section-number">36.2.4</span> Tobit-2: Heckman’s Sample Selection Model</a></li>
<li><a class="nav-link" href="#tobit-5-switching-regression-model"><span class="header-section-number">36.2.5</span> Tobit-5: Switching Regression Model</a></li>
<li><a class="nav-link" href="#sec-pattern-mixture-models"><span class="header-section-number">36.2.6</span> Pattern-Mixture Models</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/36-endogeneity.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/36-endogeneity.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-05-14.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
