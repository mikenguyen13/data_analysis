<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Linear Regression | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Linear regression is one of the most fundamental tools in statistics and econometrics, widely used for modeling relationships between variables. It forms the cornerstone of predictive analysis,...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="Chapter 5 Linear Regression | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/linear-regression.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Linear regression is one of the most fundamental tools in statistics and econometrics, widely used for modeling relationships between variables. It forms the cornerstone of predictive analysis,...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Linear Regression | A Guide on Data Analysis">
<meta name="twitter:description" content="Linear regression is one of the most fundamental tools in statistics and econometrics, widely used for modeling relationships between variables. It forms the cornerstone of predictive analysis,...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="active" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">39</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="linear-regression" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Linear Regression<a class="anchor" aria-label="anchor" href="#linear-regression"><i class="fas fa-link"></i></a>
</h1>
<div style="text-align:center">
<div class="inline-figure"><img src="images/econometrics.PNG" width="450" height="200"></div>
</div>
<p>Linear regression is one of the most fundamental tools in statistics and econometrics, widely used for modeling relationships between variables. It forms the cornerstone of predictive analysis, enabling us to understand and quantify how changes in one or more explanatory variables are associated with a dependent variable. Its simplicity and versatility make it an essential tool in fields ranging from economics and marketing to healthcare and environmental studies.</p>
<p>At its core, linear regression addresses questions about associations rather than causation. For example:</p>
<ul>
<li>How are advertising expenditures associated with sales performance?</li>
<li>What is the relationship between a company’s revenue and its stock price?</li>
<li>How does the level of education correlate with income?</li>
</ul>
<p>These questions are about patterns in data—not necessarily causal effects. While regression can provide insights into potential causal relationships, establishing causality requires more than just regression analysis. It requires careful consideration of the study design, assumptions, and potential confounding factors.</p>
<p>So, why is it called “linear”? The term refers to the structure of the model, where the dependent variable (outcome) is modeled as a linear combination of one or more independent variables (predictors). For example, in simple linear regression, the relationship is represented as:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon,\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is the dependent variable, <span class="math inline">\(X\)</span> is the independent variable, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are parameters to be estimated, and <span class="math inline">\(\epsilon\)</span> is the error term capturing randomness or unobserved factors.</p>
<p>Linear regression serves as a foundation for much of applied data analysis because of its wide-ranging applications:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Understanding Patterns in Data:</strong> Regression provides a framework to summarize and explore relationships between variables. It allows us to identify patterns such as trends or associations, which can guide further analysis or decision-making.</p></li>
<li><p><strong>Prediction:</strong> Beyond exploring relationships, regression is widely used for making predictions. For instance, given historical data, we can use a regression model to predict future outcomes like sales, prices, or demand.</p></li>
<li><p><strong>Building Blocks for Advanced Techniques:</strong> Linear regression is foundational for many advanced statistical and machine learning models, such as logistic regression, ridge regression, and neural networks. Mastering linear regression equips you with the skills to tackle more complex methods.</p></li>
</ol>
<p><strong>Regression and Causality: A Crucial Distinction</strong></p>
<p>It’s essential to remember that regression alone does not establish causation. For instance, a regression model might show a strong association between advertising and sales, but this does not prove that advertising directly causes sales to increase. Other factors—such as seasonality, market trends, or unobserved variables—could also influence the results.</p>
<p>Establishing causality requires additional steps, such as controlled experiments, instrumental variable techniques, or careful observational study designs. As we work through the details of linear regression, we’ll revisit this distinction and highlight scenarios where causality might or might not be inferred.</p>
<hr>
<p><strong>What is an Estimator?</strong></p>
<p>At the heart of regression lies the process of <strong>estimation</strong>—the act of using data to determine the unknown characteristics of a population or model.</p>
<p>An <strong>estimator</strong> is a mathematical rule or formula used to calculate an estimate of an unknown quantity based on observed data. For example, when we calculate the average height of a sample to estimate the average height of a population, the sample mean is the estimator.</p>
<p>In the context of regression, the quantities we typically estimate are:</p>
<ul>
<li>
<strong>Parameters</strong>: Fixed, unknown values that describe the relationship between variables (e.g., coefficients in a regression equation).
<ul>
<li>
<strong>Estimating parameters</strong> → Parametric models (finite parameters, e.g., coefficients in regression).</li>
</ul>
</li>
<li>
<strong>Functions</strong>: Unknown relationships or patterns in the data, often modeled without assuming a fixed functional form.
<ul>
<li>
<strong>Estimating functions</strong> → Non-parametric models (focus on shapes or trends, not a fixed number of parameters).</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Types of Estimators</strong></p>
<p>To better understand the estimation process, let’s introduce two broad categories of estimators that we’ll work with:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Parametric Estimators</strong><br>
Parametric estimation focuses on a finite set of parameters that define a model. For example, in a simple linear regression:<br><span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon,\]</span><br>
the task is to estimate the parameters <span class="math inline">\(\beta_0\)</span> (intercept) and <span class="math inline">\(\beta_1\)</span> (slope). Parametric estimators rely on specific assumptions about the form of the model (e.g., linearity) and the distribution of the error term (e.g., normality).</p></li>
<li><p><strong>Non-Parametric Estimators</strong><br>
Non-parametric estimation avoids assuming a specific functional form for the relationship between variables. Instead, it focuses on estimating patterns or trends directly from the data. For example, using a scatterplot smoothing technique to visualize how sales vary with advertising spend without imposing a linear or quadratic relationship.</p></li>
</ol>
<p>These two categories reflect a fundamental trade-off in statistical analysis: <strong>parametric models</strong> are often simpler and more interpretable but require strong assumptions, while <strong>non-parametric models</strong> are more flexible but may require more data and computational resources.</p>
<hr>
<p><strong>Desirable Properties of Estimators</strong></p>
<p>Regardless of whether we are estimating parameters or functions, we want our estimators to possess certain desirable properties. Think of these as the “golden standards” that help us judge whether an estimator is reliable:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Unbiasedness</strong><br>
An estimator is unbiased if it hits the true value of the parameter, on average, over repeated samples. Mathematically:<br><span class="math display">\[E[\hat{\beta}] = \beta.\]</span><br>
This means that, across multiple samples, the estimator does not systematically overestimate or underestimate the true parameter.</p></li>
<li><p><strong>Consistency</strong><br>
Consistency ensures that as the sample size increases, the estimator converges to the true value of the parameter. Formally:<br><span class="math display">\[plim\ \hat{\beta_n} = \beta.\]</span><br>
This property relies on the <a href="prerequisites.html#law-of-large-numbers">Law of Large Numbers</a>, which guarantees that larger samples reduce random fluctuations, leading to more precise estimates.</p></li>
<li>
<p><strong>Efficiency</strong><br>
Among all unbiased estimators, an efficient estimator has the smallest variance.</p>
<ul>
<li>The <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> method is efficient because it is the <strong>Best Linear Unbiased Estimator (BLUE)</strong> under the <a href="linear-regression.html#gauss-markov-theorem">Gauss-Markov Theorem</a>.</li>
<li>For estimators that meet specific distributional assumptions (e.g., normality), <strong>Maximum Likelihood Estimators (MLE)</strong> are asymptotically efficient, meaning they achieve the lowest possible variance as the sample size grows.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Why These Properties Matter</strong></p>
<p>Understanding these properties is crucial because they ensure that the methods we use for estimation are reliable, precise, and robust. Whether we are estimating coefficients in a regression model or uncovering a complex pattern in the data, these properties provide the foundation for statistical inference and decision-making.</p>
<p>Now that we’ve established what estimators are, the types we’ll encounter, and their desirable properties, we can move on to understanding how these concepts apply specifically to the <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> method—the backbone of linear regression.</p>
<p>Reference Table</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="11%">
<col width="32%">
<col width="28%">
<col width="26%">
</colgroup>
<thead><tr class="header">
<th><strong>Estimator</strong></th>
<th><strong>Key Assumptions</strong></th>
<th><strong>Strengths</strong></th>
<th><strong>Limitations</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a></td>
<td>
<p>Errors are independent, identically distributed (i.i.d.) with mean 0 and constant variance.</p>
<p>Linear relationship between predictors and response.</p>
</td>
<td>
<p>Simple, well-understood method.</p>
<p>Minimizes residual sum of squares (easy to interpret coefficients).</p>
</td>
<td>
<p>Sensitive to outliers and violations of normality.</p>
<p>Can perform poorly if predictors are highly correlated (multicollinearity).</p>
</td>
</tr>
<tr class="even">
<td><a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a></td>
<td>Errors have a known correlation structure or heteroscedasticity structure that can be modeled.</td>
<td>
<p>Handles correlated or non-constant-variance errors.</p>
<p>More flexible than OLS when noise structure is known.</p>
</td>
<td>
<p>Requires specifying (or estimating) the error covariance structure.</p>
<p>Misspecification can lead to biased estimates.</p>
</td>
</tr>
<tr class="odd">
<td><a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a></td>
<td>Underlying probability distribution (e.g., normal) must be specified correctly.</td>
<td>
<p>Provides a general framework for estimating parameters under well-defined probability models.</p>
<p>Can extend to complex likelihoods.</p>
</td>
<td>
<p>Highly sensitive to model misspecification.</p>
<p>May require more computation than OLS or GLS.</p>
</td>
</tr>
<tr class="even">
<td><a href="linear-regression.html#penalized-regularized-estimators">Penalized (Regularized) Estimators</a></td>
<td>Coefficients assumed to be shrinkable; model typically allows coefficient penalization.</td>
<td>
<p>Controls overfitting via regularization.</p>
<p>Handles high-dimensional data or many predictors.</p>
<p>Can perform feature selection (e.g., Lasso).</p>
</td>
<td>
<p>Requires choosing tuning parameter(s) (e.g., λ).</p>
<p>Interpretation of coefficients becomes less straightforward.</p>
</td>
</tr>
<tr class="odd">
<td><a href="linear-regression.html#robust-estimators">Robust Estimators</a></td>
<td>Less sensitive to heavy-tailed or outlier-prone distributions (weaker assumptions on the error structure).</td>
<td>
<p>Resistant to large deviations or outliers in data.</p>
<p>Often maintains good performance under mild model misspecifications.</p>
</td>
<td>
<p>Less efficient if errors are truly normal.</p>
<p>Choice of robust method and tuning can be subjective.</p>
</td>
</tr>
<tr class="even">
<td><a href="linear-regression.html#partial-least-squares">Partial Least Squares</a></td>
<td>Predictors may be highly correlated; dimension reduction is desired.</td>
<td>
<p>Simultaneously reduces dimensionality and fits regression.</p>
<p>Works well with collinear, high-dimensional data.</p>
</td>
<td>
<p>Can be harder to interpret than OLS (latent components instead of original predictors).</p>
<p>Requires choosing the number of components.</p>
</td>
</tr>
</tbody>
</table></div>
<hr>
<div id="ordinary-least-squares" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Ordinary Least Squares<a class="anchor" aria-label="anchor" href="#ordinary-least-squares"><i class="fas fa-link"></i></a>
</h2>
<p>Ordinary Least Squares (OLS) is the backbone of statistical modeling, a method so foundational that it often serves as the starting point for understanding data relationships. Whether predicting sales, estimating economic trends, or uncovering patterns in scientific research, OLS remains a critical tool. Its appeal lies in simplicity: OLS models the relationship between a dependent variable and one or more predictors by minimizing the squared differences between observed and predicted values.</p>
<hr>
<p><strong>Why OLS Works: Linear and Nonlinear Relationships</strong></p>
<p>OLS rests on the Conditional Expectation Function (CEF), <span class="math inline">\(E[Y | X]\)</span>, which describes the expected value of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Regression shines in two key scenarios:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Perfect Fit (Linear CEF):</strong><br>
If <span class="math inline">\(E[Y_i | X_{1i}, \dots, X_{Ki}] = a + \sum_{k=1}^K b_k X_{ki}\)</span>, the regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(X_{1i}, \dots, X_{Ki}\)</span> exactly equals the CEF. In other words, the regression gives the true average relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.<br>
If the true relationship is linear, regression delivers the exact CEF. For instance, imagine you’re estimating the relationship between advertising spend and sales revenue. If the true impact is linear, OLS will perfectly capture it.</p></li>
<li><p><strong>Approximation (Nonlinear CEF):</strong><br>
If <span class="math inline">\(E[Y_i | X_{1i}, \dots, X_{Ki}]\)</span> is nonlinear, OLS provides the best linear approximation to this relationship. Specifically, it minimizes the expected squared deviation between the linear regression line and the nonlinear CEF.<br>
For example, the effect of advertising diminishes at higher spending levels? OLS still works, providing the best linear approximation to this nonlinear relationship by minimizing the squared deviations between predictions and the true (but unknown) CEF.</p></li>
</ol>
<p>In other words, regression is not just a tool for “linear” relationships—it’s a workhorse that adapts remarkably well to messy, real-world data.</p>
<div class="inline-figure"><img src="images/meme-linear-regerssion.jpg" style="width:80.0%"></div>
<hr>
<div id="simple-regression-basic-model" class="section level3" number="5.1.1">
<h3>
<span class="header-section-number">5.1.1</span> Simple Regression (Basic) Model<a class="anchor" aria-label="anchor" href="#simple-regression-basic-model"><i class="fas fa-link"></i></a>
</h3>
<p>The simplest form of regression is a straight line:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span>: The dependent variable or outcome we’re trying to predict (e.g., sales, temperature).</li>
<li>
<span class="math inline">\(X_i\)</span>: The independent variable or predictor (e.g., advertising spend, time).</li>
<li>
<span class="math inline">\(\beta_0\)</span>: The intercept—where the line crosses the <span class="math inline">\(Y\)</span>-axis when <span class="math inline">\(X = 0\)</span>.</li>
<li>
<span class="math inline">\(\beta_1\)</span>: The slope, representing the change in <span class="math inline">\(Y\)</span> for a one-unit increase in <span class="math inline">\(X\)</span>.</li>
<li>
<span class="math inline">\(\epsilon_i\)</span>: The error term, accounting for random factors that <span class="math inline">\(X\)</span> cannot explain.</li>
</ul>
<p>Assumptions About the Error Term (<span class="math inline">\(\epsilon_i\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
E(\epsilon_i) &amp;= 0 \\
\text{Var}(\epsilon_i) &amp;= \sigma^2 \\
\text{Cov}(\epsilon_i, \epsilon_j) &amp;= 0 \quad \text{for all } i \neq j
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\epsilon_i\)</span> is random, <span class="math inline">\(Y_i\)</span> is also random:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_i) &amp;= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&amp;= \beta_0 + \beta_1 X_i
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(Y_i) &amp;= \text{Var}(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&amp;= \text{Var}(\epsilon_i) \\
&amp;= \sigma^2
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\text{Cov}(\epsilon_i, \epsilon_j) = 0\)</span>, the outcomes across observations are independent. Hence, <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> are uncorrelated as well, conditioned on the <span class="math inline">\(X\)</span>’s.</p>
<hr>
<div id="estimation-in-ordinary-least-squares" class="section level4" number="5.1.1.1">
<h4>
<span class="header-section-number">5.1.1.1</span> Estimation in Ordinary Least Squares<a class="anchor" aria-label="anchor" href="#estimation-in-ordinary-least-squares"><i class="fas fa-link"></i></a>
</h4>
<p>The goal of OLS is to estimate the regression parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>) that best describe the relationship between the dependent variable <span class="math inline">\(Y\)</span> and the independent variable <span class="math inline">\(X\)</span>. To achieve this, we minimize the sum of squared deviations between observed values of <span class="math inline">\(Y_i\)</span> and their expected values predicted by the model.</p>
<p>The deviation of an observed value <span class="math inline">\(Y_i\)</span> from its expected value, based on the regression model, is:</p>
<p><span class="math display">\[
Y_i - E(Y_i) = Y_i - (\beta_0 + \beta_1 X_i).
\]</span></p>
<p>This deviation represents the error in prediction for the <span class="math inline">\(i\)</span>-th observation.</p>
<p>To ensure that the errors don’t cancel each other out and to prioritize larger deviations, we consider the squared deviations. The sum of squared deviations, denoted by <span class="math inline">\(Q\)</span>, is defined as:</p>
<p><span class="math display">\[
Q = \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2.
\]</span></p>
<p>The goal of OLS is to find the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize <span class="math inline">\(Q\)</span>. These values are called the <strong>OLS estimators</strong>.</p>
<p>To minimize <span class="math inline">\(Q\)</span>, we take partial derivatives with respect to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, set them to zero, and solve the resulting system of equations. After simplifying, the estimators for the slope (<span class="math inline">\(b_1\)</span>) and intercept (<span class="math inline">\(b_0\)</span>) are obtained as follows:</p>
<p>Slope (<span class="math inline">\(b_1\)</span>):</p>
<p><span class="math display">\[
b_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>Here, <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{Y}\)</span> represent the means of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively. This formula reveals that the slope is proportional to the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, scaled by the variance of <span class="math inline">\(X\)</span>.</p>
<p>Intercept (<span class="math inline">\(b_0\)</span>):</p>
<p><span class="math display">\[
b_0 = \frac{1}{n} \left( \sum_{i=1}^{n} Y_i - b_1 \sum_{i=1}^{n} X_i \right) = \bar{Y} - b_1 \bar{X}.
\]</span></p>
<p>The intercept is determined by aligning the regression line with the center of the data.</p>
<hr>
<p><strong>Intuition Behind the Estimators</strong></p>
<ul>
<li><p><span class="math inline">\(b_1\)</span> (Slope): This measures the average change in <span class="math inline">\(Y\)</span> for a one-unit increase in <span class="math inline">\(X\)</span>. The formula uses deviations from the mean to ensure that the relationship captures the joint variability of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p><span class="math inline">\(b_0\)</span> (Intercept): This ensures that the regression line passes through the mean of the data points <span class="math inline">\((\bar{X}, \bar{Y})\)</span>, anchoring the model in the center of the observed data.</p></li>
</ul>
<hr>
<p>Equivalently, we can also write these parameters in terms of covariances.</p>
<p>The covariance between two variables is defined as:</p>
<p><span class="math display">\[ \text{Cov}(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])] \]</span></p>
<p>Properties of Covariance:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{Cov}(X_i, X_i) = \sigma^2_X\)</span></li>
<li>If <span class="math inline">\(E(X_i) = 0\)</span> or <span class="math inline">\(E(Y_i) = 0\)</span>, then <span class="math inline">\(\text{Cov}(X_i, Y_i) = E[X_i Y_i]\)</span>
</li>
<li>For <span class="math inline">\(W_i = a + b X_i\)</span> and <span class="math inline">\(Z_i = c + d Y_i\)</span>,<br><span class="math inline">\(\text{Cov}(W_i, Z_i) = bd \cdot \text{Cov}(X_i, Y_i)\)</span>
</li>
</ol>
<p>For a bivariate regression, the slope <span class="math inline">\(\beta\)</span> in a bivariate regression is given by:</p>
<p><span class="math display">\[ \beta = \frac{\text{Cov}(Y_i, X_i)}{\text{Var}(X_i)} \]</span></p>
<p>For a multivariate case, the slope for <span class="math inline">\(X_k\)</span> is:</p>
<p><span class="math display">\[ \beta_k = \frac{\text{Cov}(Y_i, \tilde{X}_{ki})}{\text{Var}(\tilde{X}_{ki})} \]</span></p>
<p>Where <span class="math inline">\(\tilde{X}_{ki}\)</span> represents the residual from a regression of <span class="math inline">\(X_{ki}\)</span> on the <span class="math inline">\(K-1\)</span> other covariates in the model.</p>
<p>The intercept is:</p>
<p><span class="math display">\[ \beta_0 = E[Y_i] - \beta_1 E(X_i) \]</span></p>
<p>Note:</p>
<ul>
<li>OLS does not require the assumption of a specific distribution for the variables. Its robustness is based on the minimization of squared errors (i.e., no distributional assumptions).</li>
</ul>
</div>
<div id="properties-of-least-squares-estimators" class="section level4" number="5.1.1.2">
<h4>
<span class="header-section-number">5.1.1.2</span> Properties of Least Squares Estimators<a class="anchor" aria-label="anchor" href="#properties-of-least-squares-estimators"><i class="fas fa-link"></i></a>
</h4>
<p>The properties of the Ordinary Least Squares estimators (<span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>) are derived based on their statistical behavior. These properties provide insights into the accuracy, variability, and reliability of the estimates.</p>
<hr>
<div id="expectation-of-the-ols-estimators" class="section level5" number="5.1.1.2.1">
<h5>
<span class="header-section-number">5.1.1.2.1</span> Expectation of the OLS Estimators<a class="anchor" aria-label="anchor" href="#expectation-of-the-ols-estimators"><i class="fas fa-link"></i></a>
</h5>
<p>The OLS estimators <span class="math inline">\(b_0\)</span> (intercept) and <span class="math inline">\(b_1\)</span> (slope) are unbiased. This means their expected values equal the true population parameters:</p>
<p><span class="math display">\[
\begin{aligned}
E(b_1) &amp;= \beta_1, \\
E(b_0) &amp;= E(\bar{Y}) - \bar{X}\beta_1.
\end{aligned}
\]</span></p>
<p>Since the expected value of the sample mean of <span class="math inline">\(Y\)</span>, <span class="math inline">\(E(\bar{Y})\)</span>, is:</p>
<p><span class="math display">\[
E(\bar{Y}) = \beta_0 + \beta_1 \bar{X},
\]</span></p>
<p>the expected value of <span class="math inline">\(b_0\)</span> simplifies to:</p>
<p><span class="math display">\[
E(b_0) = \beta_0.
\]</span></p>
<p>Thus, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are unbiased estimators of their respective population parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<hr>
</div>
<div id="variance-of-the-ols-estimators" class="section level5" number="5.1.1.2.2">
<h5>
<span class="header-section-number">5.1.1.2.2</span> Variance of the OLS Estimators<a class="anchor" aria-label="anchor" href="#variance-of-the-ols-estimators"><i class="fas fa-link"></i></a>
</h5>
<p>The variability of the OLS estimators depends on the spread of the predictor variable <span class="math inline">\(X\)</span> and the error variance <span class="math inline">\(\sigma^2\)</span>. The variances are given by:</p>
<p>Variance of <span class="math inline">\(b_1\)</span> (Slope):</p>
<p><span class="math display">\[
\text{Var}(b_1) = \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>Variance of <span class="math inline">\(b_0\)</span> (Intercept):</p>
<p><span class="math display">\[
\text{Var}(b_0) = \sigma^2 \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>These formulas highlight that:</p>
<ul>
<li>
<span class="math inline">\(\text{Var}(b_1) \to 0\)</span> as the number of observations increases, provided <span class="math inline">\(X_i\)</span> values are distributed around their mean <span class="math inline">\(\bar{X}\)</span>.</li>
<li>
<span class="math inline">\(\text{Var}(b_0) \to 0\)</span> as <span class="math inline">\(n\)</span> increases, assuming <span class="math inline">\(X_i\)</span> values are appropriately selected (i.e., not all clustered near the mean).</li>
</ul>
<hr>
</div>
</div>
<div id="mean-square-error-mse" class="section level4" number="5.1.1.3">
<h4>
<span class="header-section-number">5.1.1.3</span> Mean Square Error (MSE)<a class="anchor" aria-label="anchor" href="#mean-square-error-mse"><i class="fas fa-link"></i></a>
</h4>
<p>The Mean Square Error (MSE) quantifies the average squared residual (error) in the model:</p>
<p><span class="math display">\[
MSE = \frac{SSE}{n-2} = \frac{\sum_{i=1}^{n} e_i^2}{n-2} = \frac{\sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2}{n-2},
\]</span></p>
<p>where <span class="math inline">\(SSE\)</span> is the Sum of Squared Errors and <span class="math inline">\(n-2\)</span> represents the degrees of freedom for a simple linear regression model (two parameters estimated: <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>).</p>
<p>The expected value of the MSE equals the error variance (i.e., unbiased Estimator of MSE:):</p>
<p><span class="math display">\[
E(MSE) = \sigma^2.
\]</span></p>
<hr>
</div>
<div id="estimating-variance-of-the-ols-coefficients" class="section level4" number="5.1.1.4">
<h4>
<span class="header-section-number">5.1.1.4</span> Estimating Variance of the OLS Coefficients<a class="anchor" aria-label="anchor" href="#estimating-variance-of-the-ols-coefficients"><i class="fas fa-link"></i></a>
</h4>
<p>The sample-based estimates of the variances of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are expressed as follows:</p>
<p>Estimated Variance of <span class="math inline">\(b_1\)</span> (Slope):</p>
<p><span class="math display">\[
s^2(b_1) = \widehat{\text{Var}}(b_1) = \frac{MSE}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>Estimated Variance of <span class="math inline">\(b_0\)</span> (Intercept):</p>
<p><span class="math display">\[
s^2(b_0) = \widehat{\text{Var}}(b_0) = MSE \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>These estimates rely on the MSE to approximate <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The variance estimates are unbiased:</p>
<p><span class="math display">\[
\begin{aligned}
E(s^2(b_1)) &amp;= \text{Var}(b_1), \\
E(s^2(b_0)) &amp;= \text{Var}(b_0).
\end{aligned}
\]</span></p>
<hr>
<p><strong>Implications of These Properties</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Unbiasedness:</strong> The unbiased nature of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> ensures that, on average, the regression model accurately reflects the true relationship in the population.</li>
<li>
<strong>Decreasing Variance:</strong> As the sample size <span class="math inline">\(n\)</span> increases or as the spread of <span class="math inline">\(X_i\)</span> values grows, the variances of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> decrease, leading to more precise estimates.</li>
<li>
<strong>Error Estimation with MSE:</strong> MSE provides a reliable estimate of the error variance <span class="math inline">\(\sigma^2\)</span>, which feeds directly into assessing the reliability of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>.</li>
</ol>
</div>
<div id="residuals-in-ordinary-least-squares" class="section level4" number="5.1.1.5">
<h4>
<span class="header-section-number">5.1.1.5</span> Residuals in Ordinary Least Squares<a class="anchor" aria-label="anchor" href="#residuals-in-ordinary-least-squares"><i class="fas fa-link"></i></a>
</h4>
<p>Residuals are the differences between observed values (<span class="math inline">\(Y_i\)</span>) and their predicted counterparts (<span class="math inline">\(\hat{Y}_i\)</span>). They play a central role in assessing model fit and ensuring the assumptions of OLS are met.</p>
<p>The residual for the <span class="math inline">\(i\)</span>-th observation is defined as:</p>
<p><span class="math display">\[
e_i = Y_i - \hat{Y}_i = Y_i - (b_0 + b_1 X_i),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(e_i\)</span>: Residual for the <span class="math inline">\(i\)</span>-th observation.</li>
<li>
<span class="math inline">\(\hat{Y}_i\)</span>: Predicted value based on the regression model.</li>
<li>
<span class="math inline">\(Y_i\)</span>: Actual observed value.</li>
</ul>
<p>Residuals estimate the unobservable error terms <span class="math inline">\(\epsilon_i\)</span>:</p>
<ul>
<li>
<span class="math inline">\(e_i\)</span> is an estimate of <span class="math inline">\(\epsilon_i = Y_i - E(Y_i)\)</span>.</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> is always unknown because we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
</ul>
<hr>
<div id="key-properties-of-residuals" class="section level5" number="5.1.1.5.1">
<h5>
<span class="header-section-number">5.1.1.5.1</span> Key Properties of Residuals<a class="anchor" aria-label="anchor" href="#key-properties-of-residuals"><i class="fas fa-link"></i></a>
</h5>
<p>Residuals exhibit several mathematical properties that align with the OLS estimation process:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Sum of Residuals</strong>:<br>
The residuals sum to zero:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} e_i = 0.
\]</span></p>
<p>This ensures that the regression line passes through the centroid of the data, <span class="math inline">\((\bar{X}, \bar{Y})\)</span>.</p>
</li>
<li>
<p><strong>Orthogonality of Residuals to Predictors</strong>:<br>
The residuals are orthogonal (uncorrelated) to the predictor variable <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} X_i e_i = 0.
\]</span></p>
<p>This reflects the fact that the OLS minimizes the squared deviations of residuals along the <span class="math inline">\(Y\)</span>-axis, not the <span class="math inline">\(X\)</span>-axis.</p>
</li>
</ol>
<hr>
</div>
<div id="expected-values-of-residuals" class="section level5" number="5.1.1.5.2">
<h5>
<span class="header-section-number">5.1.1.5.2</span> Expected Values of Residuals<a class="anchor" aria-label="anchor" href="#expected-values-of-residuals"><i class="fas fa-link"></i></a>
</h5>
<p>The expected values of residuals reinforce the unbiased nature of OLS:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Mean of Residuals</strong>:<br>
The residuals have an expected value of zero:</p>
<p><span class="math display">\[
E[e_i] = 0.
\]</span></p>
</li>
<li>
<p><strong>Orthogonality to Predictors and Fitted Values</strong>:<br>
Residuals are uncorrelated with both the predictor variables and the fitted values:</p>
<p><span class="math display">\[
\begin{aligned}
E[X_i e_i] &amp;= 0, \\
E[\hat{Y}_i e_i] &amp;= 0.
\end{aligned}
\]</span></p>
</li>
</ol>
<p>These properties highlight that residuals do not contain systematic information about the predictors or the fitted values, reinforcing the idea that the model has captured the underlying relationship effectively.</p>
<hr>
</div>
<div id="practical-importance-of-residuals" class="section level5" number="5.1.1.5.3">
<h5>
<span class="header-section-number">5.1.1.5.3</span> Practical Importance of Residuals<a class="anchor" aria-label="anchor" href="#practical-importance-of-residuals"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li><p><strong>Model Diagnostics:</strong><br>
Residuals are analyzed to check the assumptions of OLS, including linearity, homoscedasticity (constant variance), and independence of errors. Patterns in residual plots can signal issues such as nonlinearity or heteroscedasticity.</p></li>
<li><p><strong>Goodness-of-Fit:</strong><br>
The sum of squared residuals, <span class="math inline">\(\sum e_i^2\)</span>, measures the total unexplained variation in <span class="math inline">\(Y\)</span>. A smaller sum indicates a better fit.</p></li>
<li><p><strong>Influence Analysis:</strong><br>
Large residuals may indicate outliers or influential points that disproportionately affect the regression line.</p></li>
</ol>
</div>
</div>
<div id="inference-in-ordinary-least-squares" class="section level4" number="5.1.1.6">
<h4>
<span class="header-section-number">5.1.1.6</span> Inference in Ordinary Least Squares<a class="anchor" aria-label="anchor" href="#inference-in-ordinary-least-squares"><i class="fas fa-link"></i></a>
</h4>
<p>Inference allows us to make probabilistic statements about the regression parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>) and predictions (<span class="math inline">\(Y_h\)</span>). To perform valid inference, certain assumptions about the distribution of errors are necessary.</p>
<hr>
<p>Normality Assumption</p>
<ul>
<li>OLS estimation itself does <strong>not</strong> require the assumption of normality.</li>
<li>However, to conduct hypothesis tests or construct confidence intervals for <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and predictions, distributional assumptions are necessary.</li>
<li>Inference on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is <strong>robust</strong> to moderate departures from normality, especially in large samples due to the <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a>.</li>
<li>Inference on predicted values, <span class="math inline">\(Y_{pred}\)</span>, is more sensitive to normality violations.</li>
</ul>
<hr>
<p>When we assume a <strong>normal error model</strong>, the response variable <span class="math inline">\(Y_i\)</span> is modeled as:</p>
<p><span class="math display">\[
Y_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>: Mean response</li>
<li>
<span class="math inline">\(\sigma^2\)</span>: Variance of the errors</li>
</ul>
<p>Under this model, the sampling distributions of the OLS estimators, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, can be derived.</p>
<hr>
<div id="inference-for-beta_1-slope" class="section level5" number="5.1.1.6.1">
<h5>
<span class="header-section-number">5.1.1.6.1</span> Inference for <span class="math inline">\(\beta_1\)</span> (Slope)<a class="anchor" aria-label="anchor" href="#inference-for-beta_1-slope"><i class="fas fa-link"></i></a>
</h5>
<p>Under the normal error model:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Sampling Distribution of</strong> <span class="math inline">\(b_1\)</span>:</p>
<p><span class="math display">\[
b_1 \sim N\left(\beta_1, \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right).
\]</span></p>
<p>This indicates that <span class="math inline">\(b_1\)</span> is an unbiased estimator of <span class="math inline">\(\beta_1\)</span> with variance proportional to <span class="math inline">\(\sigma^2\)</span>.</p>
</li>
<li>
<p><strong>Test Statistic:</strong></p>
<p><span class="math display">\[
t = \frac{b_1 - \beta_1}{s(b_1)} \sim t_{n-2},
\]</span></p>
<p>where <span class="math inline">\(s(b_1)\)</span> is the standard error of <span class="math inline">\(b_1\)</span>: <span class="math display">\[
s(b_1) = \sqrt{\frac{MSE}{\sum_{i=1}^{n} (X_i - \bar{X})^2}}.
\]</span></p>
</li>
<li>
<p><strong>Confidence Interval:</strong></p>
<p>A <span class="math inline">\((1-\alpha) 100\%\)</span> confidence interval for <span class="math inline">\(\beta_1\)</span> is:</p>
<p><span class="math display">\[
b_1 \pm t_{1-\alpha/2; n-2} \cdot s(b_1).
\]</span></p>
</li>
</ol>
<hr>
</div>
<div id="inference-for-beta_0-intercept" class="section level5" number="5.1.1.6.2">
<h5>
<span class="header-section-number">5.1.1.6.2</span> Inference for <span class="math inline">\(\beta_0\)</span> (Intercept)<a class="anchor" aria-label="anchor" href="#inference-for-beta_0-intercept"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li>
<p><strong>Sampling Distribution of</strong> <span class="math inline">\(b_0\)</span>:</p>
<p>Under the normal error model, the sampling distribution of <span class="math inline">\(b_0\)</span> is:</p>
<p><span class="math display">\[
b_0 \sim N\left(\beta_0, \sigma^2 \left(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right)\right).
\]</span></p>
</li>
<li>
<p><strong>Test Statistic:</strong></p>
<p><span class="math display">\[
t = \frac{b_0 - \beta_0}{s(b_0)} \sim t_{n-2},
\]</span></p>
<p>where <span class="math inline">\(s(b_0)\)</span> is the standard error of <span class="math inline">\(b_0\)</span>: <span class="math display">\[
s(b_0) = \sqrt{MSE \left(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right)}.
\]</span></p>
</li>
<li>
<p><strong>Confidence Interval:</strong></p>
<p>A <span class="math inline">\((1-\alpha) 100\%\)</span> confidence interval for <span class="math inline">\(\beta_0\)</span> is:</p>
<p><span class="math display">\[
b_0 \pm t_{1-\alpha/2; n-2} \cdot s(b_0).
\]</span></p>
</li>
</ol>
<hr>
</div>
<div id="mean-response" class="section level5" number="5.1.1.6.3">
<h5>
<span class="header-section-number">5.1.1.6.3</span> Mean Response<a class="anchor" aria-label="anchor" href="#mean-response"><i class="fas fa-link"></i></a>
</h5>
<p>In regression, we often estimate the mean response of the dependent variable <span class="math inline">\(Y\)</span> for a given level of the predictor variable <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(X_h\)</span>. This estimation provides a predicted average outcome for a specific value of <span class="math inline">\(X\)</span> based on the fitted regression model.</p>
<ul>
<li>Let <span class="math inline">\(X_h\)</span> represent the level of <span class="math inline">\(X\)</span> for which we want to estimate the mean response.</li>
<li>The mean response when <span class="math inline">\(X = X_h\)</span> is denoted as <span class="math inline">\(E(Y_h)\)</span>.</li>
<li>A point estimator for <span class="math inline">\(E(Y_h)\)</span> is <span class="math inline">\(\hat{Y}_h\)</span>, which is the predicted value from the regression model:</li>
</ul>
<p><span class="math display">\[
\hat{Y}_h = b_0 + b_1 X_h.
\]</span></p>
<p>The estimator <span class="math inline">\(\hat{Y}_h\)</span> is unbiased because its expected value equals the true mean response <span class="math inline">\(E(Y_h)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E(\hat{Y}_h) &amp;= E(b_0 + b_1 X_h) \\
&amp;= \beta_0 + \beta_1 X_h \\
&amp;= E(Y_h).
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\hat{Y}_h\)</span> provides a reliable estimate of the mean response at <span class="math inline">\(X_h\)</span>.</p>
<hr>
<p>The variance of <span class="math inline">\(\hat{Y}_h\)</span> reflects the uncertainty in the estimate of the mean response:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{Y}_h) &amp;= \text{Var}(b_0 + b_1 X_h) \quad\text{(definition of }\hat{Y}_h\text{)}\\[6pt]&amp;= \text{Var}\bigl((\bar{Y} - b_1 \bar{X}) + b_1 X_h\bigr)\quad\text{(since } b_0 = \bar{Y} - b_1 \bar{X}\text{)}\\[6pt]&amp;= \text{Var}\bigl(\bar{Y} + b_1(X_h - \bar{X})\bigr)\quad\text{(factor out } b_1\text{)}\\[6pt]&amp;= \text{Var}\bigl(\bar{Y} + b_1 (X_h - \bar{X}) \bigr) \\
&amp;= \text{Var}(\bar{Y}) + (X_h - \bar{X})^2 \text{Var}(b_1) + 2(X_h - \bar{X}) \text{Cov}(\bar{Y}, b_1).
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\text{Cov}(\bar{Y}, b_1) = 0\)</span> (due to the independence of the errors, <span class="math inline">\(\epsilon_i\)</span>), the variance simplifies to:</p>
<p><span class="math display">\[
\text{Var}(\hat{Y}_h) = \frac{\sigma^2}{n} + (X_h - \bar{X})^2 \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]</span></p>
<p>This can also be expressed as:</p>
<p><span class="math display">\[
\text{Var}(\hat{Y}_h) = \sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>To estimate the variance of <span class="math inline">\(\hat{Y}_h\)</span>, we replace <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(MSE\)</span>, the mean squared error from the regression:</p>
<p><span class="math display">\[
s^2(\hat{Y}_h) = MSE \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<hr>
<p>Under the normal error model, the sampling distribution of <span class="math inline">\(\hat{Y}_h\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{Y}_h &amp;\sim N\left(E(Y_h), \text{Var}(\hat{Y}_h)\right), \\
\frac{\hat{Y}_h - E(Y_h)}{s(\hat{Y}_h)} &amp;\sim t_{n-2}.
\end{aligned}
\]</span></p>
<p>This result follows because <span class="math inline">\(\hat{Y}_h\)</span> is a linear combination of normally distributed random variables, and its variance is estimated using <span class="math inline">\(s^2(\hat{Y}_h)\)</span>.</p>
<hr>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for the mean response <span class="math inline">\(E(Y_h)\)</span> is given by:</p>
<p><span class="math display">\[
\hat{Y}_h \pm t_{1-\alpha/2; n-2} \cdot s(\hat{Y}_h),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{Y}_h\)</span>: Point estimate of the mean response,</li>
<li>
<span class="math inline">\(s(\hat{Y}_h)\)</span>: Estimated standard error of the mean response,</li>
<li>
<span class="math inline">\(t_{1-\alpha/2; n-2}\)</span>: Critical value from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</li>
</ul>
<hr>
</div>
<div id="prediction-of-a-new-observation" class="section level5" number="5.1.1.6.4">
<h5>
<span class="header-section-number">5.1.1.6.4</span> Prediction of a New Observation<a class="anchor" aria-label="anchor" href="#prediction-of-a-new-observation"><i class="fas fa-link"></i></a>
</h5>
<p>When analyzing regression results, it is important to distinguish between:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Estimating the mean response</strong> at a particular value of <span class="math inline">\(X\)</span>.</li>
<li>
<strong>Predicting an individual outcome</strong> for a particular value of <span class="math inline">\(X\)</span>.</li>
</ol>
<hr>
<p>Mean Response vs. Individual Outcome</p>
<ul>
<li><p><strong>Same Point Estimate</strong><br>
The formula for both the estimated mean response and the predicted individual outcome at <span class="math inline">\(X = X_h\)</span> is identical:<br><span class="math display">\[
\hat{Y}_{pred} = \hat{Y}_h = b_0 + b_1 X_h.
\]</span></p></li>
<li><p><strong>Different Variance</strong><br>
Although the point estimates are the same, the level of uncertainty differs. When predicting an individual outcome, we must consider not only the uncertainty in estimating the mean response (<span class="math inline">\(\hat{Y}_h\)</span>) but also the additional random variation within the distribution of <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p>Therefore, <strong>prediction intervals</strong> (for individual outcomes) account for more uncertainty and are consequently wider than <strong>confidence intervals</strong> (for the mean response).</p>
<hr>
<p>To predict an individual outcome for a given <span class="math inline">\(X_h\)</span>, we combine the mean response with the random error:</p>
<p><span class="math display">\[
Y_{pred} = \beta_0 + \beta_1 X_h + \epsilon.
\]</span></p>
<p>Using the least squares predictor:</p>
<p><span class="math display">\[
\hat{Y}_{pred} = b_0 + b_1 X_h,
\]</span></p>
<p>since <span class="math inline">\(E(\epsilon) = 0\)</span>.</p>
<hr>
<p>The variance of the predicted value for a new observation, <span class="math inline">\(Y_{pred}\)</span>, includes both:</p>
<ol style="list-style-type: decimal">
<li>Variance of the estimated mean response: <span class="math display">\[
\sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right),
\]</span>
</li>
<li>Variance of the error term, <span class="math inline">\(\epsilon\)</span>, which is <span class="math inline">\(\sigma^2\)</span>.</li>
</ol>
<p>Thus, the total variance is:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(Y_{pred}) &amp;= \text{Var}(b_0 + b_1 X_h + \epsilon) \\
&amp;= \text{Var}(b_0 + b_1 X_h) + \text{Var}(\epsilon) \\
&amp;= \sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right) + \sigma^2 \\
&amp;= \sigma^2 \left( 1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\end{aligned}
\]</span></p>
<p>We estimate the variance of the prediction using <span class="math inline">\(MSE\)</span>, the mean squared error:</p>
<p><span class="math display">\[
s^2(pred) = MSE \left( 1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
<p>Under the normal error model, the standardized predicted value follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\frac{Y_{pred} - \hat{Y}_h}{s(pred)} \sim t_{n-2}.
\]</span></p>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> prediction interval for <span class="math inline">\(Y_{pred}\)</span> is:</p>
<p><span class="math display">\[
\hat{Y}_{pred} \pm t_{1-\alpha/2; n-2} \cdot s(pred).
\]</span></p>
<hr>
</div>
<div id="confidence-band" class="section level5" number="5.1.1.6.5">
<h5>
<span class="header-section-number">5.1.1.6.5</span> Confidence Band<a class="anchor" aria-label="anchor" href="#confidence-band"><i class="fas fa-link"></i></a>
</h5>
<p>In regression analysis, we often want to evaluate the uncertainty around the entire regression line, not just at a single value of the predictor variable <span class="math inline">\(X\)</span>. This is achieved using a <strong>confidence band</strong>, which provides a confidence interval for the mean response, <span class="math inline">\(E(Y) = \beta_0 + \beta_1 X\)</span>, over the entire range of <span class="math inline">\(X\)</span> values.</p>
<p>The Working-Hotelling confidence band is a method to construct simultaneous confidence intervals for the regression line. For a given <span class="math inline">\(X_h\)</span>, the confidence band is expressed as:</p>
<p><span class="math display">\[
\hat{Y}_h \pm W s(\hat{Y}_h),
\]</span></p>
<p>where:</p>
<ul>
<li>
<p><span class="math inline">\(W^2 = 2F_{1-\alpha; 2, n-2}\)</span>,</p>
<ul>
<li>
<span class="math inline">\(F_{1-\alpha; 2, n-2}\)</span> is the critical value from the <span class="math inline">\(F\)</span>-distribution with 2 and <span class="math inline">\(n-2\)</span> degrees of freedom.</li>
</ul>
</li>
<li>
<p><span class="math inline">\(s(\hat{Y}_h)\)</span> is the standard error of the estimated mean response at <span class="math inline">\(X_h\)</span>:</p>
<p><span class="math display">\[
s^2(\hat{Y}_h) = MSE \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]</span></p>
</li>
</ul>
<hr>
<p><strong>Key Properties of the Confidence Band</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Width of the Interval:</strong>
<ul>
<li>The width of the confidence band changes with <span class="math inline">\(X_h\)</span> because <span class="math inline">\(s(\hat{Y}_h)\)</span> depends on how far <span class="math inline">\(X_h\)</span> is from the mean of <span class="math inline">\(X\)</span> (<span class="math inline">\(\bar{X}\)</span>).</li>
<li>The interval is narrowest at <span class="math inline">\(X = \bar{X}\)</span>, where the variance of the estimated mean response is minimized.</li>
</ul>
</li>
<li>
<strong>Shape of the Band:</strong>
<ul>
<li>The boundaries of the confidence band form a hyperbolic shape around the regression line.</li>
<li>This reflects the increasing uncertainty in the mean response as <span class="math inline">\(X_h\)</span> moves farther from <span class="math inline">\(\bar{X}\)</span>.</li>
</ul>
</li>
<li>
<strong>Simultaneous Coverage:</strong>
<ul>
<li>The Working-Hotelling band ensures that the true regression line <span class="math inline">\(E(Y) = \beta_0 + \beta_1 X\)</span> lies within the band across all values of <span class="math inline">\(X\)</span> with a specified confidence level (e.g., <span class="math inline">\(95\%\)</span>).</li>
</ul>
</li>
</ol>
</div>
</div>
<div id="analysis-of-variance-anova-in-regression" class="section level4" number="5.1.1.7">
<h4>
<span class="header-section-number">5.1.1.7</span> Analysis of Variance (ANOVA) in Regression<a class="anchor" aria-label="anchor" href="#analysis-of-variance-anova-in-regression"><i class="fas fa-link"></i></a>
</h4>
<p>ANOVA in regression decomposes the total variability in the response variable (<span class="math inline">\(Y\)</span>) into components attributed to the regression model and residual error. In the context of regression, ANOVA provides a mechanism to assess the fit of the model and test hypotheses about the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>The <strong>corrected Total Sum of Squares (SSTO)</strong> quantifies the total variation in <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
SSTO = \sum_{i=1}^n (Y_i - \bar{Y})^2,
\]</span></p>
<p>where <span class="math inline">\(\bar{Y}\)</span> is the mean of the response variable. The term “corrected” refers to the fact that the sum of squares is calculated relative to the mean (i.e., the uncorrected total sum of squares is given by <span class="math inline">\(\sum Y_i^2\)</span>)</p>
<p>Using the fitted regression model <span class="math inline">\(\hat{Y}_i = b_0 + b_1 X_i\)</span>, we estimate the conditional mean of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X_i\)</span>. The total sum of squares can be decomposed as:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \bar{Y})^2 &amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y})^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2 + 2 \sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y}) \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2
\end{aligned}
\]</span></p>
<ul>
<li>The cross-product term is zero, as shown below.</li>
<li>This decomposition simplifies to:</li>
</ul>
<p><span class="math display">\[
SSTO = SSE + SSR,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(SSE = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2\)</span>: Error Sum of Squares (variation unexplained by the model).</p></li>
<li><p><span class="math inline">\(SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2\)</span>: Regression Sum of Squares (variation explained by the model), which measure how the conditional mean varies about a central value.</p></li>
</ul>
<p>Degrees of freedom are partitioned as:</p>
<p><span class="math display">\[
\begin{aligned}
SSTO &amp;= SSR + SSE \\
(n-1) &amp;= (1) + (n-2) \\
\end{aligned}
\]</span></p>
<hr>
<p>To confirm that the cross-product term is zero:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y})
&amp;= \sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))(\bar{Y} + b_1 (X_i - \bar{X})-\bar{Y}) \quad \text{(Expand } Y_i - \hat{Y}_i \text{ and } \hat{Y}_i - \bar{Y}\text{)} \\
&amp;=\sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))( b_1 (X_i - \bar{X}))  \\
&amp;= b_1 \sum_{i=1}^n (Y_i - \bar{Y})(X_i - \bar{X}) - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 \quad \text{(Distribute terms in the product)} \\
&amp;= b_1 \frac{\sum_{i=1}^n (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^n (X_i - \bar{X})^2} \sum_{i=1}^n (X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 \quad \text{(Substitute } b_1 \text{ definition)} \\
&amp;= b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2  \\
&amp;= 0
\end{aligned}
\]</span></p>
<hr>
<p>The ANOVA table summarizes the partitioning of variability:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="19%">
<col width="19%">
<col width="19%">
<col width="22%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th>Sum of Squares</th>
<th>df</th>
<th>Mean Square</th>
<th>
<span class="math inline">\(F\)</span> Statistic</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Regression (Model)</td>
<td><span class="math inline">\(SSR\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(MSR = \frac{SSR}{1}\)</span></td>
<td><span class="math inline">\(F = \frac{MSR}{MSE}\)</span></td>
</tr>
<tr class="even">
<td>Error</td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(n-2\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{n-2}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total (Corrected)</td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(n-1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<hr>
<p>The expected values of the mean squares are:</p>
<p><span class="math display">\[
\begin{aligned}
E(MSE) &amp;= \sigma^2, \\
E(MSR) &amp;= \sigma^2 + \beta_1^2 \sum_{i=1}^n (X_i - \bar{X})^2.
\end{aligned}
\]</span></p>
<ul>
<li>
<strong>If</strong> <span class="math inline">\(\beta_1 = 0\)</span>:
<ul>
<li>The regression model does not explain any variation in <span class="math inline">\(Y\)</span> beyond the mean, and <span class="math inline">\(E(MSR) = E(MSE) = \sigma^2\)</span>.</li>
<li>This condition corresponds to the null hypothesis, <span class="math inline">\(H_0: \beta_1 = 0\)</span>.</li>
</ul>
</li>
<li>
<strong>If</strong> <span class="math inline">\(\beta_1 \neq 0\)</span>:
<ul>
<li>The regression model explains some variation in <span class="math inline">\(Y\)</span>, and <span class="math inline">\(E(MSR) &gt; E(MSE)\)</span>.</li>
<li>The additional term <span class="math inline">\(\beta_1^2 \sum_{i=1}^{n} (X_i - \bar{X})^2\)</span> represents the variance explained by the predictor <span class="math inline">\(X\)</span>.</li>
</ul>
</li>
</ul>
<p>The difference between <span class="math inline">\(E(MSR)\)</span> and <span class="math inline">\(E(MSE)\)</span> allows us to infer whether <span class="math inline">\(\beta_1 \neq 0\)</span> by comparing their ratio.</p>
<hr>
<p>Assuming the errors <span class="math inline">\(\epsilon_i\)</span> are independent and identically distributed as <span class="math inline">\(N(0, \sigma^2)\)</span>, and under the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>, we have:</p>
<ol style="list-style-type: decimal">
<li>
<p>The scaled <span class="math inline">\(MSE\)</span> follows a chi-square distribution with <span class="math inline">\(n-2\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\frac{MSE}{\sigma^2} \sim \chi_{n-2}^2.
\]</span></p>
</li>
<li>
<p>The scaled <span class="math inline">\(MSR\)</span> follows a chi-square distribution with <span class="math inline">\(1\)</span> degree of freedom:</p>
<p><span class="math display">\[
\frac{MSR}{\sigma^2} \sim \chi_{1}^2.
\]</span></p>
</li>
<li><p>These two chi-square random variables are independent.</p></li>
</ol>
<p>The ratio of two independent chi-square random variables, scaled by their respective degrees of freedom, follows an <span class="math inline">\(F\)</span>-distribution. Therefore, under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
F = \frac{MSR}{MSE} \sim F_{1, n-2}.
\]</span></p>
<p>The <span class="math inline">\(F\)</span>-statistic tests whether the regression model provides a significant improvement over the null model (constant <span class="math inline">\(E(Y)\)</span>).</p>
<p>The hypotheses for the <span class="math inline">\(F\)</span>-test are:</p>
<ul>
<li>
<strong>Null Hypothesis</strong> (<span class="math inline">\(H_0\)</span>): <span class="math inline">\(\beta_1 = 0\)</span> (no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>).</li>
<li>
<strong>Alternative Hypothesis</strong> (<span class="math inline">\(H_a\)</span>): <span class="math inline">\(\beta_1 \neq 0\)</span> (a significant relationship exists between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>).</li>
</ul>
<p>The rejection rule for <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> is:</p>
<p><span class="math display">\[
F &gt; F_{1-\alpha;1,n-2},
\]</span></p>
<p>where <span class="math inline">\(F_{1-\alpha;1,n-2}\)</span> is the critical value from the <span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(1\)</span> and <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<ol style="list-style-type: decimal">
<li>
<strong>If</strong> <span class="math inline">\(F \leq F_{1-\alpha;1,n-2}\)</span>:
<ul>
<li>Fail to reject <span class="math inline">\(H_0\)</span>. There is insufficient evidence to conclude that <span class="math inline">\(X\)</span> significantly explains variation in <span class="math inline">\(Y\)</span>.</li>
</ul>
</li>
<li>
<strong>If</strong> <span class="math inline">\(F &gt; F_{1-\alpha;1,n-2}\)</span>:
<ul>
<li>Reject <span class="math inline">\(H_0\)</span>. There is significant evidence that <span class="math inline">\(X\)</span> explains some of the variation in <span class="math inline">\(Y\)</span>.</li>
</ul>
</li>
</ol>
</div>
<div id="coefficient-of-determination-r2" class="section level4" number="5.1.1.8">
<h4>
<span class="header-section-number">5.1.1.8</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)<a class="anchor" aria-label="anchor" href="#coefficient-of-determination-r2"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Coefficient of Determination</strong> (<span class="math inline">\(R^2\)</span>) measures how well the linear regression model accounts for the variability in the response variable <span class="math inline">\(Y\)</span>. It is defined as:</p>
<p><span class="math display">\[
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(SSR\)</span>: Regression Sum of Squares (variation explained by the model).</li>
<li>
<span class="math inline">\(SSTO\)</span>: Total Sum of Squares (total variation in <span class="math inline">\(Y\)</span> about its mean).</li>
<li>
<span class="math inline">\(SSE\)</span>: Error Sum of Squares (variation unexplained by the model).</li>
</ul>
<hr>
<p><strong>Properties of</strong> <span class="math inline">\(R^2\)</span></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Range</strong>: <span class="math display">\[
0 \leq R^2 \leq 1.
\]</span></p>
<ul>
<li>
<span class="math inline">\(R^2 = 0\)</span>: The model explains none of the variability in <span class="math inline">\(Y\)</span> (e.g., <span class="math inline">\(\beta_1 = 0\)</span>).</li>
<li>
<span class="math inline">\(R^2 = 1\)</span>: The model explains all the variability in <span class="math inline">\(Y\)</span> (perfect fit).</li>
</ul>
</li>
<li><p><strong>Proportionate Reduction in Variance</strong>: <span class="math inline">\(R^2\)</span> represents the proportionate reduction in the total variation of <span class="math inline">\(Y\)</span> after fitting the model. It quantifies how much better the model predicts <span class="math inline">\(Y\)</span> compared to simply using <span class="math inline">\(\bar{Y}\)</span>.</p></li>
<li>
<p><strong>Potential Misinterpretation</strong>: It is not really correct to say <span class="math inline">\(R^2\)</span> is the “variation in <span class="math inline">\(Y\)</span> explained by <span class="math inline">\(X\)</span>.” The term “variation explained” assumes a causative or deterministic explanation, which is not always correct. For example:</p>
<ul>
<li><p><span class="math inline">\(R^2\)</span> shows how much variance in <span class="math inline">\(Y\)</span> is accounted for by the regression model, but it does not imply causation.</p></li>
<li><p>In cases with confounding variables or spurious correlations, <span class="math inline">\(R^2\)</span> can still be high, even if there’s no direct causal link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
</ul>
</li>
</ol>
<hr>
<p>For simple linear regression, <span class="math inline">\(R^2\)</span> is the square of the Pearson correlation coefficient, <span class="math inline">\(r\)</span>:</p>
<p><span class="math display">\[
R^2 = (r)^2,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(r = \text{corr}(X, Y)\)</span> is the sample correlation coefficient.</li>
</ul>
<p>The relationship between <span class="math inline">\(b_1\)</span> (the slope of the regression line) and <span class="math inline">\(r\)</span> is given by:</p>
<p><span class="math display">\[
b_1 = \left(\frac{\sum_{i=1}^n (Y_i - \bar{Y})^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\right)^{1/2}.
\]</span></p>
<p>Additionally, <span class="math inline">\(r\)</span> can be expressed as:</p>
<p><span class="math display">\[
r = \frac{s_y}{s_x} \cdot r,
\]</span></p>
<p>where <span class="math inline">\(s_y\)</span> and <span class="math inline">\(s_x\)</span> are the sample standard deviations of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, respectively.</p>
</div>
<div id="lack-of-fit-in-regression" class="section level4" number="5.1.1.9">
<h4>
<span class="header-section-number">5.1.1.9</span> Lack of Fit in Regression<a class="anchor" aria-label="anchor" href="#lack-of-fit-in-regression"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>lack of fit</strong> test evaluates whether the chosen regression model adequately captures the relationship between the predictor variable <span class="math inline">\(X\)</span> and the response variable <span class="math inline">\(Y\)</span>. When there are repeated observations at specific values of <span class="math inline">\(X\)</span>, we can partition the Error Sum of Squares (<span class="math inline">\(SSE\)</span>) into two components:</p>
<ol style="list-style-type: decimal">
<li><strong>Pure Error</strong></li>
<li>
<strong>Lack of Fit</strong>.</li>
</ol>
<p>Given the observations:</p>
<ul>
<li>
<span class="math inline">\(Y_{ij}\)</span>: The <span class="math inline">\(j\)</span>-th replicate for the <span class="math inline">\(i\)</span>-th distinct value of <span class="math inline">\(X\)</span>,
<ul>
<li>
<span class="math inline">\(Y_{11}, Y_{21}, \dots, Y_{n_1, 1}\)</span>: <span class="math inline">\(n_1\)</span> repeated observations of <span class="math inline">\(X_1\)</span>
</li>
<li>
<span class="math inline">\(Y_{1c}, Y_{2c}, \dots, Y_{n_c,c}\)</span>: <span class="math inline">\(n_c\)</span> repeated observations of <span class="math inline">\(X_c\)</span>
</li>
</ul>
</li>
<li>
<span class="math inline">\(\bar{Y}_j\)</span>: The mean response for replicates at <span class="math inline">\(X_j\)</span>,</li>
<li>
<span class="math inline">\(\hat{Y}_{ij}\)</span>: The predicted value from the regression model for <span class="math inline">\(X_j\)</span>,</li>
</ul>
<p>the Error Sum of Squares (<span class="math inline">\(SSE\)</span>) can be decomposed as:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i} \sum_{j} (Y_{ij} - \hat{Y}_{ij})^2 &amp;= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j + \bar{Y}_j - \hat{Y}_{ij})^2 \\
&amp;= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{j} n_j (\bar{Y}_j - \hat{Y}_{ij})^2 + \text{cross product term} \\
&amp;= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{j} n_j (\bar{Y}_j - \hat{Y}_{ij})^2
\end{aligned}
\]</span></p>
<ul>
<li>The <strong>cross product term</strong> is zero because the deviations within replicates and the deviations between replicates are orthogonal.</li>
<li>This simplifies to:</li>
</ul>
<p><span class="math display">\[
SSE = SSPE + SSLF,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(SSPE\)</span> (Pure Error Sum of Squares): Variation within replicates for the same <span class="math inline">\(X_j\)</span>, reflecting natural variability in the response.
<ul>
<li>Degrees of freedom: <span class="math inline">\(df_{pe} = n - c\)</span>, where <span class="math inline">\(n\)</span> is the total number of observations, and <span class="math inline">\(c\)</span> is the number of distinct <span class="math inline">\(X\)</span> values.</li>
</ul>
</li>
<li>
<span class="math inline">\(SSLF\)</span> (Lack of Fit Sum of Squares): Variation between the replicate means <span class="math inline">\(\bar{Y}_j\)</span> and the model-predicted values <span class="math inline">\(\hat{Y}_{ij}\)</span>. If SSLF is large, it suggests the model may not adequately describe the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
<ul>
<li>Degrees of freedom: <span class="math inline">\(df_{lf} = c - 2\)</span>, where 2 accounts for the parameters in the linear regression model (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>).</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p><strong>Mean Square for Pure Error (MSPE):</strong><br><span class="math display">\[
MSPE = \frac{SSPE}{df_{pe}} = \frac{SSPE}{n-c}.
\]</span></p></li>
<li><p><strong>Mean Square for Lack of Fit (MSLF):</strong><br><span class="math display">\[
MSLF = \frac{SSLF}{df_{lf}} = \frac{SSLF}{c-2}.
\]</span></p></li>
</ul>
<div id="the-f-test-for-lack-of-fit" class="section level5" number="5.1.1.9.1">
<h5>
<span class="header-section-number">5.1.1.9.1</span> The F-Test for Lack of Fit<a class="anchor" aria-label="anchor" href="#the-f-test-for-lack-of-fit"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>F-test for lack of fit</strong> evaluates whether the chosen regression model adequately captures the relationship between the predictor variable <span class="math inline">\(X\)</span> and the response variable <span class="math inline">\(Y\)</span>. Specifically, it tests whether any systematic deviations from the model exist that are not accounted for by random error.</p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>):<br>
The regression model is adequate: <span class="math display">\[
H_0: Y_{ij} = \beta_0 + \beta_1 X_i + \epsilon_{ij}, \quad \epsilon_{ij} \sim \text{i.i.d. } N(0, \sigma^2).
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_a\)</span>):<br>
The regression model is not adequate and includes an additional function <span class="math inline">\(f(X_i, Z_1, \dots)\)</span> to account for the lack of fit: <span class="math display">\[
H_a: Y_{ij} = \alpha_0 + \alpha_1 X_i + f(X_i, Z_1, \dots) + \epsilon_{ij}^*, \quad \epsilon_{ij}^* \sim \text{i.i.d. } N(0, \sigma^2).
\]</span></p></li>
</ul>
<hr>
<p><strong>Expected Mean Squares</strong></p>
<ul>
<li>
<p>The expected Mean Square for Pure Error (MSPE) is the same under both <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span>:</p>
<p><span class="math display">\[
E(MSPE) = \sigma^2.
\]</span></p>
</li>
<li>
<p>The expected Mean Square for Lack of Fit (MSLF) depends on whether <span class="math inline">\(H_0\)</span> is true:</p>
<ul>
<li>Under <span class="math inline">\(H_0\)</span> (model is adequate): <span class="math display">\[
E(MSLF) = \sigma^2.
\]</span>
</li>
<li>Under <span class="math inline">\(H_a\)</span> (model is not adequate): <span class="math display">\[
E(MSLF) = \sigma^2 + \frac{\sum n_j f(X_i, Z_1, \dots)^2}{n-2}.
\]</span>
</li>
</ul>
</li>
</ul>
<hr>
<p>The test statistic for the lack-of-fit test is:</p>
<p><span class="math display">\[
F = \frac{MSLF}{MSPE},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(MSLF = \frac{SSLF}{c-2}\)</span>,<br>
and <span class="math inline">\(SSLF\)</span> is the Lack of Fit Sum of Squares.</li>
<li>
<span class="math inline">\(MSPE = \frac{SSPE}{n-c}\)</span>,<br>
and <span class="math inline">\(SSPE\)</span> is the Pure Error Sum of Squares.</li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>, the <span class="math inline">\(F\)</span>-statistic follows an <span class="math inline">\(F\)</span>-distribution:</p>
<p><span class="math display">\[
F \sim F_{c-2, n-c}.
\]</span></p>
<hr>
<p><strong>Decision Rule</strong></p>
<ul>
<li><p>Reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if: <span class="math display">\[
F &gt; F_{1-\alpha; c-2, n-c}.
\]</span></p></li>
<li>
<p><strong>Failing to reject</strong> <span class="math inline">\(H_0\)</span>:</p>
<ul>
<li>Indicates that there is no evidence of lack of fit.</li>
<li>Does not imply the model is “true,” but it suggests that the model provides a reasonable approximation to the true relationship.</li>
</ul>
</li>
</ul>
<p>To summarize, when repeat observations exist at some levels of <span class="math inline">\(X\)</span>, the <strong>Error Sum of Squares (SSE)</strong> can be further partitioned into <strong>Lack of Fit (SSLF)</strong> and <strong>Pure Error (SSPE)</strong>. This leads to an extended ANOVA table:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="19%">
<col width="19%">
<col width="19%">
<col width="21%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th>Sum of Squares</th>
<th>df</th>
<th>Mean Square</th>
<th>F Statistic</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td>SSR</td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(MSR = \frac{SSR}{1}\)</span></td>
<td><span class="math inline">\(F = \frac{MSR}{MSE}\)</span></td>
</tr>
<tr class="even">
<td>Error</td>
<td>SSE</td>
<td><span class="math inline">\(n-2\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{n-2}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Lack of fit</td>
<td>SSLF</td>
<td><span class="math inline">\(c-2\)</span></td>
<td><span class="math inline">\(MSLF = \frac{SSLF}{c-2}\)</span></td>
<td><span class="math inline">\(F = \frac{MSLF}{MSPE}\)</span></td>
</tr>
<tr class="even">
<td>Pure Error</td>
<td>SSPE</td>
<td><span class="math inline">\(n-c\)</span></td>
<td><span class="math inline">\(MSPE = \frac{SSPE}{n-c}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total (Corrected)</td>
<td>SSTO</td>
<td><span class="math inline">\(n-1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Repeat observations have important implications for the coefficient of determination (<span class="math inline">\(R^2\)</span>):</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(R^2\)</span> Can’t Attain 1 with Repeat Observations:
<ul>
<li>With repeat observations, <span class="math inline">\(SSE\)</span> (Error Sum of Squares) cannot be reduced to 0 because <span class="math inline">\(SSPE &gt; 0\)</span> (variability within replicates).</li>
</ul>
</li>
<li>
<strong>Maximum</strong> <span class="math inline">\(R^2\)</span>:
<ul>
<li>
<p>The maximum attainable <span class="math inline">\(R^2\)</span> in the presence of repeat observations is:</p>
<p><span class="math display">\[
R^2_{\text{max}} = \frac{SSTO - SSPE}{SSTO}.
\]</span></p>
</li>
</ul>
</li>
<li>
<strong>Importance of Repeat Observations:</strong>
<ul>
<li>Not all levels of <span class="math inline">\(X\)</span> need repeat observations, but their presence enables the separation of pure error from lack of fit, making the <span class="math inline">\(F\)</span>-test for lack of fit possible.</li>
</ul>
</li>
</ol>
<hr>
<p>Estimation of <span class="math inline">\(\sigma^2\)</span> with Repeat Observations</p>
<ol style="list-style-type: decimal">
<li>
<strong>Use of MSE:</strong>
<ul>
<li>When <span class="math inline">\(H_0\)</span> is appropriate (the model fits well), <span class="math inline">\(MSE\)</span> is typically used as the estimate of <span class="math inline">\(\sigma^2\)</span> instead of <span class="math inline">\(MSPE\)</span> because it has more degrees of freedom and provides a more reliable estimate.</li>
</ul>
</li>
<li>
<strong>Pooling Estimates:</strong>
<ul>
<li>In practice, <span class="math inline">\(MSE\)</span> and <span class="math inline">\(MSPE\)</span> may be pooled if <span class="math inline">\(H_0\)</span> holds, resulting in a more precise estimate of <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="joint-inference-for-regression-parameters" class="section level4" number="5.1.1.10">
<h4>
<span class="header-section-number">5.1.1.10</span> Joint Inference for Regression Parameters<a class="anchor" aria-label="anchor" href="#joint-inference-for-regression-parameters"><i class="fas fa-link"></i></a>
</h4>
<p>Joint inference considers the simultaneous coverage of confidence intervals for multiple regression parameters, such as <span class="math inline">\(\beta_0\)</span> (intercept) and <span class="math inline">\(\beta_1\)</span> (slope). Ensuring adequate confidence for both parameters together requires adjustments to maintain the desired family-wise confidence level.</p>
<p>Let:</p>
<ul>
<li>
<span class="math inline">\(\bar{A}_1\)</span>: The event that the confidence interval for <span class="math inline">\(\beta_0\)</span> covers its true value.</li>
<li>
<span class="math inline">\(\bar{A}_2\)</span>: The event that the confidence interval for <span class="math inline">\(\beta_1\)</span> covers its true value.</li>
</ul>
<p>The individual confidence levels are:</p>
<p><span class="math display">\[
\begin{aligned}
P(\bar{A}_1) &amp;= 1 - \alpha, \\
P(\bar{A}_2) &amp;= 1 - \alpha.
\end{aligned}
\]</span></p>
<p>The joint confidence coefficient, <span class="math inline">\(P(\bar{A}_1 \cap \bar{A}_2)\)</span>, is:</p>
<p><span class="math display">\[
\begin{aligned}
P(\bar{A}_1 \cap \bar{A}_2) &amp;= 1 - P(\bar{A}_1 \cup \bar{A}_2), \\
&amp;= 1 - P(A_1) - P(A_2) + P(A_1 \cap A_2), \\
&amp;\geq 1 - P(A_1) - P(A_2), \\
&amp;= 1 - 2\alpha.
\end{aligned}
\]</span></p>
<p>This means that if <span class="math inline">\(\alpha\)</span> is the significance level for each parameter, the joint confidence coefficient is at least <span class="math inline">\(1 - 2\alpha\)</span>. This inequality is known as the <strong>Bonferroni Inequality</strong>.</p>
<hr>
<p><strong>Bonferroni Confidence Intervals</strong></p>
<p>To ensure a desired joint confidence level of <span class="math inline">\((1-\alpha)\)</span> for both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the Bonferroni method adjusts the confidence level for each parameter by dividing <span class="math inline">\(\alpha\)</span> by the number of parameters. For two parameters:</p>
<ol style="list-style-type: decimal">
<li><p>The confidence level for each parameter is <span class="math inline">\((1-\alpha/2)\)</span>.</p></li>
<li>
<p>The resulting <strong>Bonferroni-adjusted confidence intervals</strong> are:</p>
<p><span class="math display">\[
\begin{aligned}
b_0 &amp;\pm B \cdot s(b_0), \\
b_1 &amp;\pm B \cdot s(b_1),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(B = t_{1-\alpha/4; n-2}\)</span> is the critical value from the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
</li>
</ol>
<hr>
<p><strong>Interpretation of Bonferroni Confidence Intervals</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Coverage Probability:</strong>
<ul>
<li>If repeated samples were taken, <span class="math inline">\((1-\alpha)100\%\)</span> of the joint intervals would contain the true values of <span class="math inline">\((\beta_0, \beta_1)\)</span>.</li>
<li>This implies that <span class="math inline">\(\alpha \times 100\%\)</span> of the samples would miss at least one of the true parameter values.</li>
</ul>
</li>
<li>
<strong>Conservatism:</strong>
<ul>
<li>The Bonferroni method ensures the family-wise confidence level but is <strong>conservative</strong>. The actual joint confidence level is often higher than <span class="math inline">\((1-\alpha)100\%\)</span>.</li>
<li>This conservatism reduces statistical power.</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb164"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate synthetic data</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span>  <span class="co"># Number of observations</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Predictor</span></span>
<span><span class="va">beta_0</span> <span class="op">&lt;-</span> <span class="fl">2</span>  <span class="co"># True intercept</span></span>
<span><span class="va">beta_1</span> <span class="op">&lt;-</span> <span class="fl">3</span>  <span class="co"># True slope</span></span>
<span><span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span>  <span class="co"># Standard deviation of error</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">beta_0</span> <span class="op">+</span> <span class="va">beta_1</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span>  <span class="co"># Response</span></span>
<span></span>
<span><span class="co"># Fit linear model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ x)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.9073 -0.6835 -0.0875  0.5806  3.2904 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  1.89720    0.09755   19.45   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x            2.94753    0.10688   27.58   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.9707 on 98 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8859, Adjusted R-squared:  0.8847 </span></span>
<span><span class="co">#&gt; F-statistic: 760.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="co"># Extract coefficients and standard errors</span></span>
<span><span class="va">b0_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">b1_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">s_b0</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span>  <span class="co"># Standard error of intercept</span></span>
<span><span class="va">s_b1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span>  <span class="co"># Standard error of slope</span></span>
<span></span>
<span><span class="co"># Desired confidence level</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span>  <span class="co"># Overall significance level</span></span>
<span></span>
<span><span class="co"># Bonferroni correction</span></span>
<span><span class="va">adjusted_alpha</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span>  <span class="co"># Adjusted alpha for each parameter</span></span>
<span></span>
<span><span class="co"># Critical t-value for Bonferroni adjustment</span></span>
<span><span class="va">t_crit</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">adjusted_alpha</span>, df <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># n-2 degrees of freedom</span></span>
<span></span>
<span><span class="co"># Bonferroni confidence intervals</span></span>
<span><span class="va">ci_b0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">b0_hat</span> <span class="op">-</span> <span class="va">t_crit</span> <span class="op">*</span> <span class="va">s_b0</span>, <span class="va">b0_hat</span> <span class="op">+</span> <span class="va">t_crit</span> <span class="op">*</span> <span class="va">s_b0</span><span class="op">)</span></span>
<span><span class="va">ci_b1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">b1_hat</span> <span class="op">-</span> <span class="va">t_crit</span> <span class="op">*</span> <span class="va">s_b1</span>, <span class="va">b1_hat</span> <span class="op">+</span> <span class="va">t_crit</span> <span class="op">*</span> <span class="va">s_b1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Bonferroni Confidence Intervals:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Bonferroni Confidence Intervals:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Intercept (beta_0): ["</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">ci_b0</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>    <span class="st">","</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">ci_b0</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>    <span class="st">"]\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Intercept (beta_0): [ 1.7 , 2.09 ]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Slope (beta_1): ["</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">ci_b1</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>    <span class="st">","</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">ci_b1</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>    <span class="st">"]\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Slope (beta_1): [ 2.74 , 3.16 ]</span></span>
<span></span>
<span><span class="co"># Calculate the covariance matrix of coefficients</span></span>
<span><span class="va">cov_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate points for confidence ellipse</span></span>
<span><span class="va">ellipse_points</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>                  mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span>,</span>
<span>                  Sigma <span class="op">=</span> <span class="va">cov_matrix</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Convert to data frame for plotting</span></span>
<span><span class="va">ellipse_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">ellipse_points</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">ellipse_df</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"beta_0"</span>, <span class="st">"beta_1"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot confidence intervals and ellipse</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="co"># Confidence ellipse</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span></span>
<span>        data <span class="op">=</span> <span class="va">ellipse_df</span>,</span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">beta_0</span>, y <span class="op">=</span> <span class="va">beta_1</span><span class="op">)</span>,</span>
<span>        alpha <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"grey"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="co"># Point estimate</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">b0_hat</span>, y <span class="op">=</span> <span class="va">b1_hat</span><span class="op">)</span>,</span>
<span>               color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>               size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="co"># Bonferroni confidence intervals</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">b0_hat</span>, ymin <span class="op">=</span> <span class="va">ci_b1</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, ymax <span class="op">=</span> <span class="va">ci_b1</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>                  width <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                  color <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_errorbarh.html">geom_errorbarh</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">b1_hat</span>, xmin <span class="op">=</span> <span class="va">ci_b0</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, xmax <span class="op">=</span> <span class="va">ci_b0</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>,</span>
<span>                   height <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                   color <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Bonferroni Confidence Intervals and Joint Confidence Region"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Intercept (beta_0)"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Slope (beta_1)"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;"></div>
<ol style="list-style-type: decimal">
<li>The red point represents the estimated coefficients (b0_hat, b1_hat).</li>
<li>The blue lines represent the Bonferroni-adjusted confidence intervals for beta_0 and beta_1.</li>
<li>The grey points represent the joint confidence region based on the covariance matrix of coefficients.</li>
<li>The Bonferroni intervals ensure family-wise confidence level but are conservative.</li>
<li>Simulation results demonstrate how often the true values are captured in the intervals when repeated samples are drawn.</li>
</ol>
<p><strong>Notes</strong>:</p>
<ol style="list-style-type: decimal">
<li>Conservatism of Bonferroni Intervals
<ul>
<li>The <strong>Bonferroni interval is conservative</strong>:
<ul>
<li>The joint confidence level is a lower bound, ensuring family-wise coverage of at least <span class="math inline">\((1-\alpha)100\%\)</span>.</li>
<li>This conservatism results in wider intervals, reducing the statistical power of the test.</li>
</ul>
</li>
<li>Adjustments for Conservatism:
<ul>
<li>Practitioners often choose a larger <span class="math inline">\(\alpha\)</span> (e.g., <span class="math inline">\(\alpha = 0.1\)</span>) to reduce the width of the intervals in Bonferroni joint tests.</li>
<li>A higher <span class="math inline">\(\alpha\)</span> allows for a better balance between confidence and precision, especially for exploratory analyses.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Extending Bonferroni to Multiple Parameters</strong>: The Bonferroni method is not limited to two parameters. For testing <span class="math inline">\(g\)</span> parameters, such as <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_{g-1}\)</span>:
<ul>
<li>
<strong>Adjusted Confidence Level for Each Parameter:</strong>
<ul>
<li>The confidence level for each individual parameter is <span class="math inline">\((1-\alpha/g)\)</span>.</li>
</ul>
</li>
<li>
<strong>Critical</strong> <span class="math inline">\(t\)</span>-Value:
<ul>
<li>For two-sided intervals, the critical value for each parameter is: <span class="math display">\[
t_{1-\frac{\alpha}{2g}; n-p},
\]</span> where <span class="math inline">\(p\)</span> is the total number of parameters in the regression model.</li>
</ul>
</li>
<li>
<strong>Example:</strong>
<ul>
<li>If <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(g = 10\)</span>, each individual confidence interval is constructed at the: <span class="math display">\[
(1 - \frac{0.05}{10}) = 99.5\% \text{ confidence level}.
\]</span>
</li>
<li>This corresponds to using <span class="math inline">\(t_{1-\frac{0.005}{2}; n-p}\)</span> in the formula for the confidence intervals.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Limitations</strong> for Large <span class="math inline">\(g\)</span>
<ul>
<li>
<strong>Wide Intervals:</strong>
<ul>
<li>As <span class="math inline">\(g\)</span> increases, the intervals become excessively wide, often leading to reduced usefulness in practical applications.</li>
<li>This issue stems from the conservatism of the Bonferroni method, which prioritizes family-wise error control.</li>
</ul>
</li>
<li>
<strong>Suitability for Small</strong> <span class="math inline">\(g\)</span>:
<ul>
<li>The Bonferroni procedure works well when <span class="math inline">\(g\)</span> is relatively small (e.g., <span class="math inline">\(g \leq 5\)</span>).</li>
<li>For larger <span class="math inline">\(g\)</span>, alternative methods (discussed below) are more efficient.</li>
</ul>
</li>
</ul>
</li>
<li>Correlation Between Parameters: Correlation of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>:
<ul>
<li>The estimated regression coefficients <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are often correlated:
<ul>
<li>
<strong>Negative correlation</strong> if <span class="math inline">\(\bar{X} &gt; 0\)</span>.</li>
<li>
<strong>Positive correlation</strong> if <span class="math inline">\(\bar{X} &lt; 0\)</span>.</li>
</ul>
</li>
<li>This correlation can complicate joint inference but does not affect the validity of Bonferroni-adjusted intervals.</li>
</ul>
</li>
<li>Alternatives to Bonferroni</li>
</ol>
<p>Several alternative procedures provide more precise joint inference, especially for larger <span class="math inline">\(g\)</span>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Scheffé’s Method:</strong>
<ul>
<li>Constructs simultaneous confidence regions for all possible linear combinations of parameters.</li>
<li>Suitable for exploratory analyses but may result in even wider intervals than Bonferroni.</li>
</ul>
</li>
<li>
<strong>Tukey’s Honest Significant Difference:</strong>
<ul>
<li>Designed for pairwise comparisons in ANOVA but can be adapted for regression parameters.</li>
</ul>
</li>
<li>
<strong>Holm’s Step-Down Procedure:</strong>
<ul>
<li>A sequential testing procedure that is less conservative than Bonferroni while still controlling the family-wise error rate.</li>
</ul>
</li>
<li>
<strong>Likelihood Ratio Tests:</strong>
<ul>
<li>Construct joint confidence regions based on the likelihood function, offering more precision for large <span class="math inline">\(g\)</span>.</li>
</ul>
</li>
</ol>
</div>
<div id="assumptions-of-linear-regression" class="section level4" number="5.1.1.11">
<h4>
<span class="header-section-number">5.1.1.11</span> Assumptions of Linear Regression<a class="anchor" aria-label="anchor" href="#assumptions-of-linear-regression"><i class="fas fa-link"></i></a>
</h4>
<p>To ensure valid inference and reliable predictions in linear regression, the following assumptions must hold. We’ll cover them in depth in the next section.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="33%">
<col width="66%">
</colgroup>
<thead><tr class="header">
<th><strong>Assumption</strong></th>
<th><strong>Description</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Linearity</strong></td>
<td>Linear relationship between predictors and response.</td>
</tr>
<tr class="even">
<td><strong>Independence of Errors</strong></td>
<td>Errors are independent (important in time-series/clustering).</td>
</tr>
<tr class="odd">
<td><strong>Homoscedasticity</strong></td>
<td>Constant variance of residuals across predictors.</td>
</tr>
<tr class="even">
<td><strong>Normality of Errors</strong></td>
<td>Residuals are normally distributed.</td>
</tr>
<tr class="odd">
<td><strong>No Multicollinearity</strong></td>
<td>Predictors are not highly correlated.</td>
</tr>
<tr class="even">
<td><strong>No Outliers/Leverage Points</strong></td>
<td>No undue influence from outliers or high-leverage points.</td>
</tr>
<tr class="odd">
<td><strong>Exogeneity</strong></td>
<td>Predictors are uncorrelated with the error term (no endogeneity).</td>
</tr>
<tr class="even">
<td><strong>Full Rank</strong></td>
<td>Predictors are linearly independent (no perfect multicollinearity).</td>
</tr>
</tbody>
</table></div>
</div>
<div id="diagnostics-for-model-assumptions" class="section level4" number="5.1.1.12">
<h4>
<span class="header-section-number">5.1.1.12</span> Diagnostics for Model Assumptions<a class="anchor" aria-label="anchor" href="#diagnostics-for-model-assumptions"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Constant Variance</strong></p>
<ul>
<li>To check homoscedasticity:
<ul>
<li>Plot residuals vs. fitted values or residuals vs. predictors.</li>
<li>Look for patterns or a funnel-shaped spread indicating heteroscedasticity.</li>
</ul>
</li>
</ul>
<p><strong>Outliers</strong></p>
<ul>
<li>Detect outliers using:
<ul>
<li>Residuals vs. predictors plot.</li>
<li>Box plots.</li>
<li>Stem-and-leaf plots.</li>
<li>Scatter plots.</li>
</ul>
</li>
</ul>
<p><strong>Standardized Residuals</strong>:</p>
<p>Residuals can be standardized to have unit variance, known as <strong>studentized residuals</strong>: <span class="math display">\[
  r_i = \frac{e_i}{s(e_i)}.
  \]</span></p>
<p><strong>Semi-Studentized Residuals</strong>:</p>
<p>A simplified standardization using the mean squared error (MSE): <span class="math display">\[
  e_i^* = \frac{e_i}{\sqrt{MSE}}.
  \]</span></p>
<p><strong>Non-Independent Error Terms</strong></p>
<ul>
<li>To detect non-independence:
<ul>
<li>Plot residuals vs. time for time-series data.</li>
<li>Residuals <span class="math inline">\(e_i\)</span> are not independent because they depend on <span class="math inline">\(\hat{Y}_i\)</span>, which is derived from the same regression function.</li>
</ul>
</li>
<li>Detect dependency by plotting the residual for the <span class="math inline">\(i\)</span>-th response vs. the <span class="math inline">\((i-1)\)</span>-th.</li>
</ul>
<p><strong>Non-Normality of Error Terms</strong></p>
<ul>
<li>To assess normality:
<ul>
<li>Plot distribution of residuals.</li>
<li>Create box plots, stem-and-leaf plots, or normal probability plots.</li>
</ul>
</li>
<li>Issues such as an incorrect regression function or non-constant error variance can distort the residual distribution.</li>
<li>Normality tests require relatively large sample sizes to detect deviations.</li>
</ul>
<p><strong>Normality of Residuals</strong></p>
<ul>
<li>Use tests based on the empirical cumulative distribution function (ECDF) (check <a href="descriptive-statistics.html#normality-assessment">Normality Assessment</a>)</li>
</ul>
<p><strong>Constancy of Error Variance</strong></p>
<ul>
<li>Statistical tests for homoscedasticity:
<ul>
<li>
<strong>Brown-Forsythe Test (Modified Levene Test)</strong>:
<ul>
<li>Robust against non-normality, examines the variance of residuals across levels of predictors.</li>
</ul>
</li>
<li>
<strong>Breusch-Pagan Test (Cook-Weisberg Test)</strong>:
<ul>
<li>Tests for heteroscedasticity by regressing squared residuals on predictors.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div id="remedial-measures-for-violations-of-assumptions" class="section level4" number="5.1.1.13">
<h4>
<span class="header-section-number">5.1.1.13</span> Remedial Measures for Violations of Assumptions<a class="anchor" aria-label="anchor" href="#remedial-measures-for-violations-of-assumptions"><i class="fas fa-link"></i></a>
</h4>
<p>When the assumptions of simple linear regression are violated, appropriate remedial measures can be applied to address these issues. Below is a list of measures for specific deviations from the assumptions.</p>
<div id="general-remedies" class="section level5" number="5.1.1.13.1">
<h5>
<span class="header-section-number">5.1.1.13.1</span> General Remedies<a class="anchor" aria-label="anchor" href="#general-remedies"><i class="fas fa-link"></i></a>
</h5>
<ul>
<li>Use more <strong>complicated models</strong> (e.g., non-linear models, generalized linear models).</li>
<li>Apply <strong>transformations</strong> (see <a href="variable-transformation.html#variable-transformation">Variable Transformation</a>) on <span class="math inline">\(X\)</span> and/or <span class="math inline">\(Y\)</span> to stabilize variance, linearize relationships, or normalize residuals. Note that transformations may not always yield “optimal” results.</li>
</ul>
<hr>
</div>
<div id="specific-remedies-for-assumption-violations" class="section level5" number="5.1.1.13.2">
<h5>
<span class="header-section-number">5.1.1.13.2</span> Specific Remedies for Assumption Violations<a class="anchor" aria-label="anchor" href="#specific-remedies-for-assumption-violations"><i class="fas fa-link"></i></a>
</h5>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="22%">
<col width="36%">
<col width="41%">
</colgroup>
<thead><tr class="header">
<th><strong>Issue</strong></th>
<th><strong>Remedy</strong></th>
<th><strong>Explanation</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Non-Linearity</strong></td>
<td>- Apply transformations (e.g., log, square root).</td>
<td>Transformation of variables can help linearize the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</td>
</tr>
<tr class="even">
<td></td>
<td>- Use more complicated models (e.g., polynomial regression, splines).</td>
<td>Higher-order terms or non-linear models can capture non-linear relationships.</td>
</tr>
<tr class="odd">
<td><strong>Non-Constant Error Variance</strong></td>
<td>- Apply <strong>Weighted Least Squares</strong>.</td>
<td>WLS assigns weights to observations based on the inverse of their variance.</td>
</tr>
<tr class="even">
<td></td>
<td>- Use transformations (e.g., log, square root).</td>
<td>Transformations can stabilize error variance.</td>
</tr>
<tr class="odd">
<td><strong>Correlated Errors</strong></td>
<td>- Use serially correlated error models (e.g., ARIMA for time-series data).</td>
<td>Time-series models account for serial dependence in the errors.</td>
</tr>
<tr class="even">
<td><strong>Non-Normality of Errors</strong></td>
<td>- Transform <span class="math inline">\(Y\)</span> or use non-parametric methods.</td>
<td>Transformations can normalize residuals; non-parametric methods do not assume normality.</td>
</tr>
<tr class="odd">
<td><strong>Omitted Variables</strong></td>
<td>- Use multiple regression to include additional relevant predictors.</td>
<td>Adding relevant variables reduces omitted variable bias and improves model accuracy.</td>
</tr>
<tr class="even">
<td><strong>Outliers</strong></td>
<td>- Apply robust estimation techniques (e.g., Huber regression, M-estimation).</td>
<td>Robust methods reduce the influence of outliers on parameter estimates.</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="remedies-in-detail" class="section level5" number="5.1.1.13.3">
<h5>
<span class="header-section-number">5.1.1.13.3</span> Remedies in Detail<a class="anchor" aria-label="anchor" href="#remedies-in-detail"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li>
<strong>Non-Linearity:</strong>
<ul>
<li>Transformations: Apply transformations to the response variable <span class="math inline">\(Y\)</span> or the predictor variable <span class="math inline">\(X\)</span>. Common transformations include:
<ul>
<li>Logarithmic transformation: <span class="math inline">\(Y' = \log(Y)\)</span> or <span class="math inline">\(X' = \log(X)\)</span>.</li>
<li>Polynomial terms: Include <span class="math inline">\(X^2\)</span>, <span class="math inline">\(X^3\)</span>, etc., to capture curvature.</li>
</ul>
</li>
<li>Alternative Models:
<ul>
<li>Polynomial regression or splines for flexibility in modeling non-linear relationships.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Non-Constant Error Variance:</strong>
<ul>
<li>Weighted Least Squares:
<ul>
<li>Assigns weights to observations inversely proportional to their variance.</li>
</ul>
</li>
<li>Transformations:
<ul>
<li>Use a log or square root transformation to stabilize variance.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Correlated Errors:</strong>
<ul>
<li>For time-series data:
<ul>
<li>Use serially correlated error models such as AR(1) or ARIMA.</li>
<li>These models explicitly account for dependency in residuals over time.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Non-Normality:</strong>
<ul>
<li>Transformations:
<ul>
<li>Apply a transformation to <span class="math inline">\(Y\)</span> (e.g., log or square root) to make the residuals approximately normal.</li>
</ul>
</li>
<li>Non-parametric regression:
<ul>
<li>Methods like LOESS or Theil-Sen regression do not require the normality assumption.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Omitted Variables:</strong>
<ul>
<li>Introduce additional predictors:
<ul>
<li>Use multiple regression to include all relevant independent variables.</li>
</ul>
</li>
<li>Check for multicollinearity when adding new variables.</li>
</ul>
</li>
<li>
<strong>Outliers:</strong>
<ul>
<li>Robust Regression:
<ul>
<li>Use methods such as Huber regression or M-estimation to reduce the impact of outliers on model coefficients.</li>
</ul>
</li>
<li>Diagnostics:
<ul>
<li>Identify outliers using Cook’s Distance, leverage statistics, or studentized residuals.</li>
</ul>
</li>
</ul>
</li>
</ol>
</div>
</div>
<div id="transformations-in-regression-analysis" class="section level4" number="5.1.1.14">
<h4>
<span class="header-section-number">5.1.1.14</span> Transformations in Regression Analysis<a class="anchor" aria-label="anchor" href="#transformations-in-regression-analysis"><i class="fas fa-link"></i></a>
</h4>
<p>Transformations involve modifying one or both variables to address issues such as non-linearity, non-constant variance, or non-normality. However, it’s important to note that the properties of least-squares estimates apply to the <strong>transformed model</strong>, not the original variables.</p>
<p>When transforming the dependent variable <span class="math inline">\(Y\)</span>, we fit the model as:</p>
<p><span class="math display">\[
g(Y_i) = b_0 + b_1 X_i,
\]</span></p>
<p>where <span class="math inline">\(g(Y_i)\)</span> is the transformed response. To interpret the regression results in terms of the original <span class="math inline">\(Y\)</span>, we need to <strong>transform back</strong>:</p>
<p><span class="math display">\[
\hat{Y}_i = g^{-1}(b_0 + b_1 X_i).
\]</span></p>
<hr>
<p>Direct back-transformation of predictions can introduce <strong>bias</strong>. For example, in a log-transformed model:</p>
<p><span class="math display">\[
\log(Y_i) = b_0 + b_1 X_i,
\]</span></p>
<p>the unbiased back-transformed prediction of <span class="math inline">\(Y_i\)</span> is:</p>
<p><span class="math display">\[
\hat{Y}_i = \exp(b_0 + b_1 X_i + \frac{\sigma^2}{2}),
\]</span></p>
<p>where <span class="math inline">\(\frac{\sigma^2}{2}\)</span> accounts for the bias correction due to the log transformation.</p>
<hr>
<div id="box-cox-family-of-transformations" class="section level5" number="5.1.1.14.1">
<h5>
<span class="header-section-number">5.1.1.14.1</span> Box-Cox Family of Transformations<a class="anchor" aria-label="anchor" href="#box-cox-family-of-transformations"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>Box-Cox transformation</strong> is a versatile family of transformations defined as:</p>
<p><span class="math display">\[
Y' =
\begin{cases}
\frac{Y^\lambda - 1}{\lambda}, &amp; \text{if } \lambda \neq 0, \\
\ln(Y), &amp; \text{if } \lambda = 0.
\end{cases}
\]</span></p>
<p>This transformation introduces a parameter <span class="math inline">\(\lambda\)</span> that is estimated from the data. Common transformations include:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center"><span class="math inline">\(\lambda\)</span></th>
<th align="center">Transformation <span class="math inline">\(Y'\)</span>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">2</td>
<td align="center"><span class="math inline">\(Y^2\)</span></td>
</tr>
<tr class="even">
<td align="center">0.5</td>
<td align="center"><span class="math inline">\(\sqrt{Y}\)</span></td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center"><span class="math inline">\(\ln(Y)\)</span></td>
</tr>
<tr class="even">
<td align="center">-0.5</td>
<td align="center"><span class="math inline">\(1/\sqrt{Y}\)</span></td>
</tr>
<tr class="odd">
<td align="center">-1</td>
<td align="center"><span class="math inline">\(1/Y\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
<p>Choosing the Transformation Parameter <span class="math inline">\(\lambda\)</span></p>
<p>The value of <span class="math inline">\(\lambda\)</span> can be selected using one of the following methods:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Trial and Error</strong>:
<ul>
<li>Apply different transformations and compare the residual plots or model fit statistics (e.g., <span class="math inline">\(R^2\)</span> or AIC).</li>
</ul>
</li>
<li>
<strong>Maximum Likelihood Estimation</strong>:
<ul>
<li>Choose <span class="math inline">\(\lambda\)</span> to maximize the likelihood function under the assumption of normally distributed errors.</li>
</ul>
</li>
<li>
<strong>Numerical Search</strong>:
<ul>
<li>Use computational optimization techniques to minimize the residual sum of squares (RSS) or another goodness-of-fit criterion.</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb165"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install and load the necessary library</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="st"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">"MASS"</a></span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"MASS"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">50</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">5</span>, sd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">3</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply Box-Cox Transformation</span></span>
<span><span class="va">boxcox_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/boxcox.html">boxcox</a></span><span class="op">(</span><span class="va">model</span>, lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">0.1</span><span class="op">)</span>, plotit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb166"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Find the optimal lambda</span></span>
<span><span class="va">optimal_lambda</span> <span class="op">&lt;-</span> <span class="va">boxcox_result</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="va">boxcox_result</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Optimal lambda for Box-Cox transformation:"</span>, <span class="va">optimal_lambda</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Optimal lambda for Box-Cox transformation: 0.8686869</span></span></code></pre></div>
<p><strong>Notes</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Benefits of Transformations</strong>:</p>
<ul>
<li><p><strong>Stabilize Variance</strong>: Helps address heteroscedasticity.</p></li>
<li><p><strong>Linearize Relationships</strong>: Useful for non-linear data.</p></li>
<li><p><strong>Normalize Residuals</strong>: Addresses non-normality issues.</p></li>
</ul>
</li>
<li>
<p><strong>Caveats</strong>:</p>
<ul>
<li><p>Interpretability: Transformed variables may complicate interpretation.</p></li>
<li><p>Over-Transformation: Excessive transformations can distort the relationship between variables.</p></li>
</ul>
</li>
<li>
<p><strong>Applicability</strong>:</p>
<ul>
<li>Transformations are most effective for issues like non-linearity or non-constant variance. They are less effective for correcting independence violations or omitted variables.</li>
</ul>
</li>
</ol>
</div>
<div id="variance-stabilizing-transformations" class="section level5" number="5.1.1.14.2">
<h5>
<span class="header-section-number">5.1.1.14.2</span> Variance Stabilizing Transformations<a class="anchor" aria-label="anchor" href="#variance-stabilizing-transformations"><i class="fas fa-link"></i></a>
</h5>
<p>Variance stabilizing transformations are used when the standard deviation of the response variable depends on its mean. The <strong>delta method</strong>, which applies a Taylor series expansion, provides a systematic approach to find such transformations.</p>
<p>Given that the standard deviation of <span class="math inline">\(Y\)</span> is a function of its mean:</p>
<p><span class="math display">\[
\sigma = \sqrt{\text{var}(Y)} = f(\mu),
\]</span></p>
<p>where <span class="math inline">\(\mu = E(Y)\)</span> and <span class="math inline">\(f(\mu)\)</span> is a smooth function of the mean, we aim to find a transformation <span class="math inline">\(h(Y)\)</span> such that the variance of the transformed variable <span class="math inline">\(h(Y)\)</span> is constant for all values of <span class="math inline">\(\mu\)</span>.</p>
<p>Expanding <span class="math inline">\(h(Y)\)</span> in a <a href="prerequisites.html#taylor-expansion">Taylor Expansion</a> series around <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
h(Y) = h(\mu) + h'(\mu)(Y - \mu) + \text{higher-order terms}.
\]</span></p>
<p>Ignoring higher-order terms, the variance of <span class="math inline">\(h(Y)\)</span> can be approximated as:</p>
<p><span class="math display">\[
\text{var}(h(Y)) = \text{var}(h(\mu) + h'(\mu)(Y - \mu)).
\]</span></p>
<p>Since <span class="math inline">\(h(\mu)\)</span> is a constant:</p>
<p><span class="math display">\[
\text{var}(h(Y)) = \left(h'(\mu)\right)^2 \text{var}(Y).
\]</span></p>
<p>Substituting <span class="math inline">\(\text{var}(Y) = \left(f(\mu)\right)^2\)</span>, we get:</p>
<p><span class="math display">\[
\text{var}(h(Y)) = \left(h'(\mu)\right)^2 \left(f(\mu)\right)^2.
\]</span></p>
<p>To stabilize the variance (make it constant for all <span class="math inline">\(\mu\)</span>), we require:</p>
<p><span class="math display">\[
\left(h'(\mu)\right)^2 \left(f(\mu)\right)^2 = \text{constant}.
\]</span></p>
<p>Thus, the derivative of <span class="math inline">\(h(\mu)\)</span> must be proportional to the inverse of <span class="math inline">\(f(\mu)\)</span>:</p>
<p><span class="math display">\[
h'(\mu) \propto \frac{1}{f(\mu)}.
\]</span></p>
<p>Integrating both sides gives:</p>
<p><span class="math display">\[
h(\mu) = \int \frac{1}{f(\mu)} \, d\mu.
\]</span></p>
<p>The specific form of <span class="math inline">\(h(\mu)\)</span> depends on the function <span class="math inline">\(f(\mu)\)</span>, which describes the relationship between the standard deviation and the mean.</p>
<hr>
<p>Examples of Variance Stabilizing Transformations</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="33%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th><span class="math inline">\(f(\mu)\)</span></th>
<th>
<strong>Transformation</strong> <span class="math inline">\(h(Y)\)</span>
</th>
<th><strong>Purpose</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\sqrt{\mu}\)</span></td>
<td><span class="math inline">\(\int \frac{1}{\sqrt{\mu}} d\mu = 2\sqrt{Y}\)</span></td>
<td>Stabilizes variance for Poisson data.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\int \frac{1}{\mu} d\mu = \ln(Y)\)</span></td>
<td>Stabilizes variance for exponential or multiplicative models.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mu^2\)</span></td>
<td><span class="math inline">\(\int \frac{1}{\mu^2} d\mu = -\frac{1}{Y}\)</span></td>
<td>Stabilizes variance for certain power law data.</td>
</tr>
</tbody>
</table></div>
<p>Variance stabilizing transformations are particularly useful for:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Poisson-distributed data</strong>: Use <span class="math inline">\(h(Y) = 2\sqrt{Y}\)</span> to stabilize variance.</li>
<li>
<strong>Exponential or multiplicative models</strong>: Use <span class="math inline">\(h(Y) = \ln(Y)\)</span> for stabilization.</li>
<li>
<strong>Power law relationships</strong>: Use transformations like <span class="math inline">\(h(Y) = Y^{-1}\)</span> or other forms derived from <span class="math inline">\(f(\mu)\)</span>.</li>
</ol>
<p>Example: Variance Stabilizing Transformation for the Poisson Distribution</p>
<p>For a <strong>Poisson distribution</strong>, the variance of <span class="math inline">\(Y\)</span> is equal to its mean:</p>
<p><span class="math display">\[
\sigma^2 = \text{var}(Y) = E(Y) = \mu.
\]</span></p>
<p>Thus, the standard deviation is:</p>
<p><span class="math display">\[
\sigma = f(\mu) = \sqrt{\mu}.
\]</span></p>
<p>Using the relationship for variance stabilizing transformations:</p>
<p><span class="math display">\[
h'(\mu) \propto \frac{1}{f(\mu)} = \mu^{-0.5}.
\]</span></p>
<p>Integrating <span class="math inline">\(h'(\mu)\)</span> gives the variance stabilizing transformation:</p>
<p><span class="math display">\[
h(\mu) = \int \mu^{-0.5} \, d\mu = 2\sqrt{\mu}.
\]</span></p>
<p>Hence, the variance stabilizing transformation is:</p>
<p><span class="math display">\[
h(Y) = \sqrt{Y}.
\]</span></p>
<p>This transformation is widely used in Poisson regression to stabilize the variance of the response variable.</p>
<div class="sourceCode" id="cb167"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate Poisson data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">5</span>, sd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="va">n</span>, lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Poisson-distributed Y</span></span>
<span></span>
<span><span class="co"># Fit linear model without transformation</span></span>
<span><span class="va">model_raw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply square root transformation</span></span>
<span><span class="va">y_trans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">model_trans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y_trans</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare residual plots</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span>, mar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Residual plot for raw data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">model_raw</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">model_raw</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Residuals Raw Data"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Fitted Values"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Residuals"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">"red"</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Residual plot for transformed data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">model_trans</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">model_trans</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Residuals: Transformed Data (sqrt(Y))"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Fitted Values"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Residuals"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="general-strategy-when-fmu-is-unknown" class="section level5" number="5.1.1.14.3">
<h5>
<span class="header-section-number">5.1.1.14.3</span> General Strategy When <span class="math inline">\(f(\mu)\)</span> Is Unknown<a class="anchor" aria-label="anchor" href="#general-strategy-when-fmu-is-unknown"><i class="fas fa-link"></i></a>
</h5>
<p>If the relationship between <span class="math inline">\(\text{var}(Y)\)</span> and <span class="math inline">\(\mu\)</span> (i.e., <span class="math inline">\(f(\mu)\)</span>) is unknown, the following steps can help:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Trial and Error</strong>:
<ul>
<li>Apply common transformations (e.g., <span class="math inline">\(\log(Y)\)</span>, <span class="math inline">\(\sqrt{Y}\)</span>) and examine residual plots.</li>
<li>Select the transformation that results in stabilized variance (residuals show no pattern in plots).</li>
</ul>
</li>
<li>
<strong>Leverage Prior Research</strong>:
<ul>
<li>Consult researchers or literature on similar experiments to determine the transformations typically used.</li>
</ul>
</li>
<li>
<strong>Analyze Observations with the Same Predictor Value</strong>:
<ul>
<li>If multiple observations <span class="math inline">\(Y_{ij}\)</span> are available at the same <span class="math inline">\(X\)</span> value:
<ul>
<li>Compute the mean <span class="math inline">\(\bar{Y}_i\)</span> and standard deviation <span class="math inline">\(s_i\)</span> for each group.</li>
<li>Check if <span class="math inline">\(s_i \propto \bar{Y}_i^{\lambda}\)</span>.
<ul>
<li>For example, assume: <span class="math display">\[
s_i = a \bar{Y}_i^{\lambda}.
\]</span>
</li>
<li>Taking the natural logarithm: <span class="math display">\[
\ln(s_i) = \ln(a) + \lambda \ln(\bar{Y}_i).
\]</span>
</li>
</ul>
</li>
<li>Perform a regression of <span class="math inline">\(\ln(s_i)\)</span> on <span class="math inline">\(\ln(\bar{Y}_i)\)</span> to estimate <span class="math inline">\(\lambda\)</span> and suggest the form of <span class="math inline">\(f(\mu)\)</span>.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Group Observations</strong>:
<ul>
<li>If individual observations are sparse, try grouping similar observations by <span class="math inline">\(X\)</span> values to compute <span class="math inline">\(\bar{Y}_i\)</span> and <span class="math inline">\(s_i\)</span> for each group.</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="common-transformations-and-their-applications" class="section level5" number="5.1.1.14.4">
<h5>
<span class="header-section-number">5.1.1.14.4</span> Common Transformations and Their Applications<a class="anchor" aria-label="anchor" href="#common-transformations-and-their-applications"><i class="fas fa-link"></i></a>
</h5>
<p>The table below summarizes common transformations used to stabilize variance under various conditions, along with their appropriate contexts and comments:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="23%">
<col width="30%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th><strong>Transformation</strong></th>
<th><strong>Situation</strong></th>
<th><strong>Comments</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\sqrt{Y}\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, E(Y_i)\)</span></td>
<td>For counts following a Poisson distribution.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sqrt{Y} + \sqrt{Y+1}\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, E(Y_i)\)</span></td>
<td>Useful for small counts or datasets with zeros.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\log(Y)\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, (E(Y_i))^2\)</span></td>
<td>Appropriate for positive integers with a wide range.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\log(Y+1)\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, (E(Y_i))^2\)</span></td>
<td>Used when the data includes zero counts.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(1/Y\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, (E(Y_i))^4\)</span></td>
<td>For responses mostly near zero, with occasional large values.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\arcsin(\sqrt{Y})\)</span></td>
<td><span class="math inline">\(var(\epsilon_i) = k \, E(Y_i)(1-E(Y_i))\)</span></td>
<td>Suitable for binomial proportions or percentage data.</td>
</tr>
</tbody>
</table></div>
<ol style="list-style-type: decimal">
<li>
<strong>Choosing the Transformation</strong>:
<ul>
<li>Start by identifying the relationship between the variance of the residuals (<span class="math inline">\(var(\epsilon_i)\)</span>) and the mean of the response variable (<span class="math inline">\(E(Y_i)\)</span>).</li>
<li>Select the transformation that matches the identified variance structure.</li>
</ul>
</li>
<li>
<strong>Transformations for Zero Values</strong>:
<ul>
<li>For data with zeros, transformations like <span class="math inline">\(\sqrt{Y+1}\)</span> or <span class="math inline">\(\log(Y+1)\)</span> can be used to avoid undefined values. But this will seriously jeopardize model assumption <span class="citation">(<a href="references.html#ref-chen2024logs">J. Chen and Roth 2024</a>)</span>.</li>
</ul>
</li>
<li>
<strong>Use in Regression Models</strong>:
<ul>
<li>Apply these transformations to the dependent variable <span class="math inline">\(Y\)</span> in the regression model.</li>
<li>Always check residual plots to confirm that the transformation stabilizes variance and resolves non-linearity.</li>
</ul>
</li>
<li>
<strong>Interpretation After Transformation</strong>:
<ul>
<li>After transforming <span class="math inline">\(Y\)</span>, interpret the results in terms of the transformed variable.</li>
<li>For practical interpretation, back-transform predictions and account for any associated bias.</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level3" number="5.1.2">
<h3>
<span class="header-section-number">5.1.2</span> Multiple Linear Regression<a class="anchor" aria-label="anchor" href="#multiple-linear-regression"><i class="fas fa-link"></i></a>
</h3>
<p>The geometry of least squares regression involves projecting the response vector <span class="math inline">\(\mathbf{y}\)</span> onto the space spanned by the columns of the design matrix <span class="math inline">\(\mathbf{X}\)</span>. The fitted values <span class="math inline">\(\mathbf{\hat{y}}\)</span> can be expressed as:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{\hat{y}} &amp;= \mathbf{Xb} \\
&amp;= \mathbf{X(X'X)^{-1}X'y} \\
&amp;= \mathbf{Hy},
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{H} = \mathbf{X(X'X)^{-1}X'}\)</span> is the projection operator (sometimes denoted as <span class="math inline">\(\mathbf{P}\)</span>).</li>
<li>
<span class="math inline">\(\mathbf{\hat{y}}\)</span> is the projection of <span class="math inline">\(\mathbf{y}\)</span> onto the linear space spanned by the columns of <span class="math inline">\(\mathbf{X}\)</span> (the model space).</li>
</ul>
<p>The dimension of the model space is equal to the rank of <span class="math inline">\(\mathbf{X}\)</span> (i.e., the number of linearly independent columns in <span class="math inline">\(\mathbf{X}\)</span>).</p>
<hr>
<p>Properties of the Projection Matrix <span class="math inline">\(\mathbf{H}\)</span></p>
<ol style="list-style-type: decimal">
<li>Symmetry:
<ul>
<li>The projection matrix <span class="math inline">\(\mathbf{H}\)</span> is symmetric: <span class="math display">\[
\mathbf{H} = \mathbf{H}'.
\]</span>
</li>
</ul>
</li>
<li>Idempotence:
<ul>
<li>Applying <span class="math inline">\(\mathbf{H}\)</span> twice gives the same result: <span class="math display">\[
\mathbf{HH} = \mathbf{H}.
\]</span> Proof: <span class="math display">\[
\begin{aligned}
\mathbf{HH} &amp;= \mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\
&amp;= \mathbf{X(X'X)^{-1}IX'} \\
&amp;= \mathbf{X(X'X)^{-1}X'} \\
&amp;= \mathbf{H}.
\end{aligned}
\]</span>
</li>
</ul>
</li>
<li>Dimensionality:
<ul>
<li>
<span class="math inline">\(\mathbf{H}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix (where <span class="math inline">\(n\)</span> is the number of observations).</li>
<li>The rank of <span class="math inline">\(\mathbf{H}\)</span> is equal to the rank of <span class="math inline">\(\mathbf{X}\)</span>, which is typically the number of predictors (including the intercept).</li>
</ul>
</li>
<li>Orthogonal Complement:
<ul>
<li>The matrix <span class="math inline">\(\mathbf{(I - H)}\)</span>, where: <span class="math display">\[
\mathbf{I - H} = \mathbf{I - X(X'X)^{-1}X'},
\]</span> is also a projection operator.</li>
<li>It projects onto the orthogonal complement of the space spanned by the columns of <span class="math inline">\(\mathbf{X}\)</span> (i.e., the space orthogonal to the model space).</li>
</ul>
</li>
<li>Orthogonality of Projections:
<ul>
<li>
<span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{(I - H)}\)</span> are orthogonal: <span class="math display">\[
\mathbf{H(I - H)} = \mathbf{0}.
\]</span>
</li>
<li>Similarly: <span class="math display">\[
\mathbf{(I - H)H} = \mathbf{0}.
\]</span>
</li>
</ul>
</li>
</ol>
<hr>
<p>Intuition for <span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{(I - H)}\)</span></p>
<ul>
<li>
<span class="math inline">\(\mathbf{H}\)</span>: Projects <span class="math inline">\(\mathbf{y}\)</span> onto the model space, giving the fitted values <span class="math inline">\(\mathbf{\hat{y}}\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{I - H}\)</span>: Projects <span class="math inline">\(\mathbf{y}\)</span> onto the residual space, giving the residuals <span class="math inline">\(\mathbf{e}\)</span>: <span class="math display">\[
\mathbf{e} = \mathbf{(I - H)y}.
\]</span>
</li>
<li>
<span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{(I - H)}\)</span> divide the response vector <span class="math inline">\(\mathbf{y}\)</span> into two components: <span class="math display">\[
\mathbf{y} = \mathbf{\hat{y}} + \mathbf{e}.
\]</span>
<ul>
<li>
<span class="math inline">\(\mathbf{\hat{y}} = \mathbf{Hy}\)</span> (fitted values).</li>
<li>
<span class="math inline">\(\mathbf{e} = \mathbf{(I - H)y}\)</span> (residuals).</li>
</ul>
</li>
<li>The properties of <span class="math inline">\(\mathbf{H}\)</span> (symmetry, idempotence, dimensionality) reflect its role as a linear transformation that projects vectors onto the model space.</li>
</ul>
<p>This geometric perspective provides insight into the mechanics of least squares regression, particularly how the response variable <span class="math inline">\(\mathbf{y}\)</span> is decomposed into fitted values and residuals.</p>
<hr>
<p>Similar to simple regression, the total sum of squares in multiple regression analysis can be partitioned into components corresponding to the regression (model fit) and the residuals (errors).</p>
<p>The uncorrected total sum of squares is:</p>
<p><span class="math display">\[
\mathbf{y'y} = \mathbf{\hat{y}'\hat{y} + e'e},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{\hat{y} = Hy}\)</span> (fitted values, projected onto the model space).</p></li>
<li><p><span class="math inline">\(\mathbf{e = (I - H)y}\)</span> (residuals, projected onto the orthogonal complement of the model space).</p></li>
</ul>
<p>Expanding this using projection matrices:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y'y} &amp;= \mathbf{(Hy)'(Hy) + ((I-H)y)'((I-H)y)} \\
&amp;= \mathbf{y'H'Hy + y'(I-H)'(I-H)y} \\
&amp;= \mathbf{y'Hy + y'(I-H)y}.
\end{aligned}
\]</span></p>
<p>This equation shows the partition of <span class="math inline">\(\mathbf{y'y}\)</span> into components explained by the model (<span class="math inline">\(\mathbf{\hat{y}}\)</span>) and the unexplained variation (residuals).</p>
<p>For the corrected total sum of squares, we adjust for the mean (using the projection matrix <span class="math inline">\(\mathbf{H_1}\)</span>):</p>
<p><span class="math display">\[
\mathbf{y'(I-H_1)y = y'(H-H_1)y + y'(I-H)y}.
\]</span></p>
<p>Here:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{H_1} = \frac{1}{n} \mathbf{J}\)</span>, where <span class="math inline">\(\mathbf{J}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix of ones.</p></li>
<li><p><span class="math inline">\(\mathbf{H - H_1}\)</span> projects onto the subspace explained by the predictors after centering.</p></li>
</ul>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="12%">
<col width="39%">
<col width="46%">
</colgroup>
<thead><tr class="header">
<th>Aspect</th>
<th>Uncorrected Total Sum of Squares (<span class="math inline">\(\mathbf{y'y}\)</span>)</th>
<th>Corrected Total Sum of Squares (<span class="math inline">\(\mathbf{y'(I-H_1)y}\)</span>)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Definition</td>
<td>Total variation in <span class="math inline">\(y\)</span> relative to the origin.</td>
<td>Total variation in <span class="math inline">\(y\)</span> relative to its mean (centered data).</td>
</tr>
<tr class="even">
<td>Adjustment</td>
<td>No adjustment for the mean of <span class="math inline">\(y\)</span>.</td>
<td>Adjusts for the mean of <span class="math inline">\(y\)</span> by centering it.</td>
</tr>
<tr class="odd">
<td>Equation</td>
<td><span class="math inline">\(\mathbf{y'y} = \mathbf{\hat{y}'\hat{y}} + \mathbf{e'e}\)</span></td>
<td><span class="math inline">\(\mathbf{y'(I-H_1)y} = \mathbf{y'(H-H_1)y} + \mathbf{y'(I-H)y}\)</span></td>
</tr>
<tr class="even">
<td>Projection Matrices</td>
<td>
<p><span class="math inline">\(\mathbf{H}\)</span>: Projects onto model space.</p>
<p><span class="math inline">\(\mathbf{I-H}\)</span>: Projects onto residuals.</p>
</td>
<td>
<p><span class="math inline">\(\mathbf{H_1} = \frac{1}{n} \mathbf{J}\)</span>: Adjusts for the mean.</p>
<p><span class="math inline">\(\mathbf{H-H_1}\)</span>: Projects onto predictors after centering.</p>
<p><span class="math inline">\(\mathbf{I-H}\)</span>: Projects onto residuals.</p>
</td>
</tr>
<tr class="odd">
<td>Interpretation</td>
<td>Includes variation due to the mean of <span class="math inline">\(y\)</span>.</td>
<td>Focuses on variation in <span class="math inline">\(y\)</span> around its mean.</td>
</tr>
<tr class="even">
<td>Usage</td>
<td>Suitable for raw, uncentered data.</td>
<td>Common in regression and ANOVA to isolate variability explained by predictors.</td>
</tr>
<tr class="odd">
<td>Application</td>
<td>Measures total variability in <span class="math inline">\(y\)</span>, including overall level (mean).</td>
<td>Measures variability explained by predictors relative to the mean.</td>
</tr>
</tbody>
</table></div>
<p>Why the Correction Matters</p>
<ul>
<li><p>In ANOVA and regression, removing the contribution of the mean helps isolate the variability explained by predictors from the overall level of the response variable.</p></li>
<li><p>Corrected sums of squares are more common when comparing models or computing <span class="math inline">\(R^2\)</span>, which requires centering to ensure consistency in proportionate variance explained.</p></li>
</ul>
<p>The corrected total sum of squares can be decomposed into the sum of squares for regression (SSR) and the sum of squares for error (SSE):</p>
<div class="inline-table"><table style="width:97%;" class="table table-sm">
<colgroup>
<col width="12%">
<col width="40%">
<col width="10%">
<col width="20%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Source</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>F</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td><span class="math inline">\(SSR = \mathbf{y'(H - \frac{1}{n} J)y}\)</span></td>
<td><span class="math inline">\(p - 1\)</span></td>
<td><span class="math inline">\(MSR = SSR / (p-1)\)</span></td>
<td><span class="math inline">\(MSR / MSE\)</span></td>
</tr>
<tr class="even">
<td>Error</td>
<td><span class="math inline">\(SSE = \mathbf{y'(I - H)y}\)</span></td>
<td><span class="math inline">\(n - p\)</span></td>
<td><span class="math inline">\(MSE = SSE / (n-p)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(SST = \mathbf{y'(I - H_1)y}\)</span></td>
<td><span class="math inline">\(n - 1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(p\)</span>: Number of parameters (including intercept).</p></li>
<li><p><span class="math inline">\(n\)</span>: Number of observations.</p></li>
</ul>
<hr>
<p>Alternatively, the regression model can be expressed as:</p>
<p><span class="math display">\[
\mathbf{Y = X\hat{\beta} + (Y - X\hat{\beta})},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{\hat{Y} = X\hat{\beta}}\)</span>: Vector of fitted values (in the subspace spanned by <span class="math inline">\(\mathbf{X}\)</span>).</p></li>
<li><p><span class="math inline">\(\mathbf{e = Y - X\hat{\beta}}\)</span>: Vector of residuals (in the orthogonal complement of the subspace spanned by <span class="math inline">\(\mathbf{X}\)</span>).</p></li>
<li><p><span class="math inline">\(\mathbf{Y}\)</span> is an <span class="math inline">\(n \times 1\)</span> vector in the <span class="math inline">\(n\)</span>-dimensional space <span class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times p\)</span> full-rank matrix, with its columns generating a <span class="math inline">\(p\)</span>-dimensional subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. Hence, any estimator <span class="math inline">\(\mathbf{X\hat{\beta}}\)</span> is also in this subspace.</p></li>
</ul>
<p>In linear regression, the Ordinary Least Squares estimator <span class="math inline">\(\hat{\beta}\)</span> minimizes the squared Euclidean distance <span class="math inline">\(\|\mathbf{Y} - \mathbf{X}\beta\|^2\)</span> between the observed response vector <span class="math inline">\(\mathbf{Y}\)</span> and the fitted values <span class="math inline">\(\mathbf{X}\beta\)</span>. This minimization corresponds to the orthogonal projection of <span class="math inline">\(\mathbf{Y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>We solve the optimization problem:</p>
<p><span class="math display">\[
\min_{\beta} \|\mathbf{Y} - \mathbf{X}\beta\|^2.
\]</span></p>
<p>The objective function can be expanded as:</p>
<p><span class="math display">\[
\|\mathbf{Y} - \mathbf{X}\beta\|^2
= (\mathbf{Y} - \mathbf{X}\beta)^\top (\mathbf{Y} - \mathbf{X}\beta).
\]</span></p>
<p>Perform the multiplication:</p>
<p><span class="math display">\[
\begin{aligned}
(\mathbf{Y} - \mathbf{X}\beta)^\top (\mathbf{Y} - \mathbf{X}\beta)
&amp;= \mathbf{Y}^\top \mathbf{Y}
- \mathbf{Y}^\top \mathbf{X}\beta
- \beta^\top \mathbf{X}^\top \mathbf{Y}
+ \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta.
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{Y}^\top \mathbf{X}\beta\)</span> is a scalar, it equals <span class="math inline">\(\beta^\top \mathbf{X}^\top \mathbf{Y}\)</span>. Therefore, the expanded expression becomes:</p>
<p><span class="math display">\[
\|\mathbf{Y} - \mathbf{X}\beta\|^2
= \mathbf{Y}^\top \mathbf{Y}
- 2\beta^\top \mathbf{X}^\top \mathbf{Y}
+ \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta.
\]</span></p>
<p>To find the <span class="math inline">\(\beta\)</span> that minimizes this expression, take the derivative with respect to <span class="math inline">\(\beta\)</span> and set it to 0:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \beta}
\Bigl[
  \mathbf{Y}^\top \mathbf{Y}
  - 2\beta^\top \mathbf{X}^\top \mathbf{Y}
  + \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta
\Bigr]
= 0.
\]</span></p>
<p>Computing the gradient:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \beta} = -2\mathbf{X}^\top \mathbf{Y} + 2(\mathbf{X}^\top \mathbf{X})\beta.
\]</span></p>
<p>Setting this to zero:</p>
<p><span class="math display">\[
-2\mathbf{X}^\top \mathbf{Y} + 2\mathbf{X}^\top \mathbf{X}\beta = 0.
\]</span></p>
<p>Simplify:</p>
<p><span class="math display">\[
\mathbf{X}^\top \mathbf{X}\beta = \mathbf{X}^\top \mathbf{Y}.
\]</span></p>
<p>If <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is invertible, the solution is:</p>
<p><span class="math display">\[
\hat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}.
\]</span></p>
<hr>
<p>Orthogonal Projection Interpretation</p>
<p>The fitted values are:</p>
<p><span class="math display">\[
\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta}.
\]</span></p>
<p>From the normal equations, <span class="math inline">\(\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\hat{\beta}) = 0\)</span>, which implies that the residual vector <span class="math inline">\(\mathbf{Y} - \hat{\mathbf{Y}}\)</span> is orthogonal to every column of <span class="math inline">\(\mathbf{X}\)</span>. Therefore:</p>
<ul>
<li>
<span class="math inline">\(\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta}\)</span> is the orthogonal projection of <span class="math inline">\(\mathbf{Y}\)</span> onto <span class="math inline">\(\mathrm{Col}(\mathbf{X})\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{Y} - \mathbf{X}\hat{\beta}\)</span> lies in the orthogonal complement of <span class="math inline">\(\mathrm{Col}(\mathbf{X})\)</span>.</li>
</ul>
<hr>
<p>Pythagoras Decomposition</p>
<p>The geometric interpretation gives us the decomposition:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X}\hat{\beta} + (\mathbf{Y} - \mathbf{X}\hat{\beta}),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}\hat{\beta}\)</span> is the projection of <span class="math inline">\(\mathbf{Y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li><p><span class="math inline">\((\mathbf{Y} - \mathbf{X}\hat{\beta})\)</span> is the residual vector, orthogonal to <span class="math inline">\(\mathbf{X}\hat{\beta}\)</span>.</p></li>
</ul>
<p>Since the two components are orthogonal, their squared norms satisfy:</p>
<p><span class="math display">\[
\begin{aligned}\|\mathbf{Y}\|^2 &amp;= \mathbf{Y}^\top \mathbf{Y}&amp;&amp; \text{(definition of norm squared)} \\[6pt]&amp;= (\mathbf{Y} - \mathbf{X}\hat{\beta} + \mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta} + \mathbf{X}\hat{\beta})&amp;&amp; \text{(add and subtract the same term } \mathbf{X}\hat{\beta}\text{)} \\[6pt]&amp;= (\mathbf{Y} - \mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; 2\,(\mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; (\mathbf{X}\hat{\beta})^\top(\mathbf{X}\hat{\beta})&amp;&amp; \text{(expand }(a+b)^\top(a+b)\text{)} \\[6pt]&amp;= \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2\;+\; 2\,(\mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; \|\mathbf{X}\hat{\beta}\|^2&amp;&amp; \text{(rewrite each quadratic form as a norm)} \\[6pt]&amp;= \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2 + \|\mathbf{X}\hat{\beta}\|^2&amp;&amp; \text{(use that }(\mathbf{X}\hat{\beta})^\top(\mathbf{Y}-\mathbf{X}\hat{\beta}) = 0\text{, i.e. orthogonality)} \\[6pt]&amp; \quad = \|\mathbf{X}\hat{\beta}\|^2 \;+\; \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2.
\end{aligned}
\]</span></p>
<p>where the norm of a vector <span class="math inline">\(\mathbf{a}\)</span> in <span class="math inline">\(\mathbb{R}^p\)</span> is defined as:</p>
<p><span class="math display">\[
\|\mathbf{a}\| = \sqrt{\mathbf{a}^\top \mathbf{a}} = \sqrt{\sum_{i=1}^p a_i^2}.
\]</span></p>
<p>We are saying that <span class="math inline">\(\mathbf{Y}\)</span> is decomposed into two orthogonal components:</p>
<ol style="list-style-type: decimal">
<li>
<p><span class="math inline">\(\mathbf{X}\hat{\beta}\)</span> (the projection onto <span class="math inline">\(\mathrm{Col}(\mathbf{X})\)</span></p>
<ul>
<li>
<span class="math inline">\(\|\mathbf{X}\hat{\beta}\|\)</span> measures the part of <span class="math inline">\(\mathbf{Y}\)</span> explained by the model.</li>
</ul>
</li>
<li>
<p><span class="math inline">\(\mathbf{Y} - \mathbf{X}\hat{\beta}\)</span> (the residual lying in the orthogonal complement).</p>
<ul>
<li>
<span class="math inline">\(\|\mathbf{Y} - \mathbf{X}\hat{\beta}\|\)</span> measures the residual error.</li>
</ul>
</li>
</ol>
<p>This geometric interpretation (projection plus orthogonal remainder) is exactly why we call <span class="math inline">\(\mathbf{X}\hat{\beta}\)</span> the <em>orthogonal projection</em> of <span class="math inline">\(\mathbf{Y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span>. This decomposition also underlies the analysis of variance (ANOVA) in regression.</p>
<hr>
<p>The coefficient of multiple determination, denoted <span class="math inline">\(R^2\)</span>, measures the proportion of the total variation in the response variable (<span class="math inline">\(\mathbf{Y}\)</span>) that is explained by the regression model. It is defined as:</p>
<p><span class="math display">\[
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(SSR\)</span>: Regression sum of squares (variation explained by the model).</p></li>
<li><p><span class="math inline">\(SSE\)</span>: Error sum of squares (unexplained variation).</p></li>
<li><p><span class="math inline">\(SSTO\)</span>: Total sum of squares (total variation in <span class="math inline">\(\mathbf{Y}\)</span>).</p></li>
</ul>
<p>The adjusted <span class="math inline">\(R^2\)</span> adjusts <span class="math inline">\(R^2\)</span> for the number of predictors in the model, penalizing for adding predictors that do not improve the model’s fit substantially. It is defined as:</p>
<p><span class="math display">\[
R^2_a = 1 - \frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \frac{(n-1)SSE}{(n-p)SSTO},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: Number of observations.</p></li>
<li><p><span class="math inline">\(p\)</span>: Number of parameters (including the intercept).</p></li>
</ul>
<hr>
<p>Key Differences Between <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_a\)</span></p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="9%">
<col width="44%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th>Aspect</th>
<th><span class="math inline">\(R^2\)</span></th>
<th><span class="math inline">\(R^2_a\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Behavior with Predictors</td>
<td>Always increases (or remains constant) when more predictors are added, even if they are not statistically significant.</td>
<td>Includes a penalty for the number of predictors. May decrease if added predictors do not improve the model sufficiently.</td>
</tr>
<tr class="even">
<td>Interpretation</td>
<td>Proportion of the total variation in <span class="math inline">\(\mathbf{Y}\)</span> explained by the regression model.</td>
<td>Adjusted measure of explained variance, accounting for model complexity.</td>
</tr>
<tr class="odd">
<td>Range</td>
<td>Ranges between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</td>
<td>Can be lower than <span class="math inline">\(R^2\)</span>, particularly when the model includes irrelevant predictors.</td>
</tr>
<tr class="even">
<td>Usefulness</td>
<td>Useful for understanding the overall fit of the model.</td>
<td>Useful for comparing models with different numbers of predictors.</td>
</tr>
</tbody>
</table></div>
<p>In multiple regression, <span class="math inline">\(R^2_a\)</span> provides a more reliable measure of model fit, especially when comparing models with different numbers of predictors.</p>
<hr>
<p>In a regression model with coefficients <span class="math inline">\(\beta = (\beta_0, \beta_1, \dots, \beta_{p-1})^\top\)</span>, the sums of squares are used to evaluate the contribution of predictors to explaining the variation in the response variable.</p>
<ol style="list-style-type: decimal">
<li>Model Sums of Squares:
<ul>
<li>
<span class="math inline">\(SSM\)</span>: Total model sum of squares, capturing the variation explained by all predictors: <span class="math display">\[
SSM = SS(\beta_0, \beta_1, \dots, \beta_{p-1}).
\]</span>
</li>
</ul>
</li>
<li>Marginal Contribution:
<ul>
<li>
<span class="math inline">\(SSM_m\)</span>: Conditional model sum of squares, capturing the variation explained by predictors after accounting for others: <span class="math display">\[
SSM_m = SS(\beta_0, \beta_1, \dots, \beta_{p-1} | \beta_0).
\]</span>
</li>
</ul>
</li>
</ol>
<hr>
<p>Decompositions of <span class="math inline">\(SSM_m\)</span></p>
<ol style="list-style-type: decimal">
<li>Sequential Sums of Squares (Type I SS)</li>
</ol>
<ul>
<li>
<p>Definition:</p>
<ul>
<li>Sequential SS depends on the order in which predictors are added to the model.</li>
<li>It represents the additional contribution of each predictor given only the predictors that precede it in the sequence.</li>
</ul>
</li>
<li><p>Formula: <span class="math display">\[
SSM_m = SS(\beta_1 | \beta_0) + SS(\beta_2 | \beta_0, \beta_1) + \dots + SS(\beta_{p-1} | \beta_0, \dots, \beta_{p-2}).
\]</span></p></li>
<li>
<p>Key Points:</p>
<ul>
<li>Sequential SS is not unique; it depends on the order of the predictors.</li>
<li>Default in many statistical software functions (e.g., <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> in R).</li>
</ul>
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Marginal Sums of Squares (Type II SS)</li>
</ol>
<ul>
<li>Definition:
<ul>
<li>Marginal SS evaluates the contribution of a predictor after accounting for all other predictors except those with which it is collinear.</li>
<li>It ignores hierarchical relationships or interactions, focusing on independent contributions.</li>
</ul>
</li>
<li>Formula: <span class="math inline">\(SSM_m = SS(\beta_j | \beta_1, \dots, \beta_{j-1}, \beta_{j + 1}, \dots, \beta_{p-1})\)</span> where Type II SS evaluates the contribution of <span class="math inline">\(\beta_j\)</span> while excluding any terms collinear with <span class="math inline">\(\beta_j\)</span>.</li>
<li>Key Points:
<ul>
<li>Type II SS is independent of predictor order.</li>
<li>Suitable for models without interaction terms or when predictors are balanced.</li>
</ul>
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Partial Sums of Squares (Type III SS)</li>
</ol>
<ul>
<li>
<p>Definition:</p>
<ul>
<li>Partial SS evaluates the contribution of each predictor after accounting for all other predictors in the model.</li>
<li>It quantifies the unique contribution of a predictor, controlling for the presence of others.</li>
</ul>
</li>
<li><p>Formula: <span class="math display">\[
SSM_m = SS(\beta_1 | \beta_0, \beta_2, \dots, \beta_{p-1}) + \dots + SS(\beta_{p-1} | \beta_0, \beta_1, \dots, \beta_{p-2}).
\]</span></p></li>
<li>
<p>Key Points:</p>
<ul>
<li>Partial SS is unique for a given model.</li>
<li>More commonly used in practice for assessing individual predictor importance.</li>
</ul>
</li>
</ul>
<hr>
<p>Comparison of Sequential, Marginal, and Partial SS</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="8%">
<col width="31%">
<col width="33%">
<col width="26%">
</colgroup>
<thead><tr class="header">
<th>Aspect</th>
<th>Sequential SS (Type I)</th>
<th>Marginal SS (Type II)</th>
<th>Partial SS (Type III)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Dependency</td>
<td>Depends on the order in which predictors are entered.</td>
<td>Independent of order; adjusts for non-collinear predictors.</td>
<td>Independent of order; evaluates unique contributions.</td>
</tr>
<tr class="even">
<td>Usage</td>
<td>Default in software functions like <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> (Type I SS).</td>
<td>Models without interactions or hierarchical dependencies.</td>
<td>Commonly used for hypothesis testing.</td>
</tr>
<tr class="odd">
<td>Interpretation</td>
<td>Measures the additional contribution of predictors in sequence.</td>
<td>Measures the contribution of a predictor, ignoring collinear terms.</td>
<td>Measures the unique contribution of each predictor.</td>
</tr>
<tr class="even">
<td>Uniqueness</td>
<td>Not unique; changes with predictor order.</td>
<td>Unique for a given model without interactions.</td>
<td>Unique for a given model.</td>
</tr>
</tbody>
</table></div>
<hr>
<p>Practical Notes</p>
<ul>
<li>Use Type III SS (Partial SS) when:
<ul>
<li>The focus is on individual predictor contributions while accounting for all others.</li>
<li>Conducting hypothesis tests on predictors in complex models with interactions or hierarchical structures.</li>
</ul>
</li>
<li>Use Type II SS (Marginal SS) when:
<ul>
<li>Working with balanced datasets or models without interaction terms.</li>
<li>Ignoring interactions and focusing on independent effects.</li>
</ul>
</li>
<li>Use Type I SS (Sequential SS) when:
<ul>
<li>Interested in understanding the incremental contribution of predictors based on a specific order of entry (e.g., stepwise regression).</li>
</ul>
</li>
</ul>
<div id="ols-assumptions" class="section level4" number="5.1.2.1">
<h4>
<span class="header-section-number">5.1.2.1</span> OLS Assumptions<a class="anchor" aria-label="anchor" href="#ols-assumptions"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><a href="linear-regression.html#a1-linearity">A1 Linearity</a></li>
<li><a href="linear-regression.html#a2-full-rank">A2 Full Rank</a></li>
<li><a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a></li>
<li><a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a></li>
<li><a href="linear-regression.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></li>
<li><a href="linear-regression.html#a6-normal-distribution">A6 Normal Distribution</a></li>
</ul>
<hr>
<div id="a1-linearity" class="section level5" number="5.1.2.1.1">
<h5>
<span class="header-section-number">5.1.2.1.1</span> A1 Linearity<a class="anchor" aria-label="anchor" href="#a1-linearity"><i class="fas fa-link"></i></a>
</h5>
<p>The linear regression model is expressed as:</p>
<p><span class="math display">\[
A1: y = \mathbf{x}\beta + \epsilon
\]</span></p>
<p>This assumption is not restrictive since <span class="math inline">\(x\)</span> can include nonlinear transformations (e.g., interactions, natural logarithms, quadratic terms).</p>
<p>However, when combined with A3 (Exogeneity of Independent Variables), linearity can become restrictive.</p>
<hr>
<div id="log-model-variants" class="section level6" number="5.1.2.1.1.1">
<h6>
<span class="header-section-number">5.1.2.1.1.1</span> Log Model Variants<a class="anchor" aria-label="anchor" href="#log-model-variants"><i class="fas fa-link"></i></a>
</h6>
<p>Logarithmic transformations of variables allow for flexible modeling of nonlinear relationships. Common log model forms include:</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="7%">
<col width="27%">
<col width="22%">
<col width="40%">
</colgroup>
<thead><tr class="header">
<th>Model</th>
<th>Form</th>
<th>Interpretation of <span class="math inline">\(\beta\)</span>
</th>
<th>In Words</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Level-Level</td>
<td><span class="math inline">\(y = \beta_0 + \beta_1x + \epsilon\)</span></td>
<td><span class="math inline">\(\Delta y = \beta_1 \Delta x\)</span></td>
<td>A unit change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\(\beta_1\)</span> unit change in <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="even">
<td>Log-Level</td>
<td><span class="math inline">\(\ln(y) = \beta_0 + \beta_1x + \epsilon\)</span></td>
<td><span class="math inline">\(\% \Delta y = 100 \beta_1 \Delta x\)</span></td>
<td>A unit change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\(100 \beta_1 \%\)</span> change in <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="odd">
<td>Level-Log</td>
<td><span class="math inline">\(y = \beta_0 + \beta_1 \ln(x) + \epsilon\)</span></td>
<td><span class="math inline">\(\Delta y = (\beta_1/100)\% \Delta x\)</span></td>
<td>A 1% change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\((\beta_1 / 100)\)</span> unit change in <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="even">
<td>Log-Log</td>
<td><span class="math inline">\(\ln(y) = \beta_0 + \beta_1 \ln(x) + \epsilon\)</span></td>
<td><span class="math inline">\(\% \Delta y = \beta_1 \% \Delta x\)</span></td>
<td>A 1% change in <span class="math inline">\(x\)</span> results in a <span class="math inline">\(\beta_1 \%\)</span> change in <span class="math inline">\(y\)</span>.</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="higher-order-models" class="section level6" number="5.1.2.1.1.2">
<h6>
<span class="header-section-number">5.1.2.1.1.2</span> Higher-Order Models<a class="anchor" aria-label="anchor" href="#higher-order-models"><i class="fas fa-link"></i></a>
</h6>
<p>Higher-order terms allow the effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> to depend on the level of <span class="math inline">\(x_1\)</span>. For example:</p>
<p><span class="math display">\[
y = \beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon
\]</span></p>
<p>The partial derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(x_1\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial y}{\partial x_1} = \beta_1 + 2x_1\beta_2
\]</span></p>
<ul>
<li>The effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> depends on the value of <span class="math inline">\(x_1\)</span>.</li>
<li>Partial Effect at the Average: <span class="math inline">\(\beta_1 + 2E(x_1)\beta_2\)</span>.</li>
<li>Average Partial Effect: <span class="math inline">\(E(\beta_1 + 2x_1\beta_2)\)</span>.</li>
</ul>
<hr>
</div>
<div id="interaction-terms" class="section level6" number="5.1.2.1.1.3">
<h6>
<span class="header-section-number">5.1.2.1.1.3</span> Interaction Terms<a class="anchor" aria-label="anchor" href="#interaction-terms"><i class="fas fa-link"></i></a>
</h6>
<p>Interactions capture the joint effect of two variables. For example:</p>
<p><span class="math display">\[
y = \beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon
\]</span></p>
<ul>
<li>
<span class="math inline">\(\beta_1\)</span> is the average effect of a unit change in <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span> when <span class="math inline">\(x_2 = 0\)</span>.</li>
<li>The partial effect of <span class="math inline">\(x_1\)</span> on <span class="math inline">\(y\)</span>, which depends on the level of <span class="math inline">\(x_2\)</span>, is:</li>
</ul>
<p><span class="math display">\[
\beta_1 + x_2\beta_3.
\]</span></p>
<hr>
</div>
</div>
<div id="a2-full-rank" class="section level5" number="5.1.2.1.2">
<h5>
<span class="header-section-number">5.1.2.1.2</span> A2 Full Rank<a class="anchor" aria-label="anchor" href="#a2-full-rank"><i class="fas fa-link"></i></a>
</h5>
<p>The full rank assumption ensures the uniqueness and existence of the parameter estimates in the population regression equation. It is expressed as:</p>
<p><span class="math display">\[
A2: \text{rank}(E(\mathbf{x'x})) = k
\]</span></p>
<p>This assumption is also known as the identification condition.</p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>No Perfect Multicollinearity:
<ul>
<li>The columns of <span class="math inline">\(\mathbf{x}\)</span> (the matrix of predictors) must be linearly independent.</li>
<li>No column in <span class="math inline">\(\mathbf{x}\)</span> can be written as a linear combination of other columns.</li>
</ul>
</li>
<li>Implications:
<ul>
<li>Ensures that each parameter in the regression equation is identifiable and unique.</li>
<li>Prevents computational issues, such as the inability to invert <span class="math inline">\(\mathbf{x'x}\)</span>, which is required for estimating <span class="math inline">\(\hat{\beta}\)</span>.</li>
</ul>
</li>
</ol>
<p>Example of Violation</p>
<p>If two predictors, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, are perfectly correlated (e.g., <span class="math inline">\(x_2 = 2x_1\)</span>), the rank of <span class="math inline">\(\mathbf{x}\)</span> is reduced, and <span class="math inline">\(\mathbf{x'x}\)</span> becomes singular. In such cases:</p>
<ul>
<li><p>The regression coefficients cannot be uniquely estimated.</p></li>
<li><p>The model fails to satisfy the full rank assumption.</p></li>
</ul>
<hr>
</div>
<div id="a3-exogeneity-of-independent-variables" class="section level5" number="5.1.2.1.3">
<h5>
<span class="header-section-number">5.1.2.1.3</span> A3 Exogeneity of Independent Variables<a class="anchor" aria-label="anchor" href="#a3-exogeneity-of-independent-variables"><i class="fas fa-link"></i></a>
</h5>
<p>The exogeneity assumption ensures that the independent variables (<span class="math inline">\(\mathbf{x}\)</span>) are not systematically related to the error term (<span class="math inline">\(\epsilon\)</span>). It is expressed as:</p>
<p><span class="math display">\[
A3: E[\epsilon | x_1, x_2, \dots, x_k] = E[\epsilon | \mathbf{x}] = 0
\]</span></p>
<p>This assumption is often referred to as strict exogeneity or mean independence (see <a href="prerequisites.html#correlation-and-independence">Correlation and Independence</a>.</p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>Strict Exogeneity:
<ul>
<li>Independent variables carry no information about the error term <span class="math inline">\(\epsilon\)</span>.</li>
<li>By the [Law of Iterated Expectations], <span class="math inline">\(E(\epsilon) = 0\)</span>, which can be satisfied by always including an intercept in the regression model.</li>
</ul>
</li>
<li>Implication:
<ul>
<li>A3 implies: <span class="math display">\[
E(y | \mathbf{x}) = \mathbf{x}\beta,
\]</span> meaning the conditional mean function is a linear function of <span class="math inline">\(\mathbf{x}\)</span>. This aligns with <a href="linear-regression.html#a1-linearity">A1 Linearity</a>.</li>
</ul>
</li>
<li>Relationship with Independence:
<ul>
<li>Also referred to as mean independence, which is a weaker condition than full independence (see <a href="prerequisites.html#correlation-and-independence">Correlation and Independence</a>).</li>
</ul>
</li>
</ol>
<hr>
<div id="a3a-weak-exogeneity" class="section level6" number="5.1.2.1.3.1">
<h6>
<span class="header-section-number">5.1.2.1.3.1</span> A3a: Weaker Exogeneity Assumption<a class="anchor" aria-label="anchor" href="#a3a-weak-exogeneity"><i class="fas fa-link"></i></a>
</h6>
<p>A weaker version of the exogeneity assumption is:</p>
<p><span class="math display">\[
A3a: E(\mathbf{x_i'}\epsilon_i) = 0
\]</span></p>
<p>This implies:</p>
<ul>
<li><p>The independent variables (<span class="math inline">\(\mathbf{x}_i\)</span>) are uncorrelated with the error term (<span class="math inline">\(\epsilon_i\)</span>).</p></li>
<li><p>Weaker than mean independence in A3.</p></li>
</ul>
<hr>
<p>Comparison Between A3 and A3a</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="14%">
<col width="45%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th>Aspect</th>
<th>A3 (Strict Exogeneity)</th>
<th>A3a (Weaker Exogeneity)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Definition</td>
<td>
<span class="math inline">\(E(\epsilon | \mathbf{x}) = 0\)</span>.</td>
<td>
<span class="math inline">\(E(\mathbf{x}_i'\epsilon_i) = 0\)</span>.</td>
</tr>
<tr class="even">
<td>Strength</td>
<td>Stronger assumption; implies A3a.</td>
<td>Weaker assumption; does not imply A3.</td>
</tr>
<tr class="odd">
<td>Interpretation</td>
<td>Predictors provide no information about <span class="math inline">\(\epsilon\)</span>.</td>
<td>Predictors are uncorrelated with <span class="math inline">\(\epsilon\)</span>.</td>
</tr>
<tr class="even">
<td>Causality</td>
<td>Enables causal interpretation.</td>
<td>Does not allow causal interpretations.</td>
</tr>
</tbody>
</table></div>
<hr>
<p>Notes on Practical Relevance</p>
<ul>
<li>Checking for Exogeneity:
<ul>
<li>Strict exogeneity cannot be tested directly, but violations can manifest as omitted variable bias, endogeneity, or measurement error.</li>
<li>Including all relevant predictors and ensuring accurate measurement can help satisfy this assumption.</li>
</ul>
</li>
<li>Violations of Exogeneity:
<ul>
<li>If A3 is violated, standard OLS estimates are biased and inconsistent.</li>
<li>In such cases, instrumental variable (IV) methods or other approaches may be required to correct for endogeneity.</li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="a4-homoskedasticity" class="section level5" number="5.1.2.1.4">
<h5>
<span class="header-section-number">5.1.2.1.4</span> A4 Homoskedasticity<a class="anchor" aria-label="anchor" href="#a4-homoskedasticity"><i class="fas fa-link"></i></a>
</h5>
<p>The homoskedasticity assumption ensures that the variance of the error term (<span class="math inline">\(\epsilon\)</span>) is constant across all levels of the independent variables (<span class="math inline">\(\mathbf{x}\)</span>). It is expressed as:</p>
<p><span class="math display">\[
A4: \text{Var}(\epsilon | \mathbf{x}) = \text{Var}(\epsilon) = \sigma^2
\]</span></p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>Definition:
<ul>
<li>The variance of the disturbance term <span class="math inline">\(\epsilon\)</span> is the same for all observations, regardless of the values of the predictors <span class="math inline">\(\mathbf{x}\)</span>.</li>
</ul>
</li>
<li>Practical Implication:
<ul>
<li>Homoskedasticity ensures that the errors do not systematically vary with the predictors.</li>
<li>This is critical for valid inference, as the standard errors of the coefficients rely on this assumption.</li>
</ul>
</li>
<li>Violation (Heteroskedasticity):
<ul>
<li>If the variance of <span class="math inline">\(\epsilon\)</span> depends on <span class="math inline">\(\mathbf{x}\)</span>, the assumption is violated.</li>
<li>Common signs include funnel-shaped patterns in residual plots or varying error sizes.</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="a5-data-generation-random-sampling" class="section level5" number="5.1.2.1.5">
<h5>
<span class="header-section-number">5.1.2.1.5</span> A5 Data Generation (Random Sampling)<a class="anchor" aria-label="anchor" href="#a5-data-generation-random-sampling"><i class="fas fa-link"></i></a>
</h5>
<p>The random sampling assumption ensures that the observations <span class="math inline">\((y_i, x_{i1}, \dots, x_{ik-1})\)</span> are drawn independently and identically distributed (iid) from the joint distribution of <span class="math inline">\((y, \mathbf{x})\)</span>. It is expressed as:</p>
<p><span class="math display">\[
A5: \{y_i, x_{i1}, \dots, x_{ik-1} : i = 1, \dots, n\}
\]</span></p>
<p>Key Points</p>
<ol style="list-style-type: decimal">
<li>Random Sampling:
<ul>
<li>The dataset is assumed to be a random sample from the population.</li>
<li>Each observation is independent of others and follows the same probability distribution.</li>
</ul>
</li>
<li>Implications:
<ul>
<li>With <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3 (Exogeneity of Independent Variables)</a> and <a href="linear-regression.html#a4-homoskedasticity">A4 (Homoskedasticity)</a>, random sampling implies:
<ul>
<li>Strict Exogeneity: <span class="math display">\[
E(\epsilon_i | x_1, \dots, x_n) = 0
\]</span> Independent variables do not contain information for predicting <span class="math inline">\(\epsilon\)</span>.</li>
<li>Non-Autocorrelation: <span class="math display">\[
E(\epsilon_i \epsilon_j | x_1, \dots, x_n) = 0 \quad \text{for } i \neq j
\]</span> The error terms are uncorrelated across observations, conditional on the independent variables.</li>
<li>Variance of Errors: <span class="math display">\[
\text{Var}(\epsilon | \mathbf{X}) = \text{Var}(\epsilon) = \sigma^2 \mathbf{I}_n
\]</span>
</li>
</ul>
</li>
</ul>
</li>
<li>When A5 May Not Hold:
<ul>
<li>In time series data, where observations are often autocorrelated.</li>
<li>In spatial data, where neighboring observations may not be independent.</li>
</ul>
</li>
</ol>
<hr>
<p>Practical Considerations</p>
<ul>
<li>Time Series Data:
<ul>
<li>Use methods such as autoregressive models or generalized least squares (GLS) to address dependency in observations.</li>
</ul>
</li>
<li>Spatial Data:
<ul>
<li>Spatial econometric models may be required to handle correlation across geographic locations.</li>
</ul>
</li>
<li>Checking Random Sampling:
<ul>
<li>While true randomness cannot always be verified, exploratory analysis of the residuals (e.g., for patterns or autocorrelation) can help detect violations.</li>
</ul>
</li>
</ul>
<div id="A5a-stationarity-in-stochastic-processes" class="section level6" number="5.1.2.1.5.1">
<h6>
<span class="header-section-number">5.1.2.1.5.1</span> A5a: Stationarity in Stochastic Processes<a class="anchor" aria-label="anchor" href="#A5a-stationarity-in-stochastic-processes"><i class="fas fa-link"></i></a>
</h6>
<p>A stochastic process <span class="math inline">\(\{x_t\}_{t=1}^T\)</span> is stationary if, for every collection of time indices <span class="math inline">\(\{t_1, t_2, \dots, t_m\}\)</span>, the joint distribution of:</p>
<p><span class="math display">\[
x_{t_1}, x_{t_2}, \dots, x_{t_m}
\]</span></p>
<p>is the same as the joint distribution of:</p>
<p><span class="math display">\[
x_{t_1+h}, x_{t_2+h}, \dots, x_{t_m+h}
\]</span></p>
<p>for any <span class="math inline">\(h \geq 1\)</span>.</p>
<p>Key Points on Stationarity</p>
<ol style="list-style-type: decimal">
<li>Definition:
<ul>
<li>A stationary process has statistical properties (mean, variance, and covariance) that are invariant over time.</li>
<li>For example, the joint distribution for the first ten observations is identical to the joint distribution for the next ten observations, regardless of their position in time.</li>
</ul>
</li>
<li>Implication:
<ul>
<li>Stationarity ensures that the relationships observed in the data remain consistent over time.</li>
<li>Independent draws automatically satisfy the stationarity condition.</li>
</ul>
</li>
</ol>
<hr>
<p>Weak Stationarity</p>
<p>A stochastic process <span class="math inline">\(\{x_t\}_{t=1}^T\)</span> is weakly stationary if:</p>
<ul>
<li>The covariance between <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_{t+h}\)</span> depends only on the lag <span class="math inline">\(h\)</span> and not on <span class="math inline">\(t\)</span>.</li>
<li>As <span class="math inline">\(h \to \infty\)</span>, the covariance diminishes, meaning <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_{t+h}\)</span> become “almost independent.”</li>
</ul>
<hr>
<p>Differences Between Stationarity and Weak Stationarity</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="16%">
<col width="39%">
<col width="43%">
</colgroup>
<thead><tr class="header">
<th>Aspect</th>
<th>Stationarity</th>
<th>Weak Stationarity</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Joint Distribution</td>
<td>Entire joint distribution remains unchanged over time.</td>
<td>Focuses only on the first two moments: mean and covariance.</td>
</tr>
<tr class="even">
<td>Dependence Over Time</td>
<td>Observations at all lags are equally distributed.</td>
<td>Observations far apart are “almost independent.”</td>
</tr>
<tr class="odd">
<td>Application</td>
<td>Ensures strong consistency in time-series processes.</td>
<td>More practical for many time-series applications.</td>
</tr>
</tbody>
</table></div>
<p>Weak stationarity is often sufficient for many time-series analyses, especially when focusing on correlations and trends rather than full distributions.</p>
<hr>
<p>Common Weakly Dependent Processes</p>
<p>1. Moving Average Process of Order 1 (MA(1))</p>
<p>An MA(1) process models the dependent variable <span class="math inline">\(y_t\)</span> as a function of the current and one-period lagged stochastic error term:</p>
<p><span class="math display">\[
y_t = u_t + \alpha_1 u_{t-1},
\]</span></p>
<p>where <span class="math inline">\(u_t\)</span> is white noise, independently and identically distributed (iid) with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Key Properties</p>
<ol style="list-style-type: decimal">
<li><p>Mean: <span class="math display">\[
E(y_t) = E(u_t) + \alpha_1E(u_{t-1}) = 0
\]</span></p></li>
<li><p>Variance: <span class="math display">\[
\begin{aligned}
\text{Var}(y_t) &amp;= \text{Var}(u_t) + \alpha_1^2 \text{Var}(u_{t-1}) \\
&amp;= \sigma^2 + \alpha_1^2 \sigma^2 \\
&amp;= \sigma^2 (1 + \alpha_1^2)
\end{aligned}
\]</span> An increase in the absolute value of <span class="math inline">\(\alpha_1\)</span> increases the variance.</p></li>
<li>
<p>Autocovariance:</p>
<ul>
<li>For lag 1: <span class="math display">\[
\text{Cov}(y_t, y_{t-1}) = \alpha_1 \text{Var}(u_{t-1}) = \alpha_1 \sigma^2.
\]</span>
</li>
<li>For lag 2 or greater: <span class="math display">\[
\text{Cov}(y_t, y_{t-2}) = 0.
\]</span>
</li>
</ul>
</li>
</ol>
<p>The MA(1) process is invertible if <span class="math inline">\(|\alpha_1| &lt; 1\)</span>, allowing it to be rewritten as an autoregressive (AR) representation:</p>
<p><span class="math display">\[
u_t = y_t - \alpha_1 u_{t-1}.
\]</span></p>
<p>Invertibility implies that we can express the current observation in terms of past observations.</p>
<hr>
<p>An MA(q) process generalizes the MA(1) process to include <span class="math inline">\(q\)</span> lags:</p>
<p><span class="math display">\[
y_t = u_t + \alpha_1 u_{t-1} + \dots + \alpha_q u_{t-q},
\]</span></p>
<p>where <span class="math inline">\(u_t \sim WN(0, \sigma^2)\)</span> (white noise with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>).</p>
<p>Key Characteristics:</p>
<ol style="list-style-type: decimal">
<li>Covariance Stationary:
<ul>
<li>An MA(q) process is covariance stationary irrespective of the parameter values.</li>
</ul>
</li>
<li>Invertibility:
<ul>
<li>An MA(q) process is invertible if the parameters satisfy certain conditions (e.g., <span class="math inline">\(|\alpha_i| &lt; 1\)</span>).</li>
</ul>
</li>
<li>Autocorrelations:
<ul>
<li>The autocorrelations are nonzero for lags up to <span class="math inline">\(q\)</span> but are 0 for lags beyond <span class="math inline">\(q\)</span>.</li>
</ul>
</li>
<li>Conditional Mean:
<ul>
<li>The conditional mean of <span class="math inline">\(y_t\)</span> depends on the <span class="math inline">\(q\)</span> lags, indicating “long-term memory.”</li>
</ul>
</li>
</ol>
<p>Example: Autocovariance of an MA(1)</p>
<ol style="list-style-type: decimal">
<li><p>Lag 1: <span class="math display">\[
\begin{aligned}
\text{Cov}(y_t, y_{t-1}) &amp;= \text{Cov}(u_t + \alpha_1 u_{t-1}, u_{t-1} + \alpha_1 u_{t-2}) \\
&amp;= \alpha_1 \text{Var}(u_{t-1}) \\
&amp;= \alpha_1 \sigma^2.
\end{aligned}
\]</span></p></li>
<li><p>Lag 2: <span class="math display">\[
\begin{aligned}
\text{Cov}(y_t, y_{t-2}) &amp;= \text{Cov}(u_t + \alpha_1 u_{t-1}, u_{t-2} + \alpha_1 u_{t-3}) \\
&amp;= 0.
\end{aligned}
\]</span></p></li>
</ol>
<p>An MA process captures a linear relationship between the dependent variable <span class="math inline">\(y_t\)</span> and the current and past values of a stochastic error term <span class="math inline">\(u_t\)</span>. Its properties make it useful for modeling time-series data with limited memory and short-term dependencies.</p>
<ol start="2" style="list-style-type: decimal">
<li>Auto-Regressive Process of Order 1 (AR(1))</li>
</ol>
<p>An auto-regressive process of order 1 (AR(1)) is defined as:</p>
<p><span class="math display">\[
y_t = \rho y_{t-1} + u_t, \quad |\rho| &lt; 1
\]</span></p>
<p>where <span class="math inline">\(u_t\)</span> represents independent and identically distributed (i.i.d.) random noise over <span class="math inline">\(t\)</span> with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Covariance at lag 1:</p>
<p><span class="math display">\[
\begin{aligned}
Cov(y_t, y_{t-1}) &amp;= Cov(\rho y_{t-1} + u_t, y_{t-1}) \\
&amp;= \rho Var(y_{t-1}) \\
&amp;= \rho \frac{\sigma^2}{1-\rho^2}.
\end{aligned}
\]</span></p>
<p>Covariance at lag <span class="math inline">\(h\)</span>:</p>
<p><span class="math display">\[
Cov(y_t, y_{t-h}) = \rho^h \frac{\sigma^2}{1-\rho^2}.
\]</span></p>
<p>Stationarity implies that the distribution of <span class="math inline">\(y_t\)</span> does not change over time, requiring constant mean and variance. For this process:</p>
<ol style="list-style-type: decimal">
<li>
<p>Mean Stationarity:<br>
Assuming <span class="math inline">\(E(y_t) = 0\)</span>, we have: <span class="math display">\[
y_t = \rho^t y_0 + \rho^{t-1}u_1 + \rho^{t-2}u_2 + \dots + \rho u_{t-1} + u_t.
\]</span></p>
<p>If the initial observation <span class="math inline">\(y_0 = 0\)</span>, then <span class="math inline">\(y_t\)</span> is simply a weighted sum of the random shocks <span class="math inline">\(u_t\)</span> from all prior time periods. Thus, <span class="math inline">\(E(y_t) = 0\)</span> for all <span class="math inline">\(t\)</span>.</p>
</li>
<li>
<p>Variance Stationarity:<br>
The variance is computed as: <span class="math display">\[
Var(y_t) = Var(\rho y_{t-1} + u_t).
\]</span></p>
<p>Expanding and simplifying: <span class="math display">\[
Var(y_t) = \rho^2 Var(y_{t-1}) + Var(u_t) + 2\rho Cov(y_{t-1}, u_t).
\]</span> Since <span class="math inline">\(u_t\)</span> is independent of <span class="math inline">\(y_{t-1}\)</span>, <span class="math inline">\(Cov(y_{t-1}, u_t) = 0\)</span>, giving: <span class="math display">\[
Var(y_t) = \rho^2 Var(y_{t-1}) + \sigma^2.
\]</span></p>
<p>Solving recursively, we find: <span class="math display">\[
Var(y_t) = \frac{\sigma^2}{1-\rho^2}.
\]</span></p>
</li>
</ol>
<p>For the variance to remain constant over time, it is required that <span class="math inline">\(|\rho| &lt; 1\)</span> and <span class="math inline">\(p \notin \{1,-1\}\)</span>.</p>
<p>Key Insights on Stationarity</p>
<ol style="list-style-type: decimal">
<li><p>Stationarity Requirement:<br>
The condition <span class="math inline">\(|\rho| &lt; 1\)</span> ensures stationarity, as this guarantees that both the mean and variance of the process are constant over time.</p></li>
<li><p>Weak Dependence:<br>
As <span class="math inline">\(|\rho| &lt; 1\)</span>, the dependency between observations diminishes with increasing lag <span class="math inline">\(h\)</span>, as seen from the covariance: <span class="math display">\[
Cov(y_t, y_{t-h}) = \rho^h \frac{\sigma^2}{1-\rho^2}.
\]</span></p></li>
</ol>
<p>To estimate an AR(1) process, we utilize the Yule-Walker equations, which relate the autocovariances of the process to its parameters.</p>
<p>Starting with the AR(1) process: <span class="math display">\[
y_t = \epsilon_t + \phi y_{t-1},
\]</span> multiplying both sides by <span class="math inline">\(y_{t-\tau}\)</span> and taking expectations, we get: <span class="math display">\[
y_t y_{t-\tau} = \epsilon_t y_{t-\tau} + \phi y_{t-1} y_{t-\tau}.
\]</span></p>
<p>For <span class="math inline">\(\tau \geq 1\)</span>, the autocovariance <span class="math inline">\(\gamma(\tau)\)</span> satisfies: <span class="math display">\[
\gamma(\tau) = \phi \gamma(\tau - 1).
\]</span></p>
<p>Dividing through by the variance <span class="math inline">\(\gamma(0)\)</span>, we obtain the autocorrelation: <span class="math display">\[
\rho_\tau = \phi^\tau.
\]</span></p>
<p>Thus, the autocorrelations decay geometrically as <span class="math inline">\(\phi^\tau\)</span>, where <span class="math inline">\(|\phi| &lt; 1\)</span> ensures stationarity and decay over time.</p>
<hr>
<p>Generalizing to AR(p)</p>
<p>An AR(p) process extends the AR(1) structure to include <span class="math inline">\(p\)</span> lags: <span class="math display">\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t.
\]</span></p>
<p>Here, <span class="math inline">\(\epsilon_t\)</span> is white noise with <span class="math inline">\(E(\epsilon_t) = 0\)</span> and <span class="math inline">\(Var(\epsilon_t) = \sigma^2\)</span>.</p>
<p>The AR(p) process is covariance stationary if the roots of the characteristic equation lie outside the unit circle: <span class="math display">\[
1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0.
\]</span></p>
<p>For the AR(p) process, the autocorrelations <span class="math inline">\(\rho_\tau\)</span> decay more complexly compared to the AR(1). However, they still diminish over time, ensuring weak dependence among distant observations.</p>
<hr>
<p>The Yule-Walker equations for an AR(p) process provide a system of linear equations to estimate the parameters <span class="math inline">\(\phi_1, \phi_2, \dots, \phi_p\)</span>: <span class="math display">\[
\gamma(\tau) = \phi_1 \gamma(\tau - 1) + \phi_2 \gamma(\tau - 2) + \dots + \phi_p \gamma(\tau - p), \quad \tau \geq 1.
\]</span></p>
<p>This system can be written in matrix form for <span class="math inline">\(\tau = 1, \dots, p\)</span> as:</p>
<p><span class="math display">\[
\begin{bmatrix}
\gamma(1) \\
\gamma(2) \\
\vdots \\
\gamma(p)
\end{bmatrix}
=
\begin{bmatrix}
\gamma(0) &amp; \gamma(1) &amp; \dots &amp; \gamma(p-1) \\
\gamma(1) &amp; \gamma(0) &amp; \dots &amp; \gamma(p-2) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\gamma(p-1) &amp; \gamma(p-2) &amp; \dots &amp; \gamma(0)
\end{bmatrix}
\begin{bmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_p
\end{bmatrix}.
\]</span></p>
<p>This system is solved to estimate the coefficients <span class="math inline">\(\phi_1, \phi_2, \dots, \phi_p\)</span>.</p>
<hr>
<ol start="3" style="list-style-type: decimal">
<li>ARMA(p, q) Process</li>
</ol>
<p>An ARMA(p, q) process combines autoregressive (AR) and moving average (MA) components to model time series data effectively. The general form is:</p>
<p><span class="math display">\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t + \alpha_1 \epsilon_{t-1} + \alpha_2 \epsilon_{t-2} + \dots + \alpha_q \epsilon_{t-q}.
\]</span></p>
<p>A simple case of ARMA(1, 1) is given by: <span class="math display">\[
y_t = \phi y_{t-1} + \epsilon_t + \alpha \epsilon_{t-1},
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(\phi\)</span> captures the autoregressive behavior,</p></li>
<li><p><span class="math inline">\(\alpha\)</span> controls the moving average component,</p></li>
<li><p><span class="math inline">\(\epsilon_t\)</span> represents white noise.</p></li>
</ul>
<p>ARMA processes can capture seasonality and more complex dependencies than pure AR or MA models.</p>
<hr>
<ol start="4" style="list-style-type: decimal">
<li><strong>Random Walk Process</strong></li>
</ol>
<p>A <strong>random walk</strong> is a non-stationary process defined as: <span class="math display">\[
y_t = y_0 + \sum_{s=1}^t u_s,
\]</span> where <span class="math inline">\(u_s\)</span> are i.i.d. random variables.</p>
<p>Properties:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Non-Stationarity</strong>:</p>
<ul>
<li>If <span class="math inline">\(y_0 = 0\)</span>, then <span class="math inline">\(E(y_t) = 0\)</span>, but the variance increases over time: <span class="math display">\[
Var(y_t) = t \sigma^2.
\]</span>
</li>
</ul>
</li>
<li>
<p><strong>Not Weakly Dependent</strong>:</p>
<ul>
<li>The covariance of the process does not diminish with increasing lag <span class="math inline">\(h\)</span>: <span class="math display">\[
Cov\left(\sum_{s=1}^t u_s, \sum_{s=1}^{t-h} u_s\right) = (t-h)\sigma^2.
\]</span>
</li>
</ul>
<p>As <span class="math inline">\(h\)</span> increases, the covariance remains large, violating the condition for weak dependence.</p>
</li>
</ol>
<hr>
</div>
<div id="A5a-stationarity-and-weak-dependence-in-time-series" class="section level6" number="5.1.2.1.5.2">
<h6>
<span class="header-section-number">5.1.2.1.5.2</span> A5a: Stationarity and Weak Dependence in Time Series<a class="anchor" aria-label="anchor" href="#A5a-stationarity-and-weak-dependence-in-time-series"><i class="fas fa-link"></i></a>
</h6>
<p>For time series data, the set <span class="math inline">\(\{y_t, x_{t1}, \dots, x_{tk-1}\}\)</span>, where <span class="math inline">\(t = 1, \dots, T\)</span>, must satisfy the conditions of <strong>stationarity</strong> and <strong>weak dependence</strong>. These properties are essential to ensure the consistency and efficiency of estimators in time-series regression models.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Stationarity</strong>: A stationary process has statistical properties (e.g., mean, variance, autocovariance) that remain constant over time. This ensures that the relationships in the data do not change as <span class="math inline">\(t\)</span> progresses, making it possible to draw meaningful inferences.</li>
<li>
<strong>Weak Dependence</strong>: Weak dependence implies that observations far apart in time are “almost independent.” While there may be short-term correlations, these diminish as the time lag increases. This property ensures that the sample averages are representative of the population mean.</li>
</ol>
<p>The <a href="prerequisites.html#weak-law-of-large-numbers">Weak Law of Large Numbers</a> provides a foundation for the consistency of sample means. If <span class="math inline">\(\{z_t\}\)</span> is a weakly dependent, stationary process with <span class="math inline">\(E(|z_t|) &lt; \infty\)</span> and <span class="math inline">\(E(z_t) = \mu\)</span>, then:</p>
<p><span class="math display">\[
\frac{1}{T} \sum_{t=1}^T z_t \xrightarrow{p} \mu.
\]</span></p>
<p>Interpretation:</p>
<ul>
<li><p>As the sample size <span class="math inline">\(T \to \infty\)</span>, the sample mean <span class="math inline">\(\bar{z} = \frac{1}{T} \sum_{t=1}^T z_t\)</span> converges in probability to the true mean <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>This ensures the <strong>consistency</strong> of estimators based on time-series data.</p></li>
</ul>
<p>The <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a> extends the WLLN by describing the distribution of the sample mean. Under additional regularity conditions (e.g., finite variance) <span class="citation">(<a href="references.html#ref-greene1990gamma">Greene 1990</a>)</span>, the sample mean <span class="math inline">\(\bar{z}\)</span> satisfies:</p>
<p><span class="math display">\[
\sqrt{T}(\bar{z} - \mu) \xrightarrow{d} N(0, B),
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
B = \text{Var}(z_t) + 2\sum_{h=1}^\infty \text{Cov}(z_t, z_{t-h}).
\]</span></p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><p>The sample mean <span class="math inline">\(\bar{z}\)</span> is approximately normally distributed for large <span class="math inline">\(T\)</span>.</p></li>
<li><p>The variance of the limiting distribution, <span class="math inline">\(B\)</span>, depends not only on the variance of <span class="math inline">\(z_t\)</span> but also on the covariances between <span class="math inline">\(z_t\)</span> and its past values.</p></li>
</ul>
</div>
</div>
<div id="a6-normal-distribution" class="section level5" number="5.1.2.1.6">
<h5>
<span class="header-section-number">5.1.2.1.6</span> A6 Normal Distribution<a class="anchor" aria-label="anchor" href="#a6-normal-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>A6: <span class="math inline">\(\epsilon|\mathbf{x} \sim N(0, \sigma^2 I_n)\)</span></p>
<p>The assumption here implies that the error term <span class="math inline">\(\epsilon\)</span> is normally distributed with mean zero and variance <span class="math inline">\(\sigma^2 I_n\)</span>. This assumption is fundamental for statistical inference in linear regression models.</p>
<p>Using assumptions <a href="linear-regression.html#a1-linearity">A1 Linearity</a>, <a href="linear-regression.html#a2-full-rank">A2 Full Rank</a>, and <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a>, we derive the identification (or orthogonality condition) for the population parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
y &amp;= x\beta + \epsilon &amp;&amp; \text{(A1: Model Specification)} \\
x'y &amp;= x'x\beta + x'\epsilon &amp;&amp; \text{(Multiply both sides by $x'$)} \\
E(x'y) &amp;= E(x'x)\beta + E(x'\epsilon) &amp;&amp; \text{(Taking expectation)} \\
E(x'y) &amp;= E(x'x)\beta &amp;&amp; \text{(A3: Exogeneity, $E(x'\epsilon) = 0$)} \\
[E(x'x)]^{-1}E(x'y) &amp;= [E(x'x)]^{-1}E(x'x)\beta &amp;&amp; \text{(Invertibility of $E(x'x)$, A2)} \\
[E(x'x)]^{-1}E(x'y) &amp;= \beta &amp;&amp; \text{(Simplified solution for $\beta$)}
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\beta\)</span> is identified as the vector of parameters that minimizes the expected squared error.</p>
<p>To find <span class="math inline">\(\beta\)</span>, we minimize the expected value of the squared error:</p>
<p><span class="math display">\[
\underset{\gamma}{\operatorname{argmin}} \ E\big((y - x\gamma)^2\big)
\]</span></p>
<p>The first-order condition is derived by taking the derivative of the objective function with respect to <span class="math inline">\(\gamma\)</span> and setting it to zero:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial E\big((y - x\gamma)^2\big)}{\partial \gamma} &amp;= 0 \\
-2E(x'(y - x\gamma)) &amp;= 0 \\
E(x'y) - E(x'x\gamma) &amp;= 0 \\
E(x'y) &amp;= E(x'x)\gamma \\
(E(x'x))^{-1}E(x'y) &amp;= \gamma
\end{aligned}
\]</span></p>
<p>This confirms that <span class="math inline">\(\gamma = \beta\)</span>.</p>
<p>The second-order condition ensures that the solution minimizes the objective function. Taking the second derivative:</p>
<p><span class="math display">\[
\frac{\partial^2 E\big((y - x\gamma)^2\big)}{\partial \gamma'^2} = 0 = 2E(x'x)
\]</span></p>
<p>If assumption <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a> holds, <span class="math inline">\(E(x'x)\)</span> is positive semi-definite (PSD). Thus, <span class="math inline">\(2E(x'x)\)</span> is also PSD, ensuring a minimum.</p>
</div>
<div id="hierarchy-of-ols-assumptions" class="section level5" number="5.1.2.1.7">
<h5>
<span class="header-section-number">5.1.2.1.7</span> Hierarchy of OLS Assumptions<a class="anchor" aria-label="anchor" href="#hierarchy-of-ols-assumptions"><i class="fas fa-link"></i></a>
</h5>
<p>This table summarizes the hierarchical nature of assumptions required to derive different properties of the OLS estimator.</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<caption>Usage of Assumptions</caption>
<colgroup>
<col width="27%">
<col width="12%">
<col width="10%">
<col width="30%">
<col width="18%">
</colgroup>
<thead><tr class="header">
<th>Assumption</th>
<th><strong>Identification Data Description</strong></th>
<th><strong>Unbiasedness Consistency</strong></th>
<th>
<a href="linear-regression.html#gauss-markov-theorem"><strong>Gauss-Markov</strong></a> <strong>(BLUE) Asymptotic Inference (z and Chi-squared)</strong>
</th>
<th><strong>Classical LM (BUE) Small-sample Inference (t and F)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<p><a href="linear-regression.html#a2-full-rank">A2 Full Rank</a></p>
<p>Variation in <span class="math inline">\(\mathbf{X}\)</span></p>
</td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td>
<p><a href="linear-regression.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></p>
<p>Random Sampling</p>
</td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td>
<p><a href="linear-regression.html#a1-linearity">A1 Linearity</a></p>
<p>Linearity in Parameters</p>
</td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td>
<p><a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a></p>
<p>Zero Conditional Mean</p>
</td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td>
<p><a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a></p>
<p><span class="math inline">\(\mathbf{H}\)</span> homoskedasticity</p>
</td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td>
<p><a href="linear-regression.html#a6-normal-distribution">A6 Normal Distribution</a></p>
<p>Normality of Errors</p>
</td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>
<strong>Identification Data Description:</strong> Ensures the model is identifiable and coefficients can be estimated.</li>
<li>
<strong>Unbiasedness Consistency:</strong> Guarantees that OLS estimates are unbiased and converge to the true parameter values as the sample size increases.</li>
<li>
<strong>Gauss-Markov (BLUE) and Asymptotic Inference:</strong> Requires additional assumptions (e.g., homoskedasticity) to ensure minimum variance of estimators and valid inference using large-sample tests (z and chi-squared).</li>
<li>
<strong>Classical LM (BUE) Small-sample Inference:</strong> Builds on all previous assumptions and adds normality of errors for valid t and F tests in finite samples.</li>
</ul>
<hr>
</div>
</div>
<div id="theorems" class="section level4" number="5.1.2.2">
<h4>
<span class="header-section-number">5.1.2.2</span> Theorems<a class="anchor" aria-label="anchor" href="#theorems"><i class="fas fa-link"></i></a>
</h4>
<div id="frischwaughlovell-theorem-frischwaughlovell-theorem" class="section level5" number="5.1.2.2.1">
<h5>
<span class="header-section-number">5.1.2.2.1</span> Frisch–Waugh–Lovell Theorem {#Frisch–Waugh–Lovell Theorem}<a class="anchor" aria-label="anchor" href="#frischwaughlovell-theorem-frischwaughlovell-theorem"><i class="fas fa-link"></i></a>
</h5>
<p>The Frisch–Waugh–Lovell (FWL) Theorem is a fundamental result in linear regression that allows for a deeper understanding of how coefficients are computed in a multiple regression setting <span class="citation">(<a href="references.html#ref-lovell2008simple">Lovell 2008</a>)</span>. Informally, it states:</p>
<blockquote>
<p>When estimating the effect of a subset of variables (<span class="math inline">\(X_1\)</span>) on <span class="math inline">\(y\)</span> in the presence of other variables (<span class="math inline">\(X_2\)</span>), you can “partial out” the influence of <span class="math inline">\(X_2\)</span> from both <span class="math inline">\(y\)</span> and <span class="math inline">\(X_1\)</span>. Then, regressing the residuals of <span class="math inline">\(y\)</span> on the residuals of <span class="math inline">\(X_1\)</span> produces coefficients for <span class="math inline">\(X_1\)</span> that are identical to those obtained from the full multiple regression.</p>
</blockquote>
<p>Consider the multiple linear regression model:</p>
<p><span class="math display">\[ \mathbf{y = X\beta + \epsilon = X_1\beta_1 + X_2\beta_2 + \epsilon} \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of the dependent variable.</li>
<li>
<span class="math inline">\(X_1\)</span> is an <span class="math inline">\(n \times k_1\)</span> matrix of regressors of interest.</li>
<li>
<span class="math inline">\(X_2\)</span> is an <span class="math inline">\(n \times k_2\)</span> matrix of additional regressors.</li>
<li>
<span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are coefficient vectors of sizes <span class="math inline">\(k_1 \times 1\)</span> and <span class="math inline">\(k_2 \times 1\)</span>, respectively.</li>
<li>
<span class="math inline">\(\epsilon\)</span> is an <span class="math inline">\(n \times 1\)</span> error term vector.</li>
</ul>
<p>This can be equivalently represented in partitioned matrix form as:</p>
<p><span class="math display">\[ \left( \begin{array}{cc} X_1'X_1 &amp; X_1'X_2 \\ X_2'X_1 &amp; X_2'X_2 \end{array} \right) \left( \begin{array}{c} \hat{\beta_1} \\ \hat{\beta_2} \end{array} \right) = \left( \begin{array}{c} X_1'y \\ X_2'y \end{array} \right) \]</span></p>
<p>The OLS estimator for the vector <span class="math inline">\(\begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}\)</span> is:</p>
<p><span class="math display">\[
\begin{pmatrix}
\hat{\beta}_1 \\
\hat{\beta}_2
\end{pmatrix}
=
\begin{pmatrix}
X_1'X_1 &amp; X_1'X_2 \\
X_2'X_1 &amp; X_2'X_2
\end{pmatrix}^{-1}
\begin{pmatrix}
X_1'y \\
X_2'y
\end{pmatrix}.
\]</span></p>
<p>If we only want the coefficients on <span class="math inline">\(X_1\)</span>, a known result from partitioned-inversion gives:</p>
<p><span class="math display">\[
\hat{\beta}_1
=
\bigl(X_1' M_2\, X_1\bigr)^{-1}
\,X_1' M_2\, y,
\]</span></p>
<p>where</p>
<p><span class="math display">\[
M_2
=
I
-
X_2 \bigl(X_2'X_2\bigr)^{-1} X_2'.
\]</span></p>
<p>The matrix <span class="math inline">\(M_2\)</span> is often called the <strong>residual-maker</strong> or <strong>annihilator</strong> matrix for <span class="math inline">\(X_2\)</span>. It is an <span class="math inline">\(n \times n\)</span> symmetric, idempotent projection matrix that projects any vector in <span class="math inline">\(\mathbb{R}^n\)</span> onto the orthogonal complement of the column space of <span class="math inline">\(X_2\)</span>. <span class="math inline">\(M_2\)</span> satisfies <span class="math inline">\(M_2^2 = M_2\)</span>, and <span class="math inline">\(M_2 = M_2'\)</span>.</p>
<p>Intuitively, <span class="math inline">\(M_2\)</span> captures the part of <span class="math inline">\(y\)</span> (and any other vector) that is orthogonal to the columns of <span class="math inline">\(X_2\)</span>. This “partialling out” of <span class="math inline">\(X_2\)</span> from both <span class="math inline">\(y\)</span> and <span class="math inline">\(X_1\)</span> lets us isolate <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>Equivalently, we can also represent <span class="math inline">\(\hat{\beta_1}\)</span> as:</p>
<p><span class="math display">\[ \mathbf{\hat{\beta_1} = (X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\hat{\beta_2}} \]</span></p>
<p>From this equation, we can see that</p>
<ol style="list-style-type: decimal">
<li>
<strong>Betas from Multiple vs. Simple Regressions:</strong>
<ul>
<li>The coefficients (<span class="math inline">\(\beta\)</span>) from a multiple regression are generally <strong>not the same</strong> as the coefficients from separate individual simple regressions.</li>
</ul>
</li>
<li>
<strong>Impact of Additional Variables (</strong><span class="math inline">\(X_2\)</span>):
<ul>
<li>The inclusion of different sets of explanatory variables (<span class="math inline">\(X_2\)</span>) affects all coefficient estimates, even for those in <span class="math inline">\(X_1\)</span>.</li>
</ul>
</li>
<li>
<strong>Special Cases:</strong>
<ul>
<li>If <span class="math inline">\(X_1'X_2 = 0\)</span> (orthogonality between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>) or <span class="math inline">\(\hat{\beta_2} = 0\)</span>, the above points (1 and 2) do not hold. In such cases, there is no interaction between the coefficients in <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, making the coefficients in <span class="math inline">\(X_1\)</span> unaffected by <span class="math inline">\(X_2\)</span>.</li>
</ul>
</li>
</ol>
<p><strong>Steps in FWL:</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Partial Out</strong> <span class="math inline">\(X_2\)</span> from <span class="math inline">\(y\)</span>: Regress <span class="math inline">\(y\)</span> on <span class="math inline">\(X_2\)</span> to obtain residuals:</p>
<p><span class="math display">\[
\tilde{y} = M_2y.
\]</span></p>
</li>
<li>
<p><strong>Partial Out</strong> <span class="math inline">\(X_2\)</span> from <span class="math inline">\(X_1\)</span>: For each column of <span class="math inline">\(X_1\)</span>, regress it on <span class="math inline">\(X_2\)</span> to obtain residuals:</p>
<p><span class="math display">\[
\tilde{X}_1 = M_2X_1.
\]</span></p>
</li>
<li>
<p><strong>Regression of Residuals:</strong> Regress <span class="math inline">\(\tilde{y}\)</span> on <span class="math inline">\(\tilde{X}_1\)</span>:</p>
<p><span class="math display">\[
\tilde{y} = \tilde{X}_1\beta_1 + \text{error}.
\]</span></p>
</li>
</ol>
<p>The coefficients <span class="math inline">\(\beta_1\)</span> obtained here are identical to those from the full model regression:</p>
<p><span class="math display">\[
y = X_1\beta_1 + X_2\beta_2 + \epsilon.
\]</span></p>
<p>Why It Matters</p>
<ol style="list-style-type: decimal">
<li><p><strong>Interpretation of Partial Effects:</strong> The FWL Theorem provides a way to interpret <span class="math inline">\(\beta_1\)</span> as the effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(y\)</span> after removing any linear dependence on <span class="math inline">\(X_2\)</span>.</p></li>
<li><p><strong>Computational Simplicity:</strong> It allows the decomposition of a large regression problem into smaller, computationally simpler pieces.</p></li>
</ol>
<div class="sourceCode" id="cb168"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">X1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fl">2</span><span class="op">)</span>, <span class="va">n</span>, <span class="fl">2</span><span class="op">)</span>  <span class="co"># Two regressors of interest</span></span>
<span><span class="va">X2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fl">2</span><span class="op">)</span>, <span class="va">n</span>, <span class="fl">2</span><span class="op">)</span>  <span class="co"># Two additional regressors</span></span>
<span><span class="va">beta1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="op">-</span><span class="fl">1</span><span class="op">)</span>  <span class="co"># Coefficients for X1</span></span>
<span><span class="va">beta2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.5</span><span class="op">)</span>  <span class="co"># Coefficients for X2undefined</span></span>
<span><span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>  <span class="co"># Error term</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">X1</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta1</span> <span class="op">+</span> <span class="va">X2</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta2</span> <span class="op">+</span> <span class="va">u</span>  <span class="co"># Generate dependent variable</span></span>
<span></span>
<span><span class="co"># Full regression</span></span>
<span><span class="va">full_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">X1</span> <span class="op">+</span> <span class="va">X2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">full_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ X1 + X2)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -2.47336 -0.58010  0.07461  0.68778  2.46552 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.11614    0.10000   1.161    0.248    </span></span>
<span><span class="co">#&gt; X11          1.77575    0.10899  16.293  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; X12         -1.14151    0.10204 -11.187  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; X21          0.94954    0.10468   9.071 1.60e-14 ***</span></span>
<span><span class="co">#&gt; X22          0.47667    0.09506   5.014 2.47e-06 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.9794 on 95 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8297, Adjusted R-squared:  0.8225 </span></span>
<span><span class="co">#&gt; F-statistic: 115.7 on 4 and 95 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="co"># Step 1: Partial out X2 from y</span></span>
<span><span class="va">y_residual</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">X2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: Partial out X2 from X1</span></span>
<span><span class="va">X1_residual</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">X1</span> <span class="op">~</span> <span class="va">X2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 3: Regress residuals</span></span>
<span><span class="va">fwl_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y_residual</span> <span class="op">~</span> <span class="va">X1_residual</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fwl_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y_residual ~ X1_residual - 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -2.47336 -0.58010  0.07461  0.68778  2.46552 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; X1_residual1   1.7758     0.1073   16.55   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; X1_residual2  -1.1415     0.1005  -11.36   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.9643 on 98 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8109, Adjusted R-squared:  0.807 </span></span>
<span><span class="co">#&gt; F-statistic: 210.1 on 2 and 98 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="co"># Comparison of coefficients</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Full model coefficients (X1):"</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">full_model</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Full model coefficients (X1): 1.775754 -1.141514</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"FWL model coefficients:"</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fwl_model</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; FWL model coefficients: 1.775754 -1.141514</span></span></code></pre></div>
</div>
<div id="gauss-markov-theorem" class="section level5" number="5.1.2.2.2">
<h5>
<span class="header-section-number">5.1.2.2.2</span> Gauss-Markov Theorem<a class="anchor" aria-label="anchor" href="#gauss-markov-theorem"><i class="fas fa-link"></i></a>
</h5>
<p>For a linear regression model:</p>
<p><span class="math display">\[
\mathbf{y = X\beta + \epsilon},
\]</span></p>
<p>under the assumptions:</p>
<ol style="list-style-type: decimal">
<li>
<a href="linear-regression.html#a1-linearity"><strong>A1</strong></a>: Linearity of the model.</li>
<li>
<a href="linear-regression.html#a2-full-rank"><strong>A2</strong></a>: Full rank of <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>
<a href="linear-regression.html#a3-exogeneity-of-independent-variables"><strong>A3</strong></a>: Exogeneity of <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>
<a href="linear-regression.html#a4-homoskedasticity"><strong>A4</strong></a>: Homoskedasticity of <span class="math inline">\(\epsilon\)</span>.</li>
</ol>
<p>The <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> estimator:</p>
<p><span class="math display">\[
\hat{\beta} = \mathbf{(X'X)^{-1}X'y},
\]</span></p>
<p>is the <strong>Best Linear Unbiased Estimator (BLUE)</strong>. This means that <span class="math inline">\(\hat{\beta}\)</span> has the minimum variance among all linear unbiased estimators of <span class="math inline">\(\beta\)</span>.</p>
<p><strong>1. Unbiasedness</strong></p>
<p>Suppose we consider <strong>any</strong> linear estimator of <span class="math inline">\(\beta\)</span> of the form:</p>
<p><span class="math display">\[
\tilde{\beta} = \mathbf{C\,y},
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(n \times 1\)</span> vector of observations,</li>
<li>
<span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(k \times n\)</span> matrix (with <span class="math inline">\(k\)</span> the dimension of <span class="math inline">\(\beta\)</span>) that depends <strong>only</strong> on the design matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul>
<p>Our regression model is</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X}\beta + \boldsymbol{\epsilon},
\quad
E[\boldsymbol{\epsilon} \mid \mathbf{X}] = \mathbf{0},
\quad
\mathrm{Var}(\boldsymbol{\epsilon} \mid \mathbf{X}) = \sigma^2 \mathbf{I}.
\]</span></p>
<p>We say <span class="math inline">\(\tilde{\beta}\)</span> is <strong>unbiased</strong> if its conditional expectation (given <span class="math inline">\(\mathbf{X}\)</span>) equals the true parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
E(\tilde{\beta} \mid \mathbf{X})
=
E(\mathbf{C\,y} \mid \mathbf{X})
=
\beta.
\]</span></p>
<ol style="list-style-type: decimal">
<li>Substitute <span class="math inline">\(\mathbf{y} = \mathbf{X}\beta + \boldsymbol{\epsilon}\)</span>:</li>
</ol>
<p><span class="math display">\[
E(\mathbf{C\,y} \mid \mathbf{X})
=
E\bigl(\mathbf{C}(\mathbf{X}\beta + \boldsymbol{\epsilon}) \mid \mathbf{X}\bigr)
=
\mathbf{C\,X}\,\beta
+
\mathbf{C}\,E(\boldsymbol{\epsilon} \mid \mathbf{X})
=
\mathbf{C\,X}\,\beta.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>For this to hold for <em>all</em> <span class="math inline">\(\beta\)</span>, we require</li>
</ol>
<p><span class="math display">\[
\mathbf{C\,X} = \mathbf{I}.
\]</span></p>
<p>In other words, <span class="math inline">\(\mathbf{C}\)</span> must be a “right-inverse” of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>On the other hand, the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is given by</p>
<p><span class="math display">\[
\hat{\beta}
=
(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{y}.
\]</span></p>
<p>You can verify:</p>
<ul>
<li>Let <span class="math inline">\(\mathbf{C}_{\text{OLS}} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\)</span>.</li>
<li>Then <span class="math display">\[
\mathbf{C}_{\text{OLS}}\,\mathbf{X}
=
(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{X}
=
\mathbf{I}.
\]</span>
</li>
<li>By the argument above, this makes <span class="math inline">\(\hat{\beta}\)</span> unbiased.</li>
</ul>
<p>Hence, any linear estimator <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> that is unbiased must satisfy <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>.</p>
<p><strong>2. Minimum Variance (Gauss–Markov Part)</strong></p>
<p>Among all estimators of the form <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> that are unbiased (so <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>), <strong>OLS</strong> achieves the smallest covariance matrix.</p>
<ul>
<li>Variance of a General Unbiased Estimator</li>
</ul>
<p>If <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> with <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>, then:</p>
<p><span class="math display">\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
=
\mathrm{Var}(\mathbf{C\,y} \mid \mathbf{X})
=
\mathbf{C}\,\mathrm{Var}(\mathbf{y} \mid \mathbf{X})\,\mathbf{C}'
=
\mathbf{C}\bigl(\sigma^2 \mathbf{I}\bigr)\mathbf{C}'
=
\sigma^2\,\mathbf{C}\,\mathbf{C}'.
\]</span></p>
<ul>
<li>Variance of the OLS Estimator</li>
</ul>
<p>For OLS, <span class="math inline">\(\hat{\beta} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{y}\)</span>. Thus,</p>
<p><span class="math display">\[
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
=
\sigma^2\,(\mathbf{X}'\mathbf{X})^{-1}.
\]</span></p>
<ul>
<li>Comparing <span class="math inline">\(\mathrm{Var}(\tilde{\beta})\)</span> to <span class="math inline">\(\mathrm{Var}(\hat{\beta})\)</span>
</li>
</ul>
<p>We want to show:</p>
<p><span class="math display">\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
-
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
\;\;\text{is positive semi-definite.}
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Since both <span class="math inline">\(\tilde{\beta}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are unbiased, we know: <span class="math display">\[
\mathbf{C\,X} = \mathbf{I},
\quad
(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{X} = \mathbf{I}.
\]</span></p></li>
<li><p>One can show algebraically (as in the proof provided above) that <span class="math display">\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
-
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
=
\sigma^2 \bigl[\mathbf{C}\mathbf{C}' - (\mathbf{X}'\mathbf{X})^{-1}\bigr].
\]</span> Under the condition <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>, the difference <span class="math inline">\(\mathbf{C}\mathbf{C}' - (\mathbf{X}'\mathbf{X})^{-1}\)</span> is positive semi-definite.</p></li>
<li><p>Positive semi-definite difference means</p></li>
</ol>
<p><span class="math display">\[
\mathbf{v}' \Bigl(\mathbf{C}\mathbf{C}'
-
(\mathbf{X}'\mathbf{X})^{-1}\Bigr)\mathbf{v}
\ge
0
\quad
\text{for all vectors } \mathbf{v}.
\]</span></p>
<p>Hence, <span class="math inline">\(\hat{\beta}\)</span> has the smallest variance (in the sense of covariance matrices) among all linear unbiased estimators <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span>.</p>
<p><strong>Summary of the Key Points</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Unbiasedness:</strong><br>
A linear estimator <span class="math inline">\(\tilde{\beta} = \mathbf{C\,y}\)</span> is unbiased if <span class="math inline">\(E(\tilde{\beta}\mid \mathbf{X}) = \beta\)</span>.<br>
This forces <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>.</p></li>
<li><p><strong>OLS is Unbiased:</strong><br>
The OLS estimator <span class="math inline">\(\hat{\beta} = (X'X)^{-1} X' \, y\)</span> satisfies <span class="math inline">\((X'X)^{-1} X' \, X = I\)</span>, hence is unbiased.</p></li>
<li><p><strong>OLS has Minimum Variance:</strong><br>
Among all <span class="math inline">\(\mathbf{C}\)</span> that satisfy <span class="math inline">\(\mathbf{C\,X} = \mathbf{I}\)</span>, the matrix <span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1}\)</span> gives the smallest possible <span class="math inline">\(\mathrm{Var}(\tilde{\beta})\)</span>.<br>
In matrix form, <span class="math inline">\(\mathrm{Var}(\tilde{\beta}) - \mathrm{Var}(\hat{\beta})\)</span> is positive semi-definite, showing OLS is optimal (the <strong>Best Linear Unbiased Estimator</strong>, BLUE).</p></li>
</ol>
</div>
</div>
<div id="finite-sample-properties" class="section level4" number="5.1.2.3">
<h4>
<span class="header-section-number">5.1.2.3</span> Finite Sample Properties<a class="anchor" aria-label="anchor" href="#finite-sample-properties"><i class="fas fa-link"></i></a>
</h4>
<p>The finite sample properties of an estimator are considered when the sample size <span class="math inline">\(n\)</span> is fixed (not asymptotically large). Key properties include <strong>bias</strong>, <strong>distribution</strong>, and <strong>standard deviation</strong> of the estimator.</p>
<hr>
<p><strong>Bias</strong> measures how close an estimator is, on average, to the true parameter value <span class="math inline">\(\beta\)</span>. It is defined as:</p>
<p><span class="math display">\[
\text{Bias} = E(\hat{\beta}) - \beta
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\beta\)</span>: True parameter value.</li>
<li>
<span class="math inline">\(\hat{\beta}\)</span>: Estimator for <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p><strong>Unbiased Estimator</strong>: An estimator is unbiased if:</p>
<p><span class="math display">\[
\text{Bias} = E(\hat{\beta}) - \beta = 0 \quad \text{or equivalently} \quad E(\hat{\beta}) = \beta
\]</span></p>
<p>This means the estimator will produce estimates that are, on average, equal to the value it is trying to estimate.</p>
<hr>
<p>An estimator is a function of random variables (data). Its <strong>distribution</strong> describes how the estimates vary across repeated samples. Key aspects include:</p>
<ul>
<li>
<strong>Center</strong>: Mean of the distribution, which relates to bias.</li>
<li>
<strong>Spread</strong>: Variability of the estimator, captured by its standard deviation or variance.</li>
</ul>
<hr>
<p>The <strong>standard deviation</strong> of an estimator measures the spread of its sampling distribution. It indicates the variability of the estimator across different samples.</p>
<hr>
<div id="ordinary-least-squares-properties" class="section level5" number="5.1.2.3.1">
<h5>
<span class="header-section-number">5.1.2.3.1</span> Ordinary Least Squares Properties<a class="anchor" aria-label="anchor" href="#ordinary-least-squares-properties"><i class="fas fa-link"></i></a>
</h5>
<p>Under the standard assumptions for OLS:</p>
<ol style="list-style-type: decimal">
<li>
<a href="linear-regression.html#a1-linearity">A1</a>: The relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li>
<a href="linear-regression.html#a2-full-rank">A2</a>: The matrix <span class="math inline">\(\mathbf{X'X}\)</span> is invertible.</li>
<li>
<a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a>: <span class="math inline">\(E(\epsilon|X) = 0\)</span> (errors are uncorrelated with predictors).</li>
</ol>
<p><strong>OLS is unbiased</strong> under these assumptions. The proof is as follows:</p>
<p><span class="math display">\[
\begin{aligned}
E(\hat{\beta}) &amp;= E(\mathbf{(X'X)^{-1}X'y}) &amp;&amp; \text{A2}\\
               &amp;= E(\mathbf{(X'X)^{-1}X'(X\beta + \epsilon)}) &amp;&amp; \text{A1}\\
               &amp;= E(\mathbf{(X'X)^{-1}X'X\beta + (X'X)^{-1}X'\epsilon}) \\
               &amp;= E(\beta + \mathbf{(X'X)^{-1}X'\epsilon}) \\
               &amp;= \beta + E(\mathbf{(X'X)^{-1}X'\epsilon}) \\
               &amp;= \beta + E(E(\mathbf{(X'X)^{-1}X'\epsilon}|X)) &amp;&amp; \text{LIE (Law of Iterated Expectation)} \\
               &amp;= \beta + E(\mathbf{(X'X)^{-1}X'}E(\epsilon|X)) \\
               &amp;= \beta + E(\mathbf{(X'X)^{-1}X'} \cdot 0) &amp;&amp; \text{A3}\\
               &amp;= \beta
\end{aligned}
\]</span></p>
<p><strong>Key Points</strong>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Linearity of Expectation</strong>: Used to separate terms involving <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\epsilon\)</span>.</li>
<li>
<a href="prerequisites.html#law-of-iterated-expectation">Law of Iterated Expectation</a> <strong>(LIE)</strong>: Simplifies nested expectations.</li>
<li>
<strong>Exogeneity of Errors (A3)</strong>: Ensures <span class="math inline">\(E(\epsilon|X) = 0\)</span>, eliminating bias.</li>
</ol>
<hr>
<p>Implications of Unbiasedness</p>
<ul>
<li>OLS estimators are centered around the true value <span class="math inline">\(\beta\)</span> across repeated samples.</li>
<li>In small samples, OLS estimators may exhibit variability, but their expected value remains <span class="math inline">\(\beta\)</span>.</li>
</ul>
<p>If the assumption of exogeneity (<a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a>) is violated, the OLS estimator becomes <strong>biased</strong>. Specifically, omitted variables or endogeneity can introduce systematic errors into the estimation.</p>
<p>From the <strong>Frisch-Waugh-Lovell Theorem</strong>:</p>
<ul>
<li>If an omitted variable <span class="math inline">\(\hat{\beta}_2 \neq 0\)</span> (non-zero effect) and the omitted variable is correlated with the included predictors (<span class="math inline">\(\mathbf{X_1'X_2} \neq 0\)</span>), then the OLS estimator will be biased.</li>
<li>This bias arises because the omitted variable contributes to the variation in the dependent variable, but its effect is incorrectly attributed to other predictors.</li>
</ul>
<hr>
</div>
<div id="conditional-variance-of-ols-estimator" class="section level5" number="5.1.2.3.2">
<h5>
<span class="header-section-number">5.1.2.3.2</span> Conditional Variance of OLS Estimator<a class="anchor" aria-label="anchor" href="#conditional-variance-of-ols-estimator"><i class="fas fa-link"></i></a>
</h5>
<p>Under assumptions <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a>, and <a href="linear-regression.html#a4-homoskedasticity">A4</a>, the <strong>conditional variance of the OLS estimator</strong> is:</p>
<p><span class="math display">\[
\begin{aligned}
Var(\hat{\beta}|\mathbf{X}) &amp;= Var(\beta + \mathbf{(X'X)^{-1}X'\epsilon|X}) &amp;&amp; \text{A1-A2} \\
    &amp;= Var((\mathbf{X'X)^{-1}X'\epsilon|X}) \\
    &amp;= \mathbf{(X'X)^{-1}X'} Var(\epsilon|\mathbf{X})\mathbf{X(X'X)^{-1}} \\
    &amp;= \mathbf{(X'X)^{-1}X'} \sigma^2 I \mathbf{X(X'X)^{-1}} &amp;&amp; \text{A4} \\
    &amp;= \sigma^2 \mathbf{(X'X)^{-1}}
\end{aligned}
\]</span></p>
<p>This result shows that the variance of <span class="math inline">\(\hat{\beta}\)</span> depends on:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\sigma^2\)</span>: The variance of the errors.</li>
<li>
<span class="math inline">\(\mathbf{X'X}\)</span>: The information content in the design matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ol>
<hr>
</div>
<div id="sources-of-variation-in-ols-estimator" class="section level5" number="5.1.2.3.3">
<h5>
<span class="header-section-number">5.1.2.3.3</span> Sources of Variation in OLS Estimator<a class="anchor" aria-label="anchor" href="#sources-of-variation-in-ols-estimator"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li>
<strong>Unexplained Variation in the Dependent Variable</strong>: <span class="math inline">\(\sigma^2 = Var(\epsilon_i|\mathbf{X})\)</span>
</li>
</ol>
<ul>
<li>Large <span class="math inline">\(\sigma^2\)</span> indicates that the amount of unexplained variation (noise) is high relative to the explained variation (<span class="math inline">\(\mathbf{x_i \beta}\)</span>).</li>
<li>This increases the variance of the OLS estimator.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Small Variation in Predictor Variables</strong></li>
</ol>
<ul>
<li>If the variance of predictors (<span class="math inline">\(Var(x_{i1}), Var(x_{i2}), \dots\)</span>) is small, the design matrix <span class="math inline">\(\mathbf{X}\)</span> lacks information, leading to:
<ul>
<li>High variability in <span class="math inline">\(\hat{\beta}\)</span>.</li>
<li>Potential issues in estimating coefficients accurately.</li>
</ul>
</li>
<li>
<strong>Small sample size</strong> exacerbates this issue, as fewer observations reduce the robustness of parameter estimates.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Correlation Between Explanatory Variables (Collinearity)</strong></li>
</ol>
<ul>
<li>
<strong>Strong correlation</strong> among explanatory variables creates problems:
<ul>
<li>
<span class="math inline">\(x_{i1}\)</span> being highly correlated with a linear combination of <span class="math inline">\(1, x_{i2}, x_{i3}, \dots\)</span> contributes to inflated standard errors for <span class="math inline">\(\hat{\beta}_1\)</span>.</li>
<li>Including many irrelevant variables exacerbates this issue.</li>
</ul>
</li>
<li>
<strong>Perfect Collinearity</strong>:
<ul>
<li>If <span class="math inline">\(x_1\)</span> is perfectly determined by a linear combination of other predictors, the matrix <span class="math inline">\(\mathbf{X'X}\)</span> becomes singular.</li>
<li>This violates <a href="linear-regression.html#a2-full-rank">A2</a>, making OLS impossible to compute.</li>
</ul>
</li>
<li>
<strong>Multicollinearity</strong>:
<ul>
<li>If <span class="math inline">\(x_1\)</span> is highly correlated (but not perfectly) with a linear combination of other variables, the variance of <span class="math inline">\(\hat{\beta}_1\)</span> increases.</li>
<li>Multicollinearity does not violate OLS assumptions but weakens inference by inflating standard errors.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="standard-errors" class="section level5" number="5.1.2.3.4">
<h5>
<span class="header-section-number">5.1.2.3.4</span> Standard Errors<a class="anchor" aria-label="anchor" href="#standard-errors"><i class="fas fa-link"></i></a>
</h5>
<p>Standard errors measure the variability of an estimator, specifically the standard deviation of <span class="math inline">\(\hat{\beta}\)</span>. They are crucial for inference, as they quantify the uncertainty associated with parameter estimates.</p>
<hr>
<p>The variance of the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is:</p>
<p><span class="math display">\[
Var(\hat{\beta}|\mathbf{X}) = \sigma^2 \mathbf{(X'X)^{-1}}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\sigma^2\)</span>: Variance of the error terms.</li>
<li>
<span class="math inline">\(\mathbf{(X'X)^{-1}}\)</span>: Inverse of the design matrix product, capturing the geometry of the predictors.</li>
</ul>
<hr>
<p><strong>Estimation of</strong> <span class="math inline">\(\sigma^2\)</span></p>
<p>Under assumptions <a href="linear-regression.html#a1-linearity">A1</a> through <a href="#a5-homoskedasticity">A5</a>, we can estimate <span class="math inline">\(\sigma^2\)</span> as:</p>
<p><span class="math display">\[
s^2 = \frac{1}{n-k} \sum_{i=1}^{n} e_i^2 = \frac{1}{n-k} SSR
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span>: Number of observations.</li>
<li>
<span class="math inline">\(k\)</span>: Number of predictors, including the intercept.</li>
<li>
<span class="math inline">\(e_i\)</span>: Residuals from the regression model (<span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>).</li>
<li>
<span class="math inline">\(SSR\)</span>: Sum of squared residuals (<span class="math inline">\(\sum e_i^2\)</span>).</li>
</ul>
<p>The degrees of freedom adjustment (<span class="math inline">\(n-k\)</span>) accounts for the fact that residuals <span class="math inline">\(e_i\)</span> are not true errors <span class="math inline">\(\epsilon_i\)</span>. Since the regression model uses <span class="math inline">\(k\)</span> parameters, we lose <span class="math inline">\(k\)</span> degrees of freedom in estimating variance.</p>
<p>The standard error for <span class="math inline">\(\sigma\)</span> is:</p>
<p><span class="math display">\[
s = \sqrt{s^2}
\]</span></p>
<p>However, <span class="math inline">\(s\)</span> is a biased estimator of <span class="math inline">\(\sigma\)</span> due to <a href="prerequisites.html#jensens-inequality">Jensen’s Inequality</a>.</p>
<hr>
<p>The standard error of each regression coefficient <span class="math inline">\(\hat{\beta}_{j-1}\)</span> is:</p>
<p><span class="math display">\[
SE(\hat{\beta}_{j-1}) = s \sqrt{[(\mathbf{X'X})^{-1}]_{jj}}
\]</span></p>
<p>Alternatively, it can be expressed in terms of <span class="math inline">\(SST_{j-1}\)</span> and <span class="math inline">\(R_{j-1}^2\)</span>:</p>
<p><span class="math display">\[
SE(\hat{\beta}_{j-1}) = \frac{s}{\sqrt{SST_{j-1}(1 - R_{j-1}^2)}}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(SST_{j-1}\)</span>: Total sum of squares for <span class="math inline">\(x_{j-1}\)</span> from a regression of <span class="math inline">\(x_{j-1}\)</span> on all other predictors.</li>
<li>
<span class="math inline">\(R_{j-1}^2\)</span>: Coefficient of determination for the same regression.</li>
</ul>
<p>This formulation highlights the role of multicollinearity, as <span class="math inline">\(R_{j-1}^2\)</span> reflects the correlation between <span class="math inline">\(x_{j-1}\)</span> and other predictors.</p>
<hr>
</div>
<div id="summary-of-finite-sample-properties-of-ols-under-different-assumptions" class="section level5" number="5.1.2.3.5">
<h5>
<span class="header-section-number">5.1.2.3.5</span> Summary of Finite Sample Properties of OLS Under Different Assumptions<a class="anchor" aria-label="anchor" href="#summary-of-finite-sample-properties-of-ols-under-different-assumptions"><i class="fas fa-link"></i></a>
</h5>
<ol style="list-style-type: decimal">
<li><p><strong>Under <a href="linear-regression.html#a1-linearity">A1-A3</a></strong>: OLS is unbiased. <span class="math display">\[ E(\hat{\beta}) = \beta \]</span></p></li>
<li><p><strong>Under <a href="linear-regression.html#a4-homoskedasticity">A1-A4</a></strong>: The variance of the OLS estimator is: <span class="math display">\[ Var(\hat{\beta}|\mathbf{X}) = \sigma^2 \mathbf{(X'X)^{-1}} \]</span></p></li>
<li><p><strong>Under <a href="#a6-normality-of-errors">A1-A4, A6</a></strong>: The OLS estimator is normally distributed: <span class="math display">\[ \hat{\beta} \sim N(\beta, \sigma^2 \mathbf{(X'X)^{-1}}) \]</span></p></li>
<li><p><strong>Under <a href="linear-regression.html#gauss-markov-theorem">A1-A4, Gauss-Markov Theorem</a></strong>: OLS is BLUE (Best Linear Unbiased Estimator).</p></li>
<li><p><strong>Under <a href="#a5-homoskedasticity">A1-A5</a></strong>: The standard errors for <span class="math inline">\(\hat{\beta}\)</span> are unbiased estimators of the standard deviation of <span class="math inline">\(\hat{\beta}\)</span>.</p></li>
</ol>
<hr>
</div>
</div>
<div id="large-sample-properties" class="section level4" number="5.1.2.4">
<h4>
<span class="header-section-number">5.1.2.4</span> Large Sample Properties<a class="anchor" aria-label="anchor" href="#large-sample-properties"><i class="fas fa-link"></i></a>
</h4>
<p>Large sample properties provide a framework to evaluate the quality of estimators when finite sample properties are either uninformative or computationally infeasible. This perspective becomes crucial in modern data analysis, especially for methods like GLS or MLE, where assumptions for finite sample analysis may not hold.</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<caption>When to Use Finite vs. Large Sample Analysis</caption>
<colgroup>
<col width="17%">
<col width="42%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Finite Sample Properties</strong></th>
<th><strong>Large Sample Properties</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Applicability</strong></td>
<td>Limited to fixed sample sizes</td>
<td>Relevant for <span class="math inline">\(n \to \infty\)</span>
</td>
</tr>
<tr class="even">
<td><strong>Exactness</strong></td>
<td>Exact results (e.g., distributions, unbiasedness)</td>
<td>Approximate results</td>
</tr>
<tr class="odd">
<td><strong>Assumptions</strong></td>
<td>May require stronger assumptions (e.g., normality, independence)</td>
<td>Relies on asymptotic approximations (e.g., CLT, LLN)</td>
</tr>
<tr class="even">
<td><strong>Estimator Behavior</strong></td>
<td>Performance may vary significantly</td>
<td>Estimators stabilize and improve in accuracy</td>
</tr>
<tr class="odd">
<td><strong>Ease of Use</strong></td>
<td>Often complex due to reliance on exact distributions</td>
<td>Simplifies analysis by leveraging asymptotic approximations</td>
</tr>
<tr class="even">
<td><strong>Real-World Relevance</strong></td>
<td>More realistic for small datasets</td>
<td>More relevant for large datasets</td>
</tr>
</tbody>
</table></div>
<ul>
<li>
<p><strong>Finite Sample Analysis</strong>:</p>
<ul>
<li><p>Small sample sizes (e.g., <span class="math inline">\(n &lt; 30\)</span>).</p></li>
<li><p>Critical for studies where exact results are needed.</p></li>
<li><p>Useful in experimental designs and case studies.</p></li>
</ul>
</li>
<li>
<p><strong>Large Sample Analysis</strong>:</p>
<ul>
<li><p>Large datasets (e.g., <span class="math inline">\(n &gt; 100\)</span>).</p></li>
<li><p>Necessary when asymptotic approximations improve computational simplicity.</p></li>
</ul>
</li>
</ul>
<hr>
<p><strong>Key Concepts:</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Consistency</strong>:
<ul>
<li>Consistency ensures that an estimator converges in probability to the true parameter value as the sample size increases.</li>
<li>Mathematically, an estimator <span class="math inline">\(\hat{\theta}\)</span> is consistent for <span class="math inline">\(\theta\)</span> if: <span class="math display">\[
\hat{\theta}_n \to^p \theta \quad \text{as } n \to \infty.
\]</span>
</li>
<li>Consistency does not imply unbiasedness, and unbiasedness does not guarantee consistency.</li>
</ul>
</li>
<li>
<strong>Asymptotic Distribution</strong>:
<ul>
<li>The limiting distribution describes the shape of the scaled estimator as <span class="math inline">\(n \to \infty\)</span>.</li>
<li>Asymptotic distributions often follow normality due to the <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a>, which underpins much of inferential statistics.</li>
</ul>
</li>
<li>
<strong>Asymptotic Variance</strong>:
<ul>
<li>Represents the spread of the estimator with respect to its limiting distribution.</li>
<li>Smaller asymptotic variance implies greater precision in large samples.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Motivation</strong></p>
<p><a href="linear-regression.html#finite-sample-properties">Finite Sample Properties</a>, such as unbiasedness, rely on strong assumptions like:</p>
<ul>
<li><a href="linear-regression.html#a1-linearity">A1 Linearity</a></li>
<li><a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a></li>
<li><a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a></li>
<li><a href="linear-regression.html#a6-normal-distribution">A6 Normal Distribution</a></li>
</ul>
<p>When these assumptions are violated or impractical to verify, finite sample properties lose relevance. In such cases, <a href="linear-regression.html#large-sample-properties">Large Sample Properties</a>serve as an essential alternative for evaluating estimators.</p>
<p>For example, let the conditional expectation function (CEF) be: <span class="math display">\[
\mu(\mathbf{X}) = E(y | \mathbf{X}),
\]</span> which represents the <strong>minimum mean squared predictor</strong> over all possible functions <span class="math inline">\(f(\mathbf{X})\)</span>: <span class="math display">\[
\min_f E((y - f(\mathbf{X}))^2).
\]</span></p>
<p>Under the assumptions A1 and A3, the CEF simplifies to: <span class="math display">\[
\mu(\mathbf{X}) = \mathbf{X}\beta.
\]</span></p>
<p>The <strong>linear projection</strong> is given by: <span class="math display">\[
L(y | 1, \mathbf{X}) = \gamma_0 + \mathbf{X}\text{Var}(\mathbf{X})^{-1}\text{Cov}(\mathbf{X}, y),
\]</span> where: <span class="math display">\[
\gamma = \mathbf{X}\text{Var}(\mathbf{X})^{-1}\text{Cov}(\mathbf{X}, y).
\]</span></p>
<p>This linear projection minimizes the mean squared error: <span class="math display">\[
(\gamma_0, \gamma) = \arg\min_{(a, b)} E\left[\left(E(y|\mathbf{X}) - \left(a + \mathbf{X}b\right)\right)^2\right].
\]</span></p>
<p>Implications for OLS</p>
<ul>
<li>
<strong>Consistency</strong>: OLS is always consistent for the linear projection, ensuring convergence to the true parameter value as <span class="math inline">\(n \to \infty\)</span>.</li>
<li>
<strong>Causal Interpretation</strong>: The linear projection has no inherent causal interpretation—it approximates the conditional mean function.</li>
<li>
<strong>Assumption Independence</strong>: Unlike the CEF, the linear projection does not depend on assumptions A1 and A3.</li>
</ul>
<hr>
<p><strong>Evaluating Estimators via Large Sample Properties</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Consistency</strong>:
<ul>
<li>Measures the estimator’s centrality to the true value.</li>
<li>A consistent estimator ensures that with larger samples, estimates become arbitrarily close to the population parameter.</li>
</ul>
</li>
<li>
<strong>Limiting Distribution</strong>:
<ul>
<li>Helps infer the sampling behavior of the estimator as <span class="math inline">\(n\)</span> grows.</li>
<li>Often approximated by a normal distribution for practical use in hypothesis testing and confidence interval construction.</li>
</ul>
</li>
<li>
<strong>Asymptotic Variance</strong>:
<ul>
<li>Quantifies the dispersion of the estimator around its limiting distribution.</li>
<li>Smaller variance is desirable for greater reliability.</li>
</ul>
</li>
</ol>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is <strong>consistent</strong> for a parameter <span class="math inline">\(\theta\)</span> if, as the sample size <span class="math inline">\(n\)</span> increases, <span class="math inline">\(\hat{\theta}\)</span> converges in probability to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\hat{\theta}_n \to^p \theta \quad \text{as } n \to \infty.
\]</span></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Convergence in Probability</strong>:</p>
<ul>
<li>The probability that <span class="math inline">\(\hat{\theta}\)</span> deviates from <span class="math inline">\(\theta\)</span> by more than a small margin (no matter how small) approaches zero as <span class="math inline">\(n\)</span> increases.</li>
</ul>
<p>Formally: <span class="math display">\[
\forall \epsilon &gt; 0, \quad P(|\hat{\theta}_n - \theta| &gt; \epsilon) \to 0 \quad \text{as } n \to \infty.
\]</span></p>
</li>
<li><p><strong>Interpretation</strong>: Consistency ensures that the estimator becomes arbitrarily close to the true population parameter <span class="math inline">\(\theta\)</span> as the sample size grows.</p></li>
<li><p><strong>Asymptotic Behavior</strong>: Large sample properties rely on consistency to provide valid approximations of an estimator’s behavior in finite samples.</p></li>
</ol>
<hr>
<p><strong>Relationship Between Consistency and Unbiasedness</strong></p>
<ul>
<li>
<strong>Unbiasedness</strong>:
<ul>
<li>An estimator <span class="math inline">\(\hat{\theta}\)</span> is unbiased if its expected value equals the true parameter: <span class="math display">\[
E(\hat{\theta}) = \theta.
\]</span>
</li>
<li>Unbiasedness is a finite-sample property and does not depend on the sample size.</li>
</ul>
</li>
<li>
<strong>Consistency</strong>:
<ul>
<li>Consistency is a large-sample property and requires <span class="math inline">\(\hat{\theta}\)</span> to converge to <span class="math inline">\(\theta\)</span> as <span class="math inline">\(n \to \infty\)</span>.</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Important Distinctions</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Unbiasedness Does Not Imply Consistency</strong>:
<ul>
<li>Example: Consider an unbiased estimator with extremely high variance that does not diminish as <span class="math inline">\(n\)</span> increases. Such an estimator does not converge to <span class="math inline">\(\theta\)</span> in probability.</li>
</ul>
</li>
<li>
<strong>Consistency Does Not Imply Unbiasedness</strong>:
<ul>
<li>Example: <span class="math inline">\(\hat{\theta}_n = \frac{n-1}{n}\theta\)</span> is biased for all finite <span class="math inline">\(n\)</span>, but as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\hat{\theta}_n \to^p \theta\)</span>, making it consistent.</li>
</ul>
</li>
</ol>
<hr>
<p>From the OLS formula: <span class="math display">\[
\hat{\beta} = \mathbf{(X'X)^{-1}X'y},
\]</span> we can expand as: <span class="math display">\[
\hat{\beta} = \mathbf{(\sum_{i=1}^{n}x_i'x_i)^{-1} \sum_{i=1}^{n}x_i'y_i},
\]</span> or equivalently: <span class="math display">\[
\hat{\beta} = (n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}.
\]</span></p>
<p>Taking the probability limit under the assumptions <a href="linear-regression.html#a2-full-rank">A2</a> and <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a>, we apply the <a href="prerequisites.html#weak-law-of-large-numbers">Weak Law of Large Numbers</a> (for a random sample, averages converge to expectations as <span class="math inline">\(n \to \infty\)</span>): <span class="math display">\[
plim(\hat{\beta}) = plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}),
\]</span> which simplifies to: <span class="math display">\[
\begin{aligned}
plim(\hat{\beta})
  &amp;= (E(\mathbf{x_i'x_i}))^{-1}E(\mathbf{x_i'y_i}) &amp; \text{A1}\\
  &amp;= (E(\mathbf{x_i'x_i}))^{-1} \bigl(E(\mathbf{x_i'x_i} \,\beta) + E(\mathbf{x_i\,\epsilon_i})\bigr) &amp; (A3a) \\
  &amp;= (E(\mathbf{x_i' x_i}))^{-1}E(\mathbf{x_i' x_i})\, \beta &amp; (A2)\\
  &amp;= \beta
\end{aligned}
\]</span></p>
<p><strong>Proof:</strong></p>
<p>For a model of <span class="math inline">\(y_i = x_i \beta + \epsilon_i\)</span>, where <span class="math inline">\(\beta\)</span> is the true parameter vector, <span class="math inline">\(\epsilon_i\)</span> is the random error:</p>
<p>Expanding <span class="math inline">\(E(x_i y_i)\)</span>:</p>
<p><span class="math display">\[
E(x_i'y_i) = E(x_i'(x_i\beta + \epsilon_i))
\]</span></p>
<p>By the linearity of expectation:</p>
<p><span class="math display">\[
E(x_i'y_i) = E(x_i'x_i \beta) + E(x_i \epsilon_i)
\]</span></p>
<p>The second term <span class="math inline">\(E(x_i \epsilon_i) = 0\)</span> under assumption <a href="#a3a">A3a</a>.</p>
<p>Thus, <span class="math display">\[
E(x_i'y_i) = E(x_i'x_i)\beta.
\]</span></p>
<hr>
<p><strong>Consistency of OLS</strong></p>
<p>Hence, in short, under the assumptions:</p>
<ul>
<li><a href="linear-regression.html#a1-linearity">A1 Linearity</a></li>
<li><a href="linear-regression.html#a2-full-rank">A2 Full Rank</a></li>
<li><a href="linear-regression.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a></li>
<li><a href="linear-regression.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></li>
</ul>
<p>the term <span class="math inline">\(E(\mathbf{x_i'\epsilon_i}) = 0\)</span>, and the OLS estimator is <strong>consistent</strong>: <span class="math display">\[
plim(\hat{\beta}) = \beta.
\]</span></p>
<p>However, OLS consistency does not guarantee unbiasedness in small samples.</p>
<hr>
<div id="asymptotic-distribution-of-ols" class="section level5" number="5.1.2.4.1">
<h5>
<span class="header-section-number">5.1.2.4.1</span> Asymptotic Distribution of OLS<a class="anchor" aria-label="anchor" href="#asymptotic-distribution-of-ols"><i class="fas fa-link"></i></a>
</h5>
<p>Under the same assumptions :</p>
<ul>
<li><a href="linear-regression.html#a1-linearity">A1 Linearity</a></li>
<li><a href="linear-regression.html#a2-full-rank">A2 Full Rank</a></li>
<li><a href="linear-regression.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a></li>
<li><a href="linear-regression.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></li>
</ul>
<p>and if <span class="math inline">\(\mathbf{x_i'x_i}\)</span> has finite first and second moments (required for the <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a>), we have:</p>
<ol style="list-style-type: decimal">
<li><p>Convergence of <span class="math inline">\(n^{-1}\sum_{i=1}^n \mathbf{x_i'x_i}\)</span>: <span class="math display">\[
n^{-1}\sum_{i=1}^n \mathbf{x_i'x_i} \to^p E(\mathbf{x_i'x_i}).
\]</span></p></li>
<li><p>Asymptotic normality of <span class="math inline">\(\sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i'\epsilon_i})\)</span>: <span class="math display">\[
\sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i'\epsilon_i}) \to^d N(0, \mathbf{B}),
\]</span> where <span class="math inline">\(\mathbf{B} = Var(\mathbf{x_i'\epsilon_i})\)</span>.</p></li>
</ol>
<p>From these results, the scaled difference between <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\beta\)</span> follows: <span class="math display">\[
\sqrt{n}(\hat{\beta} - \beta) = (n^{-1}\sum_{i=1}^n \mathbf{x_i'x_i})^{-1} \sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i'\epsilon_i}).
\]</span></p>
<p>By the <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a>: <span class="math display">\[
\sqrt{n}(\hat{\beta} - \beta) \to^d N(0, \Sigma),
\]</span> where: <span class="math display">\[
\Sigma = (E(\mathbf{x_i'x_i}))^{-1} \mathbf{B} (E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p>
<p>The sandwich form is <span class="math inline">\(\Sigma\)</span> is standard.</p>
<p><strong>Implications for Homoskedasticity (A4) vs. Heteroskedasticity</strong></p>
<ol style="list-style-type: decimal">
<li>
<p>No Homoskedasticity (<a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a>) needed:</p>
<ul>
<li>the CLT and the large-sample distribution of <span class="math inline">\(\hat{\beta}\)</span> do <em>not</em> require homoskedasticity. The only place homoskedasticity would simplify things is that <span class="math display">\[
\mathbf{B} = Var(\mathbf{x_i'\epsilon_i}) = \sigma^2 E(\mathbf{x_i'x_i}),
\]</span>
</li>
</ul>
<p>only if <span class="math inline">\(Var(\epsilon_i | \mathbf{x}_i) \sigma^2\)</span></p>
<p>Then <span class="math display">\[
\Sigma = \sigma^2 (E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p>
</li>
<li>
<p>Adjusting for Heteroskedasticity:</p>
<ul>
<li>In practice, <span class="math inline">\(\sigma_i^2\)</span> can vary with <span class="math inline">\(\mathbf{x}_i\)</span>​, leading to heteroskedasticity.</li>
<li>The standard OLS formula for <span class="math inline">\(Var(\hat{\beta})\)</span> is inconsistent under heteroskedasticity, so one uses <em>robust (White) standard errors</em>.</li>
<li>Heteroskedasticity can arise from (but not limited to):
<ul>
<li>Limited dependent variables.</li>
<li>Dependent variables with large or skewed ranges.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="derivation-of-asymptotic-variance" class="section level5" number="5.1.2.4.2">
<h5>
<span class="header-section-number">5.1.2.4.2</span> Derivation of Asymptotic Variance<a class="anchor" aria-label="anchor" href="#derivation-of-asymptotic-variance"><i class="fas fa-link"></i></a>
</h5>
<p>The asymptotic variance of the OLS estimator is derived as follows:</p>
<p><span class="math display">\[
\Sigma = (E(\mathbf{x_i'x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Substituting <span class="math inline">\(\mathbf{B} = Var(\mathbf{x_i'}\epsilon_i)\)</span>: <span class="math display">\[
\Sigma = (E(\mathbf{x_i'x_i}))^{-1}Var(\mathbf{x_i'}\epsilon_i)(E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p></li>
<li><p>Using the definition of variance: <span class="math display">\[
\Sigma = (E(\mathbf{x_i'x_i}))^{-1}E[(\mathbf{x_i'}\epsilon_i - 0)(\mathbf{x_i'}\epsilon_i - 0)'](E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p></li>
<li><p>By the [Law of Iterated Expectations] and <a href="linear-regression.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a>, we have: <span class="math display">\[
E[(\mathbf{x_i'}\epsilon_i)(\mathbf{x_i'}\epsilon_i)'] = E[E(\epsilon_i^2|\mathbf{x_i})\mathbf{x_i'x_i}],
\]</span></p></li>
<li><p>Assuming homoskedasticity (<a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a>), <span class="math inline">\(E(\epsilon_i^2|\mathbf{x_i}) = \sigma^2\)</span>, so: <span class="math display">\[
\Sigma = (E(\mathbf{x_i'x_i}))^{-1}\sigma^2E(\mathbf{x_i'x_i})(E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p></li>
<li><p>Simplifying: <span class="math display">\[
\Sigma = \sigma^2(E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p></li>
</ol>
<p>Hence, under the assumptions:</p>
<ul>
<li><p><a href="linear-regression.html#a1-linearity">A1 Linearity</a></p></li>
<li><p><a href="linear-regression.html#a2-full-rank">A2 Full Rank</a></p></li>
<li><p><a href="linear-regression.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a></p></li>
<li><p><a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a></p></li>
<li><p><a href="linear-regression.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a></p></li>
</ul>
<p>we have<span class="math display">\[
\sqrt{n}(\hat{\beta} - \beta) \to^d N(0, \sigma^2(E(\mathbf{x_i'x_i}))^{-1}).
\]</span></p>
<p>The asymptotic variance provides an approximation of the scaled estimator’s variance for large <span class="math inline">\(n\)</span>. This leads to:</p>
<p><span class="math display">\[
Avar(\sqrt{n}(\hat{\beta} - \beta)) = \sigma^2(E(\mathbf{x_i'x_i}))^{-1}.
\]</span></p>
<p>The finite sample variance of an estimator can be approximated using the asymptotic variance for large sample sizes:</p>
<p><span class="math display">\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta)) &amp;\approx Var(\sqrt{n}(\hat{\beta}-\beta)) \\
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &amp;\approx Var(\sqrt{n}(\hat{\beta}-\beta))/n = Var(\hat{\beta})
\end{aligned}
\]</span></p>
<p>However, it is critical to note that <strong>asymptotic variance</strong> (<span class="math inline">\(Avar(.)\)</span>) does not behave in the same manner as finite sample variance (<span class="math inline">\(Var(.)\)</span>). This distinction is evident in the following expressions:</p>
<p><span class="math display">\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &amp;\neq Avar(\sqrt{n}(\hat{\beta}-\beta)/\sqrt{n}) \\
&amp;\neq Avar(\hat{\beta})
\end{aligned}
\]</span></p>
<p>Thus, while <span class="math inline">\(Avar(.)\)</span> provides a useful approximation for large samples, its conceptual properties differ from finite sample variance.</p>
<hr>
<p>In <a href="linear-regression.html#finite-sample-properties">Finite Sample Properties</a>, the standard errors are calculated as estimates of the conditional standard deviation:</p>
<p><span class="math display">\[
SE_{fs}(\hat{\beta}_{j-1}) = \sqrt{\hat{Var}(\hat{\beta}_{j-1}|\mathbf{X})} = \sqrt{s^2 \cdot [\mathbf{(X'X)}^{-1}]_{jj}},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(s^2\)</span> is the estimator of the error variance,</p></li>
<li><p><span class="math inline">\([\mathbf{(X'X)}^{-1}]_{jj}\)</span> represents the <span class="math inline">\(j\)</span>th diagonal element of the inverse design matrix.</p></li>
</ul>
<p>In contrast, in <a href="linear-regression.html#large-sample-properties">Large Sample Properties</a>, the standard errors are calculated as estimates of the square root of the asymptotic variance:</p>
<p><span class="math display">\[
SE_{ls}(\hat{\beta}_{j-1}) = \sqrt{\hat{Avar}(\sqrt{n} \hat{\beta}_{j-1})/n} = \sqrt{s^2 \cdot [\mathbf{(X'X)}^{-1}]_{jj}}.
\]</span></p>
<p>Interestingly, the standard error estimator is <strong>identical</strong> for both finite and large samples:</p>
<ul>
<li>The expressions for <span class="math inline">\(SE_{fs}\)</span> and <span class="math inline">\(SE_{ls}\)</span> are mathematically the same.</li>
<li>However, they are conceptually estimating two <strong>different quantities</strong>:
<ul>
<li>
<strong>Finite Sample Standard Error</strong>: An estimate of the conditional standard deviation of <span class="math inline">\(\hat{\beta}_{j-1}\)</span> given <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>
<strong>Large Sample Standard Error</strong>: An estimate of the square root of the asymptotic variance of <span class="math inline">\(\hat{\beta}_{j-1}\)</span>.</li>
</ul>
</li>
</ul>
<p>The assumptions required for these estimators to be valid differ in their stringency:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Finite Sample Variance (Conditional Variance):</strong>
<ul>
<li>Requires stronger assumptions (A1-A5).</li>
</ul>
</li>
<li>
<strong>Asymptotic Variance:</strong>
<ul>
<li>Valid under weaker assumptions (A1, A2, A3a, A4, A5).</li>
</ul>
</li>
</ol>
<p>This distinction highlights the utility of asymptotic properties in providing robust approximations when finite sample assumptions may not hold.</p>
</div>
</div>
<div id="diagnostics" class="section level4" number="5.1.2.5">
<h4>
<span class="header-section-number">5.1.2.5</span> Diagnostics<a class="anchor" aria-label="anchor" href="#diagnostics"><i class="fas fa-link"></i></a>
</h4>
<div id="normality-of-errors" class="section level5" number="5.1.2.5.1">
<h5>
<span class="header-section-number">5.1.2.5.1</span> Normality of Errors<a class="anchor" aria-label="anchor" href="#normality-of-errors"><i class="fas fa-link"></i></a>
</h5>
<p>Ensuring the normality of errors is a critical assumption in many regression models. Deviations from this assumption can impact inference and model interpretation. For diagnoses assessing normality, see <a href="descriptive-statistics.html#normality-assessment">Normality Assessment</a>.</p>
<p>Plots are invaluable for visual inspection of normality. One common approach is the Q-Q plot, which compares the quantiles of the residuals against those of a standard normal distribution:</p>
<div class="sourceCode" id="cb169"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example Q-Q plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> <span class="co"># For reproducibility</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span> <span class="co"># Generating random normal data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/qqnorm.html">qqplot</a></span><span class="op">(</span><span class="va">x</span>,</span>
<span>       <span class="va">y</span>,</span>
<span>       main <span class="op">=</span> <span class="st">"Q-Q Plot"</span>,</span>
<span>       xlab <span class="op">=</span> <span class="st">"Theoretical Quantiles"</span>,</span>
<span>       ylab <span class="op">=</span> <span class="st">"Sample Quantiles"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-5-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="influential-observations-and-outliers" class="section level5" number="5.1.2.5.2">
<h5>
<span class="header-section-number">5.1.2.5.2</span> Influential Observations and Outliers<a class="anchor" aria-label="anchor" href="#influential-observations-and-outliers"><i class="fas fa-link"></i></a>
</h5>
<p>Identifying influential observations or outliers is essential for robust regression modeling. The <strong>hat matrix</strong> (<span class="math inline">\(\mathbf{H}\)</span>) plays a key role in diagnosing influence.</p>
<div id="hat-matrix-outliers-in-x-space" class="section level6" number="5.1.2.5.2.1">
<h6>
<span class="header-section-number">5.1.2.5.2.1</span> Hat Matrix: Outliers in X-Space<a class="anchor" aria-label="anchor" href="#hat-matrix-outliers-in-x-space"><i class="fas fa-link"></i></a>
</h6>
<p>The hat matrix is primarily concerned with <strong>leverage</strong>, which reflects how far an observation’s predictor values (<span class="math inline">\(X\)</span>-space) are from the centroid of the predictor space.</p>
<ul>
<li><p><strong>What it measures:</strong> The diagonal elements of the hat matrix quantify <strong>leverage</strong>, not residual size or model fit. High leverage suggests that an observation has an unusual predictor configuration and might disproportionately influence the regression line, irrespective of the response variable.</p></li>
<li><p><strong>What it doesn’t measure:</strong> It doesn’t directly account for outliers in <span class="math inline">\(Y\)</span>-space or residuals.</p></li>
</ul>
<p>The hat matrix, defined as:</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}
\]</span></p>
<p>has the following properties:</p>
<ul>
<li><p><strong>Fitted Values</strong>: <span class="math inline">\(\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}\)</span>.</p></li>
<li><p><strong>Residuals</strong>: <span class="math inline">\(\mathbf{e} = (\mathbf{I} - \mathbf{H})\mathbf{Y}\)</span>.</p></li>
<li><p><strong>Variance of Residuals</strong>: <span class="math inline">\(\text{var}(\mathbf{e}) = \sigma^2 (\mathbf{I} - \mathbf{H})\)</span>.</p></li>
</ul>
<p>Diagonal Elements of the Hat Matrix (<span class="math inline">\(h_{ii}\)</span>)</p>
<ul>
<li><p><span class="math inline">\(h_{ii}\)</span> is the <span class="math inline">\(i\)</span>-th element on the main diagonal of <span class="math inline">\(\mathbf{H}\)</span>. It must satisfy <span class="math inline">\(0 \leq h_{ii} \leq 1\)</span>.</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^{n} h_{ii} = p\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters (including the intercept).</p></li>
<li><p>The variance of residuals for observation <span class="math inline">\(i\)</span> is given by <span class="math inline">\(\sigma^2(e_i) = \sigma^2 (1 - h_{ii})\)</span>.</p></li>
<li><p>The covariance between residuals for observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> (<span class="math inline">\(i \neq j\)</span>) is <span class="math inline">\(-h_{ij}\sigma^2\)</span>.</p></li>
</ul>
<p>For large datasets, the off-diagonal elements (<span class="math inline">\(h_{ij}\)</span>) tend to have small covariance if model assumptions hold.</p>
<p>Estimations Using MSE</p>
<ul>
<li><p>Variance of residuals: <span class="math inline">\(s^2(e_i) = MSE (1 - h_{ii})\)</span>, where <span class="math inline">\(MSE\)</span> is the mean squared error.</p></li>
<li><p>Covariance of residuals: <span class="math inline">\(\hat{\text{cov}}(e_i, e_j) = -h_{ij}(MSE)\)</span>.</p></li>
</ul>
<p>Interpretation of <span class="math inline">\(h_{ii}\)</span></p>
<p>If <span class="math inline">\(\mathbf{x}_i = [1, X_{i,1}, \ldots, X_{i,p-1}]'\)</span> represents the vector of predictor values for observation <span class="math inline">\(i\)</span>, then:</p>
<p><span class="math display">\[
h_{ii} = \mathbf{x}_i' (\mathbf{X}'\mathbf{X})^{-1} \mathbf{x}_i
\]</span></p>
<p>The value of <span class="math inline">\(h_{ii}\)</span> depends on the relative positions of the design points <span class="math inline">\(X_{i,1}, \ldots, X_{i,p-1}\)</span>. Observations with high <span class="math inline">\(h_{ii}\)</span> are more influential and warrant closer inspection for leverage or outlier behavior.</p>
</div>
<div id="studentized-residuals-outliers-in-y-space" class="section level6" number="5.1.2.5.2.2">
<h6>
<span class="header-section-number">5.1.2.5.2.2</span> Studentized Residuals: Outliers in Y-Space<a class="anchor" aria-label="anchor" href="#studentized-residuals-outliers-in-y-space"><i class="fas fa-link"></i></a>
</h6>
<p>Residuals focus on discrepancies between observed (<span class="math inline">\(Y\)</span>) and predicted (<span class="math inline">\(\hat{Y}\)</span>) values, helping to identify outliers in <span class="math inline">\(Y\)</span>-space.</p>
<ul>
<li>
<p><strong>What they measure:</strong></p>
<ul>
<li><p>Standardized or studentized residuals assess how far an observation’s response is from the regression line, adjusted for variance.</p></li>
<li><p><strong>Externally studentized residuals</strong> are more robust because they exclude the <span class="math inline">\(i\)</span>-th observation when estimating variance.</p></li>
<li><p>Large studentized residuals (e.g., <span class="math inline">\(&gt;2\)</span> or <span class="math inline">\(&gt;3\)</span>) indicate observations that are unusual in <span class="math inline">\(Y\)</span>-space.</p></li>
</ul>
</li>
<li><p><strong>What they don’t measure:</strong> They do not consider leverage or the <span class="math inline">\(X\)</span>-space configuration. A point with a large residual could have low leverage, making it less influential overall.</p></li>
</ul>
<p>Studentized residuals, also known as standardized residuals, adjust for the variance of residuals by dividing the residuals by their standard error:</p>
<p><span class="math display">\[
\begin{aligned}
r_i &amp;= \frac{e_i}{s(e_i)} \\
r_i &amp;\sim N(0,1),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(s(e_i) = \sqrt{MSE(1-h_{ii})}\)</span>, and <span class="math inline">\(r_i\)</span> accounts for the varying variances of residuals. These residuals allow for a better comparison of model fit across observations.</p>
<ul>
<li>
<strong>Semi-Studentized Residuals</strong>: In contrast, the semi-studentized residuals are defined as:</li>
</ul>
<p><span class="math display">\[
e_i^* = \frac{e_i}{\sqrt{MSE}}
\]</span></p>
<p>This approach does not adjust for the heterogeneity in variances of residuals, as <span class="math inline">\(e_i^*\)</span> assumes equal variance for all residuals.</p>
<hr>
<p>To assess the influence of individual observations, we consider the model without a particular value. When the <span class="math inline">\(i\)</span>-th observation is removed, the <strong>deleted residual</strong> is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
d_i &amp;= Y_i - \hat{Y}_{i(i)} \\
    &amp;= \frac{e_i}{1-h_{ii}},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the actual observation, and <span class="math inline">\(\hat{Y}_{i(i)}\)</span> is the predicted value for the <span class="math inline">\(i\)</span>-th observation, computed using the regression model fitted to the remaining <span class="math inline">\(n-1\)</span> observations. Importantly, we do not need to refit the regression model for each observation to compute <span class="math inline">\(d_i\)</span>.</p>
<ul>
<li>As <span class="math inline">\(h_{ii}\)</span> (leverage) increases, <span class="math inline">\(d_i\)</span> also increases, indicating higher influence of the observation.</li>
</ul>
<p>The variance of the deleted residual is given by:</p>
<p><span class="math display">\[
s^2(d_i) = \frac{MSE_{(i)}}{1-h_{ii}},
\]</span></p>
<p>where <span class="math inline">\(MSE_{(i)}\)</span> is the mean squared error when the <span class="math inline">\(i\)</span>-th case is omitted.</p>
<hr>
<p>The <strong>studentized deleted residual</strong> accounts for variability and follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-p-1\)</span> degrees of freedom:</p>
<p><span class="math display">\[
t_i = \frac{d_i}{s(d_i)} = \frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}},
\]</span></p>
<p>where <span class="math inline">\(t_i\)</span> helps identify outliers more effectively.</p>
<p>We can compute <span class="math inline">\(t_i\)</span> without fitting the regression model multiple times. Using the relationship:</p>
<p><span class="math display">\[
(n-p)MSE = (n-p-1)MSE_{(i)} + \frac{e_i^2}{1-h_{ii}},
\]</span></p>
<p>we derive:</p>
<p><span class="math display">\[
t_i = e_i \sqrt{\frac{n-p-1}{SSE(1-h_{ii}) - e_i^2}}.
\]</span></p>
<p>This formulation avoids the need for recalculating <span class="math inline">\(MSE_{(i)}\)</span> explicitly for each case.</p>
<hr>
<p>Outlying <span class="math inline">\(Y\)</span>-observations are those with large studentized deleted residuals in absolute value. To handle multiple testing when there are many residuals, we use a Bonferroni-adjusted critical value:</p>
<p><span class="math display">\[
t_{1-\alpha/2n; n-p-1},
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the desired significance level, <span class="math inline">\(n\)</span> is the sample size, and <span class="math inline">\(p\)</span> is the number of parameters in the model. Observations exceeding this threshold are flagged as potential outliers.</p>
<div class="sourceCode" id="cb170"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example R Code for Demonstrating Residual Diagnostics</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> <span class="co"># For reproducibility</span></span>
<span></span>
<span><span class="co"># Simulate some data</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">10</span>, sd <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Extract residuals, fitted values, and hat values</span></span>
<span><span class="va">residuals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span><span class="va">hat_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.influence.html">lm.influence</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">hat</span></span>
<span><span class="va">mse</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">residuals</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">residuals</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">)</span> <span class="co"># Number of parameters</span></span>
<span></span>
<span><span class="co"># Compute studentized residuals</span></span>
<span><span class="va">studentized_residuals</span> <span class="op">&lt;-</span> <span class="va">residuals</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">mse</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">hat_values</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute deleted residuals</span></span>
<span><span class="va">deleted_residuals</span> <span class="op">&lt;-</span> <span class="va">residuals</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">hat_values</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute studentized deleted residuals</span></span>
<span><span class="va">studentized_deleted_residuals</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">residuals</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">p</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span>                     <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">residuals</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">hat_values</span><span class="op">)</span> <span class="op">-</span> <span class="va">residuals</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Flag potential outliers using Bonferroni-adjusted critical value</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span></span>
<span><span class="va">bonferroni_threshold</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">n</span><span class="op">)</span>, df <span class="op">=</span> <span class="va">n</span> <span class="op">-</span> <span class="va">p</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">outliers</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">studentized_deleted_residuals</span><span class="op">)</span> <span class="op">&gt;</span> <span class="va">bonferroni_threshold</span></span>
<span></span>
<span><span class="co"># Print results</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Residuals                     <span class="op">=</span> <span class="va">residuals</span>,</span>
<span>    Hat_Values                    <span class="op">=</span> <span class="va">hat_values</span>,</span>
<span>    Studentized_Residuals         <span class="op">=</span> <span class="va">studentized_residuals</span>,</span>
<span>    Deleted_Residuals             <span class="op">=</span> <span class="va">deleted_residuals</span>,</span>
<span>    Studentized_Deleted_Residuals <span class="op">=</span> <span class="va">studentized_deleted_residuals</span>,</span>
<span>    Outlier                       <span class="op">=</span> <span class="va">outliers</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu">causalverse</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/causalverse/man/nice_tab.html">nice_tab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;   Residuals Hat_Values Studentized_Residuals Deleted_Residuals</span></span>
<span><span class="co">#&gt; 1     -1.27       0.02                 -0.67             -1.29</span></span>
<span><span class="co">#&gt; 2      0.70       0.01                  0.36              0.70</span></span>
<span><span class="co">#&gt; 3     -0.12       0.04                 -0.07             -0.13</span></span>
<span><span class="co">#&gt; 4     -0.48       0.01                 -0.25             -0.49</span></span>
<span><span class="co">#&gt; 5     -1.68       0.01                 -0.88             -1.70</span></span>
<span><span class="co">#&gt; 6      0.30       0.04                  0.16              0.31</span></span>
<span><span class="co">#&gt;   Studentized_Deleted_Residuals Outlier</span></span>
<span><span class="co">#&gt; 1                         -0.66   FALSE</span></span>
<span><span class="co">#&gt; 2                          0.36   FALSE</span></span>
<span><span class="co">#&gt; 3                         -0.06   FALSE</span></span>
<span><span class="co">#&gt; 4                         -0.25   FALSE</span></span>
<span><span class="co">#&gt; 5                         -0.87   FALSE</span></span>
<span><span class="co">#&gt; 6                          0.15   FALSE</span></span>
<span></span>
<span><span class="co"># Plot studentized deleted residuals for visualization</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="va">studentized_deleted_residuals</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Studentized Deleted Residuals"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Observation"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Studentized Deleted Residuals"</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>    col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">outliers</span>, <span class="st">"red"</span>, <span class="st">"black"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span></span>
<span>    h <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="va">bonferroni_threshold</span>, <span class="va">bonferroni_threshold</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>    lty <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>    <span class="st">"topright"</span>,</span>
<span>    legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Potential Outliers"</span>, <span class="st">"Threshold"</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span>,</span>
<span>    pch <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">16</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>    lty <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="identifying-influential-cases" class="section level5" number="5.1.2.5.3">
<h5>
<span class="header-section-number">5.1.2.5.3</span> Identifying Influential Cases<a class="anchor" aria-label="anchor" href="#identifying-influential-cases"><i class="fas fa-link"></i></a>
</h5>
<p>By <strong>influential</strong>, we refer to observations whose exclusion causes major changes in the fitted regression model. Note that not all outliers are influential.</p>
<p>Types of Influence Measures</p>
<ul>
<li>
<strong>Influence on Single Fitted Values</strong>: <a href="linear-regression.html#dffits">DFFITS</a>
</li>
<li>
<strong>Influence on All Fitted Values</strong>: <a href="linear-regression.html#cooks-d">Cook’s D</a>
</li>
<li>
<strong>Influence on the Regression Coefficients</strong>: <a href="linear-regression.html#dfbetas">DFBETAS</a>
</li>
</ul>
<p>Measures like Cook’s D, DFFITS, and DFBETAS combine leverage (from the hat matrix) and residual size (from studentized residuals) to assess the <strong>influence</strong> of an observation on the model as a whole. Hence, these effectively combine impact of <span class="math inline">\(X\)</span>-space and <span class="math inline">\(Y\)</span>-space.</p>
<div id="dffits" class="section level6" number="5.1.2.5.3.1">
<h6>
<span class="header-section-number">5.1.2.5.3.1</span> DFFITS<a class="anchor" aria-label="anchor" href="#dffits"><i class="fas fa-link"></i></a>
</h6>
<p>DFFITS measures the <strong>influence on single fitted values</strong>. It is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
(DFFITS)_i &amp;= \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)}h_{ii}}} \\
&amp;= t_i \left(\frac{h_{ii}}{1-h_{ii}}\right)^{1/2}
\end{aligned}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\hat{Y}_i\)</span> = fitted value for observation <span class="math inline">\(i\)</span> using all data.</li>
<li>
<span class="math inline">\(\hat{Y}_{i(i)}\)</span> = fitted value for observation <span class="math inline">\(i\)</span> with the <span class="math inline">\(i\)</span>th case removed.</li>
<li>
<span class="math inline">\(MSE_{(i)}\)</span> = mean squared error with observation <span class="math inline">\(i\)</span> excluded.</li>
<li>
<span class="math inline">\(h_{ii}\)</span> = leverage of the <span class="math inline">\(i\)</span>th observation.</li>
<li>
<span class="math inline">\(t_i\)</span> = studentized deleted residual.</li>
</ul>
<p>High DFFITS values occur when leverage and residuals are jointly significant.</p>
<p>DFFITS captures the <strong>standardized difference between the fitted value for observation</strong> <span class="math inline">\(i\)</span> with and without the <span class="math inline">\(i\)</span>th case in the model. It is a product of:</p>
<ol style="list-style-type: decimal">
<li>The <strong>studentized deleted residual</strong>.</li>
<li>A scaling factor based on the leverage of the <span class="math inline">\(i\)</span>th observation, <span class="math inline">\(h_{ii}\)</span>.</li>
</ol>
<p>An observation is considered influential based on DFFITS if:</p>
<ul>
<li>
<strong>Small to medium data sets</strong>: <span class="math inline">\(|DFFITS| &gt; 1\)</span>
</li>
<li>
<strong>Large data sets</strong>: <span class="math inline">\(|DFFITS| &gt; 2 \sqrt{p/n}\)</span>
</li>
</ul>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(p\)</span> = number of predictors (including the intercept).</li>
<li>
<span class="math inline">\(n\)</span> = total number of observations.</li>
</ul>
<p>This provides a practical threshold for detecting influential observations in different dataset sizes.</p>
<div class="sourceCode" id="cb171"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute DFFITS</span></span>
<span><span class="va">dffits_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">dffits</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display influential observations based on the threshold for a large dataset</span></span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">influential_obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">dffits_values</span><span class="op">)</span> <span class="op">&gt;</span> <span class="va">threshold</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>    DFFITS                   <span class="op">=</span> <span class="va">dffits_values</span>,</span>
<span>    Threshold                <span class="op">=</span> <span class="va">threshold</span>,</span>
<span>    Influential_Observations <span class="op">=</span> <span class="va">influential_obs</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $DFFITS</span></span>
<span><span class="co">#&gt;           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive </span></span>
<span><span class="co">#&gt;        -0.218494101        -0.126664789        -0.249103400         0.011699160 </span></span>
<span><span class="co">#&gt;   Hornet Sportabout             Valiant          Duster 360           Merc 240D </span></span>
<span><span class="co">#&gt;         0.028162679        -0.253806124        -0.191618944         0.221917842 </span></span>
<span><span class="co">#&gt;            Merc 230            Merc 280           Merc 280C          Merc 450SE </span></span>
<span><span class="co">#&gt;         0.079763706        -0.067222732        -0.190099538         0.064280875 </span></span>
<span><span class="co">#&gt;          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental </span></span>
<span><span class="co">#&gt;         0.020560728        -0.135714533         0.008984366         0.227919348 </span></span>
<span><span class="co">#&gt;   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla </span></span>
<span><span class="co">#&gt;         1.231668760         0.749153703         0.165329646         0.865985851 </span></span>
<span><span class="co">#&gt;       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 </span></span>
<span><span class="co">#&gt;        -0.292008465        -0.253389811        -0.294709853        -0.170476763 </span></span>
<span><span class="co">#&gt;    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa </span></span>
<span><span class="co">#&gt;         0.207813200        -0.041423665        -0.004054382         0.471518032 </span></span>
<span><span class="co">#&gt;      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E </span></span>
<span><span class="co">#&gt;        -0.161026362        -0.129395315         0.907521354        -0.128232538 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Threshold</span></span>
<span><span class="co">#&gt; [1] 0.6123724</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Influential_Observations</span></span>
<span><span class="co">#&gt; Chrysler Imperial          Fiat 128    Toyota Corolla     Maserati Bora </span></span>
<span><span class="co">#&gt;                17                18                20                31</span></span></code></pre></div>
</div>
<div id="cooks-d" class="section level6" number="5.1.2.5.3.2">
<h6>
<span class="header-section-number">5.1.2.5.3.2</span> Cook’s D<a class="anchor" aria-label="anchor" href="#cooks-d"><i class="fas fa-link"></i></a>
</h6>
<p><strong>Cook’s D</strong> measures the <strong>influence of the</strong> <span class="math inline">\(i\)</span>th case on all fitted values in a regression model. It is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
D_i &amp;= \frac{\sum_{j=1}^{n}(\hat{Y}_j - \hat{Y}_{j(i)})^2}{p(MSE)} \\
&amp;= \frac{e^2_i}{p(MSE)}\left(\frac{h_{ii}}{(1-h_{ii})^2}\right)
\end{aligned}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\hat{Y}_j\)</span> = fitted value for observation <span class="math inline">\(j\)</span> using all data.</li>
<li>
<span class="math inline">\(\hat{Y}_{j(i)}\)</span> = fitted value for observation <span class="math inline">\(j\)</span> with the <span class="math inline">\(i\)</span>th case removed.</li>
<li>
<span class="math inline">\(e_i\)</span> = residual for observation <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(h_{ii}\)</span> = leverage of the <span class="math inline">\(i\)</span>th observation.</li>
<li>
<span class="math inline">\(p\)</span> = number of predictors (including the intercept).</li>
<li>
<span class="math inline">\(MSE\)</span> = mean squared error of the model.</li>
</ul>
<p>Key Insights</p>
<ul>
<li>
<strong>Cook’s D quantifies the overall influence</strong> of the <span class="math inline">\(i\)</span>th observation on the entire set of fitted values.</li>
<li>If either the residual <span class="math inline">\(e_i\)</span> increases or the leverage <span class="math inline">\(h_{ii}\)</span> increases, then <span class="math inline">\(D_i\)</span> also increases, indicating higher influence.</li>
<li>Observations with both high leverage and large residuals are flagged as influential.</li>
</ul>
<p>Threshold for Influence</p>
<ul>
<li>
<span class="math inline">\(D_i\)</span> can be interpreted as a percentile of an <span class="math inline">\(F_{(p,n-p)}\)</span> distribution.</li>
<li>Practical thresholds:
<ul>
<li>
<strong>If</strong> <span class="math inline">\(D_i &gt; 4/n\)</span>, the <span class="math inline">\(i\)</span>th case has major influence.</li>
<li>Alternatively, cases where <span class="math inline">\(D_i\)</span> exceeds the 50th percentile of the <span class="math inline">\(F\)</span>-distribution may also be considered influential.</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb172"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Cook's D</span></span>
<span><span class="va">cooks_d_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">cooks.distance</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display influential observations based on the threshold</span></span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">4</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span></span>
<span><span class="va">influential_obs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">cooks_d_values</span> <span class="op">&gt;</span> <span class="va">threshold</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>  Cooks_D                  <span class="op">=</span> <span class="va">cooks_d_values</span>,</span>
<span>  Threshold                <span class="op">=</span> <span class="va">threshold</span>,</span>
<span>  Influential_Observations <span class="op">=</span> <span class="va">influential_obs</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $Cooks_D</span></span>
<span><span class="co">#&gt;           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive </span></span>
<span><span class="co">#&gt;        1.589652e-02        5.464779e-03        2.070651e-02        4.724822e-05 </span></span>
<span><span class="co">#&gt;   Hornet Sportabout             Valiant          Duster 360           Merc 240D </span></span>
<span><span class="co">#&gt;        2.736184e-04        2.155064e-02        1.255218e-02        1.677650e-02 </span></span>
<span><span class="co">#&gt;            Merc 230            Merc 280           Merc 280C          Merc 450SE </span></span>
<span><span class="co">#&gt;        2.188702e-03        1.554996e-03        1.215737e-02        1.423008e-03 </span></span>
<span><span class="co">#&gt;          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental </span></span>
<span><span class="co">#&gt;        1.458960e-04        6.266049e-03        2.786686e-05        1.780910e-02 </span></span>
<span><span class="co">#&gt;   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla </span></span>
<span><span class="co">#&gt;        4.236109e-01        1.574263e-01        9.371446e-03        2.083933e-01 </span></span>
<span><span class="co">#&gt;       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 </span></span>
<span><span class="co">#&gt;        2.791982e-02        2.087419e-02        2.751510e-02        9.943527e-03 </span></span>
<span><span class="co">#&gt;    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa </span></span>
<span><span class="co">#&gt;        1.443199e-02        5.920440e-04        5.674986e-06        7.353985e-02 </span></span>
<span><span class="co">#&gt;      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E </span></span>
<span><span class="co">#&gt;        8.919701e-03        5.732672e-03        2.720397e-01        5.600804e-03 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Threshold</span></span>
<span><span class="co">#&gt; [1] 0.125</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Influential_Observations</span></span>
<span><span class="co">#&gt; Chrysler Imperial          Fiat 128    Toyota Corolla     Maserati Bora </span></span>
<span><span class="co">#&gt;                17                18                20                31</span></span></code></pre></div>
</div>
<div id="dfbetas" class="section level6" number="5.1.2.5.3.3">
<h6>
<span class="header-section-number">5.1.2.5.3.3</span> DFBETAS<a class="anchor" aria-label="anchor" href="#dfbetas"><i class="fas fa-link"></i></a>
</h6>
<p><strong>DFBETAS</strong> measures the <strong>influence of the</strong> <span class="math inline">\(i\)</span>th observation on each regression coefficient in a regression model. It is defined as:</p>
<p><span class="math display">\[
(DFBETAS)_{k(i)} = \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)}c_{kk}}}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(b_k\)</span> = regression coefficient for the <span class="math inline">\(k\)</span>th predictor using all observations.</li>
<li>
<span class="math inline">\(b_{k(i)}\)</span> = regression coefficient for the <span class="math inline">\(k\)</span>th predictor with the <span class="math inline">\(i\)</span>th observation omitted.</li>
<li>
<span class="math inline">\(MSE_{(i)}\)</span> = mean squared error with the <span class="math inline">\(i\)</span>th observation excluded.</li>
<li>
<span class="math inline">\(c_{kk}\)</span> = <span class="math inline">\(k\)</span>th diagonal element of <span class="math inline">\(\mathbf{X'X}^{-1}\)</span>, representing the variance of <span class="math inline">\(b_k\)</span>.</li>
</ul>
<p>Key Insights</p>
<ul>
<li>
<strong>DFBETAS quantifies the change in each regression coefficient (</strong><span class="math inline">\(b_k\)</span>) caused by omitting the <span class="math inline">\(i\)</span>th observation.</li>
<li>The sign of DFBETAS indicates whether the inclusion of an observation increases or decreases the regression coefficient.
<ul>
<li>Positive DFBETAS: Inclusion increases <span class="math inline">\(b_k\)</span>.</li>
<li>Negative DFBETAS: Inclusion decreases <span class="math inline">\(b_k\)</span>.</li>
</ul>
</li>
<li>High DFBETAS indicate that a single observation disproportionately affects one or more predictors.</li>
</ul>
<p>The thresholds for identifying influential observations based on DFBETAS are:</p>
<ul>
<li>
<strong>Small data sets</strong>: <span class="math inline">\(|DFBETAS| &gt; 1\)</span>
</li>
<li>
<strong>Large data sets</strong>: <span class="math inline">\(|DFBETAS| &gt; 2 / \sqrt{n}\)</span>
</li>
</ul>
<p>Where <span class="math inline">\(n\)</span> is the total number of observations.</p>
<div class="sourceCode" id="cb173"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute DFBETAS</span></span>
<span><span class="va">dfbetas_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">dfbetas</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display influential observations based on the threshold for each predictor</span></span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">influential_obs</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">dfbetas_values</span>, <span class="fl">2</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">&gt;</span> <span class="va">threshold</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>    DFBETAS                  <span class="op">=</span> <span class="va">dfbetas_values</span>,</span>
<span>    Threshold                <span class="op">=</span> <span class="va">threshold</span>,</span>
<span>    Influential_Observations <span class="op">=</span> <span class="va">influential_obs</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $DFBETAS</span></span>
<span><span class="co">#&gt;                      (Intercept)           hp            wt</span></span>
<span><span class="co">#&gt; Mazda RX4           -0.161347204  0.032966471  0.0639304305</span></span>
<span><span class="co">#&gt; Mazda RX4 Wag       -0.069324050  0.045785122 -0.0004066495</span></span>
<span><span class="co">#&gt; Datsun 710          -0.211199646  0.043374926  0.0972314374</span></span>
<span><span class="co">#&gt; Hornet 4 Drive       0.002672687 -0.006839301  0.0044886906</span></span>
<span><span class="co">#&gt; Hornet Sportabout    0.001784844  0.009208434 -0.0015536931</span></span>
<span><span class="co">#&gt; Valiant             -0.005985946  0.180374447 -0.1516565139</span></span>
<span><span class="co">#&gt; Duster 360           0.004705177 -0.159988770  0.0781031774</span></span>
<span><span class="co">#&gt; Merc 240D            0.034255292 -0.189552940  0.1224118752</span></span>
<span><span class="co">#&gt; Merc 230             0.019788247 -0.055075623  0.0332570461</span></span>
<span><span class="co">#&gt; Merc 280            -0.003198686  0.036709039 -0.0337297820</span></span>
<span><span class="co">#&gt; Merc 280C           -0.009045583  0.103809696 -0.0953846390</span></span>
<span><span class="co">#&gt; Merc 450SE          -0.026973686 -0.005712458  0.0356973740</span></span>
<span><span class="co">#&gt; Merc 450SL          -0.003961562  0.003399822  0.0049302300</span></span>
<span><span class="co">#&gt; Merc 450SLC          0.031572445 -0.016800308 -0.0400515832</span></span>
<span><span class="co">#&gt; Cadillac Fleetwood  -0.006420656 -0.002577897  0.0075499557</span></span>
<span><span class="co">#&gt; Lincoln Continental -0.168791258 -0.058242601  0.1903129995</span></span>
<span><span class="co">#&gt; Chrysler Imperial   -0.924056752 -0.148009806  0.9355996760</span></span>
<span><span class="co">#&gt; Fiat 128             0.605181396 -0.311246566 -0.1672758566</span></span>
<span><span class="co">#&gt; Honda Civic          0.156388333 -0.034026915 -0.0819144214</span></span>
<span><span class="co">#&gt; Toyota Corolla       0.804669969 -0.170934240 -0.4114605894</span></span>
<span><span class="co">#&gt; Toyota Corona       -0.231328587  0.066064464  0.0882138248</span></span>
<span><span class="co">#&gt; Dodge Challenger     0.003923967  0.049775308 -0.0888481611</span></span>
<span><span class="co">#&gt; AMC Javelin         -0.019610048  0.037837437 -0.0734203131</span></span>
<span><span class="co">#&gt; Camaro Z28           0.029920076 -0.128670440  0.0390740055</span></span>
<span><span class="co">#&gt; Pontiac Firebird    -0.058806962 -0.002278294  0.0868742949</span></span>
<span><span class="co">#&gt; Fiat X1-9           -0.037559007  0.010208853  0.0174261386</span></span>
<span><span class="co">#&gt; Porsche 914-2       -0.003655931  0.000316321  0.0020588013</span></span>
<span><span class="co">#&gt; Lotus Europa         0.423409344  0.188396749 -0.4072338373</span></span>
<span><span class="co">#&gt; Ford Pantera L      -0.022536462 -0.148176049  0.0999346699</span></span>
<span><span class="co">#&gt; Ferrari Dino        -0.065508308 -0.085182962  0.0869804902</span></span>
<span><span class="co">#&gt; Maserati Bora       -0.007482815  0.865763737 -0.4999048760</span></span>
<span><span class="co">#&gt; Volvo 142E          -0.080001907  0.038406565  0.0127537553</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Threshold</span></span>
<span><span class="co">#&gt; [1] 0.3535534</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Influential_Observations</span></span>
<span><span class="co">#&gt; $Influential_Observations$`(Intercept)`</span></span>
<span><span class="co">#&gt; Chrysler Imperial          Fiat 128    Toyota Corolla      Lotus Europa </span></span>
<span><span class="co">#&gt;                17                18                20                28 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Influential_Observations$hp</span></span>
<span><span class="co">#&gt; Maserati Bora </span></span>
<span><span class="co">#&gt;            31 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Influential_Observations$wt</span></span>
<span><span class="co">#&gt; Chrysler Imperial    Toyota Corolla      Lotus Europa     Maserati Bora </span></span>
<span><span class="co">#&gt;                17                20                28                31</span></span></code></pre></div>
</div>
</div>
<div id="collinearity" class="section level5" number="5.1.2.5.4">
<h5>
<span class="header-section-number">5.1.2.5.4</span> Collinearity<a class="anchor" aria-label="anchor" href="#collinearity"><i class="fas fa-link"></i></a>
</h5>
<p><strong>Collinearity</strong> (or multicollinearity) refers to the correlation among explanatory variables in a regression model. It can lead to various issues, including:</p>
<ul>
<li>Large changes in the estimated regression coefficients when a predictor variable is added or removed, or when observations are altered.</li>
<li>Non-significant results for individual tests on regression coefficients of important predictor variables.</li>
<li>Regression coefficients with signs opposite to theoretical expectations or prior experience.</li>
<li>Large coefficients of simple correlation between pairs of predictor variables in the correlation matrix.</li>
<li>Wide confidence intervals for the regression coefficients representing important predictor variables.</li>
</ul>
<p>When some <span class="math inline">\(X\)</span> variables are highly correlated, the inverse <span class="math inline">\((X'X)^{-1}\)</span> either does not exist or is computationally unstable. This can result in:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Non-interpretability of parameters</strong>: <span class="math display">\[\mathbf{b = (X'X)^{-1}X'y}\]</span>
</li>
<li>
<strong>Infinite sampling variability</strong>: <span class="math display">\[\mathbf{s^2(b) = MSE (X'X)^{-1}}\]</span>
</li>
</ol>
<p>If some predictor variables (<span class="math inline">\(X\)</span>) are “perfectly” correlated, the system becomes undetermined, leading to an infinite number of models that fit the data. Specifically:</p>
<ul>
<li>If <span class="math inline">\(X'X\)</span> is singular, then <span class="math inline">\((X'X)^{-1}\)</span> does not exist.</li>
<li>This results in poor parameter estimation and invalid statistical inference.</li>
</ul>
<hr>
<div id="variance-inflation-factors-vifs" class="section level6" number="5.1.2.5.4.1">
<h6>
<span class="header-section-number">5.1.2.5.4.1</span> Variance Inflation Factors (VIFs)<a class="anchor" aria-label="anchor" href="#variance-inflation-factors-vifs"><i class="fas fa-link"></i></a>
</h6>
<p>The <strong>Variance Inflation Factor (VIF)</strong> quantifies how much the variance of a regression coefficient is inflated due to multicollinearity. It is defined as:</p>
<p><span class="math display">\[
VIF_k = \frac{1}{1-R^2_k}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(R^2_k\)</span> is the coefficient of multiple determination when <span class="math inline">\(X_k\)</span> is regressed on the other <span class="math inline">\(p-2\)</span> predictor variables in the model.</li>
</ul>
<hr>
<p><strong>Interpretation of VIFs</strong></p>
<ul>
<li>Large <span class="math inline">\(VIF_k\)</span> values indicate that near collinearity is inflating the variance of <span class="math inline">\(b_k\)</span>. The relationship is given by: <span class="math display">\[ var(b_k) \propto \sigma^2 (VIF_k) \]</span>
</li>
<li>
<strong>Thresholds</strong>:
<ul>
<li>
<span class="math inline">\(VIF &gt; 4\)</span>: Investigate the cause of multicollinearity.</li>
<li>
<span class="math inline">\(VIF_k &gt; 10\)</span>: Serious multicollinearity problem that can lead to poor parameter estimates.</li>
</ul>
</li>
<li>The <strong>mean VIF</strong> provides an estimate of the degree of multicollinearity:
<ul>
<li>If <span class="math inline">\(avg(VIF) &gt;&gt; 1\)</span>, serious multicollinearity is present.</li>
</ul>
</li>
<li>
<strong>Multicollinearity and VIF:</strong>
<ul>
<li><p>High VIFs with indicator variables are normal and not problematic.</p></li>
<li><p>VIF is generally not useful for detecting multicollinearity concerns in models with fixed effects.</p></li>
</ul>
</li>
<li>
<strong>Overemphasis on Multicollinearity:</strong>
<ul>
<li><p>Multicollinearity inflates standard errors and widens confidence intervals but does not bias results.</p></li>
<li><p>If key variables have narrow confidence intervals, multicollinearity is not an issue.</p></li>
</ul>
</li>
<li>
<strong>Goldberger’s Insight</strong> <span class="citation">(<a href="references.html#ref-goldberger1991course">Goldberger 1991</a>)</span><strong>:</strong>
<ul>
<li><p>Multicollinearity is akin to small sample size (“micronumerosity”).</p></li>
<li><p>Large standard errors are expected with highly correlated independent variables.</p></li>
</ul>
</li>
<li>
<strong>Practical Implications:</strong>
<ul>
<li><p>Evaluate whether confidence intervals for key variables are sufficiently narrow.</p></li>
<li><p>If not, the study is inconclusive, and a larger dataset or redesigned study is needed.</p></li>
</ul>
</li>
</ul>
<hr>
<div class="sourceCode" id="cb174"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span> <span class="op">+</span> <span class="va">disp</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Variance Inflation Factors</span></span>
<span><span class="va">vif_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Check for high multicollinearity</span></span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">high_vif</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">vif_values</span> <span class="op">&gt;</span> <span class="va">threshold</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>  VIFs <span class="op">=</span> <span class="va">vif_values</span>,</span>
<span>  High_VIF_Threshold <span class="op">=</span> <span class="va">threshold</span>,</span>
<span>  High_VIF_Indices <span class="op">=</span> <span class="va">high_vif</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $VIFs</span></span>
<span><span class="co">#&gt;       hp       wt     disp </span></span>
<span><span class="co">#&gt; 2.736633 4.844618 7.324517 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $High_VIF_Threshold</span></span>
<span><span class="co">#&gt; [1] 10</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $High_VIF_Indices</span></span>
<span><span class="co">#&gt; named integer(0)</span></span></code></pre></div>
</div>
<div id="condition-number" class="section level6" number="5.1.2.5.4.2">
<h6>
<span class="header-section-number">5.1.2.5.4.2</span> Condition Number<a class="anchor" aria-label="anchor" href="#condition-number"><i class="fas fa-link"></i></a>
</h6>
<p><strong>Condition Number</strong> is a diagnostic measure for detecting multicollinearity, derived from the spectral decomposition of the matrix <span class="math inline">\(\mathbf{X'X}\)</span>.</p>
<p>The spectral decomposition of <span class="math inline">\(\mathbf{X'X}\)</span> is:</p>
<p><span class="math display">\[
\mathbf{X'X}= \sum_{i=1}^{p} \lambda_i \mathbf{u_i u_i'}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\lambda_i\)</span>: Eigenvalue associated with the <span class="math inline">\(i\)</span>th eigenvector.</li>
<li>
<span class="math inline">\(\mathbf{u}_i\)</span>: Eigenvector associated with <span class="math inline">\(\lambda_i\)</span>.</li>
<li>
<span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \dots &gt; \lambda_p\)</span> (ordered eigenvalues).</li>
<li>The eigenvectors are orthogonal: <span class="math display">\[
\begin{cases}
\mathbf{u_i'u_j} = 0 &amp; \text{for } i \neq j \\
\mathbf{u_i'u_j} = 1 &amp; \text{for } i = j
\end{cases}
\]</span>
</li>
</ul>
<hr>
<p>Definition of the Condition Number</p>
<p>The <strong>Condition Number</strong> is defined as:</p>
<p><span class="math display">\[
k = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\lambda_{\text{max}}\)</span>: Largest eigenvalue.</li>
<li>
<span class="math inline">\(\lambda_{\text{min}}\)</span>: Smallest eigenvalue.</li>
</ul>
<p>Interpretation</p>
<ul>
<li>
<span class="math inline">\(k &gt; 30\)</span>: Cause for concern.</li>
<li>
<span class="math inline">\(30 &lt; k &lt; 100\)</span>: Moderate dependencies among predictors.</li>
<li>
<span class="math inline">\(k &gt; 100\)</span>: Strong collinearity, indicating serious multicollinearity.</li>
</ul>
<hr>
<p>The <strong>Condition Index</strong> for the <span class="math inline">\(i\)</span>th eigenvalue is defined as:</p>
<p><span class="math display">\[
\delta_i = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_i}}
\]</span></p>
<p>Where <span class="math inline">\(i = 1, \dots, p\)</span>.</p>
<hr>
<p><strong>Variance proportions</strong> can be used to identify collinearity issues. The proportion of total variance associated with the <span class="math inline">\(k\)</span>th regression coefficient and the <span class="math inline">\(i\)</span>th eigen mode is given by:</p>
<p><span class="math display">\[
\frac{u_{ik}^2/\lambda_i}{\sum_j \left(u^2_{jk}/\lambda_j\right)}
\]</span></p>
<p><strong>Key Indicators</strong>:</p>
<ul>
<li>A <strong>large condition index</strong> <span class="math inline">\(\delta_i\)</span> suggests potential collinearity.</li>
<li>Variance proportions <span class="math inline">\(&gt; 0.5\)</span> for at least two regression coefficients indicate serious collinearity problems.</li>
</ul>
<hr>
<div class="sourceCode" id="cb175"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span> <span class="op">+</span> <span class="va">disp</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute eigenvalues and eigenvectors of the correlation matrix of predictors</span></span>
<span><span class="va">cor_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"hp"</span>, <span class="st">"wt"</span>, <span class="st">"disp"</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">eigen_decomp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="va">cor_matrix</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract eigenvalues and compute the Condition Number</span></span>
<span><span class="va">eigenvalues</span> <span class="op">&lt;-</span> <span class="va">eigen_decomp</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="va">condition_number</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">eigenvalues</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">eigenvalues</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Condition Indices</span></span>
<span><span class="va">condition_indices</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">eigenvalues</span><span class="op">)</span> <span class="op">/</span> <span class="va">eigenvalues</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>  Condition_Number <span class="op">=</span> <span class="va">condition_number</span>,</span>
<span>  Condition_Indices <span class="op">=</span> <span class="va">condition_indices</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $Condition_Number</span></span>
<span><span class="co">#&gt; [1] 5.469549</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Condition_Indices</span></span>
<span><span class="co">#&gt; [1] 1.000000 2.697266 5.469549</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li>
<strong>Condition Number</strong>: A single value indicating the degree of multicollinearity.</li>
<li>
<strong>Condition Indices</strong>: A vector showing the relative dependency associated with each eigenvalue.</li>
</ol>
</div>
</div>
<div id="constancy-of-error-variance" class="section level5" number="5.1.2.5.5">
<h5>
<span class="header-section-number">5.1.2.5.5</span> Constancy of Error Variance<a class="anchor" aria-label="anchor" href="#constancy-of-error-variance"><i class="fas fa-link"></i></a>
</h5>
<p>Testing for the constancy of error variance (homoscedasticity) ensures that the assumptions of linear regression are met.</p>
<p>Below are two commonly used tests to assess error variance.</p>
<hr>
<div id="brown-forsythe-test-modified-levene-test" class="section level6" number="5.1.2.5.5.1">
<h6>
<span class="header-section-number">5.1.2.5.5.1</span> Brown-Forsythe Test (Modified Levene Test)<a class="anchor" aria-label="anchor" href="#brown-forsythe-test-modified-levene-test"><i class="fas fa-link"></i></a>
</h6>
<p>The <strong>Brown-Forsythe Test</strong> does not depend on the normality of errors and is suitable when error variance increases or decreases with <span class="math inline">\(X\)</span>.</p>
<p>Procedure</p>
<ol style="list-style-type: decimal">
<li><p>Split the residuals into two groups: <span class="math display">\[ e_{i1}, i = 1, \dots, n_1 \quad \text{and} \quad e_{j2}, j = 1, \dots, n_2 \]</span></p></li>
<li><p>Compute absolute deviations from the group medians: <span class="math display">\[ d_{i1} = |e_{i1} - \tilde{e}_1| \quad \text{and} \quad d_{j2} = |e_{j2} - \tilde{e}_2| \]</span> where <span class="math inline">\(\tilde{e}_1\)</span> and <span class="math inline">\(\tilde{e}_2\)</span> are the medians of groups 1 and 2, respectively.</p></li>
<li><p>Perform a two-sample t-test on <span class="math inline">\(d_{i1}\)</span> and <span class="math inline">\(d_{j2}\)</span>: <span class="math display">\[
t_L = \frac{\bar{d}_1 - \bar{d}_2}{s\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
\]</span> where <span class="math display">\[
s^2 = \frac{\sum_i (d_{i1} - \bar{d}_1)^2 + \sum_j (d_{j2} - \bar{d}_2)^2}{n - 2}
\]</span></p></li>
<li><p>Reject the null hypothesis of constant error variance if: <span class="math display">\[ |t_L| &gt; t_{1-\alpha/2; n-2} \]</span></p></li>
</ol>
<div class="sourceCode" id="cb176"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform the Brown-Forsythe Test</span></span>
<span><span class="va">levene_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/leveneTest.html">leveneTest</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">residuals</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/cut.html">cut</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">hp</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Split HP into 2 groups</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="va">levene_test</span></span>
<span><span class="co">#&gt; Levene's Test for Homogeneity of Variance (center = median)</span></span>
<span><span class="co">#&gt;       Df F value Pr(&gt;F)</span></span>
<span><span class="co">#&gt; group  1  0.0851 0.7724</span></span>
<span><span class="co">#&gt;       30</span></span></code></pre></div>
<p>The p-value determines whether to reject the null hypothesis of constant variance.</p>
<hr>
</div>
<div id="breusch-pagan-test-cook-weisberg-test" class="section level6" number="5.1.2.5.5.2">
<h6>
<span class="header-section-number">5.1.2.5.5.2</span> Breusch-Pagan Test (Cook-Weisberg Test)<a class="anchor" aria-label="anchor" href="#breusch-pagan-test-cook-weisberg-test"><i class="fas fa-link"></i></a>
</h6>
<p>The <strong>Breusch-Pagan Test</strong> assumes independent and normally distributed errors. It tests the hypothesis:</p>
<ul>
<li>
<span class="math inline">\(H_0: \gamma_1 = 0\)</span> (Constant variance).</li>
<li>
<span class="math inline">\(H_1: \gamma_1 \neq 0\)</span> (Non-constant variance).</li>
</ul>
<p>Procedure</p>
<ol style="list-style-type: decimal">
<li><p>Assume the variance of the error terms is modeled as: <span class="math display">\[ \sigma^2_i = \gamma_0 + \gamma_1 X_i \]</span></p></li>
<li>
<p>Regress the squared residuals (<span class="math inline">\(e_i^2\)</span>) on <span class="math inline">\(X_i\)</span>:</p>
<ul>
<li>Obtain the <strong>regression sum of squares</strong> (<span class="math inline">\(SSR^*\)</span>).</li>
</ul>
</li>
<li><p>Compute the Breusch-Pagan statistic: <span class="math display">\[
X^2_{BP} = \frac{SSR^*/2}{\left(\frac{SSE}{n}\right)^2}
\]</span> where <span class="math inline">\(SSE\)</span> is the error sum of squares from the regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p></li>
<li>
<p>Compare <span class="math inline">\(X^2_{BP}\)</span> to the critical value from the <span class="math inline">\(\chi^2\)</span> distribution with 1 degree of freedom:</p>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> (homogeneous variance) if: <span class="math display">\[ X^2_{BP} &gt; \chi^2_{1-\alpha;1} \]</span>
</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb177"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform the Breusch-Pagan Test</span></span>
<span><span class="va">bp_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/bptest.html">bptest</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="va">bp_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  studentized Breusch-Pagan test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; BP = 0.88072, df = 2, p-value = 0.6438</span></span></code></pre></div>
<p>If the p-value is below the chosen significance level (e.g., 0.05), reject $H_0$ and conclude non-constant variance.</p>
</div>
</div>
<div id="independence" class="section level5" number="5.1.2.5.6">
<h5>
<span class="header-section-number">5.1.2.5.6</span> Independence<a class="anchor" aria-label="anchor" href="#independence"><i class="fas fa-link"></i></a>
</h5>
<p>Testing for independence ensures that the residuals of a regression model are uncorrelated. Violation of this assumption may lead to biased or inefficient parameter estimates. Below, we discuss three primary methods for diagnosing dependence: plots, the Durbin-Watson test, and specific methods for time-series and spatial data.</p>
<hr>
<div id="plots" class="section level6" number="5.1.2.5.6.1">
<h6>
<span class="header-section-number">5.1.2.5.6.1</span> Plots<a class="anchor" aria-label="anchor" href="#plots"><i class="fas fa-link"></i></a>
</h6>
<p>A residual plot can help detect dependence in the residuals:</p>
<ol style="list-style-type: decimal">
<li>Plot residuals (<span class="math inline">\(e_i\)</span>) versus the predicted values (<span class="math inline">\(\hat{Y}_i\)</span>).</li>
<li>Patterns such as systematic waves, trends, or clusters indicate possible dependence.</li>
<li>Independence is suggested if residuals are randomly scattered without clear patterns.</li>
</ol>
<div class="sourceCode" id="cb178"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit a regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Residual plot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">fitted.values</span>, <span class="va">model</span><span class="op">$</span><span class="va">residuals</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Fitted Values"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Residuals"</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Residual Plot"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">0</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-14-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="durbin-watson-test" class="section level6" number="5.1.2.5.6.2">
<h6>
<span class="header-section-number">5.1.2.5.6.2</span> Durbin-Watson Test<a class="anchor" aria-label="anchor" href="#durbin-watson-test"><i class="fas fa-link"></i></a>
</h6>
<p>The <strong>Durbin-Watson test</strong> specifically detects autocorrelation in residuals, especially in time-series data.</p>
<ul>
<li>
<p><strong>Hypotheses</strong>:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Residuals are uncorrelated.</p></li>
<li><p><span class="math inline">\(H_1\)</span>: Residuals are autocorrelated.</p></li>
</ul>
</li>
<li>
<p>The Durbin-Watson statistic (<span class="math inline">\(d\)</span>) is calculated as: <span class="math inline">\(d = \frac{\sum_{t=2}^n (e_t - e_{t-1})^2}{\sum_{t=1}^n e_t^2}\)</span></p>
<ul>
<li><p><span class="math inline">\(d \approx 2\)</span>: No autocorrelation.</p></li>
<li><p><span class="math inline">\(d &lt; 2\)</span>: Positive autocorrelation.</p></li>
<li><p><span class="math inline">\(d &gt; 2\)</span>: Negative autocorrelation.</p></li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb179"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform the Durbin-Watson test</span></span>
<span><span class="va">dw_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/dwtest.html">dwtest</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="va">dw_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Durbin-Watson test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; DW = 1.3624, p-value = 0.02061</span></span>
<span><span class="co">#&gt; alternative hypothesis: true autocorrelation is greater than 0</span></span></code></pre></div>
</div>
<div id="time-series-autocorrelation" class="section level6" number="5.1.2.5.6.3">
<h6>
<span class="header-section-number">5.1.2.5.6.3</span> Time-Series Autocorrelation<a class="anchor" aria-label="anchor" href="#time-series-autocorrelation"><i class="fas fa-link"></i></a>
</h6>
<p>For time-series data, autocorrelation often occurs due to the temporal structure. Key diagnostics include:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Autocorrelation Function (ACF)</strong>:</p>
<ul>
<li><p>Shows the correlation of residuals with their lagged values.</p></li>
<li><p>Significant spikes in the ACF plot indicate autocorrelation.</p></li>
</ul>
</li>
<li>
<p><strong>Partial Autocorrelation Function (PACF)</strong>:</p>
<ul>
<li>Identifies the correlation of residuals after removing the influence of intermediate lags.</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb180"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://pkg.robjhyndman.com/forecast/">forecast</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Residuals from a time-series model</span></span>
<span><span class="va">time_series_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ts.html">ts</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">residuals</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot ACF and PACF</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">time_series_res</span>, main <span class="op">=</span> <span class="st">"ACF of Residuals"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-16-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb181"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">time_series_res</span>, main <span class="op">=</span> <span class="st">"PACF of Residuals"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-16-2.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="spatial-statistics" class="section level6" number="5.1.2.5.6.4">
<h6>
<span class="header-section-number">5.1.2.5.6.4</span> Spatial Statistics<a class="anchor" aria-label="anchor" href="#spatial-statistics"><i class="fas fa-link"></i></a>
</h6>
<p>Spatial dependence occurs when residuals are correlated across geographical or spatial locations. Two primary tests are used to diagnose spatial autocorrelation:</p>
<hr>
<p><strong>Moran’s I</strong> measures global spatial autocorrelation. It determines whether similar values cluster spatially. The statistic is defined as:</p>
<p><span class="math display">\[
I = \frac{n}{W} \cdot \frac{\sum_i \sum_j w_{ij} (x_i - \bar{x})(x_j - \bar{x})}{\sum_i (x_i - \bar{x})^2}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span>: Number of observations.</li>
<li>
<span class="math inline">\(W\)</span>: Sum of all spatial weights <span class="math inline">\(w_{ij}\)</span>.</li>
<li>
<span class="math inline">\(w_{ij}\)</span>: Spatial weight between locations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</li>
<li>
<span class="math inline">\(x_i\)</span>: Residual value at location <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(\bar{x}\)</span>: Mean of the residuals.</li>
</ul>
<p>Spatial Weight Matrix (<span class="math inline">\(W\)</span>): <span class="math inline">\(W\)</span> defines the spatial relationship between observations. It is often derived from:</p>
<ul>
<li>
<strong>Distance-based methods</strong>: E.g., k-nearest neighbors or distance bands.</li>
<li>
<strong>Adjacency methods</strong>: Based on shared boundaries in geographic data.</li>
</ul>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><p><span class="math inline">\(I &gt; 0\)</span>: Positive spatial autocorrelation (similar values cluster together).</p></li>
<li><p><span class="math inline">\(I &lt; 0\)</span>: Negative spatial autocorrelation (dissimilar values are neighbors).</p></li>
<li><p><span class="math inline">\(I \approx 0\)</span>: Random spatial distribution.</p></li>
</ul>
<div class="sourceCode" id="cb182"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-spatial.github.io/sf/">sf</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/r-spatial/spdep/">spdep</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate spatial data (example with mtcars dataset)</span></span>
<span><span class="va">coords</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">hp</span>, <span class="va">mtcars</span><span class="op">$</span><span class="va">wt</span><span class="op">)</span>  <span class="co"># Coordinates based on two variables</span></span>
<span></span>
<span><span class="co"># Add small jitter to avoid duplicate coordinates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># For reproducibility</span></span>
<span><span class="va">coords_jittered</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">coords</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">coords</span><span class="op">)</span>,<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">0.01</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Find nearest neighbors</span></span>
<span><span class="va">neighbors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-spatial.github.io/spdep/reference/knn2nb.html">knn2nb</a></span><span class="op">(</span><span class="fu"><a href="https://r-spatial.github.io/spdep/reference/knearneigh.html">knearneigh</a></span><span class="op">(</span><span class="va">coords_jittered</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create spatial weights matrix</span></span>
<span><span class="va">weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-spatial.github.io/spdep/reference/nb2listw.html">nb2listw</a></span><span class="op">(</span><span class="va">neighbors</span>, style <span class="op">=</span> <span class="st">"W"</span><span class="op">)</span>  <span class="co"># Row-standardized weights</span></span>
<span></span>
<span><span class="co"># Fit the linear model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Check lengths of residuals and weights</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">residuals</span><span class="op">)</span>  <span class="co"># Should be 32</span></span>
<span><span class="co">#&gt; [1] 32</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">weights</span><span class="op">$</span><span class="va">neighbours</span><span class="op">)</span>  <span class="co"># Should also be 32</span></span>
<span><span class="co">#&gt; [1] 32</span></span>
<span></span>
<span><span class="co"># Compute Moran's I for residuals</span></span>
<span><span class="va">moran_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-spatial.github.io/spdep/reference/moran.test.html">moran.test</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">residuals</span>, <span class="va">weights</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">moran_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Moran I test under randomisation</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model$residuals  </span></span>
<span><span class="co">#&gt; weights: weights    </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Moran I statistic standard deviate = 1.7679, p-value = 0.03854</span></span>
<span><span class="co">#&gt; alternative hypothesis: greater</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; Moran I statistic       Expectation          Variance </span></span>
<span><span class="co">#&gt;        0.18544790       -0.03225806        0.01516371</span></span>
<span></span>
<span><span class="co"># Moran's scatterplot</span></span>
<span><span class="fu"><a href="https://r-spatial.github.io/spdep/reference/moran.plot.html">moran.plot</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">residuals</span>, <span class="va">weights</span>, main <span class="op">=</span> <span class="st">"Moran's Scatterplot"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Significant Moran’s I: Indicates global clustering of similar residuals, suggesting spatial dependence.</p>
<hr>
<p><strong>Geary’s C</strong> measures spatial autocorrelation at a local level, emphasizing differences between neighboring observations. The statistic is defined as:</p>
<p><span class="math display">\[
C = \frac{n - 1}{2W} \cdot \frac{\sum_i \sum_j w_{ij} (x_i - x_j)^2}{\sum_i (x_i - \bar{x})^2}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(C\)</span> ranges from 0 to 2:
<ul>
<li>
<span class="math inline">\(C \approx 0\)</span>: High positive spatial autocorrelation.</li>
<li>
<span class="math inline">\(C \approx 1\)</span>: Spatial randomness.</li>
<li>
<span class="math inline">\(C \approx 2\)</span>: High negative spatial autocorrelation.</li>
</ul>
</li>
</ul>
<p><strong>Comparison of Moran’s I and Geary’s C</strong>:</p>
<ul>
<li><p>Moran’s I is more global and measures the overall pattern of autocorrelation.</p></li>
<li><p>Geary’s C is more sensitive to local spatial autocorrelation, detecting small-scale patterns.</p></li>
</ul>
<hr>
<div class="sourceCode" id="cb183"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute Geary's C for residuals</span></span>
<span><span class="va">geary_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-spatial.github.io/spdep/reference/geary.test.html">geary.test</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">residuals</span>, <span class="va">weights</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Results</span></span>
<span><span class="va">geary_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Geary C test under randomisation</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model$residuals </span></span>
<span><span class="co">#&gt; weights: weights   </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Geary C statistic standard deviate = 1.0606, p-value = 0.1444</span></span>
<span><span class="co">#&gt; alternative hypothesis: Expectation greater than statistic</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; Geary C statistic       Expectation          Variance </span></span>
<span><span class="co">#&gt;        0.84515868        1.00000000        0.02131399</span></span></code></pre></div>
<p>Significant Geary’s C: Highlights local spatial autocorrelation, useful for identifying specific regions or groups of observations where dependence is strong.</p>
</div>
</div>
</div>
</div>
</div>
<div id="generalized-least-squares" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Generalized Least Squares<a class="anchor" aria-label="anchor" href="#generalized-least-squares"><i class="fas fa-link"></i></a>
</h2>
<div id="infeasible-generalized-least-squares" class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> Infeasible Generalized Least Squares<a class="anchor" aria-label="anchor" href="#infeasible-generalized-least-squares"><i class="fas fa-link"></i></a>
</h3>
<p>Motivation for a More Efficient Estimator</p>
<ul>
<li>The <a href="linear-regression.html#gauss-markov-theorem">Gauss-Markov Theorem</a> guarantees that OLS is the <strong>Best Linear Unbiased Estimator (BLUE)</strong> under assumptions A1-A4:
<ul>
<li>
<strong>A4:</strong> <span class="math inline">\(Var(\epsilon | \mathbf{X}) = \sigma^2 \mathbf{I}_n\)</span> (homoscedasticity and no autocorrelation).</li>
</ul>
</li>
<li>When <strong>A4</strong> does not hold:
<ul>
<li>
<strong>Heteroskedasticity:</strong> <span class="math inline">\(Var(\epsilon_i | \mathbf{X}) \neq \sigma^2\)</span>.</li>
<li>
<strong>Serial Correlation:</strong> <span class="math inline">\(Cov(\epsilon_i, \epsilon_j | \mathbf{X}) \neq 0\)</span> for (<span class="math inline">\(i \neq j\)</span>).</li>
</ul>
</li>
</ul>
<p>Without <strong>A4</strong>, OLS is <strong>unbiased</strong> but no longer <strong>efficient</strong>. This motivates the need for an alternative approach to identify the most efficient estimator.</p>
<p>The unweighted (standard) regression model is given by:</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X \beta} + \boldsymbol{\epsilon}
\]</span></p>
<p>Assuming <strong>A1-A3</strong> hold (linearity, full rank, exogeneity), but <strong>A4</strong> does not, the variance of the error term is no longer proportional to an identity</p>
<p><span class="math display">\[
Var(\boldsymbol{\epsilon} | \mathbf{X}) = \boldsymbol{\Omega} \neq \sigma^2 \mathbf{I}_n.
\]</span></p>
<p>To address the violation of <strong>A4</strong> (<span class="math inline">\(\boldsymbol{\Omega} \neq \sigma^2 \mathbf{I}_n\)</span>), one can transform the model by premultiplying both sides by a full-rank matrix <span class="math inline">\(\mathbf{w}\)</span> to have a weighted (transformed) regression model:</p>
<p><span class="math display">\[
\mathbf{w y} = \mathbf{w X \beta} + \mathbf{w \epsilon},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> is a full-rank matrix chosen such that:</p>
<p><span class="math display">\[
\mathbf{w'w} = \boldsymbol{\Omega}^{-1}.
\]</span></p>
<ul>
<li>
<span class="math inline">\(\mathbf{w}\)</span> is the <a href="prerequisites.html#cholesky-decomposition">Cholesky Decomposition</a> of <span class="math inline">\(\boldsymbol{\Omega}^{-1}\)</span>.</li>
<li>The Cholesky decomposition ensures <span class="math inline">\(\mathbf{w}\)</span> satisfies <span class="math inline">\(\mathbf{w'w = \Omega^{-1}}\)</span>, where <span class="math inline">\(\mathbf{w}\)</span> is the “square root” of <span class="math inline">\(\boldsymbol{\Omega}^{-1}\)</span> in matrix terms.</li>
</ul>
<p>By transforming the original model, the variance of the transformed errors becomes:</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{\Omega} &amp;= Var(\boldsymbol{\epsilon} | \mathbf{X}), \\
\boldsymbol{\Omega}^{-1} &amp;= Var(\boldsymbol{\epsilon} | \mathbf{X})^{-1}.
\end{aligned}
\]</span></p>
<p>The transformed equation allows us to compute a more efficient estimator.</p>
<p>Using the transformed model, the <strong>Infeasible Generalized Least Squares (IGLS)</strong> estimator is:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{\hat{\beta}_{IGLS}} &amp;= \mathbf{(X'w'wX)^{-1}X'w'wy} \\
&amp;= \mathbf{(X' \boldsymbol{\Omega}^{-1} X)^{-1} X' \boldsymbol{\Omega}^{-1} y} \\
&amp;= \mathbf{\beta + (X' \boldsymbol{\Omega}^{-1} X)^{-1} X' \boldsymbol{\Omega}^{-1} \boldsymbol{\epsilon}}.
\end{aligned}
\]</span></p>
<ol style="list-style-type: decimal">
<li><strong>Unbiasedness</strong></li>
</ol>
<p>Since assumptions <strong>A1-A3</strong> hold for the unweighted model:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{E(\hat{\beta}_{IGLS}|\mathbf{X})}
&amp;= \mathbf{E(\beta + (X'\Omega^{-1}X'\Omega^{-1}\epsilon)|\mathbf{X})} \\
&amp;= \mathbf{\beta + E(X'\Omega^{-1}X'\Omega^{-1}\epsilon|\mathbf{X})} \\
&amp;= \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}E(\epsilon|\mathbf{X})}  &amp;&amp; \text{since A3: } E(\epsilon|\mathbf{X})=0, \\
&amp;= \mathbf{\beta}.
\end{aligned}
\]</span></p>
<p>Thus, the <strong>IGLS estimator is unbiased</strong>.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Variance</strong></li>
</ol>
<p>The variance of the transformed errors is given by:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Var(w\epsilon|\mathbf{X})}
&amp;= \mathbf{wVar(\epsilon|\mathbf{X})w'} \\
&amp;= \mathbf{w\Omega w'} \\
&amp;= \mathbf{w(w'w)^{-1}w'} &amp;&amp; \text{since } \mathbf{w} \text{ is full-rank,} \\
&amp;= \mathbf{ww^{-1}(w')^{-1}w'} \\
&amp;= \mathbf{I_n}.
\end{aligned}
\]</span></p>
<p>Hence, <strong>A4</strong> holds for the transformed (weighted) equation, satisfying the Gauss-Markov conditions.</p>
<p>The variance of the IGLS estimator is:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Var(\hat{\beta}_{IGLS}|\mathbf{X})}
&amp;= \mathbf{Var(\beta + (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}\epsilon|\mathbf{X})} \\
&amp;= \mathbf{Var((X'\Omega^{-1}X)^{-1}X'\Omega^{-1}\epsilon|\mathbf{X})} \\
&amp;= \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1} Var(\epsilon|\mathbf{X}) \Omega^{-1}X(X'\Omega^{-1}X)^{-1}} &amp;&amp; \text{because A4 holds}, \\
&amp;= \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1} \Omega \Omega^{-1} \Omega^{-1}X(X'\Omega^{-1}X)^{-1}}, \\
&amp;= \mathbf{(X'\Omega^{-1}X)^{-1}}.
\end{aligned}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Efficiency</strong></li>
</ol>
<p>The difference in variances between OLS and IGLS is:</p>
<p><span class="math display">\[
\mathbf{Var(\hat{\beta}_{OLS}|\mathbf{X}) - Var(\hat{\beta}_{IGLS}|\mathbf{X})} = \mathbf{A\Omega A'},
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{A = (X'X)^{-1}X' - (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}}.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{\Omega}\)</span> is positive semi-definite, <span class="math inline">\(\mathbf{A\Omega A'}\)</span> is also positive semi-definite. Thus, the <strong>IGLS estimator is more efficient than OLS</strong> under heteroskedasticity or autocorrelation.</p>
<p>In short, properties of <span class="math inline">\(\mathbf{\hat{\beta}_{IGLS}}\)</span>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Unbiasedness</strong>: <span class="math inline">\(\mathbf{\hat{\beta}_{IGLS}}\)</span> remains unbiased as long as <strong>A1-A3</strong> hold.</li>
<li>
<strong>Efficiency</strong>: <span class="math inline">\(\mathbf{\hat{\beta}_{IGLS}}\)</span> is more efficient than OLS under heteroskedasticity or serial correlation since it accounts for the structure of <span class="math inline">\(\boldsymbol{\Omega}\)</span>.</li>
</ol>
<hr>
<p><strong>Why Is IGLS “Infeasible”?</strong></p>
<p>The name <strong>infeasible</strong> arises because it is generally impossible to compute the estimator directly due to the structure of <span class="math inline">\(\mathbf{w}\)</span> (or equivalently <span class="math inline">\(\boldsymbol{\Omega}^{-1}\)</span>). The matrix <span class="math inline">\(\mathbf{w}\)</span> is defined as:</p>
<p><span class="math display">\[
\mathbf{w} =
\begin{pmatrix}
w_{11} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
w_{21} &amp; w_{22} &amp; 0 &amp; \cdots &amp; 0 \\
w_{31} &amp; w_{32} &amp; w_{33} &amp; \cdots &amp; \cdots \\
w_{n1} &amp; w_{n2} &amp; w_{n3} &amp; \cdots &amp; w_{nn} \\
\end{pmatrix},
\]</span></p>
<p>with <span class="math inline">\(n(n+1)/2\)</span> unique elements for <span class="math inline">\(n\)</span> observations. This results in more parameters than data points, making direct estimation infeasible.</p>
<p>To make the estimation feasible, assumptions about the structure of <span class="math inline">\(\mathbf{\Omega}\)</span> are required. Common approaches include:</p>
<ul>
<li>
<p><a href="linear-regression.html#heteroskedasticity-errors">Heteroskedasticity Errors</a>: Assume a multiplicative exponential model for the variance, such as <span class="math inline">\(Var(\epsilon_i|\mathbf{X}) = \sigma_i^2\)</span>.</p>
<ul>
<li><p>Assume no correlation between errors, but allow heterogeneous variances: <span class="math display">\[ \mathbf{\Omega} = \begin{pmatrix} \sigma_1^2 &amp; 0          &amp; \cdots &amp; 0 \\ 0          &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\ \vdots     &amp; \vdots     &amp; \ddots &amp; \vdots \\ 0          &amp; 0          &amp; \cdots &amp; \sigma_n^2 \end{pmatrix}. \]</span></p></li>
<li>
<p>Estimate <span class="math inline">\(\sigma_i^2\)</span> using methods such as:</p>
<ul>
<li>Modeling <span class="math inline">\(\sigma_i^2\)</span> as a function of predictors (e.g., <span class="math inline">\(\sigma_i^2 = \exp(\mathbf{x}_i \gamma)\)</span>).</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="linear-regression.html#serial-correlation">Serial Correlation</a>: Assume serial correlation follows an autoregressive process <a href="linear-regression.html#ar1">AR(1) Model</a>, e.g., <span class="math inline">\(\epsilon_t = \rho \epsilon_{t-1} + u_t\)</span> and <span class="math inline">\(Cov(\epsilon_t, \epsilon_{t -h}) = \rho^h \sigma^2\)</span>, where we have a variance-covariance matrix with off-diagonal elements decaying geometrically: <span class="math display">\[ \mathbf{\Omega} = \frac{\sigma^2}{1-\rho^2} \begin{pmatrix} 1      &amp; \rho    &amp; \rho^2 &amp; \cdots &amp; \rho^{n-1} \\ \rho   &amp; 1       &amp; \rho   &amp; \cdots &amp; \rho^{n-2} \\ \rho^2 &amp; \rho    &amp; 1      &amp; \cdots &amp; \rho^{n-3} \\ \vdots &amp; \vdots  &amp; \vdots &amp; \ddots &amp; \vdots \\ \rho^{n-1} &amp; \rho^{n-2} &amp; \rho^{n-3} &amp; \cdots &amp; 1 \end{pmatrix}. \]</span></p></li>
<li><p><a href="linear-regression.html#cluster-errors">Cluster Errors</a>: Assume block-diagonal structure for <span class="math inline">\(\mathbf{\Omega}\)</span> to account for grouped or panel data.</p></li>
</ul>
<p>Each assumption simplifies the estimation of <span class="math inline">\(\mathbf{\Omega}\)</span> and thus <span class="math inline">\(\mathbf{w}\)</span>, enabling <a href="FGLS">Feasible Generalized Least Squares</a> with fewer unknown parameters to estimate.</p>
</div>
<div id="feasible-generalized-least-squares" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Feasible Generalized Least Squares<a class="anchor" aria-label="anchor" href="#feasible-generalized-least-squares"><i class="fas fa-link"></i></a>
</h3>
<div id="heteroskedasticity-errors" class="section level4" number="5.2.2.1">
<h4>
<span class="header-section-number">5.2.2.1</span> Heteroskedasticity Errors<a class="anchor" aria-label="anchor" href="#heteroskedasticity-errors"><i class="fas fa-link"></i></a>
</h4>
<p>Heteroskedasticity occurs when the variance of the error term is not constant across observations. Specifically:</p>
<p><span class="math display">\[
Var(\epsilon_i | x_i) = E(\epsilon_i^2 | x_i) \neq \sigma^2,
\]</span></p>
<p>but instead depends on a function of <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[
Var(\epsilon_i | x_i) = h(x_i) = \sigma_i^2
\]</span></p>
<p>This violates the assumption of homoscedasticity (constant variance), impacting the efficiency of OLS estimates.</p>
<p>For the model:</p>
<p><span class="math display">\[
y_i = x_i\beta + \epsilon_i,
\]</span></p>
<p>we apply a transformation to standardize the variance:</p>
<p><span class="math display">\[
\frac{y_i}{\sigma_i} = \frac{x_i}{\sigma_i} \beta + \frac{\epsilon_i}{\sigma_i}.
\]</span></p>
<p>By scaling each observation with <span class="math inline">\(1/\sigma_i\)</span>, the variance of the transformed error term becomes:</p>
<p><span class="math display">\[
\begin{aligned}
Var\left(\frac{\epsilon_i}{\sigma_i} \bigg| X \right) &amp;= \frac{1}{\sigma_i^2} Var(\epsilon_i | X) \\
&amp;= \frac{1}{\sigma_i^2} \sigma_i^2 \\
&amp;= 1.
\end{aligned}
\]</span></p>
<p>Thus, the heteroskedasticity is corrected in the transformed model.</p>
<p>In matrix notation, the transformed model is:</p>
<p><span class="math display">\[
\mathbf{w y} = \mathbf{w X \beta + w \epsilon},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> is the weight matrix used to standardize the variance. The weight matrix <span class="math inline">\(\mathbf{w}\)</span> is defined as:</p>
<p><span class="math display">\[
\mathbf{w} =
\begin{pmatrix}
1/\sigma_1 &amp; 0          &amp; 0          &amp; \cdots &amp; 0 \\
0          &amp; 1/\sigma_2 &amp; 0          &amp; \cdots &amp; 0 \\
0          &amp; 0          &amp; 1/\sigma_3 &amp; \cdots &amp; 0 \\
\vdots     &amp; \vdots     &amp; \vdots     &amp; \ddots &amp; \vdots \\
0          &amp; 0          &amp; 0          &amp; \cdots &amp; 1/\sigma_n
\end{pmatrix}.
\]</span></p>
<hr>
<p>In the presence of heteroskedasticity, the variance of the error term, <span class="math inline">\(Var(\epsilon_i|\mathbf{x}_i)\)</span>, is not constant across observations. This leads to inefficient OLS estimates.</p>
<p><strong>Infeasible Weighted Least Squares (IWLS)</strong> assumes that the variances <span class="math inline">\(\sigma_i^2 = Var(\epsilon_i|\mathbf{x}_i)\)</span> are <strong>known</strong>. This allows us to adjust the regression equation to correct for heteroskedasticity.</p>
<p>The model is transformed as follows:</p>
<p><span class="math display">\[
y_i = \mathbf{x}_i\beta + \epsilon_i \quad \text{(original equation)},
\]</span></p>
<p>where <span class="math inline">\(\epsilon_i\)</span> has variance <span class="math inline">\(\sigma_i^2\)</span>. To make the errors homoskedastic, we divide through by <span class="math inline">\(\sigma_i\)</span>:</p>
<p><span class="math display">\[
\frac{y_i}{\sigma_i} = \frac{\mathbf{x}_i}{\sigma_i}\beta + \frac{\epsilon_i}{\sigma_i}.
\]</span></p>
<p>Now, the transformed error term <span class="math inline">\(\epsilon_i / \sigma_i\)</span> has a constant variance of 1:</p>
<p><span class="math display">\[
Var\left(\frac{\epsilon_i}{\sigma_i} | \mathbf{x}_i \right) = 1.
\]</span></p>
<hr>
<p>The IWLS estimator minimizes the <strong>weighted sum of squared residuals</strong> for the transformed model:</p>
<p><span class="math display">\[
\text{Minimize: } \sum_{i=1}^n \left( \frac{y_i - \mathbf{x}_i\beta}{\sigma_i} \right)^2.
\]</span></p>
<p>In matrix form, the IWLS estimator is:</p>
<p><span class="math display">\[
\hat{\beta}_{IWLS} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{W}\)</span> is a diagonal matrix of weights:</p>
<p><span class="math display">\[
\mathbf{W} =
\begin{pmatrix}
1/\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1/\sigma_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1/\sigma_n^2
\end{pmatrix}.
\]</span></p>
<hr>
<p>Properties of IWLS</p>
<ol style="list-style-type: decimal">
<li>
<strong>Valid Standard Errors</strong>:
<ul>
<li>If <span class="math inline">\(Var(\epsilon_i | \mathbf{X}) = \sigma_i^2\)</span>, the usual standard errors from IWLS are valid.</li>
</ul>
</li>
<li>
<strong>Robustness</strong>:
<ul>
<li>If the variance assumption is incorrect (<span class="math inline">\(Var(\epsilon_i | \mathbf{X}) \neq \sigma_i^2\)</span>), heteroskedasticity-robust standard errors must be used instead.</li>
</ul>
</li>
</ol>
<hr>
<p>The primary issue with IWLS is that <span class="math inline">\(\sigma_i^2 = Var(\epsilon_i|\mathbf{x}_i)\)</span> is generally unknown. Specifically, we do not know:</p>
<p><span class="math display">\[
\sigma_i^2 = Var(\epsilon_i|\mathbf{x}_i) = E(\epsilon_i^2|\mathbf{x}_i).
\]</span></p>
<p>The challenges are:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Single Observation</strong>:
<ul>
<li>For each observation <span class="math inline">\(i\)</span>, there is only one <span class="math inline">\(\epsilon_i\)</span>, which is insufficient to estimate the variance <span class="math inline">\(\sigma_i^2\)</span> directly.</li>
</ul>
</li>
<li>
<strong>Dependence on Assumptions</strong>:
<ul>
<li>To estimate <span class="math inline">\(\sigma_i^2\)</span>, we must impose assumptions about its relationship to <span class="math inline">\(\mathbf{x}_i\)</span>.</li>
</ul>
</li>
</ol>
<hr>
<p>To make IWLS feasible, we model <span class="math inline">\(\sigma_i^2\)</span> as a function of the predictors <span class="math inline">\(\mathbf{x}_i\)</span>. A common approach is:</p>
<p><span class="math display">\[
\epsilon_i^2 = v_i \exp(\mathbf{x}_i\gamma),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(v_i\)</span> is an independent error term with strictly positive values, representing random noise.</p></li>
<li><p><span class="math inline">\(\exp(\mathbf{x}_i\gamma)\)</span> is a deterministic function of the predictors <span class="math inline">\(\mathbf{x}_i\)</span>.</p></li>
</ul>
<p>Taking the natural logarithm of both sides linearizes the model:</p>
<p><span class="math display">\[
\ln(\epsilon_i^2) = \mathbf{x}_i\gamma + \ln(v_i),
\]</span></p>
<p>where <span class="math inline">\(\ln(v_i)\)</span> is independent of <span class="math inline">\(\mathbf{x}_i\)</span>. This transformation enables us to estimate <span class="math inline">\(\gamma\)</span> using standard OLS techniques.</p>
<hr>
<p><strong>Estimation Procedure for Feasible GLS (FGLS)</strong></p>
<p>Since we do not observe the true errors <span class="math inline">\(\epsilon_i\)</span>, we approximate them using the OLS residuals <span class="math inline">\(e_i\)</span>. Here’s the step-by-step process:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Compute OLS Residuals</strong>: First, fit the original model using OLS and calculate the residuals:</p>
<p><span class="math display">\[
e_i = y_i - \mathbf{x}_i\hat{\beta}_{OLS}.
\]</span></p>
</li>
<li>
<p><strong>Approximate</strong> <span class="math inline">\(\epsilon_i^2\)</span> with <span class="math inline">\(e_i^2\)</span>: Use the squared residuals as a proxy for the squared errors:</p>
<p><span class="math display">\[
e_i^2 \approx \epsilon_i^2.
\]</span></p>
</li>
<li>
<p><strong>Log-Linear Model</strong>: Fit the log-transformed model to estimate <span class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
\ln(e_i^2) = \mathbf{x}_i\gamma + \ln(v_i).
\]</span></p>
<p>Estimate <span class="math inline">\(\gamma\)</span> using OLS, where <span class="math inline">\(\ln(v_i)\)</span> is treated as the error term.</p>
</li>
<li>
<p><strong>Estimate Variances</strong>: Use the fitted values <span class="math inline">\(\hat{\gamma}\)</span> to estimate <span class="math inline">\(\sigma_i^2\)</span> for each observation:</p>
<p><span class="math display">\[
\hat{\sigma}_i^2 = \exp(\mathbf{x}_i\hat{\gamma}).
\]</span></p>
</li>
<li>
<p><strong>Perform Weighted Least Squares</strong>: Use the estimated variances <span class="math inline">\(\hat{\sigma}_i^2\)</span> to construct the weight matrix <span class="math inline">\(\mathbf{\hat{W}}\)</span>:</p>
<p><span class="math display">\[
\mathbf{\hat{W}} =
\begin{pmatrix}
1/\hat{\sigma}_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1/\hat{\sigma}_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1/\hat{\sigma}_n^2
\end{pmatrix}.
\]</span></p>
<p>Then, compute the <strong>Feasible GLS (FGLS)</strong> estimator:</p>
<p><span class="math display">\[
\hat{\beta}_{FGLS} = (\mathbf{X}'\mathbf{\hat{W}}\mathbf{X})^{-1}\mathbf{X}'\mathbf{\hat{W}}\mathbf{y}.
\]</span></p>
</li>
</ol>
</div>
<div id="serial-correlation" class="section level4" number="5.2.2.2">
<h4>
<span class="header-section-number">5.2.2.2</span> Serial Correlation<a class="anchor" aria-label="anchor" href="#serial-correlation"><i class="fas fa-link"></i></a>
</h4>
<p>Serial correlation (also called autocorrelation) occurs when the error terms in a regression model are correlated across observations. Formally:</p>
<p><span class="math display">\[
Cov(\epsilon_i, \epsilon_j | \mathbf{X}) \neq 0 \quad \text{for } i \neq j.
\]</span></p>
<p>This violates the Gauss-Markov assumption that <span class="math inline">\(Cov(\epsilon_i, \epsilon_j | \mathbf{X}) = 0\)</span>, leading to inefficiencies in OLS estimates.</p>
<hr>
<div id="covariance-stationarity" class="section level5" number="5.2.2.2.1">
<h5>
<span class="header-section-number">5.2.2.2.1</span> Covariance Stationarity<a class="anchor" aria-label="anchor" href="#covariance-stationarity"><i class="fas fa-link"></i></a>
</h5>
<p>If the errors are <strong>covariance stationary</strong>, the covariance between errors depends only on their relative time or positional difference (<span class="math inline">\(h\)</span>), not their absolute position:</p>
<p><span class="math display">\[
Cov(\epsilon_i, \epsilon_j | \mathbf{X}) = Cov(\epsilon_i, \epsilon_{i+h} | \mathbf{x}_i, \mathbf{x}_{i+h}) = \gamma_h,
\]</span></p>
<p>where <span class="math inline">\(\gamma_h\)</span> represents the covariance at lag <span class="math inline">\(h\)</span>.</p>
<p>Under covariance stationarity, the variance-covariance matrix of the error term <span class="math inline">\(\boldsymbol{\epsilon}\)</span> takes the following form:</p>
<p><span class="math display">\[
Var(\boldsymbol{\epsilon}|\mathbf{X}) = \boldsymbol{\Omega} =
\begin{pmatrix}
\sigma^2 &amp; \gamma_1 &amp; \gamma_2 &amp; \cdots &amp; \gamma_{n-1} \\
\gamma_1 &amp; \sigma^2 &amp; \gamma_1 &amp; \cdots &amp; \gamma_{n-2} \\
\gamma_2 &amp; \gamma_1 &amp; \sigma^2 &amp; \cdots &amp; \vdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \gamma_1 \\
\gamma_{n-1} &amp; \gamma_{n-2} &amp; \cdots &amp; \gamma_1 &amp; \sigma^2
\end{pmatrix}.
\]</span></p>
<p>Key Points:</p>
<ol style="list-style-type: decimal">
<li>The diagonal elements represent the variance of the error term: <span class="math inline">\(\sigma^2\)</span>.</li>
<li>The off-diagonal elements <span class="math inline">\(\gamma_h\)</span> represent covariances at different lags <span class="math inline">\(h\)</span>.</li>
</ol>
<hr>
<p><strong>Why Serial Correlation Is a Problem?</strong></p>
<p>The matrix <span class="math inline">\(\boldsymbol{\Omega}\)</span> introduces <span class="math inline">\(n\)</span> parameters to estimate (e.g., <span class="math inline">\(\sigma^2, \gamma_1, \gamma_2, \ldots, \gamma_{n-1}\)</span>). Estimating such a large number of parameters becomes impractical, especially for large datasets. To address this, we impose additional structure to reduce the number of parameters.</p>
<hr>
</div>
<div id="ar1" class="section level5" number="5.2.2.2.2">
<h5>
<span class="header-section-number">5.2.2.2.2</span> AR(1) Model<a class="anchor" aria-label="anchor" href="#ar1"><i class="fas fa-link"></i></a>
</h5>
<p>In the <strong>AR(1)</strong> process, the errors follow a first-order autoregressive process:</p>
<p><span class="math display">\[
\begin{aligned}
y_t &amp;= \beta_0 + x_t\beta_1 + \epsilon_t, \\
\epsilon_t &amp;= \rho \epsilon_{t-1} + u_t,
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\rho\)</span> is the first-order autocorrelation coefficient, capturing the relationship between consecutive errors.</p></li>
<li><p><span class="math inline">\(u_t\)</span> is white noise, satisfying <span class="math inline">\(Var(u_t) = \sigma_u^2\)</span> and <span class="math inline">\(Cov(u_t, u_{t-h}) = 0\)</span> for <span class="math inline">\(h \neq 0\)</span>.</p></li>
</ul>
<p>Under the AR(1) assumption, the variance-covariance matrix of the error term <span class="math inline">\(\boldsymbol{\epsilon}\)</span> becomes:</p>
<p><span class="math display">\[
Var(\boldsymbol{\epsilon} | \mathbf{X}) = \frac{\sigma_u^2}{1-\rho^2}
\begin{pmatrix}
1 &amp; \rho &amp; \rho^2 &amp; \cdots &amp; \rho^{n-1} \\
\rho &amp; 1 &amp; \rho &amp; \cdots &amp; \rho^{n-2} \\
\rho^2 &amp; \rho &amp; 1 &amp; \cdots &amp; \rho^{n-3} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\rho^{n-1} &amp; \rho^{n-2} &amp; \cdots &amp; \rho &amp; 1
\end{pmatrix}.
\]</span></p>
<p>Key Features:</p>
<ol style="list-style-type: decimal">
<li>The diagonal elements represent the variance: <span class="math inline">\(Var(\epsilon_t | \mathbf{X}) = \sigma_u^2 / (1-\rho^2)\)</span>.</li>
<li>The off-diagonal elements decay exponentially with lag <span class="math inline">\(h\)</span>: <span class="math inline">\(Cov(\epsilon_t, \epsilon_{t-h} | \mathbf{X}) = \rho^h \cdot Var(\epsilon_t | \mathbf{X})\)</span>.</li>
</ol>
<p>Under AR(1), only <strong>one parameter</strong> <span class="math inline">\(\rho\)</span> needs to be estimated (in addition to <span class="math inline">\(\sigma_u^2\)</span>), greatly simplifying the structure of <span class="math inline">\(\boldsymbol{\Omega}\)</span>.</p>
<hr>
<p><strong>OLS Properties Under AR(1)</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Consistency</strong>: If assumptions A1, A2, A3a, and A5a hold, OLS remains consistent.</li>
<li>
<strong>Asymptotic Normality</strong>: OLS estimates are asymptotically normal.</li>
<li>
<strong>Inference with Serial Correlation</strong>:
<ul>
<li>Standard OLS errors are invalid.</li>
<li>Use <strong>Newey-West standard errors</strong> to obtain robust inference.</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="infeasible-cochrane-orcutt" class="section level5" number="5.2.2.2.3">
<h5>
<span class="header-section-number">5.2.2.2.3</span> Infeasible Cochrane-Orcutt<a class="anchor" aria-label="anchor" href="#infeasible-cochrane-orcutt"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>Infeasible</strong> <strong>Cochrane-Orcutt</strong> procedure addresses serial correlation in the error terms by assuming an <strong>AR(1)</strong> process for the errors:</p>
<p><span class="math display">\[
\epsilon_t = \rho \epsilon_{t-1} + u_t,
\]</span></p>
<p>where <span class="math inline">\(u_t\)</span> is white noise and <span class="math inline">\(\rho\)</span> is the autocorrelation coefficient.</p>
<p>By transforming the original regression equation:</p>
<p><span class="math display">\[
y_t = \beta_0 + x_t\beta_1 + \epsilon_t,
\]</span></p>
<p>we subtract <span class="math inline">\(\rho\)</span> times the lagged equation:</p>
<p><span class="math display">\[
\rho y_{t-1} = \rho (\beta_0 + x_{t-1}\beta_1 + \epsilon_{t-1}),
\]</span></p>
<p>to obtain the weighted first-difference equation:</p>
<p><span class="math display">\[
y_t - \rho y_{t-1} = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t.
\]</span></p>
<p>Key Points:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Dependent Variable</strong>: <span class="math inline">\(y_t - \rho y_{t-1}\)</span>.</li>
<li>
<strong>Independent Variable</strong>: <span class="math inline">\(x_t - \rho x_{t-1}\)</span>.</li>
<li>
<strong>Error Term</strong>: <span class="math inline">\(u_t\)</span>, which satisfies the Gauss-Markov assumptions (A3, A4, A5).</li>
</ol>
<p>The ICO estimator minimizes the sum of squared residuals for this transformed equation.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Standard Errors</strong>:
<ul>
<li>If the errors truly follow an AR(1) process, the standard errors for the transformed equation are valid.</li>
<li>For more complex error structures, <strong>Newey-West HAC standard errors</strong> are required.</li>
</ul>
</li>
<li>
<strong>Loss of Observations</strong>:
<ul>
<li>The transformation involves first differences, which means the first observation (<span class="math inline">\(y_1\)</span>) cannot be used. This reduces the effective sample size by one.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>The Problem:</strong> <span class="math inline">\(\rho\)</span> <strong>Is Unknown</strong></p>
<p>The ICO procedure is <strong>infeasible</strong> because it requires knowledge of <span class="math inline">\(\rho\)</span>, the autocorrelation coefficient. In practice, we estimate <span class="math inline">\(\rho\)</span> from the data.</p>
<p>To estimate <span class="math inline">\(\rho\)</span>, we use the OLS residuals (<span class="math inline">\(e_t\)</span>) as a proxy for the errors (<span class="math inline">\(\epsilon_t\)</span>). The estimate <span class="math inline">\(\hat{\rho}\)</span> is given by:</p>
<p><span class="math display">\[
\hat{\rho} = \frac{\sum_{t=2}^{T} e_t e_{t-1}}{\sum_{t=2}^{T} e_t^2}.
\]</span></p>
<p>Estimation via OLS:</p>
<ol style="list-style-type: decimal">
<li>Regress the OLS residuals <span class="math inline">\(e_t\)</span> on their lagged values <span class="math inline">\(e_{t-1}\)</span>, without an intercept: <span class="math display">\[
e_t = \rho e_{t-1} + u_t.
\]</span>
</li>
<li>The slope of this regression is the estimate <span class="math inline">\(\hat{\rho}\)</span>.</li>
</ol>
<p>This estimation is efficient under the AR(1) assumption and provides a practical approximation for <span class="math inline">\(\rho\)</span>.</p>
<hr>
</div>
<div id="feasiable-prais-winsten" class="section level5" number="5.2.2.2.4">
<h5>
<span class="header-section-number">5.2.2.2.4</span> Feasible Prais-Winsten<a class="anchor" aria-label="anchor" href="#feasiable-prais-winsten"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>Feasible Prais-Winsten (FPW)</strong> method addresses <strong>AR(1) serial correlation</strong> in regression models by transforming the data to eliminate serial dependence in the errors. Unlike the <a href="linear-regression.html#infeasible-cochrane-orcutt">Infeasible Cochrane-Orcutt</a> procedure, which discards the first observation, the Prais-Winsten method retains it using a weighted transformation.</p>
<p>The FPW transformation uses the following weighting matrix <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[
\mathbf{w} =
\begin{pmatrix}
\sqrt{1 - \hat{\rho}^2} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
-\hat{\rho} &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; -\hat{\rho} &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; -\hat{\rho} &amp; 1
\end{pmatrix}.
\]</span></p>
<p>where</p>
<ul>
<li>The first row accounts for the transformation of the first observation, using <span class="math inline">\(\sqrt{1 - \hat{\rho}^2}\)</span>.</li>
<li>Subsequent rows represent the AR(1) transformation for the remaining observations.</li>
</ul>
<hr>
<p><strong>Step-by-Step Procedure</strong></p>
<p>Step 1: Initial OLS Estimation</p>
<p>Estimate the regression model using OLS:</p>
<p><span class="math display">\[
y_t = \mathbf{x}_t \beta + \epsilon_t,
\]</span></p>
<p>and compute the residuals:</p>
<p><span class="math display">\[
e_t = y_t - \mathbf{x}_t \hat{\beta}.
\]</span></p>
<hr>
<p>Step 2: Estimate the AR(1) Correlation Coefficient</p>
<p>Estimate the AR(1) correlation coefficient <span class="math inline">\(\rho\)</span> by regressing <span class="math inline">\(e_t\)</span> on <span class="math inline">\(e_{t-1}\)</span> without an intercept:</p>
<p><span class="math display">\[
e_t = \rho e_{t-1} + u_t.
\]</span></p>
<p>The slope of this regression is the estimated <span class="math inline">\(\hat{\rho}\)</span>.</p>
<hr>
<p>Step 3: Transform the Data</p>
<p>Apply the transformation using the weighting matrix <span class="math inline">\(\mathbf{w}\)</span> to transform both the dependent variable <span class="math inline">\(\mathbf{y}\)</span> and the independent variables <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><span class="math display">\[
\mathbf{wy} = \mathbf{wX} \beta + \mathbf{w\epsilon}.
\]</span></p>
<p>Specifically: 1. For <span class="math inline">\(t=1\)</span>, the transformed dependent and independent variables are: <span class="math display">\[
   \tilde{y}_1 = \sqrt{1 - \hat{\rho}^2} \cdot y_1, \quad \tilde{\mathbf{x}}_1 = \sqrt{1 - \hat{\rho}^2} \cdot \mathbf{x}_1.
   \]</span> 2. For <span class="math inline">\(t=2, \dots, T\)</span>, the transformed variables are: <span class="math display">\[
   \tilde{y}_t = y_t - \hat{\rho} \cdot y_{t-1}, \quad \tilde{\mathbf{x}}_t = \mathbf{x}_t - \hat{\rho} \cdot \mathbf{x}_{t-1}.
   \]</span></p>
<hr>
<p>Step 4: Feasible Prais-Winsten Estimation</p>
<p>Run OLS on the transformed equation:</p>
<p><span class="math display">\[
\mathbf{wy} = \mathbf{wX} \beta + \mathbf{w\epsilon}.
\]</span></p>
<p>The resulting estimator is the <strong>Feasible Prais-Winsten (FPW) estimator</strong>:</p>
<p><span class="math display">\[
\hat{\beta}_{FPW} = (\mathbf{X}'\mathbf{w}'\mathbf{w}\mathbf{X})^{-1} \mathbf{X}'\mathbf{w}'\mathbf{w}\mathbf{y}.
\]</span></p>
<hr>
<p>Properties of Feasible <a href="linear-regression.html#feasiable-prais-winsten">Prais-Winsten</a> Estimator</p>
<ol style="list-style-type: decimal">
<li>
<strong>Infeasible Prais-Winsten Estimator</strong>:
<ul>
<li>The infeasible Prais-Winsten (PW) estimator assumes the AR(1) parameter <span class="math inline">\(\rho\)</span> is known.</li>
<li>Under assumptions <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, and <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> for the unweighted equation, the infeasible PW estimator is <strong>unbiased</strong> and efficient.</li>
</ul>
</li>
<li>
<strong>Feasible Prais-Winsten (FPW) Estimator</strong>: The FPW estimator replaces the unknown <span class="math inline">\(\rho\)</span> with an estimate <span class="math inline">\(\hat{\rho}\)</span> derived from the OLS residuals, introducing bias in small samples.
<ol style="list-style-type: decimal">
<li>
<strong>Bias</strong>:
<ul>
<li>The FPW estimator is <strong>biased</strong> due to the estimation of <span class="math inline">\(\hat{\rho}\)</span>, which introduces an additional layer of approximation.</li>
</ul>
</li>
<li>
<strong>Consistency</strong>:
<ol style="list-style-type: decimal">
<li>The FPW estimator is <strong>consistent</strong> under the following assumptions:
<ul>
<li>
<a href="linear-regression.html#a1-linearity">A1</a>: The model is linear in parameters.</li>
<li>
<a href="linear-regression.html#a2-full-rank">A2</a>: The independent variables are linearly independent.</li>
<li>
<a href="linear-regression.html#a5-data-generation-random-sampling">A5</a>: The data is generated through random sampling.</li>
<li>Additionally: <span class="math display">\[
E\big((\mathbf{x_t - \rho x_{t-1}})'\big(\epsilon_t - \rho \epsilon_{t-1}\big)\big) = 0.
\]</span> This condition ensures the transformed error term <span class="math inline">\(\epsilon_t - \rho \epsilon_{t-1}\)</span> is uncorrelated with the transformed regressors <span class="math inline">\(\mathbf{x_t - \rho x_{t-1}}\)</span>.</li>
</ul>
</li>
<li>
<strong>Note</strong>: <a href="linear-regression.html#a3a-weak-exogeneity">A3a</a> (zero conditional mean of the error term, <span class="math inline">\(E(\epsilon_t|\mathbf{x}_t) = 0\)</span>) is <strong>not sufficient</strong> for the above condition. Full exogeneity of the independent variables (<a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a>) is required.</li>
</ol>
</li>
<li>
<strong>Efficiency</strong>
<ol style="list-style-type: decimal">
<li>
<strong>Asymptotic Efficiency</strong>: The FPW estimator is asymptotically <strong>more efficient than OLS</strong> if the errors are truly generated by an AR(1) process: <span class="math display">\[
\epsilon_t = \rho \epsilon_{t-1} + u_t, \quad Var(u_t) = \sigma^2.
\]</span>
</li>
<li>
<strong>Standard Errors</strong>:
<ol style="list-style-type: decimal">
<li>
<strong>Usual Standard Errors</strong>: If the errors are correctly specified as an AR(1) process, the usual standard errors from FPW are valid.</li>
<li>
<strong>Robust Standard Errors</strong>: If there is concern about a more complex dependence structure (e.g., higher-order autocorrelation or heteroskedasticity), use <strong>Newey-West Standard Errors</strong> for inference. These are robust to both serial correlation and heteroskedasticity.</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
</div>
</div>
<div id="cluster-errors" class="section level4" number="5.2.2.3">
<h4>
<span class="header-section-number">5.2.2.3</span> Cluster Errors<a class="anchor" aria-label="anchor" href="#cluster-errors"><i class="fas fa-link"></i></a>
</h4>
<p>Consider the regression model with clustered errors:</p>
<p><span class="math display">\[
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(g\)</span> indexes the group (e.g., households, firms, schools).</p></li>
<li><p><span class="math inline">\(i\)</span> indexes the individual within the group.</p></li>
</ul>
<p>The covariance structure for the errors <span class="math inline">\(\epsilon_{gi}\)</span> is defined as:</p>
<p><span class="math display">\[
Cov(\epsilon_{gi}, \epsilon_{hj})
\begin{cases}
= 0 &amp; \text{if } g \neq h \text{ (independent across groups)}, \\
\neq 0 &amp; \text{for any pair } (i,j) \text{ within group } g.
\end{cases}
\]</span></p>
<p>Within each group, individuals’ errors may be correlated (i.e., intra-group correlation), while errors are independent across groups. This violates <a href="linear-regression.html#a4-homoskedasticity">A4</a> (constant variance and no correlation of errors).</p>
<hr>
<p>Suppose there are three groups with varying sizes. The variance-covariance matrix <span class="math inline">\(\boldsymbol{\Omega}\)</span> for the errors <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is:</p>
<p><span class="math display">\[
Var(\boldsymbol{\epsilon}| \mathbf{X}) = \boldsymbol{\Omega} =
\begin{pmatrix}
\sigma^2 &amp; \delta_{12}^1 &amp; \delta_{13}^1 &amp; 0 &amp; 0 &amp; 0 \\
\delta_{12}^1 &amp; \sigma^2 &amp; \delta_{23}^1 &amp; 0 &amp; 0 &amp; 0 \\
\delta_{13}^1 &amp; \delta_{23}^1 &amp; \sigma^2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma^2 &amp; \delta_{12}^2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \delta_{12}^2 &amp; \sigma^2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma^2
\end{pmatrix}.
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\delta_{ij}^g = Cov(\epsilon_{gi}, \epsilon_{gj})\)</span> is the covariance between errors for individuals <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in group <span class="math inline">\(g\)</span>.</li>
<li>
<span class="math inline">\(Cov(\epsilon_{gi}, \epsilon_{hj}) = 0\)</span> for <span class="math inline">\(g \neq h\)</span> (independent groups).</li>
</ul>
<hr>
<p><strong>Infeasible Generalized Least Squares (Cluster)</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Assume Known Variance-Covariance Matrix</strong>: If <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\delta_{ij}^g\)</span> are known, construct <span class="math inline">\(\boldsymbol{\Omega}\)</span> and compute its inverse <span class="math inline">\(\boldsymbol{\Omega}^{-1}\)</span>.</p></li>
<li>
<p><strong>Infeasible GLS Estimator</strong>: The infeasible generalized least squares (IGLS) estimator is:</p>
<p><span class="math display">\[
\hat{\beta}_{IGLS} = (\mathbf{X}'\boldsymbol{\Omega}^{-1}\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\Omega}^{-1}\mathbf{y}.
\]</span></p>
</li>
</ol>
<p>Problem:</p>
<ul>
<li>We do not know <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\delta_{ij}^g\)</span>, making this approach <strong>infeasible</strong>.</li>
<li>Even if <span class="math inline">\(\boldsymbol{\Omega}\)</span> is estimated, incorrect assumptions about its structure may lead to invalid inference.</li>
</ul>
<hr>
<p>To make the estimation feasible, we assume a <strong>group-level random effects</strong> specification for the error:</p>
<p><span class="math display">\[
\begin{aligned}
y_{gi} &amp;= \mathbf{x}_{gi}\beta + c_g + u_{gi}, \\
Var(c_g|\mathbf{x}_i) &amp;= \sigma_c^2, \\
Var(u_{gi}|\mathbf{x}_i) &amp;= \sigma_u^2,
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(c_g\)</span> represents the <strong>group-level random effect</strong> (common shocks within each group, independent across groups).</p></li>
<li><p><span class="math inline">\(u_{gi}\)</span> represents the <strong>individual-level error</strong> (idiosyncratic shocks within each group, independent across individuals and groups).</p></li>
<li><p><span class="math inline">\(\epsilon_{gi} = c_g + u_{gi}\)</span></p></li>
</ul>
<p>Independence Assumptions:</p>
<ul>
<li>
<span class="math inline">\(c_g\)</span> and <span class="math inline">\(u_{gi}\)</span> are independent of each other.</li>
<li>Both are mean-independent of <span class="math inline">\(\mathbf{x}_i\)</span>.</li>
</ul>
<p>Under this specification, the variance-covariance matrix <span class="math inline">\(\boldsymbol{\Omega}\)</span> becomes block diagonal, where each block corresponds to a group:</p>
<p><span class="math display">\[
Var(\boldsymbol{\epsilon}| \mathbf{X}) = \boldsymbol{\Omega} =
\begin{pmatrix}
\sigma_c^2 + \sigma_u^2 &amp; \sigma_c^2 &amp; \sigma_c^2 &amp; 0 &amp; 0 &amp; 0 \\
\sigma_c^2 &amp; \sigma_c^2 + \sigma_u^2 &amp; \sigma_c^2 &amp; 0 &amp; 0 &amp; 0 \\
\sigma_c^2 &amp; \sigma_c^2  &amp; \sigma_c^2 + \sigma_u^2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_c^2 + \sigma_u^2 &amp; \sigma_c^2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_c^2 &amp; \sigma_c^2 + \sigma_u^2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma_c^2 + \sigma_u^2
\end{pmatrix}.
\]</span></p>
<p>When the variance components <span class="math inline">\(\sigma_c^2\)</span> and <span class="math inline">\(\sigma_u^2\)</span> are unknown, we can use the <strong>Feasible Group-Level Random Effects (RE)</strong> estimator to simultaneously estimate these variances and the regression coefficients <span class="math inline">\(\beta\)</span>. This practical approach allows us to account for intra-group correlation in the errors and still obtain consistent and efficient estimates of the parameters.</p>
<hr>
<p><strong>Step-by-Step Procedure</strong></p>
<p>Step 1: Initial OLS Estimation</p>
<p>Estimate the regression model using OLS:</p>
<p><span class="math display">\[ y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}, \]</span></p>
<p>and compute the residuals:</p>
<p><span class="math display">\[ e_{gi} = y_{gi} - \mathbf{x}_{gi}\hat{\beta}. \]</span></p>
<hr>
<p>Step 2: Estimate Variance Components</p>
<p>Use the standard OLS variance estimator <span class="math inline">\(s^2\)</span> to estimate the <strong>total variance</strong>:</p>
<p><span class="math display">\[ s^2 = \frac{1}{n - k} \sum_{i=1}^{n} e_i^2, \]</span></p>
<p>where <span class="math inline">\(n\)</span> is the total number of observations and <span class="math inline">\(k\)</span> is the number of regressors (including the intercept).</p>
<p>Estimate the <strong>between-group variance</strong> <span class="math inline">\(\hat{\sigma}_c^2\)</span> using:</p>
<p><span class="math display">\[ \hat{\sigma}_c^2 = \frac{1}{G} \sum_{g=1}^{G} \left( \frac{1}{\sum_{i=1}^{n_g - 1} i} \sum_{i \neq j} \sum_{j=1}^{n_g} e_{gi} e_{gj} \right), \]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(G\)</span> is the total number of groups,</p></li>
<li><p><span class="math inline">\(n_g\)</span> is the size of group <span class="math inline">\(g\)</span>,</p></li>
<li><p>The term <span class="math inline">\(\sum_{i \neq j} e_{gi} e_{gj}\)</span> accounts for within-group covariance.</p></li>
</ul>
<p>Estimate the <strong>within-group variance</strong> as:</p>
<p><span class="math display">\[ \hat{\sigma}_u^2 = s^2 - \hat{\sigma}_c^2. \]</span></p>
<hr>
<p>Step 3: Construct the Variance-Covariance Matrix</p>
<p>Use the estimated variances <span class="math inline">\(\hat{\sigma}_c^2\)</span> and <span class="math inline">\(\hat{\sigma}_u^2\)</span> to construct the variance-covariance matrix <span class="math inline">\(\hat{\Omega}\)</span> for the error term:</p>
<p><span class="math display">\[ \hat{\Omega}_{gi,gj} = \begin{cases} \hat{\sigma}_c^2 + \hat{\sigma}_u^2 &amp; \text{if } i = j \text{ (diagonal elements)}, \\ \hat{\sigma}_c^2 &amp; \text{if } i \neq j \text{ (off-diagonal elements within group)}, \\ 0 &amp; \text{if } g \neq h \text{ (across groups)}. \end{cases} \]</span></p>
<hr>
<p>Step 4: Feasible GLS Estimation</p>
<p>With <span class="math inline">\(\hat{\Omega}\)</span> in hand, perform Feasible Generalized Least Squares (FGLS) to estimate <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[ \hat{\beta}_{RE} = (\mathbf{X}'\hat{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}'\hat{\Omega}^{-1}\mathbf{y}. \]</span></p>
<p>If the assumptions about <span class="math inline">\(\boldsymbol{\Omega}\)</span> are incorrect or infeasible, use <strong>cluster-robust standard errors</strong> to account for intra-group correlation without explicitly modeling the variance-covariance structure. These standard errors remain valid under arbitrary within-cluster dependence, provided clusters are independent.</p>
<hr>
<p><strong>Properties of the Feasible Group-Level Random Effects Estimator</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Infeasible Group RE Estimator</strong></li>
</ol>
<ul>
<li>The infeasible RE estimator (assuming known variances) is <strong>unbiased</strong> under assumptions <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, and <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> for the unweighted equation.</li>
<li>
<a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> requires: <span class="math display">\[ E(\epsilon_{gi}|\mathbf{x}_i) = E(c_g|\mathbf{x}_i) + E(u_{gi}|\mathbf{x}_i) = 0. \]</span> This assumes:
<ul>
<li>
<span class="math inline">\(E(c_g|\mathbf{x}_i) = 0\)</span>: The <strong>random effects assumption</strong> (group-level effects are uncorrelated with the regressors).</li>
<li>
<span class="math inline">\(E(u_{gi}|\mathbf{x}_i) = 0\)</span>: No endogeneity at the individual level.</li>
</ul>
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Feasible Group RE Estimator</strong></li>
</ol>
<ul>
<li>The feasible RE estimator is <strong>biased</strong> because the variances <span class="math inline">\(\sigma_c^2\)</span> and <span class="math inline">\(\sigma_u^2\)</span> are estimated, introducing approximation errors.</li>
<li>However, the estimator is <strong>consistent</strong> under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, <a href="linear-regression.html#a3a-weak-exogeneity">A3a</a> (<span class="math inline">\(E(\mathbf{x}_i'\epsilon_{gi}) = E(\mathbf{x}_i'c_g) + E(\mathbf{x}_i'u_{gi}) = 0\)</span>), <a href="#a5a-random-sampling">A5a</a>.</li>
<li>
<strong>Efficiency</strong>
<ul>
<li>
<strong>Asymptotic Efficiency</strong>:
<ul>
<li>The feasible RE estimator is asymptotically more efficient than OLS if the errors follow the random effects specification.</li>
</ul>
</li>
<li>
<strong>Standard Errors</strong>:
<ul>
<li>If the random effects specification is correct, the usual standard errors are consistent.</li>
<li>If there is concern about more complex dependence structures or heteroskedasticity, use <strong>cluster robust standard errors</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="weighted-least-squares" class="section level3" number="5.2.3">
<h3>
<span class="header-section-number">5.2.3</span> Weighted Least Squares<a class="anchor" aria-label="anchor" href="#weighted-least-squares"><i class="fas fa-link"></i></a>
</h3>
<p>In the presence of heteroskedasticity, the errors <span class="math inline">\(\epsilon_i\)</span> have non-constant variance <span class="math inline">\(Var(\epsilon_i|\mathbf{x}_i) = \sigma_i^2\)</span>. This violates the Gauss-Markov assumption of homoskedasticity, leading to inefficient OLS estimates.</p>
<p><strong>Weighted Least Squares (WLS)</strong> addresses this by applying weights inversely proportional to the variance of the errors, ensuring that observations with larger variances have less influence on the estimation.</p>
<ul>
<li>
<p>Weighted Least Squares is essentially <a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a> in the special case that <span class="math inline">\(\mathbf{\Omega}\)</span> is a <em>diagonal</em> matrix with variances <span class="math inline">\(\sigma_i^2\)</span> on the diagonal (i.e., errors are uncorrelated but have non-constant variance).</p>
<ul>
<li><p>That is, assume the errors are uncorrelated but <strong>heteroskedastic</strong>: <span class="math inline">\(\mathbf{\Omega} = \text{diag}\bigl(\sigma_1^2, \ldots, \sigma_n^2\bigr)\)</span></p></li>
<li><p>Then <span class="math inline">\(\mathbf{\Omega}^{-1} = \text{diag}\bigl(1/\sigma_1^2, \ldots, 1/\sigma_n^2\bigr)\)</span></p></li>
</ul>
</li>
</ul>
<p><strong>Steps for Feasible Weighted Least Squares (FWLS)</strong></p>
<p>1. Initial OLS Estimation</p>
<p>First, estimate the model using OLS:</p>
<p><span class="math display">\[ y_i = \mathbf{x}_i\beta + \epsilon_i, \]</span></p>
<p>and compute the residuals:</p>
<p><span class="math display">\[ e_i = y_i - \mathbf{x}_i \hat{\beta}. \]</span></p>
<p>2. Model the Error Variance</p>
<p>Transform the residuals to model the variance as a function of the predictors:</p>
<p><span class="math display">\[ \ln(e_i^2) = \mathbf{x}_i\gamma + \ln(v_i), \]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(e_i^2\)</span> approximates <span class="math inline">\(\epsilon_i^2\)</span>,</p></li>
<li><p><span class="math inline">\(\ln(v_i)\)</span> is the error term in this auxiliary regression, assumed independent of <span class="math inline">\(\mathbf{x}_i\)</span>.</p></li>
</ul>
<p>Estimate this equation using OLS to obtain the predicted values:</p>
<p><span class="math display">\[ \hat{g}_i = \mathbf{x}_i \hat{\gamma}. \]</span></p>
<p>3. Estimate Weights</p>
<p>Use the predicted values from the auxiliary regression to compute the weights:</p>
<p><span class="math display">\[ \hat{\sigma}_i = \sqrt{\exp(\hat{g}_i)}. \]</span></p>
<p>These weights approximate the standard deviation of the errors.</p>
<p>4. Weighted Regression</p>
<p>Transform the original equation by dividing through by <span class="math inline">\(\hat{\sigma}_i\)</span>:</p>
<p><span class="math display">\[ \frac{y_i}{\hat{\sigma}_i} = \frac{\mathbf{x}_i}{\hat{\sigma}_i}\beta + \frac{\epsilon_i}{\hat{\sigma}_i}. \]</span></p>
<p>Estimate the transformed equation using OLS. The resulting estimator is the <strong>Feasible Weighted Least Squares (FWLS)</strong> estimator:</p>
<p><span class="math display">\[ \hat{\beta}_{FWLS} = (\mathbf{X}'\mathbf{\hat{W}}\mathbf{X})^{-1}\mathbf{X}'\mathbf{\hat{W}}\mathbf{y}, \]</span></p>
<p>where <span class="math inline">\(\mathbf{\hat{W}}\)</span> is a diagonal weight matrix with elements <span class="math inline">\(1/\hat{\sigma}_i^2\)</span>.</p>
<hr>
<p><strong>Properties of the FWLS Estimator</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Unbiasedness</strong>:
<ul>
<li>The infeasible WLS estimator (where <span class="math inline">\(\sigma_i\)</span> is known) is unbiased under assumptions A1-A3 for the unweighted model.</li>
<li>The <strong>FWLS estimator</strong> is <strong>not unbiased</strong> due to the approximation of <span class="math inline">\(\sigma_i\)</span> using <span class="math inline">\(\hat{\sigma}_i\)</span>.</li>
</ul>
</li>
<li>
<strong>Consistency</strong>:
<ul>
<li>The FWLS estimator is <strong>consistent</strong> under the following assumptions:
<ul>
<li>
<a href="linear-regression.html#a1-linearity">A1</a> (for the unweighted equation): The model is linear in parameters.</li>
<li>
<a href="linear-regression.html#a2-full-rank">A2</a> (for the unweighted equation): The independent variables are linearly independent.</li>
<li>
<a href="linear-regression.html#a5-data-generation-random-sampling">A5</a>: The data is randomly sampled.</li>
<li>
<span class="math inline">\(E(\mathbf{x}_i'\epsilon_i/\sigma_i^2) = 0\)</span>. <a href="linear-regression.html#a3a-weak-exogeneity">A3a: Weaker Exogeneity Assumption</a> is not sufficient, but <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> is.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Efficiency</strong>:
<ul>
<li>The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity: <span class="math display">\[ Var(\epsilon_i|\mathbf{x}_i) = \sigma_i^2 = \exp(\mathbf{x}_i\gamma). \]</span>
</li>
</ul>
</li>
</ol>
<p>The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Usual Standard Errors</strong>:
<ul>
<li>If the errors are truly multiplicative exponential heteroskedastic, the usual standard errors for FWLS are valid.</li>
</ul>
</li>
<li>
<strong>Heteroskedastic Robust Standard Errors</strong>:
<ul>
<li>If there is potential mis-specification of the multiplicative exponential model for <span class="math inline">\(\sigma_i^2\)</span>, heteroskedastic-robust standard errors should be reported to ensure valid inference.</li>
</ul>
</li>
</ol>
</div>
</div>
<div id="maximum-likelihood-estimator" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Maximum Likelihood<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimator"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>Maximum Likelihood Estimation (MLE)</strong> is a statistical method used to estimate the parameters of a model by maximizing the likelihood of observing the given data. The premise is to find the parameter values that maximize the probability (or likelihood) of the observed data.</p>
<p>The likelihood function, denoted as <span class="math inline">\(L(\theta)\)</span>, is expressed as:</p>
<p><span class="math display">\[
L(\theta) = \prod_{i=1}^{n} f(y_i|\theta)
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(f(y|\theta)\)</span> is the probability density or mass function of observing a single value of <span class="math inline">\(Y\)</span> given the parameter <span class="math inline">\(\theta\)</span>.</li>
<li>The product runs over all <span class="math inline">\(n\)</span> observations.</li>
</ul>
<p>For different types of data, <span class="math inline">\(f(y|\theta)\)</span> can take different forms. For example, if <span class="math inline">\(y\)</span> is dichotomous (e.g., success/failure), then the likelihood function becomes:</p>
<p><span class="math display">\[
L(\theta) = \prod_{i=1}^{n} \theta^{y_i} (1-\theta)^{1-y_i}
\]</span></p>
<p>Here, <span class="math inline">\(\hat{\theta}\)</span> is the Maximum Likelihood Estimator (MLE) if:</p>
<p><span class="math display">\[
L(\hat{\theta}) &gt; L(\theta_0), \quad \forall \theta_0 \text{ in the parameter space.}
\]</span></p>
<p>See <a href="prerequisites.html#distributions">Distributions</a> for a review on variable distributions.</p>
<div id="motivation-for-mle" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> Motivation for MLE<a class="anchor" aria-label="anchor" href="#motivation-for-mle"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we know the <strong>conditional distribution</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, denoted as:</p>
<p><span class="math display">\[
f_{Y|X}(y, x; \theta)
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is an unknown parameter of the distribution. Sometimes, we are only concerned with the unconditional distribution <span class="math inline">\(f_Y(y; \theta)\)</span>.</p>
<p>For a sample of independent and identically distributed (i.i.d.) data, the joint probability of the sample is:</p>
<p><span class="math display">\[
f_{Y_1, \ldots, Y_n|X_1, \ldots, X_n}(y_1, \ldots, y_n, x_1, \ldots, x_n; \theta) = \prod_{i=1}^{n} f_{Y|X}(y_i, x_i; \theta)
\]</span></p>
<p>The <strong>joint distribution</strong>, evaluated at the observed data, defines the likelihood function. The goal of MLE is to find the parameter <span class="math inline">\(\theta\)</span> that maximizes this likelihood.</p>
<p>To estimate <span class="math inline">\(\theta\)</span>, we maximize the likelihood function:</p>
<p><span class="math display">\[
\max_{\theta} \prod_{i=1}^{n} f_{Y|X}(y_i, x_i; \theta)
\]</span></p>
<p>In practice, it is easier to work with the natural logarithm of the likelihood (log-likelihood), as it transforms the product into a sum:</p>
<p><span class="math display">\[
\max_{\theta} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \theta))
\]</span></p>
<hr>
<p>Solving for the Maximum Likelihood Estimator</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>First-Order Condition</strong>: Solve the first derivative of the log-likelihood function with respect to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \theta} \ell(\theta) \;=\; \frac{\partial}{\partial \theta} \ln L(\theta) \;=\; \frac{\partial}{\partial \theta} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \hat{\theta}_{MLE})) = 0
\]</span></p>
<p>This yields the critical points where the likelihood is maximized. This derivative, sometimes written as <span class="math inline">\(U(\theta)\)</span>, is called the <strong>score</strong>. Intuitively, the log-likelihood’s “peak” indicates the parameter value(s) that make the observed data “most likely.”</p>
</li>
<li>
<p><strong>Second-Order Condition</strong>: Verify that the second derivative of the log-likelihood function is negative at the critical point:</p>
<p><span class="math display">\[
\frac{\partial^2}{\partial \theta^2} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \hat{\theta}_{MLE})) &lt; 0
\]</span></p>
<p>This ensures that the solution corresponds to a maximum.</p>
</li>
</ol>
<hr>
<p>Examples of Likelihood Functions</p>
<ol style="list-style-type: decimal">
<li>Unconditional <a href="prerequisites.html#poisson-distribution">Poisson Distribution</a>
</li>
</ol>
<p>The Poisson distribution models count data, such as the number of website visits in a day or product orders per hour. Its likelihood function is:</p>
<p><span class="math display">\[
L(\theta) = \prod_{i=1}^{n} \frac{\theta^{y_i} e^{-\theta}}{y_i!}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><a href="prerequisites.html#exponential-distribution">Exponential Distribution</a></li>
</ol>
<p>The exponential distribution is often used to model the time between events, such as the time until a machine fails. Its probability density function (PDF) is:</p>
<p><span class="math display">\[
f_{Y|X}(y, x; \theta) = \frac{\exp(-y / (x \theta))}{x \theta}
\]</span></p>
<p>The joint likelihood for <span class="math inline">\(n\)</span> observations is:</p>
<p><span class="math display">\[
L(\theta) = \prod_{i=1}^{n} \frac{\exp(-y_i / (x_i \theta))}{x_i \theta}
\]</span></p>
<p>By taking the logarithm, we obtain the log-likelihood for ease of maximization.</p>
</div>
<div id="key-quantities-for-inference" class="section level3" number="5.3.2">
<h3>
<span class="header-section-number">5.3.2</span> Key Quantities for Inference<a class="anchor" aria-label="anchor" href="#key-quantities-for-inference"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p><strong>Score Function</strong><br>
The <strong>score</strong> is given by<br><span class="math display">\[
U(\theta) \;=\; \frac{d}{d\theta} \ell(\theta).
\]</span><br>
Setting <span class="math inline">\(U(\hat{\theta}_{\mathrm{MLE}}) = 0\)</span> yields the critical points of the log-likelihood, from which we can find <span class="math inline">\(\hat{\theta}_{\mathrm{MLE}}\)</span>.</p></li>
<li>
<p><strong>Observed Information</strong><br>
The second derivative of the log-likelihood, taken at the MLE, is called the <strong>observed information</strong>:</p>
<p><span class="math display">\[
I_O(\theta) \;=\; - \frac{d^2}{d\theta^2} \ell(\theta).
\]</span></p>
<p>(The negative sign is often included so that <span class="math inline">\(I_O(\theta)\)</span> is <em>positive</em> if <span class="math inline">\(\ell(\theta)\)</span> is concave near its maximum. In some texts, you will see it defined without the negative sign, but the idea is the same: it measures the “pointedness” or curvature of <span class="math inline">\(\ell(\theta)\)</span> at its maximum.)</p>
</li>
<li>
<p><strong>Fisher Information</strong><br>
The <strong>Fisher Information</strong> (or <strong>expected information</strong>) is the expectation of the observed information over the distribution of the data:</p>
<p><span class="math display">\[
I(\theta) \;=\; \mathbb{E}\bigl[I_O(\theta)\bigr].
\]</span></p>
<p>It quantifies how much information the data carry about the parameter <span class="math inline">\(\theta\)</span>. A larger Fisher information suggests that you can estimate <span class="math inline">\(\theta\)</span> more precisely.</p>
</li>
<li>
<p><strong>Approximate Variance of</strong> <span class="math inline">\(\hat{\theta}_{\mathrm{MLE}}\)</span><br>
One of the key results from standard asymptotic theory is that, for large <span class="math inline">\(n\)</span>, the variance of <span class="math inline">\(\hat{\theta}_{\mathrm{MLE}}\)</span> can be approximated by the inverse of the Fisher information:</p>
<p><span class="math display">\[
\mathrm{Var}\bigl(\hat{\theta}_{\mathrm{MLE}}\bigr) \;\approx\; I(\theta)^{-1}.
\]</span></p>
<p>This also lays the groundwork for constructing confidence intervals for <span class="math inline">\(\theta\)</span> in large samples.</p>
</li>
</ol>
<hr>
</div>
<div id="assumptions-of-mle" class="section level3" number="5.3.3">
<h3>
<span class="header-section-number">5.3.3</span> Assumptions of MLE<a class="anchor" aria-label="anchor" href="#assumptions-of-mle"><i class="fas fa-link"></i></a>
</h3>
<p>MLE has desirable properties—<em>consistency</em>, <em>asymptotic normality</em>, and <em>efficiency</em>—but these do not come “for free.” Instead, they rely on certain assumptions. Below is a breakdown of the main <strong>regularity conditions</strong>. These conditions are typically mild in many practical settings (for example, in exponential families, such as the normal distribution), but need to be checked in more complex models.</p>
<p><strong>High-Level Regulatory Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Independence and Identical Distribution (iid)</strong><br>
The sample <span class="math inline">\(\{(x_i, y_i)\}\)</span> is usually assumed to be composed of independent and identically distributed observations. This independence assumption simplifies the likelihood to a product of individual densities: <span class="math display">\[
L(\theta) = \prod_{i=1}^n f_{Y\mid X}(y_i, x_i; \theta).
\]</span> In practice, if you have dependent data (e.g., time series, spatial data), modifications are required in the likelihood function.</p></li>
<li><p><strong>Same Density Function</strong><br>
All observations must come from the <em>same</em> conditional probability density function <span class="math inline">\(f_{Y\mid X}(\cdot,\cdot;\theta)\)</span>. If the model changes across observations, you cannot simply multiply all of them together in one unified likelihood.</p></li>
<li><p><strong>Multivariate Normality (for certain models)</strong><br>
In many practical cases—especially for continuous outcomes—you might assume (multivariate) normal distributions with finite second or fourth moments <span class="citation">(<a href="references.html#ref-little1988test">Little 1988</a>)</span>. Under these assumptions, the MLE for the mean vector and covariance matrix is consistent and (under further conditions) asymptotically normal. This assumption is quite common in regression, ANOVA, and other classical statistical frameworks.</p></li>
</ol>
<hr>
<div id="large-sample-properties-of-mle" class="section level4" number="5.3.3.1">
<h4>
<span class="header-section-number">5.3.3.1</span> Large Sample Properties of MLE<a class="anchor" aria-label="anchor" href="#large-sample-properties-of-mle"><i class="fas fa-link"></i></a>
</h4>
<div id="consistency-of-mle" class="section level5" number="5.3.3.1.1">
<h5>
<span class="header-section-number">5.3.3.1.1</span> Consistency of MLE<a class="anchor" aria-label="anchor" href="#consistency-of-mle"><i class="fas fa-link"></i></a>
</h5>
<p><strong>Definition:</strong> An estimator <span class="math inline">\(\hat{\theta}_n\)</span> is <em>consistent</em> if it converges in probability to the true parameter value <span class="math inline">\(\theta_0\)</span> as the sample size <span class="math inline">\(n \to \infty\)</span>:</p>
<p><span class="math display">\[
\hat{\theta}_n \;\to^p\; \theta_0.
\]</span></p>
<p>For the MLE, a set of regularity conditions <span class="math inline">\(R1\)</span>–<span class="math inline">\(R4\)</span> is commonly used to ensure consistency:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>R1</strong><br>
If <span class="math inline">\(\theta \neq \theta_0\)</span>, then<br><span class="math display">\[
f_{Y\mid X}(y_i, x_i; \theta) \;\neq\; f_{Y\mid X}(y_i, x_i; \theta_0).
\]</span></p>
<p>In simpler terms, the model is identifiable: no two distinct parameter values generate the <em>exact</em> same distribution for the data.</p>
</li>
<li><p><strong>R2</strong><br>
The parameter space <span class="math inline">\(\Theta\)</span> is compact (closed and bounded), and it contains the true parameter <span class="math inline">\(\theta_0\)</span>. This ensures that <span class="math inline">\(\theta\)</span> lies in a “nice” region (no parameter going to infinity, etc.), making it easier to prove that a maximum in that space indeed exists.</p></li>
<li><p><strong>R3</strong><br>
The log-likelihood function <span class="math inline">\(\ln(f_{Y\mid X}(y_i, x_i; \theta))\)</span> is continuous in <span class="math inline">\(\theta\)</span> with probability <span class="math inline">\(1\)</span>. Continuity is important so that we can apply theorems (like the Continuous Mapping Theorem or the Extreme Value Theorem) to find maxima.</p></li>
<li>
<p><strong>R4</strong><br>
The expected supremum of the absolute value of the log-likelihood is finite:</p>
<p><span class="math display">\[
\mathbb{E}\!\Bigl(\sup_{\theta \in \Theta} \bigl|\ln(f_{Y\mid X}(y_i, x_i; \theta))\bigr|\Bigr) \;&lt;\;\infty.
\]</span></p>
<p>This is a technical condition that helps ensure we can “exchange” expectations and suprema, a step needed in many consistency proofs.</p>
</li>
</ol>
<p>When these conditions are satisfied, you can show via standard arguments (e.g., the <a href="prerequisites.html#law-of-large-numbers">Law of Large Numbers</a>, uniform convergence of the log-likelihood) that:</p>
<p><span class="math display">\[
\hat{\theta}_{\mathrm{MLE}} \;\to^p\; \theta_0 \quad (\text{consistency}).
\]</span></p>
<hr>
</div>
<div id="asymptotic-normality-of-mle" class="section level5" number="5.3.3.1.2">
<h5>
<span class="header-section-number">5.3.3.1.2</span> Asymptotic Normality of MLE<a class="anchor" aria-label="anchor" href="#asymptotic-normality-of-mle"><i class="fas fa-link"></i></a>
</h5>
<p><strong>Definition:</strong> An estimator <span class="math inline">\(\hat{\theta}_n\)</span> is <em>asymptotically normal</em> if</p>
<p><span class="math display">\[
\sqrt{n}\,(\hat{\theta}_n - \theta_0) \;\to^d\; \mathcal{N}\bigl(0,\Sigma\bigr),
\]</span></p>
<p>where <span class="math inline">\(\to^d\)</span> denotes convergence in distribution and <span class="math inline">\(\Sigma\)</span> is some covariance matrix. For the MLE, <span class="math inline">\(\Sigma\)</span> is typically <span class="math inline">\(I(\theta_0)^{-1}\)</span>, where <span class="math inline">\(I(\theta_0)\)</span> is the Fisher information evaluated at the true parameter.</p>
<p>Beyond <span class="math inline">\(R1\)</span>–<span class="math inline">\(R4\)</span>, we need the following additional assumptions:</p>
<ol style="list-style-type: decimal">
<li><p><strong>R5</strong><br>
The true parameter <span class="math inline">\(\theta_0\)</span> is in the <em>interior</em> of the parameter space <span class="math inline">\(\Theta\)</span>. If <span class="math inline">\(\theta_0\)</span> sits on the boundary, different arguments are required to handle edge effects.</p></li>
<li><p><strong>R6</strong><br>
The pdf <span class="math inline">\(f_{Y\mid X}(y_i, x_i; \theta)\)</span> is <em>twice</em> continuously differentiable (in <span class="math inline">\(\theta\)</span>) and strictly positive in a neighborhood <span class="math inline">\(N\)</span> of <span class="math inline">\(\theta_0\)</span>. This allows us to use second-order Taylor expansions around <span class="math inline">\(\theta_0\)</span> to get the approximate distribution of <span class="math inline">\(\hat{\theta}_{\mathrm{MLE}}\)</span>.</p></li>
<li>
<p><strong>R7</strong><br>
The following integrals are finite in some neighborhood <span class="math inline">\(N\)</span> of <span class="math inline">\(\theta_0\)</span>:</p>
<ul>
<li>
<span class="math inline">\(\displaystyle \int \sup_{\theta \in N} \Bigl\|\partial f_{Y\mid X}(y_i, x_i; \theta)/\partial \theta \Bigr\|\; d(y,x) &lt; \infty\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \int \sup_{\theta \in N} \Bigl\|\partial^2 f_{Y\mid X}(y_i, x_i; \theta)/\partial \theta \partial \theta' \Bigr\|\; d(y,x) &lt; \infty\)</span>.</li>
<li>
<span class="math inline">\(\displaystyle \mathbb{E}\Bigl(\sup_{\theta \in N} \Bigl\|\partial^2 \ln(f_{Y\mid X}(y_i, x_i; \theta))/\partial \theta \partial \theta' \Bigr\|\Bigr) &lt; \infty\)</span>.</li>
</ul>
<p>These conditions ensure that differentiating inside integrals is justified (via the dominated convergence theorem) and that we can expand the log-likelihood in a Taylor series safely.</p>
</li>
<li>
<p><strong>R8</strong><br>
The information matrix <span class="math inline">\(I(\theta_0)\)</span> exists and is nonsingular:</p>
<p><span class="math display">\[
I(\theta_0) \;=\; \mathrm{Var}\Bigl(\frac{\partial}{\partial \theta} \ln\bigl(f_{Y\mid X}(y_i, x_i; \theta_0)\bigr)\Bigr) \;\neq\; 0.
\]</span></p>
<p>Nonsingularity implies there is enough information in the data to estimate <span class="math inline">\(\theta\)</span> uniquely.</p>
</li>
</ol>
<p>Under <span class="math inline">\(R1\)</span>–<span class="math inline">\(R8\)</span>, you can show that</p>
<p><span class="math display">\[
\sqrt{n}\,(\hat{\theta}_{\mathrm{MLE}} - \theta_0) \;\to^d\; \mathcal{N}\Bigl(0,\,I(\theta_0)^{-1}\Bigr).
\]</span></p>
<p>This result is central to frequentist inference, allowing you to construct approximate confidence intervals and hypothesis tests using the normal approximation for large <span class="math inline">\(n\)</span>.</p>
</div>
</div>
</div>
<div id="properties-of-mle" class="section level3" number="5.3.4">
<h3>
<span class="header-section-number">5.3.4</span> Properties of MLE<a class="anchor" aria-label="anchor" href="#properties-of-mle"><i class="fas fa-link"></i></a>
</h3>
<p>Having established in earlier sections that Maximum Likelihood Estimators (MLEs) are <strong>consistent</strong> (<a href="linear-regression.html#consistency-of-mle">Consistency of MLE</a>) and <strong>asymptotically normal</strong> (<a href="linear-regression.html#asymptotic-normality-of-mle">Asymptotic Normality of MLE</a>) under standard regularity conditions, we now highlight additional properties that make MLE a powerful estimation technique.</p>
<ol style="list-style-type: decimal">
<li><strong>Asymptotic Efficiency</strong></li>
</ol>
<ul>
<li>
<strong>Definition</strong>: An estimator is <em>asymptotically efficient</em> if it attains the smallest possible asymptotic variance among all consistent estimators (i.e., it achieves the <em>Cramér-Rao Lower Bound</em>).</li>
<li>
<strong>Interpretation</strong>: In large samples, MLE typically has smaller standard errors than other consistent estimators that do not fully use the assumed distributional form.</li>
<li>
<strong>Implication</strong>: When the true model is correctly specified, MLE is the <em>most efficient</em> among a broad class of estimators, leading to more precise inference for <span class="math inline">\(\theta\)</span>.
<ul>
<li>
<strong>Cramér-Rao Lower Bound (CRLB)</strong>: A theoretical lower limit on the variance of any unbiased (or asymptotically unbiased) estimator <span class="citation">C. R. Rao (<a href="references.html#ref-rao1992information">1992</a>)</span>.</li>
<li>
<strong>When MLE Meets CRLB</strong>: Under correct specification and standard regularity conditions, the asymptotic variance of the MLE matches the CRLB, making it <em>asymptotically efficient</em>.</li>
<li>
<strong>Interpretation</strong>: Achieving CRLB means no other unbiased estimator can consistently outperform MLE in terms of variance for large <span class="math inline">\(n\)</span>.</li>
</ul>
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Invariance</strong></li>
</ol>
<ul>
<li>
<strong>Core Idea</strong>: If <span class="math inline">\(\hat{\theta}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then for any <em>smooth</em> transformation <span class="math inline">\(g(\theta)\)</span>, the MLE for <span class="math inline">\(g(\theta)\)</span> is simply <span class="math inline">\(g(\hat{\theta})\)</span>.</li>
<li>
<strong>Example</strong>: If <span class="math inline">\(\theta\)</span> is a mean parameter and you want the MLE for the <em>variance</em> <span class="math inline">\(\theta^2\)</span>, you can just square the MLE for <span class="math inline">\(\theta\)</span>.</li>
<li>
<strong>Key Point</strong>: This <em>invariance property</em> saves considerable effort—there is no need to re-derive a new likelihood for the transformed parameter.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Explicit vs. Implicit MLE</li>
</ol>
<ul>
<li>
<strong>Explicit MLE</strong>:<br>
Occurs when the score equation can be solved in <em>closed form</em>. A classic example is the MLE for the mean and variance in a normal distribution.</li>
<li>
<strong>Implicit MLE</strong>:<br>
Happens when no closed-form solution exists. Iterative numerical methods, such as <strong>Newton-Raphson</strong>, <strong>Expectation-Maximization (EM)</strong>, or other optimization algorithms, are used to find <span class="math inline">\(\hat{\theta}\)</span>.</li>
</ul>
<hr>
<p><strong>Distributional Mis-Specification</strong></p>
<ul>
<li>
<strong>Definition</strong>: If you assume a distribution for <span class="math inline">\(f_{Y|X}(\cdot;\theta)\)</span> that does <em>not</em> reflect the true data-generating process, the MLE may become <em>inconsistent</em> or biased in finite samples.</li>
<li>
<strong>Quasi-MLE</strong>:
<ul>
<li>A strategy to handle certain forms of mis-specification.</li>
<li>If the chosen distribution belongs to a flexible class or meets certain conditions (e.g., generalized linear models with a robust link), the resulting parameter estimates can remain consistent <em>for some parameters of interest</em>.</li>
</ul>
</li>
<li>
<strong>Nonparametric &amp; Semiparametric Approaches</strong>:
<ul>
<li>Require minimal or no distributional assumptions.</li>
<li>More robust to mis-specification but can be <em>harder to implement</em> and may exhibit higher variance or require larger sample sizes to achieve comparable precision.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="practical-considerations" class="section level3" number="5.3.5">
<h3>
<span class="header-section-number">5.3.5</span> Practical Considerations<a class="anchor" aria-label="anchor" href="#practical-considerations"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<strong>Use Cases</strong>
<ul>
<li>MLE is extremely popular for:
<ul>
<li>
<strong>Binary Outcomes</strong> (logistic regression)</li>
<li>
<strong>Count Data</strong> (Poisson regression)</li>
<li>
<strong>Strictly Positive Outcomes</strong> (Gamma regression)</li>
<li>
<strong>Heteroskedastic Settings</strong> (models with variance related to mean, e.g., GLMs)</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Distributional Assumptions</strong>
<ul>
<li>The efficiency gains of MLE stem from using a specific probability model.</li>
<li>If the assumed model closely reflects the data-generating process, MLE gives accurate parameter estimates and reliable standard errors.</li>
<li>MLE assumes knowledge of the conditional distribution of the outcome variable. This assumption parallels the normality assumption in linear regression models (e.g., <a href="linear-regression.html#a6-normal-distribution">A6 Normal Distribution</a>).</li>
<li>If severely mis-specified, consider robust or semi-/nonparametric methods.</li>
</ul>
</li>
<li>
<strong>Comparison with OLS</strong>: See <a href="linear-regression.html#comparison-of-mle-and-ols">Comparison of MLE and OLS</a> for more details.
<ul>
<li>
<a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> is a special case of MLE when errors are normally distributed and homoscedastic.</li>
<li>In more general settings (e.g., non-Gaussian or heteroskedastic data), MLE can outperform OLS in terms of smaller standard errors and better inference.</li>
</ul>
</li>
<li>
<strong>Numerical Stability &amp; Computation</strong>
<ul>
<li>For complex likelihoods, iterative methods can fail to converge or converge to local maxima.</li>
<li>Proper initialization and diagnostics (e.g., checking multiple start points) are crucial.</li>
</ul>
</li>
</ol>
</div>
<div id="comparison-of-mle-and-ols" class="section level3" number="5.3.6">
<h3>
<span class="header-section-number">5.3.6</span> Comparison of MLE and OLS<a class="anchor" aria-label="anchor" href="#comparison-of-mle-and-ols"><i class="fas fa-link"></i></a>
</h3>
<p>While <a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a> Estimation is a powerful estimation method, it does not solve all of the challenges associated with <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a>. Below is a detailed comparison highlighting similarities, differences, and limitations.</p>
<p><strong>Key Points of Comparison</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Inference Methods</strong>:
<ul>
<li>
<strong>MLE</strong>:
<ul>
<li>Joint inference is typically conducted using <strong>log-likelihood calculations</strong>, such as likelihood ratio tests or information criteria (e.g., AIC, BIC).</li>
<li>These methods replace the use of F-statistics commonly associated with OLS.</li>
</ul>
</li>
<li>
<strong>OLS</strong>:
<ul>
<li>Relies on the <strong>F-statistic</strong> for hypothesis testing and joint inference.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Sensitivity to Functional Form</strong>:
<ul>
<li>Both MLE and OLS are sensitive to the <strong>functional form</strong> of the model. Incorrect specification (e.g., linear vs. nonlinear relationships) can lead to biased or inefficient estimates in both cases.</li>
</ul>
</li>
<li>
<strong>Perfect Collinearity and Multicollinearity</strong>:
<ul>
<li>Both methods are affected by collinearity:
<ul>
<li>
<strong>Perfect collinearity</strong> (e.g., two identical predictors) makes parameter estimation impossible.</li>
<li>
<strong>Multicollinearity</strong> (highly correlated predictors) inflates standard errors, reducing the precision of estimates.</li>
</ul>
</li>
<li>Neither MLE nor OLS directly resolves these issues without additional measures, such as regularization or variable selection.</li>
</ul>
</li>
<li>
<strong>Endogeneity</strong>:
<ul>
<li>Problems like <strong>omitted variable bias</strong> or <strong>simultaneous equations</strong> affect both MLE and OLS:
<ul>
<li>If relevant predictors are omitted, estimates from both methods are likely to be biased and inconsistent.</li>
<li>Similarly, in systems of simultaneous equations, both methods yield biased results unless endogeneity is addressed through instrumental variables or other approaches.</li>
</ul>
</li>
<li>
<strong>MLE</strong>, while efficient under correct model specification, does not inherently address endogeneity.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Situations Where MLE and OLS Differ</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="18%">
<col width="43%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>MLE</strong></th>
<th><strong>OLS</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Estimator Efficiency</strong></td>
<td>Efficient for correctly specified distributions.</td>
<td>Efficient under Gauss-Markov assumptions.</td>
</tr>
<tr class="even">
<td><strong>Assumptions about Errors</strong></td>
<td>Requires specifying a distribution (e.g., normal, binomial).</td>
<td>Requires only mean-zero errors and homoscedasticity.</td>
</tr>
<tr class="odd">
<td><strong>Use of Likelihood</strong></td>
<td>Based on maximizing the likelihood function for parameter estimation.</td>
<td>Based on minimizing the sum of squared residuals.</td>
</tr>
<tr class="even">
<td><strong>Model Flexibility</strong></td>
<td>More flexible (supports various distributions, non-linear models).</td>
<td>Primarily linear models (extensions for non-linear exist).</td>
</tr>
<tr class="odd">
<td><strong>Interpretation</strong></td>
<td>Log-likelihood values guide model comparison (AIC/BIC).</td>
<td>R-squared and adjusted R-squared measure fit.</td>
</tr>
</tbody>
</table></div>
<p><strong>Practical Considerations</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>When to Use MLE</strong>:
<ul>
<li>Situations where the dependent variable is:
<ul>
<li>Binary (e.g., logistic regression)</li>
<li>Count data (e.g., Poisson regression)</li>
<li>Skewed or bounded (e.g., survival models)</li>
</ul>
</li>
<li>When the model naturally arises from a probabilistic framework.</li>
</ul>
</li>
<li>
<strong>When to Use OLS</strong>:
<ul>
<li>Suitable for continuous dependent variables with approximately linear relationships between predictors and outcomes.</li>
<li>Simpler to implement and interpret when the assumptions of linear regression are reasonably met.</li>
</ul>
</li>
</ol>
</div>
<div id="applications-of-mle" class="section level3" number="5.3.7">
<h3>
<span class="header-section-number">5.3.7</span> Applications of MLE<a class="anchor" aria-label="anchor" href="#applications-of-mle"><i class="fas fa-link"></i></a>
</h3>
<p>MLE is widely used across various applications to estimate parameters in models tailored for specific data structures. Below are key applications of MLE, categorized by problem type and estimation method.</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="8%">
<col width="12%">
<col width="22%">
<col width="12%">
<col width="42%">
</colgroup>
<thead><tr class="header">
<th align="left"><strong>Model Type</strong></th>
<th align="left"><strong>Examples</strong></th>
<th align="left"><strong>Key Characteristics</strong></th>
<th align="left"><strong>Common Estimation Methods</strong></th>
<th align="left"><strong>Additional Notes</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Corner Solution Models</strong></td>
<td align="left">
<p>Hours worked</p>
<p>Donations to charity</p>
<p>Household consumption of a good</p>
</td>
<td align="left">
<p>Dependent variable is often <strong>censored at zero</strong> (or another threshold).</p>
<p>Large fraction of observations at the corner (e.g., 0 hours, 0 donations).</p>
</td>
<td align="left">
<p>Tobit regression</p>
<p>(latent variable approach with censoring)</p>
</td>
<td align="left">Useful when a continuous outcome has a <strong>mass point</strong> at zero but also positive values (e.g., 30% of individuals donate $0, the rest donate &gt; $0).</td>
</tr>
<tr class="even">
<td align="left"><strong>Non-Negative Count Models</strong></td>
<td align="left">
<p>Number of arrests</p>
<p>Number of cigarettes smoked</p>
<p>Doctor visits per year</p>
</td>
<td align="left">
<p>Dependent variable consists of <strong>non-negative integer counts</strong>.</p>
<p>Possible <strong>overdispersion</strong> (variance &gt; mean).</p>
</td>
<td align="left">
<p>Poisson regression,</p>
<p>Negative Binomial regression</p>
</td>
<td align="left">
<p>Poisson assumes mean = variance, so often Negative Binomial is preferred for real data.</p>
<p>Zero-inflated models (ZIP/ZINB) may be used for data with <strong>excess zeros</strong>.</p>
</td>
</tr>
<tr class="odd">
<td align="left"><strong>Multinomial Choice Models</strong></td>
<td align="left">
<p>Demand for different car brands</p>
<p>Votes in a primary election</p>
<p>Choice of travel mode</p>
</td>
<td align="left">
<p>Dependent variable is a <strong>categorical choice</strong> among <strong>3+ alternatives</strong>.</p>
<p>Each category is distinct, with no inherent ordering (e.g., brand A, B, or C).</p>
</td>
<td align="left">
<p>Multinomial logit,</p>
<p>Multinomial probit</p>
</td>
<td align="left">
<p>Extension of binary choice (logit/probit) to multiple categories.</p>
<p><strong>Independence of Irrelevant Alternatives (IIA)</strong> can be a concern for the multinomial logit.</p>
</td>
</tr>
<tr class="even">
<td align="left"><strong>Ordinal Choice Models</strong></td>
<td align="left">
<p>Self-reported happiness (low/medium/high)</p>
<p>Income level brackets</p>
<p>Likert-scale surveys</p>
</td>
<td align="left">
<p>Dependent variable is <strong>ordered</strong> (e.g., <strong>low &lt; medium &lt; high</strong>).</p>
<p>Distances between categories are not necessarily equal.</p>
</td>
<td align="left">
<p>Ordered logit,</p>
<p>Ordered probit</p>
</td>
<td align="left">
<p>Probit/logit framework adapted to preserve <strong>ordinal information</strong>.</p>
<p>Interprets latent continuous variable mapped to discrete ordered categories.</p>
</td>
</tr>
</tbody>
</table></div>
<div id="binary-response-models" class="section level4" number="5.3.7.1">
<h4>
<span class="header-section-number">5.3.7.1</span> Binary Response Models<a class="anchor" aria-label="anchor" href="#binary-response-models"><i class="fas fa-link"></i></a>
</h4>
<p>A binary response variable (<span class="math inline">\(y_i\)</span>) follows a <a href="prerequisites.html#bernoulli-distribution">Bernoulli</a> distribution:</p>
<p><span class="math display">\[
f_Y(y_i; p) = p^{y_i}(1-p)^{(1-y_i)}
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the probability of success. For conditional models, the likelihood becomes:</p>
<p><span class="math display">\[
f_{Y|X}(y_i, x_i; p(.)) = p(x_i)^{y_i}(1 - p(x_i))^{(1-y_i)}
\]</span></p>
<p>To model <span class="math inline">\(p(x_i)\)</span>, we use a function of <span class="math inline">\(x_i\)</span> and unknown parameters <span class="math inline">\(\theta\)</span>. A common approach involves a <strong>latent variable model</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;= 1\{y_i^* &gt; 0 \}, \\
y_i^* &amp;= x_i \beta - \epsilon_i,
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(y_i^*\)</span> is an unobserved (latent) variable.</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> is a random variable with mean 0, representing unobserved noise.</li>
</ul>
<p>Rewriting in terms of observed data:</p>
<p><span class="math display">\[
y_i = 1\{x_i \beta &gt; \epsilon_i\}.
\]</span></p>
<p>The probability function becomes:</p>
<p><span class="math display">\[
\begin{aligned}
p(x_i) &amp;= P(y_i = 1 | x_i) \\
&amp;= P(x_i \beta &gt; \epsilon_i | x_i) \\
&amp;= F_{\epsilon|X}(x_i \beta | x_i),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(F_{\epsilon|X}(.)\)</span> is the cumulative distribution function (CDF) of <span class="math inline">\(\epsilon_i\)</span>. Assuming independence of <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(x_i\)</span>, the probability function simplifies to:</p>
<p><span class="math display">\[
p(x_i) = F_\epsilon(x_i \beta).
\]</span></p>
<p>The conditional expectation function is equivalent:</p>
<p><span class="math display">\[
E(y_i | x_i) = P(y_i = 1 | x_i) = F_\epsilon(x_i \beta).
\]</span></p>
<p>Common Distributional Assumptions</p>
<ol style="list-style-type: decimal">
<li>
<strong>Probit Model</strong>:
<ul>
<li>Assumes <span class="math inline">\(\epsilon_i\)</span> follows a standard normal distribution.</li>
<li>
<span class="math inline">\(F_\epsilon(.) = \Phi(.)\)</span>, where <span class="math inline">\(\Phi(.)\)</span> is the standard normal CDF.</li>
</ul>
</li>
<li>
<strong>Logit Model</strong>:
<ul>
<li>Assumes <span class="math inline">\(\epsilon_i\)</span> follows a standard logistic distribution.</li>
<li>
<span class="math inline">\(F_\epsilon(.) = \Lambda(.)\)</span>, where <span class="math inline">\(\Lambda(.)\)</span> is the logistic CDF.</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Steps to Derive MLE for Binary Models</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Specify the Log-Likelihood</strong>:
<ul>
<li>
<p>For a chosen distribution (e.g., normal for Probit or logistic for Logit), the log-likelihood is:</p>
<p><span class="math display">\[
\ln(f_{Y|X}(y_i, x_i; \beta)) = y_i \ln(F_\epsilon(x_i \beta)) + (1 - y_i) \ln(1 - F_\epsilon(x_i \beta)).
\]</span></p>
</li>
</ul>
</li>
<li>
<strong>Maximize the Log-Likelihood</strong>:
<ul>
<li>
<p>Find the parameter estimates that maximize the log-likelihood:</p>
<p><span class="math display">\[
\hat{\beta}_{MLE} = \underset{\beta}{\text{argmax}} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \beta)).
\]</span></p>
</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Properties of Probit and Logit Estimators</strong></p>
<ul>
<li>
<strong>Consistency and Asymptotic Normality</strong>:
<ul>
<li>Probit and Logit estimators are consistent and asymptotically normal if:
<ul>
<li>
<a href="linear-regression.html#a2-full-rank">A2 Full Rank</a>: <span class="math inline">\(E(x_i' x_i)\)</span> exists and is non-singular.</li>
<li>
<a href="linear-regression.html#a5-data-generation-random-sampling">A5 Data Generation (Random Sampling)</a>: <span class="math inline">\(\{y_i, x_i\}\)</span> are iid (or stationary and weakly dependent).</li>
<li>Distributional assumptions on <span class="math inline">\(\epsilon_i\)</span> hold (e.g., normal or logistic, independent of <span class="math inline">\(x_i\)</span>).</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Asymptotic Efficiency</strong>:
<ul>
<li>
<p>Under these assumptions, Probit and Logit estimators are asymptotically efficient with variance:</p>
<p><span class="math display">\[
I(\beta_0)^{-1} = \left[E\left(\frac{(f_\epsilon(x_i \beta_0))^2}{F_\epsilon(x_i \beta_0)(1 - F_\epsilon(x_i \beta_0))} x_i' x_i \right)\right]^{-1},
\]</span></p>
<p>where <span class="math inline">\(f_\epsilon(x_i \beta_0)\)</span> is the PDF (derivative of the CDF).</p>
</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Interpretation of Binary Response Models</strong></p>
<p>Binary response models, such as Probit and Logit, estimate the probability of an event occurring (<span class="math inline">\(y_i = 1\)</span>) given predictor variables <span class="math inline">\(x_i\)</span>. However, interpreting the estimated coefficients (<span class="math inline">\(\beta\)</span>) in these models differs significantly from linear models. Below, we explore how to interpret these coefficients and the concept of <strong>partial effects</strong>.</p>
<ol style="list-style-type: decimal">
<li>Interpreting <span class="math inline">\(\beta\)</span> in Binary Response Models</li>
</ol>
<p>In binary response models, the coefficient <span class="math inline">\(\beta_j\)</span> represents the average change in the <strong>latent variable</strong> <span class="math inline">\(y_i^*\)</span> (an unobserved variable) for a one-unit change in <span class="math inline">\(x_{ij}\)</span>. While this provides insight into the direction of the relationship:</p>
<ul>
<li>
<strong>Magnitudes</strong> of <span class="math inline">\(\beta_j\)</span> do not have a direct, meaningful interpretation in terms of <span class="math inline">\(y_i\)</span>.</li>
<li>
<strong>Direction</strong> of <span class="math inline">\(\beta_j\)</span> is meaningful:
<ul>
<li>
<span class="math inline">\(\beta_j &gt; 0\)</span>: A positive association between <span class="math inline">\(x_{ij}\)</span> and the probability of <span class="math inline">\(y_i = 1\)</span>.</li>
<li>
<span class="math inline">\(\beta_j &lt; 0\)</span>: A negative association between <span class="math inline">\(x_{ij}\)</span> and the probability of <span class="math inline">\(y_i = 1\)</span>.</li>
</ul>
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Partial Effects in Nonlinear Binary Models</li>
</ol>
<p>To interpret the effect of a change in a predictor <span class="math inline">\(x_{ij}\)</span> on the probability of an event occurring (<span class="math inline">\(P(y_i = 1|x_i)\)</span>), we use the <strong>partial effect</strong>:</p>
<p><span class="math display">\[
E(y_i | x_i) = F_\epsilon(x_i \beta),
\]</span></p>
<p>where <span class="math inline">\(F_\epsilon(.)\)</span> is the cumulative distribution function (CDF) of the error term <span class="math inline">\(\epsilon_i\)</span> (e.g., standard normal for Probit, logistic for Logit). The <strong>partial effect</strong> is the derivative of the expected probability with respect to <span class="math inline">\(x_{ij}\)</span>:</p>
<p><span class="math display">\[
PE(x_{ij}) = \frac{\partial E(y_i | x_i)}{\partial x_{ij}} = f_\epsilon(x_i \beta) \beta_j,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(f_\epsilon(.)\)</span> is the probability density function (PDF) of the error term <span class="math inline">\(\epsilon_i\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_j\)</span> is the coefficient associated with <span class="math inline">\(x_{ij}\)</span>.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Key Characteristics of Partial Effects</li>
</ol>
<ul>
<li>
<strong>Scaling Factor</strong>:
<ul>
<li>The partial effect depends on a <strong>scaling factor</strong>, <span class="math inline">\(f_\epsilon(x_i \beta)\)</span>, which is derived from the density function <span class="math inline">\(f_\epsilon(.)\)</span>.</li>
<li>The scaling factor varies depending on the values of <span class="math inline">\(x_i\)</span>, making the partial effect nonlinear and <strong>context-dependent</strong>.</li>
</ul>
</li>
<li>
<strong>Non-Constant Partial Effects</strong>:
<ul>
<li>Unlike linear models where coefficients directly represent constant marginal effects, the partial effect in binary models changes based on <span class="math inline">\(x_i\)</span>.</li>
<li>For example, in a Logit model, the partial effect is largest when <span class="math inline">\(P(y_i = 1 | x_i)\)</span> is around 0.5 (the midpoint of the S-shaped logistic curve) and smaller at the extremes (close to 0 or 1).</li>
</ul>
</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Single Values for Partial Effects</li>
</ol>
<p>In practice, researchers often summarize partial effects using either:</p>
<ul>
<li>
<strong>Partial Effect at the Average (PEA)</strong>:
<ul>
<li>The partial effect is calculated for an “average individual,” where <span class="math inline">\(x_i = \bar{x}\)</span> (the sample mean of predictors): <span class="math display">\[
PEA = f_\epsilon(\bar{x}\hat{\beta}) \hat{\beta}_j.
\]</span>
</li>
<li>This provides a single, interpretable value but assumes the average effect applies to all individuals.</li>
</ul>
</li>
<li>
<strong>Average Partial Effect (APE)</strong>:
<ul>
<li>The average of all individual-level partial effects across the sample: <span class="math display">\[
APE = \frac{1}{n} \sum_{i=1}^{n} f_\epsilon(x_i \hat{\beta}) \hat{\beta}_j.
\]</span>
</li>
<li>This accounts for the nonlinearity of the partial effects and provides a more accurate summary of the marginal effect in the population.</li>
</ul>
</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Comparing Partial Effects in Linear and Nonlinear Models</li>
</ol>
<ul>
<li>
<strong>Linear Models</strong>:
<ul>
<li>Partial effects are constant: <span class="math inline">\(APE = PEA\)</span>.</li>
<li>The coefficients directly represent the marginal effects on <span class="math inline">\(E(y_i | x_i)\)</span>.</li>
</ul>
</li>
<li>
<strong>Nonlinear Models</strong>:
<ul>
<li>Partial effects are <strong>not constant</strong> due to the dependence on <span class="math inline">\(f_\epsilon(x_i \beta)\)</span>.</li>
<li>As a result, <span class="math inline">\(APE \neq PEA\)</span> in general.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div id="penalized-regularized-estimators" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Penalized (Regularized) Estimators<a class="anchor" aria-label="anchor" href="#penalized-regularized-estimators"><i class="fas fa-link"></i></a>
</h2>
<p>Penalized or regularized estimators are extensions of <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> designed to address its limitations, particularly in high-dimensional settings. Regularization methods introduce a penalty term to the loss function to prevent overfitting, handle multicollinearity, and improve model interpretability.</p>
<p>There are three popular regularization techniques (but not limited to):</p>
<ol style="list-style-type: decimal">
<li><a href="linear-regression.html#ridge-regression">Ridge Regression</a></li>
<li><a href="linear-regression.html#lasso-regression">Lasso Regression</a></li>
<li><a href="linear-regression.html#elastic-net">Elastic Net</a></li>
</ol>
<hr>
<div id="motivation-for-penalized-estimators" class="section level3" number="5.4.1">
<h3>
<span class="header-section-number">5.4.1</span> Motivation for Penalized Estimators<a class="anchor" aria-label="anchor" href="#motivation-for-penalized-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>OLS minimizes the Residual Sum of Squares (RSS):</p>
<p><span class="math display">\[
RSS = \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span> is the observed outcome,</p></li>
<li><p><span class="math inline">\(x_i\)</span> is the vector of predictors for observation <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(\beta\)</span> is the vector of coefficients.</p></li>
</ul>
<p>While OLS works well under ideal conditions (e.g., low dimensionality, no multicollinearity), it struggles when:</p>
<ul>
<li><p><strong>Multicollinearity</strong>: Predictors are highly correlated, leading to large variances in <span class="math inline">\(\beta\)</span> estimates.</p></li>
<li><p><strong>High Dimensionality</strong>: The number of predictors (<span class="math inline">\(p\)</span>) exceeds or approaches the sample size (<span class="math inline">\(n\)</span>), making OLS inapplicable or unstable.</p></li>
<li><p><strong>Overfitting</strong>: When <span class="math inline">\(p\)</span> is large, OLS fits noise in the data, reducing generalizability.</p></li>
</ul>
<p>To address these issues, <strong>penalized regression</strong> modifies the OLS loss function by adding a <strong>penalty term</strong> that shrinks the coefficients toward zero. This discourages overfitting and improves predictive performance.</p>
<p>The general form of the penalized loss function is:</p>
<p><span class="math display">\[
L(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda P(\beta),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\lambda \geq 0\)</span>: Tuning parameter controlling the strength of regularization.</p></li>
<li><p><span class="math inline">\(P(\beta)\)</span>: Penalty term that quantifies model complexity.</p></li>
</ul>
<p>Different choices of <span class="math inline">\(P(\beta)\)</span> lead to ridge regression, lasso regression, or elastic net.</p>
<hr>
</div>
<div id="ridge-regression" class="section level3" number="5.4.2">
<h3>
<span class="header-section-number">5.4.2</span> Ridge Regression<a class="anchor" aria-label="anchor" href="#ridge-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Ridge regression, also known as <strong>L2 regularization</strong>, penalizes the sum of squared coefficients:</p>
<p><span class="math display">\[
P(\beta) = \sum_{j=1}^p \beta_j^2.
\]</span></p>
<p>The ridge objective function becomes:</p>
<p><span class="math display">\[
L_{ridge}(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\lambda \geq 0\)</span> controls the degree of shrinkage. Larger <span class="math inline">\(\lambda\)</span> leads to greater shrinkage.</li>
</ul>
<p>Ridge regression has a closed-form solution:</p>
<p><span class="math display">\[
\hat{\beta}_{ridge} = \left( X'X + \lambda I \right)^{-1} X'y,
\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the <span class="math inline">\(p \times p\)</span> identity matrix.</p>
<p><strong>Key Features</strong></p>
<ul>
<li>Shrinks coefficients but does <strong>not set them exactly to zero</strong>.</li>
<li>Handles multicollinearity effectively by stabilizing the coefficient estimates <span class="citation">(<a href="references.html#ref-hoerl1970">Hoerl and Kennard 1970</a>)</span>.</li>
<li>Works well when all predictors contribute to the response.</li>
</ul>
<p><strong>Example Use Case</strong></p>
<p>Ridge regression is ideal for applications with many correlated predictors, such as:</p>
<ul>
<li>Predicting housing prices based on a large set of features (e.g., size, location, age of the house).</li>
</ul>
<hr>
</div>
<div id="lasso-regression" class="section level3" number="5.4.3">
<h3>
<span class="header-section-number">5.4.3</span> Lasso Regression<a class="anchor" aria-label="anchor" href="#lasso-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Lasso regression, or <strong>L1 regularization</strong>, penalizes the sum of absolute coefficients:</p>
<p><span class="math display">\[
P(\beta) = \sum_{j=1}^p |\beta_j|.
\]</span></p>
<p>The lasso objective function is:</p>
<p><span class="math display">\[
L_{lasso}(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda \sum_{j=1}^p |\beta_j|.
\]</span></p>
<p><strong>Key Features</strong></p>
<ul>
<li>Unlike ridge regression, lasso can set coefficients to <strong>exactly zero</strong>, performing automatic feature selection.</li>
<li>Encourages sparse models, making it suitable for high-dimensional data <span class="citation">(<a href="references.html#ref-tibshirani1996">Tibshirani 1996</a>)</span>.</li>
</ul>
<p><strong>Optimization</strong></p>
<p>Lasso does not have a closed-form solution due to the non-differentiability of <span class="math inline">\(|\beta_j|\)</span> at <span class="math inline">\(\beta_j = 0\)</span>. It requires iterative algorithms, such as:</p>
<ul>
<li><p><strong>Coordinate Descent</strong>,</p></li>
<li><p><strong>Least Angle Regression (LARS)</strong>.</p></li>
</ul>
<p><strong>Example Use Case</strong></p>
<p>Lasso regression is useful when many predictors are irrelevant, such as:</p>
<ul>
<li>Genomics, where only a subset of genes are associated with a disease outcome.</li>
</ul>
<hr>
</div>
<div id="elastic-net" class="section level3" number="5.4.4">
<h3>
<span class="header-section-number">5.4.4</span> Elastic Net<a class="anchor" aria-label="anchor" href="#elastic-net"><i class="fas fa-link"></i></a>
</h3>
<p>Elastic Net combines the penalties of ridge and lasso regression:</p>
<p><span class="math display">\[
P(\beta) = \alpha \sum_{j=1}^p |\beta_j| + \frac{1 - \alpha}{2} \sum_{j=1}^p \beta_j^2,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(0 \leq \alpha \leq 1\)</span> determines the balance between lasso (L1) and ridge (L2) penalties.</p></li>
<li><p><span class="math inline">\(\lambda\)</span> controls the overall strength of regularization.</p></li>
</ul>
<p>The elastic net objective function is:</p>
<p><span class="math display">\[
L_{elastic\ net}(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda \left( \alpha \sum_{j=1}^p |\beta_j| + \frac{1 - \alpha}{2} \sum_{j=1}^p \beta_j^2 \right).
\]</span></p>
<p><strong>Key Features</strong></p>
<ul>
<li>Combines the strengths of lasso (sparse models) and ridge (stability with correlated predictors) <span class="citation">(<a href="references.html#ref-zou2005a">H. Zou and Hastie 2005</a>)</span>.</li>
<li>Effective when predictors are highly correlated or when <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
<p><strong>Example Use Case</strong></p>
<p>Elastic net is ideal for high-dimensional datasets with correlated predictors, such as:</p>
<ul>
<li>Predicting customer churn using demographic and behavioral features.</li>
</ul>
<hr>
</div>
<div id="tuning-parameter-selection" class="section level3" number="5.4.5">
<h3>
<span class="header-section-number">5.4.5</span> Tuning Parameter Selection<a class="anchor" aria-label="anchor" href="#tuning-parameter-selection"><i class="fas fa-link"></i></a>
</h3>
<p>Choosing the regularization parameter <span class="math inline">\(\lambda\)</span> (and <span class="math inline">\(\alpha\)</span> for elastic net) is critical for balancing model complexity (fit) and regularization (parsimony). If <span class="math inline">\(\lambda\)</span> is too large, coefficients are overly shrunk (or even set to zero in the case of L1 penalty), leading to underfitting. If <span class="math inline">\(\lambda\)</span> is too small, the model might overfit because coefficients are not penalized sufficiently. Hence, a systematic approach is needed to determine the optimal <span class="math inline">\(\lambda\)</span>. For elastic net, we also choose an appropriate <span class="math inline">\(\alpha\)</span> to balance the L1 and L2 penalties.</p>
<div id="cross-validation" class="section level4" number="5.4.5.1">
<h4>
<span class="header-section-number">5.4.5.1</span> Cross-Validation<a class="anchor" aria-label="anchor" href="#cross-validation"><i class="fas fa-link"></i></a>
</h4>
<p>A common approach to selecting <span class="math inline">\(\lambda\)</span> (and <span class="math inline">\(\alpha\)</span>) is <span class="math inline">\(K\)</span>-Fold Cross-Validation:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Partition the data</strong> into <span class="math inline">\(K\)</span> roughly equal-sized “folds.”</li>
<li>
<strong>Train the model</strong> on <span class="math inline">\(K-1\)</span> folds and <strong>validate</strong> on the remaining fold, computing a validation error.</li>
<li>Repeat this process <strong>for all folds</strong>, and compute the average validation error across the <span class="math inline">\(K\)</span> folds.</li>
<li>
<strong>Select</strong> the value of <span class="math inline">\(\lambda\)</span> (and <span class="math inline">\(\alpha\)</span> if tuning it) that <strong>minimizes</strong> the cross-validated error.</li>
</ol>
<p>This method helps us maintain a good bias-variance trade-off because every point is used for both training and validation exactly once.</p>
</div>
<div id="information-criteria" class="section level4" number="5.4.5.2">
<h4>
<span class="header-section-number">5.4.5.2</span> Information Criteria<a class="anchor" aria-label="anchor" href="#information-criteria"><i class="fas fa-link"></i></a>
</h4>
<p>Alternatively, one can use <strong>information criteria</strong>—like the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC)—to guide model selection. These criteria reward goodness-of-fit while penalizing model complexity, thereby helping in selecting an appropriately regularized model.</p>
<hr>
</div>
</div>
<div id="properties-of-penalized-estimators" class="section level3" number="5.4.6">
<h3>
<span class="header-section-number">5.4.6</span> Properties of Penalized Estimators<a class="anchor" aria-label="anchor" href="#properties-of-penalized-estimators"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<strong>Bias-Variance Tradeoff</strong>:
<ul>
<li>Regularization introduces some bias in exchange for reducing variance, often resulting in better predictive performance on new data.</li>
</ul>
</li>
<li>
<strong>Shrinkage</strong>:
<ul>
<li>Ridge shrinks coefficients toward zero but usually retains all predictors.</li>
<li>Lasso shrinks some coefficients exactly to zero, performing inherent feature selection.</li>
</ul>
</li>
<li>
<strong>Flexibility</strong>:
<ul>
<li>Elastic net allows for a continuum between ridge and lasso, so it can adapt to different data structures (e.g., many correlated features or very high-dimensional feature spaces).</li>
</ul>
</li>
</ol>
<hr>
<div class="sourceCode" id="cb184"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load required libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://glmnet.stanford.edu">glmnet</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span>   <span class="co"># Number of observations</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">20</span>    <span class="co"># Number of predictors</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="va">p</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="va">n</span>, ncol <span class="op">=</span> <span class="va">p</span><span class="op">)</span>  <span class="co"># Predictor matrix</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>                                 <span class="co"># Response vector</span></span>
<span></span>
<span><span class="co"># Ridge regression (alpha = 0)</span></span>
<span><span class="va">ridge_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ridge_fit</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span>, label <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="st">"Coefficient Paths for Ridge Regression"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-19-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>In this plot, each curve represents a coefficient’s value as a function of <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>As <span class="math inline">\(\lambda\)</span> increases (moving from left to right on a log-scale by default), coefficients shrink toward zero but typically stay non-zero.</p></li>
<li><p>Ridge regression tends to shrink coefficients but does not force them to be exactly zero.</p></li>
</ul>
<div class="sourceCode" id="cb185"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Lasso regression (alpha = 1)</span></span>
<span><span class="va">lasso_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lasso_fit</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span>, label <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="st">"Coefficient Paths for Lasso Regression"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-20-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Here, as <span class="math inline">\(\lambda\)</span> grows, several coefficient paths <strong>hit zero exactly</strong>, illustrating the variable selection property of lasso.</p>
<div class="sourceCode" id="cb186"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Elastic net (alpha = 0.5)</span></span>
<span><span class="va">elastic_net_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">elastic_net_fit</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span>, label <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/title.html">title</a></span><span class="op">(</span><span class="st">"Coefficient Paths for Elastic Net (alpha = 0.5)"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-21-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>Elastic net combines ridge and lasso penalties. At <span class="math inline">\(\lambda = 0.5\)</span>, we see partial shrinkage and some coefficients going to zero.</p></li>
<li><p>This model is often helpful when you suspect both group-wise shrinkage (like ridge) and sparse solutions (like lasso) might be beneficial.</p></li>
</ul>
<p>We can further refine our choice of <span class="math inline">\(\lambda\)</span> by performing cross-validation on the lasso model:</p>
<div class="sourceCode" id="cb187"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv_lasso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cv_lasso</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-22-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb188"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">best_lambda</span> <span class="op">&lt;-</span> <span class="va">cv_lasso</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="va">best_lambda</span></span>
<span><span class="co">#&gt; [1] 0.1449586</span></span></code></pre></div>
<ul>
<li><p>The plot displays the cross-validated error (often mean-squared error or deviance) on the y-axis versus <span class="math inline">\(\log(\lambda)\)</span> on the x-axis.</p></li>
<li>
<p>Two vertical dotted lines typically appear:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\lambda.min\)</span>: The <span class="math inline">\(\lambda\)</span> that achieves the minimum cross-validated error.</p></li>
<li><p><span class="math inline">\(\lambda.1se\)</span>: The largest <span class="math inline">\(\lambda\)</span> such that the cross-validated error is still within one standard error of the minimum. This is a more conservative choice that favors higher regularization (simpler models).</p></li>
</ol>
</li>
<li><p><code>best_lambda</code> above prints the numeric value of <span class="math inline">\(\lambda.min\)</span>. This is the <span class="math inline">\(\lambda\)</span> that gave the lowest cross-validation error for the lasso model.</p></li>
</ul>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><p>By using <code>cv.glmnet</code>, we systematically compare different values of <span class="math inline">\(\lambda\)</span> in terms of their predictive performance (cross-validation error).</p></li>
<li><p>The selected <span class="math inline">\(\lambda\)</span> typically balances having a smaller model (due to regularization) with retaining sufficient predictive power.</p></li>
<li><p>If we used real-world data, we might also look at performance metrics on a hold-out test set to ensure that the chosen <span class="math inline">\(\lambda\)</span> generalizes well.</p></li>
</ul>
</div>
</div>
<div id="robust-estimators" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Robust Estimators<a class="anchor" aria-label="anchor" href="#robust-estimators"><i class="fas fa-link"></i></a>
</h2>
<p>Robust estimators are statistical techniques designed to provide reliable parameter estimates even when the assumptions underlying classical methods, such as <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a>, are violated. Specifically, they address issues caused by outliers, non-normal errors, or heavy-tailed distributions, which can render OLS inefficient or biased.</p>
<p>The goal of robust estimation is to reduce the sensitivity of the estimator to extreme or aberrant data points, thereby ensuring a more reliable and accurate fit to the majority of the data.</p>
<p>We will cover the key robust estimation techniques, their properties, and applications, along with practical examples and mathematical derivations. The focus will include <span class="math inline">\(M\)</span>-estimators, <span class="math inline">\(R\)</span>-estimators, <span class="math inline">\(L\)</span>-estimators, <span class="math inline">\(LTS\)</span>, <span class="math inline">\(S\)</span>-estimators, <span class="math inline">\(MM\)</span>-estimators, and more.</p>
<hr>
<div id="motivation-for-robust-estimation" class="section level3" number="5.5.1">
<h3>
<span class="header-section-number">5.5.1</span> Motivation for Robust Estimation<a class="anchor" aria-label="anchor" href="#motivation-for-robust-estimation"><i class="fas fa-link"></i></a>
</h3>
<p>OLS seeks to minimize the Residual Sum of Squares (RSS):</p>
<p><span class="math display">\[
RSS = \sum_{i=1}^n (y_i - x_i'\beta)^2,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(y_i\)</span> is the observed response for the <span class="math inline">\(i\)</span>th observation,</li>
<li>
<span class="math inline">\(x_i\)</span> is the vector of predictors for the <span class="math inline">\(i\)</span>th observation,</li>
<li>
<span class="math inline">\(\beta\)</span> is the vector of coefficients.</li>
</ul>
<p>OLS assumes:</p>
<ol style="list-style-type: decimal">
<li>Errors are normally distributed and no outliers in the data (<a href="linear-regression.html#a6-normal-distribution">A6 Normal Distribution</a>).</li>
<li>Homoscedasticity (constant variance of errors) (<a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a>).</li>
</ol>
<p>In real-world scenarios:</p>
<ul>
<li><p><strong>Outliers</strong> in <span class="math inline">\(y\)</span> or <span class="math inline">\(x\)</span> can disproportionately affect the estimates, leading to biased or inefficient results.</p></li>
<li><p><strong>Heavy-tailed distributions</strong> (e.g., Cauchy) violate the normality assumption, making OLS inappropriate.</p></li>
</ul>
<p>For example, <span class="citation">P. J. Huber (<a href="references.html#ref-huber1964">1964</a>)</span> demonstrates that a single extreme observation can arbitrarily distort OLS estimates, while <span class="citation">Hampel et al. (<a href="references.html#ref-hampel2005">2005</a>)</span> define the breakdown point as a measure of robustness. Robust estimators aim to mitigate these problems by limiting the influence of problematic observations.</p>
<p>OLS inherently squares the residuals <span class="math inline">\(e_i = y_i - x_i'\beta\)</span>, amplifying the influence of large residuals. For example, if a single residual is much larger than the others, its squared value can dominate the RSS, distorting the estimated coefficients.</p>
<p>Consider a simple case where <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i + e_i\)</span>, with <span class="math inline">\(e_i \sim N(0, \sigma^2)\)</span> under the classical assumptions. Now introduce an outlier: a single observation with an unusually large <span class="math inline">\(e_i\)</span>. The squared residual for this point will dominate the RSS and pull the estimated regression line towards it, leading to biased estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>The <strong>breakdown point</strong> of an estimator is the proportion of contamination (e.g., outliers) that the estimator can tolerate before yielding arbitrarily large or incorrect results. For OLS, the breakdown point is <span class="math inline">\(1/n\)</span>, meaning even one outlier can cause substantial distortion in the estimates.</p>
<hr>
</div>
<div id="m-estimators" class="section level3" number="5.5.2">
<h3>
<span class="header-section-number">5.5.2</span> <span class="math inline">\(M\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#m-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>To address the sensitivity of OLS, robust estimators minimize a different objective function:</p>
<p><span class="math display">\[
\sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\rho(\cdot)\)</span> is a robust loss function that grows slower than the quadratic function used in OLS,</li>
<li>
<span class="math inline">\(\sigma\)</span> is a scale parameter to normalize residuals.</li>
</ul>
<p>In OLS, the quadratic loss function <span class="math inline">\(\rho(z) = z^2\)</span> penalizes large residuals disproportionately. Robust estimators replace this with alternative <span class="math inline">\(\rho\)</span> functions that limit the penalty for large residuals, thus reducing their influence on the parameter estimates.</p>
<p>A robust <span class="math inline">\(\rho\)</span> function should satisfy the following properties:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Bounded Influence</strong>: Large residuals contribute a finite amount to the objective function.</li>
<li>
<strong>Symmetry</strong>: <span class="math inline">\(\rho(z) = \rho(-z)\)</span> ensures that positive and negative residuals are treated equally.</li>
<li>
<strong>Differentiability</strong>: For computational tractability, <span class="math inline">\(\rho\)</span> should be smooth and differentiable.</li>
</ol>
<hr>
<div id="examples-of-robust-rho-functions" class="section level4" number="5.5.2.1">
<h4>
<span class="header-section-number">5.5.2.1</span> Examples of Robust <span class="math inline">\(\rho\)</span> Functions<a class="anchor" aria-label="anchor" href="#examples-of-robust-rho-functions"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Huber’s Loss Function</strong> <span class="citation">(<a href="references.html#ref-huber1964">P. J. Huber 1964</a>)</span>
</li>
</ol>
<p>Huber’s loss function transitions between quadratic and linear growth:</p>
<p><span class="math display">\[
\rho(z) =
\begin{cases}
\frac{z^2}{2} &amp; \text{if } |z| \leq c, \\
c|z| - \frac{c^2}{2} &amp; \text{if } |z| &gt; c.
\end{cases}
\]</span></p>
<p>Key features:</p>
<ul>
<li>For small residuals (<span class="math inline">\(|z| \leq c\)</span>), the loss is quadratic, mimicking OLS.</li>
<li>For large residuals (<span class="math inline">\(|z| &gt; c\)</span>), the loss grows linearly, limiting their influence.</li>
</ul>
<p>The parameter <span class="math inline">\(c\)</span> controls the threshold at which the loss function transitions from quadratic to linear. Smaller values of <span class="math inline">\(c\)</span> make the estimator more robust but potentially less efficient under normality.</p>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Tukey’s Bisquare Function</strong> <span class="citation">(<a href="references.html#ref-beaton1974">Beaton and Tukey 1974</a>)</span>
</li>
</ol>
<p>Tukey’s bisquare function completely bounds the influence of large residuals:</p>
<p><span class="math display">\[
\rho(z) =
\begin{cases}
c^2 \left(1 - \left(1 - \left(\frac{z}{c}\right)^2\right)^3\right)/6 &amp; \text{if } |z| \leq c, \\
c^2/6 &amp; \text{if } |z| &gt; c.
\end{cases}
\]</span></p>
<p>Key features:</p>
<ul>
<li>Residuals larger than <span class="math inline">\(c\)</span> contribute a constant value to the objective function, effectively excluding them from the estimation process.</li>
<li>This approach achieves high robustness at the cost of lower efficiency for small residuals.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>Andrews’ Sine Function</strong> <span class="citation">(<a href="references.html#ref-andrews1974">D. F. Andrews 1974</a>)</span>:
<ul>
<li>Smoothly downweights extreme residuals: <span class="math display">\[ \rho(z) = \begin{cases}  c^2 \left(1 - \cos\left(\frac{z}{c}\right)\right)/2 &amp; \text{if } |z| \leq \pi c, \\ c^2/2 &amp; \text{if } |z| &gt; \pi c. \end{cases} \]</span>
</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="weighting-scheme-influence-functions" class="section level4" number="5.5.2.2">
<h4>
<span class="header-section-number">5.5.2.2</span> Weighting Scheme: Influence Functions<a class="anchor" aria-label="anchor" href="#weighting-scheme-influence-functions"><i class="fas fa-link"></i></a>
</h4>
<p>A critical concept in robust estimation is the <strong>influence function</strong>, which describes the sensitivity of the estimator to individual observations. For <span class="math inline">\(M\)</span>-estimators, the influence function is derived as the derivative of the loss function <span class="math inline">\(\rho(z)\)</span> with respect to <span class="math inline">\(z\)</span>:</p>
<p><span class="math display">\[
\psi(z) = \frac{d}{dz} \rho(z).
\]</span></p>
<p>This function plays a crucial role in downweighting large residuals. The weight assigned to each residual is proportional to <span class="math inline">\(\psi(z)/z\)</span>, which decreases as <span class="math inline">\(|z|\)</span> increases for robust estimators.</p>
<p>For Huber’s loss function, the influence function is:</p>
<p><span class="math display">\[
\psi(z) =
\begin{cases}
z &amp; \text{if } |z| \leq c, \\
c \cdot \text{sign}(z) &amp; \text{if } |z| &gt; c.
\end{cases}
\]</span></p>
<ul>
<li>For small residuals, <span class="math inline">\(\psi(z) = z\)</span>, matching OLS.</li>
<li>For large residuals, <span class="math inline">\(\psi(z)\)</span> is constant, ensuring bounded influence.</li>
</ul>
<hr>
<p>A key consideration when selecting a robust estimator is the trade-off between <strong>robustness</strong> (resistance to outliers) and <strong>efficiency</strong> (performance under ideal conditions). The tuning parameters in <span class="math inline">\(\rho\)</span> functions (e.g., <span class="math inline">\(c\)</span> in Huber’s loss) directly affect this balance:</p>
<ul>
<li>Smaller <span class="math inline">\(c\)</span> increases robustness but reduces efficiency under normality.</li>
<li>Larger <span class="math inline">\(c\)</span> improves efficiency under normality but decreases robustness to outliers.</li>
</ul>
<p>This trade-off reflects the fundamental goal of robust estimation: to achieve a balance between reliability and precision across a wide range of data scenarios.</p>
<hr>
</div>
<div id="properties-of-m-estimators" class="section level4" number="5.5.2.3">
<h4>
<span class="header-section-number">5.5.2.3</span> Properties of <span class="math inline">\(M\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#properties-of-m-estimators"><i class="fas fa-link"></i></a>
</h4>
<p>Robust estimators, particularly <span class="math inline">\(M\)</span>-estimators, possess the following mathematical properties:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Asymptotic Normality</strong>: Under mild regularity conditions, <span class="math inline">\(M\)</span>-estimators are asymptotically normal: <span class="math display">\[
\sqrt{n} (\hat{\beta} - \beta) \xrightarrow{d} N(0, \Sigma),
\]</span> where <span class="math inline">\(\Sigma\)</span> depends on the choice of <span class="math inline">\(\rho\)</span> and the distribution of residuals.</li>
<li>
<strong>Consistency</strong>: As <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\hat{\beta} \to \beta\)</span> in probability, provided the majority of the data satisfies the model assumptions.</li>
<li>
<strong>Breakdown Point</strong>: <span class="math inline">\(M\)</span>-estimators typically have a moderate breakdown point, sufficient to handle a reasonable proportion of contamination.</li>
</ol>
</div>
</div>
<div id="r-estimators" class="section level3" number="5.5.3">
<h3>
<span class="header-section-number">5.5.3</span> <span class="math inline">\(R\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#r-estimators"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(R\)</span>-estimators are a class of robust estimators that rely on the ranks of residuals rather than their raw magnitudes. This approach makes them naturally resistant to the influence of outliers and highly effective in scenarios involving ordinal data or heavy-tailed error distributions. By leveraging rank-based methods, <span class="math inline">\(R\)</span>-estimators are particularly useful in situations where classical assumptions about the data, such as normality or homoscedasticity, do not hold.</p>
<p>The general form of an <span class="math inline">\(R\)</span>-estimator can be expressed as:</p>
<p><span class="math display">\[
\hat{\beta}_R = \arg\min_\beta \sum_{i=1}^n w_i R_i \left(y_i - x_i'\beta\right),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(R_i\)</span> are the ranks of residuals <span class="math inline">\(e_i = y_i - x_i'\beta\)</span>,</li>
<li>
<span class="math inline">\(w_i\)</span> are rank-based weights determined by a chosen scoring function,</li>
<li>
<span class="math inline">\(y_i\)</span> are observed responses, <span class="math inline">\(x_i\)</span> are predictor values, and <span class="math inline">\(\beta\)</span> is the vector of coefficients.</li>
</ul>
<p>This formulation differs from <span class="math inline">\(M\)</span>-estimators, which directly minimize a loss function <span class="math inline">\(\rho\)</span>, by instead using the <strong>ordering of residuals</strong> to drive the estimation.</p>
<hr>
<div id="ranks-and-scoring-function" class="section level4" number="5.5.3.1">
<h4>
<span class="header-section-number">5.5.3.1</span> Ranks and Scoring Function<a class="anchor" aria-label="anchor" href="#ranks-and-scoring-function"><i class="fas fa-link"></i></a>
</h4>
<div id="definition-of-ranks" class="section level5" number="5.5.3.1.1">
<h5>
<span class="header-section-number">5.5.3.1.1</span> Definition of Ranks<a class="anchor" aria-label="anchor" href="#definition-of-ranks"><i class="fas fa-link"></i></a>
</h5>
<p>The rank <span class="math inline">\(R_i\)</span> of a residual <span class="math inline">\(e_i\)</span> is its position in the sorted sequence of all residuals:</p>
<p><span class="math display">\[
R_i = \text{rank}(e_i) = \sum_{j=1}^n \mathbb{I}(e_j \leq e_i),
\]</span></p>
<p>where <span class="math inline">\(\mathbb{I}(\cdot)\)</span> is the indicator function, equal to 1 if the condition is true and 0 otherwise. This step transforms the residuals into an ordinal scale, eliminating their dependency on magnitude.</p>
</div>
<div id="scoring-function" class="section level5" number="5.5.3.1.2">
<h5>
<span class="header-section-number">5.5.3.1.2</span> Scoring Function<a class="anchor" aria-label="anchor" href="#scoring-function"><i class="fas fa-link"></i></a>
</h5>
<p>The weights <span class="math inline">\(w_i\)</span> are derived from a scoring function <span class="math inline">\(S(R_i)\)</span>, which assigns importance to each rank. A common choice is the <strong>Wilcoxon scoring function</strong>, defined as:</p>
<p><span class="math display">\[
S(R_i) = \frac{R_i}{n + 1},
\]</span></p>
<p>which gives equal weight to all ranks, scaled by their position relative to the total number of observations <span class="math inline">\(n\)</span>.</p>
<p>Other scoring functions can emphasize different parts of the rank distribution:</p>
<ul>
<li>
<strong>Normal Scores</strong>: Derived from the quantiles of a standard normal distribution.</li>
<li>
<strong>Logarithmic Scores</strong>: Weight lower ranks more heavily.</li>
</ul>
<p>The flexibility of the scoring function allows <span class="math inline">\(R\)</span>-estimators to adapt to various data structures and assumptions.</p>
<hr>
</div>
</div>
<div id="properties-of-r-estimators" class="section level4" number="5.5.3.2">
<h4>
<span class="header-section-number">5.5.3.2</span> Properties of <span class="math inline">\(R\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#properties-of-r-estimators"><i class="fas fa-link"></i></a>
</h4>
<div id="influence-function-and-robustness" class="section level5" number="5.5.3.2.1">
<h5>
<span class="header-section-number">5.5.3.2.1</span> Influence Function and Robustness<a class="anchor" aria-label="anchor" href="#influence-function-and-robustness"><i class="fas fa-link"></i></a>
</h5>
<p>A key feature of <span class="math inline">\(R\)</span>-estimators is their <strong>bounded influence function</strong>, which ensures robustness. Because the estimator depends only on the ranks of the residuals, extreme values in <span class="math inline">\(y\)</span> or <span class="math inline">\(x\)</span> do not disproportionately affect the results.</p>
<p>For <span class="math inline">\(R\)</span>-estimators, the influence function <span class="math inline">\(\psi(e_i)\)</span> is proportional to the derivative of the rank-based objective function:</p>
<p><span class="math display">\[
\psi(e_i) = S'(R_i),
\]</span></p>
<p>where <span class="math inline">\(S'(R_i)\)</span> is the derivative of the scoring function. Since <span class="math inline">\(R_i\)</span> depends only on the ordering of residuals, outliers in the data cannot produce excessive changes in <span class="math inline">\(R_i\)</span>, resulting in bounded influence.</p>
</div>
<div id="breakdown-point" class="section level5" number="5.5.3.2.2">
<h5>
<span class="header-section-number">5.5.3.2.2</span> Breakdown Point<a class="anchor" aria-label="anchor" href="#breakdown-point"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>breakdown point</strong> of <span class="math inline">\(R\)</span>-estimators is higher than that of OLS and comparable to other robust methods. This means they can tolerate a larger proportion of contaminated data without yielding unreliable results.</p>
</div>
<div id="asymptotic-efficiency" class="section level5" number="5.5.3.2.3">
<h5>
<span class="header-section-number">5.5.3.2.3</span> Asymptotic Efficiency<a class="anchor" aria-label="anchor" href="#asymptotic-efficiency"><i class="fas fa-link"></i></a>
</h5>
<p>Under specific scoring functions, <span class="math inline">\(R\)</span>-estimators achieve high asymptotic efficiency. For example, the Wilcoxon <span class="math inline">\(R\)</span>-estimator performs nearly as well as OLS under normality while retaining robustness to non-normality.</p>
<hr>
</div>
</div>
<div id="derivation-of-r-estimators-for-simple-linear-regression" class="section level4" number="5.5.3.3">
<h4>
<span class="header-section-number">5.5.3.3</span> Derivation of <span class="math inline">\(R\)</span>-Estimators for Simple Linear Regression<a class="anchor" aria-label="anchor" href="#derivation-of-r-estimators-for-simple-linear-regression"><i class="fas fa-link"></i></a>
</h4>
<p>Consider the simple linear regression model:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + e_i,
\]</span></p>
<p>where <span class="math inline">\(e_i = y_i - (\beta_0 + \beta_1 x_i)\)</span> are the residuals.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Rank the Residuals</strong>: Compute the residuals <span class="math inline">\(e_i\)</span> for all observations and rank them from smallest to largest.</p></li>
<li><p><strong>Assign Weights</strong>: Compute weights <span class="math inline">\(w_i\)</span> for each residual rank based on the scoring function <span class="math inline">\(S(R_i)\)</span>.</p></li>
<li>
<p><strong>Minimize the Rank-Based Objective</strong>: Solve the following optimization problem:</p>
<p><span class="math display">\[
\hat{\beta}_R = \arg\min_{\beta_0, \beta_1} \sum_{i=1}^n w_i R_i \left( y_i - (\beta_0 + \beta_1 x_i) \right).
\]</span></p>
<p>This minimization can be performed iteratively using numerical methods, as the rank-based nature of the function makes direct analytic solutions challenging.</p>
</li>
</ol>
<hr>
</div>
<div id="comparison-to-m-estimators" class="section level4" number="5.5.3.4">
<h4>
<span class="header-section-number">5.5.3.4</span> Comparison to <span class="math inline">\(M\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#comparison-to-m-estimators"><i class="fas fa-link"></i></a>
</h4>
<p>While <span class="math inline">\(M\)</span>-estimators downweight large residuals using robust loss functions, <span class="math inline">\(R\)</span>-estimators completely avoid reliance on the magnitude of residuals by using their ranks. This distinction has important implications:</p>
<ul>
<li>
<span class="math inline">\(R\)</span>-estimators are naturally robust to leverage points and extreme outliers.</li>
<li>The performance of <span class="math inline">\(R\)</span>-estimators is less sensitive to the choice of scale parameter compared to <span class="math inline">\(M\)</span>-estimators.</li>
<li>However, <span class="math inline">\(R\)</span>-estimators may be less efficient than <span class="math inline">\(M\)</span>-estimators under normality because they do not use the full information contained in the residual magnitudes.</li>
</ul>
<hr>
</div>
</div>
<div id="l-estimators" class="section level3" number="5.5.4">
<h3>
<span class="header-section-number">5.5.4</span> <span class="math inline">\(L\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#l-estimators"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(L\)</span>-estimators are a class of robust estimators constructed as <strong>linear combinations of order statistics</strong>, where order statistics are simply the sorted values of a dataset. These estimators are particularly appealing due to their intuitive nature and computational simplicity. By using the relative ranks of observations, <span class="math inline">\(L\)</span>-estimators offer robustness against outliers and heavy-tailed distributions.</p>
<p>Order statistics are denoted as <span class="math inline">\(y_{(1)}, y_{(2)}, \dots, y_{(n)}\)</span>, where <span class="math inline">\(y_{(i)}\)</span> is the <span class="math inline">\(i\)</span>th smallest observation in the sample.</p>
<hr>
<p>The general form of an <span class="math inline">\(L\)</span>-estimator is:</p>
<p><span class="math display">\[
\hat{\theta}_L = \sum_{i=1}^n c_i y_{(i)},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(y_{(i)}\)</span> are the order statistics (sorted observations),</li>
<li>
<span class="math inline">\(c_i\)</span> are coefficients (weights) that determine the contribution of each order statistic to the estimator.</li>
</ul>
<p>By appropriately choosing the weights <span class="math inline">\(c_i\)</span>, different types of <span class="math inline">\(L\)</span>-estimators can be constructed to suit specific needs, such as handling outliers or capturing central tendencies robustly.</p>
<hr>
<p>Examples of <span class="math inline">\(L\)</span>-Estimators</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Sample Median</strong>: The sample median is a simple <span class="math inline">\(L\)</span>-estimator where only the middle order statistic contributes (for odd <span class="math inline">\(n\)</span>) or the average of the two middle order statistics contributes (for even <span class="math inline">\(n\)</span>):</p>
<p><span class="math display">\[
\hat{\mu}_{\text{median}} =
\begin{cases}
y_{\left(\frac{n+1}{2}\right)} &amp; \text{if } n \text{ is odd}, \\
\frac{1}{2}\left(y_{\left(\frac{n}{2}\right)} + y_{\left(\frac{n}{2} + 1\right)}\right) &amp; \text{if } n \text{ is even}.
\end{cases}
\]</span></p>
<ul>
<li>
<strong>Robustness</strong>: The median has a breakdown point of <span class="math inline">\(50\%\)</span>, meaning it remains unaffected unless more than half the data are corrupted.</li>
<li>
<strong>Efficiency</strong>: Under normality, the efficiency of the median is lower than that of the mean (about <span class="math inline">\(64\%\)</span>).</li>
</ul>
</li>
<li>
<p><strong>Trimmed Mean</strong>: The trimmed mean excludes the smallest and largest <span class="math inline">\(k\%\)</span> of observations before averaging the remaining values:</p>
<p><span class="math display">\[
\hat{\mu}_T = \frac{1}{n - 2k} \sum_{i=k+1}^{n-k} y_{(i)},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(k\)</span> is the number of observations trimmed from each tail,</p></li>
<li><p><span class="math inline">\(n\)</span> is the sample size.</p></li>
<li><p><strong>Robustness</strong>: The trimmed mean is less sensitive to extreme values than the sample mean.</p></li>
<li><p><strong>Efficiency</strong>: By retaining most observations, the trimmed mean achieves a good balance between robustness and efficiency.</p></li>
</ul>
</li>
<li>
<p><strong>Winsorized Mean</strong>: Similar to the trimmed mean, but instead of excluding extreme values, it replaces them with the nearest remaining observations:</p>
<p><span class="math display">\[
\hat{\mu}_W = \frac{1}{n} \sum_{i=1}^n y_{(i)}^*,
\]</span></p>
<p>where <span class="math inline">\(y_{(i)}^*\)</span> are “Winsorized” values: <span class="math display">\[
y_{(i)}^* =
\begin{cases}
y_{(k+1)} &amp; \text{if } i \leq k, \\
y_{(i)} &amp; \text{if } k+1 \leq i \leq n-k, \\
y_{(n-k)} &amp; \text{if } i &gt; n-k.
\end{cases}
\]</span></p>
<ul>
<li>
<strong>Robustness</strong>: The Winsorized mean reduces the influence of outliers without discarding data.</li>
<li>
<strong>Efficiency</strong>: Slightly less efficient than the trimmed mean under normality.</li>
</ul>
</li>
<li>
<p><strong>Midrange</strong>: The midrange is the average of the smallest and largest observations:</p>
<p><span class="math display">\[
\hat{\mu}_{\text{midrange}} = \frac{y_{(1)} + y_{(n)}}{2}.
\]</span></p>
<ul>
<li>
<strong>Robustness</strong>: Poor robustness, as it depends entirely on the extreme observations.</li>
<li>
<strong>Simplicity</strong>: Highly intuitive and computationally trivial.</li>
</ul>
</li>
</ol>
<hr>
<div id="properties-of-l-estimators" class="section level4" number="5.5.4.1">
<h4>
<span class="header-section-number">5.5.4.1</span> Properties of <span class="math inline">\(L\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#properties-of-l-estimators"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Robustness to Outliers</strong>: <span class="math inline">\(L\)</span>-estimators gain robustness by downweighting or excluding extreme observations. For instance:
<ul>
<li>The trimmed mean completely removes outliers from the estimation process.</li>
<li>The Winsorized mean limits the influence of outliers by bounding their values.</li>
</ul>
</li>
<li>
<strong>Breakdown Point</strong>:
<ul>
<li>The breakdown point of an <span class="math inline">\(L\)</span>-estimator depends on how many extreme observations are excluded or replaced.</li>
<li>The median has the highest possible breakdown point (<span class="math inline">\(50\%\)</span>), while the trimmed and Winsorized means have breakdown points proportional to the trimming percentage.</li>
</ul>
</li>
<li>
<strong>Efficiency</strong>:
<ul>
<li>The efficiency of <span class="math inline">\(L\)</span>-estimators varies depending on the underlying data distribution and the specific estimator.</li>
<li>For symmetric distributions, the trimmed mean and Winsorized mean approach the efficiency of the sample mean while being much more robust.</li>
</ul>
</li>
<li>
<strong>Computational Simplicity</strong>:
<ul>
<li>
<span class="math inline">\(L\)</span>-estimators involve simple operations like sorting and averaging, making them computationally efficient even for large datasets.</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="derivation-of-the-trimmed-mean" class="section level4" number="5.5.4.2">
<h4>
<span class="header-section-number">5.5.4.2</span> Derivation of the Trimmed Mean<a class="anchor" aria-label="anchor" href="#derivation-of-the-trimmed-mean"><i class="fas fa-link"></i></a>
</h4>
<p>To understand the robustness of the trimmed mean, consider a dataset with <span class="math inline">\(n\)</span> observations. Sorting the data gives <span class="math inline">\(y_{(1)} \leq y_{(2)} \leq \dots \leq y_{(n)}\)</span>. After trimming the smallest <span class="math inline">\(k\)</span> and largest <span class="math inline">\(k\)</span> observations, the remaining <span class="math inline">\(n - 2k\)</span> observations are used to compute the mean:</p>
<p><span class="math display">\[
\hat{\mu}_T = \frac{1}{n - 2k} \sum_{i=k+1}^{n-k} y_{(i)}.
\]</span></p>
<p>Key observations:</p>
<ul>
<li>
<strong>Impact of</strong> <span class="math inline">\(k\)</span>: Larger <span class="math inline">\(k\)</span> increases robustness by removing more extreme values but reduces efficiency by discarding more data.</li>
<li>
<strong>Choosing</strong> <span class="math inline">\(k\)</span>: In practice, <span class="math inline">\(k\)</span> is often chosen as a percentage of the total sample size, such as <span class="math inline">\(10\%\)</span> trimming (<span class="math inline">\(k = 0.1n\)</span>).</li>
</ul>
<hr>
</div>
</div>
<div id="least-trimmed-squares-lts" class="section level3" number="5.5.5">
<h3>
<span class="header-section-number">5.5.5</span> Least Trimmed Squares (LTS)<a class="anchor" aria-label="anchor" href="#least-trimmed-squares-lts"><i class="fas fa-link"></i></a>
</h3>
<p>Least Trimmed Squares (LTS) is a robust regression method that minimizes the sum of the smallest <span class="math inline">\(h\)</span> squared residuals, rather than using all residuals as in <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a>. This approach ensures that large residuals, often caused by outliers or leverage points, have no influence on the parameter estimation.</p>
<p>The LTS estimator is defined as:</p>
<p><span class="math display">\[
\hat{\beta}_{LTS} = \arg\min_\beta \sum_{i=1}^h r_{[i]}^2,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(r_{[i]}^2\)</span> are the ordered squared residuals, ranked from smallest to largest,</li>
<li>
<span class="math inline">\(h\)</span> is the subset size of residuals to include in the minimization, typically chosen as <span class="math inline">\(h = \lfloor n/2 \rfloor + 1\)</span> (where <span class="math inline">\(n\)</span> is the sample size).</li>
</ul>
<p>This trimming process ensures robustness by focusing on the best-fitting <span class="math inline">\(h\)</span> observations and ignoring the most extreme residuals.</p>
<hr>
<div id="motivation-for-lts" class="section level4" number="5.5.5.1">
<h4>
<span class="header-section-number">5.5.5.1</span> Motivation for LTS<a class="anchor" aria-label="anchor" href="#motivation-for-lts"><i class="fas fa-link"></i></a>
</h4>
<p>In OLS regression, the objective is to minimize the Residual Sum of Squares (RSS):</p>
<p><span class="math display">\[
RSS = \sum_{i=1}^n r_i^2,
\]</span></p>
<p>where <span class="math inline">\(r_i = y_i - x_i'\beta\)</span> are the residuals. However, this method is highly sensitive to outliers because even one large residual (<span class="math inline">\(r_i^2\)</span>) can dominate the RSS, distorting the parameter estimates <span class="math inline">\(\beta\)</span>.</p>
<p>LTS addresses this issue by trimming the largest residuals and focusing only on the <span class="math inline">\(h\)</span> smallest ones, thus preventing extreme values from affecting the fit. This approach provides a more robust estimate of the regression coefficients <span class="math inline">\(\beta\)</span>.</p>
<hr>
</div>
<div id="properties-of-lts" class="section level4" number="5.5.5.2">
<h4>
<span class="header-section-number">5.5.5.2</span> Properties of LTS<a class="anchor" aria-label="anchor" href="#properties-of-lts"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<p><strong>Objective Function</strong>: The LTS objective function is non-differentiable because it involves ordering the squared residuals. Formally, the ordered residuals are denoted as:</p>
<p><span class="math display">\[
r_{[1]}^2 \leq r_{[2]}^2 \leq \dots \leq r_{[n]}^2,
\]</span></p>
<p>and the objective is to minimize:</p>
<p><span class="math display">\[
\sum_{i=1}^h r_{[i]}^2.
\]</span></p>
<p>This requires sorting the squared residuals, making the computation more complex than OLS.</p>
</li>
<li>
<p><strong>Choice of</strong> <span class="math inline">\(h\)</span>: The parameter <span class="math inline">\(h\)</span> determines the number of residuals included in the minimization. A common choice is:</p>
<p><span class="math display">\[
h = \lfloor n/2 \rfloor + 1,
\]</span></p>
<p>which ensures a high breakdown point (discussed below). Smaller values of <span class="math inline">\(h\)</span> increase robustness but reduce efficiency, while larger <span class="math inline">\(h\)</span> values improve efficiency but decrease robustness.</p>
</li>
<li><p><strong>Breakdown Point</strong>: LTS has a <strong>breakdown point</strong> of approximately <span class="math inline">\(50\%\)</span>, the highest possible for a regression estimator. This means that LTS can handle up to <span class="math inline">\(50\%\)</span> of contaminated data (e.g., outliers) without yielding unreliable estimates.</p></li>
<li><p><strong>Robustness</strong>: By focusing only on the <span class="math inline">\(h\)</span> best-fitting observations, LTS naturally excludes outliers from the estimation process, making it highly robust to both vertical outliers (extreme values in <span class="math inline">\(y\)</span>) and leverage points (extreme values in <span class="math inline">\(x\)</span>).</p></li>
</ol>
<hr>
</div>
<div id="algorithm-for-lts" class="section level4" number="5.5.5.3">
<h4>
<span class="header-section-number">5.5.5.3</span> Algorithm for LTS<a class="anchor" aria-label="anchor" href="#algorithm-for-lts"><i class="fas fa-link"></i></a>
</h4>
<p>Computing the LTS estimator involves the following steps:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Initialization</strong>: Select an initial subset of <span class="math inline">\(h\)</span> observations to compute a preliminary fit for <span class="math inline">\(\beta\)</span>.</p></li>
<li>
<p><strong>Residual Calculation</strong>: For each observation, compute the squared residuals:</p>
<p><span class="math display">\[
r_i^2 = \left(y_i - x_i'\beta\right)^2.
\]</span></p>
</li>
<li><p><strong>Trimming</strong>: Rank the residuals from smallest to largest and retain only the <span class="math inline">\(h\)</span> smallest residuals.</p></li>
<li><p><strong>Refitting</strong>: Use the <span class="math inline">\(h\)</span> retained observations to recompute the regression coefficients <span class="math inline">\(\beta\)</span>.</p></li>
<li><p><strong>Iterative Refinement</strong>: Repeat the process (residual calculation, trimming, refitting) until convergence, typically when <span class="math inline">\(\beta\)</span> stabilizes.</p></li>
</ol>
<p>Efficient algorithms, such as the <strong>Fast-LTS</strong> algorithm, are used in practice to reduce computational complexity.</p>
<hr>
</div>
<div id="comparison-of-lts-with-ols" class="section level4" number="5.5.5.4">
<h4>
<span class="header-section-number">5.5.5.4</span> Comparison of LTS with OLS<a class="anchor" aria-label="anchor" href="#comparison-of-lts-with-ols"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="27%">
<col width="29%">
<col width="41%">
</colgroup>
<thead><tr class="header">
<th>Property</th>
<th>OLS</th>
<th>LTS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Objective</strong></td>
<td>Minimize <span class="math inline">\(\sum_{i=1}^n r_i^2\)</span>
</td>
<td>Minimize <span class="math inline">\(\sum_{i=1}^h r_{[i]}^2\)</span>
</td>
</tr>
<tr class="even">
<td><strong>Sensitivity to Outliers</strong></td>
<td>High</td>
<td>Low</td>
</tr>
<tr class="odd">
<td><strong>Breakdown Point</strong></td>
<td><span class="math inline">\(1/n\)</span></td>
<td><span class="math inline">\(\approx 50\%\)</span></td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td>Low</td>
<td>Moderate (requires sorting and iterations)</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="s-estimators" class="section level3" number="5.5.6">
<h3>
<span class="header-section-number">5.5.6</span> <span class="math inline">\(S\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#s-estimators"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(S\)</span>-estimators are a class of robust estimators that focus on minimizing a robust measure of the <strong>dispersion</strong> of residuals. Unlike methods such as <span class="math inline">\(M\)</span>-estimators, which directly minimize a loss function based on residuals, <span class="math inline">\(S\)</span>-estimators aim to find the parameter values <span class="math inline">\(\beta\)</span> that produce residuals with the smallest robust scale. These estimators are particularly useful in handling datasets with outliers, heavy-tailed distributions, or other violations of classical assumptions.</p>
<p>The scale <span class="math inline">\(\sigma\)</span> is estimated by solving the following minimization problem:</p>
<p><span class="math display">\[
\hat{\sigma}_S = \arg\min_\sigma \frac{1}{n} \sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\rho\)</span> is a robust loss function that controls the influence of residuals,</li>
<li>
<span class="math inline">\(y_i\)</span> are observed responses, <span class="math inline">\(x_i\)</span> are predictors, <span class="math inline">\(\beta\)</span> is the vector of regression coefficients,</li>
<li>
<span class="math inline">\(\sigma\)</span> represents the robust scale of the residuals.</li>
</ul>
<p>Once <span class="math inline">\(\sigma\)</span> is estimated, the <span class="math inline">\(S\)</span>-estimator of <span class="math inline">\(\beta\)</span> is obtained by solving:</p>
<p><span class="math display">\[
\hat{\beta}_S = \arg\min_\beta \hat{\sigma}_S.
\]</span></p>
<hr>
<div id="motivation-for-s-estimators" class="section level4" number="5.5.6.1">
<h4>
<span class="header-section-number">5.5.6.1</span> Motivation for <span class="math inline">\(S\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#motivation-for-s-estimators"><i class="fas fa-link"></i></a>
</h4>
<p>In regression analysis, classical methods such as Ordinary Least Squares rely on minimizing the Residual Sum of Squares (RSS). However, OLS is highly sensitive to outliers because even a single extreme residual can dominate the sum of squared residuals, leading to biased estimates of <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math inline">\(S\)</span>-estimators address this limitation by using a robust scale <span class="math inline">\(\sigma\)</span> to evaluate the dispersion of residuals. By minimizing this scale, <span class="math inline">\(S\)</span>-estimators effectively downweight the influence of outliers, resulting in parameter estimates that are more resistant to contamination in the data.</p>
<hr>
</div>
<div id="key-concepts-in-s-estimators" class="section level4" number="5.5.6.2">
<h4>
<span class="header-section-number">5.5.6.2</span> Key Concepts in <span class="math inline">\(S\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#key-concepts-in-s-estimators"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<p><strong>Robust Scale Function</strong>: The key idea of <span class="math inline">\(S\)</span>-estimators is to minimize a robust measure of scale. The scale <span class="math inline">\(\sigma\)</span> is computed such that the residuals normalized by <span class="math inline">\(\sigma\)</span> produce a value close to the expected contribution of well-behaved observations.</p>
<p>Formally, <span class="math inline">\(\sigma\)</span> satisfies:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right) = \delta,
\]</span></p>
<p>where <span class="math inline">\(\delta\)</span> is a constant that depends on the choice of <span class="math inline">\(\rho\)</span> and ensures consistency under normality. This equation balances the residuals and controls their influence on the scale estimate.</p>
</li>
<li>
<p><strong>Choice of</strong> <span class="math inline">\(\rho\)</span>-Function: The choice of the robust <span class="math inline">\(\rho\)</span> function is critical in determining the behavior of <span class="math inline">\(S\)</span>-estimators. Common <span class="math inline">\(\rho\)</span> functions include:</p>
<ul>
<li><p><strong>Huber’s</strong> <span class="math inline">\(\rho\)</span>-Function: <span class="math display">\[
\rho(z) =
\begin{cases}
z^2/2 &amp; \text{if } |z| \leq c, \\
c|z| - c^2/2 &amp; \text{if } |z| &gt; c.
\end{cases}
\]</span></p></li>
<li><p><strong>Tukey’s Bisquare</strong>: <span class="math display">\[
\rho(z) =
\begin{cases}
c^2 \left(1 - \left(1 - \left(\frac{z}{c}\right)^2\right)^3\right)/6 &amp; \text{if } |z| \leq c, \\
c^2/6 &amp; \text{if } |z| &gt; c.
\end{cases}
\]</span></p></li>
<li><p><strong>Andrews’ Sine</strong>: <span class="math display">\[
\rho(z) =
\begin{cases}
c^2 \left(1 - \cos\left(\frac{z}{c}\right)\right)/2 &amp; \text{if } |z| \leq \pi c, \\
c^2/2 &amp; \text{if } |z| &gt; \pi c.
\end{cases}
\]</span></p></li>
</ul>
<p>Robust <span class="math inline">\(\rho\)</span> functions grow more slowly than the quadratic function used in OLS, limiting the impact of large residuals.</p>
</li>
</ol>
<hr>
</div>
<div id="properties-of-s-estimators" class="section level4" number="5.5.6.3">
<h4>
<span class="header-section-number">5.5.6.3</span> Properties of <span class="math inline">\(S\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#properties-of-s-estimators"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><p><strong>Breakdown Point</strong>: <span class="math inline">\(S\)</span>-estimators have a breakdown point of up to <span class="math inline">\(50\%\)</span>, meaning they can tolerate up to half the data being contaminated (e.g., outliers) without yielding unreliable estimates.</p></li>
<li><p><strong>Efficiency</strong>: The efficiency of <span class="math inline">\(S\)</span>-estimators depends on the choice of <span class="math inline">\(\rho\)</span>. While they are highly robust, their efficiency under ideal conditions (e.g., normality) may be lower than that of OLS. Proper tuning of <span class="math inline">\(\rho\)</span> can balance robustness and efficiency.</p></li>
<li><p><strong>Influence Function</strong>: The <strong>influence function</strong> measures the sensitivity of the estimator to a small perturbation in the data. For <span class="math inline">\(S\)</span>-estimators, the influence function is bounded, ensuring robustness to outliers.</p></li>
<li><p><strong>Consistency</strong>: Under mild regularity conditions, <span class="math inline">\(S\)</span>-estimators are consistent, meaning <span class="math inline">\(\hat{\beta}_S \to \beta\)</span> as the sample size <span class="math inline">\(n \to \infty\)</span>.</p></li>
<li>
<p><strong>Asymptotic Normality</strong>: <span class="math inline">\(S\)</span>-estimators are asymptotically normal, with:</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\beta}_S - \beta) \xrightarrow{d} N(0, \Sigma),
\]</span></p>
<p>where <span class="math inline">\(\Sigma\)</span> depends on the choice of <span class="math inline">\(\rho\)</span> and the distribution of residuals.</p>
</li>
</ol>
<hr>
</div>
<div id="algorithm-for-computing-s-estimators" class="section level4" number="5.5.6.4">
<h4>
<span class="header-section-number">5.5.6.4</span> Algorithm for Computing <span class="math inline">\(S\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#algorithm-for-computing-s-estimators"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><p><strong>Initial Guess</strong>: Compute an initial estimate of <span class="math inline">\(\beta\)</span> using a robust method (e.g., LTS or an <span class="math inline">\(M\)</span>-estimator).</p></li>
<li>
<p><strong>Scale Estimation</strong>: Compute a robust estimate of scale <span class="math inline">\(\hat{\sigma}\)</span> by solving:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right) = \delta.
\]</span></p>
</li>
<li>
<p><strong>Iterative Refinement</strong>:</p>
<ul>
<li>Recalculate residuals <span class="math inline">\(r_i = y_i - x_i'\beta\)</span>.</li>
<li>Update <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> iteratively until convergence, typically using numerical optimization techniques.</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="mm-estimators" class="section level3" number="5.5.7">
<h3>
<span class="header-section-number">5.5.7</span> <span class="math inline">\(MM\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#mm-estimators"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(MM\)</span>-estimators are a robust regression method that combines the strengths of two powerful techniques: <span class="math inline">\(S\)</span>-estimators and <span class="math inline">\(M\)</span>-estimators. They are designed to achieve both a <strong>high breakdown point</strong> (up to <span class="math inline">\(50\%\)</span>) and <strong>high efficiency</strong> under ideal conditions (e.g., normality). This combination makes <span class="math inline">\(MM\)</span>-estimators one of the most versatile and widely used robust regression methods.</p>
<p>The process of computing <span class="math inline">\(MM\)</span>-estimators involves three main steps:</p>
<ol style="list-style-type: decimal">
<li>Compute an initial robust estimate of scale using an <span class="math inline">\(S\)</span>-estimator.</li>
<li>Use this robust scale to define weights for an <span class="math inline">\(M\)</span>-estimator.</li>
<li>Estimate regression coefficients by solving the weighted <span class="math inline">\(M\)</span>-estimation problem.</li>
</ol>
<p>This stepwise approach ensures robustness in the initial scale estimation while leveraging the efficiency of <span class="math inline">\(M\)</span>-estimators for the final parameter estimates.</p>
<hr>
<p>Step 1: Robust Scale Estimation</p>
<p>The first step is to estimate the robust scale <span class="math inline">\(\sigma\)</span> using an <span class="math inline">\(S\)</span>-estimator. This involves solving:</p>
<p><span class="math display">\[
\hat{\sigma}_S = \arg\min_\sigma \frac{1}{n} \sum_{i=1}^n \rho_S\left(\frac{y_i - x_i'\beta}{\sigma}\right),
\]</span></p>
<p>where <span class="math inline">\(\rho_S\)</span> is a robust loss function chosen to control the influence of extreme residuals. Common choices for <span class="math inline">\(\rho_S\)</span> include Huber’s or Tukey’s bisquare functions. This scale estimation provides a robust baseline for weighting residuals in the subsequent <span class="math inline">\(M\)</span>-estimation step.</p>
<hr>
<p>Step 2: Weight Definition for <span class="math inline">\(M\)</span>-Estimation</p>
<p>Using the robust scale <span class="math inline">\(\hat{\sigma}_S\)</span> obtained in Step 1, the weights for the <span class="math inline">\(M\)</span>-estimator are defined based on a second loss function, <span class="math inline">\(\rho_M\)</span>. The weights downweight residuals proportional to their deviation relative to <span class="math inline">\(\hat{\sigma}_S\)</span>. For each residual <span class="math inline">\(r_i = y_i - x_i'\beta\)</span>, the weight is computed as:</p>
<p><span class="math display">\[
w_i = \psi_M\left(\frac{r_i}{\hat{\sigma}_S}\right) / \frac{r_i}{\hat{\sigma}_S},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\psi_M\)</span> is the derivative of the robust <span class="math inline">\(\rho_M\)</span> function, known as the <strong>influence function</strong>.</li>
<li>
<span class="math inline">\(\rho_M\)</span> is often chosen to provide high efficiency under normality, such as Huber’s or Hampel’s function.</li>
</ul>
<p>These weights reduce the impact of large residuals while preserving the influence of small, well-behaved residuals.</p>
<hr>
<p>Step 3: Final <span class="math inline">\(M\)</span>-Estimation</p>
<p>The final step involves solving the <span class="math inline">\(M\)</span>-estimation problem using the weights defined in Step 2. The coefficients <span class="math inline">\(\hat{\beta}_{MM}\)</span> are estimated by minimizing the weighted residuals:</p>
<p><span class="math display">\[
\hat{\beta}_{MM} = \arg\min_\beta \sum_{i=1}^n w_i \rho_M\left(\frac{y_i - x_i'\beta}{\hat{\sigma}_S}\right).
\]</span></p>
<p>This ensures that the final estimates combine the robustness of the initial <span class="math inline">\(S\)</span>-estimator with the efficiency of the <span class="math inline">\(M\)</span>-estimator.</p>
<hr>
<div id="properties-of-mm-estimators" class="section level4" number="5.5.7.1">
<h4>
<span class="header-section-number">5.5.7.1</span> Properties of <span class="math inline">\(MM\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#properties-of-mm-estimators"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>High Breakdown Point</strong>:
<ul>
<li>The <span class="math inline">\(S\)</span>-estimator in the first step ensures a breakdown point of up to <span class="math inline">\(50\%\)</span>, meaning the estimator can handle up to half the data being contaminated without producing unreliable results.</li>
</ul>
</li>
<li>
<strong>Asymptotic Efficiency</strong>:
<ul>
<li>The use of an efficient <span class="math inline">\(\rho_M\)</span> function in the final <span class="math inline">\(M\)</span>-estimation step ensures that <span class="math inline">\(MM\)</span>-estimators achieve high asymptotic efficiency under normality, often close to that of OLS.</li>
</ul>
</li>
<li>
<strong>Robustness</strong>:
<ul>
<li>The combination of robust scale estimation and downweighting of large residuals makes <span class="math inline">\(MM\)</span>-estimators highly robust to outliers and leverage points.</li>
</ul>
</li>
<li>
<strong>Influence Function</strong>:
<ul>
<li>The influence function of <span class="math inline">\(MM\)</span>-estimators is bounded, ensuring that no single observation can exert disproportionate influence on the parameter estimates.</li>
</ul>
</li>
<li>
<strong>Consistency</strong>:
<ul>
<li>
<span class="math inline">\(MM\)</span>-estimators are consistent, converging to the true parameter values as the sample size increases, provided the majority of the data satisfies the model assumptions.</li>
</ul>
</li>
<li>
<strong>Asymptotic Normality</strong>:
<ul>
<li>
<p><span class="math inline">\(MM\)</span>-estimators are asymptotically normal, with:</p>
<p><span class="math display">\[
\sqrt{n} (\hat{\beta}_{MM} - \beta) \xrightarrow{d} N(0, \Sigma),
\]</span></p>
<p>where <span class="math inline">\(\Sigma\)</span> depends on the choice of <span class="math inline">\(\rho_M\)</span> and the distribution of residuals.</p>
</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="choice-of-rho-functions-for-mm-estimators" class="section level4" number="5.5.7.2">
<h4>
<span class="header-section-number">5.5.7.2</span> Choice of <span class="math inline">\(\rho\)</span>-Functions for <span class="math inline">\(MM\)</span>-Estimators<a class="anchor" aria-label="anchor" href="#choice-of-rho-functions-for-mm-estimators"><i class="fas fa-link"></i></a>
</h4>
<p>The robustness and efficiency of <span class="math inline">\(MM\)</span>-estimators depend on the choice of <span class="math inline">\(\rho_S\)</span> (for scale) and <span class="math inline">\(\rho_M\)</span> (for final estimation). Common choices include:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Huber’s</strong> <span class="math inline">\(\rho\)</span>-Function: Combines quadratic and linear growth to balance robustness and efficiency:</p>
<p><span class="math display">\[
\rho(z) =
\begin{cases}
\frac{z^2}{2} &amp; \text{if } |z| \leq c, \\
c|z| - \frac{c^2}{2} &amp; \text{if } |z| &gt; c.
\end{cases}
\]</span></p>
</li>
<li>
<p><strong>Tukey’s Bisquare Function</strong>: Provides high robustness by completely bounding large residuals:</p>
<p><span class="math display">\[
\rho(z) =
\begin{cases}
c^2 \left(1 - \left(1 - \left(\frac{z}{c}\right)^2\right)^3\right)/6 &amp; \text{if } |z| \leq c, \\
c^2/6 &amp; \text{if } |z| &gt; c.
\end{cases}
\]</span></p>
</li>
<li>
<p><strong>Hampel’s Three-Part Redescending Function</strong>: Further limits the influence of large residuals by assigning a constant penalty beyond a certain threshold.</p>
<p><span class="math display">\[
\rho(z) =
\begin{cases}
z^2/2 &amp; \text{if } |z| \leq a, \\
a|z| - a^2/2 &amp; \text{if } a &lt; |z| \leq b, \\
\text{constant} &amp; \text{if } |z| &gt; b.
\end{cases}
\]</span></p>
</li>
</ol>
<hr>
</div>
</div>
<div id="practical-considerations-1" class="section level3" number="5.5.8">
<h3>
<span class="header-section-number">5.5.8</span> Practical Considerations<a class="anchor" aria-label="anchor" href="#practical-considerations-1"><i class="fas fa-link"></i></a>
</h3>
<p>The following table summarizes the key properties, advantages, and limitations of the robust estimators discussed:
+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+
| Estimator | Key Features | Breakdown Point | Efficiency (Under Normality) | Applications | Advantages |
+=================+======================================================================+=========================+==============================+==========================================================================+===============================================+
| <span class="math inline">\(M\)</span>-Estimators | Generalization of OLS Robust <span class="math inline">\(\rho\)</span> reduces large residual influence | Moderate (up to <span class="math inline">\(0.29\)</span>) | High with proper tuning | Wide applicability in regression with moderate robustness | Balances robustness and efficiency |
| | | | | | |
| | | | | | Flexible tuning via <span class="math inline">\(\rho\)</span>-function |
+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+
| <span class="math inline">\(R\)</span>-Estimators | Rank-based method | High (depends on ranks) | Moderate | Ordinal data or heavily skewed distributions | Handles both predictor and response outliers |
| | | | | | |
| | Immune to outliers in <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> | | | | Suitable for ordinal or rank-based data |
+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+
| <span class="math inline">\(L\)</span>-Estimators | Linear combination of order statistics | High (up to <span class="math inline">\(50\%\)</span>) | Moderate | Descriptive statistics, robust averages | Simple and intuitive |
| | | | | | |
| | | | | | Easy to compute, even for large datasets |
+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+
| <strong>LTS</strong> | Minimizes smallest <span class="math inline">\(h\)</span> squared residuals | High (up to <span class="math inline">\(50\%\)</span>) | Moderate | Data with high contamination, fault detection | High robustness to outliers |
| | | | | | |
| | | | | | Resistant to leverage points |
+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+
| <span class="math inline">\(S\)</span>-Estimators | Minimizes robust scale of residuals | High (up to <span class="math inline">\(50\%\)</span>) | Low to moderate | Outlier detection, data with heavy-tailed distributions | Focus on robust scale estimation |
| | | | | | |
| | | | | | Effective at detecting extreme outliers |
+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+
| <span class="math inline">\(MM\)</span>-Estimators | High robustness (scale) + high efficiency (coefficients) | High (up to <span class="math inline">\(50\%\)</span>) | High | Real-world applications with mixed contamination and heavy-tailed errors | Combinesrobustness and efficiency effectively |
| | | | | | |
| | | | | | Versatile and flexible |
+—————–+———————————————————————-+————————-+——————————+————————————————————————–+———————————————–+</p>
<hr>
<p>Notes on Choosing an Estimator</p>
<ul>
<li>
<span class="math inline">\(M\)</span>-Estimators: Best suited for general-purpose robust regression, offering a balance between robustness and efficiency with moderate contamination.</li>
<li>
<span class="math inline">\(R\)</span>-Estimators: Ideal for rank-based data or ordinal data, especially when outliers are present in both predictors and responses.</li>
<li>
<span class="math inline">\(L\)</span>-Estimators: Simple and effective for descriptive statistics or data cleaning with limited computational resources.</li>
<li>
<strong>LTS</strong>: Recommended for datasets with significant contamination or leverage points due to its high breakdown point.</li>
<li>
<span class="math inline">\(S\)</span>-Estimators: Focus on robust scale estimation, suitable for identifying and mitigating the influence of extreme residuals.</li>
<li>
<span class="math inline">\(MM\)</span>-Estimators: Combines the robustness of <span class="math inline">\(S\)</span>-estimators with the efficiency of <span class="math inline">\(M\)</span>-estimators, making it the most versatile choice for heavily contaminated data.</li>
</ul>
<hr>
<div class="sourceCode" id="cb189"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span>       <span class="co"># For robust regression functions like rlm</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://robustbase.R-forge.R-project.org/">robustbase</a></span><span class="op">)</span> <span class="co"># For LTS regression and MM-estimators</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span>      <span class="co"># For data manipulation</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>    <span class="co"># For visualization</span></span>
<span></span>
<span><span class="co"># Simulate dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">5</span>, sd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>   <span class="co"># Predictor</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">3</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="co"># Response</span></span>
<span></span>
<span><span class="co"># Introduce outliers</span></span>
<span><span class="va">y</span><span class="op">[</span><span class="fl">95</span><span class="op">:</span><span class="fl">100</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="fl">95</span><span class="op">:</span><span class="fl">100</span><span class="op">]</span> <span class="op">+</span> <span class="fl">20</span>  <span class="co"># Vertical outliers</span></span>
<span><span class="va">x</span><span class="op">[</span><span class="fl">90</span><span class="op">:</span><span class="fl">95</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">90</span><span class="op">:</span><span class="fl">95</span><span class="op">]</span> <span class="op">+</span> <span class="fl">10</span>    <span class="co"># Leverage points</span></span>
<span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualize the data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Scatterplot of Simulated Data with Outliers"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Predictor (x)"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Response (y)"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-23-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb190"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Ordinary Least Squares</span></span>
<span><span class="va">ols_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">ols_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ x, data = data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -12.6023  -2.4590  -0.5717   0.9247  24.4024 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   8.8346     1.1550   7.649 1.41e-11 ***</span></span>
<span><span class="co">#&gt; x             0.9721     0.1749   5.558 2.36e-07 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 5.583 on 98 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.2396, Adjusted R-squared:  0.2319 </span></span>
<span><span class="co">#&gt; F-statistic: 30.89 on 1 and 98 DF,  p-value: 2.358e-07</span></span></code></pre></div>
<p>OLS coefficients are highly influenced by the presence of outliers. For example, the slope (x coefficient) and intercept are shifted to fit the outliers, resulting in a poor fit to the majority of the data.</p>
<div class="sourceCode" id="cb191"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># $M$-Estimators</span></span>
<span><span class="va">m_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/rlm.html">rlm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">data</span>, psi <span class="op">=</span> <span class="va">psi.huber</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call: rlm(formula = y ~ x, data = data, psi = psi.huber)</span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;       Min        1Q    Median        3Q       Max </span></span>
<span><span class="co">#&gt; -18.43919  -0.97575  -0.03297   0.76967  21.85546 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Value   Std. Error t value</span></span>
<span><span class="co">#&gt; (Intercept)  4.3229  0.2764    15.6421</span></span>
<span><span class="co">#&gt; x            1.7250  0.0419    41.2186</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.349 on 98 degrees of freedom</span></span></code></pre></div>
<p>The <span class="math inline">\(M\)</span>-estimator reduces the influence of large residuals using Huber’s psi function. This results in coefficients that are less affected by outliers compared to OLS.</p>
<div class="sourceCode" id="cb192"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Least Trimmed Squares (LTS)</span></span>
<span><span class="va">lts_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/robustbase/man/ltsReg.html">ltsReg</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span><span class="va">lts_coefficients</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">lts_model</span><span class="op">)</span></span></code></pre></div>
<p>LTS minimizes the smallest squared residuals, ignoring extreme residuals. This results in a more robust fit, particularly in the presence of both vertical outliers and leverage points.</p>
<div class="sourceCode" id="cb193"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># $MM$-Estimators</span></span>
<span><span class="va">mm_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/robustbase/man/lmrob.html">lmrob</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">data</span>, setting <span class="op">=</span> <span class="st">"KS2014"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mm_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lmrob(formula = y ~ x, data = data, setting = "KS2014")</span></span>
<span><span class="co">#&gt;  \--&gt; method = "SMDM"</span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;       Min        1Q    Median        3Q       Max </span></span>
<span><span class="co">#&gt; -20.45989  -0.69436  -0.01455   0.73614  22.10173 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  3.02192    0.25850   11.69   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x            1.96672    0.04538   43.34   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Robust residual standard error: 0.9458 </span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.9562, Adjusted R-squared:  0.9558 </span></span>
<span><span class="co">#&gt; Convergence in 7 IRWLS iterations</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Robustness weights: </span></span>
<span><span class="co">#&gt;  10 observations c(90,91,92,93,94,96,97,98,99,100)</span></span>
<span><span class="co">#&gt;   are outliers with |weight| = 0 ( &lt; 0.001); </span></span>
<span><span class="co">#&gt;  67 weights are ~= 1. The remaining 23 ones are summarized as</span></span>
<span><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">#&gt;  0.2496  0.7969  0.9216  0.8428  0.9548  0.9943 </span></span>
<span><span class="co">#&gt; Algorithmic parameters: </span></span>
<span><span class="co">#&gt;       tuning.chi1       tuning.chi2       tuning.chi3       tuning.chi4 </span></span>
<span><span class="co">#&gt;        -5.000e-01         1.500e+00                NA         5.000e-01 </span></span>
<span><span class="co">#&gt;                bb       tuning.psi1       tuning.psi2       tuning.psi3 </span></span>
<span><span class="co">#&gt;         5.000e-01        -5.000e-01         1.500e+00         9.500e-01 </span></span>
<span><span class="co">#&gt;       tuning.psi4        refine.tol           rel.tol         scale.tol </span></span>
<span><span class="co">#&gt;                NA         1.000e-07         1.000e-07         1.000e-10 </span></span>
<span><span class="co">#&gt;         solve.tol          zero.tol       eps.outlier             eps.x </span></span>
<span><span class="co">#&gt;         1.000e-07         1.000e-10         1.000e-03         3.223e-11 </span></span>
<span><span class="co">#&gt; warn.limit.reject warn.limit.meanrw </span></span>
<span><span class="co">#&gt;         5.000e-01         5.000e-01 </span></span>
<span><span class="co">#&gt;      nResample         max.it       best.r.s       k.fast.s          k.max </span></span>
<span><span class="co">#&gt;           1000            500             20              2           2000 </span></span>
<span><span class="co">#&gt;    maxit.scale      trace.lev            mts     compute.rd      numpoints </span></span>
<span><span class="co">#&gt;            200              0           1000              0             10 </span></span>
<span><span class="co">#&gt; fast.s.large.n </span></span>
<span><span class="co">#&gt;           2000 </span></span>
<span><span class="co">#&gt;               setting                   psi           subsampling </span></span>
<span><span class="co">#&gt;              "KS2014"                 "lqq"         "nonsingular" </span></span>
<span><span class="co">#&gt;                   cov compute.outlier.stats </span></span>
<span><span class="co">#&gt;             ".vcov.w"                "SMDM" </span></span>
<span><span class="co">#&gt; seed : int(0)</span></span></code></pre></div>
<p><span class="math inline">\(MM\)</span>-estimators combine robust scale estimation (from <span class="math inline">\(S\)</span>-estimators) with efficient coefficient estimation (from <span class="math inline">\(M\)</span>-estimators).
This achieves both high robustness and high efficiency under normal conditions.</p>
<div class="sourceCode" id="cb194"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Visualizing results</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>        ols_fit <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ols_model</span>, newdata <span class="op">=</span> <span class="va">data</span><span class="op">)</span>,</span>
<span>        m_fit <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">m_model</span>, newdata <span class="op">=</span> <span class="va">data</span><span class="op">)</span>,</span>
<span>        lts_fit <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">lts_model</span><span class="op">)</span>,</span>
<span>        <span class="co"># Use `fitted()` for ltsReg objects</span></span>
<span>        mm_fit <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mm_model</span>, newdata <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">ols_fit</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1</span>,</span>
<span>        label <span class="op">=</span> <span class="st">"OLS"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">m_fit</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1</span>,</span>
<span>        label <span class="op">=</span> <span class="st">"$M$-Estimator"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">lts_fit</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1</span>,</span>
<span>        label <span class="op">=</span> <span class="st">"LTS"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">mm_fit</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1</span>,</span>
<span>        label <span class="op">=</span> <span class="st">"$MM$-Estimator"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Comparison of Regression Fits"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Predictor (x)"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Response (y)"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-27-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Visualization shows the differences in regression fits:
- OLS is heavily influenced by outliers and provides a poor fit to the majority of the data.
)
- The <span class="math inline">\(M\)</span>-estimator downweights large residuals, resulting in a better fit.
- LTS regression ignores the extreme residuals entirely, providing the most robust fit.
- <span class="math inline">\(MM\)</span>-estimators balance robustness and efficiency, producing coefficients close to the LTS but with improved efficiency under normality.</p>
<div class="sourceCode" id="cb195"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Comparing Coefficients</span></span>
<span><span class="va">comparison</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"OLS"</span>, <span class="st">"$M$-Estimator"</span>, <span class="st">"LTS"</span>, <span class="st">"$MM$-Estimator"</span><span class="op">)</span>,</span>
<span>    Intercept <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ols_model</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">m_model</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,</span>
<span>        <span class="va">lts_coefficients</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mm_model</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>    <span class="op">)</span>,</span>
<span>    Slope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ols_model</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">m_model</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,</span>
<span>        <span class="va">lts_coefficients</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mm_model</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">comparison</span><span class="op">)</span></span>
<span><span class="co">#&gt;           Method Intercept     Slope</span></span>
<span><span class="co">#&gt; 1            OLS  8.834553 0.9720994</span></span>
<span><span class="co">#&gt; 2  $M$-Estimator  4.322869 1.7250441</span></span>
<span><span class="co">#&gt; 3            LTS  2.954960 1.9777635</span></span>
<span><span class="co">#&gt; 4 $MM$-Estimator  3.021923 1.9667208</span></span></code></pre></div>
<p>The table above shows how the coefficients vary across methods:
- OLS coefficients are the most distorted by outliers.
- <span class="math inline">\(M\)</span>-estimators and <span class="math inline">\(MM\)</span>-estimators provide coefficients that are less influenced by extreme values.
- LTS regression, with its trimming mechanism, produces the most robust coefficients by excluding the largest residuals.</p>
</div>
</div>
<div id="partial-least-squares" class="section level2" number="5.6">
<h2>
<span class="header-section-number">5.6</span> Partial Least Squares<a class="anchor" aria-label="anchor" href="#partial-least-squares"><i class="fas fa-link"></i></a>
</h2>
<p>Partial Least Squares (PLS) is a dimensionality reduction technique used for regression and predictive modeling. It is particularly useful when predictors are highly collinear or when the number of predictors (<span class="math inline">\(p\)</span>) exceeds the number of observations (<span class="math inline">\(n\)</span>). Unlike methods such as Principal Component Regression (PCR), PLS simultaneously considers the relationship between predictors and the response variable.</p>
<hr>
<div id="motivation-for-pls" class="section level3" number="5.6.1">
<h3>
<span class="header-section-number">5.6.1</span> Motivation for PLS<a class="anchor" aria-label="anchor" href="#motivation-for-pls"><i class="fas fa-link"></i></a>
</h3>
<p>Limitations of Classical Methods</p>
<ol style="list-style-type: decimal">
<li>
<strong>Multicollinearity</strong>:
<ul>
<li>OLS fails when predictors are highly correlated because the design matrix <span class="math inline">\(X'X\)</span> becomes nearly singular, leading to unstable estimates.</li>
</ul>
</li>
<li>
<strong>High-Dimensional Data</strong>:
<ul>
<li>When <span class="math inline">\(p &gt; n\)</span>, OLS cannot be directly applied as <span class="math inline">\(X'X\)</span> is not invertible.</li>
</ul>
</li>
<li>
<strong>Principal Component Regression (PCR)</strong>:
<ul>
<li>While PCR addresses multicollinearity by using principal components of <span class="math inline">\(X\)</span>, it does not account for the relationship between predictors and the response variable <span class="math inline">\(y\)</span> when constructing components.</li>
</ul>
</li>
</ol>
<p>PLS overcomes these limitations by constructing components that maximize the covariance between predictors <span class="math inline">\(X\)</span> and the response <span class="math inline">\(y\)</span>. It finds a compromise between explaining the variance in <span class="math inline">\(X\)</span> and predicting <span class="math inline">\(y\)</span>, making it particularly suited for regression in high-dimensional or collinear datasets.</p>
<hr>
<p>Let:</p>
<ul>
<li><p><span class="math inline">\(X\)</span> be the <span class="math inline">\(n \times p\)</span> matrix of predictors,</p></li>
<li><p><span class="math inline">\(y\)</span> be the <span class="math inline">\(n \times 1\)</span> response vector,</p></li>
<li><p><span class="math inline">\(t_k\)</span> be the <span class="math inline">\(k\)</span>-th latent component derived from <span class="math inline">\(X\)</span>,</p></li>
<li><p><span class="math inline">\(p_k\)</span> and <span class="math inline">\(q_k\)</span> be the loadings for <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>, respectively.</p></li>
</ul>
<p>PLS aims to construct latent components <span class="math inline">\(t_1, t_2, \ldots, t_K\)</span> such that:</p>
<ol style="list-style-type: decimal">
<li>Each <span class="math inline">\(t_k\)</span> is a linear combination of the predictors: <span class="math inline">\(t_k = X w_k\)</span>, where <span class="math inline">\(w_k\)</span> is a weight vector. 2</li>
<li>The covariance between <span class="math inline">\(t_k\)</span> and <span class="math inline">\(y\)</span> is maximized: <span class="math display">\[
   \text{Maximize } Cov(t_k, y) = w_k' X' y.
   \]</span>
</li>
</ol>
<hr>
</div>
<div id="steps-to-construct-pls-components" class="section level3" number="5.6.2">
<h3>
<span class="header-section-number">5.6.2</span> Steps to Construct PLS Components<a class="anchor" aria-label="anchor" href="#steps-to-construct-pls-components"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<strong>Compute Weights</strong>:
<ul>
<li>The weights <span class="math inline">\(w_k\)</span> for the <span class="math inline">\(k\)</span>-th component are obtained by solving: <span class="math display">\[
w_k = \frac{X'y}{\|X'y\|}.
\]</span>
</li>
</ul>
</li>
<li>
<strong>Construct Latent Component</strong>:
<ul>
<li>Form the <span class="math inline">\(k\)</span>-th latent component: <span class="math display">\[
t_k = X w_k.
\]</span>
</li>
</ul>
</li>
<li>
<strong>Deflate the Predictors</strong>:
<ul>
<li>After extracting <span class="math inline">\(t_k\)</span>, the predictors are deflated to remove the information explained by <span class="math inline">\(t_k\)</span>: <span class="math display">\[
X \leftarrow X - t_k p_k',
\]</span> where <span class="math inline">\(p_k = \frac{X't_k}{t_k't_k}\)</span> are the loadings for <span class="math inline">\(X\)</span>.</li>
</ul>
</li>
<li>
<strong>Deflate the Response</strong>:
<ul>
<li>Similarly, deflate <span class="math inline">\(y\)</span> to remove the variance explained by <span class="math inline">\(t_k\)</span>: <span class="math display">\[
y \leftarrow y - t_k q_k,
\]</span> where <span class="math inline">\(q_k = \frac{t_k'y}{t_k't_k}\)</span>.</li>
</ul>
</li>
<li>
<strong>Repeat for All Components</strong>:
<ul>
<li>Repeat the steps above until <span class="math inline">\(K\)</span> components are extracted.</li>
</ul>
</li>
</ol>
<p>After constructing <span class="math inline">\(K\)</span> components, the response <span class="math inline">\(y\)</span> is modeled as:</p>
<p><span class="math display">\[
y = T C + \epsilon,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(T = [t_1, t_2, \ldots, t_K]\)</span> is the matrix of latent components,</p></li>
<li><p><span class="math inline">\(C\)</span> is the vector of regression coefficients.</p></li>
</ul>
<p>The estimated coefficients for the original predictors are then:</p>
<p><span class="math display">\[
\hat{\beta} = W (P' W)^{-1} C,
\]</span></p>
<p>where <span class="math inline">\(W = [w_1, w_2, \ldots, w_K]\)</span> and <span class="math inline">\(P = [p_1, p_2, \ldots, p_K]\)</span>.</p>
<hr>
</div>
<div id="properties-of-pls" class="section level3" number="5.6.3">
<h3>
<span class="header-section-number">5.6.3</span> Properties of PLS<a class="anchor" aria-label="anchor" href="#properties-of-pls"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<strong>Dimensionality Reduction</strong>:
<ul>
<li>PLS reduces <span class="math inline">\(X\)</span> to <span class="math inline">\(K\)</span> components, where <span class="math inline">\(K \leq \min(n, p)\)</span>.</li>
</ul>
</li>
<li>
<strong>Handles Multicollinearity</strong>:
<ul>
<li>By constructing uncorrelated components, PLS avoids the instability caused by multicollinearity in OLS.</li>
</ul>
</li>
<li>
<strong>Supervised Dimensionality Reduction</strong>:
<ul>
<li>Unlike PCR, PLS considers the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span> when constructing components.</li>
</ul>
</li>
<li>
<strong>Efficiency</strong>:
<ul>
<li>PLS requires fewer components than PCR to achieve a similar level of predictive accuracy.</li>
</ul>
</li>
</ol>
<hr>
<p>Practical Considerations</p>
<ol style="list-style-type: decimal">
<li>
<strong>Number of Components</strong>:
<ul>
<li>The optimal number of components <span class="math inline">\(K\)</span> can be determined using cross-validation.</li>
</ul>
</li>
<li>
<strong>Preprocessing</strong>:
<ul>
<li>Standardizing predictors is essential for PLS, as it ensures that all variables are on the same scale.</li>
</ul>
</li>
<li>
<strong>Comparison with Other Methods</strong>:
<ul>
<li>PLS outperforms OLS and PCR in cases of multicollinearity or when <span class="math inline">\(p &gt; n\)</span>, but it may be less interpretable than sparse methods like Lasso.</li>
</ul>
</li>
</ol>
<hr>
<div class="sourceCode" id="cb196"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load required library</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/khliland/pls">pls</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 1: Simulate data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># Ensure reproducibility</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span>       <span class="co"># Number of observations</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">10</span>        <span class="co"># Number of predictors</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="va">p</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="va">n</span>, ncol <span class="op">=</span> <span class="va">p</span><span class="op">)</span>  <span class="co"># Design matrix (predictors)</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>                               <span class="co"># True coefficients</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>                     <span class="co"># Response variable with noise</span></span>
<span></span>
<span><span class="co"># Step 2: Fit Partial Least Squares (PLS) Regression</span></span>
<span><span class="va">pls_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/pls/man/mvr.html">plsr</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">X</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, validation <span class="op">=</span> <span class="st">"CV"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 3: Summarize the PLS Model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">pls_fit</span><span class="op">)</span></span>
<span><span class="co">#&gt; Data:    X dimension: 100 10 </span></span>
<span><span class="co">#&gt;  Y dimension: 100 1</span></span>
<span><span class="co">#&gt; Fit method: kernelpls</span></span>
<span><span class="co">#&gt; Number of components considered: 5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; VALIDATION: RMSEP</span></span>
<span><span class="co">#&gt; Cross-validated using 10 random segments.</span></span>
<span><span class="co">#&gt;        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps</span></span>
<span><span class="co">#&gt; CV           1.339    1.123    1.086    1.090    1.088    1.087</span></span>
<span><span class="co">#&gt; adjCV        1.339    1.112    1.078    1.082    1.080    1.080</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; TRAINING: % variance explained</span></span>
<span><span class="co">#&gt;    1 comps  2 comps  3 comps  4 comps  5 comps</span></span>
<span><span class="co">#&gt; X    10.88    20.06    30.80    42.19    51.61</span></span>
<span><span class="co">#&gt; y    44.80    48.44    48.76    48.78    48.78</span></span>
<span></span>
<span><span class="co"># Step 4: Perform Cross-Validation and Select Optimal Components</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/pls/man/validationplot.html">validationplot</a></span><span class="op">(</span><span class="va">pls_fit</span>, val.type <span class="op">=</span> <span class="st">"MSEP"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-29-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb197"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Step 5: Extract Coefficients for Predictors</span></span>
<span><span class="va">pls_coefficients</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">pls_fit</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">pls_coefficients</span><span class="op">)</span></span>
<span><span class="co">#&gt; , , 5 comps</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;               y</span></span>
<span><span class="co">#&gt; X1   0.30192935</span></span>
<span><span class="co">#&gt; X2  -0.03161151</span></span>
<span><span class="co">#&gt; X3   0.22392538</span></span>
<span><span class="co">#&gt; X4   0.42315637</span></span>
<span><span class="co">#&gt; X5   0.33000198</span></span>
<span><span class="co">#&gt; X6   0.66228763</span></span>
<span><span class="co">#&gt; X7   0.40452691</span></span>
<span><span class="co">#&gt; X8  -0.05704037</span></span>
<span><span class="co">#&gt; X9  -0.02699757</span></span>
<span><span class="co">#&gt; X10  0.05944765</span></span>
<span></span>
<span><span class="co"># Step 6: Evaluate Model Performance</span></span>
<span><span class="va">predicted_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">pls_fit</span>, <span class="va">X</span><span class="op">)</span></span>
<span><span class="va">actual_vs_predicted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  Actual <span class="op">=</span> <span class="va">y</span>,</span>
<span>  Predicted <span class="op">=</span> <span class="va">predicted_y</span><span class="op">[</span>, , <span class="fl">5</span><span class="op">]</span>  <span class="co"># Predicted values using 5 components</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot Actual vs Predicted</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">actual_vs_predicted</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Actual</span>, y <span class="op">=</span> <span class="va">Predicted</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_abline</a></span><span class="op">(</span></span>
<span>        intercept <span class="op">=</span> <span class="fl">0</span>,</span>
<span>        slope <span class="op">=</span> <span class="fl">1</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Actual vs Predicted Values (PLS Regression)"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Actual Values"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Predicted Values"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="05-linear-regression_files/figure-html/unnamed-chunk-29-2.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb198"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Step 7: Extract and Interpret Variable Importance (Loadings)</span></span>
<span><span class="va">loadings_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/class.html">unclass</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/pls/man/scores.html">loadings</a></span><span class="op">(</span><span class="va">pls_fit</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">variable_importance</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">loadings_matrix</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">variable_importance</span><span class="op">)</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"Component_"</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">variable_importance</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">variable_importance</span><span class="op">)</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"X"</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">variable_importance</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print variable importance</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">variable_importance</span><span class="op">)</span></span>
<span><span class="co">#&gt;     Component_1 Component_2 Component_3 Component_4 Component_5</span></span>
<span><span class="co">#&gt; X1  -0.04991097   0.5774569  0.24349681 -0.41550345 -0.02098351</span></span>
<span><span class="co">#&gt; X2   0.08913192  -0.1139342 -0.17582957 -0.05709948 -0.06707863</span></span>
<span><span class="co">#&gt; X3   0.13773357   0.1633338  0.07622919 -0.07248620 -0.61962875</span></span>
<span><span class="co">#&gt; X4   0.40369572  -0.2730457  0.69994206 -0.07949013  0.35239113</span></span>
<span><span class="co">#&gt; X5   0.50562681  -0.1788131 -0.27936562  0.36197480 -0.41919645</span></span>
<span><span class="co">#&gt; X6   0.57044281   0.3358522 -0.38683260  0.17656349  0.31154275</span></span>
<span><span class="co">#&gt; X7   0.36258623   0.1202109 -0.01753715 -0.12980483 -0.06919411</span></span>
<span><span class="co">#&gt; X8   0.12975452  -0.1164935 -0.30479310 -0.65654861  0.49948167</span></span>
<span><span class="co">#&gt; X9  -0.29521786   0.6170234 -0.32082508 -0.01041860  0.04904396</span></span>
<span><span class="co">#&gt; X10  0.23930055  -0.3259554  0.20006888 -0.53547258 -0.17963372</span></span></code></pre></div>
<p>The loadings provide the contribution of each predictor to the PLS components. Higher absolute values indicate stronger contributions to the corresponding component.</p>
<ul>
<li>
<p><strong>Summary of the Model</strong>:</p>
<ul>
<li><p>The <strong>proportion of variance explained</strong> indicates how much of the variability in both the predictors and response is captured by each PLS component.</p></li>
<li><p>The goal is to retain enough components to explain most of the variance while avoiding overfitting.</p></li>
</ul>
</li>
<li>
<p><strong>Validation Plot</strong>:</p>
<ul>
<li><p>The <strong>Mean Squared Error of Prediction (MSEP)</strong> curve is used to select the optimal number of components.</p></li>
<li><p>Adding too many components can lead to overfitting, while too few may underfit the data.</p></li>
</ul>
</li>
<li>
<p><strong>Coefficients</strong>:</p>
<ul>
<li><p>The extracted coefficients are the weights applied to the predictors in the final PLS model.</p></li>
<li><p>These coefficients are derived from the PLS components and may differ from OLS regression coefficients due to dimensionality reduction.</p></li>
</ul>
</li>
<li>
<p><strong>Actual vs Predicted Plot</strong>:</p>
<ul>
<li><p>This visualization evaluates how well the PLS model predicts the response variable.</p></li>
<li><p>Points tightly clustered around the diagonal indicate good performance.</p></li>
</ul>
</li>
<li>
<p><strong>VIP Scores</strong>:</p>
<ul>
<li><p>VIP scores help identify the most important predictors in the PLS model.</p></li>
<li><p>Predictors with higher VIP scores contribute more to explaining the response variable.</p></li>
</ul>
</li>
</ul>
</div>
<div id="comparison-with-related-methods" class="section level3" number="5.6.4">
<h3>
<span class="header-section-number">5.6.4</span> Comparison with Related Methods<a class="anchor" aria-label="anchor" href="#comparison-with-related-methods"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="13%">
<col width="22%">
<col width="29%">
<col width="15%">
<col width="16%">
</colgroup>
<thead><tr class="header">
<th><strong>Method</strong></th>
<th><strong>Handles Multicollinearity</strong></th>
<th><strong>Supervised Dimensionality Reduction</strong></th>
<th><strong>Sparse Solution</strong></th>
<th><strong>Interpretability</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>OLS</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>High</td>
</tr>
<tr class="even">
<td>Ridge Regression</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>Moderate</td>
</tr>
<tr class="odd">
<td>Lasso Regression</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>High</td>
</tr>
<tr class="even">
<td>PCR</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>Low</td>
</tr>
<tr class="odd">
<td>PLS</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Moderate</td>
</tr>
</tbody>
</table></div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></div>
<div class="next"><a href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#linear-regression"><span class="header-section-number">5</span> Linear Regression</a></li>
<li>
<a class="nav-link" href="#ordinary-least-squares"><span class="header-section-number">5.1</span> Ordinary Least Squares</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#simple-regression-basic-model"><span class="header-section-number">5.1.1</span> Simple Regression (Basic) Model</a></li>
<li><a class="nav-link" href="#multiple-linear-regression"><span class="header-section-number">5.1.2</span> Multiple Linear Regression</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#generalized-least-squares"><span class="header-section-number">5.2</span> Generalized Least Squares</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#infeasible-generalized-least-squares"><span class="header-section-number">5.2.1</span> Infeasible Generalized Least Squares</a></li>
<li><a class="nav-link" href="#feasible-generalized-least-squares"><span class="header-section-number">5.2.2</span> Feasible Generalized Least Squares</a></li>
<li><a class="nav-link" href="#weighted-least-squares"><span class="header-section-number">5.2.3</span> Weighted Least Squares</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#maximum-likelihood-estimator"><span class="header-section-number">5.3</span> Maximum Likelihood</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#motivation-for-mle"><span class="header-section-number">5.3.1</span> Motivation for MLE</a></li>
<li><a class="nav-link" href="#key-quantities-for-inference"><span class="header-section-number">5.3.2</span> Key Quantities for Inference</a></li>
<li><a class="nav-link" href="#assumptions-of-mle"><span class="header-section-number">5.3.3</span> Assumptions of MLE</a></li>
<li><a class="nav-link" href="#properties-of-mle"><span class="header-section-number">5.3.4</span> Properties of MLE</a></li>
<li><a class="nav-link" href="#practical-considerations"><span class="header-section-number">5.3.5</span> Practical Considerations</a></li>
<li><a class="nav-link" href="#comparison-of-mle-and-ols"><span class="header-section-number">5.3.6</span> Comparison of MLE and OLS</a></li>
<li><a class="nav-link" href="#applications-of-mle"><span class="header-section-number">5.3.7</span> Applications of MLE</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#penalized-regularized-estimators"><span class="header-section-number">5.4</span> Penalized (Regularized) Estimators</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#motivation-for-penalized-estimators"><span class="header-section-number">5.4.1</span> Motivation for Penalized Estimators</a></li>
<li><a class="nav-link" href="#ridge-regression"><span class="header-section-number">5.4.2</span> Ridge Regression</a></li>
<li><a class="nav-link" href="#lasso-regression"><span class="header-section-number">5.4.3</span> Lasso Regression</a></li>
<li><a class="nav-link" href="#elastic-net"><span class="header-section-number">5.4.4</span> Elastic Net</a></li>
<li><a class="nav-link" href="#tuning-parameter-selection"><span class="header-section-number">5.4.5</span> Tuning Parameter Selection</a></li>
<li><a class="nav-link" href="#properties-of-penalized-estimators"><span class="header-section-number">5.4.6</span> Properties of Penalized Estimators</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#robust-estimators"><span class="header-section-number">5.5</span> Robust Estimators</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#motivation-for-robust-estimation"><span class="header-section-number">5.5.1</span> Motivation for Robust Estimation</a></li>
<li><a class="nav-link" href="#m-estimators"><span class="header-section-number">5.5.2</span> \(M\)-Estimators</a></li>
<li><a class="nav-link" href="#r-estimators"><span class="header-section-number">5.5.3</span> \(R\)-Estimators</a></li>
<li><a class="nav-link" href="#l-estimators"><span class="header-section-number">5.5.4</span> \(L\)-Estimators</a></li>
<li><a class="nav-link" href="#least-trimmed-squares-lts"><span class="header-section-number">5.5.5</span> Least Trimmed Squares (LTS)</a></li>
<li><a class="nav-link" href="#s-estimators"><span class="header-section-number">5.5.6</span> \(S\)-Estimators</a></li>
<li><a class="nav-link" href="#mm-estimators"><span class="header-section-number">5.5.7</span> \(MM\)-Estimators</a></li>
<li><a class="nav-link" href="#practical-considerations-1"><span class="header-section-number">5.5.8</span> Practical Considerations</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#partial-least-squares"><span class="header-section-number">5.6</span> Partial Least Squares</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#motivation-for-pls"><span class="header-section-number">5.6.1</span> Motivation for PLS</a></li>
<li><a class="nav-link" href="#steps-to-construct-pls-components"><span class="header-section-number">5.6.2</span> Steps to Construct PLS Components</a></li>
<li><a class="nav-link" href="#properties-of-pls"><span class="header-section-number">5.6.3</span> Properties of PLS</a></li>
<li><a class="nav-link" href="#comparison-with-related-methods"><span class="header-section-number">5.6.4</span> Comparison with Related Methods</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/05-linear-regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/05-linear-regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-04-07.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
