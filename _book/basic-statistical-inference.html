<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Basic Statistical Inference | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Statistical inference involves drawing conclusions about population parameters based on sample data. The two primary goals of inference are: Making inferences about the true parameter value...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 4 Basic Statistical Inference | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/basic-statistical-inference.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Statistical inference involves drawing conclusions about population parameters based on sample data. The two primary goals of inference are: Making inferences about the true parameter value...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Basic Statistical Inference | A Guide on Data Analysis">
<meta name="twitter:description" content="Statistical inference involves drawing conclusions about population parameters based on sample data. The two primary goals of inference are: Making inferences about the true parameter value...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="active" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification-4.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="synthetic-difference-in-differences.html"><span class="header-section-number">25</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">26</span> Difference-in-differences</a></li>
<li><a class="" href="changes-in-changes.html"><span class="header-section-number">27</span> Changes-in-Changes</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">28</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">29</span> Event Studies</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">30</span> Instrumental Variables</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">31</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">32</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">33</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">34</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">35</span> Controls</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">36</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">37</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">38</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">39</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">40</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">41</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="basic-statistical-inference" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Basic Statistical Inference<a class="anchor" aria-label="anchor" href="#basic-statistical-inference"><i class="fas fa-link"></i></a>
</h1>
<p>Statistical inference involves drawing conclusions about population parameters based on sample data. The two primary goals of inference are:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Making inferences</strong> about the true parameter value (<span class="math inline">\(\beta\)</span>) based on our estimator or estimate:
<ul>
<li>This involves interpreting the sample-derived estimate to understand the population parameter.</li>
<li>Examples include estimating population means, variances, or proportions.</li>
</ul>
</li>
<li>
<strong>Testing whether underlying assumptions hold true</strong>, including:
<ul>
<li>Assumptions about the true population parameters (e.g., <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>).</li>
<li>Assumptions about random variables (e.g., independence, normality).</li>
<li>Assumptions about the model specification (e.g., linearity in regression).</li>
</ul>
</li>
</ol>
<p><strong>Note</strong>: Statistical testing does not:</p>
<ul>
<li><p>Confirm with absolute certainty that a hypothesis is true or false.</p></li>
<li>
<p>Interpret the magnitude of the estimated value in economic, practical, or business contexts without additional analysis.</p>
<ul>
<li><p><strong>Statistical significance</strong>: Refers to whether an observed effect is unlikely due to chance.</p></li>
<li><p><strong>Practical significance</strong>: Focuses on the real-world importance of the effect.</p></li>
</ul>
</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>A marketing campaign increases sales by <span class="math inline">\(0.5\%\)</span>, which is statistically significant (<span class="math inline">\(p &lt; 0.05\)</span>). However, in a small market, this may lack practical significance.</li>
</ul>
<p>Instead, inference provides a framework for making probabilistic statements about population parameters, given sample data.</p>
<hr>
<div id="hypothesis-testing-framework" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Hypothesis Testing Framework<a class="anchor" aria-label="anchor" href="#hypothesis-testing-framework"><i class="fas fa-link"></i></a>
</h2>
<p>Hypothesis testing is one of the fundamental tools in statistics. It provides a formal procedure to test claims or assumptions (hypotheses) about population parameters using sample data. This process is essential in various fields, including business, medicine, and social sciences, as it helps answer questions like “Does a new marketing strategy improve sales?” or “Is there a significant difference in test scores between two teaching methods?”</p>
<p>The goal of hypothesis testing is to make decisions or draw conclusions about a population based on sample data. This is necessary because we rarely have access to the entire population. For example, if a company wants to determine whether a new advertising campaign increases sales, it might analyze data from a sample of stores rather than every store globally.</p>
<p><strong>Key Steps in Hypothesis Testing</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Formulate Hypotheses</strong>: Define the null and alternative hypotheses.</li>
<li>
<strong>Choose a Significance Level</strong> (<span class="math inline">\(\alpha\)</span>): Determine the acceptable probability of making a Type I error.</li>
<li>
<strong>Select a Test Statistic</strong>: Identify the appropriate statistical test based on the data and hypotheses.</li>
<li>
<strong>Define the Rejection Region</strong>: Specify the range of values for which the null hypothesis will be rejected.</li>
<li>
<strong>Compute the Test Statistic</strong>: Use sample data to calculate the test statistic.</li>
<li>
<strong>Make a Decision</strong>: Compare the test statistic to the critical value or use the p-value to decide whether to reject or fail to reject the null hypothesis.</li>
</ol>
<hr>
<div id="null-and-alternative-hypotheses" class="section level3" number="4.1.1">
<h3>
<span class="header-section-number">4.1.1</span> Null and Alternative Hypotheses<a class="anchor" aria-label="anchor" href="#null-and-alternative-hypotheses"><i class="fas fa-link"></i></a>
</h3>
<p>At the heart of hypothesis testing lies the formulation of two competing hypotheses:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>):
<ul>
<li>Represents the current state of knowledge, status quo, or no effect.</li>
<li>It is assumed true unless there is strong evidence against it.</li>
<li>Examples:
<ul>
<li>
<span class="math inline">\(H_0: \mu_1 = \mu_2\)</span> (no difference in means between two groups).</li>
<li>
<span class="math inline">\(H_0: \beta = 0\)</span> (a predictor variable has no effect in a regression model).</li>
</ul>
</li>
<li>Think of <span class="math inline">\(H_0\)</span> as the “default assumption.”</li>
</ul>
</li>
<li>
<strong>Alternative Hypothesis (</strong><span class="math inline">\(H_a\)</span> or <span class="math inline">\(H_1\)</span>):
<ul>
<li>Represents a claim that contradicts the null hypothesis.</li>
<li>It is what you are trying to prove or find evidence for.</li>
<li>Examples:
<ul>
<li>
<span class="math inline">\(H_a: \mu_1 \neq \mu_2\)</span> (means of two groups are different).</li>
<li>
<span class="math inline">\(H_a: \beta \neq 0\)</span> (a predictor variable has an effect).</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="errors-in-hypothesis-testing" class="section level3" number="4.1.2">
<h3>
<span class="header-section-number">4.1.2</span> Errors in Hypothesis Testing<a class="anchor" aria-label="anchor" href="#errors-in-hypothesis-testing"><i class="fas fa-link"></i></a>
</h3>
<p>Hypothesis testing involves decision-making under uncertainty, meaning there is always a risk of making errors. These errors are classified into two types:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Type I Error</strong> (<span class="math inline">\(\alpha\)</span>):
<ul>
<li>Occurs when the null hypothesis is rejected, even though it is true.</li>
<li>Example: Concluding that a medication is effective when it actually has no effect.</li>
<li>The probability of making a Type I error is denoted by <span class="math inline">\(\alpha\)</span>, called the <strong>significance level</strong> (commonly set at 0.05 or 5%).</li>
</ul>
</li>
<li>
<strong>Type II Error</strong> (<span class="math inline">\(\beta\)</span>):
<ul>
<li>Occurs when the null hypothesis is not rejected, but the alternative hypothesis is true.</li>
<li>Example: Failing to detect that a medication is effective when it actually works.</li>
<li>The complement of <span class="math inline">\(\beta\)</span> is called the <strong>power</strong> of the test (<span class="math inline">\(1 - \beta\)</span>), representing the probability of correctly rejecting the null hypothesis.</li>
</ul>
</li>
</ol>
<p><strong>Analogy: The Legal System</strong></p>
<p>To make this concept more intuitive, consider the analogy of a courtroom:</p>
<ul>
<li>
<strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The defendant is innocent.</li>
<li>
<strong>Alternative Hypothesis (</strong><span class="math inline">\(H_a\)</span>): The defendant is guilty.</li>
<li>
<strong>Type I Error:</strong> Convicting an innocent person (false positive).</li>
<li>
<strong>Type II Error:</strong> Letting a guilty person go free (false negative).</li>
</ul>
<p>Balancing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> is critical in hypothesis testing, as reducing one often increases the other. For example, if you make it harder to reject <span class="math inline">\(H_0\)</span> (reducing <span class="math inline">\(\alpha\)</span>), you increase the chance of failing to detect a true effect (increasing <span class="math inline">\(\beta\)</span>).</p>
<hr>
</div>
<div id="the-role-of-distributions-in-hypothesis-testing" class="section level3" number="4.1.3">
<h3>
<span class="header-section-number">4.1.3</span> The Role of Distributions in Hypothesis Testing<a class="anchor" aria-label="anchor" href="#the-role-of-distributions-in-hypothesis-testing"><i class="fas fa-link"></i></a>
</h3>
<p>Distributions play a fundamental role in hypothesis testing because they provide a mathematical model for understanding how a test statistic behaves under the null hypothesis (<span class="math inline">\(H_0\)</span>). Without distributions, it would be impossible to determine whether the observed results are due to random chance or provide evidence to reject the null hypothesis.</p>
<div id="expected-outcomes" class="section level4" number="4.1.3.1">
<h4>
<span class="header-section-number">4.1.3.1</span> Expected Outcomes<a class="anchor" aria-label="anchor" href="#expected-outcomes"><i class="fas fa-link"></i></a>
</h4>
<p>One of the key reasons distributions are so crucial is that they describe the range of values a test statistic is likely to take when <span class="math inline">\(H_0\)</span> is true. This helps us understand what is considered “normal” variation in the data due to random chance. For example:</p>
<ul>
<li>Imagine you are conducting a study to test whether a new marketing strategy increases the average monthly sales. Under the null hypothesis, you assume the new strategy has no effect, and the average sales remain unchanged.</li>
<li>When you collect a sample and calculate the test statistic, you compare it to the expected distribution (e.g., the normal distribution for a <span class="math inline">\(z\)</span>-test). This distribution shows the range of test statistic values that are likely to occur purely due to random fluctuations in the data, assuming <span class="math inline">\(H_0\)</span> is true.</li>
</ul>
<p>By providing this baseline of what is “normal,” distributions allow us to identify unusual results that may indicate the null hypothesis is false.</p>
</div>
<div id="critical-values-and-rejection-regions" class="section level4" number="4.1.3.2">
<h4>
<span class="header-section-number">4.1.3.2</span> Critical Values and Rejection Regions<a class="anchor" aria-label="anchor" href="#critical-values-and-rejection-regions"><i class="fas fa-link"></i></a>
</h4>
<p>Distributions also help define critical values and rejection regions in hypothesis testing. Critical values are specific points on the distribution that mark the boundaries of the rejection region. The rejection region is the range of values for the test statistic that lead us to reject <span class="math inline">\(H_0\)</span>.</p>
<p>The location of these critical values depends on:</p>
<ul>
<li><p>The <strong>level of significance</strong> (<span class="math inline">\(\alpha\)</span>), which is the probability of rejecting <span class="math inline">\(H_0\)</span> when it is true (a Type I error).</p></li>
<li><p>The shape of the test statistic’s distribution under <span class="math inline">\(H_0\)</span>.</p></li>
</ul>
<p>For example:</p>
<ul>
<li>In a one-tailed <span class="math inline">\(z\)</span>-test with <span class="math inline">\(\alpha = 0.05\)</span>, the critical value is approximately <span class="math inline">\(1.645\)</span> for a standard normal distribution. If the calculated test statistic exceeds this value, we reject <span class="math inline">\(H_0\)</span> because such a result would be very unlikely under <span class="math inline">\(H_0\)</span>.</li>
</ul>
<p>Distributions help us visually and mathematically determine these critical points. By examining the distribution, we can see where the rejection region lies and what the probability is of observing a value in that region by random chance alone.</p>
</div>
<div id="p-values" class="section level4" number="4.1.3.3">
<h4>
<span class="header-section-number">4.1.3.3</span> P-values<a class="anchor" aria-label="anchor" href="#p-values"><i class="fas fa-link"></i></a>
</h4>
<p>The p-value, a central concept in hypothesis testing, is directly derived from the distribution of the test statistic under <span class="math inline">\(H_0\)</span>. The p-value represents the probability of observing a test statistic as extreme as (or more extreme than) the one calculated, assuming <span class="math inline">\(H_0\)</span> is true.</p>
<p>The <strong>p-value</strong> quantifies the strength of evidence against <span class="math inline">\(H_0\)</span>. It represents the probability of observing a test statistic as extreme as (or more extreme than) the one calculated, assuming <span class="math inline">\(H_0\)</span> is true.</p>
<ul>
<li>
<strong>Small p-value</strong> (<strong>&lt;</strong> <span class="math inline">\(\alpha\)</span>): Strong evidence against <span class="math inline">\(H_0\)</span>; reject <span class="math inline">\(H_0\)</span>.</li>
<li>
<strong>Large p-value</strong> (<strong>&gt;</strong> <span class="math inline">\(\alpha\)</span>): Weak evidence against <span class="math inline">\(H_0\)</span>; fail to reject <span class="math inline">\(H_0\)</span>.</li>
</ul>
<p>For example:</p>
<ul>
<li><p>Suppose you calculate a <span class="math inline">\(z\)</span>-test statistic of <span class="math inline">\(2.1\)</span> in a one-tailed test. Using the standard normal distribution, the p-value is the area under the curve to the right of <span class="math inline">\(z = 2.1\)</span>. This area represents the likelihood of observing a result as extreme as <span class="math inline">\(z = 2.1\)</span> if <span class="math inline">\(H_0\)</span> is true.</p></li>
<li><p>In this case, the p-value is approximately <span class="math inline">\(0.0179\)</span>. A small p-value (typically less than <span class="math inline">\(\alpha = 0.05\)</span>) suggests that the observed result is unlikely under <span class="math inline">\(H_0\)</span> and provides evidence to reject the null hypothesis.</p></li>
</ul>
</div>
<div id="why-does-all-this-matter" class="section level4" number="4.1.3.4">
<h4>
<span class="header-section-number">4.1.3.4</span> Why Does All This Matter?<a class="anchor" aria-label="anchor" href="#why-does-all-this-matter"><i class="fas fa-link"></i></a>
</h4>
<p>To summarize, distributions are the backbone of hypothesis testing because they allow us to:</p>
<ul>
<li><p>Define what is expected under <span class="math inline">\(H_0\)</span> by modeling the behavior of the test statistic.</p></li>
<li><p>Identify results that are unlikely to occur by random chance, which leads to the rejection of <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>Calculate p-values to quantify the strength of evidence against <span class="math inline">\(H_0\)</span>.</p></li>
</ul>
<p>Distributions provide the framework for understanding the role of chance in statistical analysis. They are essential for determining expected outcomes, setting thresholds for decision-making (critical values and rejection regions), and calculating p-values. A solid grasp of distributions will greatly enhance your ability to interpret and conduct hypothesis tests, making it easier to draw meaningful conclusions from data.</p>
<hr>
</div>
</div>
<div id="the-test-statistic" class="section level3" number="4.1.4">
<h3>
<span class="header-section-number">4.1.4</span> The Test Statistic<a class="anchor" aria-label="anchor" href="#the-test-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>The test statistic is a crucial component in hypothesis testing, as it quantifies how far the observed data deviates from what we would expect if the null hypothesis (<span class="math inline">\(H_0\)</span>) were true. Essentially, it provides a standardized way to compare the observed outcomes against the expectations set by <span class="math inline">\(H_0\)</span>, enabling us to assess whether the observed results are likely due to random chance or indicative of a significant effect.</p>
<p>The general formula for a test statistic is:</p>
<p><span class="math display">\[
\text{Test Statistic} = \frac{\text{Observed Value} - \text{Expected Value under } H_0}{\text{Standard Error}}
\]</span></p>
<p>Each component of this formula has an important role:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Numerator:</strong>
<ul>
<li>The numerator represents the difference between the actual data (observed value) and the hypothetical value (expected value) that is assumed under <span class="math inline">\(H_0\)</span>.</li>
<li>This difference quantifies the extent of the deviation. A larger deviation suggests stronger evidence against <span class="math inline">\(H_0\)</span>.</li>
</ul>
</li>
<li>
<strong>Denominator:</strong>
<ul>
<li>The denominator is the <strong>standard error</strong>, which measures the variability or spread of the data. It accounts for factors such as sample size and the inherent randomness of the data.</li>
<li>By dividing the numerator by the standard error, the test statistic is standardized, allowing comparisons across different studies, sample sizes, and distributions.</li>
</ul>
</li>
</ol>
<p>The test statistic plays a central role in determining whether to reject <span class="math inline">\(H_0\)</span>. Once calculated, it is compared to a known distribution (e.g., standard normal distribution for <span class="math inline">\(z\)</span>-tests or <span class="math inline">\(t\)</span>-distribution for <span class="math inline">\(t\)</span>-tests). This comparison allows us to evaluate the likelihood of observing such a test statistic under <span class="math inline">\(H_0\)</span>:</p>
<ul>
<li>
<strong>If the test statistic is close to 0:</strong> This indicates that the observed data is very close to what is expected under <span class="math inline">\(H_0\)</span>. There is little evidence to suggest rejecting <span class="math inline">\(H_0\)</span>.</li>
<li>
<strong>If the test statistic is far from 0 (in the tails of the distribution):</strong> This suggests that the observed data deviates significantly from the expectations under <span class="math inline">\(H_0\)</span>. Such deviations may provide strong evidence against <span class="math inline">\(H_0\)</span>.</li>
</ul>
<div id="why-standardizing-matters" class="section level4" number="4.1.4.1">
<h4>
<span class="header-section-number">4.1.4.1</span> Why Standardizing Matters<a class="anchor" aria-label="anchor" href="#why-standardizing-matters"><i class="fas fa-link"></i></a>
</h4>
<p>Standardizing the difference between the observed and expected values ensures that the test statistic is not biased by factors such as the scale of measurement or the size of the sample. For instance:</p>
<ul>
<li><p>A raw difference of 5 might be highly significant in one context but negligible in another, depending on the variability (standard error).</p></li>
<li><p>Standardizing ensures that the magnitude of the test statistic reflects both the size of the difference and the reliability of the sample data.</p></li>
</ul>
</div>
<div id="interpreting-the-test-statistic" class="section level4" number="4.1.4.2">
<h4>
<span class="header-section-number">4.1.4.2</span> Interpreting the Test Statistic<a class="anchor" aria-label="anchor" href="#interpreting-the-test-statistic"><i class="fas fa-link"></i></a>
</h4>
<p>After calculating the test statistic, it is used to:</p>
<ol style="list-style-type: decimal">
<li>Compare with a critical value: For example, in a <span class="math inline">\(z\)</span>-test with <span class="math inline">\(\alpha = 0.05\)</span>, the critical values are <span class="math inline">\(-1.96\)</span> and <span class="math inline">\(1.96\)</span> for a two-tailed test. If the test statistic falls beyond these values, <span class="math inline">\(H_0\)</span> is rejected.</li>
<li>Calculate the p-value: The p-value is derived from the distribution and reflects the probability of observing a test statistic as extreme as the one calculated if <span class="math inline">\(H_0\)</span> is true.</li>
</ol>
<hr>
</div>
</div>
<div id="critical-values-and-rejection-regions-1" class="section level3" number="4.1.5">
<h3>
<span class="header-section-number">4.1.5</span> Critical Values and Rejection Regions<a class="anchor" aria-label="anchor" href="#critical-values-and-rejection-regions-1"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>critical value</strong> is a point on the distribution that separates the rejection region from the non-rejection region:</p>
<ul>
<li>
<strong>Rejection Region</strong>: If the test statistic falls in this region, we reject <span class="math inline">\(H_0\)</span>.</li>
<li>
<strong>Non-Rejection Region</strong>: If the test statistic falls here, we fail to reject <span class="math inline">\(H_0\)</span>.</li>
</ul>
<p>The rejection region depends on the significance level (<span class="math inline">\(\alpha\)</span>). For a two-tailed test with <span class="math inline">\(\alpha = 0.05\)</span>, the critical values correspond to the top 2.5% and bottom 2.5% of the distribution.</p>
<hr>
</div>
<div id="visualizing-hypothesis-testing" class="section level3" number="4.1.6">
<h3>
<span class="header-section-number">4.1.6</span> Visualizing Hypothesis Testing<a class="anchor" aria-label="anchor" href="#visualizing-hypothesis-testing"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s create a visualization to tie these concepts together:</p>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Parameters</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span>  <span class="co"># Significance level</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fl">29</span>       <span class="co"># Degrees of freedom (for t-distribution)</span></span>
<span><span class="va">t_critical</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="fl">2</span>, <span class="va">df</span><span class="op">)</span>  <span class="co"># Critical value for two-tailed test</span></span>
<span></span>
<span><span class="co"># Generate t-distribution values</span></span>
<span><span class="va">t_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4</span>, <span class="fl">4</span>, length.out <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">density</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">dt</a></span><span class="op">(</span><span class="va">t_values</span>, <span class="va">df</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Observed test statistic</span></span>
<span><span class="va">t_obs</span> <span class="op">&lt;-</span> <span class="fl">2.5</span>  <span class="co"># Example observed test statistic</span></span>
<span></span>
<span><span class="co"># Plot the t-distribution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="va">t_values</span>,</span>
<span>    <span class="va">density</span>,</span>
<span>    type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>    lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Hypothesis Testing with Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Test Statistic (t-value)"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Density"</span>,</span>
<span>    ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.4</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Shade the rejection regions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/polygon.html">polygon</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">t_values</span><span class="op">[</span><span class="va">t_values</span> <span class="op">&lt;=</span> <span class="op">-</span><span class="va">t_critical</span><span class="op">]</span>, <span class="op">-</span><span class="va">t_critical</span><span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">density</span><span class="op">[</span><span class="va">t_values</span> <span class="op">&lt;=</span> <span class="op">-</span><span class="va">t_critical</span><span class="op">]</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>        col <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        border <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/polygon.html">polygon</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">t_values</span><span class="op">[</span><span class="va">t_values</span> <span class="op">&gt;=</span> <span class="va">t_critical</span><span class="op">]</span>, <span class="va">t_critical</span><span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">density</span><span class="op">[</span><span class="va">t_values</span> <span class="op">&gt;=</span> <span class="va">t_critical</span><span class="op">]</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>        col <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        border <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Add observed test statistic</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span></span>
<span>    <span class="va">t_obs</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">dt</a></span><span class="op">(</span><span class="va">t_obs</span>, <span class="va">df</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>    pch <span class="op">=</span> <span class="fl">19</span>,</span>
<span>    cex <span class="op">=</span> <span class="fl">1.5</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span></span>
<span>    <span class="va">t_obs</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">dt</a></span><span class="op">(</span><span class="va">t_obs</span>, <span class="va">df</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.02</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Observed t:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">t_obs</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>    pos <span class="op">=</span> <span class="fl">3</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Highlight the critical values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span></span>
<span>    v <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="va">t_critical</span>, <span class="va">t_critical</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"black"</span>,</span>
<span>    lty <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span></span>
<span>    <span class="op">-</span><span class="va">t_critical</span>,</span>
<span>    <span class="fl">0.05</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Critical Value:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="op">-</span><span class="va">t_critical</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    pos <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"black"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span></span>
<span>    <span class="va">t_critical</span>,</span>
<span>    <span class="fl">0.05</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Critical Value:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">t_critical</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    pos <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"black"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate p-value</span></span>
<span><span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">t_obs</span><span class="op">)</span>, <span class="va">df</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Two-tailed p-value</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">0</span>,</span>
<span>     <span class="fl">0.35</span>,</span>
<span>     <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"P-value:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_value</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span>,</span>
<span>     col <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>     pos <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Annotate regions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>,</span>
<span>     <span class="fl">0.15</span>,</span>
<span>     <span class="st">"Rejection Region"</span>,</span>
<span>     col <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>     pos <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">0.15</span>, <span class="st">"Rejection Region"</span>, col <span class="op">=</span> <span class="st">"red"</span>, pos <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">0</span>,</span>
<span>     <span class="fl">0.05</span>,</span>
<span>     <span class="st">"Non-Rejection Region"</span>,</span>
<span>     col <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>     pos <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Add legend</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>    <span class="st">"topright"</span>,</span>
<span>    legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Rejection Region"</span>, <span class="st">"Critical Value"</span>, <span class="st">"Observed Test Statistic"</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>, <span class="st">"black"</span>, <span class="st">"green"</span><span class="op">)</span>,</span>
<span>    lty <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fl">2</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>    pch <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">15</span>, <span class="cn">NA</span>, <span class="fl">19</span><span class="op">)</span>,</span>
<span>    bty <span class="op">=</span> <span class="st">"n"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="04-basic-inference_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;"></div>
<hr>
</div>
</div>
<div id="key-concepts-and-definitions" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Key Concepts and Definitions<a class="anchor" aria-label="anchor" href="#key-concepts-and-definitions"><i class="fas fa-link"></i></a>
</h2>
<div id="random-sample" class="section level3" number="4.2.1">
<h3>
<span class="header-section-number">4.2.1</span> Random Sample<a class="anchor" aria-label="anchor" href="#random-sample"><i class="fas fa-link"></i></a>
</h3>
<p>A random sample of size <span class="math inline">\(n\)</span> consists of <span class="math inline">\(n\)</span> independent observations, each drawn from the same underlying population distribution. Independence ensures that no observation influences another, and identical distribution guarantees that all observations are governed by the same probability rules.</p>
</div>
<div id="sample-statistics" class="section level3" number="4.2.2">
<h3>
<span class="header-section-number">4.2.2</span> Sample Statistics<a class="anchor" aria-label="anchor" href="#sample-statistics"><i class="fas fa-link"></i></a>
</h3>
<div id="sample-mean" class="section level4" number="4.2.2.1">
<h4>
<span class="header-section-number">4.2.2.1</span> Sample Mean<a class="anchor" aria-label="anchor" href="#sample-mean"><i class="fas fa-link"></i></a>
</h4>
<p>The sample mean is a measure of central tendency:</p>
<p><span class="math display">\[
\bar{X} = \frac{\sum_{i=1}^{n} X_i}{n}
\]</span></p>
<ul>
<li>Example: Suppose we measure the heights of 5 individuals (in cm): <span class="math inline">\(170, 165, 180, 175, 172\)</span>. The sample mean is:</li>
</ul>
<p><span class="math display">\[
\bar{X} = \frac{170 + 165 + 180 + 175 + 172}{5} = 172.4 \, \text{cm}.
\]</span></p>
</div>
<div id="sample-median" class="section level4" number="4.2.2.2">
<h4>
<span class="header-section-number">4.2.2.2</span> Sample Median<a class="anchor" aria-label="anchor" href="#sample-median"><i class="fas fa-link"></i></a>
</h4>
<p>The sample median is the middle value of ordered data:</p>
<p><span class="math display">\[
\tilde{x} = \begin{cases}
\text{Middle observation,} &amp; \text{if } n \text{ is odd}, \\
\text{Average of two middle observations,} &amp; \text{if } n \text{ is even}.
\end{cases}
\]</span></p>
</div>
<div id="sample-variance" class="section level4" number="4.2.2.3">
<h4>
<span class="header-section-number">4.2.2.3</span> Sample Variance<a class="anchor" aria-label="anchor" href="#sample-variance"><i class="fas fa-link"></i></a>
</h4>
<p>The sample variance measures data spread:</p>
<p><span class="math display">\[
S^2 = \frac{\sum_{i=1}^{n}(X_i - \bar{X})^2}{n-1}
\]</span></p>
</div>
<div id="sample-standard-deviation" class="section level4" number="4.2.2.4">
<h4>
<span class="header-section-number">4.2.2.4</span> Sample Standard Deviation<a class="anchor" aria-label="anchor" href="#sample-standard-deviation"><i class="fas fa-link"></i></a>
</h4>
<p>The sample standard deviation is the square root of the variance:</p>
<p><span class="math display">\[
S = \sqrt{S^2}
\]</span></p>
</div>
<div id="sample-proportions" class="section level4" number="4.2.2.5">
<h4>
<span class="header-section-number">4.2.2.5</span> Sample Proportions<a class="anchor" aria-label="anchor" href="#sample-proportions"><i class="fas fa-link"></i></a>
</h4>
<p>Used for categorical data:</p>
<p><span class="math display">\[
\hat{p} = \frac{X}{n} = \frac{\text{Number of successes}}{\text{Sample size}}
\]</span></p>
</div>
<div id="estimators" class="section level4" number="4.2.2.6">
<h4>
<span class="header-section-number">4.2.2.6</span> Estimators<a class="anchor" aria-label="anchor" href="#estimators"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<strong>Point Estimator</strong>: A statistic (<span class="math inline">\(\hat{\theta}\)</span>) used to estimate a population parameter (<span class="math inline">\(\theta\)</span>).</li>
<li>
<strong>Point Estimate</strong>:The numerical value assumed by <span class="math inline">\(\hat{\theta}\)</span> when evaluated for a given sample.</li>
<li>
<strong>Unbiased Estimator</strong>: A point estimator <span class="math inline">\(\hat{\theta}\)</span> is unbiased if <span class="math inline">\(E(\hat{\theta}) = \theta\)</span>.</li>
</ul>
<p>Examples of unbiased estimators:</p>
<ul>
<li><p><span class="math inline">\(\bar{X}\)</span> for <span class="math inline">\(\mu\)</span> (population mean).</p></li>
<li><p><span class="math inline">\(S^2\)</span> for <span class="math inline">\(\sigma^2\)</span> (population variance).</p></li>
<li><p><span class="math inline">\(\hat{p}\)</span> for <span class="math inline">\(p\)</span> (population proportion).</p></li>
<li><p><span class="math inline">\(\widehat{p_1-p_2}\)</span> for <span class="math inline">\(p_1- p_2\)</span> (population proportion difference)</p></li>
<li><p><span class="math inline">\(\bar{X_1} - \bar{X_2}\)</span> for <span class="math inline">\(\mu_1 - \mu_2\)</span> (population mean difference)</p></li>
</ul>
<p><strong>Note</strong>: While <span class="math inline">\(S^2\)</span> is unbiased for <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(S\)</span> is a biased estimator of <span class="math inline">\(\sigma\)</span>.</p>
<hr>
</div>
</div>
<div id="distribution-of-the-sample-mean" class="section level3" number="4.2.3">
<h3>
<span class="header-section-number">4.2.3</span> Distribution of the Sample Mean<a class="anchor" aria-label="anchor" href="#distribution-of-the-sample-mean"><i class="fas fa-link"></i></a>
</h3>
<p>The sampling distribution of the mean <span class="math inline">\(\bar{X}\)</span> depends on:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Population Distribution</strong>:
<ul>
<li>If <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\)</span>.</li>
</ul>
</li>
<li>
<strong>Central Limit Theorem</strong>:
<ul>
<li>For large <span class="math inline">\(n\)</span>, <span class="math inline">\(\bar{X}\)</span> approximately follows a normal distribution, regardless of the population’s shape.</li>
</ul>
</li>
</ol>
<div id="standard-error-of-the-mean" class="section level4" number="4.2.3.1">
<h4>
<span class="header-section-number">4.2.3.1</span> Standard Error of the Mean<a class="anchor" aria-label="anchor" href="#standard-error-of-the-mean"><i class="fas fa-link"></i></a>
</h4>
<p>The standard error quantifies variability in <span class="math inline">\(\bar{X}\)</span>:</p>
<p><span class="math display">\[
\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}
\]</span></p>
<p><strong>Example</strong>: - Suppose <span class="math inline">\(\sigma = 10\)</span> and <span class="math inline">\(n = 25\)</span>. Then: <span class="math display">\[
  \sigma_{\bar{X}} = \frac{10}{\sqrt{25}} = 2.
  \]</span></p>
<p>The smaller the standard error, the more precise our estimate of the population mean.</p>
<hr>
</div>
</div>
</div>
<div id="one-sample-inference" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> One-Sample Inference<a class="anchor" aria-label="anchor" href="#one-sample-inference"><i class="fas fa-link"></i></a>
</h2>
<div id="for-single-mean" class="section level3" number="4.3.1">
<h3>
<span class="header-section-number">4.3.1</span> For Single Mean<a class="anchor" aria-label="anchor" href="#for-single-mean"><i class="fas fa-link"></i></a>
</h3>
<p>Consider a scenario where</p>
<p><span class="math display">\[
Y_i \sim \text{i.i.d. } N(\mu, \sigma^2),
\]</span></p>
<p>where i.i.d. stands for “independent and identically distributed.” This model can be expressed as:</p>
<p><span class="math display">\[
Y_i = \mu + \epsilon_i,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\epsilon_i \sim^{\text{i.i.d.}} N(0, \sigma^2)\)</span>,</li>
<li>
<span class="math inline">\(E(Y_i) = \mu\)</span>,</li>
<li>
<span class="math inline">\(\text{Var}(Y_i) = \sigma^2\)</span>,</li>
<li>
<span class="math inline">\(\bar{y} \sim N(\mu, \sigma^2 / n)\)</span>.</li>
</ul>
<p>When <span class="math inline">\(\sigma^2\)</span> is estimated by <span class="math inline">\(s^2\)</span>, the standardized test statistic follows a <span class="math inline">\(t\)</span>-distribution:</p>
<p><span class="math display">\[
\frac{\bar{y} - \mu}{s / \sqrt{n}} \sim t_{n-1}.
\]</span></p>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> is obtained as:</p>
<p><span class="math display">\[
1 - \alpha = P\left(-t_{\alpha/2;n-1} \leq \frac{\bar{y} - \mu}{s / \sqrt{n}} \leq t_{\alpha/2;n-1}\right),
\]</span></p>
<p>or equivalently,</p>
<p><span class="math display">\[
P\left(\bar{y} - t_{\alpha/2;n-1}\frac{s}{\sqrt{n}} \leq \mu \leq \bar{y} + t_{\alpha/2;n-1}\frac{s}{\sqrt{n}}\right).
\]</span></p>
<p>The confidence interval is expressed as:</p>
<p><span class="math display">\[
\bar{y} \pm t_{\alpha/2;n-1}\frac{s}{\sqrt{n}},
\]</span></p>
<p>where <span class="math inline">\(s / \sqrt{n}\)</span> is the standard error of <span class="math inline">\(\bar{y}\)</span>.</p>
<p>If the experiment were repeated many times, <span class="math inline">\(100(1-\alpha)\%\)</span> of these intervals would contain <span class="math inline">\(\mu\)</span>.</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="24%">
<col width="25%">
<col width="24%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th>Case</th>
<th>Confidence Interval <span class="math inline">\(100(1-\alpha)\%\)</span>
</th>
<th>Sample Size (Confidence <span class="math inline">\(\alpha\)</span>, Error <span class="math inline">\(d\)</span>)</th>
<th>Hypothesis Test Statistic</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<span class="math inline">\(\sigma^2\)</span> known, <span class="math inline">\(X\)</span> normal (or <span class="math inline">\(n \geq 25\)</span>)</td>
<td><span class="math inline">\(\bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\)</span></td>
<td><span class="math inline">\(n \approx \frac{z_{\alpha/2}^2 \sigma^2}{d^2}\)</span></td>
<td><span class="math inline">\(z = \frac{\bar{X} - \mu_0}{\sigma / \sqrt{n}}\)</span></td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(\sigma^2\)</span> unknown, <span class="math inline">\(X\)</span> normal (or <span class="math inline">\(n \geq 25\)</span>)</td>
<td><span class="math inline">\(\bar{X} \pm t_{\alpha/2}\frac{s}{\sqrt{n}}\)</span></td>
<td><span class="math inline">\(n \approx \frac{z_{\alpha/2}^2 s^2}{d^2}\)</span></td>
<td><span class="math inline">\(t = \frac{\bar{X} - \mu_0}{s / \sqrt{n}}\)</span></td>
</tr>
</tbody>
</table></div>
<div id="power-in-hypothesis-testing" class="section level4" number="4.3.1.1">
<h4>
<span class="header-section-number">4.3.1.1</span> Power in Hypothesis Testing<a class="anchor" aria-label="anchor" href="#power-in-hypothesis-testing"><i class="fas fa-link"></i></a>
</h4>
<p>Power (<span class="math inline">\(\pi(\mu)\)</span>) of a hypothesis test represents the probability of correctly rejecting the null hypothesis (<span class="math inline">\(H_0\)</span>) when it is false (i.e., when alternative hypothesis <span class="math inline">\(H_A\)</span> is true). Formally, it is expressed as:</p>
<p><span class="math display">\[ \begin{aligned} \text{Power} &amp;= \pi(\mu) = 1 - \beta \\ &amp;= P(\text{test rejects } H_0|\mu) \\ &amp;= P(\text{test rejects } H_0| H_A \text{ is true}), \end{aligned} \]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is the probability of a Type II error (failing to reject <span class="math inline">\(H_0\)</span> when it is false).</p>
<p>To calculate this probability:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Under</strong> <span class="math inline">\(H_0\)</span>: The distribution of the test statistic is centered around the null parameter (e.g., <span class="math inline">\(\mu_0\)</span>).</p></li>
<li><p><strong>Under</strong> <span class="math inline">\(H_A\)</span>: The test statistic is distributed differently, shifted according to the true value under <span class="math inline">\(H_A\)</span> (e.g., <span class="math inline">\(\mu_1\)</span>).</p></li>
</ol>
<p>Hence, to evaluate the power, it is crucial to determine the distribution of the test statistic under the alternative hypothesis, <span class="math inline">\(H_A\)</span>.</p>
<p>Below, we derive the power for both one-sided and two-sided z-tests.</p>
<hr>
<div id="one-sided-z-test" class="section level5" number="4.3.1.1.1">
<h5>
<span class="header-section-number">4.3.1.1.1</span> One-Sided z-Test<a class="anchor" aria-label="anchor" href="#one-sided-z-test"><i class="fas fa-link"></i></a>
</h5>
<p>Consider the hypotheses:</p>
<p><span class="math display">\[ H_0: \mu \leq \mu_0 \quad \text{vs.} \quad H_A: \mu &gt; \mu_0 \]</span></p>
<p>The power for a one-sided z-test is derived as follows:</p>
<ol style="list-style-type: decimal">
<li>The test rejects <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\bar{y} &gt; \mu_0 + z_{\alpha} \frac{\sigma}{\sqrt{n}}\)</span>, where <span class="math inline">\(z_{\alpha}\)</span> is the critical value for the test at the significance level <span class="math inline">\(\alpha\)</span>.</li>
<li>Under the alternative hypothesis, the distribution of <span class="math inline">\(\bar{y}\)</span> is centered at <span class="math inline">\(\mu\)</span>, with standard deviation <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>.</li>
<li>The power is then:</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\pi(\mu) &amp;= P\left(\bar{y} &gt; \mu_0 + z_{\alpha} \frac{\sigma}{\sqrt{n}} \middle| \mu \right) \\
&amp;= P\left(Z &gt; z_{\alpha} + \frac{\mu_0 - \mu}{\sigma / \sqrt{n}} \middle| \mu \right), \quad \text{where } Z = \frac{\bar{y} - \mu}{\sigma / \sqrt{n}} \\
&amp;= 1 - \Phi\left(z_{\alpha} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}\right) \\
&amp;= \Phi\left(-z_{\alpha} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right).
\end{aligned}
\]</span></p>
<p>Here, we use the symmetry of the standard normal distribution: <span class="math inline">\(1 - \Phi(x) = \Phi(-x)\)</span>.</p>
<p>Suppose we wish to show that the mean response <span class="math inline">\(\mu\)</span> under the treatment is higher than the mean response <span class="math inline">\(\mu_0\)</span> without treatment (i.e., the treatment effect <span class="math inline">\(\delta = \mu - \mu_0\)</span> is large).</p>
<p>Since power is an increasing function of <span class="math inline">\(\mu - \mu_0\)</span>, it suffices to find the sample size <span class="math inline">\(n\)</span> that achieves the desired power <span class="math inline">\(1 - \beta\)</span> at <span class="math inline">\(\mu = \mu_0 + \delta\)</span>. The power at <span class="math inline">\(\mu = \mu_0 + \delta\)</span> is:</p>
<p><span class="math display">\[
\pi(\mu_0 + \delta) = \Phi\left(-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma}\right) = 1 - \beta
\]</span></p>
<p>Given <span class="math inline">\(\Phi(z_{\beta}) = 1 - \beta\)</span>, we have:</p>
<p><span class="math display">\[
-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma} = z_{\beta}
\]</span></p>
<p>Solving for <span class="math inline">\(n\)</span>, we obtain:</p>
<p><span class="math display">\[
n = \left(\frac{(z_{\alpha} + z_{\beta})\sigma}{\delta}\right)^2
\]</span></p>
<p>Larger sample sizes are required when:</p>
<ul>
<li>The sample variability is large (<span class="math inline">\(\sigma\)</span> is large).</li>
<li>The significance level <span class="math inline">\(\alpha\)</span> is small (<span class="math inline">\(z_{\alpha}\)</span> is large).</li>
<li>The desired power <span class="math inline">\(1 - \beta\)</span> is large (<span class="math inline">\(z_{\beta}\)</span> is large).</li>
<li>The magnitude of the effect is small (<span class="math inline">\(\delta\)</span> is small).</li>
</ul>
<p>In practice, <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\sigma\)</span> are often unknown. To estimate <span class="math inline">\(\sigma\)</span>, you can:</p>
<ol style="list-style-type: decimal">
<li>Use prior studies or pilot studies.</li>
<li>Approximate <span class="math inline">\(\sigma\)</span> based on the anticipated range of the observations (excluding outliers). For normally distributed data, dividing the range by 4 provides a reasonable estimate of <span class="math inline">\(\sigma\)</span>.</li>
</ol>
<p>These considerations ensure the test is adequately powered to detect meaningful effects while balancing practical constraints such as sample size.</p>
</div>
<div id="two-sided-z-test" class="section level5" number="4.3.1.1.2">
<h5>
<span class="header-section-number">4.3.1.1.2</span> Two-Sided z-Test<a class="anchor" aria-label="anchor" href="#two-sided-z-test"><i class="fas fa-link"></i></a>
</h5>
<p>For a two-sided test, the hypotheses are:</p>
<p><span class="math display">\[
H_0: \mu = \mu_0 \quad \text{vs.} \quad H_A: \mu \neq \mu_0
\]</span></p>
<p>The test rejects <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\bar{y}\)</span> lies outside the interval <span class="math inline">\(\mu_0 \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\)</span>. The power of the test is:</p>
<p><span class="math display">\[
\begin{aligned}
\pi(\mu) &amp;= P\left(\bar{y} &lt; \mu_0 - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \middle| \mu \right) + P\left(\bar{y} &gt; \mu_0 + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \middle| \mu \right) \\
&amp;= \Phi\left(-z_{\alpha/2} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) + \Phi\left(-z_{\alpha/2} - \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right).
\end{aligned}
\]</span></p>
<p>To ensure a power of <span class="math inline">\(1-\beta\)</span> when the treatment effect <span class="math inline">\(\delta = |\mu - \mu_0|\)</span> is at least a certain value, we solve for <span class="math inline">\(n\)</span>. Since the power function for a two-sided test is increasing and symmetric in <span class="math inline">\(|\mu - \mu_0|\)</span>, it suffices to find <span class="math inline">\(n\)</span> such that the power equals <span class="math inline">\(1-\beta\)</span> when <span class="math inline">\(\mu = \mu_0 + \delta\)</span>. This gives:</p>
<p><span class="math display">\[
n = \left(\frac{(z_{\alpha/2} + z_{\beta}) \sigma}{\delta}\right)^2
\]</span></p>
<p>Alternatively, the required sample size can be determined using a confidence interval approach. For a two-sided <span class="math inline">\(\alpha\)</span>-level confidence interval of the form:</p>
<p><span class="math display">\[
\bar{y} \pm D
\]</span></p>
<p>where <span class="math inline">\(D = z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\)</span>, solving for <span class="math inline">\(n\)</span> gives:</p>
<p><span class="math display">\[
n = \left(\frac{z_{\alpha/2} \sigma}{D}\right)^2
\]</span></p>
<p>This value should be rounded up to the nearest integer to ensure the required precision.</p>
<div class="sourceCode" id="cb105"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate random data and compute a 95% confidence interval</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span> <span class="co"># Generate 100 random values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">data</span>, conf.level <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span> <span class="co"># Perform t-test with 95% confidence interval</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  One Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data</span></span>
<span><span class="co">#&gt; t = -1.3809, df = 99, p-value = 0.1704</span></span>
<span><span class="co">#&gt; alternative hypothesis: true mean is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -0.33722662  0.06046365</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt;  mean of x </span></span>
<span><span class="co">#&gt; -0.1383815</span></span></code></pre></div>
<p>For a one-sided hypothesis test, such as testing <span class="math inline">\(H_0: \mu \geq 30\)</span> versus <span class="math inline">\(H_a: \mu &lt; 30\)</span>:</p>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Perform one-sided t-test</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">data</span>, mu <span class="op">=</span> <span class="fl">30</span>, alternative <span class="op">=</span> <span class="st">"less"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  One Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data</span></span>
<span><span class="co">#&gt; t = -300.74, df = 99, p-value &lt; 2.2e-16</span></span>
<span><span class="co">#&gt; alternative hypothesis: true mean is less than 30</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;        -Inf 0.02801196</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt;  mean of x </span></span>
<span><span class="co">#&gt; -0.1383815</span></span></code></pre></div>
<p>When <span class="math inline">\(\sigma\)</span> is unknown, you can estimate it using:</p>
<ol style="list-style-type: decimal">
<li><p>Prior studies or pilot studies.</p></li>
<li><p>The range of observations (excluding outliers) divided by 4, which provides a reasonable approximation for normally distributed data.</p></li>
</ol>
</div>
<div id="z-test-summary" class="section level5" number="4.3.1.1.3">
<h5>
<span class="header-section-number">4.3.1.1.3</span> z-Test Summary<a class="anchor" aria-label="anchor" href="#z-test-summary"><i class="fas fa-link"></i></a>
</h5>
<ul>
<li>For one-sided tests:</li>
</ul>
<p><span class="math display">\[ \pi(\mu) = \Phi\left(-z_{\alpha} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) \]</span></p>
<ul>
<li>For two-sided tests:</li>
</ul>
<p><span class="math display">\[ \pi(\mu) = \Phi\left(-z_{\alpha/2} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) + \Phi\left(-z_{\alpha/2} - \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) \]</span></p>
<p><strong>Factors Affecting Power</strong></p>
<ul>
<li>
<strong>Effect Size (</strong><span class="math inline">\(\mu - \mu_0\)</span>): Larger differences between <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\mu_0\)</span> increase power.</li>
<li>
<strong>Sample Size (</strong><span class="math inline">\(n\)</span>): Larger <span class="math inline">\(n\)</span> reduces the standard error, increasing power.</li>
<li>
<strong>Variance (</strong><span class="math inline">\(\sigma^2\)</span>): Smaller variance increases power.</li>
<li>
<strong>Significance Level (</strong><span class="math inline">\(\alpha\)</span>): Increasing <span class="math inline">\(\alpha\)</span> (making the test more liberal) increases power through <span class="math inline">\(z_{\alpha}\)</span>.</li>
</ul>
</div>
<div id="one-sample-t-test" class="section level5" number="4.3.1.1.4">
<h5>
<span class="header-section-number">4.3.1.1.4</span> One-Sample t-test<a class="anchor" aria-label="anchor" href="#one-sample-t-test"><i class="fas fa-link"></i></a>
</h5>
<p>In hypothesis testing, calculating the power and determining the required sample size for <strong>t-tests</strong> are more complex than for <strong>z-tests</strong>. This complexity arises from the involvement of the <strong>Student’s t-distribution</strong> and its generalized form, the <strong>non-central t-distribution</strong>.</p>
<p>The power function for a one-sample t-test can be expressed as:</p>
<p><span class="math display">\[
\pi(\mu) = P\left(\frac{\bar{y} - \mu_0}{s / \sqrt{n}} &gt; t_{n-1; \alpha} \mid \mu \right)
\]</span></p>
<p>Here:</p>
<ul>
<li><p><span class="math inline">\(\mu_0\)</span> is the hypothesized population mean under the null hypothesis,</p></li>
<li><p><span class="math inline">\(\bar{y}\)</span> is the sample mean,</p></li>
<li><p><span class="math inline">\(s\)</span> is the sample standard deviation,</p></li>
<li><p><span class="math inline">\(n\)</span> is the sample size,</p></li>
<li><p><span class="math inline">\(t_{n-1; \alpha}\)</span> is the critical t-value from the Student’s t-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom at significance level <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
<p>When <span class="math inline">\(\mu &gt; \mu_0\)</span> (i.e., <span class="math inline">\(\mu - \mu_0 = \delta\)</span>), the random variable</p>
<p><span class="math display">\[
T = \frac{\bar{y} - \mu_0}{s / \sqrt{n}}
\]</span></p>
<p>does not follow the Student’s t-distribution. Instead, it follows a <strong>non-central t-distribution</strong> with:</p>
<ul>
<li><p>a <strong>non-centrality parameter</strong> <span class="math inline">\(\lambda = \delta \sqrt{n} / \sigma\)</span>, where <span class="math inline">\(\sigma\)</span> is the population standard deviation,</p></li>
<li><p>degrees of freedom <span class="math inline">\(n-1\)</span>.</p></li>
</ul>
<p><strong>Key Properties of the Power Function</strong></p>
<ul>
<li>The power <span class="math inline">\(\pi(\mu)\)</span> is an increasing function of the non-centrality parameter <span class="math inline">\(\lambda\)</span>.</li>
<li>For <span class="math inline">\(\delta = 0\)</span> (i.e., when the null hypothesis is true), the non-central t-distribution simplifies to the regular Student’s t-distribution.</li>
</ul>
<p>To calculate the power in practice, numerical procedures (see below) or precomputed charts are typically required.</p>
<p><strong>Approximate Sample Size Adjustment for t-tests</strong></p>
<p>When planning a study, researchers often start with an approximation based on <strong>z-tests</strong> and then adjust for the specifics of the t-test. Here’s the process:</p>
<p>1. Start with the Sample Size for a z-test</p>
<p>For a two-sided test: <span class="math display">\[
n_z = \frac{\left(z_{\alpha/2} + z_\beta\right)^2 \sigma^2}{\delta^2}
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(z_{\alpha/2}\)</span> is the critical value from the standard normal distribution for a two-tailed test,</p></li>
<li><p><span class="math inline">\(z_\beta\)</span> corresponds to the desired power <span class="math inline">\(1 - \beta\)</span>,</p></li>
<li><p><span class="math inline">\(\delta\)</span> is the effect size <span class="math inline">\(\mu - \mu_0\)</span>,</p></li>
<li><p><span class="math inline">\(\sigma\)</span> is the population standard deviation.</p></li>
</ul>
<p>2. Adjust for the t-distribution</p>
<p>Let <span class="math inline">\(v = n - 1\)</span>, where <span class="math inline">\(n\)</span> is the sample size derived from the z-test. For a two-sided t-test, the approximate sample size is:</p>
<p><span class="math display">\[
n^* = \frac{\left(t_{v; \alpha/2} + t_{v; \beta}\right)^2 \sigma^2}{\delta^2}
\]</span></p>
<p>Here:</p>
<ul>
<li><p><span class="math inline">\(t_{v; \alpha/2}\)</span> and <span class="math inline">\(t_{v; \beta}\)</span> are the critical values from the Student’s t-distribution for the significance level <span class="math inline">\(\alpha\)</span> and desired power, respectively.</p></li>
<li><p>Since <span class="math inline">\(v\)</span> depends on <span class="math inline">\(n^*\)</span>, this process may require iterative refinement.</p></li>
</ul>
<p>Notes:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Approximations</strong>: The above formulas provide an intuitive starting point but may require adjustments based on exact numerical solutions.</li>
<li>
<strong>Insights</strong>: Power is an increasing function of:
<ul>
<li>the effect size <span class="math inline">\(\delta\)</span>,</li>
<li>the sample size <span class="math inline">\(n\)</span>,</li>
<li>and a decreasing function of the population variability <span class="math inline">\(\sigma\)</span>.</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Power calculation for a one-sample t-test</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/heliosdrm/pwr">pwr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Parameters</span></span>
<span><span class="va">effect_size</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># Cohen's d</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span>       <span class="co"># Significance level</span></span>
<span><span class="va">power</span> <span class="op">&lt;-</span> <span class="fl">0.8</span>        <span class="co"># Desired power</span></span>
<span></span>
<span><span class="co"># Compute sample size</span></span>
<span><span class="va">sample_size</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/pwr/man/pwr.t.test.html">pwr.t.test</a></span><span class="op">(</span></span>
<span>        d <span class="op">=</span> <span class="va">effect_size</span>,</span>
<span>        sig.level <span class="op">=</span> <span class="va">alpha</span>,</span>
<span>        power <span class="op">=</span> <span class="va">power</span>,</span>
<span>        type <span class="op">=</span> <span class="st">"one.sample"</span></span>
<span>    <span class="op">)</span><span class="op">$</span><span class="va">n</span></span>
<span></span>
<span><span class="co"># Print result</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Required sample size for one-sample t-test:"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">ceiling</a></span><span class="op">(</span><span class="va">sample_size</span><span class="op">)</span>,</span>
<span>    <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Required sample size for one-sample t-test: 34</span></span>
<span></span>
<span><span class="co"># Power calculation for a given sample size</span></span>
<span><span class="va">calculated_power</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/pwr/man/pwr.t.test.html">pwr.t.test</a></span><span class="op">(</span></span>
<span>        n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">ceiling</a></span><span class="op">(</span><span class="va">sample_size</span><span class="op">)</span>,</span>
<span>        d <span class="op">=</span> <span class="va">effect_size</span>,</span>
<span>        sig.level <span class="op">=</span> <span class="va">alpha</span>,</span>
<span>        type <span class="op">=</span> <span class="st">"one.sample"</span></span>
<span>    <span class="op">)</span><span class="op">$</span><span class="va">power</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Achieved power with computed sample size:"</span>,</span>
<span>    <span class="va">calculated_power</span>,</span>
<span>    <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Achieved power with computed sample size: 0.8077775</span></span></code></pre></div>
</div>
</div>
</div>
<div id="for-difference-of-means-independent-samples" class="section level3" number="4.3.2">
<h3>
<span class="header-section-number">4.3.2</span> For Difference of Means, Independent Samples<a class="anchor" aria-label="anchor" href="#for-difference-of-means-independent-samples"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="13%">
<col width="23%">
<col width="25%">
<col width="37%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>
<span class="math inline">\(100(1-\alpha)%\)</span> Confidence Interval</th>
<th>Hypothesis Testing Test Statistic</th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>When <span class="math inline">\(\sigma^2\)</span> is known</td>
<td><span class="math inline">\(\bar{X}_1 - \bar{X}_2 \pm z_{\alpha/2}\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}\)</span></td>
<td><span class="math inline">\(z= \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}}\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>When <span class="math inline">\(\sigma^2\)</span> is unknown, Variances Assumed EQUAL</td>
<td><span class="math inline">\(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}\)</span></td>
<td><span class="math inline">\(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}}\)</span></td>
<td>Pooled Variance: <span class="math inline">\(s_p^2 = \frac{(n_1 -1)s^2_1 - (n_2-1)s^2_2}{n_1 + n_2 -2}\)</span> Degrees of Freedom: <span class="math inline">\(\gamma = n_1 + n_2 -2\)</span>
</td>
</tr>
<tr class="odd">
<td>When <span class="math inline">\(\sigma^2\)</span> is unknown, Variances Assumed UNEQUAL</td>
<td><span class="math inline">\(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}\)</span></td>
<td><span class="math inline">\(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}}\)</span></td>
<td>Degrees of Freedom: <span class="math inline">\(\gamma = \frac{(\frac{s_1^2}{n_1}+\frac{s^2_2}{n_2})^2}{\frac{(\frac{s_1^2}{n_1})^2}{n_1-1}+\frac{(\frac{s_2^2}{n_2})^2}{n_2-1}}\)</span>
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="for-difference-of-means-paired-samples" class="section level3" number="4.3.3">
<h3>
<span class="header-section-number">4.3.3</span> For Difference of Means, Paired Samples<a class="anchor" aria-label="anchor" href="#for-difference-of-means-paired-samples"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="36%">
<col width="63%">
</colgroup>
<thead><tr class="header">
<th>Metric</th>
<th>Formula</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Confidence Interval</td>
<td><span class="math inline">\(\bar{D} \pm t_{\alpha/2}\frac{s_d}{\sqrt{n}}\)</span></td>
</tr>
<tr class="even">
<td>Hypothesis Test Statistic</td>
<td><span class="math inline">\(t = \frac{\bar{D} - D_0}{s_d / \sqrt{n}}\)</span></td>
</tr>
</tbody>
</table></div>
</div>
<div id="for-difference-of-two-proportions" class="section level3" number="4.3.4">
<h3>
<span class="header-section-number">4.3.4</span> For Difference of Two Proportions<a class="anchor" aria-label="anchor" href="#for-difference-of-two-proportions"><i class="fas fa-link"></i></a>
</h3>
<p>The mean of the difference between two sample proportions is given by:</p>
<p><span class="math display">\[
\hat{p_1} - \hat{p_2}
\]</span></p>
<p>The variance of the difference in proportions is:</p>
<p><span class="math display">\[
\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}
\]</span></p>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for the difference in proportions is calculated as:</p>
<p><span class="math display">\[
\hat{p_1} - \hat{p_2} \pm z_{\alpha/2} \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(z_{\alpha/2}\)</span>: The critical value from the standard normal distribution.</p></li>
<li><p><span class="math inline">\(\hat{p_1}\)</span>, <span class="math inline">\(\hat{p_2}\)</span>: Sample proportions.</p></li>
<li><p><span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2\)</span>: Sample sizes.</p></li>
</ul>
<p><strong>Sample Size for a Desired Confidence Level and Margin of Error</strong></p>
<p>To achieve a margin of error <span class="math inline">\(d\)</span> for a given confidence level, the required sample size can be estimated as follows:</p>
<ol style="list-style-type: decimal">
<li><p><strong>With Prior Estimates of</strong> <span class="math inline">\(\hat{p_1}\)</span> and <span class="math inline">\(\hat{p_2}\)</span>: <span class="math display">\[
n \approx \frac{z_{\alpha/2}^2 \left[p_1(1-p_1) + p_2(1-p_2)\right]}{d^2}
\]</span></p></li>
<li><p><strong>Without Prior Estimates</strong> (assuming maximum variability, <span class="math inline">\(\hat{p} = 0.5\)</span>): <span class="math display">\[
n \approx \frac{z_{\alpha/2}^2}{2d^2}
\]</span></p></li>
</ol>
<p><strong>Hypothesis Testing for Difference in Proportions</strong></p>
<p>The test statistic for hypothesis testing depends on the null hypothesis:</p>
<ol style="list-style-type: decimal">
<li><p><strong>When</strong> <span class="math inline">\((p_1 - p_2) \neq 0\)</span>: <span class="math display">\[
z = \frac{(\hat{p_1} - \hat{p_2}) - (p_1 - p_2)_0}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}
\]</span></p></li>
<li><p><strong>When</strong> <span class="math inline">\((p_1 - p_2)_0 = 0\)</span> (testing equality of proportions): <span class="math display">\[
z = \frac{\hat{p_1} - \hat{p_2}}{\sqrt{\hat{p}(1-\hat{p}) \left(\frac{1}{n_1} + \frac{1}{n_2}\right)}}
\]</span></p></li>
</ol>
<p>where <span class="math inline">\(\hat{p}\)</span> is the pooled sample proportion:</p>
<p><span class="math display">\[
\hat{p} = \frac{x_1 + x_2}{n_1 + n_2} = \frac{n_1\hat{p_1} + n_2\hat{p_2}}{n_1 + n_2}
\]</span></p>
<hr>
</div>
<div id="for-single-proportion" class="section level3" number="4.3.5">
<h3>
<span class="header-section-number">4.3.5</span> For Single Proportion<a class="anchor" aria-label="anchor" href="#for-single-proportion"><i class="fas fa-link"></i></a>
</h3>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for a population proportion <span class="math inline">\(p\)</span> is:</p>
<p><span class="math display">\[
\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\]</span></p>
<p>Sample Size Determination</p>
<ul>
<li><p><strong>With Prior Estimate</strong> (<span class="math inline">\(\hat{p}\)</span>): <span class="math display">\[
n \approx \frac{z_{\alpha/2}^2 \hat{p}(1-\hat{p})}{d^2}
\]</span></p></li>
<li><p><strong>Without Prior Estimate</strong>: <span class="math display">\[
n \approx \frac{z_{\alpha/2}^2}{4d^2}
\]</span></p></li>
</ul>
<p>The test statistic for <span class="math inline">\(H_0: p = p_0\)</span> is:</p>
<p><span class="math display">\[
z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}
\]</span></p>
<hr>
</div>
<div id="for-single-variance" class="section level3" number="4.3.6">
<h3>
<span class="header-section-number">4.3.6</span> For Single Variance<a class="anchor" aria-label="anchor" href="#for-single-variance"><i class="fas fa-link"></i></a>
</h3>
<p>For a sample variance <span class="math inline">\(s^2\)</span> with <span class="math inline">\(n\)</span> observations, the <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for the population variance <span class="math inline">\(\sigma^2\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
1 - \alpha &amp;= P( \chi_{1-\alpha/2;n-1}^2) \le (n-1)s^2/\sigma^2 \le \chi_{\alpha/2;n-1}^2)\\
&amp;=P\left(\frac{(n-1)s^2}{\chi^2_{\alpha/2; n-1}} \leq \sigma^2 \leq \frac{(n-1)s^2}{\chi^2_{1-\alpha/2; n-1}}\right)
\end{aligned}
\]</span></p>
<p>Equivalently, the confidence interval can be written as:</p>
<p><span class="math display">\[
\left(\frac{(n-1)s^2}{\chi^2_{\alpha/2}}, \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}\right)
\]</span></p>
<p>To find confidence limits for <span class="math inline">\(\sigma\)</span>, compute the square root of the interval bounds:</p>
<p><span class="math display">\[
\text{Confidence Interval for } \sigma: \quad \left(\sqrt{\frac{(n-1)s^2}{\chi^2_{\alpha/2}}}, \sqrt{\frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}}\right)
\]</span></p>
<p><strong>Hypothesis Testing for Variance</strong></p>
<p>The test statistic for testing a null hypothesis about a population variance (<span class="math inline">\(\sigma^2_0\)</span>) is:</p>
<p><span class="math display">\[
\chi^2 = \frac{(n-1)s^2}{\sigma^2_0}
\]</span></p>
<p>This test statistic follows a chi-squared distribution with <span class="math inline">\(n-1\)</span> degrees of freedom under the null hypothesis.</p>
</div>
<div id="non-parametric-tests" class="section level3" number="4.3.7">
<h3>
<span class="header-section-number">4.3.7</span> Non-parametric Tests<a class="anchor" aria-label="anchor" href="#non-parametric-tests"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="49%">
<col width="24%">
<col width="25%">
</colgroup>
<thead><tr class="header">
<th><strong>Method</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>Assumptions</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="basic-statistical-inference.html#sign-test">Sign Test</a></td>
<td>Test median</td>
<td>None (ordinal data sufficient)</td>
</tr>
<tr class="even">
<td><a href="basic-statistical-inference.html#wilcoxon-signed-rank-test">Wilcoxon Signed Rank Test</a></td>
<td>Test symmetry around a value</td>
<td>Symmetry of distribution</td>
</tr>
<tr class="odd">
<td><a href="basic-statistical-inference.html#wald-wolfowitz-runs-test">Wald-Wolfowitz Runs Test</a></td>
<td>Test for randomness</td>
<td>Independent observations</td>
</tr>
<tr class="even">
<td><a href="basic-statistical-inference.html#quantile-or-percentile-test">Quantile (or Percentile) Test</a></td>
<td>Test specific quantile</td>
<td>None (ordinal data sufficient)</td>
</tr>
</tbody>
</table></div>
<div id="sign-test" class="section level4" number="4.3.7.1">
<h4>
<span class="header-section-number">4.3.7.1</span> Sign Test<a class="anchor" aria-label="anchor" href="#sign-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Sign Test</strong> is used to test hypotheses about the median of a population, <span class="math inline">\(\mu_{(0.5)}\)</span>, without assuming a specific distribution for the data. This test is ideal for small sample sizes or when normality assumptions are not met.</p>
<p>To test the population median, consider the hypotheses:</p>
<ul>
<li>Null Hypothesis: <span class="math inline">\(H_0: \mu_{(0.5)} = 0\)</span>
</li>
<li>Alternative Hypothesis: <span class="math inline">\(H_a: \mu_{(0.5)} &gt; 0\)</span> (one-sided test)</li>
</ul>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Count Positive and Negative Deviations</strong>:</p>
<ul>
<li>Count observations (<span class="math inline">\(y_i\)</span>) greater than 0: <span class="math inline">\(s_+\)</span> (number of positive signs).</li>
<li>Count observations less than 0: <span class="math inline">\(s_-\)</span> (number of negative signs).</li>
<li>
<span class="math inline">\(s_- = n - s_+\)</span>.</li>
</ul>
</li>
<li>
<p><strong>Decision Rule</strong>:</p>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(s_+\)</span> is large (or equivalently, <span class="math inline">\(s_-\)</span> is small).</li>
<li>To determine how large <span class="math inline">\(s_+\)</span> must be, use the distribution of <span class="math inline">\(S_+\)</span> under <span class="math inline">\(H_0\)</span>, which is <strong>Binomial</strong> with <span class="math inline">\(p = 0.5\)</span>.</li>
</ul>
</li>
<li><p><strong>Null Distribution</strong>:<br>
Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S_+\)</span> follows: <span class="math display">\[
S_+ \sim Binomial(n, p = 0.5)
\]</span></p></li>
<li><p><strong>Critical Value</strong>:<br>
Reject <span class="math inline">\(H_0\)</span> if: <span class="math display">\[
s_+ \ge b_{n,\alpha}
\]</span> where <span class="math inline">\(b_{n,\alpha}\)</span> is the upper <span class="math inline">\(\alpha\)</span> critical value of the binomial distribution.</p></li>
<li>
<p><strong>p-value Calculation</strong>:<br>
Compute the p-value for the observed (one-tailed) <span class="math inline">\(s_+\)</span> as: <span class="math display">\[
\text{p-value} = P(S \ge s_+) = \sum_{i=s_+}^{n} \binom{n}{i} \left(\frac{1}{2}\right)^n
\]</span></p>
<p>Alternatively: <span class="math display">\[
P(S \le s_-) = \sum_{i=0}^{s_-} \binom{n}{i} \left(\frac{1}{2}\right)^n
\]</span></p>
</li>
</ol>
<hr>
<p>Large Sample Normal Approximation</p>
<p>For large <span class="math inline">\(n\)</span>, use a normal approximation for the binomial test. Reject <span class="math inline">\(H_0\)</span> if: <span class="math display">\[
s_+ \ge \frac{n}{2} + \frac{1}{2} + z_{\alpha} \sqrt{\frac{n}{4}}
\]</span> where <span class="math inline">\(z_\alpha\)</span> is the critical value for a one-sided test.</p>
<p>For two-sided tests, use the maximum or minimum of <span class="math inline">\(s_+\)</span> and <span class="math inline">\(s_-\)</span>:</p>
<ul>
<li><p>Test statistic: <span class="math inline">\(s_{\text{max}} = \max(s_+, s_-)\)</span> or <span class="math inline">\(s_{\text{min}} = \min(s_+, s_-)\)</span></p></li>
<li><p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(\alpha\)</span>, where: <span class="math display">\[
p\text{-value} = 2 \sum_{i=s_{\text{max}}}^{n} \binom{n}{i} \left(\frac{1}{2}\right)^n = 2 \sum_{i = 0}^{s_{min}} \binom{n}{i} \left( \frac{1}{2} \right)^n
\]</span></p></li>
</ul>
<p>Equivalently, rejecting <span class="math inline">\(H_0\)</span> if <span class="math inline">\(s_{max} \ge b_{n,\alpha/2}\)</span>.</p>
<p>For large <span class="math inline">\(n\)</span>, the normal approximation uses: <span class="math display">\[
z = \frac{s_{\text{max}} - \frac{n}{2} - \frac{1}{2}}{\sqrt{\frac{n}{4}}}
\]</span><br>
Reject <span class="math inline">\(H_0\)</span> at <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(z \ge z_{\alpha/2}\)</span>.</p>
<p>Handling zeros in the data is a common issue with the Sign Test:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Random Assignment</strong>: Assign zeros randomly to either <span class="math inline">\(s_+\)</span> or <span class="math inline">\(s_-\)</span> (2 researchers might get different results).</li>
<li>
<strong>Fractional Assignment</strong>: Count each zero as <span class="math inline">\(0.5\)</span> toward both <span class="math inline">\(s_+\)</span> and <span class="math inline">\(s_-\)</span> (but then we could not apply the <a href="prerequisites.html#binomial-distribution">Binomial Distribution</a> afterward).</li>
<li>
<strong>Ignore Zeros</strong>: Ignore zeros, but note this reduces the sample size and power.</li>
</ol>
<div class="sourceCode" id="cb108"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example Data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.76</span>, <span class="fl">0.82</span>, <span class="fl">0.80</span>, <span class="fl">0.79</span>, <span class="fl">1.06</span>, <span class="fl">0.83</span>, <span class="op">-</span><span class="fl">0.43</span>, <span class="op">-</span><span class="fl">0.34</span>, <span class="fl">3.34</span>, <span class="fl">2.33</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Count positive signs</span></span>
<span><span class="va">s_plus</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">data</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sample size excluding zeros</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform a one-sided binomial test</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/binom.test.html">binom.test</a></span><span class="op">(</span><span class="va">s_plus</span>, <span class="va">n</span>, p <span class="op">=</span> <span class="fl">0.5</span>, alternative <span class="op">=</span> <span class="st">"greater"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Exact binomial test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  s_plus and n</span></span>
<span><span class="co">#&gt; number of successes = 8, number of trials = 10, p-value = 0.05469</span></span>
<span><span class="co">#&gt; alternative hypothesis: true probability of success is greater than 0.5</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.4930987 1.0000000</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; probability of success </span></span>
<span><span class="co">#&gt;                    0.8</span></span></code></pre></div>
</div>
<div id="wilcoxon-signed-rank-test" class="section level4" number="4.3.7.2">
<h4>
<span class="header-section-number">4.3.7.2</span> Wilcoxon Signed Rank Test<a class="anchor" aria-label="anchor" href="#wilcoxon-signed-rank-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Wilcoxon Signed Rank Test</strong> is an improvement over the <a href="basic-statistical-inference.html#sign-test">Sign Test</a> as it considers both the magnitude and direction of deviations from the null hypothesis value (e.g., 0). However, this test assumes that the data are symmetrically distributed around the median, unlike the Sign Test.</p>
<p>We test the following hypotheses:</p>
<p><span class="math display">\[
H_0: \mu_{(0.5)} = 0 \\
H_a: \mu_{(0.5)} &gt; 0
\]</span></p>
<p>This example assumes no ties or duplicate observations in the data.</p>
<p>Procedure for the Signed Rank Test</p>
<ol style="list-style-type: decimal">
<li>
<strong>Rank the Absolute Values</strong>:
<ul>
<li>Rank the observations <span class="math inline">\(y_i\)</span> based on their absolute values.</li>
<li>Let <span class="math inline">\(r_i\)</span> denote the rank of <span class="math inline">\(y_i\)</span>.</li>
<li>Since there are no ties, ranks <span class="math inline">\(r_i\)</span> are uniquely determined and form a permutation of integers <span class="math inline">\(1, 2, \dots, n\)</span>.</li>
</ul>
</li>
<li>
<strong>Calculate</strong> <span class="math inline">\(w_+\)</span> and <span class="math inline">\(w_-\)</span>:
<ul>
<li>
<span class="math inline">\(w_+\)</span> is the sum of the ranks corresponding to positive values of <span class="math inline">\(y_i\)</span>.</li>
<li>
<span class="math inline">\(w_-\)</span> is the sum of the ranks corresponding to negative values of <span class="math inline">\(y_i\)</span>.</li>
<li>By definition: <span class="math display">\[
w_+ + w_- = \sum_{i=1}^n r_i = \frac{n(n+1)}{2}
\]</span>
</li>
</ul>
</li>
<li>
<strong>Decision Rule</strong>:
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(w_+\)</span> is large (or equivalently, if <span class="math inline">\(w_-\)</span> is small).</li>
</ul>
</li>
</ol>
<hr>
<p><strong>Null Distribution</strong> of <span class="math inline">\(W_+\)</span></p>
<p>Under the null hypothesis, the distributions of <span class="math inline">\(W_+\)</span> and <span class="math inline">\(W_-\)</span> are identical and symmetric. The p-value for a one-sided test is:</p>
<p><span class="math display">\[
\text{p-value} = P(W \ge w_+) = P(W \le w_-)
\]</span></p>
<p>An <span class="math inline">\(\alpha\)</span>-level test rejects <span class="math inline">\(H_0\)</span> if <span class="math inline">\(w_+ \ge w_{n,\alpha}\)</span>, where <span class="math inline">\(w_{n,\alpha}\)</span> is the critical value from a table of the null distribution of <span class="math inline">\(W_+\)</span>.</p>
<p>For two-sided tests, use:</p>
<p><span class="math display">\[
p\text{-value} = 2P(W \ge w_{max}) = 2P(W \le w_{min})
\]</span></p>
<hr>
<p><strong>Normal Approximation for Large Samples</strong></p>
<p>For large <span class="math inline">\(n\)</span>, the null distribution of <span class="math inline">\(W_+\)</span> can be approximated by a normal distribution:</p>
<p><span class="math display">\[
z = \frac{w_+ - \frac{n(n+1)}{4} - \frac{1}{2}}{\sqrt{\frac{n(n+1)(2n+1)}{24}}}
\]</span></p>
<p>The test rejects <span class="math inline">\(H_0\)</span> at level <span class="math inline">\(\alpha\)</span> if:</p>
<p><span class="math display">\[
w_+ \ge \frac{n(n+1)}{4} + \frac{1}{2} + z_{\alpha} \sqrt{\frac{n(n+1)(2n+1)}{24}} \approx w_{n,\alpha}
\]</span></p>
<p>For a two-sided test, the decision rule uses the maximum or minimum of <span class="math inline">\(w_+\)</span> and <span class="math inline">\(w_-\)</span>:</p>
<ul>
<li><p><span class="math inline">\(w_{max} = \max(w_+, w_-)\)</span></p></li>
<li><p><span class="math inline">\(w_{min} = \min(w_+, w_-)\)</span></p></li>
</ul>
<p>The p-value is computed as:</p>
<p><span class="math display">\[
p\text{-value} = 2P(W \ge w_{max}) = 2P(W \le w_{min})
\]</span></p>
<hr>
<p><strong>Handling Tied Ranks</strong></p>
<p>If some observations <span class="math inline">\(|y_i|\)</span> have tied absolute values, assign the average rank (or “midrank”) to all tied values. For example:</p>
<ul>
<li>Suppose <span class="math inline">\(y_1 = -1\)</span>, <span class="math inline">\(y_2 = 3\)</span>, <span class="math inline">\(y_3 = -3\)</span>, and <span class="math inline">\(y_4 = 5\)</span>.</li>
<li>The ranks for <span class="math inline">\(|y_i|\)</span> are:
<ul>
<li>
<span class="math inline">\(|y_1| = 1\)</span>: <span class="math inline">\(r_1 = 1\)</span>
</li>
<li>
<span class="math inline">\(|y_2| = |y_3| = 3\)</span>: <span class="math inline">\(r_2 = r_3 = \frac{2+3}{2} = 2.5\)</span>
</li>
<li>
<span class="math inline">\(|y_4| = 5\)</span>: <span class="math inline">\(r_4 = 4\)</span>
</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example Data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.76</span>, <span class="fl">0.82</span>, <span class="fl">0.80</span>, <span class="fl">0.79</span>, <span class="fl">1.06</span>, <span class="fl">0.83</span>, <span class="op">-</span><span class="fl">0.43</span>, <span class="op">-</span><span class="fl">0.34</span>, <span class="fl">3.34</span>, <span class="fl">2.33</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform Wilcoxon Signed Rank Test (exact test)</span></span>
<span><span class="va">wilcox_exact</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/wilcox.test.html">wilcox.test</a></span><span class="op">(</span><span class="va">data</span>, exact <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">wilcox_exact</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wilcoxon signed rank exact test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data</span></span>
<span><span class="co">#&gt; V = 52, p-value = 0.009766</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location is not equal to 0</span></span></code></pre></div>
<p>For large samples, you can use the normal approximation by setting <code>exact = FALSE</code>:</p>
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Perform Wilcoxon Signed Rank Test (normal approximation)</span></span>
<span><span class="va">wilcox_normal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/wilcox.test.html">wilcox.test</a></span><span class="op">(</span><span class="va">data</span>, exact <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">wilcox_normal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wilcoxon signed rank test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data</span></span>
<span><span class="co">#&gt; V = 52, p-value = 0.01443</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location is not equal to 0</span></span></code></pre></div>
</div>
<div id="wald-wolfowitz-runs-test" class="section level4" number="4.3.7.3">
<h4>
<span class="header-section-number">4.3.7.3</span> Wald-Wolfowitz Runs Test<a class="anchor" aria-label="anchor" href="#wald-wolfowitz-runs-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Runs Test</strong> is a non-parametric test used to examine the randomness of a sequence. Specifically, it tests whether the order of observations in a sequence is random. This test is useful in detecting non-random patterns, such as trends, clustering, or periodicity.</p>
<p>The hypotheses for the Runs Test are:</p>
<ul>
<li>Null Hypothesis: <span class="math inline">\(H_0\)</span>: The sequence is random.</li>
<li>Alternative Hypothesis: <span class="math inline">\(H_a\)</span>: The sequence is not random.</li>
</ul>
<p>A <strong>run</strong> is a sequence of consecutive observations of the same type. For example: - In the binary sequence <code>+ + - - + - + +</code>, there are <strong>5 runs</strong>: <code>++</code>, <code>--</code>, <code>+</code>, <code>-</code>, <code>++</code>.</p>
<p>Runs can be formed based on any classification criteria, such as:</p>
<ul>
<li><p>Positive vs. Negative values</p></li>
<li><p>Above vs. Below the median</p></li>
<li><p>Success vs. Failure in binary outcomes</p></li>
</ul>
<p><strong>Test Statistic</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Number of Runs</strong> (<span class="math inline">\(R\)</span>):<br>
The observed number of runs in the sequence.</p></li>
<li>
<p><strong>Expected Number of Runs</strong> (<span class="math inline">\(E[R]\)</span>):<br>
Under the null hypothesis of randomness, the expected number of runs is: <span class="math display">\[
E[R] = \frac{2 n_1 n_2}{n_1 + n_2} + 1
\]</span> where:</p>
<ul>
<li>
<span class="math inline">\(n_1\)</span>: Number of observations in the first category (e.g., positives).</li>
<li>
<span class="math inline">\(n_2\)</span>: Number of observations in the second category (e.g., negatives).</li>
<li>
<span class="math inline">\(n = n_1 + n_2\)</span>: Total number of observations.</li>
</ul>
</li>
<li><p><strong>Variance of Runs</strong> (<span class="math inline">\(\text{Var}[R]\)</span>):<br>
The variance of the number of runs is given by: <span class="math display">\[
\text{Var}[R] = \frac{2 n_1 n_2 (2 n_1 n_2 - n)}{n^2 (n - 1)}
\]</span></p></li>
<li><p><strong>Standardized Test Statistic (</strong><span class="math inline">\(z\)</span>):<br>
For large samples (<span class="math inline">\(n \geq 20\)</span>), the test statistic is approximately normally distributed: <span class="math display">\[
z = \frac{R - E[R]}{\sqrt{\text{Var}[R]}}
\]</span></p></li>
</ol>
<p><strong>Decision Rule</strong></p>
<ul>
<li>Compute the <span class="math inline">\(z\)</span>-value and compare it to the critical value of the standard normal distribution.</li>
<li>For a significance level <span class="math inline">\(\alpha\)</span>:
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|z| \ge z_{\alpha/2}\)</span> (two-sided test).</li>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(z \ge z_\alpha\)</span> or <span class="math inline">\(z \le -z_\alpha\)</span> for one-sided tests.</li>
</ul>
</li>
</ul>
<p>Steps for Conducting a Runs Test:</p>
<ol style="list-style-type: decimal">
<li>Classify the data into two groups (e.g., above/below median, positive/negative).</li>
<li>Count the total number of runs (<span class="math inline">\(R\)</span>).</li>
<li>Compute <span class="math inline">\(E[R]\)</span> and <span class="math inline">\(\text{Var}[R]\)</span> based on <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>.</li>
<li>Compute the <span class="math inline">\(z\)</span>-value for the observed number of runs.</li>
<li>Compare the <span class="math inline">\(z\)</span>-value to the critical value to decide whether to reject <span class="math inline">\(H_0\)</span>.</li>
</ol>
<p>For a numerical dataset where the test is based on values above and below the median:</p>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example dataset</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.2</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">3.4</span>, <span class="op">-</span><span class="fl">1.1</span>, <span class="fl">2.8</span>, <span class="op">-</span><span class="fl">0.8</span>, <span class="fl">4.5</span>, <span class="fl">0.7</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">randtests</span><span class="op">)</span></span>
<span><span class="co"># Perform Runs Test (above/below median)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/randtests/man/runs.test.html">runs.test</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Runs Test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data</span></span>
<span><span class="co">#&gt; statistic = 2.2913, runs = 8, n1 = 4, n2 = 4, n = 8, p-value = 0.02195</span></span>
<span><span class="co">#&gt; alternative hypothesis: nonrandomness</span></span></code></pre></div>
<p>The output of the <code>runs.test</code> function includes:</p>
<ul>
<li><p><strong>Observed Runs</strong>: The actual number of runs in the sequence.</p></li>
<li><p><strong>Expected Runs</strong>: The expected number of runs under <span class="math inline">\(H_0\)</span>.</p></li>
<li><p><strong>p-value</strong>: The probability of observing a number of runs as extreme as the observed one under <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>If the p-value is less than <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span> and conclude that the sequence is not random.</p></li>
</ul>
<p>Limitations of the Runs Test</p>
<ul>
<li><p>The test assumes that observations are independent.</p></li>
<li><p>For small sample sizes, the test may have limited power.</p></li>
<li><p>Ties in the data must be resolved by a predefined rule (e.g., treating ties as belonging to one group or excluding them).</p></li>
</ul>
</div>
<div id="quantile-or-percentile-test" class="section level4" number="4.3.7.4">
<h4>
<span class="header-section-number">4.3.7.4</span> Quantile (or Percentile) Test<a class="anchor" aria-label="anchor" href="#quantile-or-percentile-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Quantile Test</strong> (also called the Percentile Test) is a non-parametric test used to evaluate whether the proportion of observations falling within a specific quantile matches the expected proportion under the null hypothesis. This test is useful for assessing the distribution of data when specific quantiles (e.g., medians or percentiles) are of interest.</p>
<p>Suppose we want to test whether the true proportion of data below a specified quantile <span class="math inline">\(q\)</span> matches a given probability <span class="math inline">\(p\)</span>. The hypotheses are:</p>
<ul>
<li>Null Hypothesis: <span class="math inline">\(H_0\)</span>: The true proportion is equal to <span class="math inline">\(p\)</span>.</li>
<li>Alternative Hypothesis: <span class="math inline">\(H_a\)</span>: The true proportion is not equal to <span class="math inline">\(p\)</span> (two-sided), greater than <span class="math inline">\(p\)</span> (right-tailed), or less than <span class="math inline">\(p\)</span> (left-tailed).</li>
</ul>
<p><strong>Test Statistic</strong></p>
<p>The test statistic is based on the observed count of data points below the specified quantile.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Observed Count</strong> (<span class="math inline">\(k\)</span>):<br>
The number of data points <span class="math inline">\(y_i\)</span> such that <span class="math inline">\(y_i \leq q\)</span>.</p></li>
<li><p><strong>Expected Count</strong> (<span class="math inline">\(E[k]\)</span>):<br>
The expected number of observations below the quantile <span class="math inline">\(q\)</span> under <span class="math inline">\(H_0\)</span> is: <span class="math display">\[
E[k] = n \cdot p
\]</span></p></li>
<li><p><strong>Variance</strong>:<br>
Under the binomial distribution, the variance is: <span class="math display">\[
\text{Var}[k] = n \cdot p \cdot (1 - p)
\]</span></p></li>
<li><p><strong>Standardized Test Statistic</strong> (<span class="math inline">\(z\)</span>):<br>
For large <span class="math inline">\(n\)</span>, the test statistic is approximately normally distributed: <span class="math display">\[
z = \frac{k - E[k]}{\sqrt{\text{Var}[k]}} = \frac{k - n \cdot p}{\sqrt{n \cdot p \cdot (1 - p)}}
\]</span></p></li>
</ol>
<p><strong>Decision Rule</strong></p>
<ol style="list-style-type: decimal">
<li>Compute the <span class="math inline">\(z\)</span>-value for the observed count.</li>
<li>Compare the <span class="math inline">\(z\)</span>-value to the critical value of the standard normal distribution:
<ul>
<li>For a two-sided test, reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|z| \geq z_{\alpha/2}\)</span>.</li>
<li>For a one-sided test, reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(z \geq z_\alpha\)</span> (right-tailed) or <span class="math inline">\(z \leq -z_\alpha\)</span> (left-tailed).</li>
</ul>
</li>
</ol>
<p>Alternatively, calculate the p-value and reject <span class="math inline">\(H_0\)</span> if the p-value <span class="math inline">\(\leq \alpha\)</span>.</p>
<p>Suppose we have a dataset and want to test whether the proportion of observations below the 50th percentile (median) matches the expected value of <span class="math inline">\(p = 0.5\)</span>.</p>
<div class="sourceCode" id="cb112"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">12</span>, <span class="fl">15</span>, <span class="fl">14</span>, <span class="fl">10</span>, <span class="fl">13</span>, <span class="fl">11</span>, <span class="fl">14</span>, <span class="fl">16</span>, <span class="fl">15</span>, <span class="fl">13</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define the quantile to test</span></span>
<span><span class="va">quantile_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">data</span>, <span class="fl">0.5</span><span class="op">)</span> <span class="co"># Median</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>                             <span class="co"># Proportion under H0</span></span>
<span></span>
<span><span class="co"># Count observed values below or equal to the quantile</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">data</span> <span class="op">&lt;=</span> <span class="va">quantile_value</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sample size</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Expected count under H0</span></span>
<span><span class="va">expected_count</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">*</span> <span class="va">p</span></span>
<span></span>
<span><span class="co"># Variance</span></span>
<span><span class="va">variance</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">*</span> <span class="va">p</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Test statistic (z-value)</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">k</span> <span class="op">-</span> <span class="va">expected_count</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">variance</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate p-value for two-sided test</span></span>
<span><span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">z</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Output results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  quantile_value <span class="op">=</span> <span class="va">quantile_value</span>,</span>
<span>  observed_count <span class="op">=</span> <span class="va">k</span>,</span>
<span>  expected_count <span class="op">=</span> <span class="va">expected_count</span>,</span>
<span>  z_value <span class="op">=</span> <span class="va">z</span>,</span>
<span>  p_value <span class="op">=</span> <span class="va">p_value</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $quantile_value</span></span>
<span><span class="co">#&gt;  50% </span></span>
<span><span class="co">#&gt; 13.5 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $observed_count</span></span>
<span><span class="co">#&gt; [1] 5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $expected_count</span></span>
<span><span class="co">#&gt; [1] 5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $z_value</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $p_value</span></span>
<span><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>For a one-sided test (e.g., testing whether the proportion is greater than <span class="math inline">\(p\)</span>):</p>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate one-sided p-value</span></span>
<span><span class="va">p_value_one_sided</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">z</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Output one-sided p-value</span></span>
<span><span class="va">p_value_one_sided</span></span>
<span><span class="co">#&gt; [1] 0.5</span></span></code></pre></div>
<p>Interpretation of Results</p>
<ul>
<li><p><strong>p-value</strong>: If the p-value is less than <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span> and conclude that the proportion of observations below the quantile deviates significantly from <span class="math inline">\(p\)</span>.</p></li>
<li><p><strong>Quantile Test Statistic (</strong><span class="math inline">\(z\)</span><strong>)</strong>: The <span class="math inline">\(z\)</span>-value indicates how many standard deviations the observed count is from the expected count under the null hypothesis. Large positive or negative <span class="math inline">\(z\)</span> values suggest non-random deviations.</p></li>
</ul>
<p>Assumptions of the Test</p>
<ol style="list-style-type: decimal">
<li><p>Observations are independent.</p></li>
<li><p>The sample size is large enough for the normal approximation to the binomial distribution to be valid (<span class="math inline">\(n \cdot p \geq 5\)</span> and <span class="math inline">\(n \cdot (1 - p) \geq 5\)</span>).</p></li>
</ol>
<p>Limitations of the Test</p>
<ul>
<li><p>For small sample sizes, the normal approximation may not hold. In such cases, exact binomial tests are more appropriate.</p></li>
<li><p>The test assumes that the quantile used (e.g., the median) is well-defined and correctly calculated from the data.</p></li>
</ul>
</div>
</div>
</div>
<div id="two-sample-inference" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Two-Sample Inference<a class="anchor" aria-label="anchor" href="#two-sample-inference"><i class="fas fa-link"></i></a>
</h2>
<div id="for-means" class="section level3" number="4.4.1">
<h3>
<span class="header-section-number">4.4.1</span> For Means<a class="anchor" aria-label="anchor" href="#for-means"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have two sets of observations:</p>
<ul>
<li><span class="math inline">\(y_1, \dots, y_{n_y}\)</span></li>
<li><span class="math inline">\(x_1, \dots, x_{n_x}\)</span></li>
</ul>
<p>These are random samples from two independent populations with means <span class="math inline">\(\mu_y\)</span> and <span class="math inline">\(\mu_x\)</span> and variances <span class="math inline">\(\sigma_y^2\)</span> and <span class="math inline">\(\sigma_x^2\)</span>. Our goal is to compare <span class="math inline">\(\mu_y\)</span> and <span class="math inline">\(\mu_x\)</span> or test whether <span class="math inline">\(\sigma_y^2 = \sigma_x^2\)</span>.</p>
<hr>
<div id="large-sample-tests" class="section level4" number="4.4.1.1">
<h4>
<span class="header-section-number">4.4.1.1</span> Large Sample Tests<a class="anchor" aria-label="anchor" href="#large-sample-tests"><i class="fas fa-link"></i></a>
</h4>
<p>If <span class="math inline">\(n_y\)</span> and <span class="math inline">\(n_x\)</span> are large (<span class="math inline">\(\geq 30\)</span>), the <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a> allows us to make the following assumptions:</p>
<ul>
<li>
<strong>Expectation</strong>: <span class="math display">\[
E(\bar{y} - \bar{x}) = \mu_y - \mu_x
\]</span>
</li>
<li>
<strong>Variance</strong>: <span class="math display">\[
\text{Var}(\bar{y} - \bar{x}) = \frac{\sigma_y^2}{n_y} + \frac{\sigma_x^2}{n_x}
\]</span>
</li>
</ul>
<p>The test statistic is:</p>
<p><span class="math display">\[
Z = \frac{\bar{y} - \bar{x} - (\mu_y - \mu_x)}{\sqrt{\frac{\sigma_y^2}{n_y} + \frac{\sigma_x^2}{n_x}}} \sim N(0,1)
\]</span></p>
<p>For large samples, replace variances with their unbiased estimators <span class="math inline">\(s_y^2\)</span> and <span class="math inline">\(s_x^2\)</span>, yielding the same large sample distribution.</p>
<p><strong>Confidence Interval</strong></p>
<p>An approximate <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu_y - \mu_x\)</span> is:</p>
<p><span class="math display">\[
\bar{y} - \bar{x} \pm z_{\alpha/2} \sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}
\]</span></p>
<p><strong>Hypothesis Test</strong></p>
<p>Testing:</p>
<p><span class="math display">\[
H_0: \mu_y - \mu_x = \delta_0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x \neq \delta_0
\]</span></p>
<p>The test statistic:</p>
<p><span class="math display">\[
z = \frac{\bar{y} - \bar{x} - \delta_0}{\sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> at the <span class="math inline">\(\alpha\)</span>-level if:</p>
<p><span class="math display">\[
|z| &gt; z_{\alpha/2}
\]</span></p>
<p>If <span class="math inline">\(\delta_0 = 0\)</span>, this tests whether the two means are equal.</p>
<div class="sourceCode" id="cb114"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Large sample test</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">12</span>, <span class="fl">14</span>, <span class="fl">16</span>, <span class="fl">18</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">9</span>, <span class="fl">11</span>, <span class="fl">13</span>, <span class="fl">15</span>, <span class="fl">17</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Mean and variance</span></span>
<span><span class="va">mean_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">mean_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">var_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">var_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">n_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">n_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Test statistic</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">mean_y</span> <span class="op">-</span> <span class="va">mean_x</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">var_y</span> <span class="op">/</span> <span class="va">n_y</span> <span class="op">+</span> <span class="va">var_x</span> <span class="op">/</span> <span class="va">n_x</span><span class="op">)</span></span>
<span><span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">z</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>z <span class="op">=</span> <span class="va">z</span>, p_value <span class="op">=</span> <span class="va">p_value</span><span class="op">)</span></span>
<span><span class="co">#&gt; $z</span></span>
<span><span class="co">#&gt; [1] 0.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $p_value</span></span>
<span><span class="co">#&gt; [1] 0.6170751</span></span></code></pre></div>
<hr>
</div>
<div id="small-sample-tests" class="section level4" number="4.4.1.2">
<h4>
<span class="header-section-number">4.4.1.2</span> Small Sample Tests<a class="anchor" aria-label="anchor" href="#small-sample-tests"><i class="fas fa-link"></i></a>
</h4>
<p>If the samples are small, assume the data come from independent normal distributions:</p>
<ul>
<li><p><span class="math inline">\(y_i \sim N(\mu_y, \sigma_y^2)\)</span></p></li>
<li><p><span class="math inline">\(x_i \sim N(\mu_x, \sigma_x^2)\)</span></p></li>
</ul>
<p>We can do inference based on the <a href="prerequisites.html#students-t-distribution">Student’s T Distribution</a>, where we have 2 cases:</p>
<ul>
<li><p><a href="basic-statistical-inference.html#equal-variances">Equal Variances</a></p></li>
<li><p><a href="basic-statistical-inference.html#unequal-variances">Unequal Variances</a></p></li>
</ul>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="33%">
<col width="47%">
<col width="18%">
</colgroup>
<thead><tr class="header">
<th>Assumption</th>
<th>Tests</th>
<th>Plots</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Independence and Identically Distributed (i.i.d.) Observations</td>
<td>Test for serial correlation</td>
<td></td>
</tr>
<tr class="even">
<td>Independence Between Samples</td>
<td>Correlation Coefficient</td>
<td><a href="descriptive-statistics.html#scatterplot">Scatterplot</a></td>
</tr>
<tr class="odd">
<td>Normality</td>
<td>See <a href="descriptive-statistics.html#normality-assessment">Normality Assessment</a>
</td>
<td>See <a href="descriptive-statistics.html#normality-assessment">Normality Assessment</a>
</td>
</tr>
<tr class="even">
<td>Equality of Variances</td>
<td><ol style="list-style-type: decimal">
<li><a href="basic-statistical-inference.html#f-test">F-Test</a></li>
<li><a href="basic-statistical-inference.html#levenes-test">Levene’s Test</a></li>
<li><a href="basic-statistical-inference.html#modified-levene-test-brown-forsythe-test">Modified Levene Test (Brown-Forsythe Test)</a></li>
<li><a href="basic-statistical-inference.html#bartletts-test">Bartlett’s Test</a></li>
</ol></td>
<td><ol style="list-style-type: decimal">
<li>Boxplots with overlayed means</li>
<li>Residuals spread plots</li>
</ol></td>
</tr>
</tbody>
</table></div>
<div id="equal-variances" class="section level5" number="4.4.1.2.1">
<h5>
<span class="header-section-number">4.4.1.2.1</span> Equal Variances<a class="anchor" aria-label="anchor" href="#equal-variances"><i class="fas fa-link"></i></a>
</h5>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>Independence and Identically Distributed (i.i.d.) Observations</li>
</ol>
<p>Assume that observations in each sample are i.i.d., which implies:</p>
<p><span class="math display">\[
var(\bar{y}) = \frac{\sigma^2_y}{n_y}, \quad var(\bar{x}) = \frac{\sigma^2_x}{n_x}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Independence Between Samples</li>
</ol>
<p>The samples are assumed to be independent, meaning no observation from one sample influences observations from the other. This independence allows us to write:</p>
<p><span class="math display">\[
\begin{aligned}
var(\bar{y} - \bar{x}) &amp;= var(\bar{y}) + var(\bar{x}) - 2cov(\bar{y}, \bar{x}) \\
&amp;= var(\bar{y}) + var(\bar{x}) \\
&amp;= \frac{\sigma^2_y}{n_y} + \frac{\sigma^2_x}{n_x}
\end{aligned}
\]</span></p>
<p>This calculation assumes <span class="math inline">\(cov(\bar{y}, \bar{x}) = 0\)</span> due to the independence between the samples.</p>
<ol start="3" style="list-style-type: decimal">
<li>Normality Assumption</li>
</ol>
<p>We assume that the underlying populations are normally distributed. This assumption justifies the use of the <a href="prerequisites.html#students-t-distribution">Student’s T Distribution</a>, which is critical for hypothesis testing and constructing confidence intervals.</p>
<ol start="4" style="list-style-type: decimal">
<li>Equality of Variances</li>
</ol>
<p>If the population variances are equal, i.e., <span class="math inline">\(\sigma^2_y = \sigma^2_x = \sigma^2\)</span>, then <span class="math inline">\(s^2_y\)</span> and <span class="math inline">\(s^2_x\)</span> are both unbiased estimators of <span class="math inline">\(\sigma^2\)</span>. This allows us to pool the variances.</p>
<p>The pooled variance estimator is calculated as:</p>
<p><span class="math display">\[
s^2 = \frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y - 1) + (n_x - 1)}
\]</span></p>
<p>The pooled variance estimate has degrees of freedom equal to:</p>
<p><span class="math display">\[
df = (n_y + n_x - 2)
\]</span></p>
<p><strong>Test Statistic</strong></p>
<p>The test statistic is: <span class="math display">\[
T = \frac{\bar{y} - \bar{x} - (\mu_y - \mu_x)}{s \sqrt{\frac{1}{n_y} + \frac{1}{n_x}}} \sim t_{n_y + n_x - 2}
\]</span></p>
<p><strong>Confidence Interval</strong></p>
<p>A <span class="math inline">\(100(1 - \alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu_y - \mu_x\)</span> is: <span class="math display">\[
\bar{y} - \bar{x} \pm t_{n_y + n_x - 2, \alpha/2} \cdot s \sqrt{\frac{1}{n_y} + \frac{1}{n_x}}
\]</span></p>
<p><strong>Hypothesis Test</strong></p>
<p>Testing: <span class="math display">\[
H_0: \mu_y - \mu_x = \delta_0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x \neq \delta_0
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if: <span class="math display">\[
|T| &gt; t_{n_y + n_x - 2, \alpha/2}
\]</span></p>
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Small sample test with equal variance</span></span>
<span><span class="va">t_test_equal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x</span>, var.equal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">t_test_equal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Two Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  y and x</span></span>
<span><span class="co">#&gt; t = 0.5, df = 8, p-value = 0.6305</span></span>
<span><span class="co">#&gt; alternative hypothesis: true difference in means is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -3.612008  5.612008</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean of x mean of y </span></span>
<span><span class="co">#&gt;        14        13</span></span></code></pre></div>
<hr>
</div>
<div id="unequal-variances" class="section level5" number="4.4.1.2.2">
<h5>
<span class="header-section-number">4.4.1.2.2</span> Unequal Variances<a class="anchor" aria-label="anchor" href="#unequal-variances"><i class="fas fa-link"></i></a>
</h5>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>Independence and Identically Distributed (i.i.d.) Observations</li>
</ol>
<p>Assume that observations in each sample are i.i.d., which implies:</p>
<p><span class="math display">\[ var(\bar{y}) = \frac{\sigma^2_y}{n_y}, \quad var(\bar{x}) = \frac{\sigma^2_x}{n_x} \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Independence Between Samples</li>
</ol>
<p>The samples are assumed to be independent, meaning no observation from one sample influences observations from the other. This independence allows us to write:</p>
<p><span class="math display">\[ \begin{aligned} var(\bar{y} - \bar{x}) &amp;= var(\bar{y}) + var(\bar{x}) - 2cov(\bar{y}, \bar{x}) \\ &amp;= var(\bar{y}) + var(\bar{x}) \\ &amp;= \frac{\sigma^2_y}{n_y} + \frac{\sigma^2_x}{n_x} \end{aligned} \]</span></p>
<p>This calculation assumes <span class="math inline">\(cov(\bar{y}, \bar{x}) = 0\)</span> due to the independence between the samples.</p>
<ol start="3" style="list-style-type: decimal">
<li>Normality Assumption</li>
</ol>
<p>We assume that the underlying populations are normally distributed. This assumption justifies the use of the <a href="prerequisites.html#students-t-distribution">Student’s T Distribution</a>, which is critical for hypothesis testing and constructing confidence intervals.</p>
<ol start="4" style="list-style-type: decimal">
<li>Unequal Variances</li>
</ol>
<p><span class="math inline">\(\sigma_y^2 \neq \sigma_x^2\)</span></p>
<p><strong>Test Statistic</strong></p>
<p>The test statistic is:</p>
<p><span class="math display">\[
T = \frac{\bar{y} - \bar{x} - (\mu_y - \mu_x)}{\sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}}
\]</span></p>
<p><strong>Degrees of Freedom (Welch-Satterthwaite Approximation)</strong> <span class="citation">(<a href="references.html#ref-Satterthwaite_1946">Satterthwaite 1946</a>)</span></p>
<p>The degrees of freedom are approximated by:</p>
<p><span class="math display">\[
v = \frac{\left(\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}\right)^2}{\frac{\left(\frac{s_y^2}{n_y}\right)^2}{n_y - 1} + \frac{\left(\frac{s_x^2}{n_x}\right)^2}{n_x - 1}}
\]</span></p>
<p>Since <span class="math inline">\(v\)</span> is fractional, truncate to the nearest integer.</p>
<p><strong>Confidence Interval</strong></p>
<p>A <span class="math inline">\(100(1 - \alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu_y - \mu_x\)</span> is:</p>
<p><span class="math display">\[
\bar{y} - \bar{x} \pm t_{v, \alpha/2} \sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}
\]</span></p>
<p><strong>Hypothesis Test</strong></p>
<p>Testing:</p>
<p><span class="math display">\[
H_0: \mu_y - \mu_x = \delta_0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x \neq \delta_0
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
|T| &gt; t_{v, \alpha/2}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
t = \frac{\bar{y} - \bar{x}-\delta_0}{\sqrt{s^2_y/n_y + s^2_x /n_x}}
\]</span></p>
<div class="sourceCode" id="cb116"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Small sample test with unequal variance</span></span>
<span><span class="va">t_test_unequal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x</span>, var.equal <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">t_test_unequal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Welch Two Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  y and x</span></span>
<span><span class="co">#&gt; t = 0.5, df = 8, p-value = 0.6305</span></span>
<span><span class="co">#&gt; alternative hypothesis: true difference in means is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -3.612008  5.612008</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean of x mean of y </span></span>
<span><span class="co">#&gt;        14        13</span></span></code></pre></div>
</div>
</div>
</div>
<div id="for-variances" class="section level3" number="4.4.2">
<h3>
<span class="header-section-number">4.4.2</span> For Variances<a class="anchor" aria-label="anchor" href="#for-variances"><i class="fas fa-link"></i></a>
</h3>
<p>To compare the variances of two independent samples, we can use the <strong>F-test</strong>. The test statistic is defined as:</p>
<p><span class="math display">\[
F_{ndf,ddf} = \frac{s_1^2}{s_2^2}
\]</span></p>
<p>where <span class="math inline">\(s_1^2 &gt; s_2^2\)</span>, <span class="math inline">\(ndf = n_1 - 1\)</span>, and <span class="math inline">\(ddf = n_2 - 1\)</span> are the numerator and denominator degrees of freedom, respectively.</p>
<hr>
<div id="f-test" class="section level4" number="4.4.2.1">
<h4>
<span class="header-section-number">4.4.2.1</span> F-Test<a class="anchor" aria-label="anchor" href="#f-test"><i class="fas fa-link"></i></a>
</h4>
<p>The hypotheses for the F-test are:</p>
<p><span class="math display">\[
H_0: \sigma_y^2 = \sigma_x^2 \quad \text{(equal variances)} \\
H_a: \sigma_y^2 \neq \sigma_x^2 \quad \text{(unequal variances)}
\]</span></p>
<p>The test statistic is:</p>
<p><span class="math display">\[
F = \frac{s_y^2}{s_x^2}
\]</span></p>
<p>where <span class="math inline">\(s_y^2\)</span> and <span class="math inline">\(s_x^2\)</span> are the sample variances of the two groups.</p>
<p><strong>Decision Rule</strong></p>
<p>Reject <span class="math inline">\(H_0\)</span> if:</p>
<ul>
<li><p><span class="math inline">\(F &gt; F_{n_y-1, n_x-1, \alpha/2}\)</span> (upper critical value), or</p></li>
<li><p><span class="math inline">\(F &lt; F_{n_y-1, n_x-1, 1-\alpha/2}\)</span> (lower critical value).</p></li>
</ul>
<p>Here:</p>
<ul>
<li>
<span class="math inline">\(F_{n_y-1, n_x-1, \alpha/2}\)</span> and <span class="math inline">\(F_{n_y-1, n_x-1, 1-\alpha/2}\)</span> are the critical points of the <strong>F-distribution</strong>, with <span class="math inline">\(n_y - 1\)</span> and <span class="math inline">\(n_x - 1\)</span> degrees of freedom.</li>
</ul>
<p><strong>Assumptions</strong></p>
<ul>
<li>The F-test requires that the data in both groups follow a <strong>normal distribution</strong>.</li>
<li>The F-test is sensitive to deviations from normality (e.g., heavy-tailed distributions). If the normality assumption is violated, it may lead to an inflated Type I error rate (false positives).</li>
</ul>
<p><strong>Limitations and Alternatives</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Sensitivity to Non-Normality</strong>:
<ul>
<li>When data have long-tailed distributions (positive kurtosis), the F-test may produce misleading results.</li>
<li>To assess normality, see <a href="descriptive-statistics.html#normality-assessment">Normality Assessment</a>.</li>
</ul>
</li>
<li>
<strong>Nonparametric Alternatives</strong>:
<ul>
<li>If the normality assumption is not met, use robust tests such as the <a href="basic-statistical-inference.html#modified-levene-test-brown-forsythe-test">Modified Levene Test (Brown-Forsythe Test)</a>, which compares group variances based on medians instead of means.</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load iris dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Subset data for two species</span></span>
<span><span class="va">irisVe</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"versicolor"</span><span class="op">]</span></span>
<span><span class="va">irisVi</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"virginica"</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Perform F-test</span></span>
<span><span class="va">f_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/var.test.html">var.test</a></span><span class="op">(</span><span class="va">irisVe</span>, <span class="va">irisVi</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">f_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  F test to compare two variances</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  irisVe and irisVi</span></span>
<span><span class="co">#&gt; F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335</span></span>
<span><span class="co">#&gt; alternative hypothesis: true ratio of variances is not equal to 1</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.2941935 0.9135614</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; ratio of variances </span></span>
<span><span class="co">#&gt;          0.5184243</span></span></code></pre></div>
</div>
<div id="levenes-test" class="section level4" number="4.4.2.2">
<h4>
<span class="header-section-number">4.4.2.2</span> Levene’s Test<a class="anchor" aria-label="anchor" href="#levenes-test"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Levene’s Test</strong> is a robust method for testing the equality of variances across multiple groups. Unlike the F-test, it is less sensitive to departures from normality and is particularly useful for handling non-normal distributions and datasets with outliers. The test works by analyzing the deviations of individual observations from their group mean or median.</p>
<p><strong>Test Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>Compute the absolute deviations of each observation from its group mean or median:
<ul>
<li>For group <span class="math inline">\(y\)</span>: <span class="math display">\[
d_{y,i} = |y_i - \text{Central Value}_y|
\]</span>
</li>
<li>For group <span class="math inline">\(x\)</span>: <span class="math display">\[
d_{x,j} = |x_j - \text{Central Value}_x|
\]</span>
</li>
<li>The “central value” can be either the <strong>mean</strong> (classic Levene’s test) or the <strong>median</strong> (<a href="basic-statistical-inference.html#modified-levene-test-brown-forsythe-test">Modified Levene Test (Brown-Forsythe Test)</a> variation, more robust for non-normal data).</li>
</ul>
</li>
<li>Perform a one-way ANOVA on the absolute deviations to test for differences in group variances.</li>
</ol>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): All groups have equal variances.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_a\)</span>): At least one group has a variance different from the others.</li>
</ul>
<p><strong>Test Statistic</strong></p>
<p>The Levene test statistic is calculated as an ANOVA on the absolute deviations. Let:</p>
<ul>
<li><p><span class="math inline">\(k\)</span>: Number of groups,</p></li>
<li><p><span class="math inline">\(n_i\)</span>: Number of observations in group <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(n\)</span>: Total number of observations.</p></li>
</ul>
<p>The test statistic is:</p>
<p><span class="math display">\[
W = \frac{(n - k) \sum_{i=1}^k n_i (\bar{d}_i - \bar{d})^2}{(k - 1) \sum_{i=1}^k \sum_{j=1}^{n_i} (d_{i,j} - \bar{d}_i)^2}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(d_{i,j}\)</span>: Absolute deviations within group <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(\bar{d}_i\)</span>: Mean of the absolute deviations for group <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(\bar{d}\)</span>: Overall mean of the absolute deviations.</p></li>
</ul>
<p>Under the null hypothesis, <span class="math inline">\(W \sim F_{k-1, n - k}\)</span>.</p>
<p><strong>Decision Rule</strong></p>
<ul>
<li>Compute the test statistic <span class="math inline">\(W\)</span>.</li>
<li>Reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if: <span class="math display">\[
W &gt; F_{k-1, n-k, \alpha}
\]</span>
</li>
</ul>
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load required package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform Levene's Test (absolute deviations from the mean)</span></span>
<span><span class="va">levene_test_mean</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/leveneTest.html">leveneTest</a></span><span class="op">(</span><span class="va">Petal.Width</span> <span class="op">~</span> <span class="va">Species</span>, data <span class="op">=</span> <span class="va">iris</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform Levene's Test (absolute deviations from the median)</span></span>
<span><span class="va">levene_test_median</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/car/man/leveneTest.html">leveneTest</a></span><span class="op">(</span><span class="va">Petal.Width</span> <span class="op">~</span> <span class="va">Species</span>, data <span class="op">=</span> <span class="va">iris</span>, center <span class="op">=</span> <span class="va">median</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">levene_test_mean</span></span>
<span><span class="co">#&gt; Levene's Test for Homogeneity of Variance (center = median)</span></span>
<span><span class="co">#&gt;        Df F value    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; group   2  19.892 2.261e-08 ***</span></span>
<span><span class="co">#&gt;       147                      </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="va">levene_test_median</span></span>
<span><span class="co">#&gt; Levene's Test for Homogeneity of Variance (center = median)</span></span>
<span><span class="co">#&gt;        Df F value    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; group   2  19.892 2.261e-08 ***</span></span>
<span><span class="co">#&gt;       147                      </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>The output includes:</p>
<ul>
<li><p><strong>Df</strong>: Degrees of freedom for the numerator and denominator.</p></li>
<li><p><strong>F-value</strong>: The computed value of the test statistic <span class="math inline">\(W\)</span>.</p></li>
<li><p><strong>p-value</strong>: The probability of observing such a value under the null hypothesis.</p></li>
<li><p>If the p-value is less than <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span> and conclude that the group variances are significantly different.</p></li>
<li><p>Otherwise, fail to reject <span class="math inline">\(H_0\)</span> and conclude there is no evidence of a difference in variances.</p></li>
</ul>
<p><strong>Advantages of Levene’s Test</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Robustness</strong>:</p>
<ul>
<li>Handles non-normal data and outliers better than the F-test.</li>
</ul>
</li>
<li>
<p><strong>Flexibility</strong>:</p>
<ul>
<li>
<p>By choosing the center value (mean or median), it can adapt to different data characteristics:</p>
<ul>
<li><p>Use the mean for symmetric distributions.</p></li>
<li><p>Use the median for non-normal or skewed data.</p></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Versatility</strong>:</p>
<ul>
<li>Applicable to comparing variances across more than two groups, unlike the <a href="basic-statistical-inference.html#modified-levene-test-brown-forsythe-test">Modified Levene Test (Brown-Forsythe Test)</a>, which is limited to two groups.</li>
</ul>
</li>
</ol>
</div>
<div id="modified-levene-test-brown-forsythe-test" class="section level4" number="4.4.2.3">
<h4>
<span class="header-section-number">4.4.2.3</span> Modified Levene Test (Brown-Forsythe Test)<a class="anchor" aria-label="anchor" href="#modified-levene-test-brown-forsythe-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Modified Levene Test</strong> is a robust alternative to the <a href="basic-statistical-inference.html#f-test">F-test</a> for comparing variances between two groups. Instead of using squared deviations (as in the F-test), this test considers the <strong>absolute deviations</strong> from the median, making it less sensitive to non-normal data and long-tailed distributions. It is, however, still appropriate for normally distributed data.</p>
<p>For each sample, compute the absolute deviations from the median:</p>
<p><span class="math display">\[
d_{y,i} = |y_i - y_{.5}| \quad \text{and} \quad d_{x,i} = |x_i - x_{.5}|
\]</span></p>
<p>Let:</p>
<ul>
<li>
<span class="math inline">\(\bar{d}_y\)</span> and <span class="math inline">\(\bar{d}_x\)</span> be the means of the absolute deviations for groups <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, respectively.</li>
</ul>
<p>The test statistic is:</p>
<p><span class="math display">\[
t_L^* = \frac{\bar{d}_y - \bar{d}_x}{s \sqrt{\frac{1}{n_y} + \frac{1}{n_x}}}
\]</span></p>
<p>where the pooled variance <span class="math inline">\(s^2\)</span> is:</p>
<p><span class="math display">\[
s^2 = \frac{\sum_{i=1}^{n_y} (d_{y,i} - \bar{d}_y)^2 + \sum_{j=1}^{n_x} (d_{x,j} - \bar{d}_x)^2}{n_y + n_x - 2}
\]</span></p>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Constant Variance of Error Terms</strong>:<br>
The test assumes equal error variances in each group under the null hypothesis.</p></li>
<li><p><strong>Moderate Sample Size</strong>:<br>
The approximation <span class="math inline">\(t_L^* \sim t_{n_y + n_x - 2}\)</span> holds well for moderate or large sample sizes.</p></li>
</ol>
<p><strong>Decision Rule</strong></p>
<ul>
<li>Compute <span class="math inline">\(t_L^*\)</span> using the formula above.</li>
<li>Reject the null hypothesis of equal variances if: <span class="math display">\[
|t_L^*| &gt; t_{n_y + n_x - 2; \alpha/2}
\]</span>
</li>
</ul>
<p>This is equivalent to applying a two-sample t-test to the absolute deviations.</p>
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Absolute deviations from the median</span></span>
<span><span class="va">dVe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">irisVe</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">irisVe</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dVi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">irisVi</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">irisVi</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform t-test on absolute deviations</span></span>
<span><span class="va">levene_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">dVe</span>, <span class="va">dVi</span>, var.equal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">levene_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Two Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  dVe and dVi</span></span>
<span><span class="co">#&gt; t = -2.5584, df = 98, p-value = 0.01205</span></span>
<span><span class="co">#&gt; alternative hypothesis: true difference in means is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -0.12784786 -0.01615214</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean of x mean of y </span></span>
<span><span class="co">#&gt;     0.154     0.226</span></span></code></pre></div>
<p>For small sample sizes, use the unequal variance t-test directly on the original data as a robust alternative:</p>
<div class="sourceCode" id="cb120"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Small sample t-test with unequal variances</span></span>
<span><span class="va">small_sample_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">irisVe</span>, <span class="va">irisVi</span>, var.equal <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">small_sample_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Welch Two Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  irisVe and irisVi</span></span>
<span><span class="co">#&gt; t = -14.625, df = 89.043, p-value &lt; 2.2e-16</span></span>
<span><span class="co">#&gt; alternative hypothesis: true difference in means is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -0.7951002 -0.6048998</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean of x mean of y </span></span>
<span><span class="co">#&gt;     1.326     2.026</span></span></code></pre></div>
</div>
<div id="bartletts-test" class="section level4" number="4.4.2.4">
<h4>
<span class="header-section-number">4.4.2.4</span> Bartlett’s Test<a class="anchor" aria-label="anchor" href="#bartletts-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Bartlett’s Test</strong> is a statistical procedure for testing the equality of variances across multiple groups. It assumes that the data in each group are normally distributed and is sensitive to deviations from normality. When the assumption of normality holds, Bartlett’s Test is more powerful than <a href="basic-statistical-inference.html#levenes-test">Levene’s Test</a>.</p>
<p><strong>Hypotheses for Bartlett’s Test</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): All groups have equal variances.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_a\)</span>): At least one group has a variance different from the others.</li>
</ul>
<p>The test statistic for Bartlett’s Test is:</p>
<p><span class="math display">\[
B = \frac{(n - k) \log(S_p^2) - \sum_{i=1}^k (n_i - 1) \log(S_i^2)}{1 + \frac{1}{3(k - 1)} \left( \sum_{i=1}^k \frac{1}{n_i - 1} - \frac{1}{n - k} \right)}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(k\)</span>: Number of groups,</p></li>
<li><p><span class="math inline">\(n_i\)</span>: Number of observations in group <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(n = \sum_{i=1}^k n_i\)</span>: Total number of observations,</p></li>
<li><p><span class="math inline">\(S_i^2\)</span>: Sample variance of group <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(S_p^2\)</span>: Pooled variance, given by: <span class="math display">\[
  S_p^2 = \frac{\sum_{i=1}^k (n_i - 1) S_i^2}{n - k}
  \]</span></p></li>
</ul>
<p>Under the null hypothesis, the test statistic <span class="math inline">\(B \sim \chi^2_{k - 1}\)</span>.</p>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Normality</strong>: The data in each group must follow a normal distribution.</li>
<li>
<strong>Independence</strong>: Observations within and between groups must be independent.</li>
<li>
<strong>Equal Sample Sizes (Optional)</strong>: Bartlett’s Test is more robust if sample sizes are approximately equal.</li>
</ol>
<p><strong>Decision Rule</strong></p>
<ul>
<li>Compute the test statistic <span class="math inline">\(B\)</span>.</li>
<li>Compare <span class="math inline">\(B\)</span> to the critical value of the Chi-Square distribution at <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(k - 1\)</span> degrees of freedom.</li>
<li>Reject <span class="math inline">\(H_0\)</span> if: <span class="math display">\[
B &gt; \chi^2_{k-1, \alpha}
\]</span>
</li>
</ul>
<p>Alternatively, use the p-value:</p>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if the p-value <span class="math inline">\(\leq \alpha\)</span>.</li>
</ul>
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Perform Bartlett's Test</span></span>
<span><span class="va">bartlett_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/bartlett.test.html">bartlett.test</a></span><span class="op">(</span><span class="va">Petal.Width</span> <span class="op">~</span> <span class="va">Species</span>, data <span class="op">=</span> <span class="va">iris</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">bartlett_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Bartlett test of homogeneity of variances</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  Petal.Width by Species</span></span>
<span><span class="co">#&gt; Bartlett's K-squared = 39.213, df = 2, p-value = 3.055e-09</span></span></code></pre></div>
<p>The output includes:</p>
<ul>
<li><p><strong>Bartlett’s K-squared</strong>: The value of the test statistic <span class="math inline">\(B\)</span>.</p></li>
<li><p><strong>df</strong>: Degrees of freedom (<span class="math inline">\(k - 1\)</span>), where <span class="math inline">\(k\)</span> is the number of groups.</p></li>
<li><p><strong>p-value</strong>: The probability of observing such a value of <span class="math inline">\(B\)</span> under <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>If the p-value is less than <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span> and conclude that the variances are significantly different across groups.</p></li>
<li><p>If the p-value is greater than <span class="math inline">\(\alpha\)</span>, fail to reject <span class="math inline">\(H_0\)</span> and conclude that there is no significant evidence of variance differences.</p></li>
</ul>
<p><strong>Limitations of Bartlett’s Test</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Sensitivity to Non-Normality</strong>:<br>
Bartlett’s Test is highly sensitive to departures from normality. Even slight deviations can lead to misleading results.</p></li>
<li><p><strong>Not Robust to Outliers</strong>:<br>
Outliers can disproportionately affect the test result.</p></li>
<li>
<p><strong>Alternatives</strong>:<br>
If the normality assumption is violated, use robust alternatives like:</p>
<ul>
<li><p><a href="basic-statistical-inference.html#levenes-test">Levene’s Test</a> (absolute deviations)</p></li>
<li><p><a href="basic-statistical-inference.html#modified-levene-test-brown-forsythe-test">Modified Levene Test (Brown-Forsythe Test)</a> (median-based absolute deviations)</p></li>
</ul>
</li>
</ol>
<p><strong>Advantages of Bartlett’s Test</strong></p>
<ul>
<li><p><strong>High Power</strong>: Bartlett’s Test is more powerful than robust alternatives when the normality assumption holds.</p></li>
<li><p><strong>Simple Implementation</strong>: The test is easy to perform and interpret.</p></li>
</ul>
</div>
</div>
<div id="power" class="section level3" number="4.4.3">
<h3>
<span class="header-section-number">4.4.3</span> Power<a class="anchor" aria-label="anchor" href="#power"><i class="fas fa-link"></i></a>
</h3>
<p>To evaluate the power of a test, we consider the situation where the variances are equal across groups:</p>
<p><span class="math display">\[
\sigma_y^2 = \sigma_x^2 = \sigma^2
\]</span></p>
<p>Under the assumption of equal variances, we take <strong>equal sample sizes</strong> from both groups, i.e., <span class="math inline">\(n_y = n_x = n\)</span>.</p>
<p><strong>Hypotheses for One-Sided Testing</strong></p>
<p>We are testing:</p>
<p><span class="math display">\[
H_0: \mu_y - \mu_x \leq 0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x &gt; 0
\]</span></p>
<p><strong>Test Statistic</strong></p>
<p>The <span class="math inline">\(\alpha\)</span>-level <strong>z-test</strong> rejects <span class="math inline">\(H_0\)</span> if the test statistic:</p>
<p><span class="math display">\[
z = \frac{\bar{y} - \bar{x}}{\sigma \sqrt{\frac{2}{n}}} &gt; z_\alpha
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{x}\)</span> are the sample means,</p></li>
<li><p><span class="math inline">\(\sigma\)</span> is the common standard deviation,</p></li>
<li><p><span class="math inline">\(z_\alpha\)</span> is the critical value from the standard normal distribution.</p></li>
</ul>
<p><strong>Power Function</strong></p>
<p>The <strong>power</strong> of the test, denoted as <span class="math inline">\(\pi(\mu_y - \mu_x)\)</span>, is the probability of correctly rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(\mu_y - \mu_x\)</span> is some specified value. Under the alternative hypothesis, the power function is:</p>
<p><span class="math display">\[
\pi(\mu_y - \mu_x) = \Phi\left(-z_\alpha + \frac{\mu_y - \mu_x}{\sigma} \sqrt{\frac{n}{2}}\right)
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\Phi\)</span> is the cumulative distribution function (CDF) of the standard normal distribution,</p></li>
<li><p><span class="math inline">\(\frac{\mu_y - \mu_x}{\sigma} \sqrt{\frac{n}{2}}\)</span> represents the standardized effect size.</p></li>
</ul>
<p><strong>Determining the Required Sample Size</strong></p>
<p>To achieve a desired power of <span class="math inline">\(1 - \beta\)</span> when the true difference is <span class="math inline">\(\delta\)</span> (the smallest difference of interest), we solve for the required sample size <span class="math inline">\(n\)</span>. The power equation is:</p>
<p><span class="math display">\[
\Phi\left(-z_\alpha + \frac{\delta}{\sigma} \sqrt{\frac{n}{2}}\right) = 1 - \beta
\]</span></p>
<p>Rearranging for <span class="math inline">\(n\)</span>, the required sample size is:</p>
<p><span class="math display">\[
n = \frac{2 \sigma^2}{\delta^2} \left(z_\alpha + z_\beta\right)^2
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\sigma\)</span>: The common standard deviation,</p></li>
<li><p><span class="math inline">\(z_{\alpha}\)</span>: The critical value for the Type I error rate <span class="math inline">\(\alpha\)</span> (one-sided test),</p></li>
<li><p><span class="math inline">\(z_{\beta}\)</span>: The critical value for the Type II error rate <span class="math inline">\(\beta\)</span> (related to power <span class="math inline">\(1 - \beta\)</span>),</p></li>
<li><p><span class="math inline">\(\delta\)</span>: The minimum detectable difference between the means.</p></li>
</ul>
<div class="sourceCode" id="cb122"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Parameters</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span>   <span class="co"># Significance level</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fl">0.2</span>     <span class="co"># Type II error rate (1 - Power = 0.2)</span></span>
<span><span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span>      <span class="co"># Common standard deviation</span></span>
<span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>    <span class="co"># Minimum detectable difference</span></span>
<span></span>
<span><span class="co"># Critical values</span></span>
<span><span class="va">z_alpha</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span><span class="op">)</span></span>
<span><span class="va">z_beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">beta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sample size calculation</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">sigma</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="va">z_alpha</span> <span class="op">+</span> <span class="va">z_beta</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="va">delta</span> <span class="op">^</span> <span class="fl">2</span></span>
<span></span>
<span><span class="co"># Output the required sample size (per group)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">ceiling</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 50</span></span></code></pre></div>
<p><strong>Sample Size for Two-Sided Tests</strong></p>
<p>For a <strong>two-sided test</strong>, replace <span class="math inline">\(z_{\alpha}\)</span> with <span class="math inline">\(z_{\alpha/2}\)</span> to account for the two-tailed critical region:</p>
<p><span class="math display">\[
n = 2 \left( \frac{\sigma (z_{\alpha/2} + z_{\beta})}{\delta} \right)^2
\]</span></p>
<p>This ensures that the test has the required power <span class="math inline">\(1 - \beta\)</span> to detect a difference of size <span class="math inline">\(\delta\)</span> between the means at significance level <span class="math inline">\(\alpha\)</span>.</p>
<p><strong>Adjustment for the Exact t-Test</strong></p>
<p>When conducting an exact <strong>two-sample t-test</strong> for small sample sizes, the sample size calculation involves the <strong>non-central t-distribution</strong>. An approximate correction can be applied using the critical values from the t-distribution instead of the z-distribution.</p>
<p>The adjusted sample size is:</p>
<p><span class="math display">\[
n^* = 2 \left( \frac{\sigma (t_{2n-2; \alpha/2} + t_{2n-2; \beta})}{\delta} \right)^2
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(t_{2n-2; \alpha/2}\)</span>: The critical value for the t-distribution with <span class="math inline">\(2n - 2\)</span> degrees of freedom for significance level <span class="math inline">\(\alpha/2\)</span>,</p></li>
<li><p><span class="math inline">\(t_{2n-2; \beta}\)</span>: The critical value for the t-distribution with <span class="math inline">\(2n - 2\)</span> degrees of freedom for power <span class="math inline">\(1 - \beta\)</span>.</p></li>
</ul>
<p>This correction adjusts for the increased variability of the t-distribution, especially important for small sample sizes.</p>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Parameters</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.05</span>    <span class="co"># Significance level</span></span>
<span><span class="va">power</span> <span class="op">&lt;-</span> <span class="fl">0.8</span>     <span class="co"># Desired power</span></span>
<span><span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span>       <span class="co"># Common standard deviation</span></span>
<span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>     <span class="co"># Minimum detectable difference</span></span>
<span></span>
<span><span class="co"># Calculate sample size for two-sided test</span></span>
<span><span class="va">sample_size</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/power.t.test.html">power.t.test</a></span><span class="op">(</span></span>
<span>        delta <span class="op">=</span> <span class="va">delta</span>,</span>
<span>        sd <span class="op">=</span> <span class="va">sigma</span>,</span>
<span>        sig.level <span class="op">=</span> <span class="va">alpha</span>,</span>
<span>        power <span class="op">=</span> <span class="va">power</span>,</span>
<span>        type <span class="op">=</span> <span class="st">"two.sample"</span>,</span>
<span>        alternative <span class="op">=</span> <span class="st">"two.sided"</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">sample_size</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Two-sample t test power calculation </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;               n = 63.76576</span></span>
<span><span class="co">#&gt;           delta = 0.5</span></span>
<span><span class="co">#&gt;              sd = 1</span></span>
<span><span class="co">#&gt;       sig.level = 0.05</span></span>
<span><span class="co">#&gt;           power = 0.8</span></span>
<span><span class="co">#&gt;     alternative = two.sided</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; NOTE: n is number in *each* group</span></span></code></pre></div>
<p><strong>Key Insights</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Z-Test vs. T-Test</strong>:<br>
For large samples, the normal approximation (z-test) works well. For small samples, the t-test correction using the t-distribution is essential.</p></li>
<li>
<p><strong>Effect of Power and Significance Level</strong>:</p>
<ul>
<li><p>Increasing power (<span class="math inline">\(1 - \beta\)</span>) or decreasing <span class="math inline">\(\alpha\)</span> requires larger sample sizes.</p></li>
<li><p>A smaller minimum detectable difference (<span class="math inline">\(\delta\)</span>) also requires a larger sample size.</p></li>
</ul>
</li>
<li><p><strong>Two-Sided Tests</strong>:<br>
Two-sided tests require larger sample sizes compared to one-sided tests due to the split critical region.</p></li>
</ol>
<p><strong>Formula Summary</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="18%">
<col width="81%">
</colgroup>
<thead><tr class="header">
<th>Test Type</th>
<th>Formula for Sample Size</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>One-Sided Test</td>
<td><span class="math inline">\(n = 2 \left( \frac{\sigma (z_{\alpha} + z_{\beta})}{\delta} \right)^2\)</span></td>
</tr>
<tr class="even">
<td>Two-Sided Test</td>
<td><span class="math inline">\(n = 2 \left( \frac{\sigma (z_{\alpha/2} + z_{\beta})}{\delta} \right)^2\)</span></td>
</tr>
<tr class="odd">
<td>Approximate t-Test</td>
<td><span class="math inline">\(n^* = 2 \left( \frac{\sigma (t_{2n-2; \alpha/2} + t_{2n-2; \beta})}{\delta} \right)^2\)</span></td>
</tr>
</tbody>
</table></div>
</div>
<div id="matched-pair-designs" class="section level3" number="4.4.4">
<h3>
<span class="header-section-number">4.4.4</span> Matched Pair Designs<a class="anchor" aria-label="anchor" href="#matched-pair-designs"><i class="fas fa-link"></i></a>
</h3>
<p>In <strong>matched pair designs</strong>, two treatments are compared by measuring responses for the same subjects under both treatments. This ensures that the effects of subject-to-subject variability are minimized, as each subject serves as their own control.</p>
<hr>
<p>We have two treatments, and the data are structured as follows:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Subject</th>
<th>Treatment A</th>
<th>Treatment B</th>
<th>Difference</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(y_1\)</span></td>
<td><span class="math inline">\(x_1\)</span></td>
<td><span class="math inline">\(d_1 = y_1 - x_1\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(y_2\)</span></td>
<td><span class="math inline">\(x_2\)</span></td>
<td><span class="math inline">\(d_2 = y_2 - x_2\)</span></td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>n</td>
<td><span class="math inline">\(y_n\)</span></td>
<td><span class="math inline">\(x_n\)</span></td>
<td><span class="math inline">\(d_n = y_n - x_n\)</span></td>
</tr>
</tbody>
</table></div>
<p>Here:</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span> represents the observation under Treatment A,</p></li>
<li><p><span class="math inline">\(x_i\)</span> represents the observation under Treatment B,</p></li>
<li><p><span class="math inline">\(d_i = y_i - x_i\)</span> is the difference for subject <span class="math inline">\(i\)</span>.</p></li>
</ul>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>Observations <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> are measured for the same subjects, inducing correlation.</li>
<li>The differences <span class="math inline">\(d_i\)</span> are independent and identically distributed (iid), and follow a normal distribution: <span class="math display">\[
d_i \sim N(\mu_D, \sigma_D^2)
\]</span>
</li>
</ol>
<p><strong>Mean and Variance of the Difference</strong></p>
<p>The mean difference <span class="math inline">\(\mu_D\)</span> and the variance <span class="math inline">\(\sigma_D^2\)</span> are given by:</p>
<p><span class="math display">\[
\mu_D = E(y_i - x_i) = \mu_y - \mu_x
\]</span></p>
<p><span class="math display">\[
\sigma_D^2 = \text{Var}(y_i - x_i) = \text{Var}(y_i) + \text{Var}(x_i) - 2 \cdot \text{Cov}(y_i, x_i)
\]</span></p>
<ul>
<li>If the <strong>covariance</strong> between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> is <strong>positive</strong> (a typical case), the variance of the differences <span class="math inline">\(\sigma_D^2\)</span> is <strong>reduced</strong> compared to the independent sample case.</li>
<li>This is the key benefit of <strong>Matched Pair Designs</strong>: reduced variability increases the precision of estimates.</li>
</ul>
<p><strong>Sample Statistics</strong></p>
<p>For the differences <span class="math inline">\(d_i = y_i - x_i\)</span>:</p>
<ul>
<li><p>The sample mean of the differences: <span class="math display">\[
\bar{d} = \frac{1}{n} \sum_{i=1}^n d_i = \bar{y} - \bar{x}
\]</span></p></li>
<li><p>The sample variance of the differences: <span class="math display">\[
s_d^2 = \frac{1}{n-1} \sum_{i=1}^n (d_i - \bar{d})^2
\]</span></p></li>
</ul>
<p>Once the data are converted into differences <span class="math inline">\(d_i\)</span>, the problem reduces to <strong>one-sample inference</strong>. We can use tests and confidence intervals (CIs) for the mean of a single sample.</p>
<p><strong>Hypothesis Test</strong></p>
<p>We test the following hypotheses:</p>
<p><span class="math display">\[
H_0: \mu_D = 0 \quad \text{vs.} \quad H_a: \mu_D \neq 0
\]</span></p>
<p>The test statistic is:</p>
<p><span class="math display">\[
t = \frac{\bar{d}}{s_d / \sqrt{n}} \sim t_{n-1}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of subjects.</p>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if: <span class="math display">\[
|t| &gt; t_{n-1, \alpha/2}
\]</span>
</li>
</ul>
<p><strong>Confidence Interval</strong></p>
<p>A <span class="math inline">\(100(1 - \alpha)\%\)</span> confidence interval for <span class="math inline">\(\mu_D\)</span> is:</p>
<p><span class="math display">\[
\bar{d} \pm t_{n-1, \alpha/2} \cdot \frac{s_d}{\sqrt{n}}
\]</span></p>
<div class="sourceCode" id="cb124"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sample data</span></span>
<span><span class="va">treatment_a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">90</span>, <span class="fl">78</span>, <span class="fl">92</span>, <span class="fl">88</span><span class="op">)</span></span>
<span><span class="va">treatment_b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">80</span>, <span class="fl">86</span>, <span class="fl">75</span>, <span class="fl">89</span>, <span class="fl">85</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute differences</span></span>
<span><span class="va">differences</span> <span class="op">&lt;-</span> <span class="va">treatment_a</span> <span class="op">-</span> <span class="va">treatment_b</span></span>
<span></span>
<span><span class="co"># Perform one-sample t-test on the differences</span></span>
<span><span class="va">t_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">differences</span>, mu <span class="op">=</span> <span class="fl">0</span>, alternative <span class="op">=</span> <span class="st">"two.sided"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">t_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  One Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  differences</span></span>
<span><span class="co">#&gt; t = 9, df = 4, p-value = 0.0008438</span></span>
<span><span class="co">#&gt; alternative hypothesis: true mean is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  2.489422 4.710578</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean of x </span></span>
<span><span class="co">#&gt;       3.6</span></span></code></pre></div>
<p>The output includes:</p>
<ul>
<li><p><strong>t-statistic</strong>: The calculated test statistic for the matched pairs.</p></li>
<li><p><strong>p-value</strong>: The probability of observing such a difference under the null hypothesis.</p></li>
<li><p><strong>Confidence Interval</strong>: The range of plausible values for the mean difference <span class="math inline">\(\mu_D\)</span>.</p></li>
<li><p>If the p-value is less than <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span> and conclude that there is a significant difference between the two treatments.</p></li>
<li><p>If the confidence interval does not include 0, this supports the conclusion of a significant difference.</p></li>
</ul>
<p><strong>Key Insights</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Reduced Variability</strong>: Positive correlation between paired observations reduces the variance of the differences, increasing test power.</p></li>
<li><p><strong>Use of Differences</strong>: The paired design converts the data into a single-sample problem for inference.</p></li>
<li><p><strong>Robustness</strong>: The paired t-test assumes normality of the differences <span class="math inline">\(d_i\)</span>. For larger <span class="math inline">\(n\)</span>, the <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a> ensures robustness to non-normality.</p></li>
</ol>
<p>Matched pair designs are a powerful way to control for subject-specific variability, leading to more precise comparisons between treatments.</p>
</div>
<div id="nonparametric-tests-for-two-samples" class="section level3" number="4.4.5">
<h3>
<span class="header-section-number">4.4.5</span> Nonparametric Tests for Two Samples<a class="anchor" aria-label="anchor" href="#nonparametric-tests-for-two-samples"><i class="fas fa-link"></i></a>
</h3>
<p>For <a href="basic-statistical-inference.html#matched-pair-designs">Matched Pair Designs</a> or independent samples where normality cannot be assumed, we use <strong>nonparametric tests</strong>. These tests do not assume any specific distribution of the data and are robust alternatives to parametric methods.</p>
<p><strong>Stochastic Order and Location Shift</strong></p>
<p>Suppose <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are random variables with cumulative distribution functions (CDFs) <span class="math inline">\(F_Y\)</span> and <span class="math inline">\(F_X\)</span>. Then <span class="math inline">\(Y\)</span> is <strong>stochastically larger</strong> than <span class="math inline">\(X\)</span> if, for all real numbers <span class="math inline">\(u\)</span>:</p>
<p><span class="math display">\[
P(Y &gt; u) \geq P(X &gt; u) \quad \text{(equivalently, } F_Y(u) \leq F_X(u)).
\]</span></p>
<p>If the two distributions differ only in their <strong>location parameters</strong>, say <span class="math inline">\(\theta_y\)</span> and <span class="math inline">\(\theta_x\)</span>, then we can frame the relationship as:</p>
<p><span class="math display">\[
Y &gt; X \quad \text{if} \quad \theta_y &gt; \theta_x.
\]</span></p>
<p>We test the following hypotheses:</p>
<ul>
<li>
<strong>Two-Sided Hypothesis</strong>: <span class="math display">\[
H_0: F_Y = F_X \quad \text{vs.} \quad H_a: F_Y \neq F_X
\]</span>
</li>
<li>
<strong>Upper One-Sided Hypothesis</strong>: <span class="math display">\[
H_0: F_Y = F_X \quad \text{vs.} \quad H_a: F_Y &lt; F_X
\]</span>
</li>
<li>
<strong>Lower One-Sided Hypothesis</strong>: <span class="math display">\[
H_0: F_Y = F_X \quad \text{vs.} \quad H_a: F_Y &gt; F_X
\]</span>
</li>
</ul>
<p>We generally avoid the completely non-directional alternative <span class="math inline">\(H_a: F_Y \neq F_X\)</span> because it allows arbitrary differences between the distributions, without requiring one distribution to be stochastically larger than the other.</p>
<p><strong>Nonparametric Tests</strong></p>
<p>When the focus is on whether the two distributions differ <strong>only in location parameters</strong>, two equivalent nonparametric tests are commonly used:</p>
<ol style="list-style-type: decimal">
<li><a href="basic-statistical-inference.html#wilcoxon-signed-rank-test">Wilcoxon Signed Rank Test</a></li>
<li><a href="basic-statistical-inference.html#mann-whitney-u-test-1">Mann-Whitney U Test</a></li>
</ol>
<p>Both tests are mathematically equivalent and test whether one sample is systematically larger than the other.</p>
<hr>
<div id="wilcoxon-rank-sum-test" class="section level4" number="4.4.5.1">
<h4>
<span class="header-section-number">4.4.5.1</span> Wilcoxon Rank-Sum Test<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Wilcoxon Rank Test</strong> is a nonparametric test used to compare two independent samples to assess whether their distributions differ in location. It is based on the ranks of the combined observations rather than their actual values.</p>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Combine and Rank Observations</strong>:<br>
Combine all <span class="math inline">\(n = n_y + n_x\)</span> observations (from both groups) into a single dataset and rank them in ascending order. If ties exist, assign the average rank to tied values.</p></li>
<li>
<p><strong>Calculate Rank Sums</strong>:<br>
Compute the sum of ranks for each group:</p>
<ul>
<li>
<span class="math inline">\(w_y\)</span>: Sum of the ranks for group <span class="math inline">\(y\)</span> (sample 1),</li>
<li>
<span class="math inline">\(w_x\)</span>: Sum of the ranks for group <span class="math inline">\(x\)</span> (sample 2).<br>
By definition: <span class="math display">\[
w_y + w_x = \frac{n(n+1)}{2}
\]</span>
</li>
</ul>
</li>
<li><p><strong>Test Statistic</strong>:<br>
The test focuses on the rank sum <span class="math inline">\(w_y\)</span>. Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(w_y\)</span> is <strong>large</strong> (indicating <span class="math inline">\(y\)</span> systematically has larger values) or equivalently, if <span class="math inline">\(w_x\)</span> is <strong>small</strong>.</p></li>
<li>
<p><strong>Null Distribution</strong>:<br>
Under <span class="math inline">\(H_0\)</span> (no difference between groups), all possible arrangements of ranks among <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are equally likely. The total number of possible rank arrangements is:</p>
<p><span class="math display">\[
\frac{(n_y + n_x)!}{n_y! \, n_x!}
\]</span></p>
</li>
<li>
<p><strong>Computational Considerations</strong>:</p>
<ul>
<li>For small samples, the exact null distribution of the rank sums can be calculated.<br>
</li>
<li>For large samples, an <strong>approximate normal distribution</strong> can be used.</li>
</ul>
</li>
</ol>
<p><strong>Hypotheses</strong></p>
<ul>
<li><p>Null Hypothesis (<span class="math inline">\(H_0\)</span>): The two samples come from identical distributions.</p></li>
<li><p>Alternative Hypothesis (<span class="math inline">\(H_a\)</span>): The two samples come from different distributions, or one distribution is systematically larger.</p></li>
<li><p><strong>Two-Sided Test</strong>: <span class="math display">\[
H_a: F_Y \neq F_X
\]</span></p></li>
<li><p><strong>One-Sided Test</strong>: <span class="math display">\[
H_a: F_Y &gt; F_X \quad \text{or} \quad H_a: F_Y &lt; F_X
\]</span></p></li>
</ul>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Subset data for two species</span></span>
<span><span class="va">irisVe</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"versicolor"</span><span class="op">]</span></span>
<span><span class="va">irisVi</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"virginica"</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Perform Wilcoxon Rank Test (approximate version, large sample)</span></span>
<span><span class="va">wilcox_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/wilcox.test.html">wilcox.test</a></span><span class="op">(</span></span>
<span>    <span class="va">irisVe</span>,</span>
<span>    <span class="va">irisVi</span>,</span>
<span>    alternative <span class="op">=</span> <span class="st">"two.sided"</span>, <span class="co"># Two-sided test</span></span>
<span>    conf.level <span class="op">=</span> <span class="fl">0.95</span>,         <span class="co"># Confidence level</span></span>
<span>    exact <span class="op">=</span> <span class="cn">FALSE</span>,             <span class="co"># Approximate test for large samples</span></span>
<span>    correct <span class="op">=</span> <span class="cn">TRUE</span>             <span class="co"># Apply continuity correction</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">wilcox_result</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wilcoxon rank sum test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  irisVe and irisVi</span></span>
<span><span class="co">#&gt; W = 49, p-value &lt; 2.2e-16</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location shift is not equal to 0</span></span></code></pre></div>
<p>The output of <code>wilcox.test</code> includes:</p>
<ul>
<li><p><strong>W</strong>: The test statistic, which is the smaller of the two rank sums.</p></li>
<li><p><strong>p-value</strong>: The probability of observing such a difference in rank sums under <span class="math inline">\(H_0\)</span>.</p></li>
<li><p><strong>Alternative Hypothesis</strong>: Specifies whether the test was one-sided or two-sided.</p></li>
<li><p><strong>Confidence Interval</strong> (if applicable): Provides a range for the difference in medians.</p></li>
</ul>
<p><strong>Decision Rule</strong></p>
<ol style="list-style-type: decimal">
<li><p>Reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if the p-value <span class="math inline">\(\leq \alpha\)</span>.</p></li>
<li><p>For large samples, compare the test statistic to a critical value from the normal approximation.</p></li>
</ol>
<p><strong>Key Features</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Robustness</strong>:<br>
The test does not require assumptions of normality and is robust to outliers.</p></li>
<li><p><strong>Distribution-Free</strong>:<br>
It evaluates whether two samples differ in location without assuming a specific distribution.</p></li>
<li><p><strong>Rank-Based</strong>:<br>
It uses the ranks of the observations, which makes it scale-invariant (resistant to data transformation).</p></li>
</ol>
<p><strong>Computational Considerations</strong></p>
<ul>
<li><p>For small sample sizes, the <strong>exact distribution</strong> of the rank sums is used.</p></li>
<li><p>For large sample sizes, the <strong>normal approximation</strong> with continuity correction is applied for computational efficiency.</p></li>
</ul>
</div>
<div id="mann-whitney-u-test-1" class="section level4" number="4.4.5.2">
<h4>
<span class="header-section-number">4.4.5.2</span> Mann-Whitney U Test<a class="anchor" aria-label="anchor" href="#mann-whitney-u-test-1"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Mann-Whitney U Test</strong> is a nonparametric test used to compare two independent samples. It evaluates whether one sample tends to produce larger observations than the other, based on pairwise comparisons. The test does not assume normality and is robust to outliers.</p>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Pairwise Comparisons</strong>:<br>
Compare each observation <span class="math inline">\(y_i\)</span> from sample <span class="math inline">\(Y\)</span> with each observation <span class="math inline">\(x_j\)</span> from sample <span class="math inline">\(X\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(u_y\)</span> be the number of pairs where <span class="math inline">\(y_i &gt; x_j\)</span>.</li>
<li>Let <span class="math inline">\(u_x\)</span> be the number of pairs where <span class="math inline">\(y_i &lt; x_j\)</span>.</li>
</ul>
<p>By definition: <span class="math display">\[
u_y + u_x = n_y n_x
\]</span> where <span class="math inline">\(n_y\)</span> is the sample size for group <span class="math inline">\(Y\)</span>, and <span class="math inline">\(n_x\)</span> is the sample size for group <span class="math inline">\(X\)</span>.</p>
</li>
<li>
<p><strong>Test Statistic</strong>:<br>
Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(u_y\)</span> is <strong>large</strong> (or equivalently, if <span class="math inline">\(u_x\)</span> is <strong>small</strong>).</p>
<p>The <strong>Mann-Whitney U Test</strong> and <strong>Wilcoxon Rank-Sum Test</strong> are related through the rank sums:</p>
<p><span class="math display">\[
u_y = w_y - \frac{n_y (n_y + 1)}{2}, \quad u_x = w_x - \frac{n_x (n_x + 1)}{2}
\]</span></p>
<p>Here, <span class="math inline">\(w_y\)</span> and <span class="math inline">\(w_x\)</span> are the rank sums for groups <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, respectively.</p>
</li>
</ol>
<p><strong>Hypotheses</strong></p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): The two samples come from identical distributions.</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_a\)</span>):
<ul>
<li>
<strong>Upper One-Sided</strong>: <span class="math inline">\(F_Y &lt; F_X\)</span> (Sample <span class="math inline">\(Y\)</span> is stochastically larger).</li>
<li>
<strong>Lower One-Sided</strong>: <span class="math inline">\(F_Y &gt; F_X\)</span> (Sample <span class="math inline">\(X\)</span> is stochastically larger).</li>
<li>
<strong>Two-Sided</strong>: <span class="math inline">\(F_Y \neq F_X\)</span> (Distributions differ in location).</li>
</ul>
</li>
</ul>
<p><strong>Test Statistic for Large Samples</strong></p>
<p>For large sample sizes <span class="math inline">\(n_y\)</span> and <span class="math inline">\(n_x\)</span>, the null distribution of <span class="math inline">\(U\)</span> can be approximated by a normal distribution with:</p>
<ul>
<li><p><strong>Mean</strong>: <span class="math display">\[
E(U) = \frac{n_y n_x}{2}
\]</span></p></li>
<li><p><strong>Variance</strong>: <span class="math display">\[
\text{Var}(U) = \frac{n_y n_x (n_y + n_x + 1)}{12}
\]</span></p></li>
</ul>
<p>The standardized test statistic <span class="math inline">\(z\)</span> is:</p>
<p><span class="math display">\[
z = \frac{u_y - \frac{n_y n_x}{2} - \frac{1}{2}}{\sqrt{\frac{n_y n_x (n_y + n_x + 1)}{12}}}
\]</span></p>
<p>The test rejects <span class="math inline">\(H_0\)</span> at level <span class="math inline">\(\alpha\)</span> if:</p>
<p><span class="math display">\[
z \ge z_{\alpha} \quad \text{(one-sided)} \quad \text{or} \quad |z| \ge z_{\alpha/2} \quad \text{(two-sided)}.
\]</span></p>
<p>For the <strong>two-sided test</strong>, we use:</p>
<ul>
<li><p><span class="math inline">\(u_{\text{max}} = \max(u_y, u_x)\)</span>, and</p></li>
<li><p><span class="math inline">\(u_{\text{min}} = \min(u_y, u_x)\)</span>.</p></li>
</ul>
<p>The p-value is given by:</p>
<p><span class="math display">\[
p\text{-value} = 2P(U \ge u_{\text{max}}) = 2P(U \le u_{\text{min}}).
\]</span></p>
<p>When <span class="math inline">\(y_i = x_j\)</span> (ties), assign a value of <span class="math inline">\(1/2\)</span> to both <span class="math inline">\(u_y\)</span> and <span class="math inline">\(u_x\)</span> for that pair. While the exact sampling distribution differs slightly when ties exist, the <strong>large sample normal approximation</strong> remains reasonable.</p>
<div class="sourceCode" id="cb126"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Subset data for two species</span></span>
<span><span class="va">irisVe</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"versicolor"</span><span class="op">]</span></span>
<span><span class="va">irisVi</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="op">==</span> <span class="st">"virginica"</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Perform Mann-Whitney U Test</span></span>
<span><span class="va">mann_whitney</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/wilcox.test.html">wilcox.test</a></span><span class="op">(</span></span>
<span>    <span class="va">irisVe</span>, <span class="va">irisVi</span>, </span>
<span>    alternative <span class="op">=</span> <span class="st">"two.sided"</span>, </span>
<span>    conf.level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>    exact <span class="op">=</span> <span class="cn">FALSE</span>,   <span class="co"># Approximate test for large samples</span></span>
<span>    correct <span class="op">=</span> <span class="cn">TRUE</span>   <span class="co"># Apply continuity correction</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">mann_whitney</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wilcoxon rank sum test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  irisVe and irisVi</span></span>
<span><span class="co">#&gt; W = 49, p-value &lt; 2.2e-16</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location shift is not equal to 0</span></span></code></pre></div>
<p><strong>Decision Rule</strong></p>
<ol style="list-style-type: decimal">
<li><p>Reject <span class="math inline">\(H_0\)</span> if the p-value is less than <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>For large samples, check whether $z \ge z_{\alpha}$ (one-sided) or $|z| \ge z_{\alpha/2}$ (two-sided).</p></li>
</ol>
<p><strong>Key Insights</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Robustness</strong>: The Mann-Whitney U Test does not assume normality and is robust to outliers.</p></li>
<li><p><strong>Relationship to Wilcoxon Test</strong>: The test is equivalent to the <strong>Wilcoxon Rank-Sum Test</strong> but formulated differently (based on pairwise comparisons).</p></li>
<li><p><strong>Large Sample Approximation</strong>: For large <span class="math inline">\(n_y\)</span> and <span class="math inline">\(n_x\)</span>, the test statistic <span class="math inline">\(U\)</span> follows an approximate normal distribution, simplifying computation.</p></li>
<li><p><strong>Handling Ties</strong>: Ties are accounted for by assigning fractional contributions to <span class="math inline">\(u_y\)</span> and <span class="math inline">\(u_x\)</span>.</p></li>
</ol>
</div>
</div>
</div>
<div id="categorical-data-analysis" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> Categorical Data Analysis<a class="anchor" aria-label="anchor" href="#categorical-data-analysis"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Categorical Data Analysis</strong> is used when the outcome variables are <strong>categorical</strong>.</p>
<ul>
<li>
<strong>Nominal Variables</strong>: Categories have no logical order (e.g., sex: male, female).</li>
<li>
<strong>Ordinal Variables</strong>: Categories have a logical order, but the relative distances between values are not well defined (e.g., small, medium, large).</li>
</ul>
<p>In categorical data, we often analyze how the distribution of one variable changes with the levels of another variable. For example, row percentages may differ across columns in a contingency table.</p>
<hr>
<div id="association-tests" class="section level3" number="4.5.1">
<h3>
<span class="header-section-number">4.5.1</span> Association Tests<a class="anchor" aria-label="anchor" href="#association-tests"><i class="fas fa-link"></i></a>
</h3>
<div id="small-samples" class="section level4" number="4.5.1.1">
<h4>
<span class="header-section-number">4.5.1.1</span> Small Samples<a class="anchor" aria-label="anchor" href="#small-samples"><i class="fas fa-link"></i></a>
</h4>
<div id="fishers-exact-test" class="section level5" number="4.5.1.1.1">
<h5>
<span class="header-section-number">4.5.1.1.1</span> Fisher’s Exact Test<a class="anchor" aria-label="anchor" href="#fishers-exact-test"><i class="fas fa-link"></i></a>
</h5>
<p>For small samples, the approximate tests based on the asymptotic normality of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> (the difference in proportions) do not hold. In such cases, we use <strong>Fisher’s Exact Test</strong> to evaluate:</p>
<ul>
<li>Null Hypothesis (<span class="math inline">\(H_0\)</span>): <span class="math inline">\(p_1 = p_2\)</span> (no association between variables),</li>
<li>Alternative Hypothesis (<span class="math inline">\(H_a\)</span>): <span class="math inline">\(p_1 \neq p_2\)</span> (an association exists).</li>
</ul>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent <strong>Binomial random variables</strong>:
<ul>
<li>
<span class="math inline">\(X_1 \sim \text{Binomial}(n_1, p_1)\)</span>,</li>
<li>
<span class="math inline">\(X_2 \sim \text{Binomial}(n_2, p_2)\)</span>.</li>
</ul>
</li>
<li>
<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are the observed values (successes in each sample).</li>
<li>Total sample size is <span class="math inline">\(n = n_1 + n_2\)</span>.</li>
<li>Total successes are <span class="math inline">\(m = x_1 + x_2\)</span>.</li>
</ol>
<p>By conditioning on <span class="math inline">\(m\)</span>, the total number of successes, the number of successes in sample 1 follows a <strong>Hypergeometric distribution</strong>.</p>
<hr>
<p><strong>Test Statistic</strong></p>
<p>To test <span class="math inline">\(H_0: p_1 = p_2\)</span> against <span class="math inline">\(H_a: p_1 \neq p_2\)</span>, we use the test statistic:</p>
<p><span class="math display">\[
Z^2 = \left( \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\hat{p}(1 - \hat{p}) \left( \frac{1}{n_1} + \frac{1}{n_2} \right)}} \right)^2 \sim \chi^2_{1, \alpha}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\hat{p}_1\)</span> and <span class="math inline">\(\hat{p}_2\)</span> are the observed proportions of successes in samples 1 and 2,</p></li>
<li><p><span class="math inline">\(\hat{p}\)</span> is the pooled proportion: <span class="math display">\[
  \hat{p} = \frac{x_1 + x_2}{n_1 + n_2},
  \]</span></p></li>
<li><p><span class="math inline">\(\chi^2_{1, \alpha}\)</span> is the upper <span class="math inline">\(\alpha\)</span> critical value of the <strong>Chi-squared distribution</strong> with 1 degree of freedom.</p></li>
</ul>
<p>Fisher’s Exact Test can be extended to a <strong>contingency table</strong> setting to test whether the observed frequencies differ significantly from the expected frequencies under the null hypothesis of no association.</p>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create a 2x2 contingency table</span></span>
<span><span class="va">data_table</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">8</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">5</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">data_table</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Success"</span>, <span class="st">"Failure"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">data_table</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Group 1"</span>, <span class="st">"Group 2"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the table</span></span>
<span><span class="va">data_table</span></span>
<span><span class="co">#&gt;         Success Failure</span></span>
<span><span class="co">#&gt; Group 1       8       2</span></span>
<span><span class="co">#&gt; Group 2       1       5</span></span>
<span></span>
<span><span class="co"># Perform Fisher's Exact Test</span></span>
<span><span class="va">fisher_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/fisher.test.html">fisher.test</a></span><span class="op">(</span><span class="va">data_table</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the results</span></span>
<span><span class="va">fisher_result</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Fisher's Exact Test for Count Data</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data_table</span></span>
<span><span class="co">#&gt; p-value = 0.03497</span></span>
<span><span class="co">#&gt; alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;     1.008849 1049.791446</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; odds ratio </span></span>
<span><span class="co">#&gt;   15.46969</span></span></code></pre></div>
<p>The output of <code><a href="https://rdrr.io/r/stats/fisher.test.html">fisher.test()</a></code> includes:</p>
<ul>
<li><p><strong>p-value</strong>: The probability of observing such a contingency table under the null hypothesis.</p></li>
<li><p><strong>Alternative Hypothesis</strong>: Indicates whether the test is two-sided or one-sided.</p></li>
<li><p>If the p-value is less than <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span> and conclude that there is a significant association between the two variables.</p></li>
</ul>
</div>
<div id="exact-chi-square-test" class="section level5" number="4.5.1.1.2">
<h5>
<span class="header-section-number">4.5.1.1.2</span> Exact Chi-Square Test<a class="anchor" aria-label="anchor" href="#exact-chi-square-test"><i class="fas fa-link"></i></a>
</h5>
<p>For small samples where the normal approximation does not apply, we can compute the <strong>exact Chi-Square test</strong> by using Fisher’s Exact Test or Monte Carlo simulation methods.</p>
<p>The Chi-Square test statistic in the 2x2 table is:</p>
<p><span class="math inline">\(\chi^2 = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}}\)</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(O_{ij}\)</span>: Observed frequency in cell <span class="math inline">\((i, j)\)</span>,</p></li>
<li><p><span class="math inline">\(E_{ij}\)</span>: Expected frequency under the null hypothesis,</p></li>
<li><p><span class="math inline">\(r\)</span>: Number of rows,</p></li>
<li><p><span class="math inline">\(c\)</span>: Number of columns.</p></li>
</ul>
</div>
</div>
<div id="large-samples" class="section level4" number="4.5.1.2">
<h4>
<span class="header-section-number">4.5.1.2</span> Large Samples<a class="anchor" aria-label="anchor" href="#large-samples"><i class="fas fa-link"></i></a>
</h4>
<div id="pearson-chi-square-test" class="section level5" number="4.5.1.2.1">
<h5>
<span class="header-section-number">4.5.1.2.1</span> Pearson Chi-Square Test<a class="anchor" aria-label="anchor" href="#pearson-chi-square-test"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>Pearson Chi-Square Test</strong> is commonly used to test whether there is an association between two categorical variables. It compares the observed counts in a contingency table to the expected counts under the null hypothesis.</p>
<p>The test statistic is:</p>
<p><span class="math display">\[
\chi^2 = \sum_{\text{all cells}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}}
\]</span></p>
<p>The test is applied in settings where multiple proportions or frequencies are compared across independent surveys or experiments.</p>
<ul>
<li>
<strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The observed data are consistent with the expected values (no association or no deviation from a model).</li>
<li>
<strong>Alternative Hypothesis (</strong><span class="math inline">\(H_a\)</span>): The observed data differ significantly from the expected values.</li>
</ul>
<hr>
<p><strong>Characteristics of the Test</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Validation of Models</strong>:<br>
In some cases, <span class="math inline">\(H_0\)</span> represents the model whose validity is being tested. The goal is not necessarily to reject the model but to check whether the data are consistent with it. Deviations may be due to random chance.</p></li>
<li><p><strong>Strength of Association</strong>:<br>
The Chi-Square Test detects whether an association exists but does <strong>not</strong> measure the strength of the association. For measuring strength, metrics like Cramér’s V or the Phi coefficient should be used.</p></li>
<li>
<p><strong>Effect of Sample Size</strong>:</p>
<ul>
<li>The Chi-Square statistic reflects sample size. If the sample size is doubled (e.g., duplicating observations), the <span class="math inline">\(\chi^2\)</span> statistic will also double, even though the strength of the association remains unchanged.</li>
<li>This sensitivity can sometimes lead to detecting significant results that are not practically meaningful.</li>
</ul>
</li>
<li>
<p><strong>Expected Cell Frequencies</strong>:</p>
<ul>
<li>The test is not appropriate if more than <strong>20% of the cells</strong> in a contingency table have expected frequencies less than 5.</li>
<li>For small sample sizes, <a href="basic-statistical-inference.html#fishers-exact-test">Fisher’s Exact Test</a> or exact p-values should be used instead.</li>
</ul>
</li>
</ol>
<hr>
<ol style="list-style-type: decimal">
<li>
<strong>Test for a Single Proportion</strong><br>
We test whether the observed proportion of successes equals 0.5.</li>
</ol>
<p><span class="math display">\[
H_0: p_J = 0.5 \\
H_a: p_J &lt; 0.5
\]</span></p>
<div class="sourceCode" id="cb128"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Observed data</span></span>
<span><span class="va">july.x</span> <span class="op">&lt;-</span> <span class="fl">480</span></span>
<span><span class="va">july.n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="co"># Test for single proportion</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/prop.test.html">prop.test</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">july.x</span>,</span>
<span>  n <span class="op">=</span> <span class="va">july.n</span>,</span>
<span>  p <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  alternative <span class="op">=</span> <span class="st">"less"</span>,</span>
<span>  correct <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  1-sample proportions test without continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  july.x out of july.n, null probability 0.5</span></span>
<span><span class="co">#&gt; X-squared = 1.6, df = 1, p-value = 0.103</span></span>
<span><span class="co">#&gt; alternative hypothesis: true p is less than 0.5</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.0000000 0.5060055</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt;    p </span></span>
<span><span class="co">#&gt; 0.48</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Test for Equality of Proportions Between Two Groups</strong>: We test whether the proportions of successes in July and September are equal.</li>
</ol>
<p><span class="math display">\[
H_0: p_J = p_S \\
H_a: p_j \neq p_S
\]</span></p>
<div class="sourceCode" id="cb129"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Observed data for two groups</span></span>
<span><span class="va">sept.x</span> <span class="op">&lt;-</span> <span class="fl">704</span></span>
<span><span class="va">sept.n</span> <span class="op">&lt;-</span> <span class="fl">1600</span></span>
<span><span class="co"># Test for equality of proportions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/prop.test.html">prop.test</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">july.x</span>, <span class="va">sept.x</span><span class="op">)</span>,</span>
<span>  n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">july.n</span>, <span class="va">sept.n</span><span class="op">)</span>,</span>
<span>  correct <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  2-sample test for equality of proportions without continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  c(july.x, sept.x) out of c(july.n, sept.n)</span></span>
<span><span class="co">#&gt; X-squared = 3.9701, df = 1, p-value = 0.04632</span></span>
<span><span class="co">#&gt; alternative hypothesis: two.sided</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.0006247187 0.0793752813</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; prop 1 prop 2 </span></span>
<span><span class="co">#&gt;   0.48   0.44</span></span></code></pre></div>
<hr>
<p><strong>Comparison of Proportions for Multiple Groups</strong></p>
<div class="inline-table"><table style="width:96%;" class="table table-sm">
<colgroup>
<col width="26%">
<col width="18%">
<col width="18%">
<col width="15%">
<col width="18%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Experiment 1</th>
<th>Experiment 2</th>
<th>…</th>
<th>Experiment k</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Number of successes</td>
<td><span class="math inline">\(x_1\)</span></td>
<td><span class="math inline">\(x_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(x_k\)</span></td>
</tr>
<tr class="even">
<td>Number of failures</td>
<td><span class="math inline">\(n_1 - x_1\)</span></td>
<td><span class="math inline">\(n_2 - x_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k - x_k\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(n_1\)</span></td>
<td><span class="math inline">\(n_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k\)</span></td>
</tr>
</tbody>
</table></div>
<p>We test the null hypothesis:</p>
<p><span class="math display">\[
H_0: p_1 = p_2 = \dots = p_k
\]</span></p>
<p>against the alternative that at least one proportion differs.</p>
<hr>
<p><strong>Pooled Proportion</strong></p>
<p>Assuming <span class="math inline">\(H_0\)</span> is true, we estimate the common value of the probability of success as:</p>
<p><span class="math display">\[
\hat{p} = \frac{x_1 + x_2 + \dots + x_k}{n_1 + n_2 + \dots + n_k}.
\]</span></p>
<p>The <strong>expected counts</strong> under <span class="math inline">\(H_0\)</span> are:</p>
<div class="inline-table"><table style="width:97%;" class="table table-sm">
<colgroup>
<col width="15%">
<col width="22%">
<col width="22%">
<col width="15%">
<col width="22%">
</colgroup>
<thead><tr class="header">
<th>Success</th>
<th><span class="math inline">\(n_1 \hat{p}\)</span></th>
<th><span class="math inline">\(n_2 \hat{p}\)</span></th>
<th>…</th>
<th><span class="math inline">\(n_k \hat{p}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Failure</td>
<td><span class="math inline">\(n_1(1-\hat{p})\)</span></td>
<td><span class="math inline">\(n_2(1-\hat{p})\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k(1-\hat{p})\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(n_1\)</span></td>
<td><span class="math inline">\(n_2\)</span></td>
<td></td>
<td><span class="math inline">\(n_k\)</span></td>
</tr>
</tbody>
</table></div>
<p>The test statistic is:</p>
<p><span class="math display">\[
\chi^2 = \sum_{\text{all cells}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}}
\]</span></p>
<p>with <span class="math inline">\(k - 1\)</span> degrees of freedom.</p>
<hr>
<p><strong>Two-Way Contingency Tables</strong></p>
<p>When categorical data are cross-classified, we create a two-way table of observed counts.</p>
<div class="inline-table"><table style="width:94%;" class="table table-sm">
<colgroup>
<col width="16%">
<col width="11%">
<col width="11%">
<col width="8%">
<col width="11%">
<col width="8%">
<col width="11%">
<col width="12%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>1</th>
<th>2</th>
<th>…</th>
<th>j</th>
<th>…</th>
<th>c</th>
<th>Row Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{1j}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{1c}\)</span></td>
<td><span class="math inline">\(n_{1.}\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{2j}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{2c}\)</span></td>
<td><span class="math inline">\(n_{2.}\)</span></td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>r</td>
<td><span class="math inline">\(n_{r1}\)</span></td>
<td><span class="math inline">\(n_{r2}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{rj}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{rc}\)</span></td>
<td><span class="math inline">\(n_{r.}\)</span></td>
</tr>
<tr class="odd">
<td>Column Total</td>
<td><span class="math inline">\(n_{.1}\)</span></td>
<td><span class="math inline">\(n_{.2}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{.j}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{.c}\)</span></td>
<td><span class="math inline">\(n_{..}\)</span></td>
</tr>
</tbody>
</table></div>
<p><strong>Sampling Designs</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Design 1: Total Sample Size Fixed</strong></p>
<ul>
<li><p>A single random sample of size <span class="math inline">\(n\)</span> is drawn from the population.</p></li>
<li><p>Units are cross-classified into <span class="math inline">\(r\)</span> rows and <span class="math inline">\(c\)</span> columns. Both row and column totals are random variables.</p></li>
<li><p>The cell counts <span class="math inline">\(n_{ij}\)</span> follow a <strong>multinomial distribution</strong> with probabilities <span class="math inline">\(p_{ij}\)</span> such that: <span class="math display">\[ \sum_{i=1}^r \sum_{j=1}^c p_{ij} = 1. \]</span></p></li>
<li><p>Let <span class="math inline">\(p_{ij} = P(X = i, Y = j)\)</span> be the joint probability, where <span class="math inline">\(X\)</span> is the row variable and <span class="math inline">\(Y\)</span> is the column variable.</p></li>
<li><p><strong>Null Hypothesis of Independence</strong>: <span class="math display">\[ H_0: p_{ij} = p_{i.} p_{.j}, \quad \text{where } p_{i.} = P(X = i) \text{ and } p_{.j} = P(Y = j). \]</span></p></li>
<li><p><strong>Alternative Hypothesis</strong>: <span class="math display">\[ H_a: p_{ij} \neq p_{i.} p_{.j}. \]</span></p></li>
</ul>
</li>
<li>
<p><strong>Design 2: Row Totals Fixed</strong></p>
<ul>
<li><p>Random samples of sizes <span class="math inline">\(n_1, n_2, \dots, n_r\)</span> are drawn independently from <span class="math inline">\(r\)</span> row populations.</p></li>
<li><p>The row totals <span class="math inline">\(n_{i.}\)</span> are fixed, but column totals are random.</p></li>
<li><p>Counts in each row follow independent <strong>multinomial distributions</strong>.</p></li>
<li><p>The null hypothesis assumes that the conditional probabilities of the column variable <span class="math inline">\(Y\)</span> are the same across all rows: <span class="math display">\[ H_0: p_{ij} = P(Y = j | X = i) = p_j \quad \text{for all } i \text{ and } j. \]</span></p></li>
<li><p>Alternatively: <span class="math display">\[ H_0: (p_{i1}, p_{i2}, \dots, p_{ic}) = (p_1, p_2, \dots, p_c) \quad \text{for all } i. \]</span></p></li>
<li><p><strong>Alternative Hypothesis</strong>: <span class="math display">\[ H_a: (p_{i1}, p_{i2}, \dots, p_{ic}) \text{ are not the same for all } i. \]</span></p></li>
</ul>
</li>
</ol>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="8%">
<col width="45%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th><strong>Design</strong></th>
<th><strong>Total Sample Size Fixed</strong></th>
<th><strong>Row Totals Fixed</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Scenario</strong></td>
<td>A single dataset or experiment where all observations are collected together as one sample.</td>
<td>Observations are collected separately for each row, with fixed totals for each row population.</td>
</tr>
<tr class="even">
<td><strong>Example</strong></td>
<td>Survey with 100 respondents randomly selected, recording responses based on two categorical variables (e.g., age group and gender).</td>
<td>Stratified survey with specific numbers of individuals sampled from predefined groups (e.g., 30 males, 40 females, 30 non-binary).</td>
</tr>
<tr class="odd">
<td><strong>Why This Design?</strong></td>
<td>- Models situations where the total number of observations is fixed.<br>
- Both row and column categories emerge randomly.<br>
- Tests for <strong>independence</strong> between two categorical variables (row and column).</td>
<td>- Models scenarios where sampling occurs independently within predefined strata or groups.<br>
- Tests for <strong>homogeneity</strong> of column proportions across rows, ignoring differences in total counts between rows.</td>
</tr>
<tr class="even">
<td><strong>Practical Use Case</strong></td>
<td>- <strong>Market Research</strong>: Do customer demographics (rows) and purchase behavior (columns) show a dependence?<br>
- <strong>Biology</strong>: Is there an association between species (rows) and habitat types (columns)?</td>
<td>- <strong>Public Health</strong>: Are smoking rates (columns) consistent across age groups (rows)?<br>
- <strong>Education</strong>: Do pass rates (columns) differ across schools (rows), controlling for the number of students in each school?</td>
</tr>
</tbody>
</table></div>
<p><strong>Why Both Designs?</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Real-World Sampling Constraints</strong>:</p>
<ul>
<li><p>Sometimes, you have control over row totals (e.g., fixed group sizes in stratified sampling).</p></li>
<li><p>Other times, you collect data without predefined group sizes, and totals emerge randomly.</p></li>
</ul>
</li>
<li>
<p><strong>Different Null Hypotheses</strong>:</p>
<ul>
<li><p>Design 1 tests whether two variables are <strong>independent</strong> (e.g., does one variable predict the other?).</p></li>
<li><p>Design 2 tests whether column proportions are <strong>homogeneous</strong> across groups (e.g., are the groups similar?).</p></li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb130"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sampling Design 1: Total Sample Size Fixed</span></span>
<span><span class="co"># Parameters for the multinomial distribution</span></span>
<span><span class="va">r</span> <span class="op">&lt;-</span> <span class="fl">3</span>  <span class="co"># Number of rows</span></span>
<span><span class="va">c</span> <span class="op">&lt;-</span> <span class="fl">4</span>  <span class="co"># Number of columns</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span>  <span class="co"># Total sample size</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>,</span>
<span>              <span class="fl">0.05</span>, <span class="fl">0.15</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>,</span>
<span>              <span class="fl">0.05</span>, <span class="fl">0.05</span>, <span class="fl">0.025</span>, <span class="fl">0.075</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="va">r</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate a single random sample</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># For reproducibility</span></span>
<span><span class="va">n_ij</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Multinom.html">rmultinom</a></span><span class="op">(</span><span class="fl">1</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Reshape into a contingency table</span></span>
<span><span class="va">contingency_table_fixed_total</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">n_ij</span>, nrow <span class="op">=</span> <span class="va">r</span>, ncol <span class="op">=</span> <span class="va">c</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">contingency_table_fixed_total</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"Row"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">r</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">contingency_table_fixed_total</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"Col"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">c</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Hypothesis testing (Chi-squared test of independence)</span></span>
<span><span class="va">chisq_test_fixed_total</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span><span class="va">contingency_table_fixed_total</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"Contingency Table (Total Sample Size Fixed):"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Contingency Table (Total Sample Size Fixed):"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">contingency_table_fixed_total</span><span class="op">)</span></span>
<span><span class="co">#&gt;      Col1 Col2 Col3 Col4</span></span>
<span><span class="co">#&gt; Row1    8    6    4   24</span></span>
<span><span class="co">#&gt; Row2   18    1    9    7</span></span>
<span><span class="co">#&gt; Row3    2    7    5    9</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"Chi-squared Test Results:"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Chi-squared Test Results:"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">chisq_test_fixed_total</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Pearson's Chi-squared test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  contingency_table_fixed_total</span></span>
<span><span class="co">#&gt; X-squared = 28.271, df = 6, p-value = 8.355e-05</span></span></code></pre></div>
<ul>
<li><p>All counts in the contingency table come from a single multinomial sample where both row and column totals are random.</p></li>
<li><p><strong>Conclusion</strong>: Reject Null​. The data suggests significant dependence between row and column variables.</p></li>
</ul>
<div class="sourceCode" id="cb131"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sampling Design 2: Row Totals Fixed</span></span>
<span><span class="co"># Parameters for the fixed row totals</span></span>
<span><span class="va">n_row</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">40</span>, <span class="fl">30</span><span class="op">)</span>  <span class="co"># Row totals</span></span>
<span><span class="va">c</span> <span class="op">&lt;-</span> <span class="fl">4</span>  <span class="co"># Number of columns</span></span>
<span><span class="va">p_col</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span><span class="op">)</span>  <span class="co"># Common column probabilities under H0</span></span>
<span></span>
<span><span class="co"># Generate independent multinomial samples for each row</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># For reproducibility</span></span>
<span><span class="va">row_samples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">n_row</span>, <span class="kw">function</span><span class="op">(</span><span class="va">size</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Multinom.html">rmultinom</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">size</span>, prob <span class="op">=</span> <span class="va">p_col</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Combine into a contingency table</span></span>
<span><span class="va">contingency_table_fixed_rows</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html">do.call</a></span><span class="op">(</span><span class="va">rbind</span>, <span class="va">row_samples</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">contingency_table_fixed_rows</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"Row"</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">n_row</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">contingency_table_fixed_rows</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"Col"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">c</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Hypothesis testing (Chi-squared test of homogeneity)</span></span>
<span><span class="va">chisq_test_fixed_rows</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span><span class="va">contingency_table_fixed_rows</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"Contingency Table (Row Totals Fixed):"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Contingency Table (Row Totals Fixed):"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">contingency_table_fixed_rows</span><span class="op">)</span></span>
<span><span class="co">#&gt;      Col1 Col2 Col3 Col4</span></span>
<span><span class="co">#&gt; Row1    6   10    7    7</span></span>
<span><span class="co">#&gt; Row2   13   13    4   10</span></span>
<span><span class="co">#&gt; Row3    8   10    6    6</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"Chi-squared Test Results:"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Chi-squared Test Results:"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">chisq_test_fixed_rows</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Pearson's Chi-squared test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  contingency_table_fixed_rows</span></span>
<span><span class="co">#&gt; X-squared = 3.2069, df = 6, p-value = 0.7825</span></span></code></pre></div>
<ul>
<li><p>Row totals are fixed, and column counts within each row follow independent multinomial distributions.</p></li>
<li><p><strong>Conclusion</strong>: Fail to reject the null. The data does not provide evidence to suggest differences in column probabilities across rows.</p></li>
</ul>
<p><strong>Why Are the Results Different?</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Data Generation Differences</strong>:</p>
<ul>
<li><p>In <strong>Design 1</strong>, the entire table is treated as a single multinomial sample. This introduces dependencies between counts in the table.</p></li>
<li><p>In <strong>Design 2</strong>, rows are generated independently, and only the column probabilities are tested for consistency across rows.</p></li>
</ul>
</li>
<li>
<p><strong>Null Hypotheses</strong>:</p>
<ul>
<li><p><strong>Design 1</strong> tests independence between row and column variables (more restrictive).</p></li>
<li><p><strong>Design 2</strong> tests homogeneity of column probabilities across rows (less restrictive).</p></li>
</ul>
</li>
</ol>
<p><strong>Interpretation</strong></p>
<ul>
<li>
<p>The results are <strong>not directly comparable</strong> because the null hypotheses are different:</p>
<ul>
<li><p><strong>Design 1</strong> focuses on whether rows and columns are independent across the entire table.</p></li>
<li><p><strong>Design 2</strong> focuses on whether column distributions are consistent across rows.</p></li>
</ul>
</li>
<li>
<p><strong>Real-World Implication</strong>:</p>
<ul>
<li><p>If you are testing for independence (e.g., whether two variables are unrelated), use Design 1.</p></li>
<li><p>If you are testing for consistency across groups (e.g., whether proportions are the same across categories), use Design 2.</p></li>
</ul>
</li>
</ul>
<p><strong>Takeaways</strong></p>
<ul>
<li><p>The tests use the same statistical machinery (Chi-squared test), but their interpretations differ based on the experimental design and null hypothesis.</p></li>
<li><p>For the same dataset, differences in assumptions can lead to different conclusions.</p></li>
</ul>
<hr>
</div>
<div id="chi-square-test-for-independence" class="section level5" number="4.5.1.2.2">
<h5>
<span class="header-section-number">4.5.1.2.2</span> Chi-Square Test for Independence<a class="anchor" aria-label="anchor" href="#chi-square-test-for-independence"><i class="fas fa-link"></i></a>
</h5>
<p>The expected frequencies <span class="math inline">\(\hat{e}_{ij}\)</span> under the null hypothesis are:</p>
<p><span class="math display">\[
\hat{e}_{ij} = \frac{n_{i.} n_{.j}}{n_{..}},
\]</span></p>
<p>where <span class="math inline">\(n_{i.}\)</span> and <span class="math inline">\(n_{.j}\)</span> are the row and column totals, respectively, and <span class="math inline">\(n_{..}\)</span> is the total sample size.</p>
<p>The test statistic is:</p>
<p><span class="math display">\[
\chi^2 = \sum_{i=1}^r \sum_{j=1}^c \frac{(n_{ij} - \hat{e}_{ij})^2}{\hat{e}_{ij}} \sim \chi^2_{(r-1)(c-1)}.
\]</span></p>
<p>We reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if:</p>
<p><span class="math display">\[
\chi^2 &gt; \chi^2_{(r-1)(c-1), \alpha}.
\]</span></p>
<p>Notes on the Pearson Chi-Square Test</p>
<ul>
<li>
<strong>Purpose</strong>: Test for association or independence between two categorical variables.</li>
<li>
<strong>Sensitivity to Sample Size</strong>: The <span class="math inline">\(\chi^2\)</span> statistic is proportional to sample size. Doubling the sample size doubles <span class="math inline">\(\chi^2\)</span> even if the strength of the association remains unchanged.</li>
<li>
<strong>Assumption on Expected Frequencies</strong>: The test is not valid when more than 20% of the expected cell counts are less than 5. In such cases, exact tests are preferred.</li>
</ul>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create a contingency table</span></span>
<span><span class="va">data_table</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">10</span>, <span class="fl">20</span>, <span class="fl">40</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">data_table</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Category 1"</span>, <span class="st">"Category 2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">data_table</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Group 1"</span>, <span class="st">"Group 2"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the table</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">data_table</span><span class="op">)</span></span>
<span><span class="co">#&gt;         Category 1 Category 2</span></span>
<span><span class="co">#&gt; Group 1         30         10</span></span>
<span><span class="co">#&gt; Group 2         20         40</span></span>
<span></span>
<span><span class="co"># Perform Chi-Square Test</span></span>
<span><span class="va">chi_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span><span class="va">data_table</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">chi_result</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Pearson's Chi-squared test with Yates' continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data_table</span></span>
<span><span class="co">#&gt; X-squared = 15.042, df = 1, p-value = 0.0001052</span></span></code></pre></div>
<p>The output includes:</p>
<ul>
<li><p>Chi-Square Statistic (<span class="math inline">\(\chi^2\)</span>): The test statistic measuring the deviation between observed and expected counts.</p></li>
<li><p><strong>p-value</strong>: The probability of observing such a deviation under <span class="math inline">\(H_0\)</span>.</p></li>
<li><p><strong>Degrees of Freedom</strong>: <span class="math inline">\((r-1)(c-1)\)</span> for an <span class="math inline">\(r \times c\)</span> table.</p></li>
<li><p><strong>Expected Frequencies</strong>: The table of expected counts under <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>If the p-value is less than <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span> and conclude that there is a significant association between the row and column variables.</p></li>
</ul>
</div>
</div>
<div id="key-takeaways" class="section level4" number="4.5.1.3">
<h4>
<span class="header-section-number">4.5.1.3</span> Key Takeaways<a class="anchor" aria-label="anchor" href="#key-takeaways"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="13%">
<col width="28%">
<col width="19%">
<col width="12%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th><strong>Test</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>Key Features</strong></th>
<th><strong>Sample Size Suitability</strong></th>
<th><strong>Statistical Assumptions</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Fisher’s Exact Test</strong></td>
<td>Tests association between two categorical variables in a <strong>2x2 table</strong>.</td>
<td>- Computes exact p-values.<br>
- Does not rely on asymptotic assumptions.<br>
- Handles small sample sizes.</td>
<td><strong>Small sample sizes</strong></td>
<td>- Observations are independent.<br>
- Fixed marginal totals.<br>
- No normality assumption.</td>
</tr>
<tr class="even">
<td><strong>Exact Chi-Square Test</strong></td>
<td>Tests association in larger contingency tables using exact methods.</td>
<td>- Generalization of Fisher’s Exact Test.<br>
- Avoids asymptotic assumptions.<br>
- Suitable for small to medium datasets.</td>
<td><strong>Small to medium sample sizes</strong></td>
<td>- Observations are independent.<br>
- Marginal totals may not be fixed.<br>
- No normality assumption.</td>
</tr>
<tr class="odd">
<td><strong>Pearson Chi-Square Test</strong></td>
<td>Tests discrepancies between observed and expected frequencies.</td>
<td>- Most common chi-square-based test.<br>
- Includes independence and goodness-of-fit tests.<br>
- Relies on asymptotic assumptions.</td>
<td><strong>Large sample sizes</strong></td>
<td>- Observations are independent.<br>
- Expected cell frequencies ≥ 5.<br>
- Test statistic follows a chi-square distribution asymptotically.</td>
</tr>
<tr class="even">
<td><strong>Chi-Square Test for Independence</strong></td>
<td>Tests independence between two categorical variables in a contingency table.</td>
<td>- Application of Pearson Chi-Square Test.<br>
- Same assumptions as asymptotic chi-square tests.<br>
- Often used for larger contingency tables.</td>
<td><strong>Medium to large sample sizes</strong></td>
<td>- Observations are independent.<br>
- Expected cell frequencies ≥ 5.<br>
- Random sampling.</td>
</tr>
</tbody>
</table></div>
<ol style="list-style-type: decimal">
<li><p><strong>Fisher’s Exact Test</strong> is specialized for small samples and fixed margins (2x2 tables).</p></li>
<li><p><strong>Exact Chi-Square Test</strong> is a broader version of Fisher’s for larger tables but avoids asymptotic approximations.</p></li>
<li>
<p><strong>Pearson Chi-Square Test</strong> is the general framework, and its applications include:</p>
<ul>
<li><p>Goodness-of-fit testing.</p></li>
<li><p>Testing independence (same as the Chi-Square Test for Independence).</p></li>
</ul>
</li>
<li><p><strong>Chi-Square Test for Independence</strong> is a specific application of the Pearson Chi-Square Test.</p></li>
</ol>
<p>In essence:</p>
<ul>
<li><p><strong>Fisher’s Exact Test</strong> and <strong>Exact Chi-Square Test</strong> are precise methods for small datasets.</p></li>
<li><p><strong>Pearson Chi-Square Test</strong> and <strong>Chi-Square Test for Independence</strong> are interchangeable terms in many contexts, focusing on larger datasets.</p></li>
</ul>
<hr>
</div>
</div>
<div id="ordinal-association" class="section level3" number="4.5.2">
<h3>
<span class="header-section-number">4.5.2</span> Ordinal Association<a class="anchor" aria-label="anchor" href="#ordinal-association"><i class="fas fa-link"></i></a>
</h3>
<p>Ordinal association refers to a relationship between two variables where the levels of one variable exhibit a consistent pattern of increase or decrease in response to the levels of the other variable. This type of association is particularly relevant when dealing with ordinal variables, which have naturally ordered categories, such as ratings (“poor”, “fair”, “good”, “excellent”) or income brackets (“low”, “medium”, “high”).</p>
<p>For example:</p>
<ul>
<li><p>As customer satisfaction ratings increase from “poor” to “excellent,” the likelihood of recommending a product may also increase (positive ordinal association).</p></li>
<li><p>Alternatively, as stress levels move from “low” to “high,” job performance may tend to decrease (negative ordinal association).</p></li>
</ul>
<p><strong>Key Characteristics of Ordinal Association</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Logical Ordering of Levels</strong>: The levels of both variables must follow a logical sequence. For instance, “small,” “medium,” and “large” are logically ordered, whereas categories like “blue,” “round,” and “tall” lack inherent order and are unsuitable for ordinal association.</p></li>
<li><p><strong>Monotonic Trends</strong>: The association is typically monotonic, meaning that as one variable moves in a specific direction, the other variable tends to move in a consistent direction (either increasing or decreasing).</p></li>
<li><p><strong>Tests for Ordinal Association</strong>: Specialized statistical tests assess ordinal association, focusing on how the rankings of one variable relate to those of the other. These tests require the data to respect the ordinal structure of both variables.</p></li>
</ol>
<p><strong>Practical Considerations</strong></p>
<p>When using these tests, keep in mind:</p>
<ul>
<li><p><strong>Ordinal Data Handling</strong>: Ensure that the data respects the ordinal structure (e.g., categories are correctly ranked and coded).</p></li>
<li><p><strong>Sample Size</strong>: Larger sample sizes provide more reliable estimates and stronger test power.</p></li>
<li><p><strong>Contextual Relevance</strong>: Interpret results within the context of the data and the research question. For example, a significant Spearman’s correlation does not imply causation but rather a consistent trend.</p></li>
</ul>
<div id="mantel-haenszel-chi-square-test" class="section level4" number="4.5.2.1">
<h4>
<span class="header-section-number">4.5.2.1</span> Mantel-Haenszel Chi-square Test<a class="anchor" aria-label="anchor" href="#mantel-haenszel-chi-square-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Mantel-Haenszel Chi-square Test is a statistical tool for evaluating ordinal associations, particularly when the data consists of multiple <span class="math inline">\(2 \times 2\)</span> contingency tables that examine the same association under varying conditions or strata. Unlike measures of association such as correlation coefficients, this test does not quantify the strength of the association but rather evaluates whether an association exists after controlling for stratification.</p>
<p>The Mantel-Haenszel Test is applicable to <span class="math inline">\(2 \times 2 \times K\)</span> contingency tables, where <span class="math inline">\(K\)</span> represents the number of strata. Each stratum is a <span class="math inline">\(2 \times 2\)</span> table corresponding to different conditions or subgroups.</p>
<p>For each stratum <span class="math inline">\(k\)</span>, let the marginal totals of the table be:</p>
<ul>
<li><p><span class="math inline">\(n_{.1k}\)</span>: Total observations in column 1</p></li>
<li><p><span class="math inline">\(n_{.2k}\)</span>: Total observations in column 2</p></li>
<li><p><span class="math inline">\(n_{1.k}\)</span>: Total observations in row 1</p></li>
<li><p><span class="math inline">\(n_{2.k}\)</span>: Total observations in row 2</p></li>
<li><p><span class="math inline">\(n_{..k}\)</span>: Total observations in the entire table</p></li>
</ul>
<p>The observed cell count in row 1 and column 1 is denoted <span class="math inline">\(n_{11k}\)</span>. Given the marginal totals, the sampling distribution of <span class="math inline">\(n_{11k}\)</span> follows a hypergeometric distribution.</p>
<p>Under the assumption of conditional independence:</p>
<p>The expected value of <span class="math inline">\(n_{11k}\)</span> is: <span class="math display">\[
  m_{11k} = E(n_{11k}) = \frac{n_{1.k} n_{.1k}}{n_{..k}}
  \]</span> The variance of <span class="math inline">\(n_{11k}\)</span> is: <span class="math display">\[
  var(n_{11k}) = \frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2 (n_{..k} - 1)}
  \]</span></p>
<p>Mantel and Haenszel proposed the test statistic:</p>
<p><span class="math display">\[
M^2 = \frac{\left(|\sum_k n_{11k} - \sum_k m_{11k}| - 0.5\right)^2}{\sum_k var(n_{11k})} \sim \chi^2_{1}
\]</span></p>
<p>where</p>
<ul>
<li><p>The 0.5 adjustment, known as a continuity correction, improves the approximation to the <span class="math inline">\(\chi^2\)</span> distribution.</p></li>
<li><p>The test statistic follows a <span class="math inline">\(\chi^2\)</span> distribution with 1 degree of freedom under the null hypothesis of conditional independence.</p></li>
</ul>
<p>This method can be extended to general <span class="math inline">\(I \times J \times K\)</span> contingency tables, where <span class="math inline">\(I\)</span> and <span class="math inline">\(J\)</span> represent the number of rows and columns, respectively, and <span class="math inline">\(K\)</span> is the number of strata.</p>
<hr>
<p><strong>Null Hypothesis</strong> (<span class="math inline">\(H_0\)</span>):</p>
<p>There is no association between the two variables of interest across all strata, after controlling for the confounder.<br>
In mathematical terms:</p>
<p><span class="math display">\[
H_0: \text{Odds Ratio (OR)} = 1 \; \text{or} \; \text{Risk Ratio (RR)} = 1
\]</span></p>
<p><strong>Alternative Hypothesis</strong> (<span class="math inline">\(H_a\)</span>):</p>
<p>There is an association between the two variables of interest across all strata, after controlling for the confounder.<br>
In mathematical terms:</p>
<p><span class="math display">\[
H_a: \text{Odds Ratio (OR)} \neq 1 \; \text{or} \; \text{Risk Ratio (RR)} \neq 1
\]</span></p>
<hr>
<p>Let’s consider a scenario where a business wants to evaluate the relationship between customer satisfaction (Satisfied vs. Not Satisfied) and the likelihood of repeat purchases (Yes vs. No) across different regions (e.g., North, South, and West). The goal is to determine whether this relationship holds consistently across the regions.</p>
<div class="sourceCode" id="cb133"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create a 2 x 2 x 3 contingency table</span></span>
<span><span class="va">CustomerData</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">40</span>, <span class="fl">30</span>, <span class="fl">200</span>, <span class="fl">300</span>, <span class="fl">35</span>, <span class="fl">20</span>, <span class="fl">180</span>, <span class="fl">265</span>, <span class="fl">50</span>, <span class="fl">25</span>, <span class="fl">250</span>, <span class="fl">275</span><span class="op">)</span>,</span>
<span>    dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>    dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>        Satisfaction <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Satisfied"</span>, <span class="st">"Not Satisfied"</span><span class="op">)</span>,</span>
<span>        RepeatPurchase <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span>,</span>
<span>        Region <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"North"</span>, <span class="st">"South"</span>, <span class="st">"West"</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># View marginal table (summarized across regions)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/marginSums.html">margin.table</a></span><span class="op">(</span><span class="va">CustomerData</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;                RepeatPurchase</span></span>
<span><span class="co">#&gt; Satisfaction    Yes  No</span></span>
<span><span class="co">#&gt;   Satisfied     125 630</span></span>
<span><span class="co">#&gt;   Not Satisfied  75 840</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li>Calculate the overall odds ratio (ignoring strata):</li>
</ol>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/pegeler/samplesizeCMH">samplesizeCMH</a></span><span class="op">)</span></span>
<span><span class="va">marginal_table</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/marginSums.html">margin.table</a></span><span class="op">(</span><span class="va">CustomerData</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/samplesizeCMH/man/odds.ratio.html">odds.ratio</a></span><span class="op">(</span><span class="va">marginal_table</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.222222</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Calculate the conditional odds ratios for each region:</li>
</ol>
<div class="sourceCode" id="cb135"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">CustomerData</span>, <span class="fl">3</span>, <span class="va">odds.ratio</span><span class="op">)</span></span>
<span><span class="co">#&gt;    North    South     West </span></span>
<span><span class="co">#&gt; 2.000000 2.576389 2.200000</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>The Mantel-Haenszel Test evaluates whether the relationship between customer satisfaction and repeat purchases remains consistent across regions:</li>
</ol>
<div class="sourceCode" id="cb136"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/mantelhaen.test.html">mantelhaen.test</a></span><span class="op">(</span><span class="va">CustomerData</span>, correct <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Mantel-Haenszel chi-squared test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  CustomerData</span></span>
<span><span class="co">#&gt; Mantel-Haenszel X-squared = 26.412, df = 1, p-value = 2.758e-07</span></span>
<span><span class="co">#&gt; alternative hypothesis: true common odds ratio is not equal to 1</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  1.637116 3.014452</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; common odds ratio </span></span>
<span><span class="co">#&gt;          2.221488</span></span></code></pre></div>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li>
<strong>Overall Odds Ratio</strong>: This provides an estimate of the overall association between satisfaction and repeat purchases, ignoring regional differences.</li>
<li>
<strong>Conditional Odds Ratios</strong>: These show whether the odds of repeat purchases given satisfaction are similar across regions.</li>
<li>
<strong>Mantel-Haenszel Test</strong>: A significant test result (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) suggests that the relationship between satisfaction and repeat purchases is consistent across regions. Conversely, a non-significant result implies that regional differences may affect the association. By applying the Mantel-Haenszel Test, businesses can determine if a marketing or customer retention strategy should be uniformly applied or customized to account for regional variations.
<ol style="list-style-type: decimal">
<li>There is strong evidence to suggest that the two variables of interest are associated across the strata (North, South, and West), even after accounting for potential confounding effects of stratification.</li>
<li>The common odds ratio of approximately <span class="math inline">\(2.22\)</span> indicates a substantial association, meaning that the outcome is more likely in the exposed group compared to the unexposed group.</li>
<li>The variability in the stratum-specific odds ratios suggests that the strength of the association may differ slightly by region, but the Mantel-Haenszel test assumes the association is consistent (homogeneous).</li>
</ol>
</li>
</ol>
</div>
<div id="mcnemars-test" class="section level4" number="4.5.2.2">
<h4>
<span class="header-section-number">4.5.2.2</span> McNemar’s Test<a class="anchor" aria-label="anchor" href="#mcnemars-test"><i class="fas fa-link"></i></a>
</h4>
<p>McNemar’s Test is a special case of the <a href="basic-statistical-inference.html#mantel-haenszel-chi-square-test">Mantel-Haenszel Chi-square Test</a>, designed for paired nominal data. It is particularly useful for evaluating changes in categorical responses before and after a treatment or intervention, or for comparing paired responses in matched samples. Unlike the Mantel-Haenszel Test, which handles stratified data, McNemar’s Test is tailored to situations with a single <span class="math inline">\(2 \times 2\)</span> table derived from paired observations.</p>
<p>McNemar’s Test assesses whether the proportions of discordant pairs (off-diagonal elements in a <span class="math inline">\(2 \times 2\)</span> table) are significantly different. Specifically, it tests the null hypothesis that the probabilities of transitioning from one category to another are equal.</p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math display">\[
P(\text{Switch from A to B}) = P(\text{Switch from B to A})
\]</span> This implies that the probabilities of transitioning from one category to the other are equal, or equivalently, the off-diagonal cell counts (<span class="math inline">\(n_{12}\)</span> and <span class="math inline">\(n_{21}\)</span>) are symmetric: <span class="math display">\[
H_0: n_{12} = n_{21}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): <span class="math display">\[
P(\text{Switch from A to B}) \neq P(\text{Switch from B to A})
\]</span> This suggests that the probabilities of transitioning between categories are not equal, or equivalently, the off-diagonal cell counts (<span class="math inline">\(n_{12}\)</span> and <span class="math inline">\(n_{21}\)</span>) are asymmetric: <span class="math display">\[
H_A: n_{12} \neq n_{21}
\]</span></p></li>
</ul>
<p>For example, consider a business analyzing whether a new advertising campaign influences customer preference for two products (A and B). Each customer is surveyed before and after the campaign, resulting in the following <span class="math inline">\(2 \times 2\)</span> contingency table:</p>
<ul>
<li>
<strong>Before rows</strong>: Preference for Product A or B before the campaign.</li>
<li>
<strong>After columns</strong>: Preference for Product A or B after the campaign.</li>
</ul>
<p>Let the table structure be:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>After A</th>
<th>After B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Before A</strong></td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
</tr>
<tr class="even">
<td><strong>Before B</strong></td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>
<span class="math inline">\(n_{12}\)</span>: Customers who switched from Product A to B.</li>
<li>
<span class="math inline">\(n_{21}\)</span>: Customers who switched from Product B to A.</li>
</ul>
<p>The test focuses on <span class="math inline">\(n_{12}\)</span> and <span class="math inline">\(n_{21}\)</span>, as they represent the discordant pairs.</p>
<p>The McNemar’s Test statistic is: <span class="math display">\[
M^2 = \frac{(|n_{12} - n_{21}| - 0.5)^2}{n_{12} + n_{21}}
\]</span> where</p>
<ul>
<li><p>The 0.5 is a continuity correction applied when sample sizes are small.</p></li>
<li><p>Under the null hypothesis of no preference change, <span class="math inline">\(M^2\)</span> follows a <span class="math inline">\(\chi^2\)</span> distribution with 1 degree of freedom.</p></li>
</ul>
<hr>
<p>Let’s analyze a voting behavior study where participants were surveyed before and after a campaign. The table represents:</p>
<ul>
<li><p>Rows: Voting preference before the campaign (Yes, No).</p></li>
<li><p>Columns: Voting preference after the campaign (Yes, No).</p></li>
</ul>
<div class="sourceCode" id="cb137"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Voting preference before and after a campaign</span></span>
<span><span class="va">vote</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">682</span>, <span class="fl">22</span>, <span class="fl">86</span>, <span class="fl">810</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>              dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>                <span class="st">"Before"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span>,</span>
<span>                <span class="st">"After"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span></span>
<span>              <span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform McNemar's Test with continuity correction</span></span>
<span><span class="va">mcnemar_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/mcnemar.test.html">mcnemar.test</a></span><span class="op">(</span><span class="va">vote</span>, correct <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">mcnemar_result</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  McNemar's Chi-squared test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  vote</span></span>
<span><span class="co">#&gt; McNemar's chi-squared = 36.75, df = 1, p-value = 1.343e-09</span></span></code></pre></div>
<p>The test provides:</p>
<ul>
<li><p><strong>Test statistic (</strong><span class="math inline">\(M^2\)</span><strong>)</strong>: Quantifies the asymmetry in discordant pairs.</p></li>
<li><p><strong>p-value</strong>: Indicates whether there is a significant difference in the discordant proportions.</p></li>
</ul>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li><p><strong>Test Statistic</strong>: A large <span class="math inline">\(M^2\)</span> value suggests significant asymmetry in the discordant pairs.</p></li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>A low p-value (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) rejects the null hypothesis, indicating that the proportion of participants switching preferences (e.g., from Yes to No) is significantly different from those switching in the opposite direction (e.g., from No to Yes).</p></li>
<li><p>A high p-value fails to reject the null hypothesis, suggesting no significant preference change.</p></li>
</ul>
</li>
</ol>
<p>McNemar’s Test is widely used in business and other fields:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Marketing Campaigns</strong>: Evaluating whether a campaign shifts consumer preferences or purchase intentions.</p></li>
<li><p><strong>Product Testing</strong>: Determining if a new feature or redesign changes customer ratings.</p></li>
<li><p><strong>Healthcare Studies</strong>: Analyzing treatment effects in paired medical trials.</p></li>
</ol>
</div>
<div id="mcnemar-bowker-test" class="section level4" number="4.5.2.3">
<h4>
<span class="header-section-number">4.5.2.3</span> McNemar-Bowker Test<a class="anchor" aria-label="anchor" href="#mcnemar-bowker-test"><i class="fas fa-link"></i></a>
</h4>
<p>The McNemar-Bowker Test is an extension of <a href="basic-statistical-inference.html#mcnemars-test">McNemar’s Test</a>, designed for analyzing paired nominal data with more than two categories. It evaluates the symmetry of the full contingency table by comparing the off-diagonal elements across all categories. This test is particularly useful for understanding whether changes between categories are uniformly distributed or whether significant asymmetries exist.</p>
<hr>
<p>Let the data be structured in an <span class="math inline">\(r \times r\)</span> square contingency table, where <span class="math inline">\(r\)</span> is the number of categories, and the off-diagonal elements represent transitions between categories.</p>
<p>The hypotheses for the McNemar-Bowker Test are:</p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math display">\[
P(\text{Switch from Category } i \text{ to Category } j) = P(\text{Switch from Category } j \text{ to Category } i) \quad \forall \, i \neq j
\]</span> This implies that the off-diagonal elements are symmetric, and there is no directional preference in category transitions.</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): <span class="math display">\[
P(\text{Switch from Category } i \text{ to Category } j) \neq P(\text{Switch from Category } j \text{ to Category } i) \quad \text{for at least one pair } (i, j)
\]</span> This suggests that the off-diagonal elements are not symmetric, indicating a directional preference in transitions between at least one pair of categories.</p></li>
</ul>
<hr>
<p>The McNemar-Bowker Test statistic is: <span class="math display">\[
B^2 = \sum_{i &lt; j} \frac{(n_{ij} - n_{ji})^2}{n_{ij} + n_{ji}}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(n_{ij}\)</span>: Observed count of transitions from category <span class="math inline">\(i\)</span> to category <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(n_{ji}\)</span>: Observed count of transitions from category <span class="math inline">\(j\)</span> to category <span class="math inline">\(i\)</span>.</p></li>
</ul>
<p>Under the null hypothesis, the test statistic <span class="math inline">\(B^2\)</span> approximately follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(\frac{r(r-1)}{2}\)</span> degrees of freedom (corresponding to the number of unique pairs of categories).</p>
<hr>
<p>For example, a company surveys customers about their satisfaction before and after implementing a new policy. Satisfaction is rated on a scale of 1 to 3 (1 = Low, 2 = Medium, 3 = High). The paired responses are summarized in the following <span class="math inline">\(3 \times 3\)</span> contingency table.</p>
<div class="sourceCode" id="cb138"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Satisfaction ratings before and after the intervention</span></span>
<span><span class="va">satisfaction_table</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>    <span class="fl">30</span>, <span class="fl">10</span>, <span class="fl">5</span>,  <span class="co"># Before: Low</span></span>
<span>    <span class="fl">8</span>, <span class="fl">50</span>, <span class="fl">12</span>,  <span class="co"># Before: Medium</span></span>
<span>    <span class="fl">6</span>, <span class="fl">10</span>, <span class="fl">40</span>   <span class="co"># Before: High</span></span>
<span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">3</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    <span class="st">"Before"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span>,</span>
<span>    <span class="st">"After"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Function to perform McNemar-Bowker Test</span></span>
<span><span class="va">mcnemar_bowker_test</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">table</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/all.html">all</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/stop.html">stop</a></span><span class="op">(</span><span class="st">"Input must be a square matrix."</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># Extract off-diagonal elements</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span></span>
<span>  <span class="va">stat</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  </span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="op">(</span><span class="va">i</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">nij</span> <span class="op">&lt;-</span> <span class="va">table</span><span class="op">[</span><span class="va">i</span>, <span class="va">j</span><span class="op">]</span></span>
<span>      <span class="va">nji</span> <span class="op">&lt;-</span> <span class="va">table</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span></span>
<span>      <span class="va">stat</span> <span class="op">&lt;-</span> <span class="va">stat</span> <span class="op">+</span> <span class="op">(</span><span class="va">nij</span> <span class="op">-</span> <span class="va">nji</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="va">nij</span> <span class="op">+</span> <span class="va">nji</span><span class="op">)</span></span>
<span>      <span class="va">df</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">stat</span>, df <span class="op">=</span> <span class="va">df</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>statistic <span class="op">=</span> <span class="va">stat</span>, df <span class="op">=</span> <span class="va">df</span>, p_value <span class="op">=</span> <span class="va">p_value</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Run the test</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">mcnemar_bowker_test</span><span class="op">(</span><span class="va">satisfaction_table</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"McNemar-Bowker Test Results:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; McNemar-Bowker Test Results:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Test Statistic (B^2):"</span>, <span class="va">result</span><span class="op">$</span><span class="va">statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Test Statistic (B^2): 0.4949495</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Degrees of Freedom:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">df</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Degrees of Freedom: 3</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p-value:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">p_value</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p-value: 0.9199996</span></span></code></pre></div>
<p>The output includes:</p>
<ul>
<li><p><strong>Test Statistic (</strong><span class="math inline">\(B^2\)</span><strong>)</strong>: A measure of the asymmetry in the off-diagonal elements.</p></li>
<li><p><strong>p-value</strong>: The probability of observing the data under the null hypothesis of symmetry.</p></li>
</ul>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li><p><strong>Test Statistic</strong>: A large $B^2$ value suggests substantial asymmetry in transitions between categories.</p></li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>If the <strong>p-value is less than the significance level</strong> (e.g., $p &lt; 0.05$), we reject the null hypothesis, indicating significant asymmetry in the transitions between at least one pair of categories.</p></li>
<li><p>If the <strong>p-value is greater than the significance level</strong>, we fail to reject the null hypothesis, suggesting that the category transitions are symmetric.</p></li>
</ul>
</li>
</ol>
<p>The McNemar-Bowker Test has broad applications in business and other fields:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Customer Feedback Analysis</strong>: Evaluating changes in customer satisfaction levels before and after interventions.</p></li>
<li><p><strong>Marketing Campaigns</strong>: Assessing shifts in brand preferences across multiple brands in response to an advertisement.</p></li>
<li><p><strong>Product Testing</strong>: Understanding how user preferences among different product features change after a redesign.</p></li>
</ol>
</div>
<div id="stuart-maxwell-test" class="section level4" number="4.5.2.4">
<h4>
<span class="header-section-number">4.5.2.4</span> Stuart-Maxwell Test<a class="anchor" aria-label="anchor" href="#stuart-maxwell-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Stuart-Maxwell Test is used for analyzing changes in paired categorical data with more than two categories. It is a generalization of <a href="basic-statistical-inference.html#mcnemars-test">McNemar’s Test</a>, applied to square contingency tables where the off-diagonal elements represent transitions between categories. Unlike the <a href="basic-statistical-inference.html#mcnemar-bowker-test">McNemar-Bowker Test</a>, which tests for symmetry across all pairs, the <a href="basic-statistical-inference.html#stuart-maxwell-test">Stuart-Maxwell Test</a> focuses on overall marginal homogeneity.</p>
<p>The test evaluates whether the marginal distributions of paired data are consistent across categories. This is particularly useful when investigating whether the distribution of responses has shifted between two conditions, such as before and after an intervention.</p>
<hr>
<p>Hypotheses for the Stuart-Maxwell Test</p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math display">\[
\text{The marginal distributions of the paired data are homogeneous (no difference).}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): <span class="math display">\[
\text{The marginal distributions of the paired data are not homogeneous (there is a difference).}
\]</span></p></li>
</ul>
<hr>
<p>The Stuart-Maxwell Test statistic is calculated as: <span class="math display">\[
M^2 = \mathbf{b}' \mathbf{V}^{-1} \mathbf{b}
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{b}\)</span>: Vector of differences between the marginal totals of paired categories.</p></li>
<li><p><span class="math inline">\(\mathbf{V}\)</span>: Covariance matrix of <span class="math inline">\(\mathbf{b}\)</span> under the null hypothesis.</p></li>
</ul>
<p>The test statistic <span class="math inline">\(M^2\)</span> follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\((r - 1)\)</span> degrees of freedom, where <span class="math inline">\(r\)</span> is the number of categories.</p>
<hr>
<p>A company surveys employees about their satisfaction levels (Low, Medium, High) before and after implementing a new workplace policy. The results are summarized in the following <span class="math inline">\(3 \times 3\)</span> contingency table.</p>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Employee satisfaction data before and after a policy change</span></span>
<span><span class="va">satisfaction_table</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>    <span class="fl">40</span>, <span class="fl">10</span>, <span class="fl">5</span>,  <span class="co"># Before: Low</span></span>
<span>    <span class="fl">8</span>, <span class="fl">50</span>, <span class="fl">12</span>,  <span class="co"># Before: Medium</span></span>
<span>    <span class="fl">6</span>, <span class="fl">10</span>, <span class="fl">40</span>   <span class="co"># Before: High</span></span>
<span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">3</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    <span class="st">"Before"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span>,</span>
<span>    <span class="st">"After"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Function to perform the Stuart-Maxwell Test</span></span>
<span><span class="va">stuart_maxwell_test</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">table</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/all.html">all</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/stop.html">stop</a></span><span class="op">(</span><span class="st">"Input must be a square matrix."</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># Marginal totals for each category</span></span>
<span>  <span class="va">row_totals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span></span>
<span>  <span class="va">col_totals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">colSums</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Vector of differences between row and column marginal totals</span></span>
<span>  <span class="va">b</span> <span class="op">&lt;-</span> <span class="va">row_totals</span> <span class="op">-</span> <span class="va">col_totals</span></span>
<span>  </span>
<span>  <span class="co"># Covariance matrix under the null hypothesis</span></span>
<span>  <span class="va">total</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span></span>
<span>  <span class="va">V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">row_totals</span> <span class="op">+</span> <span class="va">col_totals</span><span class="op">)</span> <span class="op">-</span> </span>
<span>       <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/outer.html">outer</a></span><span class="op">(</span><span class="va">row_totals</span>, <span class="va">col_totals</span>, <span class="st">"+"</span><span class="op">)</span> <span class="op">/</span> <span class="va">total</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Calculate the test statistic</span></span>
<span>  <span class="va">M2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">b</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">b</span></span>
<span>  <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">table</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span></span>
<span>  <span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">M2</span>, df <span class="op">=</span> <span class="va">df</span>, lower.tail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>statistic <span class="op">=</span> <span class="va">M2</span>, df <span class="op">=</span> <span class="va">df</span>, p_value <span class="op">=</span> <span class="va">p_value</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Run the Stuart-Maxwell Test</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">stuart_maxwell_test</span><span class="op">(</span><span class="va">satisfaction_table</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print the results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Stuart-Maxwell Test Results:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Stuart-Maxwell Test Results:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Test Statistic (M^2):"</span>, <span class="va">result</span><span class="op">$</span><span class="va">statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Test Statistic (M^2): 0.01802387</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Degrees of Freedom:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">df</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Degrees of Freedom: 2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p-value:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">p_value</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p-value: 0.9910286</span></span></code></pre></div>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li><p><strong>Test Statistic</strong>: Measures the extent of marginal differences in the table.</p></li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>A <strong>low p-value</strong> (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) indicates significant differences between the marginal distributions, suggesting a change in the distribution of responses.</p></li>
<li><p>A <strong>high p-value</strong> suggests no evidence of marginal differences, meaning the distribution is consistent across conditions.</p></li>
</ul>
</li>
</ol>
<p>Practical Applications of the Stuart-Maxwell Test</p>
<ol style="list-style-type: decimal">
<li><p><strong>Employee Surveys</strong>: Analyzing shifts in satisfaction levels before and after policy changes.</p></li>
<li><p><strong>Consumer Studies</strong>: Evaluating changes in product preferences before and after a marketing campaign.</p></li>
<li><p><strong>Healthcare Research</strong>: Assessing changes in patient responses to treatments across categories.</p></li>
</ol>
</div>
<div id="cochran-mantel-haenszel-cmh-test" class="section level4" number="4.5.2.5">
<h4>
<span class="header-section-number">4.5.2.5</span> Cochran-Mantel-Haenszel (CMH) Test<a class="anchor" aria-label="anchor" href="#cochran-mantel-haenszel-cmh-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Cochran-Mantel-Haenszel (CMH) Test is a generalization of the <a href="basic-statistical-inference.html#mantel-haenszel-chi-square-test">Mantel-Haenszel Chi-square Test</a>. It evaluates the association between two variables while controlling for the effect of a third stratifying variable. This test is particularly suited for ordinal data, allowing researchers to detect trends and associations across strata.</p>
<p>The CMH Test addresses scenarios where:</p>
<ol style="list-style-type: decimal">
<li>Two variables (e.g., exposure and outcome) are ordinal or nominal.</li>
<li>A third variable (e.g., a demographic or environmental factor) stratifies the data into <span class="math inline">\(K\)</span> independent groups.</li>
</ol>
<p>The test answers: Is there a consistent association between the two variables across the strata defined by the third variable?</p>
<hr>
<p>The CMH Test has three main variations depending on the nature of the data:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Correlation Test for Ordinal Data</strong>: Assesses whether there is a linear association between two ordinal variables across strata.</li>
<li>
<strong>General Association Test</strong>: Tests for any association (not necessarily ordinal) between two variables while stratifying by a third.</li>
<li>
<strong>Homogeneity Test</strong>: Checks whether the strength of the association between the two variables is consistent across strata.</li>
</ol>
<hr>
<p>Hypotheses</p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): There is no association between the two variables across all strata, or the strength of the association is consistent across strata.</p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): There is an association between the two variables in at least one stratum, or the strength of the association varies across strata.</p></li>
</ul>
<hr>
<p>The CMH test statistic is: <span class="math display">\[
CMH = \frac{\left( \sum_{k} \left(O_k - E_k \right)\right)^2}{\sum_{k} V_k}
\]</span> Where:</p>
<ul>
<li><p><span class="math inline">\(O_k\)</span>: Observed counts in stratum <span class="math inline">\(k\)</span>.</p></li>
<li><p><span class="math inline">\(E_k\)</span>: Expected counts in stratum <span class="math inline">\(k\)</span>, calculated under the null hypothesis.</p></li>
<li><p><span class="math inline">\(V_k\)</span>: Variance of the observed counts in stratum <span class="math inline">\(k\)</span>.</p></li>
</ul>
<p>The test statistic follows a <span class="math inline">\(\chi^2\)</span> distribution with 1 degree of freedom under the null hypothesis.</p>
<hr>
<p>A company evaluates whether sales performance (Low, Medium, High) is associated with product satisfaction (Low, Medium, High) across three experience levels (Junior, Mid-level, Senior). The data is organized into a <span class="math inline">\(3 \times 3 \times 3\)</span> contingency table.</p>
<div class="sourceCode" id="cb140"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sales performance data</span></span>
<span><span class="va">sales_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">15</span>, <span class="fl">10</span>, <span class="fl">12</span>, <span class="fl">18</span>, <span class="fl">15</span>, <span class="fl">8</span>, <span class="fl">12</span>, <span class="fl">20</span>,   <span class="co"># Junior</span></span>
<span>      <span class="fl">25</span>, <span class="fl">20</span>, <span class="fl">15</span>, <span class="fl">20</span>, <span class="fl">25</span>, <span class="fl">30</span>, <span class="fl">10</span>, <span class="fl">15</span>, <span class="fl">20</span>,  <span class="co"># Mid-level</span></span>
<span>      <span class="fl">30</span>, <span class="fl">25</span>, <span class="fl">20</span>, <span class="fl">28</span>, <span class="fl">32</span>, <span class="fl">35</span>, <span class="fl">15</span>, <span class="fl">20</span>, <span class="fl">30</span><span class="op">)</span>, <span class="co"># Senior</span></span>
<span>    dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>    dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>        SalesPerformance <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span>,</span>
<span>        Satisfaction <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span>,</span>
<span>        ExperienceLevel <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Junior"</span>, <span class="st">"Mid-level"</span>, <span class="st">"Senior"</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load the vcd package for the CMH test</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">vcd</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform CMH Test</span></span>
<span><span class="va">cmh_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/mantelhaen.test.html">mantelhaen.test</a></span><span class="op">(</span><span class="va">sales_data</span>, correct <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">cmh_result</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Cochran-Mantel-Haenszel test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  sales_data</span></span>
<span><span class="co">#&gt; Cochran-Mantel-Haenszel M^2 = 22.454, df = 4, p-value = 0.0001627</span></span></code></pre></div>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li><p><strong>Test Statistic</strong>: A large CMH statistic suggests a significant association between sales performance and satisfaction after accounting for experience level.</p></li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>A <strong>low p-value</strong> (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) indicates a significant association between the two variables across strata.</p></li>
<li><p>A <strong>high p-value</strong> suggests no evidence of association or that the relationship is consistent across all strata.</p></li>
</ul>
</li>
</ol>
<p>Practical Applications of the CMH Test</p>
<ol style="list-style-type: decimal">
<li><p><strong>Business Performance Analysis</strong>: Investigating the relationship between customer satisfaction and sales performance across different demographic groups.</p></li>
<li><p><strong>Healthcare Studies</strong>: Assessing the effect of treatment (e.g., dosage) on outcomes while controlling for patient characteristics (e.g., age groups).</p></li>
<li><p><strong>Educational Research</strong>: Analyzing the relationship between test scores and study hours, stratified by teaching method.</p></li>
</ol>
</div>
<div id="summary-table-of-tests" class="section level4" number="4.5.2.6">
<h4>
<span class="header-section-number">4.5.2.6</span> Summary Table of Tests<a class="anchor" aria-label="anchor" href="#summary-table-of-tests"><i class="fas fa-link"></i></a>
</h4>
<p>The following table provides a concise guide on when and why to use each test:</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="16%">
<col width="34%">
<col width="26%">
<col width="22%">
</colgroup>
<thead><tr class="header">
<th><strong>Test Name</strong></th>
<th><strong>When to Use</strong></th>
<th><strong>Key Question Addressed</strong></th>
<th><strong>Data Requirements</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>[<strong>Mantel-Haenszel Chi-square Test</strong>]</td>
<td>When testing for association between two binary variables across multiple strata.</td>
<td>Is there a consistent association across strata?</td>
<td>Binary variables in <span class="math inline">\(2 \times 2 \times K\)</span> tables.</td>
</tr>
<tr class="even">
<td>[<strong>McNemar’s Test</strong>]</td>
<td>When analyzing marginal symmetry in paired binary data.</td>
<td>Are the proportions of discordant pairs equal?</td>
<td>Paired binary responses (<span class="math inline">\(2 \times 2\)</span> table).</td>
</tr>
<tr class="odd">
<td>[<strong>McNemar-Bowker Test</strong>]</td>
<td>When testing for symmetry in paired nominal data with more than two categories.</td>
<td>Are the off-diagonal elements symmetric across all categories?</td>
<td>Paired nominal data in <span class="math inline">\(r \times r\)</span> tables.</td>
</tr>
<tr class="even">
<td>[<strong>Cochran-Mantel-Haenszel (CMH) Test</strong>]</td>
<td>When testing ordinal or general associations while controlling for a stratifying variable.</td>
<td>Is there an association between two variables after stratification?</td>
<td>Ordinal or nominal data in <span class="math inline">\(I \times J \times K\)</span> tables.</td>
</tr>
<tr class="odd">
<td>[<strong>Stuart-Maxwell Test</strong>]</td>
<td>When analyzing marginal homogeneity in paired nominal data with more than two categories.</td>
<td>Are the marginal distributions of paired data homogeneous?</td>
<td>Paired nominal data in <span class="math inline">\(r \times r\)</span> tables.</td>
</tr>
</tbody>
</table></div>
<hr>
<p><strong>How to Choose the Right Test</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Paired vs. Stratified Data</strong>:
<ul>
<li>Use <strong>McNemar’s Test</strong> or <strong>McNemar-Bowker Test</strong> for paired data.</li>
<li>Use <strong>Mantel-Haenszel</strong> or <strong>CMH Test</strong> for stratified data.</li>
</ul>
</li>
<li>
<strong>Binary vs. Multi-category Variables</strong>:
<ul>
<li>Use <strong>McNemar’s Test</strong> for binary data.</li>
<li>Use <strong>McNemar-Bowker Test</strong> or <strong>Stuart-Maxwell Test</strong> for multi-category data.</li>
</ul>
</li>
<li>
<strong>Ordinal Trends</strong>:
<ul>
<li>Use the <strong>CMH Test</strong> if testing for ordinal associations while controlling for a stratifying variable.</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="ordinal-trend" class="section level3" number="4.5.3">
<h3>
<span class="header-section-number">4.5.3</span> Ordinal Trend<a class="anchor" aria-label="anchor" href="#ordinal-trend"><i class="fas fa-link"></i></a>
</h3>
<p>When analyzing ordinal data, it is often important to determine whether a consistent trend exists between variables. Tests for trend are specifically designed to detect monotonic relationships where changes in one variable are systematically associated with changes in another. These tests are widely used in scenarios involving ordered categories, such as customer satisfaction ratings, income brackets, or educational levels.</p>
<p>The primary objectives of trend tests are:</p>
<ol style="list-style-type: decimal">
<li>
<strong>To detect monotonic relationships</strong>: Determine if higher or lower categories of one variable are associated with higher or lower categories of another variable.</li>
<li>
<strong>To account for ordinal structure</strong>: Leverage the inherent order in the data to provide more sensitive and interpretable results compared to tests designed for nominal data.</li>
</ol>
<hr>
<p><strong>Key Considerations for Trend Tests</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Data Structure</strong>:
<ul>
<li>Ensure that the variables have a natural order and are treated as ordinal.</li>
<li>Verify that the trend test chosen matches the data structure (e.g., binary outcome vs. multi-level ordinal variables).</li>
</ul>
</li>
<li>
<strong>Assumptions</strong>:
<ul>
<li>Many tests assume monotonic trends, meaning that the relationship should not reverse direction.</li>
</ul>
</li>
<li>
<strong>Interpretation</strong>:
<ul>
<li>A significant result indicates the presence of a trend but does not imply causality.</li>
<li>The direction and strength of the trend should be carefully interpreted in the context of the data.</li>
</ul>
</li>
</ol>
<div id="cochran-armitage-test" class="section level4" number="4.5.3.1">
<h4>
<span class="header-section-number">4.5.3.1</span> Cochran-Armitage Test<a class="anchor" aria-label="anchor" href="#cochran-armitage-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Cochran-Armitage Test for Trend is a statistical method designed to detect a linear trend in proportions across ordered categories of a predictor variable. It is particularly useful in <span class="math inline">\(2 \times J\)</span> contingency tables, where there is a binary outcome (e.g., success/failure) and an ordinal predictor variable with <span class="math inline">\(J\)</span> ordered levels.</p>
<p>The Cochran-Armitage Test evaluates whether the proportion of a binary outcome changes systematically across the levels of an ordinal predictor. This test leverages the ordinal nature of the predictor to enhance sensitivity and power compared to general chi-square tests.</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math display">\[
\text{The proportion of the binary outcome is constant across the levels of the ordinal predictor.}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): <span class="math display">\[
\text{There is a linear trend in the proportion of the binary outcome across the levels of the ordinal predictor.}
\]</span></p></li>
</ul>
<hr>
<p>The Cochran-Armitage Test statistic is calculated as:</p>
<p><span class="math display">\[
Z = \frac{\sum_{j=1}^{J} w_j (n_{1j} - N_j \hat{p})}{\sqrt{\hat{p} (1 - \hat{p}) \sum_{j=1}^{J} w_j^2 N_j}}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n_{1j}\)</span>: Count of the binary outcome (e.g., “success”) in category <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(N_j\)</span>: Total number of observations in category <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{p}\)</span>: Overall proportion of the binary outcome, calculated as: <span class="math display">\[
  \hat{p} = \frac{\sum_{j=1}^{J} n_{1j}}{\sum_{j=1}^{J} N_j}
  \]</span></p></li>
<li><p><span class="math inline">\(w_j\)</span>: Score assigned to the <span class="math inline">\(j\)</span>th category of the ordinal predictor, often set to <span class="math inline">\(j\)</span> for equally spaced levels.</p></li>
</ul>
<p>The test statistic <span class="math inline">\(Z\)</span> follows a standard normal distribution under the null hypothesis.</p>
<p><strong>Key Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Ordinal Predictor</strong>: The categories of the predictor variable must have a natural order.</li>
<li>
<strong>Binary Outcome</strong>: The response variable must be dichotomous (e.g., success/failure).</li>
<li>
<strong>Independent Observations</strong>: Observations within and across categories are independent.</li>
</ol>
<hr>
<p>Let’s consider a study examining whether the success rate of a marketing campaign varies across three income levels (Low, Medium, High). The data is structured in a <span class="math inline">\(2 \times 3\)</span> contingency table:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Income Level</th>
<th>Success</th>
<th>Failure</th>
<th>Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Low</td>
<td>20</td>
<td>30</td>
<td>50</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>35</td>
<td>15</td>
<td>50</td>
</tr>
<tr class="odd">
<td>High</td>
<td>45</td>
<td>5</td>
<td>50</td>
</tr>
</tbody>
</table></div>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Data: Success and Failure counts by Income Level</span></span>
<span><span class="va">income_levels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span></span>
<span><span class="va">success</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">35</span>, <span class="fl">45</span><span class="op">)</span></span>
<span><span class="va">failure</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">15</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">total</span> <span class="op">&lt;-</span> <span class="va">success</span> <span class="op">+</span> <span class="va">failure</span></span>
<span></span>
<span><span class="co"># Scores for ordinal levels (can be custom weights)</span></span>
<span><span class="va">scores</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">income_levels</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Cochran-Armitage Test</span></span>
<span><span class="co"># Function to calculate Z statistic</span></span>
<span><span class="va">cochran_armitage_test</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">success</span>, <span class="va">failure</span>, <span class="va">scores</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">N</span> <span class="op">&lt;-</span> <span class="va">success</span> <span class="op">+</span> <span class="va">failure</span></span>
<span>  <span class="va">p_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">success</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span>
<span>  <span class="va">weights</span> <span class="op">&lt;-</span> <span class="va">scores</span></span>
<span>  </span>
<span>  <span class="co"># Calculate numerator</span></span>
<span>  <span class="va">numerator</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">weights</span> <span class="op">*</span> <span class="op">(</span><span class="va">success</span> <span class="op">-</span> <span class="va">N</span> <span class="op">*</span> <span class="va">p_hat</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Calculate denominator</span></span>
<span>  <span class="va">denominator</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">p_hat</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p_hat</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">weights</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">N</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Z statistic</span></span>
<span>  <span class="va">Z</span> <span class="op">&lt;-</span> <span class="va">numerator</span> <span class="op">/</span> <span class="va">denominator</span></span>
<span>  <span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>Z_statistic <span class="op">=</span> <span class="va">Z</span>, p_value <span class="op">=</span> <span class="va">p_value</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Perform the test</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">cochran_armitage_test</span><span class="op">(</span><span class="va">success</span>, <span class="va">failure</span>, <span class="va">scores</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Cochran-Armitage Test for Trend Results:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Cochran-Armitage Test for Trend Results:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Z Statistic:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">Z_statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Z Statistic: 2.004459</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p-value:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">p_value</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p-value: 0.04502088</span></span></code></pre></div>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Test Statistic (</strong><span class="math inline">\(Z\)</span><strong>)</strong>:</p>
<ul>
<li><p>The <span class="math inline">\(Z\)</span> value indicates the strength and direction of the trend.</p></li>
<li><p>Positive <span class="math inline">\(Z\)</span>: Proportions increase with higher categories.</p></li>
<li><p>Negative <span class="math inline">\(Z\)</span>: Proportions decrease with higher categories.</p></li>
</ul>
</li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>A <strong>low p-value</strong> (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) rejects the null hypothesis, indicating a significant linear trend.</p></li>
<li><p>A <strong>high p-value</strong> fails to reject the null hypothesis, suggesting no evidence of a trend.</p></li>
</ul>
</li>
</ol>
<p>Practical Applications</p>
<ol style="list-style-type: decimal">
<li><p><strong>Marketing</strong>: Analyzing whether customer success rates vary systematically across income levels or demographics.</p></li>
<li><p><strong>Healthcare</strong>: Evaluating the dose-response relationship between medication levels and recovery rates.</p></li>
<li><p><strong>Education</strong>: Studying whether pass rates improve with higher levels of educational support.</p></li>
</ol>
</div>
<div id="jonckheere-terpstra-test" class="section level4" number="4.5.3.2">
<h4>
<span class="header-section-number">4.5.3.2</span> Jonckheere-Terpstra Test<a class="anchor" aria-label="anchor" href="#jonckheere-terpstra-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Jonckheere-Terpstra Test is a nonparametric test designed to detect ordered differences between groups. It is particularly suited for ordinal data where both the predictor and response variables exhibit a monotonic trend. Unlike general nonparametric tests like the Kruskal-Wallis test, which assess any differences between groups, the Jonckheere-Terpstra Test specifically evaluates whether the data follows a prespecified ordering.</p>
<p>The Jonckheere-Terpstra Test determines whether:</p>
<ol style="list-style-type: decimal">
<li>There is a monotonic trend in the response variable across ordered groups of the predictor.</li>
<li>The data aligns with an a priori hypothesized order (e.g., group 1 &lt; group 2 &lt; group 3).</li>
</ol>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math display">\[
\text{There is no trend in the response variable across the ordered groups.}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): <span class="math display">\[
\text{The response variable exhibits a monotonic trend across the ordered groups.}
\]</span></p></li>
</ul>
<p>The trend can be increasing, decreasing, or as otherwise hypothesized.</p>
<hr>
<p>The Jonckheere-Terpstra Test statistic is based on the number of pairwise comparisons (<span class="math inline">\(U\)</span>) that are consistent with the hypothesized trend. For <span class="math inline">\(k\)</span> groups:</p>
<ol style="list-style-type: decimal">
<li>Compare all possible pairs of observations across groups.</li>
<li>Count the number of pairs where the values are consistent with the hypothesized order.</li>
</ol>
<p>The test statistic <span class="math inline">\(T\)</span> is the sum of all pairwise comparisons: <span class="math display">\[
T = \sum_{i &lt; j} T_{ij}
\]</span> Where <span class="math inline">\(T_{ij}\)</span> is the number of concordant pairs between groups <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
<p>Under the null hypothesis, <span class="math inline">\(T\)</span> follows a normal distribution with:</p>
<ul>
<li><p>Mean: <span class="math display">\[
  \mu_T = \frac{N (N - 1)}{4}
  \]</span></p></li>
<li><p>Variance: <span class="math display">\[
  \sigma_T^2 = \frac{N (N - 1) (2N + 1)}{24}
  \]</span> Where <span class="math inline">\(N\)</span> is the total number of observations.</p></li>
</ul>
<p>The standardized test statistic is: <span class="math display">\[
Z = \frac{T - \mu_T}{\sigma_T}
\]</span></p>
<p><strong>Key Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Ordinal or Interval Data</strong>: The response variable must be at least ordinal, and the groups must have a logical order.</li>
<li>
<strong>Independent Groups</strong>: Observations within and between groups are independent.</li>
<li>
<strong>Consistent Hypothesis</strong>: The trend (e.g., increasing or decreasing) must be specified in advance.</li>
</ol>
<hr>
<p>Let’s consider a study analyzing whether customer satisfaction ratings (on a scale of 1 to 5) improve with increasing levels of service tiers (Basic, Standard, Premium). The data is grouped by service tier, and we hypothesize that satisfaction ratings increase with higher service tiers.</p>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example Data: Customer Satisfaction Ratings by Service Tier</span></span>
<span><span class="va">satisfaction</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  Basic <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">4</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  Standard <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">5</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>  Premium <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">5</span>, <span class="fl">4</span>, <span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare data</span></span>
<span><span class="va">ratings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">satisfaction</span><span class="op">)</span></span>
<span><span class="va">groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">satisfaction</span><span class="op">)</span>, times <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">satisfaction</span>, <span class="va">length</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate pairwise comparisons</span></span>
<span><span class="va">manual_jonckheere</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">ratings</span>, <span class="va">groups</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">n_groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/unique.html">unique</a></span><span class="op">(</span><span class="va">groups</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">pairwise_comparisons</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  <span class="va">total_pairs</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  </span>
<span>  <span class="co"># Iterate over group pairs</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">n_groups</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="op">(</span><span class="va">i</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="va">n_groups</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">group_i</span> <span class="op">&lt;-</span> <span class="va">ratings</span><span class="op">[</span><span class="va">groups</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/levels.html">levels</a></span><span class="op">(</span><span class="va">groups</span><span class="op">)</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span></span>
<span>      <span class="va">group_j</span> <span class="op">&lt;-</span> <span class="va">ratings</span><span class="op">[</span><span class="va">groups</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/levels.html">levels</a></span><span class="op">(</span><span class="va">groups</span><span class="op">)</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">]</span></span>
<span>      </span>
<span>      <span class="co"># Count concordant pairs</span></span>
<span>      <span class="kw">for</span> <span class="op">(</span><span class="va">x</span> <span class="kw">in</span> <span class="va">group_i</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="kw">for</span> <span class="op">(</span><span class="va">y</span> <span class="kw">in</span> <span class="va">group_j</span><span class="op">)</span> <span class="op">{</span></span>
<span>          <span class="kw">if</span> <span class="op">(</span><span class="va">x</span> <span class="op">&lt;</span> <span class="va">y</span><span class="op">)</span> <span class="va">pairwise_comparisons</span> <span class="op">&lt;-</span> <span class="va">pairwise_comparisons</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>          <span class="kw">if</span> <span class="op">(</span><span class="va">x</span> <span class="op">==</span> <span class="va">y</span><span class="op">)</span> <span class="va">pairwise_comparisons</span> <span class="op">&lt;-</span> <span class="va">pairwise_comparisons</span> <span class="op">+</span> <span class="fl">0.5</span></span>
<span>          <span class="va">total_pairs</span> <span class="op">&lt;-</span> <span class="va">total_pairs</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>      <span class="op">}</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># Compute test statistic</span></span>
<span>  <span class="cn">T</span> <span class="op">&lt;-</span> <span class="va">pairwise_comparisons</span></span>
<span>  <span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">ratings</span><span class="op">)</span></span>
<span>  <span class="va">mu_T</span> <span class="op">&lt;-</span> <span class="va">total_pairs</span> <span class="op">/</span> <span class="fl">2</span></span>
<span>  <span class="va">sigma_T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">total_pairs</span> <span class="op">*</span> <span class="op">(</span><span class="va">N</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="fl">12</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">Z</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="cn">T</span> <span class="op">-</span> <span class="va">mu_T</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma_T</span></span>
<span>  <span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>T_statistic <span class="op">=</span> <span class="cn">T</span>, Z_statistic <span class="op">=</span> <span class="va">Z</span>, p_value <span class="op">=</span> <span class="va">p_value</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Perform the test</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">manual_jonckheere</span><span class="op">(</span><span class="va">ratings</span>, <span class="va">groups</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Jonckheere-Terpstra Test Results:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Jonckheere-Terpstra Test Results:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"T Statistic (Sum of Concordant Pairs):"</span>, <span class="va">result</span><span class="op">$</span><span class="va">T_statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; T Statistic (Sum of Concordant Pairs): 49.5</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Z Statistic:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">Z_statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Z Statistic: 1.2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p-value:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">p_value</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p-value: 0.2301393</span></span></code></pre></div>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Test Statistic (</strong><span class="math inline">\(T\)</span><strong>)</strong>:</p>
<ul>
<li>Represents the sum of all pairwise comparisons consistent with the hypothesized order.</li>
<li>Includes 0.5 for tied pairs.</li>
</ul>
</li>
<li>
<p><span class="math inline">\(Z\)</span> <strong>Statistic</strong>:</p>
<ul>
<li><p>A standardized measure of the strength of the trend.</p></li>
<li><p>Calculated using <span class="math inline">\(T\)</span>, the expected value of <span class="math inline">\(T\)</span> under the null hypothesis (<span class="math inline">\(\mu_T\)</span>), and the variance of <span class="math inline">\(T\)</span> (<span class="math inline">\(\sigma_T^2\)</span>).</p></li>
</ul>
</li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>A <strong>low p-value</strong> (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) rejects the null hypothesis, indicating a significant trend in the response variable across ordered groups.</p></li>
<li><p>A <strong>high p-value</strong> fails to reject the null hypothesis, suggesting no evidence of a trend.</p></li>
</ul>
</li>
</ol>
<p>Practical Applications</p>
<ol style="list-style-type: decimal">
<li><p><strong>Customer Experience Analysis</strong>: Assessing whether customer satisfaction increases with higher service levels or product tiers.</p></li>
<li><p><strong>Healthcare Studies</strong>: Testing whether recovery rates improve with increasing doses of a treatment.</p></li>
<li><p><strong>Education Research</strong>: Analyzing whether test scores improve with higher levels of educational intervention.</p></li>
</ol>
</div>
<div id="mantel-test-for-trend" class="section level4" number="4.5.3.3">
<h4>
<span class="header-section-number">4.5.3.3</span> Mantel Test for Trend<a class="anchor" aria-label="anchor" href="#mantel-test-for-trend"><i class="fas fa-link"></i></a>
</h4>
<p>The Mantel Test for Trend is a statistical method designed to detect a linear association between two ordinal variables. It is an extension of the <a href="basic-statistical-inference.html#mantel-haenszel-chi-square-test">Mantel-Haenszel Chi-square Test</a> and is particularly suited for analyzing trends in ordinal contingency tables, such as <span class="math inline">\(I \times J\)</span> tables where both variables are ordinal.</p>
<p>The Mantel Test for Trend evaluates whether an increasing or decreasing trend exists between two ordinal variables. It uses the ordering of categories to assess linear relationships, making it more sensitive to trends compared to general association tests like chi-square.</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math display">\[
\text{There is no linear association between the two ordinal variables.}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): <span class="math display">\[
\text{There is a significant linear association between the two ordinal variables.}
\]</span></p></li>
</ul>
<hr>
<p>The Mantel Test is based on the Pearson correlation between the row and column scores in an ordinal contingency table. The test statistic is: <span class="math display">\[
M = \frac{\sum_{i} \sum_{j} w_i w_j n_{ij}}{\sqrt{\sum_{i} w_i^2 n_{i\cdot} \sum_{j} w_j^2 n_{\cdot j}}}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n_{ij}\)</span>: Observed frequency in cell <span class="math inline">\((i, j)\)</span>.</p></li>
<li><p><span class="math inline">\(n_{i\cdot}\)</span>: Row marginal total for row <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(n_{\cdot j}\)</span>: Column marginal total for column <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(w_i\)</span>: Score for the <span class="math inline">\(i\)</span>th row.</p></li>
<li><p>ore for the <span class="math inline">\(j\)</span>th column.</p></li>
</ul>
<p>The test statistic <span class="math inline">\(M\)</span> is asymptotically normally distributed under the null hypothesis.</p>
<p><strong>Key Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Ordinal Variables</strong>: Both variables must have a natural order.</li>
<li>
<strong>Linear Trend</strong>: Assumes a linear relationship between the scores assigned to the rows and columns.</li>
<li>
<strong>Independence</strong>: Observations must be independent.</li>
</ol>
<hr>
<p>Let’s consider a marketing study evaluating whether customer satisfaction levels (Low, Medium, High) are associated with increasing purchase frequency (Low, Medium, High).</p>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Customer satisfaction and purchase frequency data</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">5</span>, <span class="fl">2</span>, <span class="fl">15</span>, <span class="fl">20</span>, <span class="fl">8</span>, <span class="fl">25</span>, <span class="fl">30</span>, <span class="fl">12</span><span class="op">)</span>, </span>
<span>  nrow <span class="op">=</span> <span class="fl">3</span>, </span>
<span>  byrow <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    Satisfaction <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span>,</span>
<span>    Frequency <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Assign scores for rows and columns</span></span>
<span><span class="va">row_scores</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="va">col_scores</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Mantel statistic manually</span></span>
<span><span class="va">mantel_test_manual</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span>, <span class="va">row_scores</span>, <span class="va">col_scores</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">numerator</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/outer.html">outer</a></span><span class="op">(</span><span class="va">row_scores</span>, <span class="va">col_scores</span>, <span class="st">"*"</span><span class="op">)</span> <span class="op">*</span> <span class="va">data</span><span class="op">)</span></span>
<span>  <span class="va">row_marginals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span>  <span class="va">col_marginals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">colSums</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span>  <span class="va">row_variance</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">row_scores</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">row_marginals</span><span class="op">)</span></span>
<span>  <span class="va">col_variance</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">col_scores</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">col_marginals</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">M</span> <span class="op">&lt;-</span> <span class="va">numerator</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">row_variance</span> <span class="op">*</span> <span class="va">col_variance</span><span class="op">)</span></span>
<span>  <span class="va">z_value</span> <span class="op">&lt;-</span> <span class="va">M</span></span>
<span>  <span class="va">p_value</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">z_value</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="co"># Two-tailed test</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>Mantel_statistic <span class="op">=</span> <span class="va">M</span>, p_value <span class="op">=</span> <span class="va">p_value</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Perform the Mantel Test</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">mantel_test_manual</span><span class="op">(</span><span class="va">data</span>, <span class="va">row_scores</span>, <span class="va">col_scores</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Mantel Test for Trend Results:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Mantel Test for Trend Results:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Mantel Statistic (M):"</span>, <span class="va">result</span><span class="op">$</span><span class="va">Mantel_statistic</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Mantel Statistic (M): 0.8984663</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p-value:"</span>, <span class="va">result</span><span class="op">$</span><span class="va">p_value</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p-value: 0.368937</span></span></code></pre></div>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Test Statistic (</strong><span class="math inline">\(M\)</span><strong>)</strong>:</p>
<ul>
<li><p>Represents the strength and direction of the linear association.</p></li>
<li><p>Positive <span class="math inline">\(M\)</span>: Increasing trend.</p></li>
<li><p>Negative <span class="math inline">\(M\)</span>: Decreasing trend.</p></li>
</ul>
</li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>A <strong>low p-value</strong> (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) indicates a significant linear association.</p></li>
<li><p>A <strong>high p-value</strong> suggests no evidence of a trend.</p></li>
</ul>
</li>
</ol>
<p>Practical Applications</p>
<ol style="list-style-type: decimal">
<li><p><strong>Marketing Analysis</strong>: Investigating whether satisfaction levels are associated with purchase behavior or loyalty.</p></li>
<li><p><strong>Healthcare Research</strong>: Testing for a dose-response relationship between treatment levels and outcomes.</p></li>
<li><p><strong>Social Sciences</strong>: Analyzing trends in survey responses across ordered categories.</p></li>
</ol>
</div>
<div id="chi-square-test-for-linear-trend" class="section level4" number="4.5.3.4">
<h4>
<span class="header-section-number">4.5.3.4</span> Chi-square Test for Linear Trend<a class="anchor" aria-label="anchor" href="#chi-square-test-for-linear-trend"><i class="fas fa-link"></i></a>
</h4>
<p>The Chi-square Test for Linear Trend is a statistical method used to detect a linear relationship between an ordinal predictor and a binary outcome. It is an extension of the chi-square test, designed specifically for ordered categories, making it more sensitive to linear trends in proportions compared to a general chi-square test of independence.</p>
<p>The Chi-square Test for Linear Trend evaluates whether the proportions of a binary outcome (e.g., success/failure) change systematically across ordered categories of a predictor variable. It is widely used in situations such as analyzing dose-response relationships or evaluating trends in survey responses.</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li><p><strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): <span class="math display">\[
\text{There is no linear trend in the proportions of the binary outcome across ordered categories.}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (</strong><span class="math inline">\(H_A\)</span>): <span class="math display">\[
\text{There is a significant linear trend in the proportions of the binary outcome across ordered categories.}
\]</span></p></li>
</ul>
<hr>
<p>The test statistic is:</p>
<p><span class="math display">\[
X^2_{\text{trend}} = \frac{\left( \sum_{j=1}^J w_j (p_j - \bar{p}) N_j \right)^2}{\sum_{j=1}^J w_j^2 \bar{p} (1 - \bar{p}) N_j}
\]</span></p>
<p>Where: - <span class="math inline">\(J\)</span>: Number of ordered categories. - <span class="math inline">\(w_j\)</span>: Scores assigned to the <span class="math inline">\(j\)</span>th category (typically <span class="math inline">\(j = 1, 2, \dots, J\)</span>). - <span class="math inline">\(p_j\)</span>: Proportion of success in the <span class="math inline">\(j\)</span>th category. - <span class="math inline">\(\bar{p}\)</span>: Overall proportion of success across all categories. - <span class="math inline">\(N_j\)</span>: Total number of observations in the <span class="math inline">\(j\)</span>th category.</p>
<p>The test statistic follows a chi-square distribution with 1 degree of freedom under the null hypothesis.</p>
<hr>
<p><strong>Key Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Binary Outcome</strong>: The response variable must be binary (e.g., success/failure).</li>
<li>
<strong>Ordinal Predictor</strong>: The predictor variable must have a natural order.</li>
<li>
<strong>Independent Observations</strong>: Data across categories must be independent.</li>
</ol>
<hr>
<p>Let’s consider a study analyzing whether the proportion of customers who recommend a product increases with customer satisfaction levels (Low, Medium, High).</p>
<div class="sourceCode" id="cb144"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example Data: Customer Satisfaction and Recommendation</span></span>
<span><span class="va">satisfaction_levels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Low"</span>, <span class="st">"Medium"</span>, <span class="st">"High"</span><span class="op">)</span></span>
<span><span class="va">success</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">35</span>, <span class="fl">50</span><span class="op">)</span>  <span class="co"># Number of customers who recommend the product</span></span>
<span><span class="va">failure</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">15</span>, <span class="fl">10</span><span class="op">)</span>  <span class="co"># Number of customers who do not recommend the product</span></span>
<span><span class="va">total</span> <span class="op">&lt;-</span> <span class="va">success</span> <span class="op">+</span> <span class="va">failure</span></span>
<span></span>
<span><span class="co"># Assign ordinal scores</span></span>
<span><span class="va">scores</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">satisfaction_levels</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate overall proportion of success</span></span>
<span><span class="va">p_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">success</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">total</span><span class="op">)</span></span></code></pre></div>
<p>Interpretation</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Chi-square Statistic (</strong><span class="math inline">\(X^2_{\text{trend}}\)</span><strong>)</strong>:</p>
<ul>
<li>Indicates the strength of the linear trend in the proportions.</li>
</ul>
</li>
<li>
<p><strong>p-value</strong>:</p>
<ul>
<li><p>A <strong>low p-value</strong> (e.g., <span class="math inline">\(p &lt; 0.05\)</span>) rejects the null hypothesis, indicating a significant linear trend.</p></li>
<li><p>A <strong>high p-value</strong> suggests no evidence of a linear trend.</p></li>
</ul>
</li>
</ol>
<p>Practical Applications</p>
<ol style="list-style-type: decimal">
<li><p><strong>Marketing</strong>: Analyzing whether customer satisfaction levels predict product recommendations or repurchase intentions.</p></li>
<li><p><strong>Healthcare</strong>: Evaluating dose-response relationships in clinical trials.</p></li>
<li><p><strong>Education</strong>: Testing whether higher levels of intervention improve success rates.</p></li>
</ol>
</div>
<div id="key-takeways" class="section level4" number="4.5.3.5">
<h4>
<span class="header-section-number">4.5.3.5</span> Key Takeways<a class="anchor" aria-label="anchor" href="#key-takeways"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="15%">
<col width="28%">
<col width="21%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th><strong>Test</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>Key Assumptions</strong></th>
<th><strong>Use Cases</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>[<strong>Cochran-Armitage Test</strong>]</td>
<td>Tests for a linear trend in proportions across ordinal categories.</td>
<td>- Binary response variable.<br>
- Predictor variable is ordinal.</td>
<td>Evaluating dose-response relationships, comparing proportions across ordinal groups.</td>
</tr>
<tr class="even">
<td>[<strong>Jonckheere-Terpstra Test</strong>]</td>
<td>Tests for a monotonic trend in a response variable across ordered groups.</td>
<td>- Response variable is continuous or ordinal.<br>
- Predictor variable is ordinal.</td>
<td>Comparing medians or distributions across ordinal groups, e.g., treatment levels.</td>
</tr>
<tr class="odd">
<td>[<strong>Mantel Test for Trend</strong>]</td>
<td>Evaluates a linear association between an ordinal predictor and response.</td>
<td>- Ordinal variables.<br>
- Linear trend expected.</td>
<td>Determining trends in stratified or grouped data.</td>
</tr>
<tr class="even">
<td>[<strong>Chi-square Test for Linear Trend</strong>]</td>
<td>Tests for linear trends in categorical data using contingency tables.</td>
<td>- Contingency table with ordinal predictor.<br>
- Sufficient sample size (expected frequencies &gt; 5).</td>
<td>Analyzing trends in frequency data, e.g., examining disease prevalence by age groups.</td>
</tr>
</tbody>
</table></div>
</div>
</div>
</div>
<div id="divergence-metrics-and-tests-for-comparing-distributions" class="section level2" number="4.6">
<h2>
<span class="header-section-number">4.6</span> Divergence Metrics and Tests for Comparing Distributions<a class="anchor" aria-label="anchor" href="#divergence-metrics-and-tests-for-comparing-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>Divergence metrics are powerful tools used to measure the similarity or dissimilarity between probability distributions. Unlike deviation and deviance statistics, divergence metrics focus on the broader relationships between entire distributions, rather than individual data points or specific model fit metrics. Let’s clarify these differences:</p>
<ul>
<li>
<strong>Deviation Statistics</strong>: Measure the difference between the realization of a variable and some reference value (e.g., the mean). Common statistics derived from deviations include:
<ul>
<li>Standard deviation</li>
<li>Average absolute deviation</li>
<li>Median absolute deviation</li>
<li>Maximum absolute deviation</li>
</ul>
</li>
<li>
<strong>Deviance Statistics</strong>: Assess the goodness-of-fit of statistical models. These are analogous to the sum of squared residuals in ordinary least squares (OLS) but are generalized for use in cases with maximum likelihood estimation (MLE). Deviance statistics are frequently employed in generalized linear models (GLMs).</li>
</ul>
<p>Divergence statistics differ fundamentally by focusing on statistical distances between entire probability distributions, rather than on individual data points or model errors.</p>
<p><strong>1. Divergence Metrics</strong></p>
<ul>
<li><p><strong>Definition</strong>: Divergence metrics measure how much one probability distribution differs from another.</p></li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li><p><strong>Asymmetry</strong>: Many divergence metrics, such as Kullback-Leibler (KL) divergence, are not symmetric (i.e., <span class="math inline">\(D(P \|\| Q) \neq D(Q \|\| P)\)</span>).</p></li>
<li><p><strong>Non-Metric</strong>: They don’t necessarily satisfy the properties of a metric (e.g., symmetry, triangle inequality).</p></li>
<li><p><strong>Unitless</strong>: Divergences are often expressed in terms of information (e.g., bits or nats).</p></li>
</ul>
</li>
<li>
<p><strong>When to Use</strong>:</p>
<ul>
<li>Use divergence metrics to assess the degree of mismatch between two probability distributions, especially in machine learning, statistical inference, or model evaluation.</li>
</ul>
</li>
</ul>
<p><strong>2. Distance Metrics</strong></p>
<ul>
<li><p><strong>Definition</strong>: Distance metrics measure the “distance” or dissimilarity between two objects, including probability distributions, datasets, or points in space.</p></li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li><p><strong>Symmetry</strong>: <span class="math inline">\(D(P, Q) = D(Q, P)\)</span>.</p></li>
<li><p><strong>Triangle Inequality</strong>: <span class="math inline">\(D(P, R) \leq D(P, Q) + D(Q, R)\)</span>.</p></li>
<li><p><strong>Non-Negativity</strong>: <span class="math inline">\(D(P, Q) \geq 0\)</span>, with <span class="math inline">\(D(P, Q) = 0\)</span> only if <span class="math inline">\(P=Q\)</span>.</p></li>
</ul>
</li>
<li>
<p><strong>When to Use</strong>:</p>
<ul>
<li>Use distance metrics to compare datasets, distributions, or clustering outcomes where symmetry and geometric properties are important.</li>
</ul>
</li>
</ul>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="19%">
<col width="35%">
<col width="44%">
</colgroup>
<thead><tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Divergence Metrics</strong></th>
<th><strong>Distance Metrics</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Symmetry</strong></td>
<td>Often asymmetric (e.g., KL divergence).</td>
<td>Always symmetric (e.g., Wasserstein).</td>
</tr>
<tr class="even">
<td><strong>Triangle Inequality</strong></td>
<td>Not satisfied.</td>
<td>Satisfied.</td>
</tr>
<tr class="odd">
<td><strong>Use Case</strong></td>
<td>Quantifying how different distributions are.</td>
<td>Measuring the dissimilarity or “cost” of transformation.</td>
</tr>
</tbody>
</table></div>
<hr>
<p><strong>Applications of Divergence Metrics</strong></p>
<p>Divergence metrics have found wide utility across domains, including:</p>
<ul>
<li>
<strong>Detecting Data Drift in Machine Learning</strong>: Used to monitor whether the distribution of incoming data differs significantly from training data.</li>
<li>
<strong>Feature Selection</strong>: Employed to identify features with the most distinguishing power by comparing their distributions across different classes.</li>
<li>
<strong>Variational Autoencoders (VAEs)</strong>: Divergence metrics (such as Kullback-Leibler divergence) are central to the loss functions used in training VAEs.</li>
<li>
<strong>Reinforcement Learning</strong>: Measure the similarity between policy distributions to improve decision-making processes.</li>
<li>
<strong>Assessing Consistency</strong>: Compare the distributions of two variables representing constructs to test their relationship or agreement.</li>
</ul>
<p>Divergence metrics are also highly relevant in business settings, providing insights and solutions for a variety of applications, such as:</p>
<ul>
<li><p><strong>Customer Segmentation and Targeting</strong>: Compare the distributions of customer demographics or purchase behavior across market segments to identify key differences and target strategies more effectively.</p></li>
<li><p><strong>Market Basket Analysis</strong>: Measure divergence between distributions of product co-purchases across regions or customer groups to optimize product bundling and cross-selling strategies.</p></li>
<li><p><strong>Marketing Campaign Effectiveness</strong>: Evaluate whether the distribution of customer responses (e.g., click-through rates or conversions) differs significantly before and after a marketing campaign, providing insights into its success.</p></li>
<li><p><strong>Fraud Detection</strong>: Monitor divergence in transaction patterns over time to detect anomalies that may indicate fraudulent activities.</p></li>
<li><p><strong>Supply Chain Optimization</strong>: Compare demand distributions across time periods or regions to optimize inventory allocation and reduce stock-outs or overstocking.</p></li>
<li><p><strong>Pricing Strategy Evaluation</strong>: Analyze the divergence between pricing and purchase distributions across products or customer segments to refine pricing models and improve profitability.</p></li>
<li><p><strong>Churn Prediction</strong>: Compare distributions of engagement metrics (e.g., frequency of transactions or usage time) between customers likely to churn and those who stay, to design retention strategies.</p></li>
<li><p><strong>Financial Portfolio Analysis</strong>: Assess divergence between the expected returns and actual performance distributions of different asset classes to adjust investment strategies.</p></li>
</ul>
<hr>
<div id="kolmogorov-smirnov-test-1" class="section level3" number="4.6.1">
<h3>
<span class="header-section-number">4.6.1</span> Kolmogorov-Smirnov Test<a class="anchor" aria-label="anchor" href="#kolmogorov-smirnov-test-1"><i class="fas fa-link"></i></a>
</h3>
<p>The Kolmogorov-Smirnov (KS) test is a <strong>non-parametric test</strong> used to determine whether two distributions differ significantly or whether a sample distribution matches a reference distribution. It is applicable to <strong>continuous distributions</strong> and is widely used in hypothesis testing and model evaluation.</p>
<hr>
<p><strong>Mathematical Definition</strong></p>
<p>The KS statistic is defined as:</p>
<p><span class="math display">\[
D = \max |F_P(x) - F_Q(x)|
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(F_P(x)\)</span> is the cumulative distribution function (CDF) of the first distribution (or sample).</p></li>
<li><p><span class="math inline">\(F_Q(x)\)</span> is the CDF of the second distribution (or theoretical reference distribution).</p></li>
<li><p><span class="math inline">\(D\)</span> measures the maximum vertical distance between the two CDFs.</p></li>
</ul>
<p><strong>Hypotheses</strong></p>
<ul>
<li>
<strong>Null Hypothesis</strong> (<span class="math inline">\(H_0\)</span>): The empirical distribution follows a specified distribution (or the two samples are drawn from the same distribution).</li>
<li>
<strong>Alternative Hypothesis</strong> (<span class="math inline">\(H_1\)</span>): The empirical distribution does not follow the specified distribution (or the two samples are drawn from different distributions).</li>
</ul>
<p><strong>Properties of the KS Statistic</strong></p>
<ul>
<li>
<strong>Range</strong>: <span class="math display">\[
D \in [0, 1]
\]</span>
<ul>
<li>
<span class="math inline">\(D = 0\)</span>: Perfect match between the distributions.</li>
<li>
<span class="math inline">\(D = 1\)</span>: Maximum dissimilarity between the distributions.</li>
</ul>
</li>
<li>
<strong>Non-parametric Nature</strong>: The KS test makes no assumptions about the underlying distribution of the data.</li>
</ul>
<hr>
<p>The KS test is useful in various scenarios, including:</p>
<ol style="list-style-type: decimal">
<li>Comparing two empirical distributions to evaluate similarity.</li>
<li>Testing goodness-of-fit for a sample against a theoretical distribution.</li>
<li>Detecting data drift or shifts in distributions over time.</li>
<li>Validating simulation outputs by comparing them to real-world data.</li>
</ol>
<hr>
<p><strong>Example 1: Continuous Distributions</strong></p>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">stats</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate two sample distributions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">sample_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>        <span class="co"># Sample from a standard normal distribution</span></span>
<span><span class="va">sample_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Sample with mean shifted to 1</span></span>
<span></span>
<span><span class="co"># Perform Kolmogorov-Smirnov test</span></span>
<span><span class="va">ks_test_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ks.test.html">ks.test</a></span><span class="op">(</span><span class="va">sample_1</span>, <span class="va">sample_2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ks_test_result</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Asymptotic two-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  sample_1 and sample_2</span></span>
<span><span class="co">#&gt; D = 0.36, p-value = 4.705e-06</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span></code></pre></div>
<p>This compares the CDFs of the two samples. The p-value indicates whether the null hypothesis (that the samples come from the same distribution) can be rejected.</p>
<p><strong>Example 2: Discrete Data with Bootstrapped KS Test</strong></p>
<p>For discrete data, a bootstrapped version of the KS test is often used to bypass the continuity requirement.</p>
<div class="sourceCode" id="cb146"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/JasjeetSekhon/Matching">Matching</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define two discrete samples</span></span>
<span><span class="va">discrete_sample_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">discrete_sample_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform bootstrapped KS test</span></span>
<span><span class="va">ks_boot_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matching/man/ks.boot.html">ks.boot</a></span><span class="op">(</span>Tr <span class="op">=</span> <span class="va">discrete_sample_1</span>, Co <span class="op">=</span> <span class="va">discrete_sample_2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ks_boot_result</span><span class="op">)</span></span>
<span><span class="co">#&gt; $ks.boot.pvalue</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $ks</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Exact two-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  Tr and Co</span></span>
<span><span class="co">#&gt; D = 0, p-value = 1</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $nboots</span></span>
<span><span class="co">#&gt; [1] 1000</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attr(,"class")</span></span>
<span><span class="co">#&gt; [1] "ks.boot"</span></span></code></pre></div>
<p>This method performs a bootstrapped version of the KS test, suitable for discrete data. The p-value indicates whether the null hypothesis (that the samples come from the same distribution) can be rejected.</p>
<p><strong>Example 3: Comparing Multiple Distributions with KL Divergence (Optional Enhancement)</strong></p>
<p>If you wish to extend the analysis to include divergence measures like KL divergence, use the following:</p>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://strimmerlab.github.io/software/entropy/">entropy</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define multiple samples</span></span>
<span><span class="va">lst</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>sample_1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span>, sample_2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span><span class="op">:</span><span class="fl">30</span><span class="op">)</span>, sample_3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span><span class="op">:</span><span class="fl">30</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute KL divergence between all pairs of distributions</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">lst</span><span class="op">)</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">lst</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rowwise.html">rowwise</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>KL <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/entropy/man/entropy.empirical.html">KL.empirical</a></span><span class="op">(</span><span class="va">lst</span><span class="op">[[</span><span class="va">Var1</span><span class="op">]</span><span class="op">]</span>, <span class="va">lst</span><span class="op">[[</span><span class="va">Var2</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 9 × 3</span></span>
<span><span class="co">#&gt; # Rowwise: </span></span>
<span><span class="co">#&gt;    Var1  Var2     KL</span></span>
<span><span class="co">#&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1     1     1 0     </span></span>
<span><span class="co">#&gt; 2     2     1 0.150 </span></span>
<span><span class="co">#&gt; 3     3     1 0.183 </span></span>
<span><span class="co">#&gt; 4     1     2 0.704 </span></span>
<span><span class="co">#&gt; 5     2     2 0     </span></span>
<span><span class="co">#&gt; 6     3     2 0.0679</span></span>
<span><span class="co">#&gt; 7     1     3 0.622 </span></span>
<span><span class="co">#&gt; 8     2     3 0.0870</span></span>
<span><span class="co">#&gt; 9     3     3 0</span></span></code></pre></div>
<p>This calculates the KL divergence for all pairs of distributions in the list, offering additional insights into the relationships between the distributions.</p>
</div>
<div id="anderson-darling-test-1" class="section level3" number="4.6.2">
<h3>
<span class="header-section-number">4.6.2</span> Anderson-Darling Test<a class="anchor" aria-label="anchor" href="#anderson-darling-test-1"><i class="fas fa-link"></i></a>
</h3>
<p>The Anderson-Darling (AD) test is a <strong>goodness-of-fit test</strong> that evaluates whether a sample of data comes from a specific distribution. It is an enhancement of the <a href="descriptive-statistics.html#kolmogorov-smirnov-test">Kolmogorov-Smirnov test</a>, with greater sensitivity to deviations in the tails of the distribution.</p>
<hr>
<p>The Anderson-Darling test statistic is defined as:</p>
<p><span class="math display">\[
A^2 = -n - \frac{1}{n} \sum_{i=1}^n \left[ (2i - 1) \left( \log F(Y_i) + \log(1 - F(Y_{n+1-i})) \right) \right]
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> is the sample size.</p></li>
<li><p><span class="math inline">\(F\)</span> is the cumulative distribution function (CDF) of the theoretical distribution being tested.</p></li>
<li><p><span class="math inline">\(Y_i\)</span> are the ordered sample values.</p></li>
</ul>
<p>The AD test modifies the basic framework of the KS test by giving more weight to the tails of the distribution, making it particularly sensitive to tail discrepancies.</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li>
<strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The sample data follows the specified distribution.</li>
<li>
<strong>Alternative Hypothesis (</strong><span class="math inline">\(H_1\)</span>): The sample data does not follow the specified distribution.</li>
</ul>
<p><strong>Key Properties</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Tail Sensitivity</strong>: Unlike the Kolmogorov-Smirnov test, the Anderson-Darling test emphasizes discrepancies in the tails of the distribution.</li>
<li>
<strong>Distribution-Specific Critical Values</strong>: The AD test provides critical values tailored to the specific distribution being tested (e.g., normal, exponential).</li>
</ol>
<hr>
<p>The Anderson-Darling test is commonly used in:</p>
<ol style="list-style-type: decimal">
<li>Testing goodness-of-fit for a sample against theoretical distributions such as normal, exponential, or uniform.</li>
<li>Evaluating the appropriateness of parametric models in hypothesis testing.</li>
<li>Assessing distributional assumptions in quality control and reliability analysis.</li>
</ol>
<hr>
<p><strong>Example: Testing Normality with the Anderson-Darling Test</strong></p>
<div class="sourceCode" id="cb148"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">nortest</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate a sample from a normal distribution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">sample_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform the Anderson-Darling test for normality</span></span>
<span><span class="va">ad_test_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/nortest/man/ad.test.html">ad.test</a></span><span class="op">(</span><span class="va">sample_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ad_test_result</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Anderson-Darling normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  sample_data</span></span>
<span><span class="co">#&gt; A = 0.16021, p-value = 0.9471</span></span></code></pre></div>
<p>If the p-value is below a chosen significance level (e.g., 0.05), the null hypothesis that the data is normally distributed is rejected.</p>
<p><strong>Example: Comparing Two Empirical Distributions</strong></p>
<p>The AD test can also be applied to compare two empirical distributions using resampling techniques.</p>
<div class="sourceCode" id="cb149"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define two samples</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">sample_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">sample_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform resampling-based Anderson-Darling test (custom implementation or packages like twosamples)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://twosampletest.com">twosamples</a></span><span class="op">)</span></span>
<span><span class="va">ad_test_result_empirical</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://twosampletest.com/reference/ad_test.html">ad_test</a></span><span class="op">(</span><span class="va">sample_1</span>, <span class="va">sample_2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ad_test_result_empirical</span><span class="op">)</span></span>
<span><span class="co">#&gt;  Test Stat    P-Value </span></span>
<span><span class="co">#&gt; 6796.70454    0.00025</span></span></code></pre></div>
<p>This evaluates whether the two empirical distributions differ significantly.</p>
</div>
<div id="chi-square-goodness-of-fit-test" class="section level3" number="4.6.3">
<h3>
<span class="header-section-number">4.6.3</span> Chi-Square Goodness-of-Fit Test<a class="anchor" aria-label="anchor" href="#chi-square-goodness-of-fit-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Chi-Square Goodness-of-Fit Test</strong> is a non-parametric statistical test used to evaluate whether a sample data set comes from a population with a specific distribution. It compares observed frequencies with expected frequencies under a hypothesized distribution.</p>
<ul>
<li>
<strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The data follow the specified distribution.</li>
<li>
<strong>Alternative Hypothesis (</strong><span class="math inline">\(H_a\)</span>): The data do not follow the specified distribution.</li>
</ul>
<hr>
<p>The Chi-Square test statistic is computed as:</p>
<p><span class="math display">\[
\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(O_i\)</span>: Observed frequency for category <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(E_i\)</span>: Expected frequency for category <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(k\)</span>: Number of categories.</p></li>
</ul>
<p>The test statistic follows a Chi-Square distribution with degrees of freedom:</p>
<p><span class="math display">\[
\nu = k - 1 - p
\]</span></p>
<p>Where <span class="math inline">\(p\)</span> is the number of parameters estimated from the data.</p>
<hr>
<p><strong>Assumptions of the Test</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Random Sampling</strong>: The sample data are drawn randomly from the population.</li>
<li>
<strong>Minimum Expected Frequency</strong>: The expected frequencies <span class="math inline">\(E_i\)</span> are sufficiently large (typically <span class="math inline">\(E_i \geq 5\)</span>).</li>
<li>
<strong>Independence</strong>: Observations in the sample are independent of each other.</li>
</ol>
<hr>
<p>Decision Rule</p>
<ol style="list-style-type: decimal">
<li>Compute the test statistic <span class="math inline">\(\chi^2\)</span> using the observed and expected frequencies.</li>
<li>Determine the critical value <span class="math inline">\(\chi^2_{\alpha, \nu}\)</span> for the chosen significance level <span class="math inline">\(\alpha\)</span> and degrees of freedom <span class="math inline">\(\nu\)</span>.</li>
<li>Compare <span class="math inline">\(\chi^2\)</span> to <span class="math inline">\(\chi^2_{\alpha, \nu}\)</span>:
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\chi^2 &gt; \chi^2_{\alpha, \nu}\)</span>.</li>
<li>Alternatively, use the p-value approach:
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(p \leq \alpha\)</span>.</li>
<li>Fail to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(p &gt; \alpha\)</span>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<p>Steps for the Chi-Square Goodness-of-Fit Test</p>
<ol style="list-style-type: decimal">
<li>Define the expected frequencies based on the hypothesized distribution.</li>
<li>Compute the observed frequencies from the data.</li>
<li>Calculate the test statistic <span class="math inline">\(\chi^2\)</span>.</li>
<li>Determine the degrees of freedom <span class="math inline">\(\nu\)</span>.</li>
<li>Compare <span class="math inline">\(\chi^2\)</span> with the critical value or use the p-value for decision-making.</li>
</ol>
<hr>
<p><strong>Example: Testing a Fair Die</strong></p>
<p>Suppose you are testing whether a six-sided die is fair. The die is rolled 60 times, and the observed frequencies of the outcomes are:</p>
<ul>
<li>
<strong>Observed Frequencies</strong>: <span class="math inline">\([10, 12, 8, 11, 9, 10]\)</span>
</li>
<li>
<strong>Expected Frequencies</strong>: A fair die has equal probability for each face, so <span class="math inline">\(E_i = 60 / 6 = 10\)</span> for each face.</li>
</ul>
<div class="sourceCode" id="cb150"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Observed frequencies</span></span>
<span><span class="va">observed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">12</span>, <span class="fl">8</span>, <span class="fl">11</span>, <span class="fl">9</span>, <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Expected frequencies under a fair die</span></span>
<span><span class="va">expected</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">6</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform Chi-Square Goodness-of-Fit Test</span></span>
<span><span class="va">chisq_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">observed</span>, p <span class="op">=</span> <span class="va">expected</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">expected</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">chisq_test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Chi-squared test for given probabilities</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  observed</span></span>
<span><span class="co">#&gt; X-squared = 1, df = 5, p-value = 0.9626</span></span></code></pre></div>
<p><strong>Example: Testing a Loaded Die</strong></p>
<p>For a die with unequal probabilities (e.g., a loaded die), the expected probabilities are defined explicitly:</p>
<div class="sourceCode" id="cb151"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Observed frequencies</span></span>
<span><span class="va">observed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">12</span>, <span class="fl">8</span>, <span class="fl">11</span>, <span class="fl">9</span>, <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Expected probabilities (e.g., for a loaded die)</span></span>
<span><span class="va">probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Expected frequencies</span></span>
<span><span class="va">expected</span> <span class="op">&lt;-</span> <span class="va">probabilities</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">observed</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform Chi-Square Goodness-of-Fit Test</span></span>
<span><span class="va">chisq_test_loaded</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">observed</span>, p <span class="op">=</span> <span class="va">probabilities</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="va">chisq_test_loaded</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Chi-squared test for given probabilities</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  observed</span></span>
<span><span class="co">#&gt; X-squared = 15.806, df = 5, p-value = 0.007422</span></span></code></pre></div>
<p>Limitations of the Chi-Square Test</p>
<ol style="list-style-type: decimal">
<li><p><strong>Minimum Expected Frequency</strong>: If <span class="math inline">\(E_i &lt; 5\)</span> for any category, the test may lose power. Consider merging categories to meet this criterion.</p></li>
<li><p><strong>Independence</strong>: Assumes observations are independent. Violations of this assumption can invalidate the test.</p></li>
<li><p><strong>Sample Size Sensitivity</strong>: Large sample sizes may result in significant <span class="math inline">\(\chi\^2\)</span> values even for minor deviations from the expected distribution.</p></li>
</ol>
<p>The Chi-Square Goodness-of-Fit Test is a versatile tool for evaluating the fit of observed data to a hypothesized distribution, widely used in fields like quality control, genetics, and market research.</p>
</div>
<div id="cramér-von-mises-test" class="section level3" number="4.6.4">
<h3>
<span class="header-section-number">4.6.4</span> Cramér-von Mises Test<a class="anchor" aria-label="anchor" href="#cram%C3%A9r-von-mises-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Cramér-von Mises (CvM) Test</strong> is a goodness-of-fit test that evaluates whether a sample data set comes from a specified distribution. Similar to the <a href="descriptive-statistics.html#kolmogorov-smirnov-test">Kolmogorov-Smirnov Test</a> (KS) and <a href="descriptive-statistics.html#anderson-darling-test">Anderson-Darling Test</a> (AD), it assesses the discrepancy between the empirical and theoretical cumulative distribution functions (CDFs). However, the CvM test has <strong>equal sensitivity across the entire distribution</strong>, unlike the KS test (focused on the maximum difference) or the AD test (emphasizing the tails).</p>
<hr>
<p>The Cramér-von Mises test statistic is defined as:</p>
<p><span class="math display">\[
W^2 = n \int_{-\infty}^{\infty} \left( F_n(x) - F(x) \right)^2 dF(x)
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> is the sample size.</p></li>
<li><p><span class="math inline">\(F_n(x)\)</span> is the empirical cumulative distribution function (ECDF) of the sample.</p></li>
<li><p><span class="math inline">\(F(x)\)</span> is the CDF of the specified theoretical distribution.</p></li>
</ul>
<p>For practical implementation, the test statistic is often computed as:</p>
<p><span class="math display">\[
W^2 = \sum_{i=1}^n \left[ F(X_i) - \frac{2i - 1}{2n} \right]^2 + \frac{1}{12n}
\]</span></p>
<p>Where <span class="math inline">\(X_i\)</span> are the ordered sample values.</p>
<hr>
<p><strong>Hypotheses</strong></p>
<ul>
<li>
<strong>Null Hypothesis (</strong><span class="math inline">\(H_0\)</span>): The sample data follow the specified distribution.</li>
<li>
<strong>Alternative Hypothesis (</strong><span class="math inline">\(H_a\)</span>): The sample data do not follow the specified distribution.</li>
</ul>
<hr>
<p><strong>Key Properties</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Equal Sensitivity</strong>:
<ul>
<li>The CvM test gives equal weight to discrepancies across all parts of the distribution, unlike the AD test, which emphasizes the tails.</li>
</ul>
</li>
<li>
<strong>Non-parametric</strong>:
<ul>
<li>The test makes no strong parametric assumptions about the data, aside from the specified distribution.</li>
</ul>
</li>
<li>
<strong>Complementary to KS and AD Tests</strong>:
<ul>
<li>While the KS test focuses on the maximum distance between CDFs and the AD test emphasizes tails, the CvM test provides a balanced sensitivity across the entire range of the distribution.</li>
</ul>
</li>
</ol>
<hr>
<p>The Cramér-von Mises test is widely used in:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Goodness-of-Fit Testing</strong>: Assessing whether data follow a specified theoretical distribution (e.g., normal, exponential).</li>
<li>
<strong>Model Validation</strong>: Evaluating the fit of probabilistic models in statistical and machine learning contexts.</li>
<li>
<strong>Complementary Testing</strong>: Used alongside KS and AD tests for a comprehensive analysis of distributional assumptions.</li>
</ol>
<hr>
<p><strong>Example 1: Testing Normality</strong></p>
<div class="sourceCode" id="cb152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">nortest</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate a sample from a normal distribution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">sample_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform the Cramér-von Mises test for normality</span></span>
<span><span class="va">cvm_test_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/nortest/man/cvm.test.html">cvm.test</a></span><span class="op">(</span><span class="va">sample_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">cvm_test_result</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Cramer-von Mises normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  sample_data</span></span>
<span><span class="co">#&gt; W = 0.026031, p-value = 0.8945</span></span></code></pre></div>
<p>The test evaluates whether the sample data follow a normal distribution.</p>
<p><strong>Example 2: Goodness-of-Fit for Custom Distributions</strong></p>
<p>For distributions other than normal, you can use resampling techniques or custom implementations. Here’s a pseudo-implementation for a custom theoretical distribution:</p>
<div class="sourceCode" id="cb153"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Custom ECDF and theoretical CDF comparison</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">sample_data</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Exponential.html">rexp</a></span><span class="op">(</span><span class="fl">100</span>, rate <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Sample from exponential distribution</span></span>
<span><span class="va">theoretical_cdf</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/Exponential.html">pexp</a></span><span class="op">(</span><span class="va">x</span>, rate <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span>    <span class="op">}</span>  <span class="co"># Exponential CDF</span></span>
<span></span>
<span><span class="co"># Compute empirical CDF</span></span>
<span><span class="va">empirical_cdf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ecdf.html">ecdf</a></span><span class="op">(</span><span class="va">sample_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute CvM statistic</span></span>
<span><span class="va">cvm_statistic</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="fu">empirical_cdf</span><span class="op">(</span><span class="va">sample_data</span><span class="op">)</span> <span class="op">-</span> <span class="fu">theoretical_cdf</span><span class="op">(</span><span class="va">sample_data</span><span class="op">)</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">sample_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Cramér-von Mises Statistic (Custom):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">cvm_statistic</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Cramér-von Mises Statistic (Custom): 0.0019"</span></span></code></pre></div>
<p>This demonstrates a custom calculation of the CvM statistic for testing goodness-of-fit to an exponential distribution.</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Normality Test</strong>:</p>
<ul>
<li><p>The <code>cvm.test</code> function evaluates whether the sample data follow a normal distribution.</p></li>
<li><p>A small p-value indicates significant deviation from normality.</p></li>
</ul>
</li>
<li>
<p><strong>Custom Goodness-of-Fit</strong>:</p>
<ul>
<li><p>Custom implementation allows testing for distributions other than normal.</p></li>
<li><p>The statistic measures the squared differences between the empirical and theoretical CDFs.</p></li>
</ul>
</li>
</ol>
<p><strong>Advantages and Limitations</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Advantages</strong>:</p>
<ul>
<li><p>Balanced sensitivity across the entire distribution.</p></li>
<li><p>Complements KS and AD tests by providing a different perspective on goodness-of-fit.</p></li>
</ul>
</li>
<li>
<p><strong>Limitations</strong>:</p>
<ul>
<li><p>Critical values are distribution-specific.</p></li>
<li><p>The test may be less sensitive to tail deviations compared to the AD test.</p></li>
</ul>
</li>
</ol>
<p>The Cramér-von Mises test is a robust and versatile goodness-of-fit test, offering balanced sensitivity across the entire distribution. Its complementarity to KS and AD tests makes it an essential tool for validating distributional assumptions in both theoretical and applied contexts.</p>
</div>
<div id="kullback-leibler-divergence" class="section level3" number="4.6.5">
<h3>
<span class="header-section-number">4.6.5</span> Kullback-Leibler Divergence<a class="anchor" aria-label="anchor" href="#kullback-leibler-divergence"><i class="fas fa-link"></i></a>
</h3>
<p>Kullback-Leibler (KL) divergence, also known as <strong>relative entropy</strong>, is a measure used to quantify the similarity between two probability distributions. It plays a critical role in statistical inference, machine learning, and information theory. However, KL divergence is not a true metric as it does not satisfy the triangle inequality.</p>
<p><strong>Key Properties of KL Divergence</strong></p>
<ul>
<li><p><strong>Not a Metric</strong>: KL divergence fails to meet the triangle inequality requirement, and it is not symmetric, meaning: <span class="math display">\[
D_{KL}(P \| Q) \neq D_{KL}(Q \| P)
\]</span></p></li>
<li><p><strong>Generalization to Multivariate Case</strong>: KL divergence can be extended for multivariate distributions, making it flexible for complex analyses.</p></li>
<li><p><strong>Quantifies Information Loss</strong>: It measures the “information loss” when approximating the true distribution <span class="math inline">\(P\)</span> with the predicted distribution <span class="math inline">\(Q\)</span>. Thus, smaller values indicate closer similarity between the distributions.</p></li>
</ul>
<hr>
<p><strong>Mathematical Definitions</strong></p>
<p>KL divergence is defined differently for discrete and continuous distributions.</p>
<p><strong>1. Discrete Case</strong><br>
For two discrete probability distributions <span class="math inline">\(P = \{P_i\}\)</span> and <span class="math inline">\(Q = \{Q_i\}\)</span>, the KL divergence is given by: <span class="math display">\[
D_{KL}(P \| Q) = \sum_i P_i \log\left(\frac{P_i}{Q_i}\right)
\]</span></p>
<p><strong>2. Continuous Case</strong><br>
For continuous probability density functions <span class="math inline">\(P(x)\)</span> and <span class="math inline">\(Q(x)\)</span>: <span class="math display">\[
D_{KL}(P \| Q) = \int P(x) \log\left(\frac{P(x)}{Q(x)}\right) dx
\]</span></p>
<hr>
<ul>
<li>
<strong>Range</strong>: <span class="math display">\[
D_{KL}(P \| Q) \in [0, \infty)
\]</span>
<ul>
<li>
<span class="math inline">\(D_{KL} = 0\)</span> indicates identical distributions (<span class="math inline">\(P = Q\)</span>).</li>
<li>Larger values indicate greater dissimilarity between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</li>
</ul>
</li>
<li>
<strong>Non-Symmetric Nature</strong>: As noted, <span class="math inline">\(D_{KL}(P \| Q)\)</span> and <span class="math inline">\(D_{KL}(Q \| P)\)</span> are not equal, emphasizing its directed nature.</li>
</ul>
<hr>
<div class="sourceCode" id="cb154"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/drostlab/philentropy">philentropy</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example 1: Continuous case</span></span>
<span><span class="co"># Define two continuous probability distributions with distinct patterns</span></span>
<span><span class="va">X_continuous</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span><span class="op">)</span>  <span class="co"># Normalized to sum to 1</span></span>
<span><span class="va">Y_continuous</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span><span class="op">)</span>  <span class="co"># Normalized to sum to 1</span></span>
<span></span>
<span><span class="co"># Calculate KL divergence (logarithm base 2)</span></span>
<span><span class="va">KL_continuous</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/KL.html">KL</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">X_continuous</span>, <span class="va">Y_continuous</span><span class="op">)</span>, unit <span class="op">=</span> <span class="st">"log2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"KL divergence (continuous):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">KL_continuous</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "KL divergence (continuous): 0.66"</span></span>
<span></span>
<span><span class="co"># Example 2: Discrete case</span></span>
<span><span class="co"># Define two discrete probability distributions</span></span>
<span><span class="va">X_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">15</span>, <span class="fl">20</span><span class="op">)</span>  <span class="co"># Counts for events</span></span>
<span><span class="va">Y_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">15</span>, <span class="fl">10</span>, <span class="fl">5</span><span class="op">)</span>  <span class="co"># Counts for events</span></span>
<span></span>
<span><span class="co"># Estimate probabilities empirically and compute KL divergence</span></span>
<span><span class="va">KL_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/KL.html">KL</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">X_discrete</span>, <span class="va">Y_discrete</span><span class="op">)</span>, est.prob <span class="op">=</span> <span class="st">"empirical"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"KL divergence (discrete):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">KL_discrete</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "KL divergence (discrete): 0.66"</span></span></code></pre></div>
<p>Insights:</p>
<ol style="list-style-type: decimal">
<li>Continuous case uses normalized probability values explicitly provided.</li>
<li>Discrete case relies on empirical estimation of probabilities from counts.</li>
<li>Observe how KL divergence quantifies the “distance” between the two distributions.</li>
</ol>
</div>
<div id="jensen-shannon-divergence" class="section level3" number="4.6.6">
<h3>
<span class="header-section-number">4.6.6</span> Jensen-Shannon Divergence<a class="anchor" aria-label="anchor" href="#jensen-shannon-divergence"><i class="fas fa-link"></i></a>
</h3>
<p>Jensen-Shannon (JS) divergence is a symmetric and bounded measure of the similarity between two probability distributions. It is derived from the <a href="basic-statistical-inference.html#kullback-leibler-divergence">Kullback-Leibler Divergence</a> (KL) but addresses its asymmetry and unboundedness by incorporating a mixed distribution.</p>
<p>The Jensen-Shannon divergence is defined as: <span class="math display">\[
D_{JS}(P \| Q) = \frac{1}{2} \left( D_{KL}(P \| M) + D_{KL}(Q \| M) \right)
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(M = \frac{1}{2}(P + Q)\)</span> is the <strong>mixed distribution</strong>, representing the average of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</p></li>
<li><p><span class="math inline">\(D_{KL}\)</span> is the Kullback-Leibler divergence.</p></li>
</ul>
<p><strong>Key Properties</strong></p>
<ul>
<li><p><strong>Symmetry</strong>: Unlike KL divergence, JS divergence is symmetric: <span class="math display">\[
D_{JS}(P \| Q) = D_{JS}(Q \| P)
\]</span></p></li>
<li>
<p><strong>Boundedness</strong>:</p>
<ul>
<li>For base-2 logarithms: <span class="math display">\[
D_{JS} \in [0, 1]
\]</span>
</li>
<li>For natural logarithms (base-<span class="math inline">\(e\)</span>): <span class="math display">\[
D_{JS} \in [0, \ln(2)]
\]</span>
</li>
</ul>
</li>
<li><p><strong>Interpretability</strong>: The JS divergence measures the average information gain when moving from the mixed distribution <span class="math inline">\(M\)</span> to either <span class="math inline">\(P\)</span> or <span class="math inline">\(Q\)</span>. Its bounded nature makes it easier to compare across datasets.</p></li>
</ul>
<hr>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load the required library</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/drostlab/philentropy">philentropy</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example 1: Continuous case</span></span>
<span><span class="co"># Define two continuous distributions</span></span>
<span><span class="va">X_continuous</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>  <span class="co"># Continuous sequence</span></span>
<span><span class="va">Y_continuous</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span>  <span class="co"># Continuous sequence</span></span>
<span></span>
<span><span class="co"># Compute JS divergence (logarithm base 2)</span></span>
<span><span class="va">JS_continuous</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/JSD.html">JSD</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">X_continuous</span>, <span class="va">Y_continuous</span><span class="op">)</span>, unit <span class="op">=</span> <span class="st">"log2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"JS divergence (continuous):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">JS_continuous</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "JS divergence (continuous): 20.03"</span></span>
<span></span>
<span><span class="co"># X_continuous and Y_continuous represent continuous distributions.</span></span>
<span><span class="co"># The mixed distribution (M) is computed internally as the average of the two distributions.</span></span>
<span></span>
<span><span class="co"># Example 2: Discrete case</span></span>
<span><span class="co"># Define two discrete distributions</span></span>
<span><span class="va">X_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">15</span>, <span class="fl">20</span><span class="op">)</span>  <span class="co"># Observed counts for events</span></span>
<span><span class="va">Y_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">15</span>, <span class="fl">10</span>, <span class="fl">5</span><span class="op">)</span>  <span class="co"># Observed counts for events</span></span>
<span></span>
<span><span class="co"># Compute JS divergence with empirical probability estimation</span></span>
<span><span class="va">JS_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/JSD.html">JSD</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">X_discrete</span>, <span class="va">Y_discrete</span><span class="op">)</span>, est.prob <span class="op">=</span> <span class="st">"empirical"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"JS divergence (discrete):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">JS_discrete</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "JS divergence (discrete): 0.15"</span></span>
<span></span>
<span><span class="co"># X_discrete and Y_discrete represent event counts.</span></span>
<span><span class="co"># Probabilities are estimated empirically before calculating the divergence.</span></span></code></pre></div>
</div>
<div id="hellinger-distance" class="section level3" number="4.6.7">
<h3>
<span class="header-section-number">4.6.7</span> Hellinger Distance<a class="anchor" aria-label="anchor" href="#hellinger-distance"><i class="fas fa-link"></i></a>
</h3>
<p>The Hellinger distance is a bounded and symmetric measure of similarity between two probability distributions. It is widely used in statistics and machine learning to quantify how “close” two distributions are, with values ranging between 0 (identical distributions) and 1 (completely disjoint distributions).</p>
<hr>
<p><strong>Mathematical Definition</strong></p>
<p>The Hellinger distance between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as:</p>
<p><span class="math display">\[
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_x \left(\sqrt{P(x)} - \sqrt{Q(x)}\right)^2}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(P(x)\)</span> and <span class="math inline">\(Q(x)\)</span> are the probability densities or probabilities at point <span class="math inline">\(x\)</span> for the distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</p></li>
<li><p>The term <span class="math inline">\(\sqrt{P(x)}\)</span> is the square root of the probabilities, emphasizing geometric comparisons between the distributions.</p></li>
</ul>
<p>Alternatively, for continuous distributions, the Hellinger distance can be expressed as:</p>
<p><span class="math display">\[
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\int \left(\sqrt{P(x)} - \sqrt{Q(x)}\right)^2 dx}
\]</span></p>
<p><strong>Key Properties</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Symmetry</strong>: <span class="math display">\[
H(P, Q) = H(Q, P)
\]</span> The distance is symmetric, unlike Kullback-Leibler divergence.</p></li>
<li>
<p><strong>Boundedness</strong>: <span class="math display">\[
H(P, Q) \in [0, 1]
\]</span></p>
<ul>
<li>
<span class="math inline">\(H = 0\)</span>: The distributions are identical (<span class="math inline">\(P(x) = Q(x)\)</span> for all <span class="math inline">\(x\)</span>).</li>
<li>
<span class="math inline">\(H = 1\)</span>: The distributions have no overlap (<span class="math inline">\(P(x) \neq Q(x)\)</span>).</li>
</ul>
</li>
<li>
<p><strong>Interpretability</strong>:</p>
<ul>
<li>Hellinger distance provides a scale-invariant measure, making it suitable for comparing distributions in various contexts.</li>
</ul>
</li>
</ol>
<hr>
<p>Hellinger distance is widely used in:</p>
<ul>
<li>
<strong>Hypothesis Testing</strong>: Comparing empirical distributions to theoretical models.</li>
<li>
<strong>Machine Learning</strong>: Feature selection, classification, and clustering tasks.</li>
<li>
<strong>Bayesian Analysis</strong>: Quantifying differences between prior and posterior distributions.</li>
<li>
<strong>Economics and Ecology</strong>: Measuring dissimilarity in distributions like income, species abundance, or geographical data.</li>
</ul>
<hr>
<div class="sourceCode" id="cb156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/drostlab/philentropy">philentropy</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example 1: Compute Hellinger Distance for Discrete Distributions</span></span>
<span><span class="co"># Define two discrete distributions as probabilities</span></span>
<span><span class="va">P_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span><span class="op">)</span>  <span class="co"># Normalized probabilities</span></span>
<span><span class="va">Q_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span><span class="op">)</span>  <span class="co"># Normalized probabilities</span></span>
<span></span>
<span><span class="co"># Compute Hellinger distance</span></span>
<span><span class="va">hellinger_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/distance.html">distance</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">P_discrete</span>, <span class="va">Q_discrete</span><span class="op">)</span>, method <span class="op">=</span> <span class="st">"hellinger"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Hellinger Distance (Discrete):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">hellinger_discrete</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Hellinger Distance (Discrete): 0.465"</span></span>
<span></span>
<span><span class="co"># Example 2: Compute Hellinger Distance for Empirical Distributions</span></span>
<span><span class="co"># Define two empirical distributions (counts)</span></span>
<span><span class="va">P_empirical</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">20</span>, <span class="fl">30</span>, <span class="fl">40</span><span class="op">)</span>  <span class="co"># Counts for distribution P</span></span>
<span><span class="va">Q_empirical</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">30</span>, <span class="fl">20</span>, <span class="fl">20</span><span class="op">)</span>  <span class="co"># Counts for distribution Q</span></span>
<span></span>
<span><span class="co"># Normalize counts to probabilities</span></span>
<span><span class="va">P_normalized</span> <span class="op">&lt;-</span> <span class="va">P_empirical</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">P_empirical</span><span class="op">)</span></span>
<span><span class="va">Q_normalized</span> <span class="op">&lt;-</span> <span class="va">Q_empirical</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Q_empirical</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Hellinger distance</span></span>
<span><span class="va">hellinger_empirical</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/distance.html">distance</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">P_normalized</span>, <span class="va">Q_normalized</span><span class="op">)</span>, method <span class="op">=</span> <span class="st">"hellinger"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Hellinger Distance (Empirical):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">hellinger_empirical</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Hellinger Distance (Empirical): 0.465"</span></span></code></pre></div>
</div>
<div id="bhattacharyya-distance" class="section level3" number="4.6.8">
<h3>
<span class="header-section-number">4.6.8</span> Bhattacharyya Distance<a class="anchor" aria-label="anchor" href="#bhattacharyya-distance"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Bhattacharyya Distance</strong> is a statistical measure used to quantify the similarity or overlap between two probability distributions. It is commonly used in pattern recognition, signal processing, and statistics to evaluate how closely related two distributions are. The Bhattacharyya distance is particularly effective for comparing both discrete and continuous distributions.</p>
<hr>
<p>The Bhattacharyya distance between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as:</p>
<p><span class="math display">\[
D_B(P, Q) = -\ln \left( \sum_x \sqrt{P(x) Q(x)} \right)
\]</span></p>
<p>For continuous distributions, the Bhattacharyya distance is expressed as:</p>
<p><span class="math display">\[
D_B(P, Q) = -\ln \left( \int \sqrt{P(x) Q(x)} dx \right)
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(P(x)\)</span> and <span class="math inline">\(Q(x)\)</span> are the probability densities or probabilities for the distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</p></li>
<li><p>The term <span class="math inline">\(\int \sqrt{P(x) Q(x)} dx\)</span> is known as the <strong>Bhattacharyya coefficient</strong>.</p></li>
</ul>
<hr>
<p><strong>Key Properties</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Symmetry</strong>: <span class="math display">\[
D_B(P, Q) = D_B(Q, P)
\]</span></p></li>
<li>
<p><strong>Range</strong>: <span class="math display">\[
D_B(P, Q) \in [0, \infty)
\]</span></p>
<ul>
<li>
<span class="math inline">\(D_B = 0\)</span>: The distributions are identical (<span class="math inline">\(P = Q\)</span>).</li>
<li>Larger values indicate less overlap and greater dissimilarity between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</li>
</ul>
</li>
<li>
<p><strong>Relation to Hellinger Distance</strong>:</p>
<ul>
<li>The Bhattacharyya coefficient is related to the Hellinger distance: <span class="math display">\[
H(P, Q) = \sqrt{1 - \sum_x \sqrt{P(x) Q(x)}}
\]</span>
</li>
</ul>
</li>
</ol>
<hr>
<p>The Bhattacharyya distance is widely used in:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Classification</strong>: Measuring the similarity between feature distributions in machine learning.</li>
<li>
<strong>Hypothesis Testing</strong>: Evaluating the closeness of observed data to a theoretical model.</li>
<li>
<strong>Image Processing</strong>: Comparing pixel intensity distributions or color histograms.</li>
<li>
<strong>Economics and Ecology</strong>: Assessing similarity in income distributions or species abundance.</li>
</ol>
<hr>
<p><strong>Example 1: Discrete Distributions</strong></p>
<div class="sourceCode" id="cb157"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define two discrete probability distributions</span></span>
<span><span class="va">P_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span><span class="op">)</span>  <span class="co"># Normalized probabilities</span></span>
<span><span class="va">Q_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span><span class="op">)</span>  <span class="co"># Normalized probabilities</span></span>
<span></span>
<span><span class="co"># Compute Bhattacharyya coefficient</span></span>
<span><span class="va">bhattacharyya_coefficient</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">P_discrete</span> <span class="op">*</span> <span class="va">Q_discrete</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Bhattacharyya distance</span></span>
<span><span class="va">bhattacharyya_distance</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">bhattacharyya_coefficient</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span></span>
<span>    <span class="st">"Bhattacharyya Coefficient:"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">bhattacharyya_coefficient</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Bhattacharyya Coefficient: 0.9459"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span></span>
<span>    <span class="st">"Bhattacharyya Distance (Discrete):"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">bhattacharyya_distance</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Bhattacharyya Distance (Discrete): 0.0556"</span></span></code></pre></div>
<p>A smaller Bhattacharyya distance indicates greater similarity between the two distributions.</p>
<p><strong>Example 2: Continuous Distributions (Approximation)</strong></p>
<p>For continuous distributions, the Bhattacharyya distance can be approximated using numerical integration or discretization.</p>
<div class="sourceCode" id="cb158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate two continuous distributions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">P_continuous</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Standard normal distribution</span></span>
<span><span class="va">Q_continuous</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Normal distribution with mean 1</span></span>
<span></span>
<span><span class="co"># Create histograms to approximate probabilities</span></span>
<span><span class="va">hist_P</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">P_continuous</span>, breaks <span class="op">=</span> <span class="fl">50</span>, plot <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">hist_Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">Q_continuous</span>, breaks <span class="op">=</span> <span class="fl">50</span>, plot <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Normalize histograms to probabilities</span></span>
<span><span class="va">prob_P</span> <span class="op">&lt;-</span> <span class="va">hist_P</span><span class="op">$</span><span class="va">counts</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">hist_P</span><span class="op">$</span><span class="va">counts</span><span class="op">)</span></span>
<span><span class="va">prob_Q</span> <span class="op">&lt;-</span> <span class="va">hist_Q</span><span class="op">$</span><span class="va">counts</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">hist_Q</span><span class="op">$</span><span class="va">counts</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Bhattacharyya coefficient</span></span>
<span><span class="va">bhattacharyya_coefficient_continuous</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">prob_P</span> <span class="op">*</span> <span class="va">prob_Q</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Bhattacharyya distance</span></span>
<span><span class="va">bhattacharyya_distance_continuous</span> <span class="op">&lt;-</span></span>
<span>    <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">bhattacharyya_coefficient_continuous</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span></span>
<span>    <span class="st">"Bhattacharyya Coefficient (Continuous):"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">bhattacharyya_coefficient_continuous</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Bhattacharyya Coefficient (Continuous): 0.9823"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span></span>
<span>    <span class="st">"Bhattacharyya Distance (Continuous Approximation):"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">bhattacharyya_distance_continuous</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Bhattacharyya Distance (Continuous Approximation): 0.0178"</span></span></code></pre></div>
<p>Continuous distributions are discretized into histograms to compute the Bhattacharyya coefficient and distance.</p>
<ol style="list-style-type: decimal">
<li>Discrete Case:
<ol style="list-style-type: decimal">
<li>The Bhattacharyya coefficient quantifies the overlap between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</li>
<li>The Bhattacharyya distance translates this overlap into a logarithmic measure of dissimilarity.</li>
</ol>
</li>
<li>Continuous Case:
<ol style="list-style-type: decimal">
<li>Distributions are discretized into histograms to approximate the Bhattacharyya coefficient and distance.</li>
</ol>
</li>
</ol>
</div>
<div id="wasserstein-distance" class="section level3" number="4.6.9">
<h3>
<span class="header-section-number">4.6.9</span> Wasserstein Distance<a class="anchor" aria-label="anchor" href="#wasserstein-distance"><i class="fas fa-link"></i></a>
</h3>
<p>The Wasserstein distance, also known as the <strong>Earth Mover’s Distance (EMD)</strong>, is a measure of similarity between two probability distributions. It quantifies the “cost” of transforming one distribution into another, making it particularly suitable for continuous data and applications where the geometry of the data matters.</p>
<hr>
<p><strong>Mathematical Definition</strong></p>
<p>The Wasserstein distance between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> over a domain <span class="math inline">\(\mathcal{X}\)</span> is defined as:</p>
<p><span class="math display">\[
W_p(P, Q) = \left( \int_{\mathcal{X}} |F_P(x) - F_Q(x)|^p dx \right)^{\frac{1}{p}}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(F_P(x)\)</span> and <span class="math inline">\(F_Q(x)\)</span> are the cumulative distribution functions (CDFs) of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</p></li>
<li><p><span class="math inline">\(p \geq 1\)</span> is the order of the Wasserstein distance (commonly <span class="math inline">\(p = 1\)</span>).</p></li>
<li><p><span class="math inline">\(|\cdot|^p\)</span> is the absolute difference raised to the power <span class="math inline">\(p\)</span>.</p></li>
</ul>
<p>For the case of <span class="math inline">\(p = 1\)</span>, the formula simplifies to:</p>
<p><span class="math display">\[
W_1(P, Q) = \int_{\mathcal{X}} |F_P(x) - F_Q(x)| dx
\]</span></p>
<p>This represents the minimum “cost” of transforming the distribution <span class="math inline">\(P\)</span> into <span class="math inline">\(Q\)</span>, where cost is proportional to the distance a “unit of mass” must move.</p>
<hr>
<p><strong>Key Properties</strong></p>
<ul>
<li>
<strong>Interpretability</strong>: Represents the “effort” required to morph one distribution into another.</li>
<li>
<strong>Metric</strong>: Wasserstein distance satisfies the properties of a metric, including symmetry, non-negativity, and the triangle inequality.</li>
<li>
<strong>Flexibility</strong>: Can handle both empirical and continuous distributions.</li>
</ul>
<hr>
<p>Wasserstein distance is widely used in various fields, including:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Machine Learning</strong>:
<ul>
<li>Training generative models such as Wasserstein GANs.</li>
<li>Monitoring data drift in online systems.</li>
</ul>
</li>
<li>
<strong>Statistics</strong>:
<ul>
<li>Comparing empirical distributions derived from observed data.</li>
<li>Robustness testing under distributional shifts.</li>
</ul>
</li>
<li>
<strong>Economics</strong>:
<ul>
<li>Quantifying disparities in income or wealth distributions.</li>
</ul>
</li>
<li>
<strong>Image Processing</strong>:
<ul>
<li>Measuring structural differences between image distributions.</li>
</ul>
</li>
</ol>
<hr>
<div class="sourceCode" id="cb159"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dschuhm1.pages.gwdg.de/software">transport</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://twosampletest.com">twosamples</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example 1: Compute Wasserstein Distance (1D case)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">dist_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>               <span class="co"># Generate a sample from a standard normal distribution</span></span>
<span><span class="va">dist_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>     <span class="co"># Generate a sample with mean shifted to 1</span></span>
<span></span>
<span><span class="co"># Calculate the Wasserstein distance</span></span>
<span><span class="va">wass_distance</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/transport/man/wasserstein1d.html">wasserstein1d</a></span><span class="op">(</span><span class="va">dist_1</span>, <span class="va">dist_2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"1D Wasserstein Distance:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">wass_distance</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "1D Wasserstein Distance: 0.8533"</span></span>
<span></span>
<span><span class="co"># Example 2: Wasserstein Metric as a Statistic</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">wass_stat_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://twosampletest.com/reference/wass_test.html">wass_stat</a></span><span class="op">(</span><span class="va">dist_1</span>, <span class="va">dist_2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Wasserstein Statistic:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">wass_stat_value</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Wasserstein Statistic: 0.8533"</span></span>
<span></span>
<span><span class="co"># Example 3: Wasserstein Test (Permutation-based Two-sample Test)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">wass_test_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://twosampletest.com/reference/wass_test.html">wass_test</a></span><span class="op">(</span><span class="va">dist_1</span>, <span class="va">dist_2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">wass_test_result</span><span class="op">)</span></span>
<span><span class="co">#&gt; Test Stat   P-Value </span></span>
<span><span class="co">#&gt; 0.8533046 0.0002500</span></span>
<span></span>
<span><span class="co"># - Example 1 calculates the simple Wasserstein distance between two distributions.</span></span>
<span><span class="co"># - Example 2 computes the Wasserstein distance as a statistical metric.</span></span>
<span><span class="co"># - Example 3 performs a permutation-based two-sample test using the Wasserstein metric.</span></span></code></pre></div>
</div>
<div id="energy-distance" class="section level3" number="4.6.10">
<h3>
<span class="header-section-number">4.6.10</span> Energy Distance<a class="anchor" aria-label="anchor" href="#energy-distance"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Energy Distance</strong> is a statistical metric used to quantify the similarity between two probability distributions. It is particularly effective for comparing multi-dimensional distributions.</p>
<hr>
<p>The Energy Distance between two distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as:</p>
<p><span class="math display">\[
E(P, Q) = 2 \mathbb{E}[||X - Y||] - \mathbb{E}[||X - X'||] - \mathbb{E}[||Y - Y'||]
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(X\)</span> and <span class="math inline">\(X'\)</span> are independent and identically distributed (i.i.d.) random variables from <span class="math inline">\(P\)</span>.</p></li>
<li><p><span class="math inline">\(Y\)</span> and <span class="math inline">\(Y'\)</span> are i.i.d. random variables from <span class="math inline">\(Q\)</span>.</p></li>
<li><p><span class="math inline">\(||\cdot||\)</span> denotes the Euclidean distance.</p></li>
</ul>
<p>Alternatively, for empirical distributions, the Energy Distance can be approximated as:</p>
<p><span class="math display">\[
E(P, Q) = \frac{2}{mn} \sum_{i=1}^m \sum_{j=1}^n ||X_i - Y_j|| - \frac{1}{m^2} \sum_{i=1}^m \sum_{j=1}^m ||X_i - X_j|| - \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n ||Y_i - Y_j||
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are the sample sizes from distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> respectively.</p></li>
<li><p><span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_j\)</span> are samples from <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</p></li>
</ul>
<hr>
<p><strong>Key Properties</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Metric</strong>:
<ul>
<li>Energy distance satisfies the properties of a metric: symmetry, non-negativity, and the triangle inequality.</li>
</ul>
</li>
<li>
<strong>Range</strong>: <span class="math display">\[
E(P, Q) \geq 0
\]</span>
<ul>
<li>
<span class="math inline">\(E(P, Q) = 0\)</span>: The distributions are identical.</li>
<li>Larger values indicate greater dissimilarity.</li>
</ul>
</li>
<li>
<strong>Effectiveness for Multi-dimensional Data</strong>:
<ul>
<li>Energy distance is designed to work well in higher-dimensional spaces, unlike some traditional metrics.</li>
</ul>
</li>
</ol>
<hr>
<p>The Energy Distance is widely used in:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Hypothesis Testing</strong>: Testing whether two distributions are the same.</li>
<li>
<strong>Energy Test</strong> for equality of distributions.</li>
<li>
<strong>Clustering</strong>: Measuring dissimilarity between clusters in multi-dimensional data.</li>
<li>
<strong>Feature Selection</strong>: Comparing distributions of features across different classes to evaluate their discriminative power.</li>
</ol>
<hr>
<p><strong>Example 1: Comparing Two Distributions</strong></p>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load the 'energy' package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/mariarizzo/energy">energy</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate two sample distributions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># Distribution P</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># Distribution Q</span></span>
<span></span>
<span><span class="co"># Combine X and Y and create a group identifier</span></span>
<span><span class="va">combined</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span><span class="op">)</span></span>
<span><span class="va">groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Energy Distance</span></span>
<span><span class="va">energy_dist</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/energy/man/edist.html">edist</a></span><span class="op">(</span><span class="va">combined</span>, sizes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">groups</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print the Energy Distance</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Energy Distance:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">energy_dist</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Energy Distance: 201.9202"</span></span></code></pre></div>
<p>This calculates the energy distance between two multi-dimensional distributions.</p>
<p>Example 2: Energy Test for Equality of Distributions</p>
<div class="sourceCode" id="cb161"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Perform the Energy Test</span></span>
<span><span class="va">energy_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/energy/man/eqdist.etest.html">eqdist.etest</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span><span class="op">)</span>, sizes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">)</span>, R <span class="op">=</span> <span class="fl">999</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">energy_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Multivariate 2-sample E-test of equal distributions</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  sample sizes 500 500, replicates 999</span></span>
<span><span class="co">#&gt; E-statistic = 201.92, p-value = 0.001</span></span></code></pre></div>
<p>The energy test evaluates the null hypothesis that the two distributions are identical.</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Energy Distance</strong>:</p>
<ul>
<li>Provides a single metric to quantify the dissimilarity between two distributions, considering all dimensions of the data.</li>
</ul>
</li>
<li>
<p><strong>Energy Test</strong>:</p>
<ul>
<li><p>Tests for equality of distributions using Energy Distance.</p></li>
<li><p>The p-value indicates whether the distributions are significantly different.</p></li>
</ul>
</li>
</ol>
<p>Advantages of Energy Distance</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Multi-dimensional Applicability</strong>:</p>
<ul>
<li>Works seamlessly with high-dimensional data, unlike some divergence metrics which may suffer from dimensionality issues.</li>
</ul>
</li>
<li>
<p><strong>Non-parametric</strong>:</p>
<ul>
<li>Makes no assumptions about the form of the distributions.</li>
</ul>
</li>
<li>
<p><strong>Robustness</strong>:</p>
<ul>
<li>Effective even with complex data structures.</li>
</ul>
</li>
</ol>
</div>
<div id="total-variation-distance" class="section level3" number="4.6.11">
<h3>
<span class="header-section-number">4.6.11</span> Total Variation Distance<a class="anchor" aria-label="anchor" href="#total-variation-distance"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Total Variation (TV) Distance</strong> is a measure of the maximum difference between two probability distributions. It is widely used in probability theory, statistics, and machine learning to quantify how dissimilar two distributions are.</p>
<hr>
<p>The Total Variation Distance between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as:</p>
<p><span class="math display">\[
D_{TV}(P, Q) = \frac{1}{2} \sum_x |P(x) - Q(x)|
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(P(x)\)</span> and <span class="math inline">\(Q(x)\)</span> are the probabilities assigned to the outcome <span class="math inline">\(x\)</span> by the distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>.</p></li>
<li><p>The factor <span class="math inline">\(\frac{1}{2}\)</span> ensures that the distance lies within the range <span class="math inline">\([0, 1]\)</span>.</p></li>
</ul>
<p>Alternatively, for continuous distributions, the TV distance can be expressed as:</p>
<p><span class="math display">\[
D_{TV}(P, Q) = \frac{1}{2} \int |P(x) - Q(x)| dx
\]</span></p>
<p><strong>Key Properties</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Range</strong>: <span class="math display">\[
D_{TV}(P, Q) \in [0, 1]
\]</span></p>
<ul>
<li>
<span class="math inline">\(D_{TV} = 0\)</span>: The distributions are identical (<span class="math inline">\(P = Q\)</span>).</li>
<li>
<span class="math inline">\(D_{TV} = 1\)</span>: The distributions are completely disjoint (no overlap).</li>
</ul>
</li>
<li><p><strong>Symmetry</strong>: <span class="math display">\[
D_{TV}(P, Q) = D_{TV}(Q, P)
\]</span></p></li>
<li>
<p><strong>Interpretability</strong>:</p>
<ul>
<li>
<span class="math inline">\(D_{TV}(P, Q)\)</span> represents the maximum probability mass that needs to be shifted to transform <span class="math inline">\(P\)</span> into <span class="math inline">\(Q\)</span>.</li>
</ul>
</li>
</ol>
<hr>
<p>The Total Variation Distance is used in:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Hypothesis Testing</strong>: Quantifying the difference between observed and expected distributions.</li>
<li>
<strong>Machine Learning</strong>: Evaluating similarity between predicted and true distributions.</li>
<li>
<strong>Information Theory</strong>: Comparing distributions in contexts like communication and cryptography.</li>
</ol>
<hr>
<p><strong>Example 1: Discrete Distributions</strong></p>
<div class="sourceCode" id="cb162"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define two discrete probability distributions</span></span>
<span><span class="va">P_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span><span class="op">)</span>  <span class="co"># Normalized probabilities</span></span>
<span><span class="va">Q_discrete</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span><span class="op">)</span>  <span class="co"># Normalized probabilities</span></span>
<span></span>
<span><span class="co"># Compute Total Variation Distance</span></span>
<span><span class="va">tv_distance</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">P_discrete</span> <span class="op">-</span> <span class="va">Q_discrete</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Total Variation Distance (Discrete):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">tv_distance</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Total Variation Distance (Discrete): 0.3"</span></span></code></pre></div>
<p>This calculates the maximum difference between the two distributions, scaled to lie between 0 and 1.</p>
<p><strong>Example 2: Continuous Distributions (Approximation)</strong></p>
<p>For continuous distributions, the TV distance can be approximated using discretization or numerical integration. Here’s an example using random samples:</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate two continuous distributions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">P_continuous</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Standard normal distribution</span></span>
<span><span class="va">Q_continuous</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Normal distribution with mean 1</span></span>
<span></span>
<span><span class="co"># Create histograms to approximate probabilities</span></span>
<span><span class="va">hist_P</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">P_continuous</span>, breaks <span class="op">=</span> <span class="fl">50</span>, plot <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">hist_Q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">Q_continuous</span>, breaks <span class="op">=</span> <span class="fl">50</span>, plot <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Normalize histograms to probabilities</span></span>
<span><span class="va">prob_P</span> <span class="op">&lt;-</span> <span class="va">hist_P</span><span class="op">$</span><span class="va">counts</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">hist_P</span><span class="op">$</span><span class="va">counts</span><span class="op">)</span></span>
<span><span class="va">prob_Q</span> <span class="op">&lt;-</span> <span class="va">hist_Q</span><span class="op">$</span><span class="va">counts</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">hist_Q</span><span class="op">$</span><span class="va">counts</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Total Variation Distance</span></span>
<span><span class="va">tv_distance_continuous</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">prob_P</span> <span class="op">-</span> <span class="va">prob_Q</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span></span>
<span>    <span class="st">"Total Variation Distance (Continuous Approximation):"</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">tv_distance_continuous</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Total Variation Distance (Continuous Approximation): 0.125"</span></span></code></pre></div>
<p>The continuous distributions are discretized into histograms, and TV distance is computed based on the resulting probabilities.</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Discrete Case</strong>:</p>
<ul>
<li><p>The TV distance quantifies the maximum difference between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> in terms of probability mass.</p></li>
<li><p>In this example, it highlights how much <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> diverge.</p></li>
</ul>
</li>
<li>
<p><strong>Continuous Case</strong>:</p>
<ul>
<li><p>For continuous distributions, TV distance is approximated using discretized probabilities from histograms.</p></li>
<li><p>This approach provides an intuitive measure of similarity for large samples.</p></li>
</ul>
</li>
</ol>
<p>The Total Variation Distance provides an intuitive and interpretable measure of the maximum difference between two distributions. Its symmetry and bounded nature make it a versatile tool for comparing both discrete and continuous distributions.</p>
</div>
<div id="summary" class="section level3" number="4.6.12">
<h3>
<span class="header-section-number">4.6.12</span> Summary<a class="anchor" aria-label="anchor" href="#summary"><i class="fas fa-link"></i></a>
</h3>
<p><strong>1. Tests for Comparing Distributions</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="12%">
<col width="30%">
<col width="13%">
<col width="15%">
<col width="26%">
</colgroup>
<thead><tr class="header">
<th><strong>Test Name</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>Type of Data</strong></th>
<th><strong>Advantages</strong></th>
<th><strong>Limitations</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Kolmogorov-Smirnov Test</strong></td>
<td>Tests if two distributions are the same or if a sample matches a reference distribution.</td>
<td>Empirical Distributions (Continuous)</td>
<td>Non-parametric, detects global differences.</td>
<td>Less sensitive to tail differences, limited to one-dimensional data.</td>
</tr>
<tr class="even">
<td><strong>Anderson-Darling Test</strong></td>
<td>Tests goodness-of-fit with emphasis on the tails.</td>
<td>Continuous Data</td>
<td>Strong sensitivity to tail behavior.</td>
<td>Requires specifying a reference distribution.</td>
</tr>
<tr class="odd">
<td><strong>Chi-Square Goodness-of-Fit Test</strong></td>
<td>Tests if observed frequencies match expected frequencies.</td>
<td>Categorical Data</td>
<td>Simple, intuitive for discrete data.</td>
<td>Requires large sample sizes and sufficiently large expected frequencies.</td>
</tr>
<tr class="even">
<td><strong>Cramér-von Mises Test</strong></td>
<td>Evaluates goodness-of-fit using cumulative distribution functions.</td>
<td>Empirical Distributions (Continuous)</td>
<td>Sensitive across the entire distribution.</td>
<td>Limited to one-dimensional data; requires cumulative distribution functions.</td>
</tr>
</tbody>
</table></div>
<p><strong>2. Divergence Metrics</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="12%">
<col width="27%">
<col width="19%">
<col width="19%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th><strong>Metric Name</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>Type of Data</strong></th>
<th><strong>Advantages</strong></th>
<th><strong>Limitations</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Kullback-Leibler Divergence</strong></td>
<td>Measures how one probability distribution diverges from another.</td>
<td>Probability Distributions (Continuous/Discrete)</td>
<td>Provides a clear measure of information loss.</td>
<td>Asymmetric, sensitive to zero probabilities.</td>
</tr>
<tr class="even">
<td><strong>Jensen-Shannon Divergence</strong></td>
<td>Symmetric measure of similarity between two probability distributions.</td>
<td>Probability Distributions (Continuous/Discrete)</td>
<td>Symmetric and bounded; intuitive for comparison.</td>
<td>Less sensitive to tail differences.</td>
</tr>
<tr class="odd">
<td><strong>Hellinger Distance</strong></td>
<td>Measures geometric similarity between two probability distributions.</td>
<td>Discrete or Continuous Probability Distributions</td>
<td>Easy to interpret; bounded between 0 and 1.</td>
<td>Computationally expensive for large datasets.</td>
</tr>
<tr class="even">
<td><strong>Bhattacharyya Distance</strong></td>
<td>Quantifies overlap between two statistical distributions.</td>
<td>Probability Distributions (Continuous/Discrete)</td>
<td>Useful for classification and clustering tasks.</td>
<td>Less interpretable in large-scale applications.</td>
</tr>
</tbody>
</table></div>
<p><strong>3. Distance Metrics</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="10%">
<col width="30%">
<col width="17%">
<col width="19%">
<col width="21%">
</colgroup>
<thead><tr class="header">
<th><strong>Metric Name</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>Type of Data</strong></th>
<th><strong>Advantages</strong></th>
<th><strong>Limitations</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Wasserstein Distance</strong></td>
<td>Measures the “effort” or “cost” to transform one distribution into another.</td>
<td>Continuous or Empirical Distributions</td>
<td>Provides geometric interpretation; versatile.</td>
<td>Computationally expensive for large-scale data.</td>
</tr>
<tr class="even">
<td><strong>Energy Distance</strong></td>
<td>Measures statistical dissimilarity between multivariate distributions.</td>
<td>Multivariate Empirical Distributions</td>
<td>Non-parametric, works well for high-dimensional data.</td>
<td>Requires pairwise calculations; sensitive to outliers.</td>
</tr>
<tr class="odd">
<td><strong>Total Variation Distance</strong></td>
<td>Measures the maximum absolute difference between probabilities of two distributions.</td>
<td>Probability Distributions (Discrete/Continuous)</td>
<td>Intuitive and strict divergence measure.</td>
<td>Ignores structural differences beyond the largest deviation.</td>
</tr>
</tbody>
</table></div>
</div>
</div>
</div>



<!-- Welcome to this volume dedicated to **Regression**. Whether you are a seasoned researcher, an aspiring data scientist, or simply curious about the foundations of quantitative modeling, regression is a cornerstone technique worth mastering. Here, we will delve into: -->
<!-- -   **Linear Regression** -->
<!-- -   **Nonlinear Regression** -->
<!-- -   **Generalized Linear Models (GLMs)** -->
<!-- -   **Linear Mixed Models (LMMs)** -->
<!-- -   **Nonlinear and Generalized Linear Mixed Models** -->
<!-- This volume offers a wide-ranging survey of methods to help you gain deeper insights from your data. -->
<!-- **Why Regression?** -->
<!-- Think of regression as a powerful lens that helps us interpret relationships hidden within numbers. By fitting mathematical functions to data, regression serves to: -->
<!-- -   **Illuminate Patterns**: Discover trends and relationships between variables. -->
<!-- -   **Quantify Uncertainty**: Assess confidence in the relationships observed. -->
<!-- -   **Make Predictions**: Project future outcomes based on historical trends. -->
<!-- **Asking the Right Questions** -->
<!-- Regression enables you to explore sophisticated questions, such as: -->
<!-- -   Which predictors matter most in explaining a response? -->
<!-- -   How does a change in one variable affect another? -->
<!-- -   How well can we predict future outcomes from the past? -->
<!-- **What Regression *Is*---and *Isn't*** -->
<!-- While regression is a versatile and powerful tool, it's important to clarify its scope. Here's what you can expect to learn---and what lies outside the realm of regression: -->
<!-- 1.  What You Will Get: -->
<!-- -   **Foundational Understanding**: How regression works, its assumptions, and its applications. -->
<!-- -   **Practical Skills**: Building, diagnosing, and interpreting regression models. -->
<!-- -   **Tools for Inference and Prediction**: Quantifying relationships and projecting outcomes. -->
<!-- -   **Handling Real-World Data Challenges**: Addressing nonlinearity, multicollinearity, missing data, and more. -->
<!-- 2.  What Regression Isn't: -->
<!-- -   **Causal Analysis by Itself**: Regression shows associations, not causation. Establishing causality requires careful design (e.g., experiments) or specialized methods (e.g., instrumental variables). -->
<!-- -   **A Catch-All Solution**: Regression is one tool in a broader data analysis toolkit. Problems like clustering, time series forecasting, or unsupervised learning often require alternative approaches. -->
<!-- -   **Guaranteed Answers**: The quality of insights depends on the quality of your data, model selection, and validation efforts. -->
<!-- -   **A Plug-and-Play Tool**: Effective use of regression requires understanding the data, model assumptions, and diagnostics---automating regression without context often leads to flawed conclusions. -->
<!-- This book will empower you with the knowledge to use regression effectively while recognizing its limitations. By the end of this book, you will be equipped with both **theoretical knowledge** and **practical skills** for building and interpreting regression models in a variety of contexts. -->
<!-- **The Philosophy of Regression** -->
<!-- As you explore regression, remember these key themes: -->
<!-- -   **Model Assumptions**: Ensure the assumptions of your model align with your data. -->
<!-- -   **Data Transformations**: Use transformations thoughtfully to reveal relationships. -->
<!-- -   **Diagnostic Checks**: Evaluate model fit and residual patterns. -->
<!-- -   **Complexity vs. Interpretability**: Balance the need for detail with the importance of clarity. -->
<!-- Through it all, the goal remains the same: to tell a **story with data**---one that is rigorous, evidence-based, and useful for informed decision-making. -->
<!-- Now, let's begin the journey into the fascinating world of regression analysis! -->
<div class="chapter-nav">
<div class="prev"><a href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></div>
<div class="next"><a href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#basic-statistical-inference"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li>
<a class="nav-link" href="#hypothesis-testing-framework"><span class="header-section-number">4.1</span> Hypothesis Testing Framework</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#null-and-alternative-hypotheses"><span class="header-section-number">4.1.1</span> Null and Alternative Hypotheses</a></li>
<li><a class="nav-link" href="#errors-in-hypothesis-testing"><span class="header-section-number">4.1.2</span> Errors in Hypothesis Testing</a></li>
<li><a class="nav-link" href="#the-role-of-distributions-in-hypothesis-testing"><span class="header-section-number">4.1.3</span> The Role of Distributions in Hypothesis Testing</a></li>
<li><a class="nav-link" href="#the-test-statistic"><span class="header-section-number">4.1.4</span> The Test Statistic</a></li>
<li><a class="nav-link" href="#critical-values-and-rejection-regions-1"><span class="header-section-number">4.1.5</span> Critical Values and Rejection Regions</a></li>
<li><a class="nav-link" href="#visualizing-hypothesis-testing"><span class="header-section-number">4.1.6</span> Visualizing Hypothesis Testing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#key-concepts-and-definitions"><span class="header-section-number">4.2</span> Key Concepts and Definitions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#random-sample"><span class="header-section-number">4.2.1</span> Random Sample</a></li>
<li><a class="nav-link" href="#sample-statistics"><span class="header-section-number">4.2.2</span> Sample Statistics</a></li>
<li><a class="nav-link" href="#distribution-of-the-sample-mean"><span class="header-section-number">4.2.3</span> Distribution of the Sample Mean</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#one-sample-inference"><span class="header-section-number">4.3</span> One-Sample Inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#for-single-mean"><span class="header-section-number">4.3.1</span> For Single Mean</a></li>
<li><a class="nav-link" href="#for-difference-of-means-independent-samples"><span class="header-section-number">4.3.2</span> For Difference of Means, Independent Samples</a></li>
<li><a class="nav-link" href="#for-difference-of-means-paired-samples"><span class="header-section-number">4.3.3</span> For Difference of Means, Paired Samples</a></li>
<li><a class="nav-link" href="#for-difference-of-two-proportions"><span class="header-section-number">4.3.4</span> For Difference of Two Proportions</a></li>
<li><a class="nav-link" href="#for-single-proportion"><span class="header-section-number">4.3.5</span> For Single Proportion</a></li>
<li><a class="nav-link" href="#for-single-variance"><span class="header-section-number">4.3.6</span> For Single Variance</a></li>
<li><a class="nav-link" href="#non-parametric-tests"><span class="header-section-number">4.3.7</span> Non-parametric Tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#two-sample-inference"><span class="header-section-number">4.4</span> Two-Sample Inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#for-means"><span class="header-section-number">4.4.1</span> For Means</a></li>
<li><a class="nav-link" href="#for-variances"><span class="header-section-number">4.4.2</span> For Variances</a></li>
<li><a class="nav-link" href="#power"><span class="header-section-number">4.4.3</span> Power</a></li>
<li><a class="nav-link" href="#matched-pair-designs"><span class="header-section-number">4.4.4</span> Matched Pair Designs</a></li>
<li><a class="nav-link" href="#nonparametric-tests-for-two-samples"><span class="header-section-number">4.4.5</span> Nonparametric Tests for Two Samples</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#categorical-data-analysis"><span class="header-section-number">4.5</span> Categorical Data Analysis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#association-tests"><span class="header-section-number">4.5.1</span> Association Tests</a></li>
<li><a class="nav-link" href="#ordinal-association"><span class="header-section-number">4.5.2</span> Ordinal Association</a></li>
<li><a class="nav-link" href="#ordinal-trend"><span class="header-section-number">4.5.3</span> Ordinal Trend</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#divergence-metrics-and-tests-for-comparing-distributions"><span class="header-section-number">4.6</span> Divergence Metrics and Tests for Comparing Distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#kolmogorov-smirnov-test-1"><span class="header-section-number">4.6.1</span> Kolmogorov-Smirnov Test</a></li>
<li><a class="nav-link" href="#anderson-darling-test-1"><span class="header-section-number">4.6.2</span> Anderson-Darling Test</a></li>
<li><a class="nav-link" href="#chi-square-goodness-of-fit-test"><span class="header-section-number">4.6.3</span> Chi-Square Goodness-of-Fit Test</a></li>
<li><a class="nav-link" href="#cram%C3%A9r-von-mises-test"><span class="header-section-number">4.6.4</span> Cramér-von Mises Test</a></li>
<li><a class="nav-link" href="#kullback-leibler-divergence"><span class="header-section-number">4.6.5</span> Kullback-Leibler Divergence</a></li>
<li><a class="nav-link" href="#jensen-shannon-divergence"><span class="header-section-number">4.6.6</span> Jensen-Shannon Divergence</a></li>
<li><a class="nav-link" href="#hellinger-distance"><span class="header-section-number">4.6.7</span> Hellinger Distance</a></li>
<li><a class="nav-link" href="#bhattacharyya-distance"><span class="header-section-number">4.6.8</span> Bhattacharyya Distance</a></li>
<li><a class="nav-link" href="#wasserstein-distance"><span class="header-section-number">4.6.9</span> Wasserstein Distance</a></li>
<li><a class="nav-link" href="#energy-distance"><span class="header-section-number">4.6.10</span> Energy Distance</a></li>
<li><a class="nav-link" href="#total-variation-distance"><span class="header-section-number">4.6.11</span> Total Variation Distance</a></li>
<li><a class="nav-link" href="#summary"><span class="header-section-number">4.6.12</span> Summary</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/04-basic-inference.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/04-basic-inference.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-02-03.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
