<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Basic Statistical Inference | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="One Sample Inference Two Sample Inference Categorical Data Analysis Divergence Metrics and Test for Comparing Distributions Make inferences (an interpretation) about the true parameter value...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 4 Basic Statistical Inference | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/basic-statistical-inference.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="One Sample Inference Two Sample Inference Categorical Data Analysis Divergence Metrics and Test for Comparing Distributions Make inferences (an interpretation) about the true parameter value...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Basic Statistical Inference | A Guide on Data Analysis">
<meta name="twitter:description" content="One Sample Inference Two Sample Inference Categorical Data Analysis Divergence Metrics and Test for Comparing Distributions Make inferences (an interpretation) about the true parameter value...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-stat.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="active" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="" href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">25</span> Difference-in-differences</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">26</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">27</span> Event Studies</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">28</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">29</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">30</span> Endogeneity</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">31</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="mediation.html"><span class="header-section-number">32</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">33</span> Directed Acyclic Graph</a></li>
<li><a class="" href="report.html"><span class="header-section-number">34</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">35</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">36</span> Sensitivity Analysis/ Robustness Check</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="basic-statistical-inference" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Basic Statistical Inference<a class="anchor" aria-label="anchor" href="#basic-statistical-inference"><i class="fas fa-link"></i></a>
</h1>
<ol style="list-style-type: decimal">
<li><a href="basic-statistical-inference.html#one-sample-inference">One Sample Inference</a></li>
<li><a href="basic-statistical-inference.html#two-sample-inference">Two Sample Inference</a></li>
<li><a href="basic-statistical-inference.html#categorical-data-analysis">Categorical Data Analysis</a></li>
<li><a href="basic-statistical-inference.html#divergence-metrics-and-test-for-comparing-distributions">Divergence Metrics and Test for Comparing Distributions</a></li>
</ol>
<ul>
<li>Make <strong>inferences</strong> (an interpretation) about the true parameter value <span class="math inline">\(\beta\)</span> based on our estimator/estimate</li>
<li>Test whether our underlying assumptions (about the true population parameters, random variables, or model specification) hold true.</li>
</ul>
<p>Testing does not</p>
<ul>
<li>Confirm with 100% a hypothesis is true</li>
<li>Confirm with 100% a hypothesis is false</li>
<li>Tell you how to interpret the estimate value (Economic vs. Practical vs. Statistical Significance)</li>
</ul>
<p>Hypothesis: Translate an objective in better understanding the results in terms of specifying a value (or sets of values) in which our population parameters should/should not lie.</p>
<ul>
<li>
<strong>Null hypothesis</strong> (<span class="math inline">\(H_0\)</span>): A statement about the population parameter that we take to be true in which we would need the data to provide substantial evidence that against it.
<ul>
<li>Can be either a single value (ex: <span class="math inline">\(H_0: \beta=0\)</span>) or a set of values (ex: <span class="math inline">\(H_0: \beta_1 \ge 0\)</span>)</li>
<li>Will generally be the value you would not like the population parameter to be (subjective)
<ul>
<li>
<span class="math inline">\(H_0: \beta_1=0\)</span> means you would like to see a non-zero coefficient</li>
<li>
<span class="math inline">\(H_0: \beta_1 \ge 0\)</span> means you would like to see a negative effect</li>
</ul>
</li>
<li>“Test of Significance” refers to the two-sided test: <span class="math inline">\(H_0: \beta_j=0\)</span>
</li>
</ul>
</li>
<li>
<strong>Alternative hypothesis</strong> (<span class="math inline">\(H_a\)</span> or <span class="math inline">\(H_1\)</span>) (Research Hypothesis): All other possible values that the population parameter may be if the null hypothesis does not hold.</li>
</ul>
<p><strong>Type I Error</strong></p>
<p>Error made when <span class="math inline">\(H_0\)</span> is rejected when, in fact, <span class="math inline">\(H_0\)</span> is true.<br>
The probability of committing a Type I error is <span class="math inline">\(\alpha\)</span> (known as <strong>level of significance</strong> of the test)</p>
<p>Type I error (<span class="math inline">\(\alpha\)</span>): probability of rejecting <span class="math inline">\(H_0\)</span> when it is true.</p>
<p>Legal analogy: In U.S. law, a defendant is presumed to be “innocent until proven guilty”.<br>
If the null hypothesis is that a person is innocent, the Type I error is the probability that you conclude the person is guilty when he is innocent.</p>
<p><strong>Type II Error</strong></p>
<p>Type II error level (<span class="math inline">\(\beta\)</span>): probability that you fail to reject the null hypothesis when it is false.</p>
<p>In the legal analogy, this is the probability that you fail to find the person guilty when he or she is guilty.</p>
<p>Error made when <span class="math inline">\(H_0\)</span> is not rejected when, in fact, <span class="math inline">\(H_1\)</span> is true<br>
The probability of committing a Type II error is <span class="math inline">\(\beta\)</span> (known as the <strong>power</strong> of the test)</p>
<p>Random sample of size n: A collection of n independent random variables taken from the distribution X, each with the same distribution as X.</p>
<p><strong>Sample mean</strong></p>
<p><span class="math display">\[
\bar{X}= (\sum_{i=1}^{n}X_i)/n
\]</span></p>
<p><strong>Sample Median</strong></p>
<p><span class="math inline">\(\tilde{x}\)</span> = the middle observation in a sample of observation order from smallest to largest (or vice versa).</p>
<p>If n is odd, <span class="math inline">\(\tilde{x}\)</span> is the middle observation,<br>
If n is even, <span class="math inline">\(\tilde{x}\)</span> is the average of the two middle observations.</p>
<p><strong>Sample variance</strong> <span class="math display">\[
S^2 = \frac{\sum_{i=1}^{n}(X_i = \bar{X})^2}{n-1}= \frac{n\sum_{i=1}^{n}X_i^2 -(\sum_{i=1}^{n}X_i)^2}{n(n-1)}
\]</span></p>
<p><strong>Sample standard deviation</strong> <span class="math display">\[
S = \sqrt{S^2}
\]</span></p>
<p><strong>Sample proportions</strong> <span class="math display">\[
\hat{p} = \frac{X}{n} = \frac{\text{number in the sample with trait}}{\text{sample size}}
\]</span></p>
<p><span class="math display">\[
\widehat{p_1-p_2} = \hat{p_1}-\hat{p_2} = \frac{X_1}{n_1} - \frac{X_2}{n_2} = \frac{n_2X_1 = n_1X_2}{n_1n_2}
\]</span></p>
<p><strong>Estimators</strong><br><strong>Point Estimator</strong><br><span class="math inline">\(\hat{\theta}\)</span> is a statistic used to approximate a population parameter <span class="math inline">\(\theta\)</span></p>
<p><strong>Point estimate</strong><br>
The numerical value assumed by <span class="math inline">\(\hat{\theta}\)</span> when evaluated for a given sample</p>
<p><strong>Unbiased estimator</strong><br>
If <span class="math inline">\(E(\hat{\theta}) = \theta\)</span>, then <span class="math inline">\(\hat{\theta}\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\bar{X}\)</span> is an unbiased estimator for <span class="math inline">\(\mu\)</span>
</li>
<li>
<span class="math inline">\(S^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>
</li>
<li>
<span class="math inline">\(\hat{p}\)</span> is an unbiased estimator for p</li>
<li>
<span class="math inline">\(\widehat{p_1-p_2}\)</span> is an unbiased estimator for <span class="math inline">\(p_1- p_2\)</span>
</li>
<li>
<span class="math inline">\(\bar{X_1} - \bar{X_2}\)</span> is an unbiased estimator for <span class="math inline">\(\mu_1 - \mu_2\)</span>
</li>
</ol>
<p><strong>Note</strong>: <span class="math inline">\(S\)</span> is a biased estimator for <span class="math inline">\(\sigma\)</span></p>
<p><strong>Distribution of the sample mean</strong></p>
<p>If <span class="math inline">\(\bar{X}\)</span> is the sample mean based on a random sample of size <span class="math inline">\(n\)</span> drawn from a normal distribution <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, the <span class="math inline">\(\bar{X}\)</span> is normally distributed, with mean <span class="math inline">\(\mu_{\bar{X}} = \mu\)</span> and variance <span class="math inline">\(\sigma_{\bar{X}}^2 = Var(\bar{X}) = \frac{\sigma^2}{n}\)</span>. Then the <strong>standard error of the mean</strong> is: <span class="math inline">\(\sigma_{\bar{X}}= \frac{\sigma}{\sqrt{n}}\)</span></p>
<div id="one-sample-inference" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> One Sample Inference<a class="anchor" aria-label="anchor" href="#one-sample-inference"><i class="fas fa-link"></i></a>
</h2>
<p><span class="math inline">\(Y_i \sim i.i.d. N(\mu, \sigma^2)\)</span></p>
<p>i.i.d. standards for “independent and identically distributed”</p>
<p>Hence, we have the following model:</p>
<p><span class="math inline">\(Y_i=\mu +\epsilon_i\)</span> where</p>
<ul>
<li>
<span class="math inline">\(\epsilon_i \sim^{iid} N(0,\sigma^2)\)</span><br>
</li>
<li>
<span class="math inline">\(E(Y_i)=\mu\)</span><br>
</li>
<li>
<span class="math inline">\(Var(Y_i)=\sigma^2\)</span><br>
</li>
<li><span class="math inline">\(\bar{y} \sim N(\mu,\sigma^2/n)\)</span></li>
</ul>
<div id="the-mean" class="section level3" number="4.1.1">
<h3>
<span class="header-section-number">4.1.1</span> The Mean<a class="anchor" aria-label="anchor" href="#the-mean"><i class="fas fa-link"></i></a>
</h3>
<p>When <span class="math inline">\(\sigma^2\)</span> is estimated by <span class="math inline">\(s^2\)</span>, then</p>
<p><span class="math display">\[
\frac{\bar{y}-\mu}{s/\sqrt{n}} \sim t_{n-1}
\]</span></p>
<p>Then, a <span class="math inline">\(100(1-\alpha) \%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> is obtained from:</p>
<p><span class="math display">\[
1 - \alpha = P(-t_{\alpha/2;n-1} \le \frac{\bar{y}-\mu}{s/\sqrt{n}} \le t_{\alpha/2;n-1}) \\
= P(\bar{y} - (t_{\alpha/2;n-1})s/\sqrt{n} \le \mu \le \bar{y} + (t_{\alpha/2;n-1})s/\sqrt{n})
\]</span></p>
<p>And the interval is</p>
<p><span class="math display">\[
\bar{y} \pm (t_{\alpha/2;n-1})s/\sqrt{n}
\]</span></p>
<p>and <span class="math inline">\(s/\sqrt{n}\)</span> is the standard error of <span class="math inline">\(\bar{y}\)</span></p>
<p>If the experiment were repeated many times, <span class="math inline">\(100(1-\alpha) \%\)</span> of these intervals would contain <span class="math inline">\(\mu\)</span></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="27%">
<col width="24%">
<col width="24%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Confidence Interval <span class="math inline">\(100(1-\alpha)%\)</span>
</th>
<th>Sample Sizes Confidence <span class="math inline">\(\alpha\)</span>, Error <span class="math inline">\(d\)</span>
</th>
<th>Hypothesis Testing Test Statistic</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>When <span class="math inline">\(\sigma^2\)</span> is known, X is normal (or <span class="math inline">\(n \ge 25\)</span>)</td>
<td><span class="math inline">\(\bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\)</span></td>
<td><span class="math inline">\(n \approx \frac{z_{\alpha/2}^2 \sigma^2}{d^2}\)</span></td>
<td><span class="math inline">\(z = \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}\)</span></td>
</tr>
<tr class="even">
<td>When <span class="math inline">\(\sigma^2\)</span> is unknown, X is normal (or <span class="math inline">\(n \ge 25\)</span>)</td>
<td><span class="math inline">\(\bar{X} \pm t_{\alpha/2}\frac{s}{\sqrt{n}}\)</span></td>
<td><span class="math inline">\(n \approx \frac{z_{\alpha/2}^2 s^2}{d^2}\)</span></td>
<td><span class="math inline">\(t = \frac{\bar{X}-\mu_0}{s/\sqrt{n}}\)</span></td>
</tr>
</tbody>
</table></div>
<div id="for-difference-of-means-mu_1-mu_2-independent-samples" class="section level4" number="4.1.1.1">
<h4>
<span class="header-section-number">4.1.1.1</span> For Difference of Means (<span class="math inline">\(\mu_1-\mu_2\)</span>), Independent Samples<a class="anchor" aria-label="anchor" href="#for-difference-of-means-mu_1-mu_2-independent-samples"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="21%">
<col width="21%">
<col width="23%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>
<span class="math inline">\(100(1-\alpha)%\)</span> Confidence Interval</th>
<th>Hypothesis Testing Test Statistic</th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>When <span class="math inline">\(\sigma^2\)</span> is known</td>
<td><span class="math inline">\(\bar{X}_1 - \bar{X}_2 \pm z_{\alpha/2}\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}\)</span></td>
<td><span class="math inline">\(z= \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}}\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>When <span class="math inline">\(\sigma^2\)</span> is unknown, Variances Assumed EQUAL</td>
<td><span class="math inline">\(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}\)</span></td>
<td><span class="math inline">\(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}}\)</span></td>
<td>Pooled Variance: <span class="math inline">\(s_p^2 = \frac{(n_1 -1)s^2_1 - (n_2-1)s^2_2}{n_1 + n_2 -2}\)</span> Degrees of Freedom: <span class="math inline">\(\gamma = n_1 + n_2 -2\)</span>
</td>
</tr>
<tr class="odd">
<td>When <span class="math inline">\(\sigma^2\)</span> is unknown, Variances Assumed UNEQUAL</td>
<td><span class="math inline">\(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}\)</span></td>
<td><span class="math inline">\(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}}\)</span></td>
<td>Degrees of Freedom: <span class="math inline">\(\gamma = \frac{(\frac{s_1^2}{n_1}+\frac{s^2_2}{n_2})^2}{\frac{(\frac{s_1^2}{n_1})^2}{n_1-1}+\frac{(\frac{s_2^2}{n_2})^2}{n_2-1}}\)</span>
</td>
</tr>
</tbody>
</table></div>
</div>
<div id="for-difference-of-means-mu_1---mu_2-paired-samples-d-x-y" class="section level4" number="4.1.1.2">
<h4>
<span class="header-section-number">4.1.1.2</span> For Difference of Means (<span class="math inline">\(\mu_1 - \mu_2\)</span>), Paired Samples (D = X-Y)<a class="anchor" aria-label="anchor" href="#for-difference-of-means-mu_1---mu_2-paired-samples-d-x-y"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math inline">\(100(1-\alpha)%\)</span> Confidence Interval<br><span class="math display">\[
\bar{D} \pm t_{\alpha/2}\frac{s_d}{\sqrt{n}}
\]</span></p>
<p><strong>Hypothesis Testing Test Statistic</strong></p>
<p><span class="math display">\[
t = \frac{\bar{D}-D_0}{s_d / \sqrt{n}}
\]</span></p>
</div>
<div id="difference-of-two-proportions" class="section level4" number="4.1.1.3">
<h4>
<span class="header-section-number">4.1.1.3</span> Difference of Two Proportions<a class="anchor" aria-label="anchor" href="#difference-of-two-proportions"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\hat{p_1}-\hat{p_2}
\]</span></p>
<p><strong>Variance</strong> <span class="math display">\[
\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}
\]</span></p>
<p><span class="math inline">\(100(1-\alpha)%\)</span> Confidence Interval</p>
<p><span class="math display">\[
\hat{p_1}-\hat{p_2} + z_{\alpha/2}\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
\]</span></p>
<p><strong>Sample Sizes, Confidence</strong> <span class="math inline">\(\alpha\)</span>, Error d<br>
(Prior Estimate fo <span class="math inline">\(\hat{p_1},\hat{p_2}\)</span>)</p>
<p><span class="math display">\[
n \approx \frac{z_{\alpha/2}^2[p_1(1-p_1)+p_2(1-p_2)]}{d^2}
\]</span></p>
<p>(No Prior Estimates for <span class="math inline">\(\hat{p}\)</span>)</p>
<p><span class="math display">\[
n \approx \frac{z_{\alpha/2}^2}{2d^2}
\]</span></p>
<p><strong>Hypothesis Testing - Test Statistics</strong></p>
<p>Null Value <span class="math inline">\((p_1 - p_2) \neq 0\)</span></p>
<p><span class="math display">\[
z = \frac{(\hat{p_1} - \hat{p_2})-(p_1 - p_2)_0}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}
\]</span></p>
<p>Null Value <span class="math inline">\((p_1 - p_2)_0 = 0\)</span></p>
<p><span class="math display">\[
z = \frac{\hat{p_1} - \hat{p_2}}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1}+\frac{1}{n_2})}}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{p}= \frac{x_1 + x_2}{n_1 + n_2} = \frac{n_1 \hat{p_1} + n_2 \hat{p_2}}{n_1 + n_2}
\]</span></p>
</div>
</div>
<div id="single-variance" class="section level3" number="4.1.2">
<h3>
<span class="header-section-number">4.1.2</span> Single Variance<a class="anchor" aria-label="anchor" href="#single-variance"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[
1 - \alpha = P( \chi_{1-\alpha/2;n-1}^2) \le (n-1)s^2/\sigma^2 \le \chi_{\alpha/2;n-1}^2) \\
= P(\frac{(n-1)s^2}{\chi_{\alpha/2}^2} \le \sigma^2 \le \frac{(n-1)s^2}{\chi_{1-\alpha/2}^2})
\]</span></p>
<p>and a <span class="math inline">\(100(1-\alpha) \%\)</span> confidence interval for <span class="math inline">\(\sigma^2\)</span> is:</p>
<p><span class="math display">\[
(\frac{(n-1)s^2}{\chi_{\alpha/2;n-1}^2},\frac{(n-1)s^2}{\chi_{1-\alpha/2;n-1}^2})
\]</span> Confidence limits for <span class="math inline">\(\sigma^2\)</span> are obtained by computing the positive square roots of these limits</p>
<p>Equivalently,</p>
<p><span class="math inline">\(100(1-\alpha)%\)</span> Confidence Interval</p>
<p><span class="math display">\[
L_1 = \frac{(n-1)s^2}{\chi^2_{\alpha/2}} \\
L_1 = \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}
\]</span> <strong>Hypothesis Testing Test Statistic</strong></p>
<p><span class="math display">\[
\chi^2 = \frac{(n-1)s^2}{\sigma^2_0}
\]</span></p>
</div>
<div id="single-proportion-p" class="section level3" number="4.1.3">
<h3>
<span class="header-section-number">4.1.3</span> Single Proportion (p)<a class="anchor" aria-label="anchor" href="#single-proportion-p"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="24%">
<col width="28%">
<col width="23%">
<col width="23%">
</colgroup>
<thead><tr class="header">
<th>Confidence Interval <span class="math inline">\(100(1-\alpha)%\)</span>
</th>
<th>Sample Sizes Confidence <span class="math inline">\(\alpha\)</span>, Error d (prior estimate for <span class="math inline">\(\hat{p}\)</span>)</th>
<th>(No prior estimate for <span class="math inline">\(\hat{p}\)</span>)</th>
<th>Hypothesis Testing Test Statistic</th>
</tr></thead>
<tbody><tr class="odd">
<td><span class="math inline">\(\hat{p} \pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span></td>
<td><span class="math inline">\(n \approx \frac{z_{\alpha/2}^2 \hat{p}(1-\hat{p})}{d^2}\)</span></td>
<td><span class="math inline">\(n \approx \frac{z_{\alpha/2}^2}{4d^2}\)</span></td>
<td><span class="math inline">\(z = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\)</span></td>
</tr></tbody>
</table></div>
</div>
<div id="power" class="section level3" number="4.1.4">
<h3>
<span class="header-section-number">4.1.4</span> Power<a class="anchor" aria-label="anchor" href="#power"><i class="fas fa-link"></i></a>
</h3>
<p>Formally, power (for the test of the mean) is given by:</p>
<p><span class="math display">\[
\pi(\mu) = 1 - \beta = P(\text{test rejects } H_0|\mu)
\]</span> To evaluate the power, one needs to know the distribution of the test statistic if the null hypothesis is false.</p>
<p>For 1-sided z-test where <span class="math inline">\(H_0: \mu \le \mu_0 \\ H_A: \mu &gt;0\)</span></p>
<p>The power is:</p>
<p><span class="math display">\[
\begin{aligned}
\pi(\mu) &amp;= P(\bar{y} &gt; \mu_0 + z_{\alpha} \sigma/\sqrt{n}|\mu) \\
&amp;= P(Z = \frac{\bar{y} - \mu}{\sigma / \sqrt{n}} &gt; z_{\alpha} + \frac{\mu_0 - \mu}{\sigma/ \sqrt{n}}|\mu) \\
&amp;= 1 - \Phi(z_{\alpha} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}) \\
&amp;= \Phi(-z_{\alpha}+\frac{(\mu -\mu_0)\sqrt{n}}{\sigma})
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(1-\Phi(x) = \Phi(-x)\)</span> since the normal pdf is symmetric</p>
<p>Power is correlated to the difference in <span class="math inline">\(\mu - \mu_0\)</span>, sample size n, variance <span class="math inline">\(\sigma^2\)</span>, and the <span class="math inline">\(\alpha\)</span>-level of the test (through <span class="math inline">\(z_{\alpha}\)</span>)<br>
Equivalently, power can be increased by making <span class="math inline">\(\alpha\)</span> large, <span class="math inline">\(\sigma^2\)</span> smaller, or n larger.</p>
<p>For 2-sided z-test is:</p>
<p><span class="math display">\[
\pi(\mu) = \Phi(-z_{\alpha/2} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}) + \Phi(-z_{\alpha/2}+\frac{(\mu - \mu_0)\sqrt{n}}{\sigma})
\]</span></p>
</div>
<div id="sample-size" class="section level3" number="4.1.5">
<h3>
<span class="header-section-number">4.1.5</span> Sample Size<a class="anchor" aria-label="anchor" href="#sample-size"><i class="fas fa-link"></i></a>
</h3>
<div id="sided-z-test" class="section level4" number="4.1.5.1">
<h4>
<span class="header-section-number">4.1.5.1</span> 1-sided Z-test<a class="anchor" aria-label="anchor" href="#sided-z-test"><i class="fas fa-link"></i></a>
</h4>
<p>Example: to show that the mean response <span class="math inline">\(\mu\)</span> under the treatment is higher than the mean response <span class="math inline">\(\mu_0\)</span> without treatment (show that the treatment effect <span class="math inline">\(\delta = \mu -\mu_0\)</span> is large)</p>
<p>Because power is an increasing function of <span class="math inline">\(\mu - \mu_0\)</span>, it is only necessary to find n that makes the power equal to <span class="math inline">\(1- \beta\)</span> at <span class="math inline">\(\mu = \mu_0 + \delta\)</span></p>
<p>Hence, we have</p>
<p><span class="math display">\[
\pi(\mu_0 + \delta) = \Phi(-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma}) = 1 - \beta
\]</span></p>
<p>Since <span class="math inline">\(\Phi (z_{\beta})= 1-\beta\)</span>, we have</p>
<p><span class="math display">\[
-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma} = z_{\beta}
\]</span></p>
<p>Then n is</p>
<p><span class="math display">\[
n = (\frac{(z_{\alpha}+z_{\beta})\sigma}{\delta})^2
\]</span></p>
<p>Then, we need larger samples, when</p>
<ul>
<li>the sample variability is large (<span class="math inline">\(\sigma\)</span> is large)</li>
<li>
<span class="math inline">\(\alpha\)</span> is small (<span class="math inline">\(z_{\alpha}\)</span> is large)</li>
<li>power <span class="math inline">\(1-\beta\)</span> is large (<span class="math inline">\(z_{\beta}\)</span> is large)</li>
<li>The magnitude of the effect is smaller (<span class="math inline">\(\delta\)</span> is small)</li>
</ul>
<p>Since we don’t know <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\sigma\)</span>. We can base <span class="math inline">\(\sigma\)</span> on previous studies, pilot studies. Or, obtain an estimate of <span class="math inline">\(\sigma\)</span> by anticipating the range of the observation (without outliers). divide this range by 4 and use the resulting number as an approximate estimate of <span class="math inline">\(\sigma\)</span>. For normal (distribution) data, this is reasonable.</p>
</div>
<div id="sided-z-test-1" class="section level4" number="4.1.5.2">
<h4>
<span class="header-section-number">4.1.5.2</span> 2-sided Z-test<a class="anchor" aria-label="anchor" href="#sided-z-test-1"><i class="fas fa-link"></i></a>
</h4>
<p>We want to know the min n, required to guarantee <span class="math inline">\(1-\beta\)</span> power when the treatment effect <span class="math inline">\(\delta = |\mu - \mu_0|\)</span> is at least greater than 0. Since the power function for the 2-sided is increasing and symmetric in <span class="math inline">\(|\mu - \mu_0|\)</span>, we only need to find n that makes the power equal to <span class="math inline">\(1-\beta\)</span> when <span class="math inline">\(\mu = \mu_0 + \delta\)</span></p>
<p><span class="math display">\[
n = (\frac{(z_{\alpha/2} + z_{\beta}) \sigma}{\delta})^2
\]</span></p>
<p>We could also use the confidence interval approach. If we require that an <span class="math inline">\(\alpha\)</span>-level two-sided CI for <span class="math inline">\(\mu\)</span> be</p>
<p><span class="math display">\[
\bar{y} \pm D
\]</span> where <span class="math inline">\(D = z_{\alpha/2}\sigma/\sqrt{n}\)</span> gives</p>
<p><span class="math display">\[
n = (\frac{z_{\alpha/2}\sigma}{D})^2
\]</span> (round up to the nearest integer)</p>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">data</span>, conf.level<span class="op">=</span><span class="fl">0.95</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  One Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data</span></span>
<span><span class="co">#&gt; t = 0.42865, df = 99, p-value = 0.6691</span></span>
<span><span class="co">#&gt; alternative hypothesis: true mean is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -0.1728394  0.2680940</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt;  mean of x </span></span>
<span><span class="co">#&gt; 0.04762729</span></span></code></pre></div>
<p><span class="math display">\[
H_0: \mu \ge 30 \\
H_a: \mu &lt; 30
\]</span></p>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">data</span>, mu<span class="op">=</span><span class="fl">30</span>,alternative<span class="op">=</span><span class="st">"less"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  One Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  data</span></span>
<span><span class="co">#&gt; t = -269.57, df = 99, p-value &lt; 2.2e-16</span></span>
<span><span class="co">#&gt; alternative hypothesis: true mean is less than 30</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;       -Inf 0.2321136</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt;  mean of x </span></span>
<span><span class="co">#&gt; 0.04762729</span></span></code></pre></div>
</div>
</div>
<div id="note" class="section level3" number="4.1.6">
<h3>
<span class="header-section-number">4.1.6</span> Note<a class="anchor" aria-label="anchor" href="#note"><i class="fas fa-link"></i></a>
</h3>
<p>For t-tests, the sample and power are not as easy as z-test.</p>
<p><span class="math display">\[
\pi(\mu) = P(\frac{\bar{y}-\mu_0}{s/\sqrt{n}}&gt; t_{n-1;\alpha}|\mu)
\]</span></p>
<p>when <span class="math inline">\(\mu &gt; \mu_0\)</span> (i.e., <span class="math inline">\(\mu - \mu_0 = \delta\)</span>), the random variable <span class="math inline">\((\bar{y} - \mu_0)/(s/\sqrt{n})\)</span> does not have a <a href="prerequisites.html#student-t">Student’s t distribution</a>, but rather is distributed as a non-central t-distribution with non-centrality parameter <span class="math inline">\(\delta \sqrt{n}/\sigma\)</span> and d.f. of <span class="math inline">\(n-1\)</span></p>
<ul>
<li>The power is an increasing function of this non-centrality parameter (note, when <span class="math inline">\(\delta = 0\)</span> the distribution is usual Student’s t-distribution).</li>
<li>To evaluate power, one must consider numerical procedure or use special charts</li>
</ul>
<p>Approximate Sample Size Adjustment for t-test. We use an adjustment to the z-test determination for sample size.</p>
<p>Let <span class="math inline">\(v = n-1\)</span>, where n is sample size derived based on the z-test power. Then the 2-sided t-test sample size (approximate) is given:</p>
<p><span class="math display">\[
n^* = \frac{(t_{v;\alpha/2}+t_{v;\beta})^2 \sigma^2}{\delta^2}
\]</span></p>
</div>
<div id="one-sample-non-parametric-methods" class="section level3" number="4.1.7">
<h3>
<span class="header-section-number">4.1.7</span> One-sample Non-parametric Methods<a class="anchor" aria-label="anchor" href="#one-sample-non-parametric-methods"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lecture.data</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.76</span>, <span class="fl">0.82</span>, <span class="fl">0.80</span>, <span class="fl">0.79</span>, <span class="fl">1.06</span>, <span class="fl">0.83</span>,<span class="op">-</span><span class="fl">0.43</span>,<span class="op">-</span><span class="fl">0.34</span>, <span class="fl">3.34</span>, <span class="fl">2.33</span><span class="op">)</span></span></code></pre></div>
<div id="sign-test" class="section level4" number="4.1.7.1">
<h4>
<span class="header-section-number">4.1.7.1</span> Sign Test<a class="anchor" aria-label="anchor" href="#sign-test"><i class="fas fa-link"></i></a>
</h4>
<p>If we want to test <span class="math inline">\(H_0: \mu_{(0.5)} = 0; H_a: \mu_{(0.5)} &gt;0\)</span> where <span class="math inline">\(\mu_{(0.5)}\)</span> is the population median. We can</p>
<ol style="list-style-type: decimal">
<li>Count the number of observation (<span class="math inline">\(y_i\)</span>’s) that exceed 0. Denote this number by <span class="math inline">\(s_+\)</span>, called the number of plus signs. Let <span class="math inline">\(s_- = n - s_+\)</span>, which is the number of minus signs.</li>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(s_+\)</span> is large or equivalently, if <span class="math inline">\(s_-\)</span> is small.</li>
</ol>
<p>To determine how large <span class="math inline">\(s_+\)</span> must be to reject <span class="math inline">\(H_0\)</span> at a given significance level, we need to know the distribution of the corresponding random variable <span class="math inline">\(S_+\)</span> under the null hypothesis, which is a <a href="prerequisites.html#binomial">binomial</a> with p = 1/2,w hen the null is true.</p>
<p>To work out the null distribution using the binomial formula, we have <span class="math inline">\(\alpha\)</span>-level test rejects <span class="math inline">\(H_0\)</span> if <span class="math inline">\(s_+ \ge b_{n,\alpha}\)</span>, where <span class="math inline">\(b_{n,\alpha}\)</span> is the upper <span class="math inline">\(\alpha\)</span> critical point of the <span class="math inline">\(Bin(n,1/2)\)</span> distribution. Both <span class="math inline">\(S_+\)</span> and <span class="math inline">\(S_-\)</span> have this same distribution (<span class="math inline">\(S = S_+ = S_-\)</span>).</p>
<p><span class="math display">\[
\text{p-value} = P(S \ge s_+) = \sum_{i = s_+}^{n} {{n}\choose{i}} (\frac{1}{2})^n
\]</span> equivalently,</p>
<p><span class="math display">\[
P(S \le s_-) = \sum_{i=0}^{s_-}{{n}\choose{i}} (\frac{1}{2})^2
\]</span> For large sample sizes, we could use the normal approximation for the binomial, in which case reject <span class="math inline">\(H_0\)</span> if</p>
<p><span class="math display">\[
s_+ \ge n/2 + 1/2 + z_{\alpha}\sqrt{n/4}
\]</span></p>
<p>For the 2-sided test, we use the tests statistic <span class="math inline">\(s_{max} = max(s_+,s_-)\)</span> or <span class="math inline">\(s_{min} = min(s_+, s_-)\)</span>. An <span class="math inline">\(\alpha\)</span>-level test rejects <span class="math inline">\(H_0\)</span> if the p-value is <span class="math inline">\(\le \alpha\)</span>, where the p-value is computed from:</p>
<p><span class="math display">\[
p-value = 2 \sum_{i=s_{max}}^{n} {{n}\choose{i}} (\frac{1}{2})^n = s \sum_{i=0}^{s_{min}} {{n}\choose{i}} (\frac{1}{2})^n
\]</span> Equivalently, rejecting <span class="math inline">\(H_0\)</span> if <span class="math inline">\(s_{max} \ge b_{n,\alpha/2}\)</span></p>
<p>A large sample normal approximation can be used, where</p>
<p><span class="math display">\[
z = \frac{s_{max}- n/2 -1/2}{\sqrt{n/4}}
\]</span> and reject <span class="math inline">\(H_0\)</span> at <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(z \ge z_{\alpha/2}\)</span></p>
<p>However, treatment of 0 is problematic for this test.</p>
<ul>
<li>Solution 1: randomly assign 0 to the positive or negative (2 researchers might get different results).<br>
</li>
<li>Solution 2: count each 0 as a contribution 1/2 toward <span class="math inline">\(s_+\)</span> and <span class="math inline">\(s_-\)</span> (but then could not apply the <a href="prerequisites.html#binomial">binomial</a> distribution)<br>
</li>
<li>Solution 3: ignore 0 (reduces the power of test due to decreased sample size).</li>
</ul>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/binom.test.html">binom.test</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lecture.data</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">lecture.data</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Exact binomial test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  sum(lecture.data &gt; 0) and length(lecture.data)</span></span>
<span><span class="co">#&gt; number of successes = 8, number of trials = 10, p-value = 0.1094</span></span>
<span><span class="co">#&gt; alternative hypothesis: true probability of success is not equal to 0.5</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.4439045 0.9747893</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; probability of success </span></span>
<span><span class="co">#&gt;                    0.8</span></span>
<span><span class="co"># alternative = "greater" or alternative = "less"</span></span></code></pre></div>
</div>
<div id="wilcoxon-signed-rank-test" class="section level4" number="4.1.7.2">
<h4>
<span class="header-section-number">4.1.7.2</span> Wilcoxon Signed Rank Test<a class="anchor" aria-label="anchor" href="#wilcoxon-signed-rank-test"><i class="fas fa-link"></i></a>
</h4>
<p>Since the <a href="basic-statistical-inference.html#sign-test">Sign Test</a> could not consider the magnitude of each observation from 0, the <a href="basic-statistical-inference.html#wilcoxon-signed-rank-test">Wilcoxon Signed Rank Test</a> improves by taking account the ordered magnitudes of the observation, but it will impose the requirement of symmetric to this test (while <a href="basic-statistical-inference.html#sign-test">Sign Test</a> does not)</p>
<p><span class="math display">\[
H_0: \mu_{0.5} = 0 \\
H_a: \mu_{0.5} &gt; 0
\]</span> (assume no ties or same observations)</p>
<p>The signed rank test procedure:</p>
<ol style="list-style-type: decimal">
<li>rank order the observation <span class="math inline">\(y_i\)</span> in terms of their absolute values. Let <span class="math inline">\(r_i\)</span> be the rank of <span class="math inline">\(y_i\)</span> in this ordering. Since we assume no ties, the ranks <span class="math inline">\(r_i\)</span> are uniquely determined and are a permutation of the integers <span class="math inline">\(1,2,…,n\)</span>.<br>
</li>
<li>Calculate <span class="math inline">\(w_+\)</span>, which is the sum of the ranks of the positive values, and <span class="math inline">\(w_-\)</span>, which is the sum of the ranks of the negative values. Note that <span class="math inline">\(w_+ + w_- = r_1 + r_2 + ... = 1 + 2 + ... + n = n(n+1)/2\)</span><br>
</li>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(w_+\)</span> is large (or if <span class="math inline">\(w_-\)</span> is small)</li>
</ol>
<p>To know what is large or small with regard to <span class="math inline">\(w_+\)</span> and <span class="math inline">\(w_-\)</span>, we need the distribution of <span class="math inline">\(W_+\)</span> and <span class="math inline">\(W_-\)</span> when the null is true.</p>
<p>Since these null distributions are identical and symmetric, the p-value is <span class="math inline">\(P(W \ge w_+) = P(W \le w_-)\)</span></p>
<p>An <span class="math inline">\(\alpha\)</span>-level test rejects the null if the p-value is <span class="math inline">\(\le \alpha\)</span>, or if <span class="math inline">\(w_+ \ge w_{n,\alpha}\)</span>, where <span class="math inline">\(w_{n,\alpha}\)</span> is the upper <span class="math inline">\(\alpha\)</span> critical point of the null distribution of W.</p>
<p>This distribution of W has a special table. For large n, the distribution of W is approximately normal.</p>
<p><span class="math display">\[
z = \frac{w_+ - n(n+1) /4 -1/2}{\sqrt{n(n+1)(2n+1)/24}}
\]</span></p>
<p>The test rejects <span class="math inline">\(H_0\)</span> at level <span class="math inline">\(\alpha\)</span> if</p>
<p><span class="math display">\[
w_+ \ge n(n+1)/4 +1/2 + z_{\alpha}\sqrt{n(n+1)(2n+1)/24} \approx w_{n,\alpha}
\]</span></p>
<p>For the 2-sided test, we use <span class="math inline">\(w_{max}=max(w_+,w_-)\)</span> or <span class="math inline">\(w_{min}=min(w_+,w_-)\)</span>, with p-value given by:</p>
<p><span class="math display">\[
p-value = 2P(W \ge w_{max}) = 2P(W \le w_{min})
\]</span> Same as <a href="basic-statistical-inference.html#sign-test">Sign Test</a>,we ignore 0. In some cases where some of the <span class="math inline">\(|y_i|\)</span>’s may be tied for the same rank, we simply assign each of the tied ranks the average rank (or “midrank”).</p>
<p>Example, if <span class="math inline">\(y_1 = -1\)</span>, <span class="math inline">\(y_3 = 3\)</span> and <span class="math inline">\(y_3 = -3\)</span>, and <span class="math inline">\(y_4 =5\)</span>, then <span class="math inline">\(r_1 = 1\)</span>, <span class="math inline">\(r_2 = r_3=(2+3)/2 = 2.5\)</span>, <span class="math inline">\(r_4 = 4\)</span></p>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/wilcox.test.html">wilcox.test</a></span><span class="op">(</span><span class="va">lecture.data</span><span class="op">)</span> </span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wilcoxon signed rank exact test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  lecture.data</span></span>
<span><span class="co">#&gt; V = 52, p-value = 0.009766</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location is not equal to 0</span></span>
<span><span class="co"># does not use normal approximation</span></span>
<span><span class="co"># (using the underlying W distribution)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/wilcox.test.html">wilcox.test</a></span><span class="op">(</span><span class="va">lecture.data</span>,exact<span class="op">=</span><span class="cn">F</span><span class="op">)</span> </span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wilcoxon signed rank test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  lecture.data</span></span>
<span><span class="co">#&gt; V = 52, p-value = 0.01443</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location is not equal to 0</span></span>
<span><span class="co"># uses normal approximation</span></span></code></pre></div>
</div>
</div>
</div>
<div id="two-sample-inference" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Two Sample Inference<a class="anchor" aria-label="anchor" href="#two-sample-inference"><i class="fas fa-link"></i></a>
</h2>
<div id="means" class="section level3" number="4.2.1">
<h3>
<span class="header-section-number">4.2.1</span> Means<a class="anchor" aria-label="anchor" href="#means"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have 2 sets of observations,</p>
<ul>
<li>
<span class="math inline">\(y_1,..., y_{n_y}\)</span><br>
</li>
<li><span class="math inline">\(x_1,...,x_{n_x}\)</span></li>
</ul>
<p>that are random samples from two independent populations with means <span class="math inline">\(\mu_y\)</span> and <span class="math inline">\(\mu_x\)</span> and variances <span class="math inline">\(\sigma^2_y\)</span>,<span class="math inline">\(\sigma^2_x\)</span>. Our goal is to compare <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_y\)</span> or <span class="math inline">\(\sigma^2_y = \sigma^2_x\)</span></p>
<div id="large-sample-tests" class="section level4" number="4.2.1.1">
<h4>
<span class="header-section-number">4.2.1.1</span> Large Sample Tests<a class="anchor" aria-label="anchor" href="#large-sample-tests"><i class="fas fa-link"></i></a>
</h4>
<p>Assume that <span class="math inline">\(n_y\)</span> and <span class="math inline">\(n_x\)</span> are large (<span class="math inline">\(\ge 30\)</span>). Then,</p>
<p><span class="math display">\[
E(\bar{y} - \bar{x}) = \mu_y - \mu_x \\
Var(\bar{y} - \bar{x}) = \sigma^2_y /n_y + \sigma^2_x/n_x
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
Z = \frac{\bar{y}-\bar{x} - (\mu_y - \mu_x)}{\sqrt{\sigma^2_y /n_y + \sigma^2_x/n_x}} \sim N(0,1)
\]</span> (according to <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a>). For large samples, we can replace variances by their unbiased estimators (<span class="math inline">\(s^2_y,s^2_x\)</span>), and get the same large sample distribution.</p>
<p>An approximate <span class="math inline">\(100(1-\alpha) \%\)</span> CI for <span class="math inline">\(\mu_y - \mu_x\)</span> is given by:</p>
<p><span class="math display">\[
\bar{y} - \bar{x} \pm z_{\alpha/2}\sqrt{s^2_y/n_y + s^2_x/n_x}
\]</span></p>
<p><span class="math display">\[
H_0: \mu_y - \mu_x = \delta_0 \\
H_A: \mu_y - \mu_x \neq \delta_0
\]</span></p>
<p>at the <span class="math inline">\(\alpha\)</span>-level with the statistic:</p>
<p><span class="math display">\[
z = \frac{\bar{y}-\bar{x} - \delta_0}{\sqrt{s^2_y /n_y + s^2_x/n_x}}
\]</span></p>
<p>and reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|z| &gt; z_{\alpha/2}\)</span></p>
<p>If <span class="math inline">\(\delta = )\)</span>, it means that we are testing whether two means are equal.</p>
</div>
<div id="small-sample-tests" class="section level4" number="4.2.1.2">
<h4>
<span class="header-section-number">4.2.1.2</span> Small Sample Tests<a class="anchor" aria-label="anchor" href="#small-sample-tests"><i class="fas fa-link"></i></a>
</h4>
<p>If the two samples are from normal distribution, iid <span class="math inline">\(N(\mu_y,\sigma^2_y)\)</span> and iid <span class="math inline">\(N(\mu_x,\sigma^2_x)\)</span> and the two samples are independent, we can do inference based on the <a href="prerequisites.html#student-t">t-distribution</a></p>
<p>Then we have 2 cases</p>
<ul>
<li><a href="basic-statistical-inference.html#equal-variance">Equal Variance</a></li>
<li><a href="basic-statistical-inference.html#unequal-variance">Unequal Variance</a></li>
</ul>
<div id="equal-variance" class="section level5" number="4.2.1.2.1">
<h5>
<span class="header-section-number">4.2.1.2.1</span> Equal variance<a class="anchor" aria-label="anchor" href="#equal-variance"><i class="fas fa-link"></i></a>
</h5>
<p><strong>Assumptions</strong></p>
<ul>
<li>iid: so that <span class="math inline">\(var(\bar{y}) = \sigma^2_y / n_y ; var(\bar{x}) = \sigma^2_x / n_x\)</span><br>
</li>
<li>Independence between samples: No observation from one sample can influence any observation from the other sample, to have</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
var(\bar{y} - \bar{x}) &amp;= var(\bar{y}) + var{\bar{x}} - 2cov(\bar{y},\bar{x}) \\
&amp;= var(\bar{y}) + var{\bar{x}} \\
&amp;= \sigma^2_y / n_y + \sigma^2_x / n_x
\end{aligned}
\]</span></p>
<ul>
<li>Normality: Justifies the use of the <a href="prerequisites.html#student-t">t-distribution</a>
</li>
</ul>
<p>Let <span class="math inline">\(\sigma^2 = \sigma^2_y = \sigma^2_x\)</span>. Then, <span class="math inline">\(s^2_y\)</span> and <span class="math inline">\(s^2_x\)</span> are both unbiased estimators of <span class="math inline">\(\sigma^2\)</span>. We then can pool them.</p>
<p>Then the pooled variance estimate is <span class="math display">\[
s^2 = \frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y-1)+(n_x-1)}
\]</span> has <span class="math inline">\(n_y + n_x -2\)</span> df.</p>
<p>Then the test statistic</p>
<p><span class="math display">\[
T = \frac{\bar{y}- \bar{x} -(\mu_y - \mu_x)}{s\sqrt{1/n_y + 1/n_x}} \sim t_{n_y + n_x -2}
\]</span></p>
<p><span class="math inline">\(100(1 - \alpha) \%\)</span> CI for <span class="math inline">\(\mu_y - \mu_x\)</span> is</p>
<p><span class="math display">\[
\bar{y} - \bar{x} \pm (t_{n_y + n_x -2})s\sqrt{1/n_y + 1/n_x}
\]</span></p>
<p>Hypothesis testing:<br><span class="math display">\[
H_0: \mu_y - \mu_x = \delta_0 \\
H_1: \mu_y - \mu_x \neq \delta_0
\]</span></p>
<p>we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|t| &gt; t_{n_y + n_x -2;\alpha/2}\)</span></p>
</div>
<div id="unequal-variance" class="section level5" number="4.2.1.2.2">
<h5>
<span class="header-section-number">4.2.1.2.2</span> Unequal Variance<a class="anchor" aria-label="anchor" href="#unequal-variance"><i class="fas fa-link"></i></a>
</h5>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>Two samples are independent<br><ol style="list-style-type: decimal">
<li>Scatter plots<br>
</li>
<li><a href="descriptive-stat.html#correlation-coefficient-with-normal-probability-plots">Correlation coefficient (if normal)</a></li>
</ol>
</li>
<li>Independence of observation in each sample<br><ol style="list-style-type: decimal">
<li>Test for serial correlation<br>
</li>
</ol>
</li>
<li>For each sample, homogeneity of variance<br><ol style="list-style-type: decimal">
<li>Scatter plots<br>
</li>
<li>Formal tests<br>
</li>
</ol>
</li>
<li>
<a href="descriptive-stat.html#normality-assessment">Normality</a><br>
</li>
<li>Equality of variances (homogeneity of variance between samples)<br><ol style="list-style-type: decimal">
<li>
<a href="basic-statistical-inference.html#f-test">F-test</a><br>
</li>
<li>Barlett test<br>
</li>
<li>[Modified Levene Test]</li>
</ol>
</li>
</ol>
<p>To compare 2 normal <span class="math inline">\(\sigma^2_y \neq \sigma^2_x\)</span>, we use the test statistic:</p>
<p><span class="math display">\[
T = \frac{\bar{y}- \bar{x} -(\mu_y - \mu_x)}{\sqrt{s^2_y/n_y + s^2_x/n_x}}
\]</span> In this case, T does not follow the <a href="prerequisites.html#student-t">t-distribution</a> (its distribution depends on the ratio of the unknown variances <span class="math inline">\(\sigma^2_y,\sigma^2_x\)</span>). In the case of small sizes, we can can approximate tests by using the Welch-Satterthwaite method <span class="citation">(<a href="references.html#ref-Satterthwaite_1946">Satterthwaite 1946</a>)</span>. We assume T can be approximated by a <a href="prerequisites.html#student-t">t-distribution</a>, and adjust the degrees of freedom.</p>
<p>Let <span class="math inline">\(w_y = s^2_y /n_y\)</span> and <span class="math inline">\(w_x = s^2_x /n_x\)</span> (the w’s are the square of the respective standard errors)<br>
Then, the degrees of freedom are</p>
<p><span class="math display">\[
v = \frac{(w_y + w_x)^2}{w^2_y / (n_y-1) + w^2_x / (n_x-1)}
\]</span></p>
<p>Since v is usually fractional, we truncate down to the nearest integer.</p>
<p><span class="math inline">\(100 (1-\alpha) \%\)</span> CI for <span class="math inline">\(\mu_y - \mu_x\)</span> is</p>
<p><span class="math display">\[
\bar{y} - \bar{x} \pm t_{v,\alpha/2} \sqrt{s^2_y/n_y + s^2_x /n_x}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|t| &gt; t_{v,\alpha/2}\)</span>, where</p>
<p><span class="math display">\[
t = \frac{\bar{y} - \bar{x}-\delta_0}{\sqrt{s^2_y/n_y + s^2_x /n_x}}
\]</span></p>
</div>
</div>
</div>
<div id="variances" class="section level3" number="4.2.2">
<h3>
<span class="header-section-number">4.2.2</span> Variances<a class="anchor" aria-label="anchor" href="#variances"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[
F_{ndf,ddf}= \frac{s^2_1}{s^2_2}
\]</span></p>
<p>where <span class="math inline">\(s^2_1&gt;s^2_2, ndf = n_1-1,ddf = n_2-1\)</span></p>
<div id="f-test" class="section level4" number="4.2.2.1">
<h4>
<span class="header-section-number">4.2.2.1</span> F-test<a class="anchor" aria-label="anchor" href="#f-test"><i class="fas fa-link"></i></a>
</h4>
<p>Test</p>
<p><span class="math display">\[
H_0: \sigma^2_y = \sigma^2_x \\
H_a: \sigma^2_y \neq \sigma^2_x
\]</span></p>
<p>Consider the test statistic,</p>
<p><span class="math display">\[
F= \frac{s^2_y}{s^2_x}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if</p>
<ul>
<li>
<span class="math inline">\(F&gt;f_{n_y -1,n_x -1,\alpha/2}\)</span> or<br>
</li>
<li><span class="math inline">\(F&lt;f_{n_y -1,n_x -1,1-\alpha/2}\)</span></li>
</ul>
<p>Where <span class="math inline">\(F&gt;f_{n_y -1,n_x -1,\alpha/2}\)</span> and <span class="math inline">\(F&lt;f_{n_y -1,n_x -1,1-\alpha/2}\)</span> are the upper and lower <span class="math inline">\(\alpha/2\)</span> critical points of an <a href="prerequisites.html#f-distribution">F-distribution</a>, with a <span class="math inline">\(n_y-1\)</span> and <span class="math inline">\(n_x-1\)</span> degrees of freedom.</p>
<p><strong>Note</strong></p>
<ul>
<li>This test depends heavily on the assumption Normality.<br>
</li>
<li>In particular, it could give to many significant results when observations come from long-tailed distributions (i.e., positive kurtosis).<br>
</li>
<li>If we cannot find support for <a href="descriptive-stat.html#normality-assessment">normality</a>, then we can use nonparametric tests such as the <a href="basic-statistical-inference.html#modified-levene-test-brown-forsythe-test">Modified Levene Test (Brown-Forsythe Test)</a>
</li>
</ul>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span></span>
<span><span class="va">irisVe</span><span class="op">=</span><span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span><span class="op">==</span><span class="st">"versicolor"</span><span class="op">]</span> </span>
<span><span class="va">irisVi</span><span class="op">=</span><span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">[</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span><span class="op">==</span><span class="st">"virginica"</span><span class="op">]</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/var.test.html">var.test</a></span><span class="op">(</span><span class="va">irisVe</span>,<span class="va">irisVi</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  F test to compare two variances</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  irisVe and irisVi</span></span>
<span><span class="co">#&gt; F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335</span></span>
<span><span class="co">#&gt; alternative hypothesis: true ratio of variances is not equal to 1</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.2941935 0.9135614</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; ratio of variances </span></span>
<span><span class="co">#&gt;          0.5184243</span></span></code></pre></div>
</div>
<div id="modified-levene-test-brown-forsythe-test" class="section level4" number="4.2.2.2">
<h4>
<span class="header-section-number">4.2.2.2</span> Modified Levene Test (Brown-Forsythe Test)<a class="anchor" aria-label="anchor" href="#modified-levene-test-brown-forsythe-test"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>considers averages of absolute deviations rather than squared deviations. Hence, less sensitive to long-tailed distributions.<br>
</li>
<li>This test is still good for normal data</li>
</ul>
<p>For each sample, we consider the absolute deviation of each observation form the median:</p>
<p><span class="math display">\[
d_{y,i} = |y_i - y_{.5}| \\
d_{x,i} = |x_i - x_{.5}|
\]</span> Then,</p>
<p><span class="math display">\[
t_L^* = \frac{\bar{d}_y-\bar{d}_x}{s \sqrt{1/n_y + 1/n_x}}
\]</span></p>
<p>The pooled variance <span class="math inline">\(s^2\)</span> is given by:</p>
<p><span class="math display">\[
s^2 = \frac{\sum_i^{n_y}(d_{y,i}-\bar{d}_y)^2 + \sum_j^{n_x}(d_{x,i}-\bar{d}_x)^2}{n_y + n_x -2}
\]</span></p>
<ul>
<li>If the error terms have constant variance and <span class="math inline">\(n_y\)</span> and <span class="math inline">\(n_x\)</span> are not extremely small, then <span class="math inline">\(t_L^* \sim t_{n_x + n_y -2}\)</span><br>
</li>
<li>We reject the null hypothesis when <span class="math inline">\(|t_L^*| &gt; t_{n_y + n_x -2;\alpha/2}\)</span><br>
</li>
<li>This is just the two-sample t-test applied to the absolute deviations.</li>
</ul>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dVe</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">irisVe</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">irisVe</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="va">dVi</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">irisVi</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">irisVi</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">dVe</span>,<span class="va">dVi</span>,var.equal<span class="op">=</span><span class="cn">T</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Two Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  dVe and dVi</span></span>
<span><span class="co">#&gt; t = -2.5584, df = 98, p-value = 0.01205</span></span>
<span><span class="co">#&gt; alternative hypothesis: true difference in means is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -0.12784786 -0.01615214</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean of x mean of y </span></span>
<span><span class="co">#&gt;     0.154     0.226</span></span>
<span></span>
<span><span class="co"># small samples t-test  </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/t.test.html">t.test</a></span><span class="op">(</span><span class="va">irisVe</span>,<span class="va">irisVi</span>,var.equal<span class="op">=</span><span class="cn">F</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Welch Two Sample t-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  irisVe and irisVi</span></span>
<span><span class="co">#&gt; t = -14.625, df = 89.043, p-value &lt; 2.2e-16</span></span>
<span><span class="co">#&gt; alternative hypothesis: true difference in means is not equal to 0</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  -0.7951002 -0.6048998</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; mean of x mean of y </span></span>
<span><span class="co">#&gt;     1.326     2.026</span></span></code></pre></div>
</div>
</div>
<div id="power-1" class="section level3" number="4.2.3">
<h3>
<span class="header-section-number">4.2.3</span> Power<a class="anchor" aria-label="anchor" href="#power-1"><i class="fas fa-link"></i></a>
</h3>
<p>Consider <span class="math inline">\(\sigma^2_y = \sigma^2_x = \sigma^2\)</span><br>
Under the assumption of equal variances, we take size samples from both groups (<span class="math inline">\(n_y = n_x = n\)</span>)</p>
<p>For 1-sided testing,</p>
<p><span class="math display">\[
H_0: \mu_y - \mu_x \le 0 \\
H_a: \mu_y - \mu_x &gt; 0
\]</span></p>
<p><span class="math inline">\(\alpha\)</span>-level z-test rejects <span class="math inline">\(H_0\)</span> if</p>
<p><span class="math display">\[
z = \frac{\bar{y} - \bar{x}}{\sigma \sqrt{2/n}} &gt; z_{\alpha}
\]</span></p>
<p><span class="math display">\[
\pi(\mu_y - \mu_x) = \Phi(-z_{\alpha} + \frac{\mu_y -\mu_x}{\sigma}\sqrt{n/2})
\]</span></p>
<p>We need sample size n that give at least <span class="math inline">\(1-\beta\)</span> power when <span class="math inline">\(\mu_y - \mu_x = \delta\)</span>, where <span class="math inline">\(\delta\)</span> is the smallest difference that we want to see.</p>
<p>Power is given by:</p>
<p><span class="math display">\[
\Phi(-z_{\alpha} + \frac{\delta}{\sigma}\sqrt{n/2}) = 1 - \beta
\]</span></p>
</div>
<div id="sample-size-1" class="section level3" number="4.2.4">
<h3>
<span class="header-section-number">4.2.4</span> Sample Size<a class="anchor" aria-label="anchor" href="#sample-size-1"><i class="fas fa-link"></i></a>
</h3>
<p>Then, the sample size is</p>
<p><span class="math display">\[
n = 2(\frac{\sigma (z_{\alpha} + z_{\beta}}{\delta})^2
\]</span></p>
<p>For 2-sided test, replace <span class="math inline">\(z_{\alpha}\)</span> with <span class="math inline">\(z_{\alpha/2}\)</span>.<br>
As with the one-sample case, to perform an exact 2-sample t-test sample size calculation, we must use a non-central <a href="prerequisites.html#student-t">t-distribution</a>.</p>
<p>A correction that gives the approximate t-test sample size can be obtained by using the z-test n value in the formula:<br><span class="math display">\[
n^* = 2(\frac{\sigma (t_{2n-2;\alpha} + t_{2n-2;\beta})}{\delta})^2
\]</span></p>
<p>where we use <span class="math inline">\(\alpha/2\)</span> for the two-sided test</p>
</div>
<div id="matched-pair-designs" class="section level3" number="4.2.5">
<h3>
<span class="header-section-number">4.2.5</span> Matched Pair Designs<a class="anchor" aria-label="anchor" href="#matched-pair-designs"><i class="fas fa-link"></i></a>
</h3>
<p>We have two treatments</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Subject</th>
<th>Treatment A</th>
<th>Treatment B</th>
<th>Difference</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(y_1\)</span></td>
<td><span class="math inline">\(x_1\)</span></td>
<td><span class="math inline">\(d_1 = y_1 - x_1\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(y_2\)</span></td>
<td><span class="math inline">\(x_2\)</span></td>
<td><span class="math inline">\(d_2 = y_2 - x_2\)</span></td>
</tr>
<tr class="odd">
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr class="even">
<td>n</td>
<td><span class="math inline">\(y_n\)</span></td>
<td><span class="math inline">\(x_n\)</span></td>
<td><span class="math inline">\(d_n = y_n - x_n\)</span></td>
</tr>
</tbody>
</table></div>
<p>we assume <span class="math inline">\(y_i \sim^{iid} N(\mu_y, \sigma^2_y)\)</span> and <span class="math inline">\(x_i \sim^{iid} N(\mu_x,\sigma^2_x)\)</span>, but since <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> are measured on the same subject, they are correlated.</p>
<p>Let</p>
<p><span class="math display">\[
\mu_D = E(y_i - x_i) = \mu_y -\mu_x \\
\sigma^2_D = var(y_i - x_i) = Var(y_i) + Var(x_i) -2cov(y_i,x_i)
\]</span></p>
<p>If the matching induces <strong>positive</strong> correlation, then the variance of the difference of the measurements is reduced as compared to the independent case. This is the point of <a href="basic-statistical-inference.html#matched-pair-designs">Matched Pair Designs</a>. Although covariance can be negative, giving a larger variance of the difference than the independent sample case, usually the covariance is positive. This means both <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> are large for many of the same subjects, and for others, both measurement are small. (we still assume that various subjects respond independently of each other, which is necessary for the iid assumption within groups).</p>
<p>Let <span class="math inline">\(d_i = y_i - x_i\)</span>, then</p>
<ul>
<li>
<span class="math inline">\(\bar{d} = \bar{y}-\bar{x}\)</span> is the sample mean of the <span class="math inline">\(d_i\)</span><br>
</li>
<li>
<span class="math inline">\(s_d^2=\frac{1}{n-1}\sum_{i=1}^n (d_i - \bar{d})^2\)</span> is the sample variance of the difference</li>
</ul>
<p>Once the data are converted to differences, we are back to <a href="basic-statistical-inference.html#one-sample-inference">One Sample Inference</a> and can use its tests and CIs.</p>
</div>
<div id="nonparametric-tests-for-two-samples" class="section level3" number="4.2.6">
<h3>
<span class="header-section-number">4.2.6</span> Nonparametric Tests for Two Samples<a class="anchor" aria-label="anchor" href="#nonparametric-tests-for-two-samples"><i class="fas fa-link"></i></a>
</h3>
<p>For <a href="basic-statistical-inference.html#matched-pair-designs">Matched Pair Designs</a>, we can use the <a href="basic-statistical-inference.html#one-sample-non-parametric-methods">One-sample Non-parametric Methods</a>.</p>
<p>Assume that Y and X are random variables with CDF <span class="math inline">\(F_y\)</span> and <span class="math inline">\(F_x\)</span>. then, Y is <strong>stochastically</strong> larger than X for all real number u, <span class="math inline">\(P(Y &gt; u) \ge P(X &gt; u)\)</span>.</p>
<p>Equivalently, <span class="math inline">\(P(Y \le u) \le P(X \le u)\)</span>, which is <span class="math inline">\(F_Y(u) \le F_X(u)\)</span>, same thing as <span class="math inline">\(F_Y &lt; F_X\)</span></p>
<p>If two distributions are identical, except that one is shifted relative to the other, then each of distribution can be indexed by a location parameter, say <span class="math inline">\(\theta_y\)</span> and <span class="math inline">\(\theta_x\)</span>. In this case, <span class="math inline">\(Y&gt;X\)</span> if <span class="math inline">\(\theta_y &gt; \theta_x\)</span></p>
<p>Consider the hypotheses,</p>
<p><span class="math display">\[
H_0: F_Y = F_X \\
H_a: F_Y &lt; F_X
\]</span> where the alternative is an upper one-sided alternative.</p>
<ul>
<li>We can also consider the lower one-sided alternative</li>
</ul>
<p><span class="math display">\[
H_a: F_Y &gt; F_X \text{ or} \\
H_a: F_Y &lt; F_X \text{ or } F_Y &gt; F_X
\]</span></p>
<ul>
<li>In this case, we don’t use <span class="math inline">\(H_a: F_Y \neq F_X\)</span> as that allows arbitrary differences between the distributions, without requiring one be stochastically larger than the other.</li>
</ul>
<p>If the distributions only differ in terms of their location parameters, we can focus hypothesis tests on the parameters (e.g., <span class="math inline">\(H_0: \theta_y = \theta_x\)</span> vs. <span class="math inline">\(\theta_y &gt; \theta_x\)</span>)</p>
<p>We have 2 equivalent nonparametric tests that consider the hypothesis mentioned above</p>
<ol style="list-style-type: decimal">
<li>
<a href="basic-statistical-inference.html#wilcoxon-rank-test">Wilcoxon rank test</a><br>
</li>
<li><a href="basic-statistical-inference.html#mann-whitney-u-test">Mann-Whitney U test</a></li>
</ol>
<div id="wilcoxon-rank-test" class="section level4" number="4.2.6.1">
<h4>
<span class="header-section-number">4.2.6.1</span> Wilcoxon rank test<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-test"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>Combine all <span class="math inline">\(n= n_y + n_x\)</span> observations and rank them in ascending order.<br>
</li>
<li>Sum the ranks of the <span class="math inline">\(y\)</span>’s and <span class="math inline">\(x\)</span>’s separately. Let <span class="math inline">\(w_y\)</span> and <span class="math inline">\(w_x\)</span> be these sums. (<span class="math inline">\(w_y + w_x = 1 + 2 + ... + n = n(n+1)/2\)</span>)<br>
</li>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(w_y\)</span> is large (equivalently, <span class="math inline">\(w_x\)</span> is small)</li>
</ol>
<p>Under <span class="math inline">\(H_0\)</span>, any arrangement of the <span class="math inline">\(y\)</span>’s and <span class="math inline">\(x\)</span>’s is equally likely to occur, and there are <span class="math inline">\((n_y + n_x)!/(n_y! n_x!)\)</span> possible arrangements.</p>
<ul>
<li>Technically, for each arrangement we can compute the values of <span class="math inline">\(w_y\)</span> and <span class="math inline">\(w_x\)</span>, and thus generate the distribution of the statistic under the null hypothesis.<br>
</li>
<li>This could lead to computationally intensive.</li>
</ul>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/wilcox.test.html">wilcox.test</a></span><span class="op">(</span></span>
<span>    <span class="va">irisVe</span>,</span>
<span>    <span class="va">irisVi</span>,</span>
<span>    alternative <span class="op">=</span> <span class="st">"two.sided"</span>,</span>
<span>    conf.level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>    exact <span class="op">=</span> <span class="cn">F</span>,</span>
<span>    correct <span class="op">=</span> <span class="cn">T</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wilcoxon rank sum test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  irisVe and irisVi</span></span>
<span><span class="co">#&gt; W = 49, p-value &lt; 2.2e-16</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location shift is not equal to 0</span></span></code></pre></div>
</div>
<div id="mann-whitney-u-test" class="section level4" number="4.2.6.2">
<h4>
<span class="header-section-number">4.2.6.2</span> Mann-Whitney U test<a class="anchor" aria-label="anchor" href="#mann-whitney-u-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Mann-Whitney test is computed as follows:</p>
<ol style="list-style-type: decimal">
<li>Compare each <span class="math inline">\(y_i\)</span> with each <span class="math inline">\(x_i\)</span>.<br>
Let <span class="math inline">\(u_y\)</span> be the number of pairs in which <span class="math inline">\(y_i &gt; x_i\)</span> Let <span class="math inline">\(u_x\)</span> be the number of pairs in which <span class="math inline">\(y_i &lt; x_i\)</span>. (assume there are no ties). There are <span class="math inline">\(n_y n_x\)</span> such comparisons and <span class="math inline">\(u_y + u_x = n_y n_x\)</span>.<br>
</li>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(u_y\)</span> is large (or <span class="math inline">\(u_x\)</span> is small)</li>
</ol>
<p><a href="basic-statistical-inference.html#mann-whitney-u-test">Mann-Whitney U test</a> and <a href="basic-statistical-inference.html#wilcoxon-rank-test">Wilcoxon rank test</a> are related:<br><span class="math display">\[
u_y = w_y - n_y(n_y+1) /2 \\
u_x = w_x - n_x(n_x +1)/2
\]</span></p>
<p>An <span class="math inline">\(\alpha\)</span>-level test rejects <span class="math inline">\(H_0\)</span> if <span class="math inline">\(u_y \ge u_{n_y,n_x,\alpha}\)</span>, where <span class="math inline">\(u_{n_y,n_x,\alpha}\)</span> is the upper <span class="math inline">\(\alpha\)</span> critical point of the null distribution of the random variable, U.</p>
<p>The p-value is defined to be <span class="math inline">\(P(Y \ge u_y) = P(U \le u_x)\)</span>. One advantage of <a href="basic-statistical-inference.html#mann-whitney-u-test">Mann-Whitney U test</a> is that we can use either <span class="math inline">\(u_y\)</span> or <span class="math inline">\(u_x\)</span> to carry out the test.</p>
<p>For large <span class="math inline">\(n_y\)</span> and <span class="math inline">\(n_x\)</span>, the null distribution of U can be well approximated by a normal distribution with mean <span class="math inline">\(E(U) = n_y n_x /2\)</span> and variance <span class="math inline">\(var(U) = n_y n_x (n+1)/12\)</span>. A large sample z-test can be based on the statistic:</p>
<p><span class="math display">\[
z = \frac{u_y - n_y n_x /2 -1/2}{\sqrt{n_y n_x (n+1)/12}}
\]</span></p>
<p>The test rejects <span class="math inline">\(H_0\)</span> at level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(z \ge z_{\alpha}\)</span> or if <span class="math inline">\(u_y \ge u_{n_y,n_x,\alpha}\)</span> where</p>
<p><span class="math display">\[
u_{n_y, n_x, \alpha} \approx n_y n_x /2 + 1/2 + z_{\alpha}\sqrt{n_y n_x (n+1)/12}
\]</span></p>
<p>For the 2-sided test, we use the test statistic <span class="math inline">\(u_{max} = max(u_y,u_x)\)</span> and <span class="math inline">\(u_{min} = min(u_y, u_x)\)</span> and p-value is given by</p>
<p><span class="math display">\[
p-value = 2P(U \ge u_{max}) = 2P(U \le u_{min})
\]</span> Since we assume there are no ties (when <span class="math inline">\(y_i = x_j\)</span>), we count 1/2 towards both <span class="math inline">\(u_y\)</span> and <span class="math inline">\(u_x\)</span>. Even though the sampling distribution is not the same, but large sample approximation is still reasonable,</p>
</div>
</div>
</div>
<div id="categorical-data-analysis" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Categorical Data Analysis<a class="anchor" aria-label="anchor" href="#categorical-data-analysis"><i class="fas fa-link"></i></a>
</h2>
<p><a href="basic-statistical-inference.html#categorical-data-analysis">Categorical Data Analysis</a> when we have categorical outcomes</p>
<ul>
<li>Nominal variables: no logical ordering (e.g., sex)<br>
</li>
<li>Ordinal variables: logical order, but relative distances between values are not clear (e.g., small, medium, large)</li>
</ul>
<p>The distribution of one variable changes when the level (or values) of the other variable change. The row percentages are different in each column.</p>
<div id="inferences-for-small-samples" class="section level3" number="4.3.1">
<h3>
<span class="header-section-number">4.3.1</span> Inferences for Small Samples<a class="anchor" aria-label="anchor" href="#inferences-for-small-samples"><i class="fas fa-link"></i></a>
</h3>
<p>The approximate tests based on the asymptotic normality of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> do not apply for small samples.</p>
<p>Using <strong>Fisher’s Exact Test</strong> to evaluate <span class="math inline">\(H_0: p_1 = p_2\)</span></p>
<ul>
<li>Assume <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent <a href="prerequisites.html#binomial">Binomial</a><br>
</li>
<li>Let <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> be the corresponding observed values.</li>
<li>Let <span class="math inline">\(n= n_1 + n_2\)</span> be the total sample size</li>
<li>
<span class="math inline">\(m = x_1 + x_2\)</span> be the observed number of successes.<br>
</li>
<li>By assuming that m (total successes) is fixed, and conditioning on this value, one can show that the conditional distribution of the number of successes from sample 1 is <a href="prerequisites.html#hypergeometric">Hypergeometric</a><br>
</li>
<li>If we want to test <span class="math inline">\(H_0: p_1 = p_2\)</span> and <span class="math inline">\(H_a: p_1 \neq p_2\)</span>, we have</li>
</ul>
<p><span class="math display">\[
Z^2 = (\frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})(1/n_1 + 1/n_2)}})^2 \sim \chi_{1,\alpha}^2
\]</span></p>
<p>where <span class="math inline">\(\chi_{1,\alpha}^2\)</span> is the upper <span class="math inline">\(\alpha\)</span> percentage point for the central <a href="prerequisites.html#chi-squared">Chi-squared</a> with one d.f.</p>
<p>This extends to the contingency table setting: whether the observed frequencies are equal to those expected under a null hypothesis of no association.</p>
</div>
<div id="test-of-association" class="section level3" number="4.3.2">
<h3>
<span class="header-section-number">4.3.2</span> Test of Association<a class="anchor" aria-label="anchor" href="#test-of-association"><i class="fas fa-link"></i></a>
</h3>
<p>Pearson Chi-square test statistic is</p>
<p><span class="math display">\[
\chi^2 = \sum_{\text{all categories}} \frac{(observed - epxected)^2}{expected}
\]</span></p>
<p>Comparison of proportions for several independent surveys or experiments</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="30%">
<col width="20%">
<col width="20%">
<col width="7%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Experiment 1</th>
<th>Experiment 2</th>
<th>…</th>
<th>Experiment k</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Number of successes</td>
<td><span class="math inline">\(x_1\)</span></td>
<td><span class="math inline">\(x_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(x_k\)</span></td>
</tr>
<tr class="even">
<td>Number of failures</td>
<td><span class="math inline">\(n_1 - x_1\)</span></td>
<td><span class="math inline">\(n_2 - x_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k - x_k\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">\(n_1\)</span></td>
<td><span class="math inline">\(n_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k\)</span></td>
</tr>
</tbody>
</table></div>
<p><span class="math inline">\(H_0: p_1 = p_2 = \dots = p_k\)</span> vs. the alternative that the null is not true (at least one pair are not equal).</p>
<p>We estimate the common value of the probability of success on a single trial assuming <span class="math inline">\(H_0\)</span> is true:</p>
<p><span class="math display">\[
\hat{p} = \frac{x_1 + x_2 + ... + x_k}{n_1 + n_2 + ...+ n_k}
\]</span></p>
<p>we use table of expected counts when <span class="math inline">\(H_0\)</span> is true:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="13%">
<col width="26%">
<col width="26%">
<col width="7%">
<col width="27%">
</colgroup>
<tbody>
<tr class="odd">
<td>success</td>
<td><span class="math inline">\(n_1 \hat{p}\)</span></td>
<td><span class="math inline">\(n_2 \hat{p}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k \hat{p}\)</span></td>
</tr>
<tr class="even">
<td>failure</td>
<td><span class="math inline">\(n_1(1-\hat{p})\)</span></td>
<td><span class="math inline">\(n_2(1-\hat{p})\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k (1-\hat{p})\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">\(n_1\)</span></td>
<td><span class="math inline">\(n_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_k\)</span></td>
</tr>
</tbody>
</table></div>
<p><span class="math display">\[
\chi^2 = \sum_{\text{all cells in table}} \frac{(observed - expected)^2}{expected}
\]</span></p>
<p>with <span class="math inline">\(k-1\)</span> degrees of freedom</p>
<div id="two-way-count-data" class="section level4" number="4.3.2.1">
<h4>
<span class="header-section-number">4.3.2.1</span> Two-way Count Data<a class="anchor" aria-label="anchor" href="#two-way-count-data"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="18%">
<col width="13%">
<col width="13%">
<col width="6%">
<col width="13%">
<col width="6%">
<col width="13%">
<col width="14%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>1</th>
<th>2</th>
<th>…</th>
<th>j</th>
<th>…</th>
<th>c</th>
<th>Row Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(n_{11}\)</span></td>
<td><span class="math inline">\(n_{12}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{1j}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{1c}\)</span></td>
<td><span class="math inline">\(n_{1.}\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(n_{21}\)</span></td>
<td><span class="math inline">\(n_{22}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{2j}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{2c}\)</span></td>
<td><span class="math inline">\(n_{2.}\)</span></td>
</tr>
<tr class="odd">
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr class="even">
<td>r</td>
<td><span class="math inline">\(n_{r1}\)</span></td>
<td><span class="math inline">\(n_{r2}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{rj}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{rc}\)</span></td>
<td><span class="math inline">\(n_{r.}\)</span></td>
</tr>
<tr class="odd">
<td>Column Total</td>
<td><span class="math inline">\(n_{.1}\)</span></td>
<td><span class="math inline">\(n_{.2}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{.j}\)</span></td>
<td>…</td>
<td><span class="math inline">\(n_{.c}\)</span></td>
<td><span class="math inline">\(n_{}\)</span></td>
</tr>
</tbody>
</table></div>
<p><strong>Design 1</strong><br>
total sample size fixed <span class="math inline">\(n\)</span> = constant (e.g., survey on job satisfaction and income); both row and column totals are random variables</p>
<p><strong>Design 2</strong><br>
Fix the sample size in each group (in each row) (e.g., Drug treatments success or failure); fixed number of participants for each treatment; independent random samples from the two row populations.</p>
<p>These different sampling designs imply two different probability models.</p>
</div>
<div id="total-sample-size-fixed" class="section level4" number="4.3.2.2">
<h4>
<span class="header-section-number">4.3.2.2</span> Total Sample Size Fixed<a class="anchor" aria-label="anchor" href="#total-sample-size-fixed"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Design 1</strong></p>
<p>random sample of size n drawn from a single population, and sample units are cross-classified into <span class="math inline">\(r\)</span> row categories and <span class="math inline">\(c\)</span> column</p>
<p>This results in an <span class="math inline">\(r \times c\)</span> table of observed counts</p>
<p><span class="math inline">\(n_{ij} = 1,...,r;j=1,...,c\)</span></p>
<p>Let <span class="math inline">\(p_{ij}\)</span> be the probability of classification into cell <span class="math inline">\((i,j)\)</span> and <span class="math inline">\(\sum_{i=1}^r \sum_{j=1}^c p_{ij} = 1\)</span>. Let <span class="math inline">\(N_{ij}\)</span> be the random variable corresponding to <span class="math inline">\(n_{ij}\)</span><br>
The joint distribution of the <span class="math inline">\(N_{ij}\)</span> is multinomial with unknown parameters <span class="math inline">\(p_{ij}\)</span></p>
<p>Denote the row variable by <span class="math inline">\(X\)</span> and column variable by Y, then <span class="math inline">\(p_{ij} = P(X=i,Y = j)\)</span> and <span class="math inline">\(p_{i.} = P(X = i)\)</span> and <span class="math inline">\(p_{.j} = P(Y = j)\)</span> are the marginal probabilities.</p>
<p><br>
The null hypothesis that X and Y are statistically independent (i.e., no association) is just:</p>
<p><span class="math display">\[
H_0: p_{ij} = P(X =i,Y=j) = P(X =i) P(Y =j) = p_{i.}p_{.j} \\
H_a: p_{ij} \neq p_{i.}p_{.j}
\]</span> for all <span class="math inline">\(i,j\)</span>.</p>
</div>
<div id="row-total-fixed" class="section level4" number="4.3.2.3">
<h4>
<span class="header-section-number">4.3.2.3</span> Row Total Fixed<a class="anchor" aria-label="anchor" href="#row-total-fixed"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Design 2</strong></p>
<p>Random samples of sizes <span class="math inline">\(n_1,...,n_r\)</span> are drawn independently from <span class="math inline">\(r \ge 2\)</span> row populations. In this case, the 2-way table row totals are <span class="math inline">\(n_{i.} = n_i\)</span> for <span class="math inline">\(i = 1,...,r\)</span>.</p>
<p>The counts from each row are modeled by independent multinomial distributions.</p>
<p><span class="math inline">\(X\)</span> is fixed, <span class="math inline">\(Y\)</span> is observed.</p>
<p>Then, <span class="math inline">\(p_{ij}\)</span> represent conditional probabilities <span class="math inline">\(p_{ij} = P(Y=j|X=i)\)</span></p>
<p>The null hypothesis is the probability of response j is the same, regardless of the row population (i.e., no association):</p>
<p><span class="math display">\[
\begin{cases}
H_0: p_{ij} = P(Y = j | X = i) = p_j &amp; \text{for all } i,j =1,2,...,c \\
\text{or } H_0: (p_{i1},p_{i2},...,p_{ic}) = (p_1,p_2,...,p_c) &amp; \text{ for all } i\\
H_a: (p_{i1},p_{i2},...,p_{ic}) &amp; \text{ are not the same for all } i
\end{cases}
\]</span></p>
<p>Although the hypotheses to be tested are different for two sampling designs, <strong>The</strong> <span class="math inline">\(\chi^2\)</span> <strong>test is identical</strong></p>
<p>We have estimated expected frequencies:</p>
<p><span class="math display">\[
\hat{e}_{ij} = \frac{n_{i.}n_{.j}}{n}
\]</span></p>
<p>The Chi-square statistic is</p>
<p><span class="math display">\[
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(n_{ij}-\hat{e}_{ij})^2}{\hat{e}_{ij}} \sim \chi_{(r-1)(c-1)}
\]</span></p>
<p><span class="math inline">\(\alpha\)</span>-level test rejects <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\chi^2 &gt; \chi^2_{(r-1)(c-1),\alpha}\)</span></p>
</div>
<div id="pearson-chi-square-test" class="section level4" number="4.3.2.4">
<h4>
<span class="header-section-number">4.3.2.4</span> Pearson Chi-square Test<a class="anchor" aria-label="anchor" href="#pearson-chi-square-test"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>Determine whether an association exists<br>
</li>
<li>Sometimes, <span class="math inline">\(H_0\)</span> represents the model whose validity is to be tested. Contrast this with the conventional formulation of <span class="math inline">\(H_0\)</span> as the hypothesis that is to be disproved. The goal in this case is not to disprove the model, but to see whether data are consistent with the model and if deviation can be attributed to chance.<br>
</li>
<li>These tests do not measure the strength of an association.<br>
</li>
<li>These tests depend on and reflect the sample size - double the sample size by copying each observation, double the <span class="math inline">\(\chi^2\)</span> statistic even thought the strength of the association does not change.<br>
</li>
<li>The <a href="basic-statistical-inference.html#pearson-chi-square-test">Pearson Chi-square Test</a> is not appropriate when more than about 20% of the cells have an expected cell frequency of less than 5 (large-sample p-values not appropriate).<br>
</li>
<li>When the sample size is small the exact p-values can be calculated (this is prohibitive for large samples); calculation of the exact p-values assumes that the column totals and row totals are fixed.</li>
</ul>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">july.x</span><span class="op">=</span><span class="fl">480</span> </span>
<span><span class="va">july.n</span><span class="op">=</span><span class="fl">1000</span> </span>
<span><span class="va">sept.x</span><span class="op">=</span><span class="fl">704</span> </span>
<span><span class="va">sept.n</span><span class="op">=</span><span class="fl">1600</span></span></code></pre></div>
<p><span class="math display">\[
H_0: p_J = 0.5 \\
H_a: p_J &lt; 0.5
\]</span></p>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/prop.test.html">prop.test</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">july.x</span>,</span>
<span>    n <span class="op">=</span> <span class="va">july.n</span>,</span>
<span>    p <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>    alternative <span class="op">=</span> <span class="st">"less"</span>,</span>
<span>    correct <span class="op">=</span> <span class="cn">F</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  1-sample proportions test without continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  july.x out of july.n, null probability 0.5</span></span>
<span><span class="co">#&gt; X-squared = 1.6, df = 1, p-value = 0.103</span></span>
<span><span class="co">#&gt; alternative hypothesis: true p is less than 0.5</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.0000000 0.5060055</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt;    p </span></span>
<span><span class="co">#&gt; 0.48</span></span></code></pre></div>
<p><span class="math display">\[
H_0: p_J = p_S \\
H_a: p_j \neq p_S
\]</span></p>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/prop.test.html">prop.test</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">july.x</span>, <span class="va">sept.x</span><span class="op">)</span>,</span>
<span>    n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">july.n</span>, <span class="va">sept.n</span><span class="op">)</span>,</span>
<span>    correct <span class="op">=</span> <span class="cn">F</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  2-sample test for equality of proportions without continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  c(july.x, sept.x) out of c(july.n, sept.n)</span></span>
<span><span class="co">#&gt; X-squared = 3.9701, df = 1, p-value = 0.04632</span></span>
<span><span class="co">#&gt; alternative hypothesis: two.sided</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.0006247187 0.0793752813</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; prop 1 prop 2 </span></span>
<span><span class="co">#&gt;   0.48   0.44</span></span></code></pre></div>
</div>
</div>
<div id="ordinal-association" class="section level3" number="4.3.3">
<h3>
<span class="header-section-number">4.3.3</span> Ordinal Association<a class="anchor" aria-label="anchor" href="#ordinal-association"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>An ordinal association implies that as one variable increases, the other tends to increase or decrease (depending on the nature of the association).<br>
</li>
<li>For tests for variables with two or more levels, the levels must be in a logical ordering.</li>
</ul>
<div id="mantel-haenszel-chi-square-test" class="section level4" number="4.3.3.1">
<h4>
<span class="header-section-number">4.3.3.1</span> Mantel-Haenszel Chi-square Test<a class="anchor" aria-label="anchor" href="#mantel-haenszel-chi-square-test"><i class="fas fa-link"></i></a>
</h4>
<p>The <a href="basic-statistical-inference.html#mantel-haenszel-chi-square-test">Mantel-Haenszel Chi-square Test</a> is more powerful for testing ordinal associations, but does not test for the strength of the association.</p>
<p>This test is presented in the case where one has a series of <span class="math inline">\(2 \times 2\)</span> tables that examine the same effects under different conditions (If there are <span class="math inline">\(K\)</span> such tables, we have <span class="math inline">\(2 \times 2 \times K\)</span> table)</p>
<p>In stratum <span class="math inline">\(k\)</span>, given the marginal totals <span class="math inline">\((n_{.1k},n_{.2k},n_{1.k},n_{2.k})\)</span>, the sampling model for cell counts is the <a href="prerequisites.html#hypergeometric">Hypergeometric</a> (knowing <span class="math inline">\(n_{11k}\)</span> determines <span class="math inline">\((n_{12k},n_{21k},n_{22k})\)</span>, given the marginal totals)</p>
<p>Assuming conditional independence, the <a href="prerequisites.html#hypergeometric">Hypergeometric</a> mean and variance of <span class="math inline">\(n_{11k}\)</span> are</p>
<p><span class="math display">\[
m_{11k} = E(n_{11k}) = \frac{n_{1.k} n_{.1k}}{n_{..k}} \\
var(n_{11k}) = \frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2(n_{..k}-1)}
\]</span></p>
<p>To test conditional independence, Mantel and Haenszel proposed</p>
<p><span class="math display">\[
M^2 = \frac{(|\sum_{k} n_{11k} - \sum_k m_{11k}| -.5)^2}{\sum_{k}var(n_{11k})} \sim \chi^2_{1}
\]</span> This method can be extended to general <span class="math inline">\(I \times J \times K\)</span> tables.</p>
<p><span class="math inline">\((2 \times 2 \times 3)\)</span> table</p>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Bron</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">9</span>, <span class="fl">382</span>, <span class="fl">214</span>, <span class="fl">10</span>, <span class="fl">7</span>, <span class="fl">172</span>, <span class="fl">120</span>, <span class="fl">12</span>, <span class="fl">6</span>, <span class="fl">327</span>, <span class="fl">183</span><span class="op">)</span>,</span>
<span>    dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>    dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>        Particulate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"High"</span>, <span class="st">"Low"</span><span class="op">)</span>,</span>
<span>        Bronchitis <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Yes"</span>, <span class="st">"No"</span><span class="op">)</span>,</span>
<span>        Age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"15-24"</span>, <span class="st">"25-39"</span>, <span class="st">"40+"</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/marginSums.html">margin.table</a></span><span class="op">(</span><span class="va">Bron</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;            Bronchitis</span></span>
<span><span class="co">#&gt; Particulate Yes  No</span></span>
<span><span class="co">#&gt;        High  42 881</span></span>
<span><span class="co">#&gt;        Low   22 517</span></span>
<span><span class="co"># assess whether the relationship between </span></span>
<span><span class="co"># Bronchitis by Particulate Level varies by Age</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/pegeler/samplesizeCMH">samplesizeCMH</a></span><span class="op">)</span></span>
<span><span class="va">marginal_table</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/marginSums.html">margin.table</a></span><span class="op">(</span><span class="va">Bron</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/samplesizeCMH/man/odds.ratio.html">odds.ratio</a></span><span class="op">(</span><span class="va">marginal_table</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.120318</span></span>
<span></span>
<span><span class="co">#  whether these odds vary by age. </span></span>
<span><span class="co"># The conditional odds can be calculated using the original table.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">Bron</span>, <span class="fl">3</span>, <span class="va">odds.ratio</span><span class="op">)</span></span>
<span><span class="co">#&gt;     15-24     25-39       40+ </span></span>
<span><span class="co">#&gt; 1.2449098 0.9966777 1.1192661</span></span>
<span></span>
<span><span class="co"># Mantel-Haenszel Test</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/mantelhaen.test.html">mantelhaen.test</a></span><span class="op">(</span><span class="va">Bron</span>, correct <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Mantel-Haenszel chi-squared test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  Bron</span></span>
<span><span class="co">#&gt; Mantel-Haenszel X-squared = 0.11442, df = 1, p-value = 0.7352</span></span>
<span><span class="co">#&gt; alternative hypothesis: true common odds ratio is not equal to 1</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  0.6693022 1.9265813</span></span>
<span><span class="co">#&gt; sample estimates:</span></span>
<span><span class="co">#&gt; common odds ratio </span></span>
<span><span class="co">#&gt;          1.135546</span></span></code></pre></div>
<div id="mcnemars-test" class="section level5" number="4.3.3.1.1">
<h5>
<span class="header-section-number">4.3.3.1.1</span> McNemar’s Test<a class="anchor" aria-label="anchor" href="#mcnemars-test"><i class="fas fa-link"></i></a>
</h5>
<p>special case of <a href="basic-statistical-inference.html#mantel-haenszel-chi-square-test">Mantel-Haenszel Chi-square Test</a></p>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vote</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">682</span>, <span class="fl">22</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">86</span>, <span class="fl">810</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/mcnemar.test.html">mcnemar.test</a></span><span class="op">(</span><span class="va">vote</span>, correct <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  McNemar's Chi-squared test with continuity correction</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  vote</span></span>
<span><span class="co">#&gt; McNemar's chi-squared = 36.75, df = 1, p-value = 1.343e-09</span></span></code></pre></div>
</div>
</div>
<div id="spearman-rank-correlation" class="section level4" number="4.3.3.2">
<h4>
<span class="header-section-number">4.3.3.2</span> Spearman Rank Correlation<a class="anchor" aria-label="anchor" href="#spearman-rank-correlation"><i class="fas fa-link"></i></a>
</h4>
<p>To test for the strength of association between two ordinally scaled variables, we can use <a href="basic-statistical-inference.html#spearman-rank-correlation">Spearman Rank Correlation</a> statistic</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables measured on an ordinal scale. Consider <span class="math inline">\(n\)</span> pairs of observations (<span class="math inline">\(x_i,y_i\)</span>), <span class="math inline">\(i = 1,\dots,n\)</span></p>
<p>The <a href="basic-statistical-inference.html#spearman-rank-correlation">Spearman Rank Correlation</a> coefficient (denoted by <span class="math inline">\(r_S\)</span> is calculated using the Pearson correlation formula, but based on the ranks of <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>).</p>
<p><a href="basic-statistical-inference.html#spearman-rank-correlation">Spearman Rank Correlation</a> be calculated</p>
<ol style="list-style-type: decimal">
<li>Assign ranks to <span class="math inline">\(x_i\)</span>’s and <span class="math inline">\(y_i\)</span>’s separately. Let <span class="math inline">\(u_i = rank(x_i)\)</span> and <span class="math inline">\(v_i = rank(y_i)\)</span><br>
</li>
<li>Calculate <span class="math inline">\(r_S\)</span> using the formula for the Pearson correlation coefficient, but applied to the ranks:</li>
</ol>
<p><span class="math display">\[
r_S = \frac{\sum_{i=1}^{n}(u_i - \bar{u})(v_i - \bar{v})}{\sqrt{(\sum_{i = 1}^{n}(u_i - \bar{u})^2)(\sum_{i=1}^{n}(v_i - \bar{v})^2)}}
\]</span></p>
<p><span class="math inline">\(r_S\)</span> ranges between -1 and +1 , with</p>
<ul>
<li>
<span class="math inline">\(r_S = -1\)</span> if there is a perfect negative monotone association</li>
<li>
<span class="math inline">\(r_S = +1\)</span> if there is a perfect positive monotone association between X and Y.</li>
</ul>
<p>To test</p>
<ul>
<li><p><span class="math inline">\(H_0:\)</span> <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independent</p></li>
<li><p><span class="math inline">\(H_a\)</span>: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> positively associated</p></li>
</ul>
<p>For large <span class="math inline">\(n\)</span> (e.g., <span class="math inline">\(n \ge 10\)</span>),</p>
<p><span class="math display">\[
r_S \sim N(0,1/(n-1))
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
Z = r_s \sqrt{n-1} \sim N(0,1)
\]</span></p>
</div>
</div>
</div>
<div id="divergence-metrics-and-test-for-comparing-distributions" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Divergence Metrics and Test for Comparing Distributions<a class="anchor" aria-label="anchor" href="#divergence-metrics-and-test-for-comparing-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>Similarity among distributions using divergence statistics, which is different from</p>
<ul>
<li><p>Deviation statistics: difference between the realization of a variable and some value (e.g., mean). Statistics of the deviation distributions consist of standard deviation, average absolute deviation, median absolute deviation , maximum absolute deviation.</p></li>
<li><p>Deviance statistics: goodness-of-fit statistic for statistical models (comparable to the sum of squares of residuals in OLS to cases that use ML estimation). Usually used in generalized linear models.</p></li>
</ul>
<p>Divergence statistics is a statistical distance (different from metrics)</p>
<ul>
<li><p>Divergences do not require symmetry</p></li>
<li><p>Divergences generalize squared distance (instead of linear distance). Hence, fail the triangle inequity</p></li>
</ul>
<p>Can be used for</p>
<ul>
<li><p>Detecting data drift in machine learning</p></li>
<li><p>Feature selections</p></li>
<li><p>Variational Auto Encoder</p></li>
<li><p>Detect similarity between policies (i.e., distributions) in reinforcement learning</p></li>
<li><p>To see consistency in two measured variables of two constructs.</p></li>
</ul>
<p>Techniques</p>
<ul>
<li><p><a href="basic-statistical-inference.html#kullback-leibler-divergence">Kullback-Leibler Divergence</a></p></li>
<li><p><a href="basic-statistical-inference.html#jensen-shannon-divergence">Jensen-Shannon Divergence</a></p></li>
<li><p><a href="descriptive-stat.html#kolmogorov-smirnov-test">Kolmogorov-Smirnov Test</a></p></li>
</ul>
<p>Packages</p>
<ul>
<li><p><code>entropy</code></p></li>
<li><p><code>philentropy</code></p></li>
</ul>
<div id="kullback-leibler-divergence" class="section level3" number="4.4.1">
<h3>
<span class="header-section-number">4.4.1</span> Kullback-Leibler Divergence<a class="anchor" aria-label="anchor" href="#kullback-leibler-divergence"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Also known as relative entropy</p></li>
<li><p>Not a metric (does not satisfy the triangle inequality)</p></li>
<li><p>Can be generalized to the multivariate case</p></li>
<li>
<p>Measure the similarity between two discrete probability distributions</p>
<ul>
<li><p><span class="math inline">\(P\)</span> = true data distribution</p></li>
<li><p><span class="math inline">\(Q\)</span> = predicted data distribution</p></li>
</ul>
</li>
<li><p>It quantifies info loss when moving from <span class="math inline">\(P\)</span> to <span class="math inline">\(Q\)</span> (i.e., information loss when <span class="math inline">\(P\)</span> is approximated by <span class="math inline">\(Q\)</span>)</p></li>
</ul>
<p>Discrete</p>
<p><span class="math display">\[
D_{KL}(P ||Q) = \sum_i P_i \log(\frac{P_i}{Q_i})
\]</span></p>
<p>Continuous</p>
<p><span class="math display">\[
D_{KL}(P||Q) = \int P(x) \log(\frac{P(x)}{Q(x)}) dx
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(K \in [0, \infty)\)</span> from similar to diverge</p></li>
<li><p>Non-symmetric between two distributions: <span class="math inline">\(D_{KL}(P|Q) \neq D_{KL}(Q|P)\)</span></p></li>
</ul>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/drostlab/philentropy">philentropy</a></span><span class="op">)</span></span>
<span><span class="co"># philentropy::dist.diversity(rbind(X = 1:10 / sum(1:10), </span></span>
<span><span class="co">#                                   Y = 1:20 / sum(1:20)),</span></span>
<span><span class="co">#                             p = 2,</span></span>
<span><span class="co">#                             unit = "log2")</span></span>
<span></span>
<span></span>
<span><span class="co"># continuous</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/KL.html">KL</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>X <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, Y <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span>, unit <span class="op">=</span> <span class="st">"log2"</span><span class="op">)</span></span>
<span><span class="co">#&gt; kullback-leibler </span></span>
<span><span class="co">#&gt;                0</span></span>
<span></span>
<span><span class="co"># discrete</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/KL.html">KL</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>X <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, Y <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, est.prob <span class="op">=</span> <span class="st">"empirical"</span><span class="op">)</span></span>
<span><span class="co">#&gt; kullback-leibler </span></span>
<span><span class="co">#&gt;                0</span></span></code></pre></div>
</div>
<div id="jensen-shannon-divergence" class="section level3" number="4.4.2">
<h3>
<span class="header-section-number">4.4.2</span> Jensen-Shannon Divergence<a class="anchor" aria-label="anchor" href="#jensen-shannon-divergence"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Also known as info radius or total divergence to the average</li>
</ul>
<p><span class="math display">\[
D_{JS} (P ||Q) = \frac{1}{2}( D_{KL}(P||M)+ D_{KL}(Q||M))
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(M = \frac{1}{2} (P + Q)\)</span> is a mixed distribution</p></li>
<li><p><span class="math inline">\(D_{JS} \in [0,1]\)</span> for <span class="math inline">\(\log_2\)</span> and <span class="math inline">\(D_{JS} \in [0,\ln(2)]\)</span> for <span class="math inline">\(\log_e\)</span></p></li>
</ul>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/drostlab/philentropy">philentropy</a></span><span class="op">)</span></span>
<span><span class="co"># continous</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/JSD.html">JSD</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>X <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, Y <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span>, unit <span class="op">=</span> <span class="st">"log2"</span><span class="op">)</span></span>
<span><span class="co">#&gt; jensen-shannon </span></span>
<span><span class="co">#&gt;       20.03201</span></span>
<span></span>
<span><span class="co"># discrete</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/philentropy/man/JSD.html">JSD</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span>X <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, Y <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span>, est.prob <span class="op">=</span> <span class="st">"empirical"</span><span class="op">)</span></span>
<span><span class="co">#&gt; jensen-shannon </span></span>
<span><span class="co">#&gt;     0.06004756</span></span></code></pre></div>
</div>
<div id="wasserstein-distance" class="section level3" number="4.4.3">
<h3>
<span class="header-section-number">4.4.3</span> Wasserstein Distance<a class="anchor" aria-label="anchor" href="#wasserstein-distance"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>measure the distance between two empirical CDFs</li>
</ul>
<p><span class="math display">\[
W = \int_{x \in R}|E(x) - F(X)|^p
\]</span></p>
<ul>
<li>This is also a test statistics</li>
</ul>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu">transport</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/transport/man/wasserstein1d.html">wasserstein1d</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.8533046</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># Wasserstein metric </span></span>
<span><span class="fu">twosamples</span><span class="fu">::</span><span class="fu"><a href="https://twosampletest.com/reference/wass_test.html">wass_stat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.8533046</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># permutation-based tw sample test using Wasserstein metric</span></span>
<span><span class="fu">twosamples</span><span class="fu">::</span><span class="fu"><a href="https://twosampletest.com/reference/wass_test.html">wass_test</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Test Stat   P-Value </span></span>
<span><span class="co">#&gt; 0.8533046 0.0002500</span></span></code></pre></div>
</div>
<div id="kolmogorov-smirnov-test-1" class="section level3" number="4.4.4">
<h3>
<span class="header-section-number">4.4.4</span> Kolmogorov-Smirnov Test<a class="anchor" aria-label="anchor" href="#kolmogorov-smirnov-test-1"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Can be used for continuous distribution</li>
</ul>
<p><span class="math inline">\(H_0\)</span>: Empirical distribution follows a specified distribution</p>
<p><span class="math inline">\(H_1\)</span>: Empirical distribution does not follow a specified distribution</p>
<ul>
<li>Using non-parametric</li>
</ul>
<p><span class="math display">\[
D= \max|P(X) - Q(X)|
\]</span></p>
<ul>
<li>
<span class="math inline">\(D \in [0,1]\)</span> from the densities are evenly distributed to not evenly distributed</li>
</ul>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://strimmerlab.github.io/software/entropy/">entropy</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">lst</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>sample_1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">)</span>, sample_2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span><span class="op">:</span><span class="fl">30</span><span class="op">)</span>, sample_3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span><span class="op">:</span><span class="fl">30</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">lst</span><span class="op">)</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">lst</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rowwise.html">rowwise</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>KL <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/entropy/man/entropy.empirical.html">KL.empirical</a></span><span class="op">(</span><span class="va">lst</span><span class="op">[[</span><span class="va">Var1</span><span class="op">]</span><span class="op">]</span>, <span class="va">lst</span><span class="op">[[</span><span class="va">Var2</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 9 × 3</span></span>
<span><span class="co">#&gt; # Rowwise: </span></span>
<span><span class="co">#&gt;    Var1  Var2     KL</span></span>
<span><span class="co">#&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1     1     1 0     </span></span>
<span><span class="co">#&gt; 2     2     1 0.150 </span></span>
<span><span class="co">#&gt; 3     3     1 0.183 </span></span>
<span><span class="co">#&gt; 4     1     2 0.704 </span></span>
<span><span class="co">#&gt; 5     2     2 0     </span></span>
<span><span class="co">#&gt; 6     3     2 0.0679</span></span>
<span><span class="co">#&gt; 7     1     3 0.622 </span></span>
<span><span class="co">#&gt; 8     2     3 0.0870</span></span>
<span><span class="co">#&gt; 9     3     3 0</span></span></code></pre></div>
<p>To use the test for discrete date, use bootstrap version of the KS test (bypass the continuity requirement)</p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">Matching</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Matching/man/ks.boot.html">ks.boot</a></span><span class="op">(</span>Tr <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, Co <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; $ks.boot.pvalue</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $ks</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Exact two-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  Tr and Co</span></span>
<span><span class="co">#&gt; D = 0, p-value = 1</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $nboots</span></span>
<span><span class="co">#&gt; [1] 1000</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attr(,"class")</span></span>
<span><span class="co">#&gt; [1] "ks.boot"</span></span></code></pre></div>

</div>
</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="descriptive-stat.html"><span class="header-section-number">3</span> Descriptive Statistics</a></div>
<div class="next"><a href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#basic-statistical-inference"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li>
<a class="nav-link" href="#one-sample-inference"><span class="header-section-number">4.1</span> One Sample Inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-mean"><span class="header-section-number">4.1.1</span> The Mean</a></li>
<li><a class="nav-link" href="#single-variance"><span class="header-section-number">4.1.2</span> Single Variance</a></li>
<li><a class="nav-link" href="#single-proportion-p"><span class="header-section-number">4.1.3</span> Single Proportion (p)</a></li>
<li><a class="nav-link" href="#power"><span class="header-section-number">4.1.4</span> Power</a></li>
<li><a class="nav-link" href="#sample-size"><span class="header-section-number">4.1.5</span> Sample Size</a></li>
<li><a class="nav-link" href="#note"><span class="header-section-number">4.1.6</span> Note</a></li>
<li><a class="nav-link" href="#one-sample-non-parametric-methods"><span class="header-section-number">4.1.7</span> One-sample Non-parametric Methods</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#two-sample-inference"><span class="header-section-number">4.2</span> Two Sample Inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#means"><span class="header-section-number">4.2.1</span> Means</a></li>
<li><a class="nav-link" href="#variances"><span class="header-section-number">4.2.2</span> Variances</a></li>
<li><a class="nav-link" href="#power-1"><span class="header-section-number">4.2.3</span> Power</a></li>
<li><a class="nav-link" href="#sample-size-1"><span class="header-section-number">4.2.4</span> Sample Size</a></li>
<li><a class="nav-link" href="#matched-pair-designs"><span class="header-section-number">4.2.5</span> Matched Pair Designs</a></li>
<li><a class="nav-link" href="#nonparametric-tests-for-two-samples"><span class="header-section-number">4.2.6</span> Nonparametric Tests for Two Samples</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#categorical-data-analysis"><span class="header-section-number">4.3</span> Categorical Data Analysis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#inferences-for-small-samples"><span class="header-section-number">4.3.1</span> Inferences for Small Samples</a></li>
<li><a class="nav-link" href="#test-of-association"><span class="header-section-number">4.3.2</span> Test of Association</a></li>
<li><a class="nav-link" href="#ordinal-association"><span class="header-section-number">4.3.3</span> Ordinal Association</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#divergence-metrics-and-test-for-comparing-distributions"><span class="header-section-number">4.4</span> Divergence Metrics and Test for Comparing Distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#kullback-leibler-divergence"><span class="header-section-number">4.4.1</span> Kullback-Leibler Divergence</a></li>
<li><a class="nav-link" href="#jensen-shannon-divergence"><span class="header-section-number">4.4.2</span> Jensen-Shannon Divergence</a></li>
<li><a class="nav-link" href="#wasserstein-distance"><span class="header-section-number">4.4.3</span> Wasserstein Distance</a></li>
<li><a class="nav-link" href="#kolmogorov-smirnov-test-1"><span class="header-section-number">4.4.4</span> Kolmogorov-Smirnov Test</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/04-basic-inference.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/04-basic-inference.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2023-08-21.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
