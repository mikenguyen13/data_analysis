<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 24 Regression Discontinuity | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="A regression discontinuity occurs when there is a discrete change (jump) in treatment likelihood in the distribution of a continuous (or roughly continuous) variable (i.e.,...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 24 Regression Discontinuity | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/regression-discontinuity.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="A regression discontinuity occurs when there is a discrete change (jump) in treatment likelihood in the distribution of a continuous (or roughly continuous) variable (i.e.,...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 24 Regression Discontinuity | A Guide on Data Analysis">
<meta name="twitter:description" content="A regression discontinuity occurs when there is a discrete change (jump) in treatment likelihood in the distribution of a continuous (or roughly continuous) variable (i.e.,...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-stat.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="" href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="active" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="synthetic-difference-in-differences.html"><span class="header-section-number">25</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">26</span> Difference-in-differences</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">27</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">28</span> Event Studies</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">29</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">30</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">31</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">32</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">33</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="mediation.html"><span class="header-section-number">34</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">35</span> Directed Acyclic Graph</a></li>
<li><a class="" href="report.html"><span class="header-section-number">36</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">37</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">38</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">39</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="regression-discontinuity" class="section level1" number="24">
<h1>
<span class="header-section-number">24</span> Regression Discontinuity<a class="anchor" aria-label="anchor" href="#regression-discontinuity"><i class="fas fa-link"></i></a>
</h1>
<ul>
<li>
<p>A regression discontinuity occurs when there is a discrete change (jump) in treatment likelihood in the distribution of a continuous (or roughly continuous) variable (i.e., <strong>running/forcing/assignment variable</strong>).</p>
<ul>
<li>Running variable can also be time, but the argument for time to be continuous is hard to argue because usually we do not see increment of time (e.g., quarterly or annual data). Unless we have minute or hour data, then we might be able to argue for it.</li>
</ul>
</li>
<li><p>Review paper <span class="citation">(<a href="references.html#ref-imbens2008regression">G. Imbens and Lemieux 2008</a>; <a href="references.html#ref-lee2010regression">Lee and Lemieux 2010</a>)</span></p></li>
<li>
<p>Other readings:</p>
<ul>
<li><p><a href="https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdf" class="uri">https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdf</a></p></li>
<li><p><a href="https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf" class="uri">https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf</a></p></li>
</ul>
</li>
<li><p><span class="citation">(<a href="references.html#ref-thistlethwaite1960">Thistlethwaite and Campbell 1960</a>)</span>: first paper to use RD in the context of merit awards on future academic outcomes.</p></li>
<li>
<p>RD is a localized experiment at the cutoff point</p>
<ul>
<li>Hence, we always have to qualify (perfunctory) our statement in research articles that “our research might not generalize to beyond the bandwidth.”</li>
</ul>
</li>
<li><p>In reality, RD and experimental (from random assignment) estimates are very similar (<span class="citation">(<a href="references.html#ref-chaplin2018internal">Chaplin et al. 2018</a>)</span>; <a href="https://www.mathematica.org/publications/replicating-experimental-impact-estimates-using-a-regression-discontinuity-approach">Mathematica</a>). But still, it’s hard to prove empirically for every context (there might be future study that finds a huge difference between local estimate - causal - and overall estimate - random assignment.</p></li>
<li><p>Threats: only valid near threshold: inference at threshold is valid on average. Interestingly, random experiment showed the validity already.</p></li>
<li><p>Tradeoff between efficiency and bias</p></li>
<li><p>Regression discontinuity is under the framework of <a href="endogeneity.html#instrumental-variable">Instrumental Variable</a> argued by <span class="citation">(<a href="references.html#ref-angrist1999using">Angrist and Lavy 1999</a>)</span> and a special case of the <a href="matching-methods.html#matching-methods">Matching Methods</a> (matching at one point) argued by <span class="citation">(<a href="references.html#ref-heckman1999economics">James J. Heckman, LaLonde, and Smith 1999</a>)</span>.</p></li>
<li><p>The hard part is to find a setting that can apply, but once you find one, it’s easy to apply</p></li>
<li><p>We can also have multiple cutoff lines. However, for each cutoff line, there can only be one breakup point</p></li>
<li><p>RD can have multiple coinciding effects (i.e., joint distribution or bundled treatment), then RD effect in this case would be the joint effect.</p></li>
<li><p>As the running variable becomes more discrete your framework should be <a href="interrupted-time-series.html#interrupted-time-series">Interrupted Time Series</a>, but for more granular levels you can use RD. When you have infinite data (or substantially large) the two frameworks are identical. RD is always better than <a href="interrupted-time-series.html#interrupted-time-series">Interrupted Time Series</a></p></li>
<li>
<p>Multiple alternative model specifications that produce consistent results are more reliable (parametric - linear regression with polynomials terms, and non-parametric - local linear regression). This is according to <span class="citation">(<a href="references.html#ref-lee2010regression">Lee and Lemieux 2010</a>)</span>, one straightforward method to ease the linearity assumption is by incorporating polynomial functions of the forcing variable. The choice of polynomial terms can be determined based on the data.</p>
<ul>
<li>. According to <span class="citation">(<a href="references.html#ref-gelman2019high">Gelman and Imbens 2019</a>)</span>, accounting for global high-order polynomials presents three issues: (1) imprecise estimates due to noise, (2) sensitivity to the polynomial’s degree, and (3) inadequate coverage of confidence intervals. To address this, researchers should instead employ estimators that rely on local linear or quadratic polynomials or other smooth functions.</li>
</ul>
</li>
<li><p>RD should be viewed more as a description of a data generating process, rather than a method or approach (similar to a randomized experiment)</p></li>
<li>
<p>RD is close to</p>
<ul>
<li><p>other quasi-experimental methods in the sense that it’s based on the discontinuity at a threshold</p></li>
<li><p>randomized experiments in the sense that it’s local randomization.</p></li>
</ul>
</li>
</ul>
<p>There are several types of Regression Discontinuity:</p>
<ol style="list-style-type: decimal">
<li>
<p>Sharp RD: Change in treatment probability at the cutoff point is 1</p>
<ul>
<li>Kink design: Instead of a discontinuity in the level of running variable, we have a discontinuity in the slope of the function (while the function/level can remain continuous) <span class="citation">(<a href="references.html#ref-nielsen2010estimating">Nielsen, Sørensen, and Taber 2010</a>)</span>. See <span class="citation">(<a href="references.html#ref-bockerman2018kink">Böckerman, Kanninen, and Suoniemi 2018</a>)</span> for application, and <span class="citation">(<a href="references.html#ref-card2015inference">Card et al. 2015</a>)</span> for theory.</li>
</ul>
</li>
<li><p>Kink RD</p></li>
<li><p>Fuzzy RD: Change in treatment probability less than 1</p></li>
<li><p>Fuzzy Kink RD</p></li>
<li><p>RDiT: running variable is time.</p></li>
</ol>
<p>Others:</p>
<ul>
<li><p>Multiple cutoff</p></li>
<li><p>Multiple Scores</p></li>
<li><p>Geographic RD</p></li>
<li><p>Dynamic Treatments</p></li>
<li><p>Continuous Treatments</p></li>
</ul>
<p>Consider</p>
<p><span class="math display">\[
D_i = 1_{X_i &gt; c}
\]</span></p>
<p><span class="math display">\[
D_i =
\begin{cases}
D_i = 1 \text{ if } X_i &gt; C \\
D_i = 0 \text{ if } X_i &lt; C
\end{cases}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(D_i\)</span> = treatment effect</p></li>
<li><p><span class="math inline">\(X_i\)</span> = score variable (continuous)</p></li>
<li><p><span class="math inline">\(c\)</span> = cutoff point</p></li>
</ul>
<p><strong>Identification (Identifying assumption</strong>s) of RD:</p>
<p>Average Treatment Effect at the cutoff (<a href="regression-discontinuity.html#continuity-based">Continuity-based</a>)</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_{SRDD} &amp;= E[Y_{1i} - Y_{0i} | X_i = c] \\
&amp;= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c]\\
&amp;= \lim_{x \to c^+} E[Y_{1i}|X_i = c] - \lim_{x \to c^=} E[Y_{0i}|X_i = c]
\end{aligned}
\]</span></p>
<p>Average Treatment Effect in a neighborhood (<a href="regression-discontinuity.html#local-randomization-based">Local Randomization-based</a>):</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_{LR} &amp;= E[Y_{1i} - Y_{0i}|X_i \in W] \\
&amp;= \frac{1}{N_1} \sum_{X_i \in W, T_i = 1}Y_i - \frac{1}{N_0}\sum_{X_i \in W, T_i =0} Y_i
\end{aligned}
\]</span></p>
<p>RDD estimates the local average treatment effect (LATE), at the cutoff point which is not at the individual or population levels.</p>
<p>Since researchers typically care more about the internal validity, than external validity, localness affects only external validity.</p>
<p><strong>Assumptions</strong>:</p>
<ul>
<li><p>Independent assignment</p></li>
<li>
<p>Continuity of conditional regression functions</p>
<ul>
<li>
<span class="math inline">\(E[Y(0)|X=x]\)</span> and <span class="math inline">\(E[Y(1)|X=x]\)</span> are continuous in x.</li>
</ul>
</li>
<li><p>RD is valid if cutpoint is <strong>exogenous (i.e., no endogenous selection)</strong> and running variable is <strong>not manipulable</strong></p></li>
<li><p>Only treatment(s) (e.g., could be joint distribution of multiple treatments) cause discontinuity or jump in the outcome variable</p></li>
<li><p>All other factors are <strong>smooth</strong> through the cutoff (i.e., threshold) value. (we can also test this assumption by seeing no discontinuity in other factors). If they “jump”, they will bias your causal estimate</p></li>
</ul>
<p><strong>Threats to RD</strong></p>
<ul>
<li>
<p>Variables (other than treatment) change discontinuously at the cutoff</p>
<ul>
<li>We can test for jumps in these variables (including pre-treatment outcome)</li>
</ul>
</li>
<li><p>Multiple discontinuities for the assignment variable</p></li>
<li>
<p>Manipulation of the assignment variable</p>
<ul>
<li>At the cutoff point, check for continuity in the density of the assignment variable.</li>
</ul>
</li>
</ul>
<div id="estimation-and-inference" class="section level2" number="24.1">
<h2>
<span class="header-section-number">24.1</span> Estimation and Inference<a class="anchor" aria-label="anchor" href="#estimation-and-inference"><i class="fas fa-link"></i></a>
</h2>
<div id="local-randomization-based" class="section level3" number="24.1.1">
<h3>
<span class="header-section-number">24.1.1</span> Local Randomization-based<a class="anchor" aria-label="anchor" href="#local-randomization-based"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Additional Assumption</strong>: Local Randomization approach assumes that inside the chosen window <span class="math inline">\(W = [c-w, c+w]\)</span> are assigned to treatment as good as random:</p>
<ol style="list-style-type: decimal">
<li>Joint probability distribution of scores for units inside the chosen window <span class="math inline">\(W\)</span> is known</li>
<li>Potential outcomes are not affected by value of the score</li>
</ol>
<p>This approach is stronger than the <a href="regression-discontinuity.html#continuity-based">Continuity-based</a> because we assume the regressions are continuously at <span class="math inline">\(c\)</span> and unaffected by the running variable within window <span class="math inline">\(W\)</span></p>
<p>Because we can choose the window <span class="math inline">\(W\)</span> (within which random assignment is plausible), the sample size can typically be small.</p>
<p>To choose the window <span class="math inline">\(W\)</span>, we can base on either</p>
<ol style="list-style-type: decimal">
<li>where the pre-treatment covariate-balance is observed</li>
<li>independent tests between outcome and score</li>
<li>domain knowledge</li>
</ol>
<p>To make inference, we can either use</p>
<ul>
<li><p>(Fisher) randomization inference</p></li>
<li><p>(Neyman) design-based</p></li>
</ul>
</div>
<div id="continuity-based" class="section level3" number="24.1.2">
<h3>
<span class="header-section-number">24.1.2</span> Continuity-based<a class="anchor" aria-label="anchor" href="#continuity-based"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<p>also known as the local polynomial method</p>
<ul>
<li>as the name suggests, global polynomial regression is not recommended (because of lack of robustness, and over-fitting and Runge’s phenomenon)</li>
</ul>
</li>
</ul>
<p>Step to estimate local polynomial regression</p>
<ol style="list-style-type: decimal">
<li>Choose polynomial order and weighting scheme</li>
<li>Choose bandwidth that has optimal MSE or coverage error</li>
<li>Estimate the parameter of interest</li>
<li>Examine robust bias-correct inference</li>
</ol>
</div>
</div>
<div id="specification-checks" class="section level2" number="24.2">
<h2>
<span class="header-section-number">24.2</span> Specification Checks<a class="anchor" aria-label="anchor" href="#specification-checks"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><a href="regression-discontinuity.html#balance-checks">Balance Checks</a></li>
<li><a href="regression-discontinuity.html#sortingbunchingmanipulation">Sorting/Bunching/Manipulation</a></li>
<li><a href="regression-discontinuity.html#placebo-tests">Placebo Tests</a></li>
<li><a href="regression-discontinuity.html#sensitivity-to-bandwidth-choice">Sensitivity to Bandwidth Choice</a></li>
</ol>
<div id="balance-checks" class="section level3" number="24.2.1">
<h3>
<span class="header-section-number">24.2.1</span> Balance Checks<a class="anchor" aria-label="anchor" href="#balance-checks"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Also known as checking for Discontinuities in Average Covariates</p></li>
<li><p>Null Hypothesis: The average effect of covariates on pseudo outcomes (i.e., those qualitatively cannot be affected by the treatment) is 0.</p></li>
<li><p>If this hypothesis is rejected, you better have a good reason to why because it can cast serious doubt on your RD design.</p></li>
</ul>
</div>
<div id="sortingbunchingmanipulation" class="section level3" number="24.2.2">
<h3>
<span class="header-section-number">24.2.2</span> Sorting/Bunching/Manipulation<a class="anchor" aria-label="anchor" href="#sortingbunchingmanipulation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Also known as checking for A Discontinuity in the Distribution of the Forcing Variable</p></li>
<li><p>Also known as clustering or density test</p></li>
<li><p>Formal test is McCrary sorting test <span class="citation">(<a href="references.html#ref-mccrary2008manipulation">McCrary 2008</a>)</span> or <span class="citation">(<a href="references.html#ref-cattaneo2019practical">Cattaneo, Idrobo, and Titiunik 2019</a>)</span></p></li>
<li>
<p>Since human subjects can manipulate the running variable to be just above or below the cutoff (assuming that the running variable is manipulable), especially when the cutoff point is known in advance for all subjects, this can result in a discontinuity in the distribution of the running variable at the cutoff (i.e., we will see “bunching” behavior right before or after the cutoff)&gt;</p>
<ul>
<li><p>People would like to sort into treatment if it’s desirable. The density of the running variable would be 0 just below the threshold</p></li>
<li><p>People would like to be out of treatment if it’s undesirable</p></li>
</ul>
</li>
<li>
<p><span class="citation">(<a href="references.html#ref-mccrary2008manipulation">McCrary 2008</a>)</span> proposes a density test (i.e., a formal test for manipulation of the assignment variable).</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: The continuity of the density of the running variable (i.e., the covariate that underlies the assignment at the discontinuity point)</p></li>
<li><p><span class="math inline">\(H_a\)</span>: A jump in the density function at that point</p></li>
<li><p>Even though it’s not a requirement that the density of the running must be continuous at the cutoff, but a discontinuity can suggest manipulations.</p></li>
</ul>
</li>
<li><p><span class="citation">(<a href="references.html#ref-zhang2003estimation">Zhang and Rubin 2003</a>; <a href="references.html#ref-lee2009training">Lee 2009</a>; <a href="references.html#ref-aronow2019note">Aronow, Baron, and Pinson 2019</a>)</span> offers a guide to know when you should warrant the manipulation</p></li>
<li>
<p>Usually it’s better to know your research design inside out so that you can suspect any manipulation attempts.</p>
<ul>
<li>We would suspect the direction of the manipulation. And typically, it’s one-way manipulation. In cases where we might have both ways, theoretically they would cancel each other out.</li>
</ul>
</li>
<li><p>We could also observe partial manipulation in reality (e.g., when subjects can only imperfectly manipulate). But typically, as we treat it like fuzzy RD, we would not have identification problems. But complete manipulation would lead to serious identification issues.</p></li>
<li><p>Remember: even in cases where we fail to reject the null hypothesis for the density test, we could not rule out completely that identification problem exists (just like any other hypotheses)</p></li>
<li><p>Bunching happens when people self-select to a specific value in the range of a variable (e.g., key policy thresholds).</p></li>
<li><p>Review paper <span class="citation">(<a href="references.html#ref-kleven2016bunching">Kleven 2016</a>)</span></p></li>
<li><p><strong>This test can only detect manipulation that changes the distribution of the running variable</strong>. If you can choose the cutoff point or you have 2-sided manipulation, this test will fail to detect it.</p></li>
<li><p>Histogram in bunching is similar to a density curve (we want narrower bins, wider bins bias elasticity estimates)</p></li>
<li><p>We can also use bunching method to study individuals’ or firm’s responsiveness to changes in policy.</p></li>
<li>
<p>Under RD, we assume that we don’t have any manipulation in the running variable. However, bunching behavior is a manipulation by firms or individuals. Thus, violating this assumption.</p>
<ul>
<li><p>Bunching can fix this problem by estimating what densities of individuals would have been without manipulation (i.e., manipulation-free counterfactual).</p></li>
<li><p><strong>The fraction of persons who manipulated</strong> is then calculated by comparing the observed distribution to manipulation-free counterfactual distributions.</p></li>
<li><p>Under RD, we do not need this step because the observed and manipulation-free counterfactual distributions are assumed to be the same. RD assume there is no manipulation (i.e., assume the manipulation-free counterfactual distribution)</p></li>
</ul>
</li>
</ul>
<p>When running variable and outcome variable are simultaneously determined, we can use a modified RDD estimator to have consistent estimate. <span class="citation">(<a href="references.html#ref-bajari2011regression">Bajari et al. 2011</a>)</span></p>
<ul>
<li>
<p><strong>Assumptions</strong>:</p>
<ul>
<li><p>Manipulation is <strong>one-sided</strong>: People move one way (i.e., either below the threshold to above the threshold or vice versa, but not to or away the threshold), which is similar to the monotonicity assumption under instrumental variable <a href="endogeneity.html#instrumental-variable">31.1.3.1</a></p></li>
<li><p>Manipulation is <strong>bounded</strong> (also known as regularity assumption): so that we can use people far away from this threshold to derive at our counterfactual distribution [<span class="citation">Blomquist et al. (<a href="references.html#ref-blomquist2021bunching">2021</a>)</span>]<span class="citation">(<a href="references.html#ref-bertanha2021better">Bertanha, McCallum, and Seegert 2021</a>)</span></p></li>
</ul>
</li>
</ul>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Identify the window in which the running variable contains bunching behavior. We can do this step empirically based on <span class="citation">Bosch, Dekker, and Strohmaier (<a href="references.html#ref-bosch2020data">2020</a>)</span>. Additionally robustness test is needed (i.e., varying the manipulation window).</li>
<li>Estimate the manipulation-free counterfactual</li>
<li>Calculating the standard errors for inference can follow <span class="citation">(<a href="references.html#ref-chetty2016effects">Chetty, Hendren, and Katz 2016</a>)</span> where we bootstrap re-sampling residuals in the estimation of the counts of individuals within bins (large data can render this step unnecessary).</li>
</ol>
<p>If we pass the bunching test, we can move on to the <a href="difference-in-differences.html#placebo-test">Placebo Test</a></p>
<p><span class="citation">McCrary (<a href="references.html#ref-mccrary2008manipulation">2008</a>)</span> test</p>
<p>A jump in the density at the threshold (i.e., discontinuity) hold can serve as evidence for sorting around the cutoff point</p>
<div class="sourceCode" id="cb402"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">rdd</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># you only need the runing variable and the cutoff point</span></span>
<span></span>
<span><span class="co"># Example by the package's authors</span></span>
<span><span class="co">#No discontinuity</span></span>
<span><span class="va">x</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>,<span class="op">-</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rdd/man/DCdensity.html">DCdensity</a></span><span class="op">(</span><span class="va">x</span>,<span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="24-regression-discontinuity_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; [1] 0.6126802

#Discontinuity
x&lt;-runif(1000,-1,1)
x&lt;-x+2*(runif(1000,-1,1)&gt;0&amp;x&lt;0)
DCdensity(x,0)</code></pre>
<div class="inline-figure"><img src="24-regression-discontinuity_files/figure-html/unnamed-chunk-1-2.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; [1] 0.0008519227</code></pre>
<p><span class="citation">Cattaneo, Idrobo, and Titiunik (<a href="references.html#ref-cattaneo2019practical">2019</a>)</span> test</p>
<div class="sourceCode" id="cb405"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">rddensity</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example by the package's authors</span></span>
<span><span class="co"># Continuous Density</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">2000</span>, mean <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="va">rdd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rddensity/man/rddensity.html">rddensity</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">x</span>, vce <span class="op">=</span> <span class="st">"jackknife"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">rdd</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Manipulation testing using local polynomial density estimation.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of obs =       2000</span></span>
<span><span class="co">#&gt; Model =               unrestricted</span></span>
<span><span class="co">#&gt; Kernel =              triangular</span></span>
<span><span class="co">#&gt; BW method =           estimated</span></span>
<span><span class="co">#&gt; VCE method =          jackknife</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; c = 0                 Left of c           Right of c          </span></span>
<span><span class="co">#&gt; Number of obs         1376                624                 </span></span>
<span><span class="co">#&gt; Eff. Number of obs    354                 345                 </span></span>
<span><span class="co">#&gt; Order est. (p)        2                   2                   </span></span>
<span><span class="co">#&gt; Order bias (q)        3                   3                   </span></span>
<span><span class="co">#&gt; BW est. (h)           0.514               0.609               </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method                T                   P &gt; |T|             </span></span>
<span><span class="co">#&gt; Robust                -0.6798             0.4966              </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; P-values of binomial tests (H0: p=0.5).</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|</span></span>
<span><span class="co">#&gt; 0.036                      28      20    0.3123</span></span>
<span><span class="co">#&gt; 0.072                      46      39    0.5154</span></span>
<span><span class="co">#&gt; 0.107                      68      59    0.4779</span></span>
<span><span class="co">#&gt; 0.143                      94      79    0.2871</span></span>
<span><span class="co">#&gt; 0.179                     122     103    0.2301</span></span>
<span><span class="co">#&gt; 0.215                     145     130    0.3986</span></span>
<span><span class="co">#&gt; 0.250                     163     156    0.7370</span></span>
<span><span class="co">#&gt; 0.286                     190     176    0.4969</span></span>
<span><span class="co">#&gt; 0.322                     214     200    0.5229</span></span>
<span><span class="co">#&gt; 0.358                     249     218    0.1650</span></span>
<span></span>
<span><span class="co"># you have to specify your own plot (read package manual)</span></span></code></pre></div>
</div>
<div id="placebo-tests" class="section level3" number="24.2.3">
<h3>
<span class="header-section-number">24.2.3</span> Placebo Tests<a class="anchor" aria-label="anchor" href="#placebo-tests"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Also known as Discontinuities in Average Outcomes at Other Values</p></li>
<li>
<p>We should not see any jumps at other values (either <span class="math inline">\(X_i &lt;c\)</span> or <span class="math inline">\(X_i \ge c\)</span>)</p>
<ul>
<li>Use the same bandwidth you use for the cutoff, and move it along the running variable: testing for a jump in the conditional mean of the outcome at the median of the running variable.</li>
</ul>
</li>
<li><p>Also known as falsification checks</p></li>
<li><p>Before and after the cutoff point, we can run the placebo test to see whether X’s are different).</p></li>
<li><p>The placebo test is where you expect your coefficients to be not different from 0.</p></li>
<li>
<p>This test can be used for</p>
<ul>
<li><p>Testing no discontinuity in predetermined variables:</p></li>
<li><p>Testing other discontinuities</p></li>
<li><p>Placebo outcomes: we should see any changes in other outcomes that shouldn’t have changed.</p></li>
<li><p>Inclusion and exclusion of covariates: RDD parameter estimates should not be sensitive to the inclusion or exclusion of other covariates.</p></li>
</ul>
</li>
<li><p>This is analogous to <a href="experimental-design.html#experimental-design">Experimental Design</a> where we cannot only test whether the observables are similar in both treatment and control groups (if we reject this, then we don’t have random assignment), but we cannot test unobservables.</p></li>
</ul>
<p>Balance on observable characteristics on both sides</p>
<p><span class="math display">\[
Z_i = \alpha_0 + \alpha_1 f(x_i) + [I(x_i \ge c)] \alpha_2 + [f(x_i) \times I(x_i \ge c)]\alpha_3 + u_i
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(x_i\)</span> is the running variable</p></li>
<li><p><span class="math inline">\(Z_i\)</span> is other characteristics of people (e.g., age, etc)</p></li>
</ul>
<p>Theoretically, <span class="math inline">\(Z_i\)</span> should no be affected by treatment. Hence, <span class="math inline">\(E(\alpha_2) = 0\)</span></p>
<p>Moreover, when you have multiple <span class="math inline">\(Z_i\)</span>, you typically have to simulate joint distribution (to avoid having significant coefficient based on chance).</p>
<p>The only way that you don’t need to generate joint distribution is when all <span class="math inline">\(Z_i\)</span>’s are independent (unlikely in reality).</p>
<p>Under RD, you shouldn’t have to do any <a href="matching-methods.html#matching-methods">Matching Methods</a>. Because just like when you have random assignment, there is no need to make balanced dataset before and after the cutoff. If you have to do balancing, then your RD assumptions are probably wrong in the first place.</p>
</div>
<div id="sensitivity-to-bandwidth-choice" class="section level3" number="24.2.4">
<h3>
<span class="header-section-number">24.2.4</span> Sensitivity to Bandwidth Choice<a class="anchor" aria-label="anchor" href="#sensitivity-to-bandwidth-choice"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<p>Methods for bandwidth selection</p>
<ul>
<li><p>Ad-hoc or substantively driven</p></li>
<li><p>Data driven: cross validation</p></li>
<li><p>Conservative approach: <span class="citation">(<a href="references.html#ref-calonico2020optimal">Calonico, Cattaneo, and Farrell 2020</a>)</span></p></li>
</ul>
</li>
<li><p>The objective is to minimize the mean squared error between the estimated and actual treatment effects.</p></li>
<li><p>Then, we need to see how sensitive our results will be dependent on the choice of bandwidth.</p></li>
<li><p>In some cases, the best bandwidth for testing covariates may not be the best bandwidth for treating them, but it may be close.</p></li>
</ul>
<div class="sourceCode" id="cb406"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># find optimal bandwidth by Imbens-Kalyanaraman</span></span>
<span><span class="fu">rdd</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/rdd/man/IKbandwidth.html">IKbandwidth</a></span><span class="op">(</span><span class="va">running_var</span>,</span>
<span>                 <span class="va">outcome_var</span>,</span>
<span>                 cutpoint <span class="op">=</span> <span class="st">""</span>,</span>
<span>                 kernel <span class="op">=</span> <span class="st">"triangular"</span><span class="op">)</span> <span class="co"># can also pick other kernels</span></span></code></pre></div>
</div>
</div>
<div id="fuzzy-rd-design" class="section level2" number="24.3">
<h2>
<span class="header-section-number">24.3</span> Fuzzy RD Design<a class="anchor" aria-label="anchor" href="#fuzzy-rd-design"><i class="fas fa-link"></i></a>
</h2>
<p>When you have cutoff that does not perfectly determine treatment, but creates a discontinuity in the likelihood of receiving the treatment, you need another instrument</p>
<p>For those that are close to the cutoff, we create an instrument for <span class="math inline">\(D_i\)</span></p>
<p><span class="math display">\[
Z_i=
\begin{cases}
1 &amp; \text{if } X_i \ge c \\
0 &amp; \text{if } X_c &lt; c
\end{cases}
\]</span></p>
<p>Then, we can estimate the effect of the treatment for compliers only (i.e., those treatment <span class="math inline">\(D_i\)</span> depends on <span class="math inline">\(Z_i\)</span>)</p>
<p>The LATE parameter</p>
<p><span class="math display">\[
\lim_{c - \epsilon \le X \le c + \epsilon, \epsilon \to 0}( \frac{E(Y |Z = 1) - E(Y |Z=0)}{E(D|Z = 1) - E(D|Z = 0)})
\]</span></p>
<p>equivalently, the canonical parameter:</p>
<p><span class="math display">\[
\frac{lim_{x \downarrow c}E(Y|X = x) - \lim_{x \uparrow c} E(Y|X = x)}{\lim_{x \downarrow c } E(D |X = x) - \lim_{x \uparrow c}E(D |X=x)}
\]</span></p>
<p>Two equivalent ways to estimate</p>
<ol style="list-style-type: decimal">
<li>
<p>First</p>
<ol style="list-style-type: decimal">
<li><p>Sharp RDD for <span class="math inline">\(Y\)</span></p></li>
<li><p>Sharp RDD for <span class="math inline">\(D\)</span></p></li>
<li><p>Take the estimate from step 1 divide by that of step 2</p></li>
</ol>
</li>
<li><p>Second: Subset those observations that are close to <span class="math inline">\(c\)</span> and run instrumental variable <span class="math inline">\(Z\)</span></p></li>
</ol>
</div>
<div id="regression-kink-design" class="section level2" number="24.4">
<h2>
<span class="header-section-number">24.4</span> Regression Kink Design<a class="anchor" aria-label="anchor" href="#regression-kink-design"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>If the slope of the treatment intensity changes at the cutoff (instead of the level of treatment assignment), we can have regression kink design</p></li>
<li><p>Example: unemployment benefits</p></li>
</ul>
<p>Sharp Kink RD parameter</p>
<p><span class="math display">\[
\alpha_{KRD} = \frac{\lim_{x \downarrow c} \frac{d}{dx}E[Y_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[Y_i |X_i = x]}{\lim_{x \downarrow c} \frac{d}{dx}b(x) - \lim_{x \uparrow c} \frac{d}{dx}b(x)}
\]</span></p>
<p>where <span class="math inline">\(b(x)\)</span> is a known function inducing “kink”</p>
<p>Fuzzy Kink RD parameter</p>
<p><span class="math display">\[
\alpha_{KRD} = \frac{\lim_{x \downarrow c} \frac{d}{dx}E[Y_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[Y_i |X_i = x]}{\lim_{x \downarrow c} \frac{d}{dx}E[D_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[D_i |X_i = x]}
\]</span></p>
</div>
<div id="multi-cutoff" class="section level2" number="24.5">
<h2>
<span class="header-section-number">24.5</span> Multi-cutoff<a class="anchor" aria-label="anchor" href="#multi-cutoff"><i class="fas fa-link"></i></a>
</h2>
<p><span class="math display">\[
\tau (x,c)= E[Y_{1i} - Y_{0i}|X_i = x, C_i = c]
\]</span></p>
</div>
<div id="multi-score" class="section level2" number="24.6">
<h2>
<span class="header-section-number">24.6</span> Multi-score<a class="anchor" aria-label="anchor" href="#multi-score"><i class="fas fa-link"></i></a>
</h2>
<p>Multi-score (in multiple dimensions) (e.g., math and English cutoff for certain honor class):</p>
<p><span class="math display">\[
\tau (x_1, x_2) = E[Y_{1i} - Y_{0i}|X_{1i} = x_1, X_{2i} = x]
\]</span></p>
</div>
<div id="steps-for-sharp-rd" class="section level2" number="24.7">
<h2>
<span class="header-section-number">24.7</span> Steps for Sharp RD<a class="anchor" aria-label="anchor" href="#steps-for-sharp-rd"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear).</p></li>
<li><p>Run regression on both sides of the cutoff to get the treatment effect</p></li>
<li>
<p>Robustness checks:</p>
<ol style="list-style-type: decimal">
<li><p>Assess possible jumps in other variables around the cutoff</p></li>
<li><p>Hypothesis testing for bunching</p></li>
<li><p>Placebo tests</p></li>
<li><p>Varying bandwidth</p></li>
</ol>
</li>
</ol>
</div>
<div id="steps-for-fuzzy-rd" class="section level2" number="24.8">
<h2>
<span class="header-section-number">24.8</span> Steps for Fuzzy RD<a class="anchor" aria-label="anchor" href="#steps-for-fuzzy-rd"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear).</p></li>
<li><p>Graph the probability of treatment</p></li>
<li><p>Estimate the treatment effect using 2SLS</p></li>
<li>
<p>Robustness checks:</p>
<ol style="list-style-type: decimal">
<li><p>Assess possible jumps in other variables around the cutoff</p></li>
<li><p>Hypothesis testing for bunching</p></li>
<li><p>Placebo tests</p></li>
<li><p>Varying bandwidth</p></li>
</ol>
</li>
</ol>
</div>
<div id="steps-for-rdit-regression-discontinuity-in-time" class="section level2" number="24.9">
<h2>
<span class="header-section-number">24.9</span> Steps for RDiT (Regression Discontinuity in Time)<a class="anchor" aria-label="anchor" href="#steps-for-rdit-regression-discontinuity-in-time"><i class="fas fa-link"></i></a>
</h2>
<p>Notes:</p>
<ul>
<li>Additional assumption: Time-varying confounders change smoothly across the cutoff date</li>
<li>Typically used in policy implementation in the same date for all subjects, but can also be used for cases where implementation dates are different between subjects. In the second case, researchers typically use different RDiT specification for each time series.</li>
<li>Sometimes the date of implementation is not randomly assigned by chosen strategically. Hence, RDiT should be thought of as the “discontinuity at a threshold” interpretation of RD (not as “local randomization”). <span class="citation">(<a href="references.html#ref-hausman2018">C. Hausman and Rapson 2018, 8</a>)</span>
</li>
<li>Normal RD uses variation in the <span class="math inline">\(N\)</span> dimension, while RDiT uses variation in the <span class="math inline">\(T\)</span> dimension</li>
<li>Choose polynomials based on BIC typically. And can have either global polynomial or pre-period and post-period polynomial for each time series (but usually the global one will perform better)</li>
<li>Could use <strong>augmented local linear</strong> outlined by <span class="citation">(<a href="references.html#ref-hausman2018">C. Hausman and Rapson 2018, 12</a>)</span>, where estimate the model with all the control first then take the residuals to include in the model with the RDiT treatment (remember to use bootstrapping method to account for the first-stage variance in the second stage).</li>
</ul>
<p>Pros:</p>
<ul>
<li>
<p>can overcome cases where there is no cross-sectional variation in treatment implementation (DID is not feasible)</p>
<ul>
<li>There are papers that use both RDiT and DID to (1) see the differential treatment effects across individuals/ space <span class="citation">(<a href="references.html#ref-auffhammer2011clearing">Auffhammer and Kellogg 2011</a>)</span> or (2) compare the 2 estimates where the control group’s validity is questionable <span class="citation">(<a href="references.html#ref-gallego2013effect">Gallego, Montero, and Salas 2013</a>)</span>.</li>
</ul>
</li>
<li><p>Better than pre/post comparison because it can include flexible controls</p></li>
<li><p>Better than event studies because it can use long-time horizons (may not be too relevant now since the development long-time horizon event studies), and it can use higher-order polynomials time control variables.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Taking observation for from the threshold (in time) can bias your estimates because of unobservables and time-series properties of the data generating process.</p></li>
<li><p><span class="citation">(<a href="references.html#ref-mccrary2008manipulation">McCrary 2008</a>)</span> test is not possible (see <a href="regression-discontinuity.html#sortingbunchingmanipulation">Sorting/Bunching/Manipulation</a>) because when the density of the running (time) is uniform, you can’t use the test.</p></li>
<li><p>Time-varying unobservables may impact the dependent variable discontinuously</p></li>
<li><p>Error terms are likely to include persistence (serially correlated errors)</p></li>
<li>
<p>Researchers cannot model time-varying treatment under RDiT</p>
<ul>
<li>In a small enough window, the local linear specification is fine, but the global polynomials can either be too big or too small <span class="citation">(<a href="references.html#ref-hausman2018">C. Hausman and Rapson 2018</a>)</span>
</li>
</ul>
</li>
</ul>
<p>Biases</p>
<ul>
<li>
<p>Time-Varying treatment Effects</p>
<ul>
<li>
<p>increase sample size either by</p>
<ul>
<li><p>more granular data (greater frequency): will not increase power because of the problem of serial correlation</p></li>
<li><p>increasing time window: increases bias from other confounders</p></li>
</ul>
</li>
<li>
<p>2 additional assumption:</p>
<ul>
<li><p>Model is correctly specified (with all confoudners or global polynomial approximation)</p></li>
<li><p>Treatment effect is correctly specified (whether it’s smooth and constant, or varies)</p></li>
<li><p>These 2 assumptions do not interact ( we don’t want them to interact - i.e., we don’t want the polynomial correlated with the unobserved variation in the treatment effect)</p></li>
</ul>
</li>
<li><p>There usually a difference between short-run and long-run treatment effects, but it’s also possibly that the bias can stem from the over-fitting problem of the polynomial specification. <span class="citation">(<a href="references.html#ref-hausman2018">C. Hausman and Rapson 2018, 544</a>)</span></p></li>
</ul>
</li>
<li>
<p>Autoregression (serial dependence)</p>
<ul>
<li><p>Need to use <strong>clustered standard errors</strong> to account for serial dependence in the residuals</p></li>
<li><p>In the case of serial dependence in <span class="math inline">\(\epsilon_{it}\)</span>, we don’t have a solution, including a lagged dependent variable would misspecify the model (probably find another research project)</p></li>
<li><p>In the case of serial dependence in <span class="math inline">\(y_{it}\)</span>, with long window, it becomes fuzzy to what you try to recover. You can include the <strong>lagged dependent variable</strong> (bias can still come from the time-varying treatment or over-fitting of the global polynomial)</p></li>
</ul>
</li>
<li>
<p>Sorting and Anticipation Effects</p>
<ul>
<li><p>Cannot run the <span class="citation">(<a href="references.html#ref-mccrary2008manipulation">McCrary 2008</a>)</span> because the density of the time running variable is uniform</p></li>
<li><p>Can still run tests to check discontinuities in other covariates (you want no discontinuities) and discontinuities in the outcome variable at other placebo thresholds ( you don’t want discontinuities)</p></li>
<li><p>Hence, it’s hard to argue for the causal effect here because it could be the total effect of the causal treatment and the unobserved sorting/anticipation/adaptation/avoidance effects. You can only argue that there is no such behavior</p></li>
</ul>
</li>
</ul>
<p>Recommendations for robustness check following <span class="citation">(<a href="references.html#ref-hausman2018">C. Hausman and Rapson 2018, 549</a>)</span></p>
<ol style="list-style-type: decimal">
<li>Plot the raw data and residuals (after removing confounders or trend). With varying polynomial and local linear controls, inconsistent results can be a sign of time-varying treatment effects.</li>
<li>Using global polynomial, you could overfit, then show polynomial with different order and alternative local linear bandwidths. If the results are consistent, you’re okay</li>
<li>
<a href="regression-discontinuity.html#placebo-tests">Placebo Tests</a>: estimate another RD (1) on another location or subject (that did not receive the treatment) or (2) use another date.</li>
<li>Plot RD discontinuity on continuous controls</li>
<li>Donut RD to see if avoiding the selection close to the cutoff would yield better results <span class="citation">(<a href="references.html#ref-barreca2011saving">Barreca et al. 2011</a>)</span>
</li>
<li>Test for auto-regression (using only pre-treatment data). If there is evidence for autoregression, include the lagged dependent variable</li>
<li>Augmented local linear (no need to use global polynomial and avoid over-fitting)
<ol style="list-style-type: decimal">
<li><p>Use full sample to exclude the effect of important predictors</p></li>
<li><p>Estimate the conditioned second stage on a smaller sample bandwidth</p></li>
</ol>
</li>
</ol>
<p>Examples from <span class="citation">(<a href="references.html#ref-hausman2018">C. Hausman and Rapson 2018, 534</a>)</span> in</p>
<p>econ</p>
<ul>
<li><p><span class="citation">(<a href="references.html#ref-davis2008effect">Davis 2008</a>)</span>: Air quality</p></li>
<li><p><span class="citation">(<a href="references.html#ref-auffhammer2011clearing">Auffhammer and Kellogg 2011</a>)</span>: Air quality</p></li>
<li><p><span class="citation">(<a href="references.html#ref-chen2018effect">H. Chen et al. 2018</a>)</span>: Air quality</p></li>
<li><p><span class="citation">(<a href="references.html#ref-de2013deterrent">De Paola, Scoppa, and Falcone 2013</a>)</span>: car accidents</p></li>
<li><p><span class="citation">(<a href="references.html#ref-gallego2013effect">Gallego, Montero, and Salas 2013</a>)</span>: air quality</p></li>
<li><p><span class="citation">(<a href="references.html#ref-bento2014effects">Bento et al. 2014</a>)</span>: Traffic</p></li>
<li><p><span class="citation">(<a href="references.html#ref-anderson2014subways">M. L. Anderson 2014</a>)</span>: Traffic</p></li>
<li><p><span class="citation">(<a href="references.html#ref-burger2014did">Burger, Kaffine, and Yu 2014</a>)</span>: Car accidents</p></li>
<li><p><span class="citation">(<a href="references.html#ref-brodeur2021covid">Brodeur et al. 2021</a>)</span>: Covid19 lock-downs on well-being</p></li>
</ul>
<p>marketing</p>
<ul>
<li><p><span class="citation">M. R. Busse et al. (<a href="references.html#ref-busse2013estimating">2013</a>)</span>: Vehicle prices</p></li>
<li><p><span class="citation">(<a href="references.html#ref-chen2009learning">X. Chen et al. 2009</a>)</span>: Customer Satisfaction</p></li>
<li><p><span class="citation">(<a href="references.html#ref-busse2010best">M. R. Busse, Simester, and Zettelmeyer 2010</a>)</span>: Vehicle prices</p></li>
<li><p><span class="citation">(<a href="references.html#ref-davis2010international">Davis and Kahn 2010</a>)</span>: vehicle prices</p></li>
</ul>
</div>
<div id="evaluation-of-an-rd" class="section level2" number="24.10">
<h2>
<span class="header-section-number">24.10</span> Evaluation of an RD<a class="anchor" aria-label="anchor" href="#evaluation-of-an-rd"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>
<p>Evidence for (either formal tests or graphs)</p>
<ul>
<li><p>Treatment and outcomes change discontinuously at the cutoff, while other variables and pre-treatment outcomes do not.</p></li>
<li><p>No manipulation of the assignment variable.</p></li>
</ul>
</li>
<li><p>Results are robust to various functional forms of the forcing variable</p></li>
<li><p>Is there any other (unobserved) confound that could cause the discontinuous change at the cutoff (i.e., multiple forcing variables / bundling of institutions)?</p></li>
<li><p>External Validity: How likely the result at the cutoff will generalize?</p></li>
</ul>
<p><strong>General Model</strong></p>
<p><span class="math display">\[
Y_i = \beta_0 + f(x_i) \beta_1 + [I(x_i \ge c)]\beta_2 + \epsilon_i
\]</span></p>
<p>where <span class="math inline">\(f(x_i)\)</span> is any functional form of <span class="math inline">\(x_i\)</span></p>
<p><strong>Simple case</strong></p>
<p>When <span class="math inline">\(f(x_i) = x_i\)</span> (linear function)</p>
<p><span class="math display">\[
Y_i = \beta_0 + x_i \beta_1 + [I(x_i \ge c)]\beta_2 + \epsilon_i
\]</span></p>
<div class="inline-figure"><img src="images/rd1.PNG" style="display: block; margin: 1em auto" width="600" height="300"></div>
<p>RD gives you <span class="math inline">\(\beta_2\)</span> (causal effect) of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> at the cutoff point</p>
<p>In practice, everyone does</p>
<p><span class="math display">\[
Y_i = \alpha_0 + f(x) \alpha _1 + [I(x_i \ge c)]\alpha_2 + [f(x_i)\times [I(x_i \ge c)]\alpha_3 + u_i
\]</span></p>
<div class="inline-figure"><img src="images/rd2.PNG" style="display: block; margin: 1em auto" width="600" height="300"></div>
<p>where we estimate different slope on different sides of the line</p>
<p>and if you estimate <span class="math inline">\(\alpha_3\)</span> to be no different from 0 then we return to the simple case</p>
<p><strong>Notes</strong>:</p>
<ul>
<li><p>Sparse data can make <span class="math inline">\(\alpha_3\)</span> large differential effect</p></li>
<li><p>People are very skeptical when you have complex <span class="math inline">\(f(x_i)\)</span>, usual simple function forms (e.g., linear, squared term, etc.) should be good. However, if you still insist, then <strong>non-parametric estimation</strong> can be your best bet.</p></li>
</ul>
<p>Bandwidth of <span class="math inline">\(c\)</span> (window)</p>
<ul>
<li><p>Closer to <span class="math inline">\(c\)</span> can give you lower bias, but also efficiency</p></li>
<li><p>Wider <span class="math inline">\(c\)</span> can increase bias, but higher efficiency.</p></li>
<li><p>Optimal bandwidth is very controversial, but usually we have to do it in the appendix for research article anyway.</p></li>
<li>
<p>We can either</p>
<ul>
<li><p>drop observations outside of bandwidth or</p></li>
<li><p>weight depends on how far and close to <span class="math inline">\(c\)</span></p></li>
</ul>
</li>
</ul>
</div>
<div id="applications" class="section level2" number="24.11">
<h2>
<span class="header-section-number">24.11</span> Applications<a class="anchor" aria-label="anchor" href="#applications"><i class="fas fa-link"></i></a>
</h2>
<p>Examples in marketing:</p>
<ul>
<li><p><span class="citation">(<a href="references.html#ref-narayanan2015position">Narayanan and Kalyanam 2015</a>)</span></p></li>
<li><p><span class="citation">(<a href="references.html#ref-hartmann2011identifying">Hartmann, Nair, and Narayanan 2011</a>)</span>: nonparametric estimation and guide to identifying causal marketing mix effects</p></li>
</ul>
<p><a href="https://rdpackages.github.io/">Packages</a> in R (see <span class="citation">(<a href="references.html#ref-thoemmes2017analysis">Thoemmes, Liao, and Jin 2017</a>)</span> for detailed comparisons): all can handle both sharp and fuzzy RD</p>
<ul>
<li><p><code>rdd</code></p></li>
<li><p><code>rdrobust</code> estimation, inference and plot</p></li>
<li><p><code>rddensity</code> discontinuity in density tests (<a href="regression-discontinuity.html#sortingbunchingmanipulation">Sorting/Bunching/Manipulation</a>) using local polynomials and binomial test</p></li>
<li><p><code>rdlocrand</code> covariate balance, binomial tests, window selection</p></li>
<li><p><code>rdmulti</code> multiple cutoffs and multiple scores</p></li>
<li><p><code>rdpower</code> power, sample selection</p></li>
<li><p><code>rddtools</code></p></li>
</ul>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="18%">
<col width="20%">
<col width="23%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th>Package</th>
<th>rdd</th>
<th>rdrobust</th>
<th>rddtools</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Coefficient estimator</td>
<td>Local linear regression</td>
<td>local polynomial regression</td>
<td>local polynomial regression</td>
</tr>
<tr class="even">
<td>bandwidth selectors</td>
<td><span class="citation">(<a href="references.html#ref-imbens2012optimal">G. Imbens and Kalyanaraman 2012</a>)</span></td>
<td>
<p><span class="citation">(<a href="references.html#ref-calonico2020optimal">Calonico, Cattaneo, and Farrell 2020</a>)</span></p>
<p><span class="citation">(<a href="references.html#ref-imbens2012optimal">G. Imbens and Kalyanaraman 2012</a>)</span></p>
<p><span class="citation">(<a href="references.html#ref-calonico2014robust">Calonico, Cattaneo, and Titiunik 2014</a>)</span></p>
</td>
<td><span class="citation">(<a href="references.html#ref-imbens2012optimal">G. Imbens and Kalyanaraman 2012</a>)</span></td>
</tr>
<tr class="odd">
<td>
<p>Kernel functions</p>
<ul>
<li><p>Triangular</p></li>
<li><p>Rectangular</p></li>
</ul>
</td>
<td>
<p>Epanechnikov</p>
<p>Gaussian</p>
</td>
<td>Epanechnikov</td>
<td>Gaussian</td>
</tr>
<tr class="even">
<td>Bias Correction</td>
<td></td>
<td>Local polynomial regression</td>
<td></td>
</tr>
<tr class="odd">
<td>Covariate options</td>
<td>Include</td>
<td>Include</td>
<td>
<p>Include</p>
<p>Residuals</p>
</td>
</tr>
<tr class="even">
<td>Assumptions testing</td>
<td>McCrary sorting</td>
<td></td>
<td>
<p>McCrary sorting</p>
<p>Equality of covariates distribution and mean</p>
</td>
</tr>
</tbody>
</table></div>
<p>based on table 1 <span class="citation">(<a href="references.html#ref-thoemmes2017analysis">Thoemmes, Liao, and Jin 2017</a>)</span> (p. 347)</p>
<div id="example-1-1" class="section level3" number="24.11.1">
<h3>
<span class="header-section-number">24.11.1</span> Example 1<a class="anchor" aria-label="anchor" href="#example-1-1"><i class="fas fa-link"></i></a>
</h3>
<p>Example by <a href="https://towardsdatascience.com/the-crown-jewel-of-causal-inference-regression-discontinuity-design-rdd-bad37a68e786">Leihua Ye</a></p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i
\]</span></p>
<p><span class="math display">\[
X_i =
\begin{cases}
1, W_i \ge c \\
0, W_i &lt; c
\end{cases}
\]</span></p>
<div class="sourceCode" id="cb407"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#cutoff point = 3.5</span></span>
<span><span class="va">GPA</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">0</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">future_success</span> <span class="op">&lt;-</span> <span class="fl">10</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">GPA</span> <span class="op">+</span> <span class="fl">10</span> <span class="op">*</span> <span class="op">(</span><span class="va">GPA</span> <span class="op">&gt;=</span> <span class="fl">3.5</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">)</span></span>
<span><span class="co">#install and load the package ‘rddtools’</span></span>
<span><span class="co">#install.packages(“rddtools”)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://qua.st/rddtools/">rddtools</a></span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rddtools/man/rdd_data.html">rdd_data</a></span><span class="op">(</span><span class="va">future_success</span>, <span class="va">GPA</span>, cutpoint <span class="op">=</span> <span class="fl">3.5</span><span class="op">)</span></span>
<span><span class="co"># plot the dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="va">data</span>,</span>
<span>    col <span class="op">=</span>  <span class="st">"red"</span>,</span>
<span>    cex <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>    xlab <span class="op">=</span>  <span class="st">"GPA"</span>,</span>
<span>    ylab <span class="op">=</span>  <span class="st">"future_success"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="24-regression-discontinuity_files/figure-html/unnamed-chunk-4-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb408"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># estimate the sharp RDD model</span></span>
<span><span class="va">rdd_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rddtools/man/rdd_reg_lm.html">rdd_reg_lm</a></span><span class="op">(</span>rdd_object <span class="op">=</span> <span class="va">data</span>, slope <span class="op">=</span>  <span class="st">"same"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">rdd_mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ ., data = dat_step1, weights = weights)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -2.93235 -0.66786 -0.00799  0.69991  3.01768 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 17.08582    0.07178  238.03   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; D            9.95513    0.11848   84.03   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x            2.01615    0.03546   56.85   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.046 on 997 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.9617, Adjusted R-squared:  0.9616 </span></span>
<span><span class="co">#&gt; F-statistic: 1.253e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<div class="sourceCode" id="cb409"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># plot the RDD model along with binned observations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="va">rdd_mod</span>,</span>
<span>    cex <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>    col <span class="op">=</span>  <span class="st">"red"</span>,</span>
<span>    xlab <span class="op">=</span>  <span class="st">"GPA"</span>,</span>
<span>    ylab <span class="op">=</span>  <span class="st">"future_success"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="24-regression-discontinuity_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="example-2" class="section level3" number="24.11.2">
<h3>
<span class="header-section-number">24.11.2</span> Example 2<a class="anchor" aria-label="anchor" href="#example-2"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">Bowblis and Smith (<a href="references.html#ref-bowblis2021occupational">2021</a>)</span></p>
<p>Occupational licensing can either increase or decrease market efficiency:</p>
<ul>
<li><p>More information means more efficiency</p></li>
<li><p>Increased entry barriers (i.e., friction) increase efficiency</p></li>
</ul>
<p>Components of RD</p>
<ul>
<li>Running variable</li>
<li>Cutoff: 120 beds or above</li>
<li>Treatment: you have to have the treatment before the cutoff point.</li>
</ul>
<p>Under OLS</p>
<p><span class="math display">\[
Y_i = \alpha_0 + X_i \alpha_1 + LW_i \alpha_2 + \epsilon_i
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(LW_i\)</span> Licensed/certified workers (in fraction format for each center).</p></li>
<li><p><span class="math inline">\(Y_i\)</span> = Quality of service</p></li>
</ul>
<p>Bias in <span class="math inline">\(\alpha_2\)</span></p>
<ul>
<li><p>Mitigation-based: terrible quality can lead to more hiring, which negatively bias <span class="math inline">\(\alpha_2\)</span></p></li>
<li><p>Preference-based: places that have higher quality staff want to keep high quality staffs.</p></li>
</ul>
<p>Under RD</p>
<p><span class="math display">\[
\begin{aligned}
Y_{ist} &amp;= \beta_0 + [I(Bed \ge121)_{ist}]\beta_1 + f(Size_{ist}) \beta_2\\
&amp;+ [f(Size_{ist}) \times I(Bed \ge 121)_{ist}] \beta_3 \\
&amp;+ X_{it} \delta + \gamma_s + \theta_t + \epsilon_{ist}
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(s\)</span> = state</p></li>
<li><p><span class="math inline">\(t\)</span> = year</p></li>
<li><p><span class="math inline">\(i\)</span> = hospital</p></li>
</ul>
<p>This RD is fuzzy</p>
<ul>
<li><p>If right near the threshold (bandwidth), we have states with different sorting (i.e., non-random), then we need the fixed-effect for state <span class="math inline">\(s\)</span>. But then your RD assumption wrong anyway, then you won’t do it in the first place</p></li>
<li><p>Technically, we could also run the fixed-effect regression, but because it’s lower in the causal inference hierarchy. Hence, we don’t do it.</p></li>
<li><p>Moreover, in the RD framework, we don’t include <span class="math inline">\(t\)</span> before treatment (but in the FE we have to include before and after)</p></li>
<li><p>If we include <span class="math inline">\(\pi_i\)</span> for each hospital, then we don’t have variation in the causal estimates (because hardly any hospital changes their bed size in the panel)</p></li>
<li><p>When you have <span class="math inline">\(\beta_1\)</span> as the intent to treat (because the treatment effect does not coincide with the intent to treat)</p></li>
<li><p>You cannot take those fuzzy cases out, because it will introduce the selection bias.</p></li>
<li><p>Note that we cannot drop cases based on behavioral choice (because we will exclude non-compliers), but we can drop when we have particular behaviors ((e.g., people like round numbers).</p></li>
</ul>
<p>Thus, we have to use Instrument variable <a href="endogeneity.html#instrumental-variable">31.1.3.1</a></p>
<p><strong>Stage 1:</strong></p>
<p><span class="math display">\[
\begin{aligned}
QSW_{ist} &amp;= \alpha_0 + [I(Bed \ge121)_{ist}]\alpha_1 + f(Size_{ist}) \alpha_2\\
&amp;+ [f(Size_{ist}) \times I(Bed \ge 121)_{ist}] \alpha_3 \\
&amp;+ X_{it} \delta + \gamma_s + \theta_t + \epsilon_{ist}
\end{aligned}
\]</span></p>
<p>(Note: you should have different fixed effects and error term - <span class="math inline">\(\delta, \gamma_s, \theta_t, \epsilon_{ist}\)</span> from the first equation, but I ran out of Greek letters)</p>
<p><strong>Stage 2:</strong></p>
<p><span class="math display">\[
\begin{aligned}
Y_{ist} &amp;= \gamma_0 + \gamma_1 \hat{QWS}_{ist} + f(Size_{ist}) \delta_2 \\
&amp;+ [f(Size_{ist}) \times I(Bed \ge 121)] \delta_3 \\
&amp;+ X_{it} \lambda + \eta_s + \tau_t + u_{ist}
\end{aligned}
\]</span></p>
<ul>
<li><p>The bigger the jump (discontinuity), the more similar the 2 coefficients (<span class="math inline">\(\gamma_1 \approx \beta_1\)</span>) where <span class="math inline">\(\gamma_1\)</span> is the average treatment effect (of exposing to the policy)</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> will always be closer to 0 than <span class="math inline">\(\gamma_1\)</span></p></li>
<li><p>Figure 1 shows bunching at every 5 units cutoff, but 120 is still out there.</p></li>
<li><p>If we have manipulable bunching, there should be decrease at 130</p></li>
<li><p>Since we have limited number of mass points (at the round numbers), we should clustered standard errors by the mass point</p></li>
</ul>
</div>
<div id="example-3" class="section level3" number="24.11.3">
<h3>
<span class="header-section-number">24.11.3</span> Example 3<a class="anchor" aria-label="anchor" href="#example-3"><i class="fas fa-link"></i></a>
</h3>
<p>Replication of <span class="citation">(<a href="references.html#ref-carpenter2009effect">Carpenter and Dobkin 2009</a>)</span> by <a href="https://rpubs.com/phle/r_tutorial_regression_discontinuity_design">Philipp Leppert</a>, dataset from <a href="https://www.openicpsr.org/openicpsr/project/113550/version/V1/view?flag=follow&amp;pageSize=100&amp;sortOrder=(?title)&amp;sortAsc=true">here</a></p>
</div>
<div id="example-4" class="section level3" number="24.11.4">
<h3>
<span class="header-section-number">24.11.4</span> Example 4<a class="anchor" aria-label="anchor" href="#example-4"><i class="fas fa-link"></i></a>
</h3>
<p>For a detailed application, see <span class="citation">(<a href="references.html#ref-thoemmes2017analysis">Thoemmes, Liao, and Jin 2017</a>)</span> where they use <code>rdd</code>, <code>rdrobust</code>, <code>rddtools</code></p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></div>
<div class="next"><a href="synthetic-difference-in-differences.html"><span class="header-section-number">25</span> Synthetic Difference-in-Differences</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#regression-discontinuity"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li>
<a class="nav-link" href="#estimation-and-inference"><span class="header-section-number">24.1</span> Estimation and Inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#local-randomization-based"><span class="header-section-number">24.1.1</span> Local Randomization-based</a></li>
<li><a class="nav-link" href="#continuity-based"><span class="header-section-number">24.1.2</span> Continuity-based</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#specification-checks"><span class="header-section-number">24.2</span> Specification Checks</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#balance-checks"><span class="header-section-number">24.2.1</span> Balance Checks</a></li>
<li><a class="nav-link" href="#sortingbunchingmanipulation"><span class="header-section-number">24.2.2</span> Sorting/Bunching/Manipulation</a></li>
<li><a class="nav-link" href="#placebo-tests"><span class="header-section-number">24.2.3</span> Placebo Tests</a></li>
<li><a class="nav-link" href="#sensitivity-to-bandwidth-choice"><span class="header-section-number">24.2.4</span> Sensitivity to Bandwidth Choice</a></li>
</ul>
</li>
<li><a class="nav-link" href="#fuzzy-rd-design"><span class="header-section-number">24.3</span> Fuzzy RD Design</a></li>
<li><a class="nav-link" href="#regression-kink-design"><span class="header-section-number">24.4</span> Regression Kink Design</a></li>
<li><a class="nav-link" href="#multi-cutoff"><span class="header-section-number">24.5</span> Multi-cutoff</a></li>
<li><a class="nav-link" href="#multi-score"><span class="header-section-number">24.6</span> Multi-score</a></li>
<li><a class="nav-link" href="#steps-for-sharp-rd"><span class="header-section-number">24.7</span> Steps for Sharp RD</a></li>
<li><a class="nav-link" href="#steps-for-fuzzy-rd"><span class="header-section-number">24.8</span> Steps for Fuzzy RD</a></li>
<li><a class="nav-link" href="#steps-for-rdit-regression-discontinuity-in-time"><span class="header-section-number">24.9</span> Steps for RDiT (Regression Discontinuity in Time)</a></li>
<li><a class="nav-link" href="#evaluation-of-an-rd"><span class="header-section-number">24.10</span> Evaluation of an RD</a></li>
<li>
<a class="nav-link" href="#applications"><span class="header-section-number">24.11</span> Applications</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-1-1"><span class="header-section-number">24.11.1</span> Example 1</a></li>
<li><a class="nav-link" href="#example-2"><span class="header-section-number">24.11.2</span> Example 2</a></li>
<li><a class="nav-link" href="#example-3"><span class="header-section-number">24.11.3</span> Example 3</a></li>
<li><a class="nav-link" href="#example-4"><span class="header-section-number">24.11.4</span> Example 4</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/24-regression-discontinuity.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/24-regression-discontinuity.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2024-02-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
