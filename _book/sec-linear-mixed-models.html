<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Linear Mixed Models | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="8.1 Dependent Data In many real-world applications, observations are not independent but exhibit correlations due to shared characteristics. Below are common forms of dependent data: Multivariate...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 8 Linear Mixed Models | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/sec-linear-mixed-models.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="8.1 Dependent Data In many real-world applications, observations are not independent but exhibit correlations due to shared characteristics. Below are common forms of dependent data: Multivariate...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Linear Mixed Models | A Guide on Data Analysis">
<meta name="twitter:description" content="8.1 Dependent Data In many real-world applications, observations are not independent but exhibit correlations due to shared characteristics. Below are common forms of dependent data: Multivariate...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="active" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="sec-endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="sec-directed-acyclic-graphs.html"><span class="header-section-number">38</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="sec-controls.html"><span class="header-section-number">39</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="sec-linear-mixed-models" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Linear Mixed Models<a class="anchor" aria-label="anchor" href="#sec-linear-mixed-models"><i class="fas fa-link"></i></a>
</h1>
<div id="dependent-data" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Dependent Data<a class="anchor" aria-label="anchor" href="#dependent-data"><i class="fas fa-link"></i></a>
</h2>
<p>In many real-world applications, observations are not independent but exhibit <strong>correlations</strong> due to shared characteristics. Below are common forms of dependent data:</p>
<ul>
<li>
<strong>Multivariate measurements on individuals</strong>: Multiple attributes measured on the same person may be correlated (e.g., blood pressure, cholesterol, and BMI).</li>
<li>
<strong>Clustered measurements</strong>: Individuals within a shared environment (e.g., families, schools, hospitals) often exhibit correlated responses.</li>
<li>
<strong>Repeated measurements</strong>: When the same individual is measured multiple times, correlations arise naturally.
<ul>
<li>
<em>Example</em>: Tracking cholesterol levels of a person over time.</li>
<li>If these repeated measurements follow an experimental design where treatments were applied initially, they are referred to as repeated measures <span class="citation">(<a href="references.html#ref-Schabenberger_2001">Schabenberger and Pierce 2001</a>)</span>.</li>
</ul>
</li>
<li>
<strong>Longitudinal data</strong>: Repeated measurements taken over time in an observational study are known as longitudinal data <span class="citation">(<a href="references.html#ref-Schabenberger_2001">Schabenberger and Pierce 2001</a>)</span>.</li>
<li>
<strong>Spatial data</strong>: Measurements taken from individuals in nearby locations (e.g., residents of the same neighborhood) often exhibit spatial correlation.</li>
</ul>
<p>Since standard linear regression assumes independent observations, these correlations violate its assumptions. Thus, <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Models</a> (LMMs) provide a framework to account for these dependencies.</p>
<p>A <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Model</a> (also called a <strong>Mixed Linear Model</strong>) consists of two components:</p>
<ul>
<li>
<strong>Fixed effects</strong>: Parameters associated with variables that have the same effect across all observations (e.g., gender, age, diet, time).</li>
<li>
<strong>Random effects</strong>: Individual-specific variations or correlation structures (e.g., subject-specific effects, spatial correlations), leading to dependent (correlated) errors.</li>
</ul>
<p>A key advantage of LMMs is that they model random subject-specific effects, rather than including individual dummy variables. This provides:</p>
<ul>
<li>A reduction in the number of parameters to estimate, avoiding overfitting.</li>
<li>A framework for inference on a population level, rather than restricting conclusions to observed individuals.</li>
</ul>
<div id="motivation-a-repeated-measurements-example" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> Motivation: A Repeated Measurements Example<a class="anchor" aria-label="anchor" href="#motivation-a-repeated-measurements-example"><i class="fas fa-link"></i></a>
</h3>
<p>Consider a scenario where we analyze <strong>repeated measurements</strong> of a response variable <span class="math inline">\(Y_{ij}\)</span> for the <span class="math inline">\(i\)</span>-th subject at time <span class="math inline">\(j\)</span>:</p>
<ul>
<li>
<span class="math inline">\(i = 1, \dots, N\)</span> (subjects)</li>
<li>
<span class="math inline">\(j = 1, \dots, n_i\)</span> (measurements per subject)</li>
</ul>
<p>We define the response vector for subject <span class="math inline">\(i\)</span> as:</p>
<p><span class="math display">\[
\mathbf{Y}_i =
\begin{bmatrix}
Y_{i1} \\
Y_{i2} \\
\vdots \\
Y_{in_i}
\end{bmatrix}
\]</span></p>
<p>To model this data, we use a two-stage hierarchical approach:</p>
<div id="sec-stage-1-regression-model-within-subject-variation" class="section level4" number="8.1.1.1">
<h4>
<span class="header-section-number">8.1.1.1</span> Stage 1: Regression Model (Within-Subject Variation)<a class="anchor" aria-label="anchor" href="#sec-stage-1-regression-model-within-subject-variation"><i class="fas fa-link"></i></a>
</h4>
<p>We first model how the response changes over time for each subject:</p>
<p><span class="math display">\[
\mathbf{Y}_i = Z_i \beta_i + \epsilon_i
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Z_i\)</span> is an <span class="math inline">\(n_i \times q\)</span> matrix of known covariates (e.g., time, treatment).</li>
<li>
<span class="math inline">\(\beta_i\)</span> is a <span class="math inline">\(q \times 1\)</span> vector of subject-specific regression coefficients.</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> represents random errors, often assumed to follow <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2 I)\)</span>.</li>
</ul>
<p>At this stage, each individual has their own unique regression coefficients <span class="math inline">\(\beta_i\)</span>. However, estimating a separate <span class="math inline">\(\beta_i\)</span> for each subject is impractical when <span class="math inline">\(N\)</span> is large. Thus, we introduce a [second stage](#sec-stage-2-parameter-model-(between-subject-variation) to impose structure on <span class="math inline">\(\beta_i\)</span>.</p>
</div>
<div id="sec-stage-2-parameter-model-between-subject-variation" class="section level4" number="8.1.1.2">
<h4>
<span class="header-section-number">8.1.1.2</span> Stage 2: Parameter Model (Between-Subject Variation)<a class="anchor" aria-label="anchor" href="#sec-stage-2-parameter-model-between-subject-variation"><i class="fas fa-link"></i></a>
</h4>
<p>We assume that the subject-specific coefficients <span class="math inline">\(\beta_i\)</span> arise from a common population distribution:</p>
<p><span class="math display">\[
\beta_i = K_i \beta + b_i
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(K_i\)</span> is a <span class="math inline">\(q \times p\)</span> matrix of known covariates.</li>
<li>
<span class="math inline">\(\beta\)</span> is a <span class="math inline">\(p \times 1\)</span> vector of global parameters (fixed effects).</li>
<li>
<span class="math inline">\(\mathbf{b}_i\)</span> are random effects, assumed to follow <span class="math inline">\(\mathbf{b}_i \sim N(0, D)\)</span>.</li>
</ul>
<p>Thus, each individual‚Äôs regression coefficients <span class="math inline">\(\beta_i\)</span> are modeled as deviations from a population-level mean <span class="math inline">\(\beta\)</span>, with subject-specific variations <span class="math inline">\(b_i\)</span>. This hierarchical structure enables:</p>
<ul>
<li>Borrowing of strength: Individual estimates <span class="math inline">\(\beta_i\)</span> are informed by the overall population distribution.</li>
<li>Improved generalization: The model captures both fixed and random variability efficiently.</li>
</ul>
<p>The full <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Model</a> can then be written as:</p>
<p><span class="math display">\[
\mathbf{Y}_i = Z_i K_i \beta + Z_i b_i + \epsilon_i
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Z_i K_i \beta\)</span> represents the <strong>fixed effects</strong> component.</li>
<li>
<span class="math inline">\(Z_i b_i\)</span> represents the <strong>random effects</strong> component.</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> represents the residual errors.</li>
</ul>
<p>This formulation accounts for both <strong>within-subject correlations</strong> (via random effects) and <strong>between-subject variability</strong> (via fixed effects), making it a powerful tool for analyzing dependent data.</p>
<hr>
</div>
</div>
<div id="example-linear-mixed-model-for-repeated-measurements" class="section level3" number="8.1.2">
<h3>
<span class="header-section-number">8.1.2</span> Example: Linear Mixed Model for Repeated Measurements<a class="anchor" aria-label="anchor" href="#example-linear-mixed-model-for-repeated-measurements"><i class="fas fa-link"></i></a>
</h3>
<div id="sec-stage-1-subject-specific-model-lmm-example" class="section level4" number="8.1.2.1">
<h4>
<span class="header-section-number">8.1.2.1</span> Stage 1: Subject-Specific Model<a class="anchor" aria-label="anchor" href="#sec-stage-1-subject-specific-model-lmm-example"><i class="fas fa-link"></i></a>
</h4>
<p>The first stage models the response variable <span class="math inline">\(Y_{ij}\)</span> for subject <span class="math inline">\(i\)</span> at time <span class="math inline">\(t_{ij}\)</span>:</p>
<p><span class="math display">\[
Y_{ij} = \beta_{1i} + \beta_{2i} t_{ij} + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(j = 1, \dots, n_i\)</span> represents different time points for subject <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(\beta_{1i}\)</span> is the subject-specific intercept (baseline response for subject <span class="math inline">\(i\)</span>).</li>
<li>
<span class="math inline">\(\beta_{2i}\)</span> is the subject-specific slope (rate of change over time).</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2)\)</span> are independent errors.</li>
</ul>
<p>In matrix notation, the model is written as:</p>
<p><span class="math display">\[
\mathbf{Y}_i =
\begin{bmatrix}
Y_{i1} \\
Y_{i2} \\
\vdots \\
Y_{in_i}
\end{bmatrix},
\quad
\mathbf{Z}_i =
\begin{bmatrix}
1 &amp; t_{i1} \\
1 &amp; t_{i2} \\
\vdots &amp; \vdots \\
1 &amp; t_{in_i}
\end{bmatrix},
\]</span></p>
<p><span class="math display">\[
\beta_i =
\begin{bmatrix}
\beta_{1i} \\
\beta_{2i}
\end{bmatrix},
\quad
\epsilon_i =
\begin{bmatrix}
\epsilon_{i1} \\
\epsilon_{i2} \\
\vdots \\
\epsilon_{in_i}
\end{bmatrix}.
\]</span></p>
<p>Thus, the model can be rewritten as:</p>
<p><span class="math display">\[
\mathbf{Y_i = Z_i \beta_i + \epsilon_i}.
\]</span></p>
<hr>
</div>
<div id="sec-stage-2-population-level-model-lmm-example" class="section level4" number="8.1.2.2">
<h4>
<span class="header-section-number">8.1.2.2</span> Stage 2: Population-Level Model<a class="anchor" aria-label="anchor" href="#sec-stage-2-population-level-model-lmm-example"><i class="fas fa-link"></i></a>
</h4>
<p>Since estimating a separate <span class="math inline">\(\beta_{1i}\)</span> and <span class="math inline">\(\beta_{2i}\)</span> for each subject is impractical, we assume they follow a population distribution:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_{1i} &amp;= \beta_0 + b_{1i}, \\
\beta_{2i} &amp;= \beta_1 L_i + \beta_2 H_i + \beta_3 C_i + b_{2i}.
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(L_i, H_i, C_i\)</span> are indicator variables for treatment groups:
<ul>
<li>
<span class="math inline">\(L_i = 1\)</span> if the subject belongs to treatment group 1, else 0.</li>
<li>
<span class="math inline">\(H_i = 1\)</span> if the subject belongs to treatment group 2, else 0.</li>
<li>
<span class="math inline">\(C_i = 1\)</span> if the subject belongs to treatment group 3, else 0.</li>
</ul>
</li>
<li>
<span class="math inline">\(\beta_0\)</span> represents the average baseline response across all subjects.</li>
<li>
<span class="math inline">\(\beta_1, \beta_2, \beta_3\)</span> are the average time effects for the respective treatment groups.</li>
<li>
<span class="math inline">\(b_{1i}, b_{2i}\)</span> are random effects representing subject-specific deviations.</li>
</ul>
<p>This structure implies that while the intercept <span class="math inline">\(\beta_{1i}\)</span> varies randomly across subjects, the slope <span class="math inline">\(\beta_{2i}\)</span> depends both on treatment and random subject-specific deviations.</p>
<p>In matrix form, this is:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{K}_i &amp;=
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; L_i &amp; H_i &amp; C_i
\end{bmatrix}, \\
\beta &amp;=
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{bmatrix}, \\
\mathbf{b}_i &amp;=
\begin{bmatrix}
b_{1i} \\
b_{2i}
\end{bmatrix}, \\
\beta_i &amp;= \mathbf{K_i \beta + b_i}.
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="final-mixed-model-formulation" class="section level4" number="8.1.2.3">
<h4>
<span class="header-section-number">8.1.2.3</span> Final Mixed Model Formulation<a class="anchor" aria-label="anchor" href="#final-mixed-model-formulation"><i class="fas fa-link"></i></a>
</h4>
<p>Substituting <a href="sec-linear-mixed-models.html#sec-stage-2-population-level-model-lmm-example">Stage 2</a> into <a href="sec-linear-mixed-models.html#sec-stage-1-subject-specific-model-lmm-example">Stage 1</a>, we obtain the full mixed model:</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{Z}_i (\mathbf{K}_i \beta + \mathbf{b}_i) + \epsilon_i.
\]</span></p>
<p>Expanding:</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{Z}_i \mathbf{K}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i.
\]</span></p>
<p>Interpretation:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{Z}_i \mathbf{K}_i \beta\)</span> represents the <strong>fixed effects</strong> (population-level trends).</li>
<li>
<span class="math inline">\(\mathbf{Z}_i \mathbf{b}_i\)</span> represents the <strong>random effects</strong> (subject-specific variations).</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> represents the <strong>residual errors</strong>.</li>
</ul>
<p><strong>Assumptions</strong>:</p>
<ul>
<li>Random effects: <span class="math inline">\(\mathbf{b}_i \sim N(0, D)\)</span>, where <span class="math inline">\(D\)</span> is the variance-covariance matrix.</li>
<li>Residual errors: <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2 I)\)</span>.</li>
<li>Independence: <span class="math inline">\(\mathbf{b}_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> are independent.</li>
</ul>
<hr>
<p>To estimate <span class="math inline">\(\hat{\beta}\)</span>, one might consider a sequential approach:</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\hat{\beta}_i\)</span> for each subject in the first stage.</li>
<li>Estimate <span class="math inline">\(\hat{\beta}\)</span> in the second stage by replacing <span class="math inline">\(\beta_i\)</span> with <span class="math inline">\(\hat{\beta}_i\)</span>.</li>
</ol>
<p>However, this method introduces several problems:</p>
<ul>
<li>Loss of information: Summarizing <span class="math inline">\(\mathbf{Y}_i\)</span> solely by <span class="math inline">\(\hat{\beta}_i\)</span> discards valuable within-subject variability.</li>
<li>Ignoring uncertainty: Treating estimated values <span class="math inline">\(\hat{\beta}_i\)</span> as known values leads to underestimated variability.</li>
<li>Unequal sample sizes: Subjects may have different numbers of observations (<span class="math inline">\(n_i\)</span>), which affects variance estimation.</li>
</ul>
<p>To address these issues, we adopt the <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Model</a> (LMM) framework <span class="citation">(<a href="references.html#ref-laird1982random">Laird and Ware 1982</a>)</span>.</p>
<hr>
</div>
<div id="reformulating-the-linear-mixed-model" class="section level4" number="8.1.2.4">
<h4>
<span class="header-section-number">8.1.2.4</span> Reformulating the Linear Mixed Model<a class="anchor" aria-label="anchor" href="#reformulating-the-linear-mixed-model"><i class="fas fa-link"></i></a>
</h4>
<p>Substituting [Stage 2](#sec-stage-2-parameter-model-(between-subject-variation) into [Stage 1](#sec-stage-2-parameter-model-(between-subject-variation), we obtain:</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{Z}_i \mathbf{K}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i.
\]</span></p>
<p>Defining <span class="math inline">\(\mathbf{X}_i = \mathbf{Z}_i \mathbf{K}_i\)</span> as an <span class="math inline">\(n_i \times p\)</span> matrix of covariates, we rewrite the model as:</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i.
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(i = 1, \dots, N\)</span> (subjects).</p></li>
<li><p><span class="math inline">\(\beta\)</span> are <strong>fixed effects</strong>, common to all subjects.</p></li>
<li>
<p><span class="math inline">\(\mathbf{b}_i\)</span> are <strong>subject-specific random effects</strong>, assumed to follow:</p>
<p><span class="math display">\[
\mathbf{b}_i \sim N_q(\mathbf{0,D}).
\]</span></p>
</li>
<li>
<p><span class="math inline">\(\mathbf{\epsilon}_i\)</span> represents residual errors:</p>
<p><span class="math display">\[
\mathbf{\epsilon}_i \sim N_{n_i}(\mathbf{0,\Sigma_i}).
\]</span></p>
</li>
<li><p>Independence assumption: <span class="math inline">\(\mathbf{b}_i\)</span> and <span class="math inline">\(\mathbf{\epsilon}_i\)</span> are independent.</p></li>
<li>
<p>Dimension notation:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{Z}_i\)</span> is an <span class="math inline">\(n_i \times q\)</span> matrix of known covariates for random effects.</li>
<li>
<span class="math inline">\(\mathbf{X}_i\)</span> is an <span class="math inline">\(n_i \times p\)</span> matrix of known covariates for fixed effects.</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="hierarchical-conditional-formulation" class="section level4" number="8.1.2.5">
<h4>
<span class="header-section-number">8.1.2.5</span> Hierarchical (Conditional) Formulation<a class="anchor" aria-label="anchor" href="#hierarchical-conditional-formulation"><i class="fas fa-link"></i></a>
</h4>
<p>Rewriting the LMM in <strong>hierarchical form</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y}_i | \mathbf{b}_i &amp;\sim N(\mathbf{X}_i \beta+ \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i), \\
\mathbf{b}_i &amp;\sim N(\mathbf{0,D}).
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>The first equation states that, given the subject-specific random effects <span class="math inline">\(\mathbf{b}_i\)</span>, the response follows a normal distribution with mean <span class="math inline">\(\mathbf{X}_i \beta+ \mathbf{Z}_i \mathbf{b}_i\)</span> and covariance <span class="math inline">\(\mathbf{\Sigma}_i\)</span>.</li>
<li>The second equation states that the random effects <span class="math inline">\(\mathbf{b}_i\)</span> follow a <strong>multivariate normal</strong> distribution with mean zero and covariance <span class="math inline">\(D\)</span>.</li>
</ul>
<p>We denote the respective probability density functions as:</p>
<p><span class="math display">\[
f(\mathbf{Y}_i |\mathbf{b}_i) \quad \text{and} \quad f(\mathbf{b}_i).
\]</span></p>
<hr>
<p>Using the general <strong>marginalization formula</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
f(A,B) &amp;= f(A|B)f(B) \\
f(A) &amp;= \int f(A,B)dB = \int f(A|B) f(B) dB
\end{aligned}
\]</span></p>
<p>we obtain the <strong>marginal density</strong> of <span class="math inline">\(\mathbf{Y}_i\)</span>:</p>
<p><span class="math display">\[
f(\mathbf{Y}_i) = \int f(\mathbf{Y}_i | \mathbf{b}_i) f(\mathbf{b}_i) d\mathbf{b}_i.
\]</span></p>
<p>Solving this integral, we obtain:</p>
<p><span class="math display">\[
\mathbf{Y}_i \sim N(\mathbf{X_i \beta}, \mathbf{Z_i D Z_i'} + \mathbf{\Sigma_i}).
\]</span></p>
<p>Interpretation:</p>
<ul>
<li><p><strong>Mean structure:</strong> The expectation remains <span class="math inline">\(\mathbf{X}_i \beta\)</span>, the fixed effects.</p></li>
<li>
<p><strong>Variance structure:</strong> The covariance now incorporates random effect variability:</p>
<p><span class="math display">\[
\mathbf{Z_i D Z_i'} + \mathbf{\Sigma_i}.
\]</span></p>
<p>This shows that the random effects contribute additional correlation between observations.</p>
</li>
</ul>
<blockquote>
<p>üîπ <strong>Key Takeaway:</strong><br>
The marginal formulation of LMM no longer includes <span class="math inline">\(Z_i b_i\)</span> in the mean, but instead incorporates it into the covariance structure, introducing marginal dependence in <span class="math inline">\(\mathbf{Y}_i\)</span>.</p>
</blockquote>
<p>Technically, rather than ‚Äúaveraging out‚Äù the random effect <span class="math inline">\(b_i\)</span>, we add its contribution to the variance-covariance matrix.</p>
<hr>
<p>Continue with the example:</p>
<p><span class="math display">\[
Y_{ij} = (\beta_0 + b_{1i}) + (\beta_1L_i + \beta_2 H_i + \beta_3 C_i + b_{2i})t_{ij} + \epsilon_{ij}.
\]</span></p>
<p>For each treatment group:</p>
<p><span class="math display">\[
Y_{ij} =
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + b_{2i}) t_{ij} + \epsilon_{ij}, &amp; L_i = 1 \\
\beta_0 + b_{1i} + (\beta_2 + b_{2i}) t_{ij} + \epsilon_{ij}, &amp; H_i = 1 \\
\beta_0 + b_{1i} + (\beta_3 + b_{2i}) t_{ij} + \epsilon_{ij}, &amp; C_i = 1.
\end{cases}
\]</span></p>
<p>Interpretation:</p>
<ul>
<li>
<strong>Intercepts and slopes are subject-specific</strong>: Each subject has their own baseline response and rate of change.</li>
<li>
<strong>Treatment groups affect slopes, but not intercepts</strong>:
<ul>
<li>
<span class="math inline">\(\beta_0\)</span> is the common intercept across groups.</li>
<li>Slopes differ by treatment: <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(L\)</span>, <span class="math inline">\(\beta_2\)</span> for <span class="math inline">\(H\)</span>, <span class="math inline">\(\beta_3\)</span> for <span class="math inline">\(C\)</span>.</li>
</ul>
</li>
<li>
<strong>Random effects</strong> introduce within-subject correlation:
<ul>
<li>
<span class="math inline">\(b_{1i}\)</span> allows individual variation in baseline response.</li>
<li>
<span class="math inline">\(b_{2i}\)</span> allows subject-specific deviations in slopes.</li>
</ul>
</li>
</ul>
<hr>
<p>In the <strong>hierarchical model form</strong>, we again express the LMM as:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y}_i | \mathbf{b}_i &amp;\sim N(\mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i)\\
\mathbf{b}_i &amp;\sim N(\mathbf{0,D})
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{X}_i\)</span> and <span class="math inline">\(\mathbf{Z}_i\)</span> are design matrices for <strong>fixed</strong> and <strong>random</strong> effects, respectively.</li>
<li>
<span class="math inline">\(\mathbf{b}_i\)</span> represents <strong>subject-specific random effects</strong>.</li>
<li>
<span class="math inline">\(\mathbf{\Sigma}_i\)</span> is the <strong>residual error covariance matrix</strong>.</li>
<li>
<span class="math inline">\(\mathbf{D}\)</span> is the <strong>random effects covariance matrix</strong>.</li>
</ul>
<p>The fixed-effects parameter vector is:</p>
<p><span class="math display">\[
\beta = (\beta_0, \beta_1, \beta_2, \beta_3)'.
\]</span></p>
<p>From the model structure, we define:</p>
<p><span class="math display">\[
\mathbf{X}_i = \mathbf{Z}_i \mathbf{K}_i.
\]</span></p>
<p>Expanding,</p>
<p><span class="math display">\[
\mathbf{Z}_i =
\begin{bmatrix}
1 &amp; t_{i1} \\
1 &amp; t_{i2} \\
\vdots &amp; \vdots \\
1 &amp; t_{in_i}
\end{bmatrix},
\quad
\mathbf{K}_i =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; L_i &amp; H_i &amp; C_i
\end{bmatrix}.
\]</span></p>
<p>Multiplying:</p>
<p><span class="math display">\[
\mathbf{X}_i =
\begin{bmatrix}
1 &amp; t_{i1}L_i &amp; t_{i1}H_i &amp; t_{i1}C_i \\
1 &amp; t_{i2}L_i &amp; t_{i2}H_i &amp; t_{i2}C_i \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; t_{in_i}L_i &amp; t_{in_i}H_i &amp; t_{in_i}C_i
\end{bmatrix}.
\]</span></p>
<p>The random effects vector is:</p>
<p><span class="math display">\[
\mathbf{b}_i =
\begin{bmatrix}
b_{1i} \\
b_{2i}
\end{bmatrix}.
\]</span></p>
<p>The covariance matrix <span class="math inline">\(\mathbf{D}\)</span> for random effects is:</p>
<p><span class="math display">\[
\mathbf{D} =
\begin{bmatrix}
d_{11} &amp; d_{12} \\
d_{12} &amp; d_{22}
\end{bmatrix}.
\]</span></p>
<p>We assume <strong>conditional independence</strong>:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}.
\]</span></p>
<p>This means that, <strong>given the random effects</strong> <span class="math inline">\(\mathbf{b}_i\)</span> and <span class="math inline">\(\beta\)</span>, the responses for subject <span class="math inline">\(i\)</span> are independent.</p>
<hr>
<p>To derive the <strong>marginal model</strong>, we integrate out the random effects <span class="math inline">\(\mathbf{b}_i\)</span>. This leads to:</p>
<p><span class="math display">\[
Y_{ij} = \beta_0 + \beta_1 L_i t_{ij} + \beta_2 H_i t_{ij} + \beta_3 C_i t_{ij} + \eta_{ij},
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\eta_i \sim N(\mathbf{0}, \mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' + \mathbf{\Sigma}_i).
\]</span></p>
<p>Thus, the full marginal model is:</p>
<p><span class="math display">\[
\mathbf{Y_i} \sim N(\mathbf{X}_i \beta, \mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' + \mathbf{\Sigma}_i).
\]</span></p>
<hr>
<p>Example: Case Where <span class="math inline">\(n_i = 2\)</span></p>
<p>For a subject with <strong>two observations</strong>, we compute:</p>
<p><span class="math display">\[
\mathbf{Z}_i =
\begin{bmatrix}
1 &amp; t_{i1} \\
1 &amp; t_{i2}
\end{bmatrix}.
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' =
\begin{bmatrix}
1 &amp; t_{i1} \\
1 &amp; t_{i2}
\end{bmatrix}
\begin{bmatrix}
d_{11} &amp; d_{12} \\
d_{12} &amp; d_{22}
\end{bmatrix}
\begin{bmatrix}
1 &amp; 1 \\
t_{i1} &amp; t_{i2}
\end{bmatrix}.
\]</span></p>
<p>Expanding the multiplication:</p>
<p><span class="math display">\[
\mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' =
\begin{bmatrix}
d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 &amp; d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\
d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} &amp; d_{11} + 2d_{12}t_{i2} + d_{22} t_{i2}^2
\end{bmatrix}.
\]</span></p>
<p>Finally, incorporating the residual variance:</p>
<p><span class="math display">\[
\text{Var}(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \sigma^2.
\]</span></p>
<p>Interpretation of the Marginal Model</p>
<ol style="list-style-type: decimal">
<li>
<strong>Correlation in the Errors</strong>:
<ul>
<li>The off-diagonal elements of <span class="math inline">\(\mathbf{Z}_i \mathbf{D} \mathbf{Z}_i'\)</span> introduce <strong>correlation</strong> between observations within the same subject.</li>
<li>This accounts for the fact that repeated measurements on the same individual are not independent.</li>
</ul>
</li>
<li>
<strong>Variance Structure</strong>:
<ul>
<li>
<p>The variance function of <span class="math inline">\(Y_{ij}\)</span> is quadratic in time:</p>
<p><span class="math display">\[
\text{Var}(Y_{ij}) = d_{11} + 2d_{12}t_{ij} + d_{22}t_{ij}^2 + \sigma^2.
\]</span></p>
</li>
<li><p>The curvature of this variance function is determined by <span class="math inline">\(d_{22}\)</span>.</p></li>
<li><p>If <span class="math inline">\(d_{22} &gt; 0\)</span>, variance increases over time.</p></li>
</ul>
</li>
</ol>
<blockquote>
<p>üîπ <strong>Key Takeaway</strong>:<br>
In the hierarchical model, random effects contribute to the mean structure.<br>
In the marginal model, they affect the covariance structure, leading to heteroskedasticity (changing variance over time) and correlation between repeated measures.</p>
</blockquote>
<hr>
</div>
</div>
<div id="sec-random-intercepts-model-lmm" class="section level3" number="8.1.3">
<h3>
<span class="header-section-number">8.1.3</span> Random-Intercepts Model<a class="anchor" aria-label="anchor" href="#sec-random-intercepts-model-lmm"><i class="fas fa-link"></i></a>
</h3>
<p>The <a href="">Random-Intercepts Model</a> is obtained by <strong>removing random slopes</strong>, meaning:</p>
<ul>
<li>All subject-specific variability in slopes is attributed only to treatment differences.</li>
<li>The model allows each subject to have their own intercept, but within each treatment group, all subjects share the same slope.</li>
</ul>
<p>The hierarchical (conditional) model is:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y}_i | b_i &amp;\sim N(\mathbf{X}_i \beta + 1 b_i, \mathbf{\Sigma}_i), \\
b_i &amp;\sim N(0, d_{11}).
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(b_i\)</span> is the <strong>random intercept</strong> for subject <span class="math inline">\(i\)</span>, assumed to follow <span class="math inline">\(N(0, d_{11})\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{X}_i\)</span> contains the <strong>fixed effects</strong>, which include treatment and time.</li>
<li>
<span class="math inline">\(\mathbf{\Sigma}_i\)</span> represents <strong>residual variance</strong>, typically assumed to be <span class="math inline">\(\sigma^2 \mathbf{I}\)</span> (independent errors).</li>
</ul>
<p>Since there are no <strong>random slopes</strong>, the only source of <strong>subject-specific variability</strong> is the intercept.</p>
<p>Integrating out <span class="math inline">\(b_i\)</span> and assuming conditional independence <span class="math inline">\(\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}\)</span>, the <strong>marginal distribution</strong> of <span class="math inline">\(\mathbf{Y}_i\)</span> is:</p>
<p><span class="math display">\[
\mathbf{Y}_i \sim N(\mathbf{X}_i \beta, 11'd_{11} + \sigma^2 \mathbf{I}).
\]</span></p>
<p>The <strong>marginal covariance matrix</strong> is:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Cov}(\mathbf{Y}_i) &amp;= 11'd_{11} + \sigma^2 I \\
&amp;=
\begin{bmatrix}
d_{11} + \sigma^2 &amp; d_{11} &amp; d_{11} &amp; \dots &amp; d_{11} \\
d_{11} &amp; d_{11} + \sigma^2 &amp; d_{11} &amp; \dots &amp; d_{11} \\
d_{11} &amp; d_{11} &amp; d_{11} + \sigma^2 &amp; \dots &amp; d_{11} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
d_{11} &amp; d_{11} &amp; d_{11} &amp; \dots &amp; d_{11} + \sigma^2
\end{bmatrix}.
\end{aligned}
\]</span></p>
<p>The <strong>correlation matrix</strong> is obtained by standardizing the covariance matrix:</p>
<p><span class="math display">\[
\text{Corr}(\mathbf{Y}_i) =
\begin{bmatrix}
1 &amp; \rho &amp; \rho &amp; \dots &amp; \rho \\
\rho &amp; 1 &amp; \rho &amp; \dots &amp; \rho \\
\rho &amp; \rho &amp; 1 &amp; \dots &amp; \rho \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\rho &amp; \rho &amp; \rho &amp; \dots &amp; 1
\end{bmatrix},
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\rho = \frac{d_{11}}{d_{11} + \sigma^2}.
\]</span></p>
<p>This <strong>correlation structure</strong> is known as <strong>compound symmetry</strong>, meaning:</p>
<ul>
<li>
<strong>Constant variance</strong>: <span class="math inline">\(\text{Var}(Y_{ij}) = d_{11} + \sigma^2\)</span> for all <span class="math inline">\(j\)</span>.</li>
<li>
<strong>Equal correlation</strong>: <span class="math inline">\(\text{Corr}(Y_{ij}, Y_{ik}) = \rho\)</span> for all <span class="math inline">\(j \neq k\)</span>.</li>
<li>
<strong>Positive correlation</strong>: Any two observations from the same subject are equally correlated.</li>
</ul>
<p>Interpretation of <span class="math inline">\(\rho\)</span> (Intra-Class Correlation)</p>
<ul>
<li><p><span class="math inline">\(\rho\)</span> is called the intra-class correlation coefficient (ICC).</p></li>
<li>
<p>It measures the proportion of total variability that is due to between-subject variability:</p>
<p><span class="math display">\[
\rho = \frac{\text{Between-Subject Variance}}{\text{Total Variance}} = \frac{d_{11}}{d_{11} + \sigma^2}.
\]</span></p>
</li>
<li>
<p>If <span class="math inline">\(\rho\)</span> is large:</p>
<ul>
<li>The inter-subject variability (<span class="math inline">\(d_{11}\)</span>) is large relative to the intra-subject variability (<span class="math inline">\(\sigma^2\)</span>).</li>
<li>This means subjects differ substantially in their intercepts.</li>
<li>Responses from the same subject are highly correlated.</li>
</ul>
</li>
<li>
<p>If <span class="math inline">\(\rho\)</span> is small:</p>
<ul>
<li>The residual error variance dominates, meaning individual differences in intercepts are small.</li>
<li>Measurements from the same subject are weakly correlated.</li>
</ul>
</li>
</ul>
<hr>
<p>Summary of the Random-Intercepts Model</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="34%">
<col width="65%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Explanation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Intercepts</strong></td>
<td>Random, subject-specific (<span class="math inline">\(b_i\)</span>)</td>
</tr>
<tr class="even">
<td><strong>Slopes</strong></td>
<td>Fixed within each treatment group</td>
</tr>
<tr class="odd">
<td><strong>Covariance Structure</strong></td>
<td>
<strong>Compound symmetry</strong> (constant variance, equal correlation)</td>
</tr>
<tr class="even">
<td>
<strong>Intra-Class Correlation (</strong><span class="math inline">\(\rho\)</span>)</td>
<td>Measures <strong>between-subject variability</strong> relative to total variability</td>
</tr>
<tr class="odd">
<td><strong>Interpretation</strong></td>
<td>
<p>Large <span class="math inline">\(\rho\)</span> ‚Üí Strong subject-level differences,</p>
<p>Small <span class="math inline">\(\rho\)</span> ‚Üí Mostly residual noise</p>
</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="covariance-models-in-linear-mixed-models" class="section level3" number="8.1.4">
<h3>
<span class="header-section-number">8.1.4</span> Covariance Models in Linear Mixed Models<a class="anchor" aria-label="anchor" href="#covariance-models-in-linear-mixed-models"><i class="fas fa-link"></i></a>
</h3>
<p>Previously, we assumed that the <strong>within-subject errors</strong> are <strong>conditionally independent</strong>, meaning:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}.
\]</span></p>
<p>However, real-world data often exhibit <strong>correlated errors</strong>, particularly in <strong>longitudinal studies</strong> where observations over time are influenced by underlying stochastic processes.</p>
<p>To model this dependence, we decompose the error term into two components:</p>
<p><span class="math display">\[
\epsilon_i = \epsilon_{(1)i} + \epsilon_{(2)i},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\epsilon_{(1)i}\)</span> (Serial Correlation Component):
<ul>
<li>Represents subject-specific response to time-varying stochastic processes.</li>
<li>Captures dependency across observations for the same subject.</li>
</ul>
</li>
<li>
<span class="math inline">\(\epsilon_{(2)i}\)</span> (Measurement Error Component):
<ul>
<li>Represents pure measurement error, assumed independent of <span class="math inline">\(\epsilon_{(1)i}\)</span>.</li>
</ul>
</li>
</ul>
<p>Thus, the full LMM formulation becomes:</p>
<p><span class="math display">\[
\mathbf{Y_i} = \mathbf{X_i \beta} + \mathbf{Z_i b_i} + \mathbf{\epsilon_{(1)i}} + \mathbf{\epsilon_{(2)i}}.
\]</span></p>
<p>where:</p>
<ul>
<li>Random effects: <span class="math inline">\(\mathbf{b_i} \sim N(\mathbf{0, D})\)</span>.</li>
<li>Measurement errors: <span class="math inline">\(\epsilon_{(2)i} \sim N(\mathbf{0, \sigma^2 I_{n_i}})\)</span>.</li>
<li>Serial correlation errors: <span class="math inline">\(\epsilon_{(1)i} \sim N(\mathbf{0, \tau^2 H_i})\)</span>.</li>
<li>Independence assumption: <span class="math inline">\(\mathbf{b}_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> are mutually independent.</li>
</ul>
<hr>
<p>To model the correlation structure of the <strong>serial correlation component</strong> <span class="math inline">\(\epsilon_{(1)i}\)</span>, we define the <span class="math inline">\(n_i \times n_i\)</span> correlation (or covariance) matrix <span class="math inline">\(\mathbf{H}_i\)</span>.</p>
<p>The <span class="math inline">\((j,k)\)</span>th element of <span class="math inline">\(\mathbf{H}_i\)</span> is denoted as:</p>
<p><span class="math display">\[
h_{ijk} = g(t_{ij}, t_{ik}),
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(h_{ijk}\)</span> represents the covariance (or correlation) between time points <span class="math inline">\(t_{ij}\)</span> and <span class="math inline">\(t_{ik}\)</span>.</p></li>
<li><p><span class="math inline">\(g(t_{ij}, t_{ik})\)</span> is a decreasing function that defines the correlation between time points <span class="math inline">\(t_{ij}\)</span> and <span class="math inline">\(t_{ik}\)</span>.</p></li>
<li>
<p>Typically, we assume that this function depends only on the time difference (stationarity assumption):</p>
<p><span class="math display">\[
h_{ijk} = g(|t_{ij} - t_{ik}|)
\]</span></p>
<p>meaning that the correlation only depends on the absolute time lag.</p>
</li>
<li>
<p>To be a valid correlation matrix, we require:</p>
<p><span class="math display">\[
g(0) = 1.
\]</span></p>
<p>This ensures that each observation has a perfect correlation with itself.</p>
</li>
</ul>
<hr>
<p><strong>Common Choices</strong> for <span class="math inline">\(g(|t_{ij} - t_{ik}|)\)</span></p>
<p>Several functions can be used to define the <strong>decay in correlation</strong> as time differences increase.</p>
<p><strong>1. Exponential Correlation (Continuous-Time AR(1))</strong></p>
<p><span class="math display">\[
g(|t_{ij} - t_{ik}|) = \exp(-\phi |t_{ij} - t_{ik}|)
\]</span></p>
<ul>
<li>
<strong>Decay rate:</strong> Controlled by <span class="math inline">\(\phi &gt; 0\)</span>.</li>
<li>
<strong>Interpretation:</strong>
<ul>
<li>Observations closer in time are more correlated.</li>
<li>The correlation decreases exponentially as time separation increases.</li>
</ul>
</li>
<li>
<strong>Use case:</strong> Often used in biological and economic time-series models.</li>
</ul>
<hr>
<p><strong>2. Gaussian Correlation (Squared Exponential Kernel)</strong></p>
<p><span class="math display">\[
g(|t_{ij} - t_{ik}|) = \exp(-\phi (t_{ij} - t_{ik})^2)
\]</span></p>
<ul>
<li>
<strong>Decay rate:</strong> Faster than exponential.</li>
<li>
<strong>Interpretation:</strong>
<ul>
<li>If <span class="math inline">\(\phi\)</span> is large, correlation drops sharply as time separation increases.</li>
<li>Produces smooth correlation structures.</li>
</ul>
</li>
<li>
<strong>Use case:</strong> Used in spatial statistics and machine learning (Gaussian processes).</li>
</ul>
<hr>
<p><strong>3. Autoregressive (AR(1)) Correlation</strong></p>
<p>A <strong>First-Order Autoregressive Model (AR(1))</strong> assumes that the value at time <span class="math inline">\(t\)</span> depends on its previous value:</p>
<p><span class="math display">\[
\alpha_t = \phi \alpha_{t-1} + \eta_t,
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\eta_t \sim \text{i.i.d. } N(0, \sigma^2_\eta)\)</span> (white noise process).</p></li>
<li><p><span class="math inline">\(\phi\)</span> is the <strong>autocorrelation coefficient</strong>.</p></li>
</ul>
<p>Then, the covariance between two observations at different times is:</p>
<p><span class="math display">\[ \text{Cov}(\alpha_t, \alpha_{t+h}) = \frac{\sigma^2_\eta \phi^{|h|}}{1 - \phi^2}. \]</span></p>
<p>Thus, the <strong>correlation</strong> between time points <span class="math inline">\(t\)</span> and <span class="math inline">\(t+h\)</span> is:</p>
<p><span class="math display">\[ \text{Corr}(\alpha_t, \alpha_{t+h}) = \phi^{|h|}. \]</span></p>
<p>For a sequence of <span class="math inline">\(T\)</span> time points, the resulting <strong>Toeplitz correlation matrix</strong> is:</p>
<p><span class="math display">\[ \text{Corr}(\alpha_T) = \begin{bmatrix} 1 &amp; \phi^1 &amp; \phi^2 &amp; \dots &amp; \phi^{T-1} \\ \phi^1 &amp; 1 &amp; \phi^1 &amp; \dots &amp; \phi^{T-2} \\ \phi^2 &amp; \phi^1 &amp; 1 &amp; \dots &amp; \phi^{T-3} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \phi^{T-1} &amp; \phi^{T-2} &amp; \phi^{T-3} &amp; \dots &amp; 1 \end{bmatrix}. \]</span></p>
<ul>
<li>
<strong>Decay rate:</strong> <span class="math inline">\(\phi\)</span> controls how fast correlation decreases.</li>
<li>
<strong>Use case:</strong>
<ul>
<li>Common in <strong>time series models</strong>.</li>
<li>Appropriate when <strong>observations are equally spaced in time</strong>.</li>
</ul>
</li>
</ul>
<p>Properties of the AR(1) Structure:</p>
<ul>
<li>Correlation decreases with time lag:
<ul>
<li>Observations closer in time are more correlated.</li>
<li>Decay rate is controlled by <span class="math inline">\(\phi\)</span>.</li>
</ul>
</li>
<li>Toeplitz structure:
<ul>
<li>The covariance matrix exhibits a banded diagonal pattern.</li>
<li>Useful in longitudinal and time-series data.</li>
</ul>
</li>
<li>Applicability:
<ul>
<li>Small <span class="math inline">\(\phi\)</span> (<span class="math inline">\(\approx 0\)</span>) ‚Üí Weak autocorrelation (errors are mostly independent).</li>
<li>Large <span class="math inline">\(\phi\)</span> (<span class="math inline">\(\approx 1\)</span>) ‚Üí Strong autocorrelation (highly dependent observations).</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="covariance-structures-in-mixed-models" class="section level3" number="8.1.5">
<h3>
<span class="header-section-number">8.1.5</span> Covariance Structures in Mixed Models<a class="anchor" aria-label="anchor" href="#covariance-structures-in-mixed-models"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="14%">
<col width="18%">
<col width="7%">
<col width="11%">
<col width="26%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th>Structure</th>
<th>Covariance Function <span class="math inline">\(g(|t_{ij} - t_{ik}|)\)</span>
</th>
<th>Decay Behavior</th>
<th>Correlation Matrix Pattern</th>
<th>Key Properties</th>
<th>Common Use Cases</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Compound Symmetry</strong></td>
<td>Constant: <span class="math inline">\(g(|t_{ij} - t_{ik}|) = \rho\)</span>
</td>
<td>No decay</td>
<td>Constant off-diagonal</td>
<td>Equal correlation across all time points (homogeneous)</td>
<td>Random-intercepts model, repeated measures</td>
</tr>
<tr class="even">
<td><strong>AR(1) (Autoregressive)</strong></td>
<td><span class="math inline">\(\phi^{|t_{ij} - t_{ik}|}\)</span></td>
<td>Exponential decay</td>
<td>Toeplitz (banded diagonal)</td>
<td>Strong correlation for nearby observations, weak for distant ones</td>
<td>Time series, longitudinal data</td>
</tr>
<tr class="odd">
<td><strong>Exponential</strong></td>
<td><span class="math inline">\(\exp(-\phi |t_{ij} - t_{ik}|)\)</span></td>
<td>Smooth decay</td>
<td>Spatially continuous</td>
<td>Models continuous correlation decline over time/space</td>
<td>Growth curves, ecological models</td>
</tr>
<tr class="even">
<td><strong>Gaussian (Squared Exponential)</strong></td>
<td><span class="math inline">\(\exp(-\phi (t_{ij} - t_{ik})^2)\)</span></td>
<td>Rapid decay</td>
<td>Spatially continuous</td>
<td>Smooth decay but stronger locality effect</td>
<td>Spatial processes, geostatistics</td>
</tr>
<tr class="odd">
<td><strong>Toeplitz</strong></td>
<td>Varies by lag, <span class="math inline">\(g(|t_{ij} - t_{ik}|) = c_h\)</span>
</td>
<td>General pattern</td>
<td>General symmetric matrix</td>
<td>Arbitrary structure, allows irregular spacing</td>
<td>Irregular time points, flexible spatial models</td>
</tr>
<tr class="even">
<td><strong>Unstructured</strong></td>
<td>Fully parameterized, no constraints</td>
<td>No fixed pattern</td>
<td>General symmetric matrix</td>
<td>Allows any correlation pattern, but many parameters</td>
<td>Small datasets with unknown correlation</td>
</tr>
<tr class="odd">
<td><strong>Banded</strong></td>
<td>Zero correlation beyond a certain lag</td>
<td>Abrupt cutoff</td>
<td>Banded diagonal</td>
<td>Assumes only close observations are correlated</td>
<td>Large datasets, high-dimensional time series</td>
</tr>
<tr class="even">
<td><strong>Spatial Power</strong></td>
<td><span class="math inline">\(\phi^{|s_{ij} - s_{ik}|}\)</span></td>
<td>Exponential decay</td>
<td>Distance-based structure</td>
<td>Used when correlation depends on spatial distance</td>
<td>Spatial statistics, environmental data</td>
</tr>
<tr class="odd">
<td><strong>Mat√©rn Covariance</strong></td>
<td>Function of <span class="math inline">\(|t_{ij} - t_{ik}|^\nu\)</span>
</td>
<td>Flexible decay</td>
<td>Spatially continuous</td>
<td>Generalization of Gaussian and exponential structures</td>
<td>Geostatistics, spatiotemporal models</td>
</tr>
</tbody>
</table></div>
<hr>
<p><strong>Choosing the Right Covariance Structure</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Scenario</th>
<th>Recommended Structure</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Repeated measures with equal correlation</td>
<td>Compound Symmetry</td>
</tr>
<tr class="even">
<td>Longitudinal data with time dependence</td>
<td>AR(1), Toeplitz</td>
</tr>
<tr class="odd">
<td>Continuous-time process</td>
<td>Exponential, Gaussian</td>
</tr>
<tr class="even">
<td>Spatial correlation</td>
<td>Spatial Power, Mat√©rn</td>
</tr>
<tr class="odd">
<td>Irregularly spaced time points</td>
<td>Toeplitz, Unstructured</td>
</tr>
<tr class="even">
<td>High-dimensional data</td>
<td>Banded, AR(1)</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="estimation-in-linear-mixed-models" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Estimation in Linear Mixed Models<a class="anchor" aria-label="anchor" href="#estimation-in-linear-mixed-models"><i class="fas fa-link"></i></a>
</h2>
<p>The general <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Model</a> is:</p>
<p><span class="math display">\[ \mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i, \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\beta\)</span>: Fixed effects (parameters shared across subjects).</li>
<li>
<span class="math inline">\(\mathbf{b}_i\)</span>: Random effects (subject-specific deviations).</li>
<li>
<span class="math inline">\(\mathbf{X}_i\)</span>: Design matrix for fixed effects.</li>
<li>
<span class="math inline">\(\mathbf{Z}_i\)</span>: Design matrix for random effects.</li>
<li>
<span class="math inline">\(\mathbf{D}\)</span>: Covariance matrix of the random effects.</li>
<li>
<span class="math inline">\(\mathbf{\Sigma}_i\)</span>: Covariance matrix of the residual errors.</li>
</ul>
<p>Since <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\mathbf{b}_i\)</span>, <span class="math inline">\(\mathbf{D}\)</span>, and <span class="math inline">\(\mathbf{\Sigma}_i\)</span> are <strong>unknown</strong>, they must be <strong>estimated</strong> from data.</p>
<ul>
<li>
<span class="math inline">\(\beta, \mathbf{D}, \mathbf{\Sigma}_i\)</span> are <strong>fixed parameters</strong> ‚Üí must be <strong>estimated</strong>.</li>
<li>
<span class="math inline">\(\mathbf{b}_i\)</span> is a <strong>random variable</strong> ‚Üí must be <strong>predicted</strong> (not estimated). In other words, random thing/variable can‚Äôt be estimated.</li>
</ul>
<p>Thus, we define:</p>
<ul>
<li>
<strong>Estimator of</strong> <span class="math inline">\(\beta\)</span>: <span class="math inline">\(\hat{\beta}\)</span> (fixed effect estimation).</li>
<li>
<strong>Predictor of</strong> <span class="math inline">\(\mathbf{b}_i\)</span>: <span class="math inline">\(\hat{\mathbf{b}}_i\)</span> (random effect prediction).</li>
</ul>
<p>Then:</p>
<ul>
<li>
<p>The <strong>population-level estimate</strong> of <span class="math inline">\(\mathbf{Y}_i\)</span> is:</p>
<p><span class="math display">\[
\hat{\mathbf{Y}}_i = \mathbf{X}_i \hat{\beta}.
\]</span></p>
</li>
<li>
<p>The <strong>subject-specific prediction</strong> is:</p>
<p><span class="math display">\[
\hat{\mathbf{Y}}_i = \mathbf{X}_i \hat{\beta} + \mathbf{Z}_i \hat{\mathbf{b}}_i.
\]</span></p>
</li>
</ul>
<p>For all <span class="math inline">\(N\)</span> subjects, we stack the equations into the Mixed Model Equations <span class="citation">(<a href="references.html#ref-henderson1975best">Henderson 1975</a>)</span>:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z} \mathbf{b} + \epsilon.
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbf{Y} \sim N(\mathbf{X \beta, ZBZ' + \Sigma})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{Y} =
\begin{bmatrix}
\mathbf{y}_1 \\
\vdots \\
\mathbf{y}_N
\end{bmatrix},
\quad
\mathbf{X} =
\begin{bmatrix}
\mathbf{X}_1 \\
\vdots \\
\mathbf{X}_N
\end{bmatrix},
\quad
\mathbf{b} =
\begin{bmatrix}
\mathbf{b}_1 \\
\vdots \\
\mathbf{b}_N
\end{bmatrix},
\quad\mathbf{\epsilon} =    \begin{bmatrix}\mathbf{\epsilon}_1 \\\vdots \\\mathbf{\epsilon}_N\end{bmatrix}.
\]</span></p>
<p>The covariance structure is:</p>
<p><span class="math display">\[
\text{Cov}(\mathbf{b}) = \mathbf{B}, \quad \text{Cov}(\epsilon) = \mathbf{\Sigma}, \quad \text{Cov}(\mathbf{b}, \epsilon) = 0.
\]</span></p>
<p>Expanding <span class="math inline">\(\mathbf{Z}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> as block diagonal matrices:</p>
<p><span class="math display">\[
\mathbf{Z} =
\begin{bmatrix}
\mathbf{Z}_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \mathbf{Z}_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \mathbf{Z}_N
\end{bmatrix},
\quad
\mathbf{B} =
\begin{bmatrix}
\mathbf{D} &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \mathbf{D} &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \mathbf{D}
\end{bmatrix}.
\]</span></p>
<p>The best linear unbiased estimator (BLUE) for <span class="math inline">\(\beta\)</span> and the best linear unbiased predictor (BLUP) for <span class="math inline">\(\mathbf{b}\)</span> are obtained by solving <span class="citation">(<a href="references.html#ref-henderson1975best">Henderson 1975</a>)</span>:</p>
<p><span class="math display">\[
\left[
\begin{array}{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
=
\left[
\begin{array}{cc}
\mathbf{X' \Sigma^{-1} X} &amp; \mathbf{X' \Sigma^{-1} Z} \\
\mathbf{Z' \Sigma^{-1} X} &amp; \mathbf{Z' \Sigma^{-1} Z + B^{-1}}
\end{array}
\right]^{-1}
\left[
\begin{array}{c}
\mathbf{X' \Sigma^{-1} Y} \\
\mathbf{Z' \Sigma^{-1} Y}
\end{array}
\right].
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}\)</span> is the <a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a> estimator for <span class="math inline">\(\beta\)</span>.</li>
<li>
<span class="math inline">\(\hat{\mathbf{b}}\)</span> is the BLUP for <span class="math inline">\(\mathbf{b}\)</span>.</li>
</ul>
<p>If we define:</p>
<p><span class="math display">\[
\mathbf{V} = \mathbf{Z B Z'} + \mathbf{\Sigma}.
\]</span></p>
<p>Then, the solutions to the <strong>Mixed Model Equations</strong> are:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta} &amp;= (\mathbf{X'V^{-1}X})^{-1} \mathbf{X'V^{-1}Y}, \\
\hat{\mathbf{b}} &amp;= \mathbf{BZ'V^{-1}(Y - X\hat{\beta})}.
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}\)</span> is obtained using <a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a>.</li>
<li>
<span class="math inline">\(\hat{\mathbf{b}}\)</span> is a <a href="linear-regression.html#weighted-least-squares">Weighted Least Squares</a> predictor, where weights come from <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(\mathbf{V}\)</span>.</li>
</ul>
<hr>
<p><strong>Properties of the Estimators</strong></p>
<p>For <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p><span class="math display">\[
E(\hat{\beta}) = \beta, \quad \text{Var}(\hat{\beta}) = (\mathbf{X'V^{-1}X})^{-1}.
\]</span></p>
<p>For <span class="math inline">\(\hat{\mathbf{b}}\)</span>:</p>
<p><span class="math display">\[
E(\hat{\mathbf{b}}) = 0.
\]</span></p>
<p>The <strong>variance of the prediction error</strong> (Mean Squared Prediction Error, MSPE) is:</p>
<p><span class="math display">\[
\text{Var}(\hat{\mathbf{b}} - \mathbf{b}) =
\mathbf{B - BZ'V^{-1}ZB + BZ'V^{-1}X (X'V^{-1}X)^{-1} X'V^{-1}B}.
\]</span></p>
<blockquote>
<p>üîπ <strong>Key Insight:</strong><br>
Mean Squared Prediction Error is more meaningful than <span class="math inline">\(\text{Var}(\hat{\mathbf{b}})\)</span> alone, since it accounts for both variance and bias in prediction.</p>
</blockquote>
<hr>
<div id="interpretation-of-the-mixed-model-equations" class="section level3" number="8.2.1">
<h3>
<span class="header-section-number">8.2.1</span> Interpretation of the Mixed Model Equations<a class="anchor" aria-label="anchor" href="#interpretation-of-the-mixed-model-equations"><i class="fas fa-link"></i></a>
</h3>
<p>The system:</p>
<p><span class="math display">\[
\left[
\begin{array}{cc}
\mathbf{X'\Sigma^{-1}X} &amp; \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} &amp; \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right]
\left[
\begin{array}{c}
\beta \\
\mathbf{b}
\end{array}
\right]
=
\left[
\begin{array}{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right].
\]</span></p>
<p>can be understood as:</p>
<ol style="list-style-type: decimal">
<li>Fixed Effects Estimation (<span class="math inline">\(\hat{\beta}\)</span>)
<ul>
<li>Uses <a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a>.</li>
<li>Adjusted for both random effects and correlated errors.</li>
</ul>
</li>
<li>Random Effects Prediction (<span class="math inline">\(\hat{\mathbf{b}}\)</span>)
<ul>
<li>Computed using the BLUP formula.</li>
<li>Shrinks subject-specific estimates toward the population mean.</li>
</ul>
</li>
</ol>
<hr>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="25%">
<col width="41%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th><strong>Component</strong></th>
<th><strong>Equation</strong></th>
<th><strong>Interpretation</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<strong>Fixed effects (</strong><span class="math inline">\(\hat{\beta}\)</span>)</td>
<td><span class="math inline">\((\mathbf{X'V^{-1}X})^{-1} \mathbf{X'V^{-1}Y}\)</span></td>
<td>
<a href="linear-regression.html#generalized-least-squares">Generalized Least Squares</a> estimator</td>
</tr>
<tr class="even">
<td>
<strong>Random effects (</strong><span class="math inline">\(\hat{\mathbf{b}}\)</span>)</td>
<td><span class="math inline">\(\mathbf{BZ'V^{-1}(Y - X\hat{\beta})}\)</span></td>
<td>Best Linear Unbiased Predictor (BLUP)</td>
</tr>
<tr class="odd">
<td>
<strong>Variance of</strong> <span class="math inline">\(\hat{\beta}\)</span>
</td>
<td><span class="math inline">\((\mathbf{X'V^{-1}X})^{-1}\)</span></td>
<td>Accounts for uncertainty in fixed effect estimates</td>
</tr>
<tr class="even">
<td><strong>Variance of prediction error</strong></td>
<td><span class="math inline">\(\mathbf{B - BZ'V^{-1}ZB + BZ'V^{-1}X (X'V^{-1}X)^{-1} X'V^{-1}B}\)</span></td>
<td>Includes both variance and bias in prediction</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="derivation-of-the-mixed-model-equations" class="section level3" number="8.2.2">
<h3>
<span class="header-section-number">8.2.2</span> Derivation of the Mixed Model Equations<a class="anchor" aria-label="anchor" href="#derivation-of-the-mixed-model-equations"><i class="fas fa-link"></i></a>
</h3>
<p>To derive the <strong>Mixed Model Equations</strong>, consider:</p>
<p><span class="math display">\[
\mathbf{\epsilon} = \mathbf{Y} - \mathbf{X \beta} - \mathbf{Z b}.
\]</span></p>
<p>Define:</p>
<ul>
<li>
<span class="math inline">\(T = \sum_{i=1}^N n_i\)</span> ‚Üí <strong>Total number of observations</strong>.</li>
<li>
<span class="math inline">\(Nq\)</span> ‚Üí <strong>Total number of random effects</strong>.</li>
</ul>
<p>The <strong>joint distribution</strong> of <span class="math inline">\((\mathbf{b}, \mathbf{\epsilon})\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
f(\mathbf{b}, \epsilon) &amp;= \frac{1}{(2\pi)^{(T+Nq)/2}}
\left|
\begin{array}{cc}
\mathbf{B} &amp; 0 \\
0 &amp; \mathbf{\Sigma}
\end{array}
\right|^{-1/2} \\
&amp;
\exp
\left(
-\frac{1}{2}
\begin{bmatrix}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{bmatrix}'
\begin{bmatrix}
\mathbf{B} &amp; 0 \\
0 &amp; \mathbf{\Sigma}
\end{bmatrix}^{-1}
\begin{bmatrix}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{bmatrix}
\right).
\end{aligned}
\]</span></p>
<p>Maximizing <span class="math inline">\(f(\mathbf{b},\epsilon)\)</span> with respect to <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\beta\)</span> requires <strong>minimization</strong> of:</p>
<p><span class="math display">\[
\begin{aligned}
Q &amp;= \left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]'
\left[
\begin{array}
{cc}
\mathbf{B} &amp; 0 \\
0 &amp; \mathbf{\Sigma}
\end{array}
\right]^{-1}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right] \\
&amp;=\mathbf{b' B^{-1} b} + (\mathbf{Y - X \beta - Z b})' \mathbf{\Sigma^{-1}} (\mathbf{Y - X \beta - Z b}).
\end{aligned}
\]</span></p>
<p>Setting the derivatives of <span class="math inline">\(Q\)</span> with respect to <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\mathbf{\beta}\)</span> to zero leads to the system of equations:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{X'\Sigma^{-1}X\beta + X'\Sigma^{-1}Zb} &amp;= \mathbf{X'\Sigma^{-1}Y}\\
\mathbf{(Z'\Sigma^{-1}Z + B^{-1})b + Z'\Sigma^{-1}X\beta} &amp;= \mathbf{Z'\Sigma^{-1}Y}
\end{aligned}
\]</span></p>
<p>Rearranging</p>
<p><span class="math display">\[
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}X} &amp; \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} &amp; \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right]
\left[
\begin{array}
{c}
\beta \\
\mathbf{b}
\end{array}
\right]
=
\left[
\begin{array}
{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right]
\]</span></p>
<p>Thus, the solution to the mixed model equations give:</p>
<p><span class="math display">\[
\left[
\begin{array}
{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
=
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}X} &amp; \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} &amp; \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right] ^{-1}
\left[
\begin{array}
{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right]
\]</span></p>
<hr>
</div>
<div id="bayesian-interpretation-of-linear-mixed-models" class="section level3" number="8.2.3">
<h3>
<span class="header-section-number">8.2.3</span> Bayesian Interpretation of Linear Mixed Models<a class="anchor" aria-label="anchor" href="#bayesian-interpretation-of-linear-mixed-models"><i class="fas fa-link"></i></a>
</h3>
<p>In a <strong>Bayesian framework</strong>, the posterior distribution of the <strong>random effects</strong> <span class="math inline">\(\mathbf{b}\)</span> given the observed data <span class="math inline">\(\mathbf{Y}\)</span> is derived using <strong>Bayes‚Äô theorem</strong>:</p>
<p><span class="math display">\[
f(\mathbf{b}| \mathbf{Y}) = \frac{f(\mathbf{Y}|\mathbf{b})f(\mathbf{b})}{\int f(\mathbf{Y}|\mathbf{b})f(\mathbf{b}) d\mathbf{b}}.
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(f(\mathbf{Y}|\mathbf{b})\)</span> is the likelihood function, describing how the data are generated given the random effects.</li>
<li>
<span class="math inline">\(f(\mathbf{b})\)</span> is the prior distribution of the random effects.</li>
<li>The denominator <span class="math inline">\(\int f(\mathbf{Y}|\mathbf{b}) f(\mathbf{b}) d\mathbf{b}\)</span> is the normalizing constant that ensures the posterior integrates to 1.</li>
<li>
<span class="math inline">\(f(\mathbf{b}|\mathbf{Y})\)</span> is the posterior distribution, which updates our belief about <span class="math inline">\(\mathbf{b}\)</span> given the observed data <span class="math inline">\(\mathbf{Y}\)</span>.</li>
</ul>
<hr>
<p>In the <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Model</a>, we assume:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y} | \mathbf{b} &amp;\sim N(\mathbf{X\beta+Zb, \Sigma}), \\
\mathbf{b} &amp;\sim N(\mathbf{0, B}).
\end{aligned}
\]</span></p>
<p>This means:</p>
<ul>
<li>Likelihood: Given <span class="math inline">\(\mathbf{b}\)</span>, the data <span class="math inline">\(\mathbf{Y}\)</span> follows a multivariate normal distribution with mean <span class="math inline">\(\mathbf{X\beta+Zb}\)</span> and covariance <span class="math inline">\(\mathbf{\Sigma}\)</span>.</li>
<li>Prior for <span class="math inline">\(\mathbf{b}\)</span>: The random effects are assumed to follow a multivariate normal distribution with mean 0 and covariance <span class="math inline">\(\mathbf{B}\)</span>.</li>
</ul>
<p>By applying <strong>Bayes‚Äô theorem</strong>, the posterior distribution of <span class="math inline">\(\mathbf{b}\)</span> given <span class="math inline">\(\mathbf{Y}\)</span> is:</p>
<p><span class="math display">\[
\mathbf{b} | \mathbf{Y} \sim N(\mathbf{BZ'V^{-1}(Y - X\beta)}, (\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}).
\]</span></p>
<p>where:</p>
<ul>
<li>Mean: <span class="math inline">\(\mathbf{BZ'V^{-1}(Y - X\beta)}\)</span>
<ul>
<li>This is the BLUP.</li>
<li>It represents the expected value of <span class="math inline">\(\mathbf{b}\)</span> given <span class="math inline">\(\mathbf{Y}\)</span> under squared-error loss.</li>
</ul>
</li>
<li>Covariance: <span class="math inline">\((\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}\)</span>
<ul>
<li>This posterior variance accounts for both prior uncertainty (<span class="math inline">\(\mathbf{B}\)</span>) and data uncertainty (<span class="math inline">\(\mathbf{\Sigma}\)</span>).</li>
</ul>
</li>
</ul>
<p>Thus, the Bayesian posterior mean of <span class="math inline">\(\mathbf{b}\)</span> coincides with the BLUP predictor:</p>
<p><span class="math display">\[
E(\mathbf{b}|\mathbf{Y}) = \mathbf{BZ'V^{-1}(Y-X\beta)}.
\]</span></p>
<hr>
<p><strong>Interpretation of the Posterior Distribution</strong></p>
<ol style="list-style-type: decimal">
<li>Posterior Mean as a Shrinkage Estimator (BLUP)
<ul>
<li>The expectation <span class="math inline">\(E(\mathbf{b}|\mathbf{Y})\)</span> shrinks individual estimates toward the population mean.</li>
<li>Subjects with less data or more variability will have estimates closer to zero.</li>
<li>This is similar to <a href="linear-regression.html#ridge-regression">Ridge Regression</a> in penalized estimation.</li>
</ul>
</li>
<li>Posterior Variance Quantifies Uncertainty
<ul>
<li>The matrix <span class="math inline">\((\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}\)</span> captures the remaining uncertainty in <span class="math inline">\(\mathbf{b}\)</span> after seeing <span class="math inline">\(\mathbf{Y}\)</span>.</li>
<li>If <span class="math inline">\(\mathbf{Z'\Sigma^{-1}Z}\)</span> is large, the data provide strong information about <span class="math inline">\(\mathbf{b}\)</span>, reducing posterior variance.</li>
<li>If <span class="math inline">\(\mathbf{B^{-1}}\)</span> dominates, prior information heavily influences estimates.</li>
</ul>
</li>
<li>Connection to Bayesian Inference
<ul>
<li>The random effects <span class="math inline">\(\mathbf{b}\)</span> follow a Gaussian posterior due to conjugacy.</li>
<li>This is analogous to Bayesian hierarchical models, where random effects are latent variables estimated from data.</li>
</ul>
</li>
</ol>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="15%">
<col width="62%">
<col width="21%">
</colgroup>
<thead><tr class="header">
<th><strong>Step</strong></th>
<th><strong>Equation</strong></th>
<th><strong>Interpretation</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Likelihood</strong></td>
<td><span class="math inline">\(\mathbf{Y} | \mathbf{b} \sim N(\mathbf{X\beta+Zb, \Sigma})\)</span></td>
<td>Data given random effects</td>
</tr>
<tr class="even">
<td><strong>Prior</strong></td>
<td><span class="math inline">\(\mathbf{b} \sim N(\mathbf{0, B})\)</span></td>
<td>Random effects distribution</td>
</tr>
<tr class="odd">
<td><strong>Posterior</strong></td>
<td><span class="math inline">\(\mathbf{b}|\mathbf{Y} \sim N(\mathbf{BZ'V^{-1}(Y-X\beta)}, (\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1})\)</span></td>
<td>Updated belief about <span class="math inline">\(\mathbf{b}\)</span>
</td>
</tr>
<tr class="even">
<td><strong>Posterior Mean (BLUP)</strong></td>
<td><span class="math inline">\(E(\mathbf{b}|\mathbf{Y}) = \mathbf{BZ'V^{-1}(Y-X\beta)}\)</span></td>
<td>Best predictor (squared error loss)</td>
</tr>
<tr class="odd">
<td><strong>Posterior Variance</strong></td>
<td><span class="math inline">\((\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}\)</span></td>
<td>Uncertainty in predictions</td>
</tr>
</tbody>
</table></div>
</div>
<div id="estimating-the-variance-covariance-matrix" class="section level3" number="8.2.4">
<h3>
<span class="header-section-number">8.2.4</span> Estimating the Variance-Covariance Matrix<a class="anchor" aria-label="anchor" href="#estimating-the-variance-covariance-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>If we have an estimate <span class="math inline">\(\tilde{\mathbf{V}}\)</span> for <span class="math inline">\(\mathbf{V}\)</span>, we can estimate the fixed and random effects as:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta} &amp;= \mathbf{(X'\tilde{V}^{-1}X)^{-1}X'\tilde{V}^{-1}Y}, \\
\hat{\mathbf{b}} &amp;= \mathbf{B Z' \tilde{V}^{-1} (Y - X \hat{\beta})}.
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}\)</span> is the estimated fixed effects.</li>
<li>
<span class="math inline">\(\hat{\mathbf{b}}\)</span> is the Empirical Best Linear Unbiased Predictor (EBLUP), also called the Empirical Bayes estimate of <span class="math inline">\(\mathbf{b}\)</span>.</li>
</ul>
<p>Properties of <span class="math inline">\(\hat{\beta}\)</span> and Variance Estimation</p>
<ul>
<li>
<strong>Consistency</strong>: <span class="math inline">\(\hat{\text{Var}}(\hat{\beta})\)</span> is a consistent estimator of <span class="math inline">\(\text{Var}(\hat{\beta})\)</span> if <span class="math inline">\(\tilde{\mathbf{V}}\)</span> is a consistent estimator of <span class="math inline">\(\mathbf{V}\)</span>.</li>
<li>
<strong>Bias</strong> <strong>Issue</strong>: <span class="math inline">\(\hat{\text{Var}}(\hat{\beta})\)</span> is biased because it does not account for the uncertainty in estimating <span class="math inline">\(\mathbf{V}\)</span>.</li>
<li>
<strong>Implication</strong>: This means that <span class="math inline">\(\hat{\text{Var}}(\hat{\beta})\)</span> underestimates the true variability.</li>
</ul>
<p>To estimate <span class="math inline">\(\mathbf{V}\)</span>, several approaches can be used:</p>
<ul>
<li>
<a href="sec-linear-mixed-models.html#sec-maximum-likelihood-estimation-lmm">Maximum Likelihood Estimation</a> (MLE)</li>
<li>
<a href="sec-linear-mixed-models.html#restricted-maximum-likelihood-lmm">Restricted Maximum Likelihood</a> (REML)</li>
<li>
<a href="sec-linear-mixed-models.html#sec-estimated-generalized-least-squares-lmm">Estimated Generalized Least Squares</a> (EGLS)</li>
<li>
<a href="sec-linear-mixed-models.html#sec-bayesian-hierarchical-models-lmm">Bayesian Hierarchical Models</a> (BHM)</li>
</ul>
<hr>
<div id="sec-maximum-likelihood-estimation-lmm" class="section level4" number="8.2.4.1">
<h4>
<span class="header-section-number">8.2.4.1</span> Maximum Likelihood Estimation<a class="anchor" aria-label="anchor" href="#sec-maximum-likelihood-estimation-lmm"><i class="fas fa-link"></i></a>
</h4>
<p>MLE finds parameter estimates by maximizing the likelihood function.</p>
<p>Define a parameter vector <span class="math inline">\(\theta\)</span> that includes all unknown variance components in <span class="math inline">\(\mathbf{\Sigma}\)</span> and <span class="math inline">\(\mathbf{B}\)</span>. Then, we assume:</p>
<p><span class="math display">\[
\mathbf{Y} \sim N(\mathbf{X\beta}, \mathbf{V}(\theta)).
\]</span></p>
<p>The log-likelihood function (ignoring constant terms) is:</p>
<p><span class="math display">\[
-2\log L(\mathbf{y}; \theta, \beta) =
\log |\mathbf{V}(\theta)| + (\mathbf{Y - X\beta})' \mathbf{V}(\theta)^{-1} (\mathbf{Y - X\beta}).
\]</span></p>
<p><strong>Steps for MLE Estimation</strong></p>
<ol style="list-style-type: decimal">
<li>
<p>Estimate <span class="math inline">\(\hat{\beta}\)</span>, assuming <span class="math inline">\(\theta\)</span> is known:</p>
<p><span class="math display">\[
\hat{\beta}_{MLE} = (\mathbf{X'V(\theta)^{-1}X})^{-1} \mathbf{X'V(\theta)^{-1}Y}.
\]</span></p>
</li>
<li>
<p>Obtain <span class="math inline">\(\hat{\theta}_{MLE}\)</span> by maximizing the log-likelihood:</p>
<p><span class="math display">\[
\hat{\theta}_{MLE} = \arg\max_{\theta} -2\log L(\mathbf{y}; \theta, \beta).
\]</span></p>
</li>
<li>
<p>Substitute <span class="math inline">\(\hat{\theta}_{MLE}\)</span> to get updated estimates:</p>
<p><span class="math display">\[
\hat{\beta}_{MLE} = (\mathbf{X'V(\hat{\theta}_{MLE})^{-1}X})^{-1} \mathbf{X'V(\hat{\theta}_{MLE})^{-1}Y}.
\]</span></p>
</li>
<li>
<p>Predict random effects:</p>
<p><span class="math display">\[
\hat{\mathbf{b}}_{MLE} = \mathbf{B}(\hat{\theta}_{MLE}) \mathbf{Z'V}(\hat{\theta}_{MLE})^{-1} (\mathbf{Y - X \hat{\beta}_{MLE}}).
\]</span></p>
</li>
</ol>
<p>Key Observations about MLE</p>
<ul>
<li>MLE tends to underestimate <span class="math inline">\(\theta\)</span> because it does not account for the estimation of fixed effects.</li>
<li>Bias in variance estimates can be corrected using REML.</li>
</ul>
</div>
<div id="restricted-maximum-likelihood-lmm" class="section level4" number="8.2.4.2">
<h4>
<span class="header-section-number">8.2.4.2</span> Restricted Maximum Likelihood<a class="anchor" aria-label="anchor" href="#restricted-maximum-likelihood-lmm"><i class="fas fa-link"></i></a>
</h4>
<p>Restricted Maximum Likelihood (REML) is an estimation method that improves upon <a href="sec-linear-mixed-models.html#sec-maximum-likelihood-estimation-lmm">Maximum Likelihood Estimation</a> by accounting for the loss of degrees of freedom due to the estimation of fixed effects.</p>
<p>Unlike MLE, which estimates both fixed effects (<span class="math inline">\(\beta\)</span>) and variance components (<span class="math inline">\(\theta\)</span>) simultaneously, REML focuses on estimating variance components by considering linear combinations of the data that are independent of the fixed effects.</p>
<p>Consider the <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Model</a>:</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X} \beta + \mathbf{Z} \mathbf{b} + \epsilon,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{y}\)</span>: Response vector of length <span class="math inline">\(N\)</span>
</li>
<li>
<span class="math inline">\(\mathbf{X}\)</span>: Design matrix for fixed effects (<span class="math inline">\(N \times p\)</span>)</li>
<li>
<span class="math inline">\(\beta\)</span>: Fixed effects parameter vector (<span class="math inline">\(p \times 1\)</span>)</li>
<li>
<span class="math inline">\(\mathbf{Z}\)</span>: Design matrix for random effects</li>
<li>
<span class="math inline">\(\mathbf{b} \sim N(\mathbf{0, D})\)</span>: Random effects</li>
<li>
<span class="math inline">\(\epsilon \sim N(\mathbf{0, \Sigma})\)</span>: Residual errors</li>
</ul>
<p>The marginal distribution of <span class="math inline">\(\mathbf{y}\)</span> is:</p>
<p><span class="math display">\[
\mathbf{y} \sim N(\mathbf{X} \beta, \mathbf{V}(\theta)),
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{V}(\theta) = \mathbf{Z D Z'} + \mathbf{\Sigma}.
\]</span></p>
<p>To eliminate dependence on <span class="math inline">\(\beta\)</span>, consider linear transformations of <span class="math inline">\(\mathbf{y}\)</span> that are orthogonal to the fixed effects.</p>
<p>Let <span class="math inline">\(\mathbf{K}\)</span> be a full-rank contrast matrix of size <span class="math inline">\(N \times (N - p)\)</span> such that:</p>
<p><span class="math display">\[
\mathbf{K}' \mathbf{X} = 0.
\]</span></p>
<p>Then, we consider the transformed data:</p>
<p><span class="math display">\[
\mathbf{K}' \mathbf{y} \sim N(\mathbf{0}, \mathbf{K}' \mathbf{V}(\theta) \mathbf{K}).
\]</span></p>
<ul>
<li>This transformation removes <span class="math inline">\(\beta\)</span> from the likelihood, focusing solely on the variance components <span class="math inline">\(\theta\)</span>.</li>
<li>Importantly, the choice of <span class="math inline">\(\mathbf{K}\)</span> does not affect the final REML estimates.</li>
</ul>
<p>The REML log-likelihood is:</p>
<p><span class="math display">\[
-2 \log L_{REML}(\theta) = \log |\mathbf{K}' \mathbf{V}(\theta) \mathbf{K}| + \mathbf{y}' \mathbf{K} (\mathbf{K}' \mathbf{V}(\theta) \mathbf{K})^{-1} \mathbf{K}' \mathbf{y}.
\]</span></p>
<p>An equivalent form of the REML log-likelihood, avoiding explicit use of <span class="math inline">\(\mathbf{K}\)</span>, is:</p>
<p><span class="math display">\[
-2 \log L_{REML}(\theta) = \log |\mathbf{V}(\theta)| + \log |\mathbf{X}' \mathbf{V}(\theta)^{-1} \mathbf{X}| + (\mathbf{y} - \mathbf{X} \hat{\beta})' \mathbf{V}(\theta)^{-1} (\mathbf{y} - \mathbf{X} \hat{\beta}),
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\hat{\beta} = (\mathbf{X}' \mathbf{V}(\theta)^{-1} \mathbf{X})^{-1} \mathbf{X}' \mathbf{V}(\theta)^{-1} \mathbf{y}.
\]</span></p>
<p>This form highlights how REML adjusts for the estimation of fixed effects via the second term <span class="math inline">\(\log |\mathbf{X}' \mathbf{V}^{-1} \mathbf{X}|\)</span>.</p>
<p><strong>Steps for REML Estimation</strong></p>
<ol style="list-style-type: decimal">
<li><p>Transform the data using <span class="math inline">\(\mathbf{K}' \mathbf{y}\)</span> to remove <span class="math inline">\(\beta\)</span> from the likelihood.</p></li>
<li><p>Maximize the restricted likelihood to estimate <span class="math inline">\(\hat{\theta}_{REML}\)</span>.</p></li>
<li>
<p>Estimate fixed effects using:</p>
<p><span class="math display">\[
\hat{\beta}_{REML} = (\mathbf{X}' \mathbf{V}(\hat{\theta}_{REML})^{-1} \mathbf{X})^{-1} \mathbf{X}' \mathbf{V}(\hat{\theta}_{REML})^{-1} \mathbf{y}.
\]</span></p>
</li>
</ol>
<hr>
<p><strong>Properties of REML</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Unbiased Variance Component Estimates</strong>: REML produces unbiased estimates of variance components by accounting for the degrees of freedom used to estimate fixed effects.</li>
<li>
<strong>Invariance to Fixed Effects</strong>: The restricted likelihood is constructed to be independent of the fixed effects <span class="math inline">\(\beta\)</span>.</li>
<li>
<strong>Asymptotic Normality</strong>: REML estimates are consistent and asymptotically normal under standard regularity conditions.</li>
<li>
<strong>Efficiency</strong>: While REML estimates variance components efficiently, it does not maximize the joint likelihood of all parameters, so <span class="math inline">\(\beta\)</span> estimates are slightly less efficient compared to MLE.</li>
</ol>
<hr>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<caption>Comparison of REML and MLE</caption>
<colgroup>
<col width="23%">
<col width="38%">
<col width="37%">
</colgroup>
<thead><tr class="header">
<th>Criterion</th>
<th>MLE</th>
<th>REML</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Approach</td>
<td>Maximizes full likelihood</td>
<td>Maximizes likelihood of contrasts (removes <span class="math inline">\(\beta\)</span>)</td>
</tr>
<tr class="even">
<td>Estimates Fixed Effects?</td>
<td>Yes</td>
<td>No (focuses on variance components)</td>
</tr>
<tr class="odd">
<td>Bias in Variance Estimates</td>
<td>Biased (underestimates variance components)</td>
<td>Unbiased (corrects for loss of degrees of freedom)</td>
</tr>
<tr class="even">
<td>Effect of Changing <span class="math inline">\(\mathbf{X}\)</span>
</td>
<td>Affects variance estimates</td>
<td>No effect on variance estimates</td>
</tr>
<tr class="odd">
<td>Consistency</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Asymptotic Normality</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Efficiency</td>
<td>Efficient under normality</td>
<td>More efficient for variance components</td>
</tr>
<tr class="even">
<td>Model Comparison (AIC/BIC)</td>
<td>Suitable for comparing different fixed-effect models</td>
<td>Not suitable (penalizes fixed effects differently)</td>
</tr>
<tr class="odd">
<td>Performance in Small Samples</td>
<td>Sensitive to small sample bias</td>
<td>More robust to small sample bias</td>
</tr>
<tr class="even">
<td>Handling Outliers</td>
<td>More sensitive</td>
<td>Less sensitive</td>
</tr>
<tr class="odd">
<td>Equivalent to ANOVA?</td>
<td>No</td>
<td>Yes, in balanced designs</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="sec-estimated-generalized-least-squares-lmm" class="section level4" number="8.2.4.3">
<h4>
<span class="header-section-number">8.2.4.3</span> Estimated Generalized Least Squares<a class="anchor" aria-label="anchor" href="#sec-estimated-generalized-least-squares-lmm"><i class="fas fa-link"></i></a>
</h4>
<p>MLE and REML rely on the Gaussian assumption, which may not always hold.<br>
EGLS provides an alternative by relying only on the first two moments (mean and variance).</p>
<p>The LMM framework is:</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i.
\]</span></p>
<p>where:</p>
<ul>
<li>Random effects: <span class="math inline">\(\mathbf{b}_i \sim N(\mathbf{0, D})\)</span>.</li>
<li>Residual errors: <span class="math inline">\(\epsilon_i \sim N(\mathbf{0, \Sigma_i})\)</span>.</li>
<li>Independence assumption: <span class="math inline">\(\text{Cov}(\epsilon_i, \mathbf{b}_i) = 0\)</span>.</li>
</ul>
<p>Thus, the first two moments are:</p>
<p><span class="math display">\[
E(\mathbf{Y}_i) = \mathbf{X}_i \beta, \quad \text{Var}(\mathbf{Y}_i) = \mathbf{V}_i.
\]</span></p>
<p>The EGLS estimator is:</p>
<p><span class="math display">\[
\hat{\beta}_{GLS} = \left\{ \sum_{i=1}^n \mathbf{X'_iV_i(\theta)^{-1}X_i}  \right\}^{-1}
\sum_{i=1}^n \mathbf{X'_iV_i(\theta)^{-1}Y_i}.
\]</span></p>
<p>Writing in matrix form:</p>
<p><span class="math display">\[
\hat{\beta}_{GLS} = \left\{ \mathbf{X'V(\theta)^{-1}X} \right\}^{-1} \mathbf{X'V(\theta)^{-1}Y}.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{V}(\theta)\)</span> is unknown, we estimate it as <span class="math inline">\(\hat{\mathbf{V}}\)</span>, leading to the EGLS estimator:</p>
<p><span class="math display">\[
\hat{\beta}_{EGLS} = \left\{ \mathbf{X'\hat{V}^{-1}X} \right\}^{-1} \mathbf{X'\hat{V}^{-1}Y}.
\]</span></p>
<p>Key Insights about EGLS</p>
<ul>
<li>Computational Simplicity:
<ul>
<li>EGLS does not require iterative maximization of a likelihood function, making it computationally attractive.</li>
</ul>
</li>
<li>Same Form as MLE/REML:
<ul>
<li>The fixed effects estimators for MLE, REML, and EGLS have the same form, differing only in how <span class="math inline">\(\mathbf{V}\)</span> is estimated.</li>
</ul>
</li>
<li>Robust to Non-Gaussian Data:
<ul>
<li>Since it only depends on first and second moments, it can handle cases where MLE and REML struggle with non-normality.</li>
</ul>
</li>
</ul>
<p>When to Use EGLS?</p>
<ul>
<li>When the normality assumption for MLE/REML is questionable.</li>
<li>When <span class="math inline">\(\mathbf{V}\)</span> can be estimated efficiently without requiring complex optimization.</li>
<li>In non-iterative approaches, where computational simplicity is a priority.</li>
</ul>
<hr>
</div>
<div id="sec-bayesian-hierarchical-models-lmm" class="section level4" number="8.2.4.4">
<h4>
<span class="header-section-number">8.2.4.4</span> Bayesian Hierarchical Models<a class="anchor" aria-label="anchor" href="#sec-bayesian-hierarchical-models-lmm"><i class="fas fa-link"></i></a>
</h4>
<p>Bayesian methods offer a fully probabilistic framework to estimate <span class="math inline">\(\mathbf{V}\)</span> by incorporating prior distributions.</p>
<p>The joint distribution can be decomposed hierarchically:</p>
<p><span class="math display">\[
f(A, B, C) = f(A | B, C) f(B | C) f(C).
\]</span></p>
<p>Applying this to LMMs:</p>
<p><span class="math display">\[
\begin{aligned}
f(\mathbf{Y, \beta, b, \theta}) &amp;= f(\mathbf{Y | \beta, b, \theta}) f(\mathbf{b | \theta, \beta}) f(\mathbf{\beta | \theta}) f(\mathbf{\theta}) \\
&amp;= f(\mathbf{Y | \beta, b, \theta}) f(\mathbf{b | \theta}) f(\mathbf{\beta}) f(\mathbf{\theta}).
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>The first equality follows from probability decomposition.</li>
<li>The second equality assumes conditional independence, meaning:
<ul>
<li>Given <span class="math inline">\(\theta\)</span>, no additional information about <span class="math inline">\(\mathbf{b}\)</span> is obtained from knowing <span class="math inline">\(\beta\)</span>.</li>
</ul>
</li>
</ul>
<p>Using Bayes‚Äô theorem, the posterior distribution is:</p>
<p><span class="math display">\[
f(\mathbf{\beta, b, \theta | Y}) \propto f(\mathbf{Y | \beta, b, \theta}) f(\mathbf{b | \theta}) f(\mathbf{\beta}) f(\mathbf{\theta}).
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y | \beta, b, \theta} &amp;\sim N(\mathbf{X\beta + Zb}, \mathbf{\Sigma(\theta)}), \\
\mathbf{b | \theta} &amp;\sim N(\mathbf{0, B(\theta)}).
\end{aligned}
\]</span></p>
<p>To complete the Bayesian model, we specify prior distributions:</p>
<ul>
<li>
<span class="math inline">\(f(\beta)\)</span>: Prior on fixed effects.</li>
<li>
<span class="math inline">\(f(\theta)\)</span>: Prior on variance components.</li>
</ul>
<p>Since analytical solutions are generally unavailable, we use Markov Chain Monte Carlo (MCMC) to sample from the posterior:</p>
<ul>
<li>Gibbs sampling (if conjugate priors are used).</li>
<li>Hamiltonian Monte Carlo (HMC) (for complex models).</li>
</ul>
<p><strong>Advantages</strong></p>
<ol style="list-style-type: decimal">
<li>Accounts for Parameter Uncertainty
<ul>
<li>Unlike MLE/REML, Bayesian methods propagate uncertainty in variance component estimation.</li>
</ul>
</li>
<li>Flexible Model Specification
<ul>
<li>Can incorporate prior knowledge via informative priors.</li>
<li>Extends naturally beyond Gaussian assumptions (e.g., Student-<span class="math inline">\(t\)</span> distributions for heavy-tailed errors).</li>
</ul>
</li>
<li>Robustness in Small Samples
<ul>
<li>Bayesian methods can stabilize variance estimation in small datasets where MLE/REML are unreliable.</li>
</ul>
</li>
</ol>
<p><strong>Challenges</strong></p>
<ol style="list-style-type: decimal">
<li>Computational Complexity
<ul>
<li>Requires MCMC algorithms, which can be computationally expensive.</li>
</ul>
</li>
<li>Convergence Issues
<ul>
<li>MCMC chains must be checked for convergence (e.g., using R-hat diagnostic).</li>
</ul>
</li>
<li>Choice of Priors
<ul>
<li>Poorly chosen priors can bias estimates or slow down convergence.</li>
</ul>
</li>
</ol>
<hr>
<p>Comparison of Estimation Methods for <span class="math inline">\(\mathbf{V}\)</span></p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="33%">
<col width="11%">
<col width="12%">
<col width="14%">
<col width="26%">
</colgroup>
<thead><tr class="header">
<th>Method</th>
<th>Assumptions</th>
<th>Computational Cost</th>
<th>Handles Non-Normality?</th>
<th>Best Use Case</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="sec-linear-mixed-models.html#sec-maximum-likelihood-estimation-lmm">MLE</a></td>
<td>Gaussian errors</td>
<td>High (iterative)</td>
<td>No</td>
<td>Model selection (AIC/BIC)</td>
</tr>
<tr class="even">
<td><a href="sec-linear-mixed-models.html#sec-estimated-generalized-least-squares-lmm">REML</a></td>
<td>Gaussian errors</td>
<td>High (iterative)</td>
<td>No</td>
<td>Variance estimation</td>
</tr>
<tr class="odd">
<td><a href="sec-linear-mixed-models.html#sec-estimated-generalized-least-squares-lmm">EGLS</a></td>
<td>First two moments</td>
<td>Low (non-iterative)</td>
<td>Yes</td>
<td>Large-scale models with correlated errors</td>
</tr>
<tr class="even">
<td>
<a href="sec-linear-mixed-models.html#sec-bayesian-hierarchical-models-lmm">Bayesian</a> (BHM)</td>
<td>Probabilistic</td>
<td>Very High (MCMC)</td>
<td>Yes</td>
<td>Small samples, prior information available</td>
</tr>
</tbody>
</table></div>
</div>
</div>
</div>
<div id="inference-in-linear-mixed-models" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Inference in Linear Mixed Models<a class="anchor" aria-label="anchor" href="#inference-in-linear-mixed-models"><i class="fas fa-link"></i></a>
</h2>
<div id="inference-for-fixed-effects-beta" class="section level3" number="8.3.1">
<h3>
<span class="header-section-number">8.3.1</span> Inference for Fixed Effects (<span class="math inline">\(\beta\)</span>)<a class="anchor" aria-label="anchor" href="#inference-for-fixed-effects-beta"><i class="fas fa-link"></i></a>
</h3>
<p>The goal is to test hypotheses about the fixed effects parameters <span class="math inline">\(\beta\)</span> using various statistical tests:</p>
<ul>
<li><p><a href="sec-linear-mixed-models.html#wald-test-lmm">Wald Test</a></p></li>
<li><p><a href="sec-linear-mixed-models.html#sec-f-test-lmm">F-Test</a></p></li>
<li><p><a href="sec-linear-mixed-models.html#sec-likelihood-ratio-test-lmm">Likelihood Ratio Test</a></p></li>
</ul>
<hr>
<div id="wald-test-lmm" class="section level4" number="8.3.1.1">
<h4>
<span class="header-section-number">8.3.1.1</span> Wald Test<a class="anchor" aria-label="anchor" href="#wald-test-lmm"><i class="fas fa-link"></i></a>
</h4>
<p>The Wald test assesses whether certain linear combinations of fixed effects are equal to specified values.</p>
<p>Given:</p>
<p><span class="math display">\[
\hat{\beta}(\theta) = \left( \mathbf{X}' \mathbf{V}^{-1}(\theta) \mathbf{X} \right)^{-1} \mathbf{X}' \mathbf{V}^{-1}(\theta) \mathbf{Y},
\]</span></p>
<p>and its variance:</p>
<p><span class="math display">\[
\text{Var}(\hat{\beta}(\theta)) = \left( \mathbf{X}' \mathbf{V}^{-1}(\theta) \mathbf{X} \right)^{-1}.
\]</span></p>
<p>In practice, we substitute <span class="math inline">\(\hat{\theta}\)</span> (the estimate of <span class="math inline">\(\theta\)</span>) to obtain:</p>
<ul>
<li>
<p>Hypotheses:</p>
<p><span class="math display">\[
H_0: \mathbf{A \beta} = \mathbf{d}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{A}\)</span> is a contrast matrix specifying linear combinations of <span class="math inline">\(\beta\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{d}\)</span> is a constant vector representing the null hypothesis values.</li>
</ul>
</li>
<li>
<p>Wald Test Statistic:</p>
<p><span class="math display">\[
W = (\mathbf{A} \hat{\beta} - \mathbf{d})' \left[ \mathbf{A} \left( \mathbf{X}' \hat{\mathbf{V}}^{-1} \mathbf{X} \right)^{-1} \mathbf{A}' \right]^{-1} (\mathbf{A} \hat{\beta} - \mathbf{d}).
\]</span></p>
</li>
<li>
<p>Distribution under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
W \sim \chi^2_{\text{rank}(\mathbf{A})}.
\]</span></p>
</li>
</ul>
<p><strong>Caution with Wald Test:</strong></p>
<ul>
<li>Underestimation of Variance:<br>
The Wald test ignores the variability from estimating <span class="math inline">\(\hat{\theta}\)</span>, leading to underestimated standard errors and potentially inflated Type I error rates.</li>
<li>Small Sample Issues:<br>
Less reliable in small samples or when variance components are near boundary values (e.g., variances close to zero).</li>
</ul>
<hr>
</div>
<div id="sec-f-test-lmm" class="section level4" number="8.3.1.2">
<h4>
<span class="header-section-number">8.3.1.2</span> F-Test<a class="anchor" aria-label="anchor" href="#sec-f-test-lmm"><i class="fas fa-link"></i></a>
</h4>
<p>An alternative to the Wald test, the F-test adjusts for the estimation of <span class="math inline">\(\sigma^2\)</span> and provides better performance in small samples.</p>
<p>Assume:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{Y}) = \sigma^2 \mathbf{V}(\theta).
\]</span></p>
<p>The F-statistic is:</p>
<p><span class="math display">\[
F^* = \frac{(\mathbf{A} \hat{\beta} - \mathbf{d})' \left[ \mathbf{A} \left( \mathbf{X}' \hat{\mathbf{V}}^{-1} \mathbf{X} \right)^{-1} \mathbf{A}' \right]^{-1} (\mathbf{A} \hat{\beta} - \mathbf{d})}{\hat{\sigma}^2 \, \text{rank}(\mathbf{A})}.
\]</span></p>
<ul>
<li>
<p>Distribution under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
F^* \sim F_{\text{rank}(\mathbf{A}), \, \text{df}_{\text{denominator}}}.
\]</span></p>
</li>
<li>
<p>Approximating Denominator Degrees of Freedom:</p>
<ul>
<li>Satterthwaite approximation</li>
<li>Kenward-Roger approximation (provides bias-corrected standard errors)</li>
</ul>
</li>
</ul>
<p>F-Test Advantages:</p>
<ul>
<li>More accurate in small samples compared to the Wald test.</li>
<li>Adjusts for variance estimation, reducing bias in hypothesis testing.</li>
</ul>
<p>Wald Test vs.¬†F-Test:</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="25%">
<col width="38%">
<col width="34%">
</colgroup>
<thead><tr class="header">
<th>Criterion</th>
<th>Wald Test</th>
<th>F-Test</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Small Sample Performance</td>
<td>Poor (can inflate Type I error)</td>
<td>Better control of Type I error</td>
</tr>
<tr class="even">
<td>Variance Estimation</td>
<td>Ignores variability in <span class="math inline">\(\hat{\theta}\)</span>
</td>
<td>Adjusts using <span class="math inline">\(\hat{\sigma}^2\)</span>
</td>
</tr>
<tr class="odd">
<td>Reduction to t-test</td>
<td>Yes (for single <span class="math inline">\(\beta\)</span>)</td>
<td>Yes (when rank(<span class="math inline">\(\mathbf{A}\)</span>) = 1)</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="sec-likelihood-ratio-test-lmm" class="section level4" number="8.3.1.3">
<h4>
<span class="header-section-number">8.3.1.3</span> Likelihood Ratio Test<a class="anchor" aria-label="anchor" href="#sec-likelihood-ratio-test-lmm"><i class="fas fa-link"></i></a>
</h4>
<p>The Likelihood Ratio Test (LRT) compares the fit of nested models:</p>
<ul>
<li>
<p>Null Hypothesis:</p>
<p><span class="math display">\[
H_0: \beta \in \Theta_{\beta,0}
\]</span></p>
<p>where <span class="math inline">\(\Theta_{\beta,0}\)</span> is a subset of the full parameter space <span class="math inline">\(\Theta_{\beta}\)</span>.</p>
</li>
<li>
<p>Test Statistic:</p>
<p><span class="math display">\[
-2 \log \lambda = -2 \log \left( \frac{\hat{L}_{ML,0}}{\hat{L}_{ML}} \right),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{L}_{ML,0}\)</span> = Maximized likelihood under <span class="math inline">\(H_0\)</span> (restricted model)</li>
<li>
<span class="math inline">\(\hat{L}_{ML}\)</span> = Maximized likelihood under the alternative (full model)</li>
</ul>
</li>
<li>
<p>Distribution under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
-2 \log \lambda \sim \chi^2_{df}
\]</span></p>
<p>where <span class="math inline">\(df = \dim(\Theta_{\beta}) - \dim(\Theta_{\beta,0})\)</span> (the difference in the number of parameters).</p>
</li>
</ul>
<p><strong>Important Notes:</strong></p>
<ul>
<li>LRT is applicable only for <a href="sec-linear-mixed-models.html#sec-maximum-likelihood-estimation-lmm">ML</a> estimates (not <a href="sec-linear-mixed-models.html#restricted-maximum-likelihood-lmm">REML</a>) when comparing models with different fixed effects.</li>
<li>REML-based LRT can be used for comparing models that differ in random effects (variance components), but not fixed effects.</li>
</ul>
<hr>
</div>
</div>
<div id="inference-for-variance-components-theta" class="section level3" number="8.3.2">
<h3>
<span class="header-section-number">8.3.2</span> Inference for Variance Components (<span class="math inline">\(\theta\)</span>)<a class="anchor" aria-label="anchor" href="#inference-for-variance-components-theta"><i class="fas fa-link"></i></a>
</h3>
<p>For ML and REML estimators:</p>
<p><span class="math display">\[
\hat{\theta} \sim N(\theta, I(\theta)^{-1}),
\]</span></p>
<p>where <span class="math inline">\(I(\theta)\)</span> is the <a href="generalized-linear-models.html#fisher-information-matrix">Fisher Information Matrix</a>.</p>
<p>This normal approximation holds well for large samples, enabling Wald-type tests and confidence intervals.</p>
<hr>
<div id="wald-test-for-variance-components" class="section level4" number="8.3.2.1">
<h4>
<span class="header-section-number">8.3.2.1</span> Wald Test for Variance Components<a class="anchor" aria-label="anchor" href="#wald-test-for-variance-components"><i class="fas fa-link"></i></a>
</h4>
<p>The Wald test for variance components follows the same structure as for <a href="sec-linear-mixed-models.html#wald-test-lmm">fixed effects</a>:</p>
<ul>
<li>
<p>Test Statistic:</p>
<p><span class="math display">\[
W = \frac{(\hat{\theta} - \theta_0)^2}{\widehat{\text{Var}}(\hat{\theta})}.
\]</span></p>
</li>
<li>
<p>Distribution under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
W \sim \chi^2_1.
\]</span></p>
</li>
</ul>
<p>Limitations of Wald Test for Variance Components:</p>
<ul>
<li>Boundary Issues: The normal approximation fails when the true variance component is near zero (boundary of the parameter space).</li>
<li>Less reliable for variance parameters than for covariance parameters.</li>
</ul>
<hr>
</div>
<div id="likelihood-ratio-test-for-variance-components" class="section level4" number="8.3.2.2">
<h4>
<span class="header-section-number">8.3.2.2</span> Likelihood Ratio Test for Variance Components<a class="anchor" aria-label="anchor" href="#likelihood-ratio-test-for-variance-components"><i class="fas fa-link"></i></a>
</h4>
<p>LRT can also be applied to variance components:</p>
<ul>
<li>
<p>Test Statistic:</p>
<p><span class="math display">\[
-2 \log \lambda = -2 \log \left( \frac{\hat{L}_{REML,0}}{\hat{L}_{REML}} \right).
\]</span></p>
</li>
<li>
<p>Distribution under <span class="math inline">\(H_0\)</span>:</p>
<ul>
<li>Not always <span class="math inline">\(\chi^2\)</span>-distributed when variance components are on the boundary (e.g., testing if <span class="math inline">\(\sigma^2 = 0\)</span>).</li>
<li>May require mixture distributions or adjusted critical values.</li>
</ul>
</li>
</ul>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="12%">
<col width="23%">
<col width="26%">
<col width="35%">
</colgroup>
<thead><tr class="header">
<th>Test</th>
<th>Best For</th>
<th>Strengths</th>
<th>Limitations</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Wald Test</td>
<td>Fixed effects (<span class="math inline">\(\beta\)</span>)</td>
<td>Simple, widely used</td>
<td>Underestimates variance, biased in small samples</td>
</tr>
<tr class="even">
<td>F-Test</td>
<td>Fixed effects (<span class="math inline">\(\beta\)</span>)</td>
<td>Better in small samples, adjusts df</td>
<td>Requires approximation for degrees of freedom</td>
</tr>
<tr class="odd">
<td>LRT (ML)</td>
<td>Fixed effects, nested models</td>
<td>Powerful, widely used</td>
<td>Not valid for REML with fixed effects</td>
</tr>
<tr class="even">
<td>LRT (REML)</td>
<td>Variance components</td>
<td>Robust for random effects</td>
<td>Boundary issues when variances are near zero</td>
</tr>
<tr class="odd">
<td>Wald (Variance)</td>
<td>Variance components (<span class="math inline">\(\theta\)</span>)</td>
<td>Simple extension of Wald test</td>
<td>Fails near parameter space boundaries</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
</div>
<div id="information-criteria-for-model-selection" class="section level2" number="8.4">
<h2>
<span class="header-section-number">8.4</span> Information Criteria for Model Selection<a class="anchor" aria-label="anchor" href="#information-criteria-for-model-selection"><i class="fas fa-link"></i></a>
</h2>
<p>Information Criteria are statistical tools used to compare competing models by balancing model fit (likelihood) with model complexity (number of parameters).<br>
They help in identifying the most parsimonious model that adequately explains the data without overfitting.</p>
<p>The three most commonly used criteria are:</p>
<ul>
<li>
<a href="sec-linear-mixed-models.html#sec-akaike-information-criterion-lmm">Akaike Information Criterion</a> (AIC)</li>
<li>
<a href="sec-linear-mixed-models.html#sec-corrected-aic-lmm">Corrected Akaike Information Criterion</a> (AICc)</li>
<li>
<a href="sec-linear-mixed-models.html#sec-bayesian-information-criterion-lmm">Bayesian Information Criterion</a> (BIC)</li>
</ul>
<hr>
<div id="sec-akaike-information-criterion-lmm" class="section level3" number="8.4.1">
<h3>
<span class="header-section-number">8.4.1</span> Akaike Information Criterion<a class="anchor" aria-label="anchor" href="#sec-akaike-information-criterion-lmm"><i class="fas fa-link"></i></a>
</h3>
<p>The Akaike Information Criterion is derived from the Kullback-Leibler divergence, which measures the difference between the true data-generating process and the fitted model.</p>
<p><strong>AIC Formula</strong>:</p>
<p><span class="math display">\[
AIC = -2 \, l(\hat{\theta}, \hat{\beta}) + 2q
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(l(\hat{\theta}, \hat{\beta})\)</span>: The maximized log-likelihood of the model, evaluated at the estimates <span class="math inline">\(\hat{\theta}\)</span> (variance components) and <span class="math inline">\(\hat{\beta}\)</span> (fixed effects).</li>
<li>
<span class="math inline">\(q\)</span>: The effective number of parameters, including:
<ul>
<li>The number of fixed effects.</li>
<li>The number of variance-covariance parameters (random effects).</li>
<li>Excludes parameters constrained to boundary values (e.g., variances estimated as zero).</li>
</ul>
</li>
</ul>
<p><strong>Key Points About AIC</strong></p>
<ul>
<li>Model Selection Rule:
<ul>
<li>Lower AIC indicates a better model.</li>
<li>Occasionally, software may report AIC as <span class="math inline">\(l - q\)</span>, in which case higher AIC is better (rare).</li>
</ul>
</li>
<li>Comparing Random Effects Models:
<ul>
<li>Not recommended when comparing models with different random effects because it‚Äôs difficult to accurately count the effective number of parameters.</li>
</ul>
</li>
<li>Sample Size Considerations:
<ul>
<li>Requires large sample sizes for reliable comparisons.</li>
<li>In small samples, AIC tends to favor more complex models due to insufficient penalty for model complexity.</li>
</ul>
</li>
<li>Potential Bias:
<ul>
<li>Can be negatively biased (i.e., favoring overly complex models) when the sample size is small relative to the number of parameters.</li>
</ul>
</li>
</ul>
<p>When to Use AIC:</p>
<ul>
<li>Comparing models with the same random effects structure but different fixed effects.</li>
<li>Selecting covariance structures in mixed models when the sample size is large.</li>
</ul>
<hr>
</div>
<div id="sec-corrected-aic-lmm" class="section level3" number="8.4.2">
<h3>
<span class="header-section-number">8.4.2</span> Corrected AIC<a class="anchor" aria-label="anchor" href="#sec-corrected-aic-lmm"><i class="fas fa-link"></i></a>
</h3>
<p>The Corrected AIC (AICc) addresses the bias in AIC for small sample sizes. It was developed by <span class="citation">(<a href="references.html#ref-hurvich1989regression">Hurvich and Tsai 1989</a>)</span>.</p>
<p><strong>AICc Formula</strong>:</p>
<p><span class="math display">\[
AICc = AIC + \frac{2q(q + 1)}{n - q - 1}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span>: The sample size.</li>
<li>
<span class="math inline">\(q\)</span>: The number of estimated parameters.</li>
</ul>
<p><strong>Key Points About AICc</strong></p>
<ul>
<li>Small Sample Correction:
<ul>
<li>Provides a stronger penalty for model complexity when the sample size is small.</li>
</ul>
</li>
<li>Applicability:
<ul>
<li>Valid when comparing models with fixed covariance structures.</li>
<li>Not recommended for models with general covariance structures due to difficulties in bias correction.</li>
</ul>
</li>
<li>Model Selection Rule:
<ul>
<li>Lower AICc indicates a better model.</li>
</ul>
</li>
</ul>
<p>When to Use AICc:</p>
<ul>
<li>Small sample sizes (<span class="math inline">\(n/q\)</span> ratio is low).</li>
<li>Models with fixed random effects or simple covariance structures.</li>
</ul>
<hr>
</div>
<div id="sec-bayesian-information-criterion-lmm" class="section level3" number="8.4.3">
<h3>
<span class="header-section-number">8.4.3</span> Bayesian Information Criterion<a class="anchor" aria-label="anchor" href="#sec-bayesian-information-criterion-lmm"><i class="fas fa-link"></i></a>
</h3>
<p>The Bayesian Information Criterion is derived from a Bayesian framework and incorporates a stronger penalty for model complexity compared to AIC.</p>
<p><strong>BIC Formula</strong></p>
<p><span class="math display">\[
BIC = -2 \, l(\hat{\theta}, \hat{\beta}) + q \log(n)
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span>: The number of observations.</li>
<li>
<span class="math inline">\(q\)</span>: The number of effective parameters.</li>
<li>
<span class="math inline">\(l(\hat{\theta}, \hat{\beta})\)</span>: The maximized log-likelihood.</li>
</ul>
<p><strong>Key Points About BIC</strong></p>
<ul>
<li>Model Selection Rule:
<ul>
<li>Lower BIC indicates a better model.</li>
</ul>
</li>
<li>Stronger Penalty:
<ul>
<li>The penalty term <span class="math inline">\(q \log(n)\)</span> grows with sample size, leading BIC to favor simpler models more than AIC.</li>
</ul>
</li>
<li>Applicability to MLE and REML:
<ul>
<li>BIC can be used with both MLE and REML, but:
<ul>
<li>Use MLE when comparing models with different fixed effects.</li>
<li>Use REML when comparing models with different random effects (same fixed effects).</li>
</ul>
</li>
</ul>
</li>
<li>Consistency:
<ul>
<li>BIC is consistent, meaning that as the sample size increases, it will select the true model with probability 1 (if the true model is among the candidates).</li>
</ul>
</li>
</ul>
<p>When to Use BIC:</p>
<ul>
<li>Large sample sizes where model simplicity is prioritized.</li>
<li>Model selection for hypothesis testing (due to its connection to Bayesian inference).</li>
</ul>
<hr>
<p><strong>Comparison of AIC, AICc, and BIC</strong></p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="9%">
<col width="21%">
<col width="17%">
<col width="33%">
<col width="16%">
</colgroup>
<thead><tr class="header">
<th><strong>Criterion</strong></th>
<th><strong>Formula</strong></th>
<th><strong>Penalty Term</strong></th>
<th><strong>Best For</strong></th>
<th><strong>Model Selection Rule</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>AIC</strong></td>
<td><span class="math inline">\(-2l + 2q\)</span></td>
<td><span class="math inline">\(2q\)</span></td>
<td>General model comparison (large <span class="math inline">\(n\)</span>)</td>
<td>Lower is better</td>
</tr>
<tr class="even">
<td><strong>AICc</strong></td>
<td><span class="math inline">\(AIC + \frac{2q(q+1)}{n - q - 1}\)</span></td>
<td>Adjusted for small samples</td>
<td>Small sample sizes, simple random effects</td>
<td>Lower is better</td>
</tr>
<tr class="odd">
<td><strong>BIC</strong></td>
<td><span class="math inline">\(-2l + q \log(n)\)</span></td>
<td><span class="math inline">\(q \log(n)\)</span></td>
<td>Large samples, model selection in hypothesis testing</td>
<td>Lower is better</td>
</tr>
</tbody>
</table></div>
<p><strong>Key Takeaways</strong></p>
<ol style="list-style-type: decimal">
<li>AIC is suitable for large datasets and general model comparisons but may favor overly complex models in small samples.</li>
<li>AICc corrects AIC‚Äôs bias in small sample sizes.</li>
<li>BIC favors simpler models, especially as the sample size increases, making it suitable for hypothesis testing and situations where parsimony is essential.</li>
<li>Use MLE for comparing models with different fixed effects, and REML when comparing models with different random effects (same fixed effects).</li>
<li>When comparing random effects structures, AIC and BIC may not be reliable due to difficulty in counting effective parameters accurately.</li>
</ol>
<hr>
</div>
<div id="practical-example-with-linear-mixed-models" class="section level3" number="8.4.4">
<h3>
<span class="header-section-number">8.4.4</span> Practical Example with Linear Mixed Models<a class="anchor" aria-label="anchor" href="#practical-example-with-linear-mixed-models"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the <a href="sec-linear-mixed-models.html#sec-linear-mixed-models">Linear Mixed Model</a>:</p>
<p><span class="math display">\[
Y_{ik} =
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + b_{2i}) t_{ij} + \epsilon_{ij} &amp; L \\
\beta_0 + b_{1i} + (\beta_2 + b_{2i}) t_{ij} + \epsilon_{ij} &amp; H \\
\beta_0 + b_{1i} + (\beta_3 + b_{2i}) t_{ij} + \epsilon_{ij} &amp; C
\end{cases}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(i = 1, \dots, N\)</span> (subjects)</li>
<li>
<span class="math inline">\(j = 1, \dots, n_i\)</span> (repeated measures at time <span class="math inline">\(t_{ij}\)</span>)</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y}_i | b_i &amp;\sim N(\mathbf{X}_i \beta + \mathbf{1} b_i, \sigma^2 \mathbf{I}) \\
b_i &amp;\sim N(0, d_{11})
\end{aligned}
\]</span></p>
<p>We aim to <strong>estimate</strong>:</p>
<ul>
<li>
<strong>Fixed effects</strong>: <span class="math inline">\(\beta\)</span>
</li>
<li>
<strong>Variance components</strong>: <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(d_{11}\)</span>
</li>
<li>
<strong>Random effects</strong>: Predict <span class="math inline">\(b_i\)</span>
</li>
</ul>
<p>When comparing models (e.g., different random slopes or covariance structures), we can compute:</p>
<ul>
<li>
<strong>AIC</strong>: Penalizes model complexity with <span class="math inline">\(2q\)</span>.</li>
<li>
<strong>BIC</strong>: Stronger penalty via <span class="math inline">\(q \log(n)\)</span>, favoring simpler models.</li>
<li>
<strong>AICc</strong>: Adjusted AIC for small sample sizes.</li>
</ul>
<div class="sourceCode" id="cb313"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Practical Example with Linear Mixed Models in R</span></span>
<span></span>
<span></span>
<span><span class="co"># Load required libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/lme4/lme4/">lme4</a></span><span class="op">)</span>     <span class="co"># For fitting linear mixed-effects models</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">MuMIn</span><span class="op">)</span>    <span class="co"># For calculating AICc</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span>    <span class="co"># For data manipulation</span></span>
<span></span>
<span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate Data</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">50</span>             <span class="co"># Number of subjects</span></span>
<span><span class="va">n_i</span> <span class="op">&lt;-</span> <span class="fl">5</span>            <span class="co"># Number of repeated measures per subject</span></span>
<span><span class="va">t_ij</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n_i</span>, <span class="va">N</span><span class="op">)</span>  <span class="co"># Time points</span></span>
<span></span>
<span><span class="co"># Treatment groups (L, H, C)</span></span>
<span><span class="va">treatment</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"L"</span>, <span class="st">"H"</span>, <span class="st">"C"</span><span class="op">)</span>, length.out <span class="op">=</span> <span class="va">N</span><span class="op">)</span></span>
<span><span class="va">group</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">treatment</span>, each <span class="op">=</span> <span class="va">n_i</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate random effects</span></span>
<span><span class="va">b1_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>    <span class="co"># Random intercepts</span></span>
<span><span class="va">b2_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>    <span class="co"># Random slopes</span></span>
<span></span>
<span><span class="co"># Fixed effects</span></span>
<span><span class="va">beta_0</span> <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span><span class="va">beta_1</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">beta_2</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">beta_3</span> <span class="op">&lt;-</span> <span class="fl">1.5</span></span>
<span></span>
<span><span class="co"># Generate response variable Y based on the specified model</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">N</span> <span class="op">*</span> <span class="va">n_i</span><span class="op">)</span></span>
<span><span class="va">subject_id</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, each <span class="op">=</span> <span class="va">n_i</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n_i</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">idx</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">*</span> <span class="va">n_i</span> <span class="op">+</span> <span class="va">j</span></span>
<span>        <span class="va">time</span> <span class="op">&lt;-</span> <span class="va">t_ij</span><span class="op">[</span><span class="va">idx</span><span class="op">]</span></span>
<span>        </span>
<span>        <span class="co"># Treatment-specific model</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">group</span><span class="op">[</span><span class="va">idx</span><span class="op">]</span> <span class="op">==</span> <span class="st">"L"</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">Y</span><span class="op">[</span><span class="va">idx</span><span class="op">]</span> <span class="op">&lt;-</span></span>
<span>                <span class="va">beta_0</span> <span class="op">+</span> <span class="va">b1_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="op">(</span><span class="va">beta_1</span> <span class="op">+</span> <span class="va">b2_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="va">time</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span>        <span class="op">}</span> <span class="kw">else</span> <span class="kw">if</span> <span class="op">(</span><span class="va">group</span><span class="op">[</span><span class="va">idx</span><span class="op">]</span> <span class="op">==</span> <span class="st">"H"</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">Y</span><span class="op">[</span><span class="va">idx</span><span class="op">]</span> <span class="op">&lt;-</span></span>
<span>                <span class="va">beta_0</span> <span class="op">+</span> <span class="va">b1_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="op">(</span><span class="va">beta_2</span> <span class="op">+</span> <span class="va">b2_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="va">time</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span>        <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>            <span class="va">Y</span><span class="op">[</span><span class="va">idx</span><span class="op">]</span> <span class="op">&lt;-</span></span>
<span>                <span class="va">beta_0</span> <span class="op">+</span> <span class="va">b1_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="op">(</span><span class="va">beta_3</span> <span class="op">+</span> <span class="va">b2_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="va">time</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span>        <span class="op">}</span></span>
<span>    <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Combine into a data frame</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Y <span class="op">=</span> <span class="va">Y</span>,</span>
<span>    time <span class="op">=</span> <span class="va">t_ij</span>,</span>
<span>    group <span class="op">=</span> <span class="va">group</span>,</span>
<span>    subject <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">subject_id</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit Linear Mixed Models</span></span>
<span><span class="co"># Model 1: Random Intercepts Only</span></span>
<span><span class="va">model1</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">time</span> <span class="op">*</span> <span class="va">group</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">subject</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data</span>, REML <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model 2: Random Intercepts and Random Slopes</span></span>
<span><span class="va">model2</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">time</span> <span class="op">*</span> <span class="va">group</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="va">time</span> <span class="op">|</span></span>
<span>                                 <span class="va">subject</span><span class="op">)</span>,</span>
<span>         data <span class="op">=</span> <span class="va">data</span>,</span>
<span>         REML <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model 3: Simpler Model (No Interaction)</span></span>
<span><span class="va">model3</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">time</span> <span class="op">+</span> <span class="va">group</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">subject</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data</span>, REML <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract Information Criteria</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Model <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>        <span class="st">"Random Intercepts"</span>,</span>
<span>        <span class="st">"Random Intercepts + Slopes"</span>,</span>
<span>        <span class="st">"No Interaction"</span></span>
<span>    <span class="op">)</span>,</span>
<span>    AIC <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">model1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">model2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">model3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    BIC <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">BIC</a></span><span class="op">(</span><span class="va">model1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">BIC</a></span><span class="op">(</span><span class="va">model2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">BIC</a></span><span class="op">(</span><span class="va">model3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    AICc <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/MuMIn/man/AICc.html">AICc</a></span><span class="op">(</span><span class="va">model1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/pkg/MuMIn/man/AICc.html">AICc</a></span><span class="op">(</span><span class="va">model2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/pkg/MuMIn/man/AICc.html">AICc</a></span><span class="op">(</span><span class="va">model3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display the results</span></span>
<span><span class="fu"><a href="https://alexkowa.github.io/EnvStats/reference/print.html">print</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span></span>
<span><span class="co">#&gt;                        Model       AIC      BIC      AICc</span></span>
<span><span class="co">#&gt; 1          Random Intercepts 1129.2064 1157.378 1129.8039</span></span>
<span><span class="co">#&gt; 2 Random Intercepts + Slopes  974.5514 1009.766  975.4719</span></span>
<span><span class="co">#&gt; 3             No Interaction 1164.2797 1185.408 1164.6253</span></span></code></pre></div>
<p><strong>Interpretation of Results:</strong></p>
<ul>
<li><p>Model 2 (with random intercepts and slopes) has the lowest AIC, BIC, and AICc, indicating the best fit among the models.</p></li>
<li><p>Model 1 (random intercepts only) performs worse, suggesting that allowing random slopes improves model fit.</p></li>
<li><p>Model 3 (simpler fixed effects without interaction) has the highest AIC/BIC/AICc, indicating poor fit compared to Models 1 and 2.</p></li>
</ul>
<p><strong>Model Selection Criteria:</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="19%">
<col width="20%">
<col width="59%">
</colgroup>
<thead><tr class="header">
<th><strong>Criterion</strong></th>
<th><strong>Best Model</strong></th>
<th><strong>Reason</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>AIC</strong></td>
<td>Model 2</td>
<td>Best trade-off between fit and complexity</td>
</tr>
<tr class="even">
<td><strong>BIC</strong></td>
<td>Model 2</td>
<td>Stronger penalty for complexity, still favored</td>
</tr>
<tr class="odd">
<td><strong>AICc</strong></td>
<td>Model 2</td>
<td>Adjusted for small samples, Model 2 still best</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="split-plot-designs" class="section level2" number="8.5">
<h2>
<span class="header-section-number">8.5</span> Split-Plot Designs<a class="anchor" aria-label="anchor" href="#split-plot-designs"><i class="fas fa-link"></i></a>
</h2>
<p>Split-plot designs are commonly used in experimental settings where there are <strong>two or more factors</strong>, and at least one of them requires <strong>larger experimental units</strong> compared to the other(s). This situation often arises in agricultural, industrial, and business experiments where certain treatments are harder or more expensive to apply.</p>
<p><strong>Key Characteristics</strong></p>
<ul>
<li>Two factors with different experimental unit requirements:
<ul>
<li>Factor A (Whole-plot factor): Requires large experimental units (e.g., different fields, production batches).</li>
<li>Factor B (Sub-plot factor): Can be applied to smaller units within the larger experimental units (e.g., plots within fields, products within batches).</li>
</ul>
</li>
<li>Blocking: The experiment is typically divided into blocks (or replicates) to account for variability. However, unlike <a href="sec-analysis-of-variance-anova.html#sec-randomized-block-designs">Randomized Block Designs</a>, the randomization process in split-plot designs occurs at two levels:
<ul>
<li>Whole-plot randomization: Factor A is randomized across large units within each block.</li>
<li>Sub-plot randomization: Factor B is randomized within each whole plot.</li>
</ul>
</li>
</ul>
<hr>
<div id="example-setup" class="section level3" number="8.5.1">
<h3>
<span class="header-section-number">8.5.1</span> Example Setup<a class="anchor" aria-label="anchor" href="#example-setup"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<strong>Factor A:</strong> 3 levels (applied to large units).</li>
<li>
<strong>Factor B:</strong> 2 levels (applied within the large units).</li>
<li>
<strong>4 Blocks (replicates):</strong> Each containing all combinations of A and B.</li>
</ul>
<p>Unlike in <a href="sec-analysis-of-variance-anova.html#sec-randomized-block-designs">Randomized Block Designs</a>, the randomization of Factor A is <strong>restricted</strong> due to the larger unit size. Factor A is applied <strong>once per block</strong>, while Factor B can be applied <strong>multiple times within each block</strong>.</p>
<hr>
</div>
<div id="statistical-model-for-split-plot-designs" class="section level3" number="8.5.2">
<h3>
<span class="header-section-number">8.5.2</span> Statistical Model for Split-Plot Designs<a class="anchor" aria-label="anchor" href="#statistical-model-for-split-plot-designs"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>When Factor A is the Primary Focus (Whole-Plot Analysis)</li>
</ol>
<p><span class="math display">\[
Y_{ij} = \mu + \rho_i + \alpha_j + e_{ij}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Y_{ij}\)</span> = Response for the <span class="math inline">\(j\)</span>-th level of factor A in the <span class="math inline">\(i\)</span>-th block.</p></li>
<li><p><span class="math inline">\(\mu\)</span> = Overall mean.</p></li>
<li><p><span class="math inline">\(\rho_i\)</span> = Random effect of the <span class="math inline">\(i\)</span>-th block (<span class="math inline">\(\rho_i \sim N(0, \sigma^2_{\rho})\)</span>).</p></li>
<li><p><span class="math inline">\(\alpha_j\)</span> = Fixed effect of factor A (main effect).</p></li>
<li><p><span class="math inline">\(e_{ij} \sim N(0, \sigma^2_{e})\)</span> = Whole-plot error (random), representing the variability within blocks due to factor A.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>When Factor B is the Primary Focus (Sub-Plot Analysis)</li>
</ol>
<p><span class="math display">\[
Y_{ijk} = \mu + \phi_{ij} + \beta_k + \epsilon_{ijk}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Y_{ijk}\)</span> = Response for the <span class="math inline">\(k\)</span>-th level of factor B within the <span class="math inline">\(j\)</span>-th level of factor A and <span class="math inline">\(i\)</span>-th block.</p></li>
<li><p><span class="math inline">\(\phi_{ij}\)</span> = Combined effect of block and factor A: <span class="math display">\[
  \phi_{ij} = \rho_i + \alpha_j + e_{ij}
  \]</span></p></li>
<li><p><span class="math inline">\(\beta_k\)</span> = Fixed effect of factor B (main effect).</p></li>
<li><p><span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2_{\epsilon})\)</span> = Sub-plot error (random), capturing variability within whole plots.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Full Split-Plot Model (Including Interaction)</li>
</ol>
<p><span class="math display">\[
Y_{ijk} = \mu + \rho_i + \alpha_j + e_{ij} + \beta_k + (\alpha \beta)_{jk} + \epsilon_{ijk}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(i\)</span> = Block (replication). - <span class="math inline">\(j\)</span> = Level of factor A (whole-plot factor).</p></li>
<li><p><span class="math inline">\(k\)</span> = Level of factor B (sub-plot factor).</p></li>
<li><p><span class="math inline">\(\mu\)</span> = Overall mean.</p></li>
<li><p><span class="math inline">\(\rho_i\)</span> = Random effect of the <span class="math inline">\(i\)</span>-th block.</p></li>
<li><p><span class="math inline">\(\alpha_j\)</span> = Fixed main effect of factor A.</p></li>
<li><p><span class="math inline">\(e_{ij} \sim N(0, \sigma^2_{e})\)</span> = Whole-plot error (random).</p></li>
<li><p><span class="math inline">\(\beta_k\)</span> = Fixed main effect of factor B.</p></li>
<li><p><span class="math inline">\((\alpha \beta)_{jk}\)</span> = Fixed interaction effect between factors A and B.</p></li>
<li><p><span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2_{\epsilon})\)</span> = Sub-plot error (random).</p></li>
</ul>
<hr>
</div>
<div id="approaches-to-analyzing-split-plot-designs" class="section level3" number="8.5.3">
<h3>
<span class="header-section-number">8.5.3</span> Approaches to Analyzing Split-Plot Designs<a class="anchor" aria-label="anchor" href="#approaches-to-analyzing-split-plot-designs"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<a href="sec-analysis-of-variance-anova.html#sec-analysis-of-variance-anova">ANOVA</a> Perspective</li>
</ol>
<p>Whole-Plot Comparisons:</p>
<ul>
<li>Factor A vs.¬†Whole-Plot Error:<br>
Compare the variation due to factor A (<span class="math inline">\(\alpha_j\)</span>) against the whole-plot error (<span class="math inline">\(e_{ij}\)</span>).</li>
<li>Blocks vs.¬†Whole-Plot Error:<br>
Compare the variation due to blocks (<span class="math inline">\(\rho_i\)</span>) against the whole-plot error (<span class="math inline">\(e_{ij}\)</span>).</li>
</ul>
<p>Sub-Plot Comparisons:</p>
<ul>
<li>Factor B vs.¬†Sub-Plot Error:<br>
Compare the variation due to factor B (<span class="math inline">\(\beta_k\)</span>) against the sub-plot error (<span class="math inline">\(\epsilon_{ijk}\)</span>).</li>
<li>Interaction (A √ó B) vs.¬†Sub-Plot Error:<br>
Compare the interaction effect (<span class="math inline">\((\alpha \beta)_{jk}\)</span>) against the sub-plot error (<span class="math inline">\(\epsilon_{ijk}\)</span>).</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Mixed Model Perspective</strong></li>
</ol>
<p>A more flexible approach is to treat split-plot designs using mixed-effects models, which can handle both fixed and random effects explicitly:</p>
<p><span class="math display">\[
\mathbf{Y = X \beta + Zb + \epsilon}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{Y}\)</span> = Vector of observed responses.</p></li>
<li><p><span class="math inline">\(\mathbf{X}\)</span> = Design matrix for fixed effects (e.g., factors A, B, and their interaction).</p></li>
<li><p><span class="math inline">\(\boldsymbol{\beta}\)</span> = Vector of fixed-effect coefficients (e.g., <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\alpha_j\)</span>, <span class="math inline">\(\beta_k\)</span>, <span class="math inline">\((\alpha \beta)_{jk}\)</span>).</p></li>
<li><p><span class="math inline">\(\mathbf{Z}\)</span> = Design matrix for random effects (e.g., blocks and whole-plot errors).</p></li>
<li><p><span class="math inline">\(\mathbf{b}\)</span> = Vector of random-effect coefficients (e.g., <span class="math inline">\(\rho_i\)</span>, <span class="math inline">\(e_{ij}\)</span>).</p></li>
<li><p><span class="math inline">\(\boldsymbol{\epsilon}\)</span> = Vector of residuals (sub-plot error).</p></li>
</ul>
<p>Mixed models are particularly useful when:</p>
<ul>
<li><p>There are <strong>unbalanced designs</strong> (missing data).</p></li>
<li><p>You need to account for <strong>complex correlation structures</strong> within the data.</p></li>
</ul>
</div>
<div id="application-split-plot-design" class="section level3" number="8.5.4">
<h3>
<span class="header-section-number">8.5.4</span> Application: Split-Plot Design<a class="anchor" aria-label="anchor" href="#application-split-plot-design"><i class="fas fa-link"></i></a>
</h3>
<p>Consider an agricultural experiment designed to study the effects of irrigation and crop variety on yield. This scenario is well-suited for a split-plot design because irrigation treatments are applied to large plots (fields), while different crop varieties are planted within these plots.</p>
<div id="model-specification" class="section level4" number="8.5.4.1">
<h4>
<span class="header-section-number">8.5.4.1</span> Model Specification<a class="anchor" aria-label="anchor" href="#model-specification"><i class="fas fa-link"></i></a>
</h4>
<p>The linear mixed-effects model is defined as:</p>
<p><span class="math display">\[
y_{ijk} = \mu + i_i + v_j + (iv)_{ij} + f_k + \epsilon_{ijk}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(y_{ijk}\)</span> = Observed yield for the <span class="math inline">\(i\)</span>-th irrigation, <span class="math inline">\(j\)</span>-th variety, in the <span class="math inline">\(k\)</span>-th field.</p></li>
<li><p><span class="math inline">\(\mu\)</span> = Overall mean yield.</p></li>
<li><p><span class="math inline">\(i_i\)</span> = Fixed effect of the <span class="math inline">\(i\)</span>-th irrigation level.</p></li>
<li><p><span class="math inline">\(v_j\)</span> = Fixed effect of the <span class="math inline">\(j\)</span>-th crop variety.</p></li>
<li><p><span class="math inline">\((iv)_{ij}\)</span> = Interaction effect between irrigation and variety (fixed).</p></li>
<li><p><span class="math inline">\(f_k \sim N(0, \sigma^2_f)\)</span> = Random effect of field (captures variability between fields).</p></li>
<li><p><span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2_\epsilon)\)</span> = Residual error.</p></li>
</ul>
<p><strong>Note:</strong><br>
Since each variety-field combination is observed only once, we cannot model a random interaction between variety and field.</p>
<hr>
</div>
<div id="data-exploration" class="section level4" number="8.5.4.2">
<h4>
<span class="header-section-number">8.5.4.2</span> Data Exploration<a class="anchor" aria-label="anchor" href="#data-exploration"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb314"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">irrigation</span>, package <span class="op">=</span> <span class="st">"faraway"</span><span class="op">)</span>  <span class="co"># Load the dataset</span></span>
<span></span>
<span><span class="co"># Summary statistics and preview</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">irrigation</span><span class="op">)</span></span>
<span><span class="co">#&gt;      field   irrigation variety     yield      </span></span>
<span><span class="co">#&gt;  f1     :2   i1:4       v1:8    Min.   :34.80  </span></span>
<span><span class="co">#&gt;  f2     :2   i2:4       v2:8    1st Qu.:37.60  </span></span>
<span><span class="co">#&gt;  f3     :2   i3:4               Median :40.15  </span></span>
<span><span class="co">#&gt;  f4     :2   i4:4               Mean   :40.23  </span></span>
<span><span class="co">#&gt;  f5     :2                      3rd Qu.:42.73  </span></span>
<span><span class="co">#&gt;  f6     :2                      Max.   :47.60  </span></span>
<span><span class="co">#&gt;  (Other):4</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">irrigation</span>, <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt;   field irrigation variety yield</span></span>
<span><span class="co">#&gt; 1    f1         i1      v1  35.4</span></span>
<span><span class="co">#&gt; 2    f1         i1      v2  37.9</span></span>
<span><span class="co">#&gt; 3    f2         i2      v1  36.7</span></span>
<span><span class="co">#&gt; 4    f2         i2      v2  38.2</span></span>
<span></span>
<span><span class="co"># Exploratory plot: Yield by field, colored by variety and shaped by irrigation</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">irrigation</span>,</span>
<span>       <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>           x     <span class="op">=</span> <span class="va">field</span>,</span>
<span>           y     <span class="op">=</span> <span class="va">yield</span>,</span>
<span>           shape <span class="op">=</span> <span class="va">irrigation</span>,</span>
<span>           color <span class="op">=</span> <span class="va">variety</span></span>
<span>       <span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Yield by Field, Irrigation, and Variety"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Field"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Yield"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="08-linear-mixed-models_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>This plot helps visualize how yield varies across fields, under different irrigation treatments, and for different varieties.</p>
</div>
<div id="fitting-the-initial-mixed-effects-model" class="section level4" number="8.5.4.3">
<h4>
<span class="header-section-number">8.5.4.3</span> Fitting the Initial Mixed-Effects Model<a class="anchor" aria-label="anchor" href="#fitting-the-initial-mixed-effects-model"><i class="fas fa-link"></i></a>
</h4>
<p>We fit a mixed-effects model where:</p>
<ul>
<li><p>Irrigation and variety (and their interaction) are fixed effects.</p></li>
<li><p>Field is modeled as a random effect to account for variability between fields.</p></li>
</ul>
<div class="sourceCode" id="cb315"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/runehaubo/lmerTestR">lmerTest</a></span><span class="op">)</span>  <span class="co"># Provides p-values for lmer models</span></span>
<span></span>
<span><span class="co"># Full model with interaction term</span></span>
<span><span class="va">sp_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lmerTest/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">yield</span> <span class="op">~</span> <span class="va">irrigation</span> <span class="op">*</span> <span class="va">variety</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">field</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">irrigation</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">sp_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML. t-tests use Satterthwaite's method [</span></span>
<span><span class="co">#&gt; lmerModLmerTest]</span></span>
<span><span class="co">#&gt; Formula: yield ~ irrigation * variety + (1 | field)</span></span>
<span><span class="co">#&gt;    Data: irrigation</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 45.4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -0.7448 -0.5509  0.0000  0.5509  0.7448 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  field    (Intercept) 16.200   4.025   </span></span>
<span><span class="co">#&gt;  Residual              2.107   1.452   </span></span>
<span><span class="co">#&gt; Number of obs: 16, groups:  field, 8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;                        Estimate Std. Error     df t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)              38.500      3.026  4.487  12.725 0.000109 ***</span></span>
<span><span class="co">#&gt; irrigationi2              1.200      4.279  4.487   0.280 0.791591    </span></span>
<span><span class="co">#&gt; irrigationi3              0.700      4.279  4.487   0.164 0.877156    </span></span>
<span><span class="co">#&gt; irrigationi4              3.500      4.279  4.487   0.818 0.454584    </span></span>
<span><span class="co">#&gt; varietyv2                 0.600      1.452  4.000   0.413 0.700582    </span></span>
<span><span class="co">#&gt; irrigationi2:varietyv2   -0.400      2.053  4.000  -0.195 0.855020    </span></span>
<span><span class="co">#&gt; irrigationi3:varietyv2   -0.200      2.053  4.000  -0.097 0.927082    </span></span>
<span><span class="co">#&gt; irrigationi4:varietyv2    1.200      2.053  4.000   0.584 0.590265    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation of Fixed Effects:</span></span>
<span><span class="co">#&gt;             (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2</span></span>
<span><span class="co">#&gt; irrigation2 -0.707                                          </span></span>
<span><span class="co">#&gt; irrigation3 -0.707  0.500                                   </span></span>
<span><span class="co">#&gt; irrigation4 -0.707  0.500  0.500                            </span></span>
<span><span class="co">#&gt; varietyv2   -0.240  0.170  0.170  0.170                     </span></span>
<span><span class="co">#&gt; irrgtn2:vr2  0.170 -0.240 -0.120 -0.120 -0.707              </span></span>
<span><span class="co">#&gt; irrgtn3:vr2  0.170 -0.120 -0.240 -0.120 -0.707  0.500       </span></span>
<span><span class="co">#&gt; irrgtn4:vr2  0.170 -0.120 -0.120 -0.240 -0.707  0.500  0.500</span></span>
<span></span>
<span><span class="co"># ANOVA table using Kenward-Roger approximation for accurate p-values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">sp_model</span>, ddf <span class="op">=</span> <span class="st">"Kenward-Roger"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Type III Analysis of Variance Table with Kenward-Roger's method</span></span>
<span><span class="co">#&gt;                    Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F)</span></span>
<span><span class="co">#&gt; irrigation         2.4545 0.81818     3     4  0.3882 0.7685</span></span>
<span><span class="co">#&gt; variety            2.2500 2.25000     1     4  1.0676 0.3599</span></span>
<span><span class="co">#&gt; irrigation:variety 1.5500 0.51667     3     4  0.2452 0.8612</span></span></code></pre></div>
<ul>
<li>
<p>Check the p-value of the interaction term (<code>irrigation:variety</code>).</p>
<ul>
<li>If insignificant, this suggests no strong evidence of an interaction effect, and we may simplify the model by removing it.</li>
</ul>
</li>
</ul>
</div>
<div id="model-simplification-testing-for-additivity" class="section level4" number="8.5.4.4">
<h4>
<span class="header-section-number">8.5.4.4</span> Model Simplification: Testing for Additivity<a class="anchor" aria-label="anchor" href="#model-simplification-testing-for-additivity"><i class="fas fa-link"></i></a>
</h4>
<p>We compare the <strong>full model</strong> (with interaction) to an <strong>additive model</strong> (without interaction):</p>
<div class="sourceCode" id="cb316"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/lme4/lme4/">lme4</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Additive model (no interaction)</span></span>
<span><span class="va">sp_model_additive</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">yield</span> <span class="op">~</span> <span class="va">irrigation</span> <span class="op">+</span> <span class="va">variety</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">field</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">irrigation</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Likelihood ratio test comparing the two models</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">sp_model_additive</span>, <span class="va">sp_model</span>, ddf <span class="op">=</span> <span class="st">"Kenward-Roger"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Data: irrigation</span></span>
<span><span class="co">#&gt; Models:</span></span>
<span><span class="co">#&gt; sp_model_additive: yield ~ irrigation + variety + (1 | field)</span></span>
<span><span class="co">#&gt; sp_model: yield ~ irrigation * variety + (1 | field)</span></span>
<span><span class="co">#&gt;                   npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)</span></span>
<span><span class="co">#&gt; sp_model_additive    7 83.959 89.368 -34.980    69.959                     </span></span>
<span><span class="co">#&gt; sp_model            10 88.609 96.335 -34.305    68.609 1.3503  3     0.7172</span></span></code></pre></div>
<ul>
<li>
<p><strong>Hypotheses:</strong></p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: The <strong>additive model</strong> (without interaction) fits the data adequately.</p></li>
<li><p><span class="math inline">\(H_a\)</span>: The <strong>interaction model</strong> provides a significantly better fit.</p></li>
</ul>
</li>
<li>
<p><strong>Interpretation:</strong></p>
<ul>
<li><p>If the p-value is insignificant, we fail to reject <span class="math inline">\(H_0\)</span>, meaning the simpler additive model is sufficient.</p></li>
<li><p>Check AIC and BIC: Lower values indicate a better-fitting model, supporting the use of the additive model if consistent with the hypothesis test.</p></li>
</ul>
</li>
</ul>
</div>
<div id="assessing-the-random-effect-exact-restricted-likelihood-ratio-test" class="section level4" number="8.5.4.5">
<h4>
<span class="header-section-number">8.5.4.5</span> Assessing the Random Effect: Exact Restricted Likelihood Ratio Test<a class="anchor" aria-label="anchor" href="#assessing-the-random-effect-exact-restricted-likelihood-ratio-test"><i class="fas fa-link"></i></a>
</h4>
<p>To verify whether the random field effect is necessary, we conduct an exact RLRT:</p>
<ul>
<li>
<p>Hypotheses:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\sigma^2_f = 0\)</span> (no variability between fields; random effect is unnecessary).</p></li>
<li><p><span class="math inline">\(H_a\)</span>: <span class="math inline">\(\sigma^2_f &gt; 0\)</span> (random field effect is significant).</p></li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb317"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/fabian-s/RLRsim">RLRsim</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># RLRT for the random effect of field</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/RLRsim/man/exactRLRT.html">exactRLRT</a></span><span class="op">(</span><span class="va">sp_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  simulated finite sample distribution of RLRT.</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt;  (p-value based on 10000 simulated values)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  </span></span>
<span><span class="co">#&gt; RLRT = 6.1118, p-value = 0.01</span></span></code></pre></div>
<p><strong>Interpretation:</strong></p>
<ul>
<li><p>If the p-value is significant, we reject <span class="math inline">\(H_0\)</span>, confirming that the random field effect is essential.</p></li>
<li><p>A significant random effect implies substantial variability between fields that must be accounted for in the model.</p></li>
</ul>
<hr>
</div>
</div>
</div>
<div id="repeated-measures-in-mixed-models" class="section level2" number="8.6">
<h2>
<span class="header-section-number">8.6</span> Repeated Measures in Mixed Models<a class="anchor" aria-label="anchor" href="#repeated-measures-in-mixed-models"><i class="fas fa-link"></i></a>
</h2>
<p>Repeated measures data arise when <strong>multiple observations</strong> are collected from the <strong>same subjects</strong> over time or under different conditions. This introduces <strong>correlation</strong> between observations from the same subject, which must be accounted for in the statistical model.</p>
<p>Mixed-effects models are particularly effective for repeated measures because they allow us to model both <strong>fixed effects</strong> (e.g., treatment, time) and <strong>random effects</strong> (e.g., subject-specific variability).</p>
<p>The general form of a mixed-effects model for repeated measures is:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \delta_{i(k)} + \epsilon_{ijk}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Y_{ijk}\)</span> = Response for the <span class="math inline">\(i\)</span>-th group, <span class="math inline">\(j\)</span>-th time point, and <span class="math inline">\(k\)</span>-th subject.</p></li>
<li><p><span class="math inline">\(\mu\)</span> = Overall mean.</p></li>
<li><p><span class="math inline">\(\alpha_i\)</span> = Fixed effect of the <span class="math inline">\(i\)</span>-th group (e.g., treatment group).</p></li>
<li><p><span class="math inline">\(\beta_j\)</span> = Fixed effect of the <span class="math inline">\(j\)</span>-th time point (repeated measure effect).</p></li>
<li><p><span class="math inline">\((\alpha \beta)_{ij}\)</span> = Interaction effect between group and time (fixed).</p></li>
<li><p><span class="math inline">\(\delta_{i(k)} \sim N(0, \sigma^2_\delta)\)</span> = Random effect of the <span class="math inline">\(k\)</span>-th subject within the <span class="math inline">\(i\)</span>-th group (captures subject-specific deviations).</p></li>
<li><p><span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2)\)</span> = Residual error (independent across observations).</p></li>
</ul>
<p>Here, <span class="math inline">\(i = 1, \dots, n_A\)</span> (number of groups), <span class="math inline">\(j = 1, \dots, n_B\)</span> (number of repeated measures), and <span class="math inline">\(k = 1, \dots, n_i\)</span> (number of subjects in group <span class="math inline">\(i\)</span>).</p>
<hr>
<p>The <strong>variance-covariance matrix</strong> of the repeated observations for the <span class="math inline">\(k\)</span>-th subject in the <span class="math inline">\(i\)</span>-th group is given by:</p>
<p><span class="math display">\[
\mathbf{Y}_{ik} =
\begin{pmatrix}
Y_{i1k} \\
Y_{i2k} \\
\vdots \\
Y_{in_Bk}
\end{pmatrix}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Compound Symmetry (CS) Structure</li>
</ol>
<p>Under the <strong>compound symmetry</strong> assumption (common in <a href="sec-linear-mixed-models.html#sec-random-intercepts-model-lmm">random-intercepts models</a>), the covariance matrix is:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_{\text{subject}} =
\begin{pmatrix}
\sigma^2_\delta + \sigma^2 &amp; \sigma^2_\delta &amp; \cdots &amp; \sigma^2_\delta \\
\sigma^2_\delta &amp; \sigma^2_\delta + \sigma^2 &amp; \cdots &amp; \sigma^2_\delta \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma^2_\delta &amp; \sigma^2_\delta &amp; \cdots &amp; \sigma^2_\delta + \sigma^2
\end{pmatrix}
\]</span></p>
<p>This matrix can be rewritten as:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_{\text{subject}} = (\sigma^2_\delta + \sigma^2)
\begin{pmatrix}
1 &amp; \rho &amp; \cdots &amp; \rho \\
\rho &amp; 1 &amp; \cdots &amp; \rho \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\rho &amp; \rho &amp; \cdots &amp; 1
\end{pmatrix}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\sigma^2_\delta\)</span> = Variance due to subject-specific random effects.</p></li>
<li><p><span class="math inline">\(\sigma^2\)</span> = Residual variance.</p></li>
<li><p><span class="math inline">\(\rho = \frac{\sigma^2_\delta}{\sigma^2_\delta + \sigma^2}\)</span> = Intra-class correlation coefficient (ICC).</p></li>
</ul>
<p>Key Points:</p>
<ul>
<li><p>Compound Symmetry Structure is the product of a scalar and a correlation matrix.</p></li>
<li><p>The correlation between any two repeated measures from the same subject is constant (<span class="math inline">\(\rho\)</span>).</p></li>
<li><p>This structure assumes equal correlation across time points, which may not hold if measurements are collected over time.</p></li>
</ul>
<p><em>Refer to <a href="sec-linear-mixed-models.html#sec-random-intercepts-model-lmm">Random-Intercepts Model</a> for a detailed discussion of compound symmetry.</em></p>
<hr>
<ol start="2" style="list-style-type: decimal">
<li>Autoregressive (AR(1)) Structure</li>
</ol>
<p>If repeated measures are collected over time, it may be more appropriate to assume an autoregressive correlation structure, where correlations decay as the time gap increases.</p>
<p>The AR(1) variance-covariance matrix is:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_{\text{subject}} =
\sigma^2
\begin{pmatrix}
1 &amp; \rho &amp; \rho^2 &amp; \cdots &amp; \rho^{n_B-1} \\
\rho &amp; 1 &amp; \rho &amp; \cdots &amp; \rho^{n_B-2} \\
\rho^2 &amp; \rho &amp; 1 &amp; \cdots &amp; \rho^{n_B-3} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\rho^{n_B-1} &amp; \rho^{n_B-2} &amp; \rho^{n_B-3} &amp; \cdots &amp; 1
\end{pmatrix}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\sigma^2\)</span> = Residual variance.</p></li>
<li><p><span class="math inline">\(\rho\)</span> = Autoregressive parameter (<span class="math inline">\(|\rho| &lt; 1\)</span>), representing the correlation between consecutive time points.</p></li>
</ul>
<p><strong>Key Characteristics</strong>:</p>
<ul>
<li><p>Correlations decrease exponentially as the time lag increases.</p></li>
<li><p>Appropriate for longitudinal data where temporal proximity influences correlation.</p></li>
</ul>
<hr>
<p>In matrix notation, the mixed model can be written as:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{Y}\)</span> = Vector of observed responses.</p></li>
<li><p><span class="math inline">\(\mathbf{X}\)</span> = Design matrix for fixed effects (e.g., group, time, interaction).</p></li>
<li><p><span class="math inline">\(\boldsymbol{\beta}\)</span> = Vector of fixed-effect coefficients.</p></li>
<li><p><span class="math inline">\(\boldsymbol{\epsilon} \sim N(0, \sigma^2 \mathbf{\Sigma})\)</span> = Random error vector.</p></li>
<li>
<p><span class="math inline">\(\mathbf{\Sigma}\)</span> = Variance-covariance matrix of residuals:</p>
<ul>
<li>Block diagonal structure if the covariance structure is identical for each subject.</li>
<li>Within each block (subject), the structure can be compound symmetry, AR(1), or another suitable structure depending on the data.</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Choosing the Right Covariance Structure</strong></p>
<ul>
<li>Compound Symmetry:
<ul>
<li>Suitable when correlations are constant across repeated measures (e.g., in randomized controlled trials).</li>
<li>Simple and interpretable but may be too restrictive for longitudinal data.</li>
</ul>
</li>
<li>Autoregressive (AR(1)):
<ul>
<li>Best when measurements are taken over equally spaced time intervals and correlations decay over time.</li>
<li>Assumes stronger correlation for adjacent time points.</li>
</ul>
</li>
<li>Unstructured (UN):
<ul>
<li>Allows different variances and covariances for each time point.</li>
<li>Provides maximum flexibility but requires more parameters and larger sample sizes.</li>
</ul>
</li>
</ul>
<p>Model selection criteria (AIC, BIC, likelihood ratio tests) can help determine the most appropriate covariance structure.</p>
<hr>
</div>
<div id="unbalanced-or-unequally-spaced-data" class="section level2" number="8.7">
<h2>
<span class="header-section-number">8.7</span> Unbalanced or Unequally Spaced Data<a class="anchor" aria-label="anchor" href="#unbalanced-or-unequally-spaced-data"><i class="fas fa-link"></i></a>
</h2>
<p>In many real-world applications, data are <strong>unbalanced</strong> (i.e., different numbers of observations per subject) or <strong>unequally spaced</strong> over time. This is common in longitudinal studies, clinical trials, and business analytics where subjects may be observed at irregular intervals or miss certain time points.</p>
<p>Mixed-effects models are flexible enough to handle such data structures, especially when we carefully model the <strong>variance-covariance structure</strong> of repeated measurements.</p>
<p>Consider the following mixed-effects model:</p>
<p><span class="math display">\[
Y_{ikt} = \beta_0 + \beta_{0i} + \beta_{1} t + \beta_{1i} t + \beta_{2} t^2 + \beta_{2i} t^2 + \epsilon_{ikt}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Y_{ikt}\)</span> = Response for the <span class="math inline">\(k\)</span>-th subject in the <span class="math inline">\(i\)</span>-th group at time <span class="math inline">\(t\)</span>.</p></li>
<li><p><span class="math inline">\(i = 1, 2\)</span> = Groups (e.g., treatment vs.¬†control).</p></li>
<li><p><span class="math inline">\(k = 1, \dots, n_i\)</span> = Individuals within group <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(t = (t_1, t_2, t_3, t_4)\)</span> = Time points (which may be unequally spaced).</p></li>
</ul>
<p>Model Components:</p>
<ul>
<li>
<strong>Fixed Effects:</strong>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span> = Overall intercept (baseline).</li>
<li>
<span class="math inline">\(\beta_1\)</span> = Common linear time trend.</li>
<li>
<span class="math inline">\(\beta_2\)</span> = Common quadratic time trend.</li>
</ul>
</li>
<li>
<strong>Random Effects:</strong>
<ul>
<li>
<span class="math inline">\(\beta_{0i}\)</span> = Random intercept for group <span class="math inline">\(i\)</span> (captures group-specific baseline variation).</li>
<li>
<span class="math inline">\(\beta_{1i}\)</span> = Random slope for time in group <span class="math inline">\(i\)</span> (captures group-specific linear trends).</li>
<li>
<span class="math inline">\(\beta_{2i}\)</span> = Random quadratic effect for group <span class="math inline">\(i\)</span> (captures group-specific curvature over time).</li>
</ul>
</li>
<li>
<strong>Residual Error:</strong>
<ul>
<li>
<span class="math inline">\(\epsilon_{ikt} \sim N(0, \sigma^2)\)</span> = Measurement error, assumed independent of the random effects.</li>
</ul>
</li>
</ul>
<hr>
<div id="variance-covariance-structure-power-model" class="section level3" number="8.7.1">
<h3>
<span class="header-section-number">8.7.1</span> Variance-Covariance Structure: Power Model<a class="anchor" aria-label="anchor" href="#variance-covariance-structure-power-model"><i class="fas fa-link"></i></a>
</h3>
<p>Since observations are taken at unequally spaced time points, we cannot rely on simple structures like compound symmetry or AR(1). Instead, we use a power covariance model, which allows the correlation to depend on the distance between time points.</p>
<p>The variance-covariance matrix of the repeated measurements for subject <span class="math inline">\(k\)</span> in group <span class="math inline">\(i\)</span> is:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_{ik} = \sigma^2
\begin{pmatrix}
1 &amp; \rho^{|t_2 - t_1|} &amp; \rho^{|t_3 - t_1|} &amp; \rho^{|t_4 - t_1|} \\
\rho^{|t_2 - t_1|} &amp; 1 &amp; \rho^{|t_3 - t_2|} &amp; \rho^{|t_4 - t_2|} \\
\rho^{|t_3 - t_1|} &amp; \rho^{|t_3 - t_2|} &amp; 1 &amp; \rho^{|t_4 - t_3|} \\
\rho^{|t_4 - t_1|} &amp; \rho^{|t_4 - t_2|} &amp; \rho^{|t_4 - t_3|} &amp; 1
\end{pmatrix}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\sigma^2\)</span> = Residual variance.</p></li>
<li><p><span class="math inline">\(\rho\)</span> = Correlation parameter (<span class="math inline">\(0 &lt; |\rho| &lt; 1\)</span>), controlling how correlation decays with increasing time gaps.</p></li>
<li><p><span class="math inline">\(|t_j - t_i|\)</span> = Absolute time difference between measurements at times <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span>.</p></li>
</ul>
<p><strong>Key Characteristics</strong>:</p>
<ul>
<li>The correlation between observations decreases as the time difference increases, similar to AR(1), but flexible enough to handle unequal time intervals.</li>
<li>This structure is sometimes referred to as a continuous-time autoregressive model or power covariance model.</li>
</ul>
<hr>
<p>After fitting the full model, we can evaluate whether all terms are necessary, focusing on the random effects:</p>
<ul>
<li><p><span class="math inline">\(\beta_{0i}\)</span> (Random Intercepts):<br>
Is there significant baseline variability between groups?</p></li>
<li><p><span class="math inline">\(\beta_{1i}\)</span> (Random Slopes for Time):<br>
Do groups exhibit different linear trends over time?</p></li>
<li><p><span class="math inline">\(\beta_{2i}\)</span> (Random Quadratic Terms):<br>
Is there group-specific curvature in the response over time?</p></li>
</ul>
<p>Model Comparison Approach:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Fit the Full Model:</strong><br>
Includes all random effects.</p></li>
<li><p><strong>Fit Reduced Models:</strong><br>
Systematically remove random effects (e.g., quadratic terms) to create simpler models.</p></li>
<li>
<p><strong>Compare Models Using:</strong></p>
<ul>
<li><p><strong>Likelihood Ratio Tests (LRT):</strong><br>
Test whether the more complex model significantly improves fit.</p></li>
<li><p><strong>Information Criteria (AIC, BIC):</strong><br>
Lower values indicate a better trade-off between fit and complexity.</p></li>
</ul>
</li>
<li>
<p><strong>Assess Random Effects:</strong></p>
<ul>
<li>Use the <strong>exactRLRT</strong> test to determine if random effects are significant.</li>
<li>Check variance estimates: if the variance of a random effect is near zero, it may not be necessary.</li>
</ul>
</li>
</ol>
<hr>
<p>In matrix notation, the model can be written as:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{b} + \boldsymbol{\epsilon}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{Y}\)</span> = Vector of observed responses.</p></li>
<li><p><span class="math inline">\(\mathbf{X}\)</span> = Design matrix for fixed effects (intercept, time, time¬≤, and group interactions).</p></li>
<li><p><span class="math inline">\(\boldsymbol{\beta}\)</span> = Vector of fixed-effect coefficients.</p></li>
<li><p><span class="math inline">\(\mathbf{Z}\)</span> = Design matrix for random effects (random intercepts, slopes, etc.).</p></li>
<li><p><span class="math inline">\(\mathbf{b} \sim N(0, \mathbf{G})\)</span> = Vector of random effects with covariance matrix <span class="math inline">\(\mathbf{G}\)</span>.</p></li>
<li><p><span class="math inline">\(\boldsymbol{\epsilon} \sim N(0, \mathbf{R})\)</span> = Vector of residual errors with covariance matrix <span class="math inline">\(\mathbf{R}\)</span>, where <span class="math inline">\(\mathbf{R}\)</span> follows the power covariance structure.</p></li>
</ul>
<hr>
</div>
</div>
<div id="application-mixed-models-in-practice" class="section level2" number="8.8">
<h2>
<span class="header-section-number">8.8</span> Application: Mixed Models in Practice<a class="anchor" aria-label="anchor" href="#application-mixed-models-in-practice"><i class="fas fa-link"></i></a>
</h2>
<p>Several R packages are available for fitting mixed-effects models, each with unique strengths:</p>
<ul>
<li>
<code>nlme</code>
<ul>
<li>Supports nested and crossed random effects.</li>
<li>Flexible for complex covariance structures.</li>
<li>Less intuitive syntax compared to <code>lme4</code>.</li>
</ul>
</li>
<li>
<code>lme4</code>
<ul>
<li>Computationally efficient and widely used.</li>
<li>User-friendly formula syntax.</li>
<li>Can handle non-normal responses (e.g., GLMMs).</li>
<li>For detailed documentation, refer to <span class="citation">D. Bates et al. (<a href="references.html#ref-bates2015fitting">2015</a>)</span>.</li>
</ul>
</li>
<li>Others:
<ul>
<li>Bayesian Mixed Models: <code>MCMCglmm</code>, <code>brms</code>.</li>
<li>Genetics/Plant Breeding: <code>ASReml</code>.</li>
</ul>
</li>
</ul>
<hr>
<div id="example-1-pulp-brightness-analysis" class="section level3" number="8.8.1">
<h3>
<span class="header-section-number">8.8.1</span> Example 1: Pulp Brightness Analysis<a class="anchor" aria-label="anchor" href="#example-1-pulp-brightness-analysis"><i class="fas fa-link"></i></a>
</h3>
<div id="model-specification-1" class="section level4" number="8.8.1.1">
<h4>
<span class="header-section-number">8.8.1.1</span> Model Specification<a class="anchor" aria-label="anchor" href="#model-specification-1"><i class="fas fa-link"></i></a>
</h4>
<p>We start with a <a href="sec-linear-mixed-models.html#sec-random-intercepts-model-lmm">random-intercepts model</a> for pulp brightness:</p>
<p><span class="math display">\[
y_{ij} = \mu + \alpha_i + \epsilon_{ij}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(i = 1, \dots, a\)</span> = Groups for random effect <span class="math inline">\(\alpha_i\)</span>.</p></li>
<li><p><span class="math inline">\(j = 1, \dots, n\)</span> = Observations per group.</p></li>
<li><p><span class="math inline">\(\mu\)</span> = Overall mean brightness (fixed effect).</p></li>
<li><p><span class="math inline">\(\alpha_i \sim N(0, \sigma^2_\alpha)\)</span> = Group-specific random effect.</p></li>
<li><p><span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2_\epsilon)\)</span> = Residual error.</p></li>
</ul>
<p>This implies a compound symmetry structure, where the intraclass correlation coefficient is:</p>
<p><span class="math display">\[
\rho = \frac{\sigma^2_\alpha}{\sigma^2_\alpha + \sigma^2_\epsilon}
\]</span></p>
<ul>
<li>If <span class="math inline">\(\sigma^2_\alpha \to 0\)</span>: Low correlation within groups (<span class="math inline">\(\rho \to 0\)</span>) (i.e., when factor <span class="math inline">\(a\)</span> doesn‚Äôt explain much variation).</li>
<li>If <span class="math inline">\(\sigma^2_\alpha \to \infty\)</span>: High correlation within groups (<span class="math inline">\(\rho \to 1\)</span>).</li>
</ul>
</div>
<div id="data-exploration-1" class="section level4" number="8.8.1.2">
<h4>
<span class="header-section-number">8.8.1.2</span> Data Exploration<a class="anchor" aria-label="anchor" href="#data-exploration-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb318"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">pulp</span>, package <span class="op">=</span> <span class="st">"faraway"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualize brightness by operator</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">pulp</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">operator</span>, y <span class="op">=</span> <span class="va">bright</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"lightblue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Pulp Brightness by Operator"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Operator"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Brightness"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="08-linear-mixed-models_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb319"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Group-wise summary</span></span>
<span><span class="va">pulp</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">operator</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>average_brightness <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">bright</span><span class="op">)</span>, .groups <span class="op">=</span> <span class="st">'drop'</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 4 √ó 2</span></span>
<span><span class="co">#&gt;   operator average_brightness</span></span>
<span><span class="co">#&gt;   &lt;fct&gt;                 &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 a                      60.2</span></span>
<span><span class="co">#&gt; 2 b                      60.1</span></span>
<span><span class="co">#&gt; 3 c                      60.6</span></span>
<span><span class="co">#&gt; 4 d                      60.7</span></span></code></pre></div>
</div>
<div id="fitting-the-mixed-model-with-lme4" class="section level4" number="8.8.1.3">
<h4>
<span class="header-section-number">8.8.1.3</span> Fitting the Mixed Model with <code>lme4</code><a class="anchor" aria-label="anchor" href="#fitting-the-mixed-model-with-lme4"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb320"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/lme4/lme4/">lme4</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Random intercepts model: operator as a random effect</span></span>
<span><span class="va">mixed_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">bright</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">operator</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">pulp</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML. t-tests use Satterthwaite's method [</span></span>
<span><span class="co">#&gt; lmerModLmerTest]</span></span>
<span><span class="co">#&gt; Formula: bright ~ 1 + (1 | operator)</span></span>
<span><span class="co">#&gt;    Data: pulp</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 18.6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.4666 -0.7595 -0.1244  0.6281  1.6012 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  operator (Intercept) 0.06808  0.2609  </span></span>
<span><span class="co">#&gt;  Residual             0.10625  0.3260  </span></span>
<span><span class="co">#&gt; Number of obs: 20, groups:  operator, 4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error      df t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  60.4000     0.1494  3.0000   404.2 3.34e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span></span>
<span><span class="co"># Fixed effects (overall mean)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept) </span></span>
<span><span class="co">#&gt;        60.4</span></span>
<span></span>
<span><span class="co"># Random effects (BLUPs)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; $operator</span></span>
<span><span class="co">#&gt;   (Intercept)</span></span>
<span><span class="co">#&gt; a  -0.1219403</span></span>
<span><span class="co">#&gt; b  -0.2591231</span></span>
<span><span class="co">#&gt; c   0.1676679</span></span>
<span><span class="co">#&gt; d   0.2133955</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; with conditional variances for "operator"</span></span>
<span></span>
<span><span class="co"># Variance components</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/VarCorr.html">VarCorr</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span></span>
<span><span class="co">#&gt;  Groups   Name        Std.Dev.</span></span>
<span><span class="co">#&gt;  operator (Intercept) 0.26093 </span></span>
<span><span class="co">#&gt;  Residual             0.32596</span></span>
<span><span class="va">re_dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/VarCorr.html">VarCorr</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Intraclass Correlation Coefficient</span></span>
<span><span class="va">rho</span> <span class="op">&lt;-</span> <span class="va">re_dat</span><span class="op">[</span><span class="fl">1</span>, <span class="st">'vcov'</span><span class="op">]</span> <span class="op">/</span> <span class="op">(</span><span class="va">re_dat</span><span class="op">[</span><span class="fl">1</span>, <span class="st">'vcov'</span><span class="op">]</span> <span class="op">+</span> <span class="va">re_dat</span><span class="op">[</span><span class="fl">2</span>, <span class="st">'vcov'</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">rho</span></span>
<span><span class="co">#&gt; [1] 0.3905354</span></span></code></pre></div>
</div>
<div id="inference-with-lmertest" class="section level4" number="8.8.1.4">
<h4>
<span class="header-section-number">8.8.1.4</span> Inference with <code>lmerTest</code><a class="anchor" aria-label="anchor" href="#inference-with-lmertest"><i class="fas fa-link"></i></a>
</h4>
<p>To obtain p-values for fixed effects using Satterthwaite‚Äôs approximation:</p>
<div class="sourceCode" id="cb321"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/runehaubo/lmerTestR">lmerTest</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model with p-values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/lmerTest/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">bright</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">operator</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">pulp</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span></span>
<span><span class="co">#&gt;             Estimate Std. Error df  t value     Pr(&gt;|t|)</span></span>
<span><span class="co">#&gt; (Intercept)     60.4  0.1494434  3 404.1664 3.340265e-08</span></span>
<span></span>
<span><span class="co"># Confidence interval for the fixed effect</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span>,<span class="op">]</span></span>
<span><span class="co">#&gt;   2.5 %  97.5 % </span></span>
<span><span class="co">#&gt; 60.0713 60.7287</span></span></code></pre></div>
<p>In this example, we can see that the confidence interval computed by <code>confint</code> from the <code>lme4</code> package is very close to the one computed by <code>confint</code> from the <code>lmerTest</code> package.</p>
</div>
<div id="bayesian-mixed-model-with-mcmcglmm" class="section level4" number="8.8.1.5">
<h4>
<span class="header-section-number">8.8.1.5</span> Bayesian Mixed Model with <code>MCMCglmm</code><a class="anchor" aria-label="anchor" href="#bayesian-mixed-model-with-mcmcglmm"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb322"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/jarrodhadfield/MCMCglmm">MCMCglmm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Bayesian mixed model</span></span>
<span><span class="va">mixed_model_bayes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MCMCglmm/man/MCMCglmm.html">MCMCglmm</a></span><span class="op">(</span></span>
<span>  <span class="va">bright</span> <span class="op">~</span> <span class="fl">1</span>,</span>
<span>  random <span class="op">=</span> <span class="op">~</span> <span class="va">operator</span>,</span>
<span>  data <span class="op">=</span> <span class="va">pulp</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Posterior summaries</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mixed_model_bayes</span><span class="op">)</span><span class="op">$</span><span class="va">solutions</span></span>
<span><span class="co">#&gt;             post.mean l-95% CI u-95% CI eff.samp pMCMC</span></span>
<span><span class="co">#&gt; (Intercept)  60.40789  60.1488 60.69595     1000 0.001</span></span></code></pre></div>
<p>Bayesian credible intervals may differ slightly from frequentist confidence intervals due to prior assumptions.</p>
</div>
<div id="predictions" class="section level4" number="8.8.1.6">
<h4>
<span class="header-section-number">8.8.1.6</span> Predictions<a class="anchor" aria-label="anchor" href="#predictions"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb323"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Random effects predictions (BLUPs)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span><span class="op">$</span><span class="va">operator</span></span>
<span><span class="co">#&gt;   (Intercept)</span></span>
<span><span class="co">#&gt; a  -0.1219403</span></span>
<span><span class="co">#&gt; b  -0.2591231</span></span>
<span><span class="co">#&gt; c   0.1676679</span></span>
<span><span class="co">#&gt; d   0.2133955</span></span>
<span></span>
<span><span class="co"># Predictions per operator</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span><span class="op">$</span><span class="va">operator</span></span>
<span><span class="co">#&gt;   (Intercept)</span></span>
<span><span class="co">#&gt; a    60.27806</span></span>
<span><span class="co">#&gt; b    60.14088</span></span>
<span><span class="co">#&gt; c    60.56767</span></span>
<span><span class="co">#&gt; d    60.61340</span></span>
<span></span>
<span><span class="co"># Equivalent using predict()</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mixed_model</span>, newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>operator <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'a'</span>, <span class="st">'b'</span>, <span class="st">'c'</span>, <span class="st">'d'</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;        1        2        3        4 </span></span>
<span><span class="co">#&gt; 60.27806 60.14088 60.56767 60.61340</span></span></code></pre></div>
<p>For <strong>bootstrap confidence intervals</strong>, use:</p>
<div class="sourceCode" id="cb324"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/lme4/man/bootMer.html">bootMer</a></span><span class="op">(</span><span class="va">mixed_model</span>, FUN <span class="op">=</span> <span class="va">fixef</span>, nsim <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; PARAMETRIC BOOTSTRAP</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; bootMer(x = mixed_model, FUN = fixef, nsim = 100)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Bootstrap Statistics :</span></span>
<span><span class="co">#&gt;     original        bias    std. error</span></span>
<span><span class="co">#&gt; t1*     60.4 -0.0005452538    0.156374</span></span></code></pre></div>
</div>
</div>
<div id="example-2-penicillin-yield-glmm-with-blocking" class="section level3" number="8.8.2">
<h3>
<span class="header-section-number">8.8.2</span> Example 2: Penicillin Yield (GLMM with Blocking)<a class="anchor" aria-label="anchor" href="#example-2-penicillin-yield-glmm-with-blocking"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb325"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">penicillin</span>, package <span class="op">=</span> <span class="st">"faraway"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualize yield by treatment and blend</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">penicillin</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">yield</span>, x <span class="op">=</span> <span class="va">treat</span>, shape <span class="op">=</span> <span class="va">blend</span>, color <span class="op">=</span> <span class="va">blend</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Penicillin Yield by Treatment and Blend"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="08-linear-mixed-models_files/figure-html/unnamed-chunk-12-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb326"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Mixed model: blend as random effect, treatment as fixed</span></span>
<span><span class="va">mixed_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmerTest/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">yield</span> <span class="op">~</span> <span class="va">treat</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">blend</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">penicillin</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML. t-tests use Satterthwaite's method [</span></span>
<span><span class="co">#&gt; lmerModLmerTest]</span></span>
<span><span class="co">#&gt; Formula: yield ~ treat + (1 | blend)</span></span>
<span><span class="co">#&gt;    Data: penicillin</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 103.8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.4152 -0.5017 -0.1644  0.6830  1.2836 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  blend    (Intercept) 11.79    3.434   </span></span>
<span><span class="co">#&gt;  Residual             18.83    4.340   </span></span>
<span><span class="co">#&gt; Number of obs: 20, groups:  blend, 5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error     df t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   84.000      2.475 11.075  33.941 1.51e-12 ***</span></span>
<span><span class="co">#&gt; treatB         1.000      2.745 12.000   0.364   0.7219    </span></span>
<span><span class="co">#&gt; treatC         5.000      2.745 12.000   1.822   0.0935 .  </span></span>
<span><span class="co">#&gt; treatD         2.000      2.745 12.000   0.729   0.4802    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation of Fixed Effects:</span></span>
<span><span class="co">#&gt;        (Intr) treatB treatC</span></span>
<span><span class="co">#&gt; treatB -0.555              </span></span>
<span><span class="co">#&gt; treatC -0.555  0.500       </span></span>
<span><span class="co">#&gt; treatD -0.555  0.500  0.500</span></span>
<span></span>
<span><span class="co"># BLUPs for each blend</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span><span class="op">$</span><span class="va">blend</span></span>
<span><span class="co">#&gt;        (Intercept)</span></span>
<span><span class="co">#&gt; Blend1   4.2878788</span></span>
<span><span class="co">#&gt; Blend2  -2.1439394</span></span>
<span><span class="co">#&gt; Blend3  -0.7146465</span></span>
<span><span class="co">#&gt; Blend4   1.4292929</span></span>
<span><span class="co">#&gt; Blend5  -2.8585859</span></span></code></pre></div>
<p>Testing for Treatment Effect</p>
<div class="sourceCode" id="cb327"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># ANOVA for fixed effects</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">mixed_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Type III Analysis of Variance Table with Satterthwaite's method</span></span>
<span><span class="co">#&gt;       Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F)</span></span>
<span><span class="co">#&gt; treat     70  23.333     3    12  1.2389 0.3387</span></span></code></pre></div>
<p>Since the <span class="math inline">\(p\)</span>-value <span class="math inline">\(&gt; .05\)</span>, we fail to reject the null hypothesis (no treatment effect).</p>
<p>Model Comparison with Kenward-Roger Approximation</p>
<div class="sourceCode" id="cb328"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://people.math.aau.dk/~sorenh/software/pbkrtest/">pbkrtest</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Full model vs. null model</span></span>
<span><span class="va">full_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">yield</span> <span class="op">~</span> <span class="va">treat</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">blend</span><span class="op">)</span>, <span class="va">penicillin</span>, REML <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">null_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">yield</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">blend</span><span class="op">)</span>, <span class="va">penicillin</span>, REML <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Kenward-Roger approximation</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/pbkrtest/man/kr__modcomp.html">KRmodcomp</a></span><span class="op">(</span><span class="va">full_model</span>, <span class="va">null_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; large : yield ~ treat + (1 | blend)</span></span>
<span><span class="co">#&gt; small : yield ~ 1 + (1 | blend)</span></span>
<span><span class="co">#&gt;          stat     ndf     ddf F.scaling p.value</span></span>
<span><span class="co">#&gt; Ftest  1.2389  3.0000 12.0000         1  0.3387</span></span></code></pre></div>
<p>The results are consistent with the earlier ANOVA: <strong>no significant treatment effect</strong>.</p>
</div>
<div id="example-3-growth-in-rats-over-time" class="section level3" number="8.8.3">
<h3>
<span class="header-section-number">8.8.3</span> Example 3: Growth in Rats Over Time<a class="anchor" aria-label="anchor" href="#example-3-growth-in-rats-over-time"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb329"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rats</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span></span>
<span>    <span class="st">"images/rats.dat"</span>,</span>
<span>    header <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    sep <span class="op">=</span> <span class="st">' '</span>,</span>
<span>    col.names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'Treatment'</span>, <span class="st">'rat'</span>, <span class="st">'age'</span>, <span class="st">'y'</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Log-transformed time variable</span></span>
<span><span class="va">rats</span><span class="op">$</span><span class="va">t</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="va">rats</span><span class="op">$</span><span class="va">age</span> <span class="op">-</span> <span class="fl">45</span><span class="op">)</span> <span class="op">/</span> <span class="fl">10</span><span class="op">)</span></span></code></pre></div>
<p>Model Fitting</p>
<div class="sourceCode" id="cb330"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Treatment as fixed effect, random intercepts for rats</span></span>
<span><span class="va">rat_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">t</span><span class="op">:</span><span class="va">Treatment</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">rat</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">rats</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">rat_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML. t-tests use Satterthwaite's method [</span></span>
<span><span class="co">#&gt; lmerModLmerTest]</span></span>
<span><span class="co">#&gt; Formula: y ~ t:Treatment + (1 | rat)</span></span>
<span><span class="co">#&gt;    Data: rats</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 932.4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -2.25574 -0.65898 -0.01163  0.58356  2.88309 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  rat      (Intercept) 3.565    1.888   </span></span>
<span><span class="co">#&gt;  Residual             1.445    1.202   </span></span>
<span><span class="co">#&gt; Number of obs: 252, groups:  rat, 50</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;                Estimate Std. Error       df t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)     68.6074     0.3312  89.0275  207.13   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; t:Treatmentcon   7.3138     0.2808 247.2762   26.05   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; t:Treatmenthig   6.8711     0.2276 247.7097   30.19   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; t:Treatmentlow   7.5069     0.2252 247.5196   33.34   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation of Fixed Effects:</span></span>
<span><span class="co">#&gt;             (Intr) t:Trtmntc t:Trtmnth</span></span>
<span><span class="co">#&gt; t:Tretmntcn -0.327                    </span></span>
<span><span class="co">#&gt; t:Tretmnthg -0.340  0.111             </span></span>
<span><span class="co">#&gt; t:Tretmntlw -0.351  0.115     0.119</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">rat_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Type III Analysis of Variance Table with Satterthwaite's method</span></span>
<span><span class="co">#&gt;             Sum Sq Mean Sq NumDF  DenDF F value    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; t:Treatment 3181.9  1060.6     3 223.21  734.11 &lt; 2.2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>Since the <strong>p-value is significant</strong>, we conclude that the <strong>treatment effect varies over time</strong>.</p>
</div>
<div id="example-4-tree-water-use-agridat" class="section level3" number="8.8.4">
<h3>
<span class="header-section-number">8.8.4</span> Example 4: Tree Water Use (Agridat)<a class="anchor" aria-label="anchor" href="#example-4-tree-water-use-agridat"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb331"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://kwstat.github.io/agridat/">agridat</a></span><span class="op">)</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="va">harris.wateruse</span></span>
<span></span>
<span><span class="co"># Visualizing water use by species and age</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://latticeextra.r-forge.r-project.org/">latticeExtra</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/latticeExtra/man/useOuterStrips.html">useOuterStrips</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html">xyplot</a></span><span class="op">(</span><span class="va">water</span> <span class="op">~</span> <span class="va">day</span> <span class="op">|</span> <span class="va">species</span> <span class="op">*</span> <span class="va">age</span>, </span>
<span>         <span class="va">dat</span>, group <span class="op">=</span> <span class="va">tree</span>,</span>
<span>         type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'p'</span>, <span class="st">'smooth'</span><span class="op">)</span>, </span>
<span>         main <span class="op">=</span> <span class="st">"harris.wateruse 2 species, 2 ages (10 trees each)"</span>,</span>
<span>         as.table <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="08-linear-mixed-models_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Remove outlier</p>
<div class="sourceCode" id="cb332"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">dat</span>, <span class="va">day</span><span class="op">!=</span><span class="fl">268</span><span class="op">)</span></span></code></pre></div>
<p>Plot between water use and day for one age and species group</p>
<div class="sourceCode" id="cb333"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html">xyplot</a></span><span class="op">(</span></span>
<span>    <span class="va">water</span> <span class="op">~</span> <span class="va">day</span> <span class="op">|</span> <span class="va">tree</span>,</span>
<span>    <span class="va">dat</span>,</span>
<span>    subset   <span class="op">=</span> <span class="va">age</span> <span class="op">==</span> <span class="st">"A2"</span> <span class="op">&amp;</span> <span class="va">species</span> <span class="op">==</span> <span class="st">"S2"</span>,</span>
<span>    as.table <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    type     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'p'</span>, <span class="st">'smooth'</span><span class="op">)</span>,</span>
<span>    ylab     <span class="op">=</span> <span class="st">"Water use profiles of individual trees"</span>,</span>
<span>    main     <span class="op">=</span> <span class="st">"harris.wateruse (Age 2, Species 2)"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="08-linear-mixed-models_files/figure-html/unnamed-chunk-19-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb334"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Rescale day for nicer output, and convergence issues</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/transform.html">transform</a></span><span class="op">(</span><span class="va">dat</span>, ti <span class="op">=</span> <span class="va">day</span> <span class="op">/</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="co"># add quadratic term</span></span>
<span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/transform.html">transform</a></span><span class="op">(</span><span class="va">dat</span>, ti2 <span class="op">=</span> <span class="va">ti</span> <span class="op">*</span> <span class="va">ti</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Start with a subgroup: age 2, species 2</span></span>
<span><span class="va">d22</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/droplevels.html">droplevels</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">dat</span>, <span class="va">age</span> <span class="op">==</span> <span class="st">"A2"</span> <span class="op">&amp;</span> <span class="va">species</span> <span class="op">==</span> <span class="st">"S2"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Fitting with <code>nlme</code> using <code>lme</code></p>
<div class="sourceCode" id="cb335"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://svn.r-project.org/R-packages/trunk/nlme/">nlme</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">## We use pdDiag() to get uncorrelated random effects</span></span>
<span><span class="va">m1n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/lme.html">lme</a></span><span class="op">(</span></span>
<span>    <span class="va">water</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="va">ti</span> <span class="op">+</span> <span class="va">ti2</span>,</span>
<span>    <span class="co">#intercept, time and time-squared = fixed effects</span></span>
<span>    data <span class="op">=</span> <span class="va">d22</span>,</span>
<span>    na.action <span class="op">=</span> <span class="va">na.omit</span>,</span>
<span>    random <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span>tree <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/pdDiag.html">pdDiag</a></span><span class="op">(</span><span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="va">ti</span> <span class="op">+</span> <span class="va">ti2</span><span class="op">)</span><span class="op">)</span> </span>
<span>    <span class="co"># random intercept, time </span></span>
<span>    <span class="co"># and time squared per tree = random effects</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># for all trees</span></span>
<span><span class="co"># m1n &lt;- lme(</span></span>
<span><span class="co">#   water ~ 1 + ti + ti2,</span></span>
<span><span class="co">#   random = list(tree = pdDiag(~ 1 + ti + ti2)),</span></span>
<span><span class="co">#   data = dat,</span></span>
<span><span class="co">#   na.action = na.omit</span></span>
<span><span class="co"># )</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m1n</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed-effects model fit by REML</span></span>
<span><span class="co">#&gt;   Data: d22 </span></span>
<span><span class="co">#&gt;        AIC     BIC    logLik</span></span>
<span><span class="co">#&gt;   276.5142 300.761 -131.2571</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Formula: ~1 + ti + ti2 | tree</span></span>
<span><span class="co">#&gt;  Structure: Diagonal</span></span>
<span><span class="co">#&gt;         (Intercept)           ti          ti2  Residual</span></span>
<span><span class="co">#&gt; StdDev:   0.5187869 1.631223e-05 4.374982e-06 0.3836614</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:  water ~ 1 + ti + ti2 </span></span>
<span><span class="co">#&gt;                  Value Std.Error  DF   t-value p-value</span></span>
<span><span class="co">#&gt; (Intercept) -10.798799 0.8814666 227 -12.25094       0</span></span>
<span><span class="co">#&gt; ti           12.346704 0.7827112 227  15.77428       0</span></span>
<span><span class="co">#&gt; ti2          -2.838503 0.1720614 227 -16.49704       0</span></span>
<span><span class="co">#&gt;  Correlation: </span></span>
<span><span class="co">#&gt;     (Intr) ti    </span></span>
<span><span class="co">#&gt; ti  -0.979       </span></span>
<span><span class="co">#&gt; ti2  0.970 -0.997</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Standardized Within-Group Residuals:</span></span>
<span><span class="co">#&gt;         Min          Q1         Med          Q3         Max </span></span>
<span><span class="co">#&gt; -3.07588246 -0.58531056  0.01210209  0.65402695  3.88777402 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Observations: 239</span></span>
<span><span class="co">#&gt; Number of Groups: 10</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">m1n</span><span class="op">)</span></span>
<span><span class="co">#&gt;     (Intercept)            ti           ti2</span></span>
<span><span class="co">#&gt; T04   0.1985796  2.070606e-09  6.397103e-10</span></span>
<span><span class="co">#&gt; T05   0.3492827  3.199664e-10 -6.211457e-11</span></span>
<span><span class="co">#&gt; T19  -0.1978989 -9.879555e-10 -2.514502e-10</span></span>
<span><span class="co">#&gt; T23   0.4519003 -4.206418e-10 -3.094113e-10</span></span>
<span><span class="co">#&gt; T38  -0.6457494 -2.069198e-09 -4.227912e-10</span></span>
<span><span class="co">#&gt; T40   0.3739432  4.199061e-10 -3.260161e-11</span></span>
<span><span class="co">#&gt; T49   0.8620648  1.160387e-09 -6.925457e-12</span></span>
<span><span class="co">#&gt; T53  -0.5655049 -1.064849e-09 -5.870462e-11</span></span>
<span><span class="co">#&gt; T67  -0.4394623 -4.482549e-10  2.752922e-11</span></span>
<span><span class="co">#&gt; T71  -0.3871552  1.020034e-09  4.767595e-10</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span><span class="va">m1n</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)          ti         ti2 </span></span>
<span><span class="co">#&gt;  -10.798799   12.346704   -2.838503</span></span></code></pre></div>
<p>Fitting with <code>lme4</code> using <code>lmer</code></p>
<div class="sourceCode" id="cb336"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/lme4/lme4/">lme4</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">m1lmer</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">water</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="va">ti</span> <span class="op">+</span> <span class="va">ti2</span> <span class="op">+</span> <span class="op">(</span><span class="va">ti</span> <span class="op">+</span> <span class="va">ti2</span> <span class="op">||</span></span>
<span>                                     <span class="va">tree</span><span class="op">)</span>,</span>
<span>         data <span class="op">=</span> <span class="va">d22</span>,</span>
<span>         na.action <span class="op">=</span> <span class="va">na.omit</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># for all trees</span></span>
<span><span class="co"># m1lmer &lt;- lmer(water ~ 1 + ti + ti2 + (ti + ti2 || tree),</span></span>
<span><span class="co">#                data = dat, na.action = na.omit)</span></span>
<span></span>
<span><span class="co"># summary(m1lmer)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">m1lmer</span><span class="op">)</span></span>
<span><span class="co">#&gt; $tree</span></span>
<span><span class="co">#&gt;     (Intercept) ti ti2</span></span>
<span><span class="co">#&gt; T04   0.1985796  0   0</span></span>
<span><span class="co">#&gt; T05   0.3492827  0   0</span></span>
<span><span class="co">#&gt; T19  -0.1978989  0   0</span></span>
<span><span class="co">#&gt; T23   0.4519003  0   0</span></span>
<span><span class="co">#&gt; T38  -0.6457494  0   0</span></span>
<span><span class="co">#&gt; T40   0.3739432  0   0</span></span>
<span><span class="co">#&gt; T49   0.8620648  0   0</span></span>
<span><span class="co">#&gt; T53  -0.5655049  0   0</span></span>
<span><span class="co">#&gt; T67  -0.4394623  0   0</span></span>
<span><span class="co">#&gt; T71  -0.3871552  0   0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; with conditional variances for "tree"</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span><span class="va">m1lmer</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)          ti         ti2 </span></span>
<span><span class="co">#&gt;  -10.798799   12.346704   -2.838503</span></span></code></pre></div>
<p>Notes:</p>
<ul>
<li>
<code>||</code> double pipes = uncorrelated random effects</li>
<li>To remove the intercept term:
<ul>
<li><code>(0+ti|tree)</code></li>
<li><code>(ti-1|tree)</code></li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb337"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m1l</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html">lmer</a></span><span class="op">(</span><span class="va">water</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="va">ti</span> <span class="op">+</span> <span class="va">ti2</span> </span>
<span>         <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">tree</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="fl">0</span> <span class="op">+</span> <span class="va">ti</span> <span class="op">|</span> <span class="va">tree</span><span class="op">)</span> </span>
<span>         <span class="op">+</span> <span class="op">(</span><span class="fl">0</span> <span class="op">+</span> <span class="va">ti2</span> <span class="op">|</span> <span class="va">tree</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">d22</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">m1l</span><span class="op">)</span></span>
<span><span class="co">#&gt; $tree</span></span>
<span><span class="co">#&gt;     (Intercept) ti ti2</span></span>
<span><span class="co">#&gt; T04   0.1985796  0   0</span></span>
<span><span class="co">#&gt; T05   0.3492827  0   0</span></span>
<span><span class="co">#&gt; T19  -0.1978989  0   0</span></span>
<span><span class="co">#&gt; T23   0.4519003  0   0</span></span>
<span><span class="co">#&gt; T38  -0.6457494  0   0</span></span>
<span><span class="co">#&gt; T40   0.3739432  0   0</span></span>
<span><span class="co">#&gt; T49   0.8620648  0   0</span></span>
<span><span class="co">#&gt; T53  -0.5655049  0   0</span></span>
<span><span class="co">#&gt; T67  -0.4394623  0   0</span></span>
<span><span class="co">#&gt; T71  -0.3871552  0   0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; with conditional variances for "tree"</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span><span class="va">m1l</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)          ti         ti2 </span></span>
<span><span class="co">#&gt;  -10.798799   12.346704   -2.838503</span></span></code></pre></div>
<p>Adding Correlation Structure</p>
<div class="sourceCode" id="cb338"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m2n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/lme.html">lme</a></span><span class="op">(</span></span>
<span>    <span class="va">water</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="va">ti</span> <span class="op">+</span> <span class="va">ti2</span>,</span>
<span>    data <span class="op">=</span> <span class="va">d22</span>,</span>
<span>    random <span class="op">=</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">|</span> <span class="va">tree</span>,</span>
<span>    cor <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/corExp.html">corExp</a></span><span class="op">(</span>form <span class="op">=</span>  <span class="op">~</span> <span class="va">day</span> <span class="op">|</span> <span class="va">tree</span><span class="op">)</span>,</span>
<span>    na.action <span class="op">=</span> <span class="va">na.omit</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># for all trees</span></span>
<span><span class="co"># m2n &lt;- lme(</span></span>
<span><span class="co">#   water ~ 1 + ti + ti2,</span></span>
<span><span class="co">#   random = ~ 1 | tree,</span></span>
<span><span class="co">#   cor = corExp(form = ~ day | tree),</span></span>
<span><span class="co">#   data = dat,</span></span>
<span><span class="co">#   na.action = na.omit</span></span>
<span><span class="co"># )</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">m2n</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed-effects model fit by REML</span></span>
<span><span class="co">#&gt;   Data: d22 </span></span>
<span><span class="co">#&gt;        AIC      BIC   logLik</span></span>
<span><span class="co">#&gt;   263.3081 284.0911 -125.654</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Formula: ~1 | tree</span></span>
<span><span class="co">#&gt;         (Intercept)  Residual</span></span>
<span><span class="co">#&gt; StdDev:   0.5154042 0.3925777</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation Structure: Exponential spatial correlation</span></span>
<span><span class="co">#&gt;  Formula: ~day | tree </span></span>
<span><span class="co">#&gt;  Parameter estimate(s):</span></span>
<span><span class="co">#&gt;    range </span></span>
<span><span class="co">#&gt; 3.794624 </span></span>
<span><span class="co">#&gt; Fixed effects:  water ~ 1 + ti + ti2 </span></span>
<span><span class="co">#&gt;                  Value Std.Error  DF   t-value p-value</span></span>
<span><span class="co">#&gt; (Intercept) -11.223310 1.0988725 227 -10.21348       0</span></span>
<span><span class="co">#&gt; ti           12.712094 0.9794235 227  12.97916       0</span></span>
<span><span class="co">#&gt; ti2          -2.913682 0.2148551 227 -13.56115       0</span></span>
<span><span class="co">#&gt;  Correlation: </span></span>
<span><span class="co">#&gt;     (Intr) ti    </span></span>
<span><span class="co">#&gt; ti  -0.985       </span></span>
<span><span class="co">#&gt; ti2  0.976 -0.997</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Standardized Within-Group Residuals:</span></span>
<span><span class="co">#&gt;         Min          Q1         Med          Q3         Max </span></span>
<span><span class="co">#&gt; -3.04861039 -0.55703950  0.00278101  0.62558762  3.80676991 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Observations: 239</span></span>
<span><span class="co">#&gt; Number of Groups: 10</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/random.effects.html">ranef</a></span><span class="op">(</span><span class="va">m2n</span><span class="op">)</span></span>
<span><span class="co">#&gt;     (Intercept)</span></span>
<span><span class="co">#&gt; T04   0.1929971</span></span>
<span><span class="co">#&gt; T05   0.3424631</span></span>
<span><span class="co">#&gt; T19  -0.1988495</span></span>
<span><span class="co">#&gt; T23   0.4538660</span></span>
<span><span class="co">#&gt; T38  -0.6413664</span></span>
<span><span class="co">#&gt; T40   0.3769378</span></span>
<span><span class="co">#&gt; T49   0.8410043</span></span>
<span><span class="co">#&gt; T53  -0.5528236</span></span>
<span><span class="co">#&gt; T67  -0.4452930</span></span>
<span><span class="co">#&gt; T71  -0.3689358</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlme/man/fixed.effects.html">fixef</a></span><span class="op">(</span><span class="va">m2n</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)          ti         ti2 </span></span>
<span><span class="co">#&gt;  -11.223310   12.712094   -2.913682</span></span></code></pre></div>
<p><strong>Key Takeaways</strong></p>
<ul>
<li><p><code>lme4</code> is preferred for general mixed models due to efficiency and simplicity.</p></li>
<li><p><code>nlme</code> is powerful for complex correlation structures and nested designs.</p></li>
<li><p>Bayesian models (e.g., <code>MCMCglmm</code>) offer flexible inference under uncertainty.</p></li>
<li><p>Always consider model diagnostics and random effects structure carefully.</p></li>
</ul>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></div>
<div class="next"><a href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-linear-mixed-models"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li>
<a class="nav-link" href="#dependent-data"><span class="header-section-number">8.1</span> Dependent Data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#motivation-a-repeated-measurements-example"><span class="header-section-number">8.1.1</span> Motivation: A Repeated Measurements Example</a></li>
<li><a class="nav-link" href="#example-linear-mixed-model-for-repeated-measurements"><span class="header-section-number">8.1.2</span> Example: Linear Mixed Model for Repeated Measurements</a></li>
<li><a class="nav-link" href="#sec-random-intercepts-model-lmm"><span class="header-section-number">8.1.3</span> Random-Intercepts Model</a></li>
<li><a class="nav-link" href="#covariance-models-in-linear-mixed-models"><span class="header-section-number">8.1.4</span> Covariance Models in Linear Mixed Models</a></li>
<li><a class="nav-link" href="#covariance-structures-in-mixed-models"><span class="header-section-number">8.1.5</span> Covariance Structures in Mixed Models</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#estimation-in-linear-mixed-models"><span class="header-section-number">8.2</span> Estimation in Linear Mixed Models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#interpretation-of-the-mixed-model-equations"><span class="header-section-number">8.2.1</span> Interpretation of the Mixed Model Equations</a></li>
<li><a class="nav-link" href="#derivation-of-the-mixed-model-equations"><span class="header-section-number">8.2.2</span> Derivation of the Mixed Model Equations</a></li>
<li><a class="nav-link" href="#bayesian-interpretation-of-linear-mixed-models"><span class="header-section-number">8.2.3</span> Bayesian Interpretation of Linear Mixed Models</a></li>
<li><a class="nav-link" href="#estimating-the-variance-covariance-matrix"><span class="header-section-number">8.2.4</span> Estimating the Variance-Covariance Matrix</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#inference-in-linear-mixed-models"><span class="header-section-number">8.3</span> Inference in Linear Mixed Models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#inference-for-fixed-effects-beta"><span class="header-section-number">8.3.1</span> Inference for Fixed Effects (\(\beta\))</a></li>
<li><a class="nav-link" href="#inference-for-variance-components-theta"><span class="header-section-number">8.3.2</span> Inference for Variance Components (\(\theta\))</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#information-criteria-for-model-selection"><span class="header-section-number">8.4</span> Information Criteria for Model Selection</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-akaike-information-criterion-lmm"><span class="header-section-number">8.4.1</span> Akaike Information Criterion</a></li>
<li><a class="nav-link" href="#sec-corrected-aic-lmm"><span class="header-section-number">8.4.2</span> Corrected AIC</a></li>
<li><a class="nav-link" href="#sec-bayesian-information-criterion-lmm"><span class="header-section-number">8.4.3</span> Bayesian Information Criterion</a></li>
<li><a class="nav-link" href="#practical-example-with-linear-mixed-models"><span class="header-section-number">8.4.4</span> Practical Example with Linear Mixed Models</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#split-plot-designs"><span class="header-section-number">8.5</span> Split-Plot Designs</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-setup"><span class="header-section-number">8.5.1</span> Example Setup</a></li>
<li><a class="nav-link" href="#statistical-model-for-split-plot-designs"><span class="header-section-number">8.5.2</span> Statistical Model for Split-Plot Designs</a></li>
<li><a class="nav-link" href="#approaches-to-analyzing-split-plot-designs"><span class="header-section-number">8.5.3</span> Approaches to Analyzing Split-Plot Designs</a></li>
<li><a class="nav-link" href="#application-split-plot-design"><span class="header-section-number">8.5.4</span> Application: Split-Plot Design</a></li>
</ul>
</li>
<li><a class="nav-link" href="#repeated-measures-in-mixed-models"><span class="header-section-number">8.6</span> Repeated Measures in Mixed Models</a></li>
<li>
<a class="nav-link" href="#unbalanced-or-unequally-spaced-data"><span class="header-section-number">8.7</span> Unbalanced or Unequally Spaced Data</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#variance-covariance-structure-power-model"><span class="header-section-number">8.7.1</span> Variance-Covariance Structure: Power Model</a></li></ul>
</li>
<li>
<a class="nav-link" href="#application-mixed-models-in-practice"><span class="header-section-number">8.8</span> Application: Mixed Models in Practice</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-1-pulp-brightness-analysis"><span class="header-section-number">8.8.1</span> Example 1: Pulp Brightness Analysis</a></li>
<li><a class="nav-link" href="#example-2-penicillin-yield-glmm-with-blocking"><span class="header-section-number">8.8.2</span> Example 2: Penicillin Yield (GLMM with Blocking)</a></li>
<li><a class="nav-link" href="#example-3-growth-in-rats-over-time"><span class="header-section-number">8.8.3</span> Example 3: Growth in Rats Over Time</a></li>
<li><a class="nav-link" href="#example-4-tree-water-use-agridat"><span class="header-section-number">8.8.4</span> Example 4: Tree Water Use (Agridat)</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/08-linear-mixed-models.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/08-linear-mixed-models.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-05-24.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
