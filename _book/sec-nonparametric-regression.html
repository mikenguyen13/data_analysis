<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Nonparametric Regression | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Nonparametric regression refers to a class of regression techniques that do not assume a specific functional form (e.g., linear, polynomial of fixed degree) for the relationship between a...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="Chapter 10 Nonparametric Regression | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/sec-nonparametric-regression.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Nonparametric regression refers to a class of regression techniques that do not assume a specific functional form (e.g., linear, polynomial of fixed degree) for the relationship between a...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 10 Nonparametric Regression | A Guide on Data Analysis">
<meta name="twitter:description" content="Nonparametric regression refers to a class of regression techniques that do not assume a specific functional form (e.g., linear, polynomial of fixed degree) for the relationship between a...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="active" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">39</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="sec-nonparametric-regression" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Nonparametric Regression<a class="anchor" aria-label="anchor" href="#sec-nonparametric-regression"><i class="fas fa-link"></i></a>
</h1>
<p><strong>Nonparametric regression</strong> refers to a class of regression techniques that do not assume a specific functional form (e.g., linear, polynomial of fixed degree) for the relationship between a predictor <span class="math inline">\(x \in \mathbb{R}\)</span> (or <span class="math inline">\(\mathbf{x} \in \mathbb{R}^p\)</span>) and a response variable <span class="math inline">\(y \in \mathbb{R}\)</span>. Instead, nonparametric methods aim to estimate this relationship directly from the data, allowing the data to “speak for themselves.”</p>
<p>In a standard regression framework, we have a response variable <span class="math inline">\(Y\)</span> and one or more predictors <span class="math inline">\(\mathbf{X} = (X_1, X_2, \ldots, X_p)\)</span>. Let us start with a univariate setting for simplicity. We assume the following model:</p>
<p><span class="math display">\[
Y = m(x) + \varepsilon,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(m(x) = \mathbb{E}[Y \mid X = x]\)</span> is the <strong>regression function</strong> we aim to estimate,</li>
<li>
<span class="math inline">\(\varepsilon\)</span> is a random <strong>error term</strong> (noise) with <span class="math inline">\(\mathbb{E}[\varepsilon \mid X = x] = 0\)</span> and constant variance <span class="math inline">\(\operatorname{Var}(\varepsilon) = \sigma^2\)</span>.</li>
</ul>
<p>In <strong>parametric regression</strong> (e.g., <a href="linear-regression.html#linear-regression">Linear Regression</a>), we might assume <span class="math inline">\(m(x)\)</span> has a specific form, such as:</p>
<p><span class="math display">\[
m(x) = \beta_0 + \beta_1 x + \cdots + \beta_d x^d,
\]</span></p>
<p>where <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_d\)</span> are parameters to be estimated. In contrast, <strong>nonparametric regression</strong> relaxes this assumption and employs methods that can adapt to potentially complex shapes in <span class="math inline">\(m(x)\)</span> without pre-specifying its structure.</p>
<div id="why-nonparametric" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Why Nonparametric?<a class="anchor" aria-label="anchor" href="#why-nonparametric"><i class="fas fa-link"></i></a>
</h2>
<div id="flexibility" class="section level3" number="10.1.1">
<h3>
<span class="header-section-number">10.1.1</span> Flexibility<a class="anchor" aria-label="anchor" href="#flexibility"><i class="fas fa-link"></i></a>
</h3>
<p>Nonparametric methods can capture <strong>nonlinear relationships</strong> and <strong>complex patterns</strong> in your data more effectively than many traditional parametric methods.</p>
<ul>
<li>
<strong>Adaptive Fit</strong>: They rely on the data itself to determine the shape of the relationship, rather than forcing a specific equation like <span class="math inline">\(Y = \beta_0 + \beta_1 x\)</span> (linear) or a polynomial.</li>
<li>
<strong>Local Structures</strong>: Techniques like <a href="sec-nonparametric-regression.html#sec-kernel-regression">kernel smoothing</a> or <a href="sec-nonparametric-regression.html#sec-local-polynomial-regression">local regression</a> focus on small neighborhoods around each observation, allowing the model to adjust dynamically to local variations.</li>
</ul>
<p><strong>When This Matters:</strong></p>
<ul>
<li><p><strong>Highly Variable Data</strong>: If the data shows multiple peaks, sharp transitions, or other irregular patterns.</p></li>
<li><p><strong>Exploratory Analysis</strong>: When you’re trying to uncover hidden structures or trends in a dataset without strong prior assumptions.</p></li>
</ul>
<hr>
</div>
<div id="fewer-assumptions" class="section level3" number="10.1.2">
<h3>
<span class="header-section-number">10.1.2</span> Fewer Assumptions<a class="anchor" aria-label="anchor" href="#fewer-assumptions"><i class="fas fa-link"></i></a>
</h3>
<p>Parametric methods typically assume:</p>
<ul>
<li><p>A specific functional form (e.g., linear, quadratic).</p></li>
<li><p>A specific error distribution (e.g., normal, Poisson).</p></li>
</ul>
<p>Nonparametric methods, on the other hand, <strong>relax these assumptions</strong>, making them:</p>
<ul>
<li>
<strong>Robust to Misspecification</strong>: Less risk of biased estimates due to incorrect modeling choices.</li>
<li>
<strong>Flexible in Error Structure</strong>: They can handle complex error distributions without explicitly modeling them.</li>
</ul>
<p><strong>When This Matters:</strong></p>
<ul>
<li><p><strong>Heterogeneous Populations</strong>: In fields like ecology, genomics, or finance, where data might come from unknown mixtures of distributions.</p></li>
<li><p><strong>Lack of Theoretical Guidance</strong>: If theory does not suggest a strong functional form or distribution family.</p></li>
</ul>
<hr>
</div>
<div id="interpretability" class="section level3" number="10.1.3">
<h3>
<span class="header-section-number">10.1.3</span> Interpretability<a class="anchor" aria-label="anchor" href="#interpretability"><i class="fas fa-link"></i></a>
</h3>
<p>Nonparametric models can still offer valuable insights:</p>
<ul>
<li>
<strong>Visual Interpretations</strong>: Methods like kernel smoothing provide smooth curves that you can plot to see how <span class="math inline">\(Y\)</span> changes with <span class="math inline">\(x\)</span>.</li>
<li>
<strong>Tree-Based Methods</strong>: Random forests and gradient boosting (also nonparametric in nature) can be interpreted via <strong>variable importance</strong> measures or <strong>partial dependence</strong> plots, although they can be more complex than simple curves.</li>
</ul>
<p>While you don’t get simple coefficient estimates as in <a href="linear-regression.html#linear-regression">Linear Regression</a>, you can still convey how certain predictors influence the response through plots or importance metrics.</p>
<hr>
</div>
<div id="practical-considerations-3" class="section level3" number="10.1.4">
<h3>
<span class="header-section-number">10.1.4</span> Practical Considerations<a class="anchor" aria-label="anchor" href="#practical-considerations-3"><i class="fas fa-link"></i></a>
</h3>
<div id="when-to-prefer-nonparametric" class="section level4" number="10.1.4.1">
<h4>
<span class="header-section-number">10.1.4.1</span> When to Prefer Nonparametric<a class="anchor" aria-label="anchor" href="#when-to-prefer-nonparametric"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Larger Sample Sizes:</strong> Nonparametric methods often need more data because they let the data “speak” rather than relying on a fixed formula.</li>
<li>
<strong>Unknown or Complex Relationships:</strong> If you suspect strong nonlinearity or have no strong theory about the functional form, nonparametric approaches provide the flexibility to discover patterns.</li>
<li>
<strong>Exploratory or Predictive Goals:</strong> In data-driven or machine learning contexts, minimizing predictive error often takes precedence over strict parametric assumptions.</li>
</ol>
</div>
<div id="when-to-be-cautious" class="section level4" number="10.1.4.2">
<h4>
<span class="header-section-number">10.1.4.2</span> When to Be Cautious<a class="anchor" aria-label="anchor" href="#when-to-be-cautious"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Small Sample Sizes:</strong> Nonparametric methods can overfit and exhibit high variance if there isn’t enough data to reliably estimate the relationship.</li>
<li>
<strong>Computational Cost:</strong> Some nonparametric methods (e.g., kernel methods, large random forests) can be computationally heavier than parametric approaches like linear regression.</li>
<li>
<strong>Strong Theoretical Models:</strong> If domain knowledge strongly suggests a specific parametric form, ignoring that might reduce clarity or conflict with established theory.</li>
<li>
<strong>Extrapolation:</strong> Nonparametric models typically <strong>do not</strong> extrapolate well beyond the observed data range, because they rely heavily on local patterns.</li>
</ol>
<hr>
</div>
</div>
<div id="balancing-parametric-and-nonparametric-approaches" class="section level3" number="10.1.5">
<h3>
<span class="header-section-number">10.1.5</span> Balancing Parametric and Nonparametric Approaches<a class="anchor" aria-label="anchor" href="#balancing-parametric-and-nonparametric-approaches"><i class="fas fa-link"></i></a>
</h3>
<p>In practice, it’s not always an either/or decision. Consider:</p>
<ul>
<li>
<strong>Semiparametric Models</strong>: Combine parametric components (for known relationships or effects) with nonparametric components (for unknown parts).</li>
<li>
<strong>Model Selection &amp; Regularization</strong>: Use techniques like cross-validation to choose bandwidths (kernel smoothing), number of knots (splines), or hyperparameters (tree depth) to avoid overfitting.</li>
<li>
<strong>Diagnostic Tools</strong>: Start with a simple parametric model, look at residual plots to identify patterns that might warrant a nonparametric approach.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="17%">
<col width="43%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th><strong>Criterion</strong></th>
<th><strong>Parametric Methods</strong></th>
<th><strong>Nonparametric Methods</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Assumptions</strong></td>
<td>Requires strict assumptions (e.g., linearity, distribution form)</td>
<td>Minimal assumptions, flexible functional forms</td>
</tr>
<tr class="even">
<td><strong>Data Requirements</strong></td>
<td>Often works with smaller datasets if assumptions hold</td>
<td>Generally more data-hungry due to flexibility</td>
</tr>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>Straightforward coefficients, easy to explain</td>
<td>Visual or plot-based insights; feature importance in trees</td>
</tr>
<tr class="even">
<td><strong>Complexity &amp; Overfitting</strong></td>
<td>Less prone to overfitting if form is correct</td>
<td>Can overfit if not regularized (e.g., bandwidth selection)</td>
</tr>
<tr class="odd">
<td><strong>Extrapolation</strong></td>
<td>Can extrapolate if the assumed form is correct</td>
<td>Poor extrapolation outside the observed data range</td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td>Typically low to moderate (e.g., <span class="math inline">\(O(n)\)</span> to <span class="math inline">\(O(n^2)\)</span>) depending on method</td>
<td>Can be higher (e.g., repeated local estimates or ensemble methods)</td>
</tr>
</tbody>
</table></div>
<hr>
<p><strong>Drawbacks and Challenges</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Curse of Dimensionality:</strong> As the number of predictors <span class="math inline">\(p\)</span> increases, nonparametric methods often require exponentially larger sample sizes to maintain accuracy. This phenomenon, known as the curse of dimensionality, leads to sparse data in high-dimensional spaces, making it harder to obtain reliable estimates.</li>
<li>
<strong>Choice of Hyperparameters:</strong> Methods such as kernel smoothing and splines depend on hyperparameters like bandwidth or smoothing parameters, which must be carefully selected to balance bias and variance.</li>
<li>
<strong>Computational Complexity:</strong> Nonparametric methods can be computationally intensive, especially with large datasets or in high-dimensional settings.</li>
</ol>
<hr>
</div>
</div>
<div id="basic-concepts-in-nonparametric-estimation" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Basic Concepts in Nonparametric Estimation<a class="anchor" aria-label="anchor" href="#basic-concepts-in-nonparametric-estimation"><i class="fas fa-link"></i></a>
</h2>
<div id="bias-variance-trade-off" class="section level3" number="10.2.1">
<h3>
<span class="header-section-number">10.2.1</span> Bias-Variance Trade-Off<a class="anchor" aria-label="anchor" href="#bias-variance-trade-off"><i class="fas fa-link"></i></a>
</h3>
<p>For a given method of estimating <span class="math inline">\(m(x)\)</span>, we denote the estimator as <span class="math inline">\(\hat{m}(x)\)</span>. The <strong>mean squared error</strong> (MSE) at a point <span class="math inline">\(x\)</span> is defined as:</p>
<p><span class="math display">\[
\operatorname{MSE}(x) = \mathbb{E}\bigl[\{\hat{m}(x) - m(x)\}^2\bigr].
\]</span></p>
<p>This MSE can be decomposed into two key components: <strong>bias</strong> and <strong>variance</strong>:</p>
<p><span class="math display">\[
\operatorname{MSE}(x) = \bigl[\mathbb{E}[\hat{m}(x)] - m(x)\bigr]^2 + \operatorname{Var}(\hat{m}(x)).
\]</span></p>
<p>Where:</p>
<ul>
<li><p><strong>Bias</strong>: Measures the systematic error in the estimator: <span class="math display">\[
\operatorname{Bias}^2 = \bigl[\mathbb{E}[\hat{m}(x)] - m(x)\bigr]^2.
\]</span></p></li>
<li><p><strong>Variance</strong>: Measures the variability of the estimator around its expected value: <span class="math display">\[
\operatorname{Var}(\hat{m}(x)) = \mathbb{E}\bigl[\{\hat{m}(x) - \mathbb{E}[\hat{m}(x)]\}^2\bigr].
\]</span></p></li>
</ul>
<p>Nonparametric methods often have <strong>low bias</strong> because they can adapt to a wide range of functions. However, this flexibility can lead to <strong>high variance</strong>, especially when the model captures noise rather than the underlying signal.</p>
<p>The <strong>bandwidth</strong> or <strong>smoothing parameter</strong> in nonparametric methods typically controls this trade-off:</p>
<ul>
<li>
<strong>Large bandwidth</strong> <span class="math inline">\(\Rightarrow\)</span> smoother function <span class="math inline">\(\Rightarrow\)</span> higher bias, lower variance.</li>
<li>
<strong>Small bandwidth</strong> <span class="math inline">\(\Rightarrow\)</span> more wiggly function <span class="math inline">\(\Rightarrow\)</span> lower bias, higher variance.</li>
</ul>
<p>Selecting an optimal bandwidth is critical, as it determines the balance between underfitting (high bias) and overfitting (high variance).</p>
<hr>
</div>
<div id="kernel-smoothing-and-local-averages" class="section level3" number="10.2.2">
<h3>
<span class="header-section-number">10.2.2</span> Kernel Smoothing and Local Averages<a class="anchor" aria-label="anchor" href="#kernel-smoothing-and-local-averages"><i class="fas fa-link"></i></a>
</h3>
<p>Many nonparametric regression estimators can be viewed as <strong>weighted local averages</strong> of the observed responses <span class="math inline">\(\{Y_i\}\)</span>. In the univariate case, if <span class="math inline">\(x_i\)</span> are observations of the predictor and <span class="math inline">\(y_i\)</span> are the corresponding responses, the nonparametric estimator at a point <span class="math inline">\(x\)</span> often takes the form:</p>
<p><span class="math display">\[
\hat{m}(x) = \sum_{i=1}^n w_i(x) \, y_i,
\]</span></p>
<p>where the weights <span class="math inline">\(w_i(x)\)</span> depend on the distance between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x\)</span>, and they satisfy:</p>
<p><span class="math display">\[
\sum_{i=1}^n w_i(x) = 1.
\]</span></p>
<p>We will see how this arises more concretely in <a href="sec-nonparametric-regression.html#sec-kernel-regression">kernel regression</a> below.</p>
<hr>
</div>
</div>
<div id="sec-kernel-regression" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Kernel Regression<a class="anchor" aria-label="anchor" href="#sec-kernel-regression"><i class="fas fa-link"></i></a>
</h2>
<div id="basic-setup" class="section level3" number="10.3.1">
<h3>
<span class="header-section-number">10.3.1</span> Basic Setup<a class="anchor" aria-label="anchor" href="#basic-setup"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>kernel function</strong> <span class="math inline">\(K(\cdot)\)</span> is a non-negative, symmetric function whose integral (or sum, in a discrete setting) equals 1. In nonparametric statistics—such as kernel density estimation or local regression—kernels serve as <strong>weighting</strong> mechanisms, assigning higher weights to points closer to the target location and lower weights to points farther away. Specifically, a valid kernel function must satisfy:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Non-negativity</strong>:<br><span class="math display">\[
K(u) \ge 0 \quad \text{for all } u.
\]</span></p></li>
<li><p><strong>Normalization</strong>:<br><span class="math display">\[
\int_{-\infty}^{\infty} K(u)\,du = 1.
\]</span></p></li>
<li><p><strong>Symmetry</strong>:<br><span class="math display">\[
K(u) = K(-u) \quad \text{for all } u.
\]</span></p></li>
</ol>
<p>In practice, the <strong>bandwidth</strong> (sometimes called the smoothing parameter) used alongside a kernel usually has a greater impact on the quality of the estimate than the particular form of the kernel. However, choosing a suitable kernel can still influence computational efficiency and the smoothness of the resulting estimates.</p>
<hr>
<div id="common-kernel-functions" class="section level4" number="10.3.1.1">
<h4>
<span class="header-section-number">10.3.1.1</span> Common Kernel Functions<a class="anchor" aria-label="anchor" href="#common-kernel-functions"><i class="fas fa-link"></i></a>
</h4>
<p>A kernel function essentially measures <em>proximity</em>, assigning higher weights to observations <span class="math inline">\(x_i\)</span> that are close to the target point <span class="math inline">\(x\)</span>, and smaller weights to those farther away.</p>
<ol style="list-style-type: decimal">
<li>Gaussian Kernel <span class="math display">\[
K(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}}.
\]</span>
</li>
</ol>
<ul>
<li><p><strong>Shape</strong>: Bell-shaped and infinite support (i.e., <span class="math inline">\(K(u)\)</span> is technically nonzero for all <span class="math inline">\(u \in (-\infty,\infty)\)</span>), though values decay rapidly as <span class="math inline">\(|u|\)</span> grows.</p></li>
<li><p><strong>Usage</strong>: Due to its smoothness and mathematical convenience (especially in closed-form expressions and asymptotic analysis), it is the most widely used kernel in both density estimation and regression smoothing.</p></li>
<li><p><strong>Properties</strong>: The Gaussian kernel minimizes mean square error in many asymptotic scenarios, making it a common “default choice.”</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Epanechnikov Kernel <span class="math display">\[
K(u) = \begin{cases} \frac{3}{4}(1 - u^2) &amp; \text{if } |u| \le 1,\\ 0 &amp; \text{otherwise}. \end{cases}
\]</span>
</li>
</ol>
<ul>
<li><p><strong>Shape</strong>: Parabolic (inverted) on <span class="math inline">\([-1, 1]\)</span>, dropping to 0 at <span class="math inline">\(|u|=1\)</span>.</p></li>
<li><p><strong>Usage</strong>: Known for being <strong>optimal</strong> in a minimax sense for certain classes of problems, and it is frequently preferred when compact support (zero weights outside <span class="math inline">\(|u|\le 1\)</span>) is desirable.</p></li>
<li><p><strong>Efficiency</strong>: Because it is only supported on a finite interval, computations often involve fewer points (those outside <span class="math inline">\(|u|\le 1\)</span> have zero weight), which can be computationally more efficient in large datasets.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Uniform (or Rectangular) Kernel <span class="math display">\[
K(u) = \begin{cases} \frac{1}{2} &amp; \text{if } |u| \le 1,\\ 0 &amp; \text{otherwise}. \end{cases}
\]</span>
</li>
</ol>
<ul>
<li><p><strong>Shape</strong>: A simple “flat top” distribution on <span class="math inline">\([-1, 1]\)</span>.</p></li>
<li><p><strong>Usage</strong>: Sometimes used for its simplicity. In certain methods (e.g., a “moving average” approach), the uniform kernel equates to giving all points within a fixed window the same weight.</p></li>
<li><p><strong>Drawback</strong>: Lacks smoothness at the boundaries ∣u∣=1|u|=1∣u∣=1, and it can introduce sharper transitions in estimates compared to smoother kernels.</p></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Triangular Kernel <span class="math display">\[
K(u) = \begin{cases} 1 - |u| &amp; \text{if } |u| \le 1,\\ 0 &amp; \text{otherwise}. \end{cases}
\]</span>
</li>
</ol>
<ul>
<li><p><strong>Shape</strong>: Forms a triangle with a peak at <span class="math inline">\(u=0\)</span> and linearly descends to 0 at <span class="math inline">\(|u|=1\)</span>.</p></li>
<li><p><strong>Usage</strong>: Provides a continuous but piecewise-linear alternative to the uniform kernel; places relatively more weight near the center compared to the uniform kernel.</p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Biweight (or Quartic) Kernel <span class="math display">\[
K(u) = \begin{cases} \frac{15}{16} \left(1 - u^2\right)^2 &amp; \text{if } |u| \le 1,\\ 0 &amp; \text{otherwise}. \end{cases}
\]</span>
</li>
</ol>
<ul>
<li><p><strong>Shape</strong>: Smooth and “bump-shaped,” similar to the Epanechnikov but with a steeper drop-off near <span class="math inline">\(|u|=1\)</span>.</p></li>
<li><p><strong>Usage</strong>: Popular when a smoother, polynomial-based kernel with compact support is desired.</p></li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>Cosine Kernel <span class="math display">\[
K(u) = \begin{cases} \frac{\pi}{4}\cos\left(\frac{\pi}{2}u\right) &amp; \text{if } |u| \le 1,\\ 0 &amp; \text{otherwise}. \end{cases}
\]</span>
</li>
</ol>
<ul>
<li><p><strong>Shape</strong>: A single “arch” of a cosine wave on the interval <span class="math inline">\([-1,1]\)</span>.</p></li>
<li><p><strong>Usage</strong>: Used less frequently but can be appealing for certain smoothness criteria or specific signal processing contexts.</p></li>
</ul>
<hr>
<p>Below is a comparison of widely used kernel functions, their functional forms, support, and main characteristics.</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="9%">
<col width="50%">
<col width="10%">
<col width="28%">
</colgroup>
<thead><tr class="header">
<th><strong>Kernel</strong></th>
<th><strong>Formula</strong></th>
<th><strong>Support</strong></th>
<th><strong>Key Characteristics</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Gaussian</strong></td>
<td><span class="math inline">\(\displaystyle K(u) = \frac{1}{\sqrt{2\pi}}\, e^{-\frac{u^2}{2}}\)</span></td>
<td><span class="math inline">\(u \in (-\infty,\infty)\)</span></td>
<td>
<p><strong>Smooth, bell-shaped</strong></p>
<p>Nonzero for all <span class="math inline">\(u\)</span>, but decays quickly</p>
<p>Often the default choice due to favorable analytical properties</p>
</td>
</tr>
<tr class="even">
<td><strong>Epanechnikov</strong></td>
<td><span class="math inline">\(\displaystyle K(u) = \begin{cases}\frac{3}{4}(1 - u^2) &amp; |u|\le 1 \\ 0 &amp; \text{otherwise}\end{cases}\)</span></td>
<td><span class="math inline">\([-1,1]\)</span></td>
<td>
<p><strong>Parabolic shape</strong></p>
<p>Compact support</p>
<p>Minimizes mean integrated squared error in certain theoretical contexts</p>
</td>
</tr>
<tr class="odd">
<td><strong>Uniform</strong></td>
<td><span class="math inline">\(\displaystyle K(u) = \begin{cases}\tfrac{1}{2} &amp; |u|\le 1 \\ 0 &amp; \text{otherwise}\end{cases}\)</span></td>
<td><span class="math inline">\([-1,1]\)</span></td>
<td>
<p><strong>Flat (rectangular) shape</strong></p>
<p>Equal weight for all points within <span class="math inline">\([-1,1]\)</span></p>
<p>Sharp boundary can lead to less smooth estimates</p>
</td>
</tr>
<tr class="even">
<td><strong>Triangular</strong></td>
<td><span class="math inline">\(\displaystyle K(u) = \begin{cases}1 - |u| &amp; |u|\le 1 \\ 0 &amp; \text{otherwise}\end{cases}\)</span></td>
<td><span class="math inline">\([-1,1]\)</span></td>
<td>
<p><strong>Linear decrease</strong> from the center <span class="math inline">\(u=0\)</span> to 0 at <span class="math inline">\(|u|=1\)</span></p>
<p>Compact support</p>
<p>A bit smoother than the uniform kernel</p>
</td>
</tr>
<tr class="odd">
<td>
<strong>Biweight</strong> (Quartic)</td>
<td><span class="math inline">\(\displaystyle K(u) = \begin{cases}\frac{15}{16}(1 - u^2)^2 &amp; |u|\le 1 \\ 0 &amp; \text{otherwise}\end{cases}\)</span></td>
<td><span class="math inline">\([-1,1]\)</span></td>
<td>
<p><strong>Polynomial shape</strong>, smooth</p>
<p>Compact support</p>
<p>Often used for its relatively smooth taper near the boundaries</p>
</td>
</tr>
<tr class="even">
<td><strong>Cosine</strong></td>
<td><span class="math inline">\(\displaystyle K(u) = \begin{cases}\frac{\pi}{4}\cos\left(\frac{\pi}{2}u\right) &amp; |u|\le 1 \\ 0 &amp; \text{otherwise}\end{cases}\)</span></td>
<td><span class="math inline">\([-1,1]\)</span></td>
<td>
<p><strong>Single arch</strong> of a cosine wave</p>
<p>Compact support</p>
<p>Less commonly used, but still mathematically straightforward</p>
</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="additional-details-and-usage-notes" class="section level4" number="10.3.1.2">
<h4>
<span class="header-section-number">10.3.1.2</span> Additional Details and Usage Notes<a class="anchor" aria-label="anchor" href="#additional-details-and-usage-notes"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Smoothness and Differentiability</strong>
<ul>
<li>Kernels with <strong>infinite support</strong> (like the Gaussian) can yield very smooth estimates but require summing over (practically) all data points.</li>
<li>Kernels with <strong>compact support</strong> (like Epanechnikov, biweight, triangular, etc.) go to zero outside a fixed interval. This can make computations more efficient since only data within a certain range of the target point matter.</li>
</ul>
</li>
<li>
<strong>Choice of Kernel vs. Choice of Bandwidth</strong>
<ul>
<li>While the kernel shape does have some effect on the estimator’s smoothness, <strong>the choice of bandwidth (sometimes denoted</strong> <span class="math inline">\(h\)</span>) is typically more critical. If <span class="math inline">\(h\)</span> is too large, the estimate can be excessively smooth (high bias). If <span class="math inline">\(h\)</span> is too small, the estimate can exhibit high variance or appear “noisy.”</li>
</ul>
</li>
<li>
<strong>Local Weighting Principle</strong>
<ul>
<li>At a target location <span class="math inline">\(x\)</span>, a kernel function <span class="math inline">\(K\bigl(\frac{x - x_i}{h}\bigr)\)</span> down-weights data points <span class="math inline">\((x_i)\)</span> that are farther from <span class="math inline">\(x\)</span>. Nearer points have larger kernel values, hence exert greater influence on the local estimate.</li>
</ul>
</li>
<li>
<strong>Interpretation in Density Estimation</strong>
<ul>
<li>In <strong>kernel density estimation</strong>, each data point contributes a small “bump” (shaped by the kernel) to the overall density. Summing or integrating these bumps yields a continuous estimate of the underlying density function, in contrast to discrete histograms.</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="sec-nadaraya-watson-kernel-estimator" class="section level3" number="10.3.2">
<h3>
<span class="header-section-number">10.3.2</span> Nadaraya-Watson Kernel Estimator<a class="anchor" aria-label="anchor" href="#sec-nadaraya-watson-kernel-estimator"><i class="fas fa-link"></i></a>
</h3>
<p>The most widely used kernel-based regression estimator is the <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson estimator</a> <span class="citation">(<a href="references.html#ref-nadaraya1964estimating">Nadaraya 1964</a>; <a href="references.html#ref-watson1964smooth">Watson 1964</a>)</span>, defined as:</p>
<p><span class="math display">\[
\hat{m}_h(x) = \frac{\sum_{i=1}^n K\!\left(\frac{x - x_i}{h}\right) y_i}{\sum_{i=1}^n K\!\left(\frac{x - x_i}{h}\right)},
\]</span></p>
<p>where <span class="math inline">\(h &gt; 0\)</span> is the <strong>bandwidth parameter</strong>. Intuitively, this formula computes a weighted average of the observed <span class="math inline">\(y_i\)</span> values, with weights determined by the kernel function applied to the scaled distance between <span class="math inline">\(x\)</span> and each <span class="math inline">\(x_i\)</span>.</p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>When <span class="math inline">\(|x - x_i|\)</span> is <strong>small</strong> (i.e., <span class="math inline">\(x_i\)</span> is close to <span class="math inline">\(x\)</span>), the kernel value <span class="math inline">\(K\!\left(\frac{x - x_i}{h}\right)\)</span> is <strong>large</strong>, giving more weight to <span class="math inline">\(y_i\)</span>.</li>
<li>When <span class="math inline">\(|x - x_i|\)</span> is <strong>large</strong>, the kernel value becomes <strong>small</strong> (or even zero for compactly supported kernels like the Epanechnikov), reducing the influence of <span class="math inline">\(y_i\)</span> on <span class="math inline">\(\hat{m}_h(x)\)</span>.</li>
</ul>
<p>Thus, observations near <span class="math inline">\(x\)</span> have a larger impact on the estimated value <span class="math inline">\(\hat{m}_h(x)\)</span> than distant ones.</p>
<hr>
<div id="weights-representation" class="section level4" number="10.3.2.1">
<h4>
<span class="header-section-number">10.3.2.1</span> Weights Representation<a class="anchor" aria-label="anchor" href="#weights-representation"><i class="fas fa-link"></i></a>
</h4>
<p>We can define the <strong>normalized weights</strong>:</p>
<p><span class="math display">\[
w_i(x) = \frac{K\!\left(\frac{x - x_i}{h}\right)}{\sum_{j=1}^n K\!\left(\frac{x - x_j}{h}\right)},
\]</span></p>
<p>so that the estimator can be rewritten as:</p>
<p><span class="math display">\[
\hat{m}_h(x) = \sum_{i=1}^n w_i(x) y_i,
\]</span></p>
<p>where <span class="math inline">\(\sum_{i=1}^n w_i(x) = 1\)</span> for any <span class="math inline">\(x\)</span>. Notice that <span class="math inline">\(0 \le w_i(x) \le 1\)</span> for all <span class="math inline">\(i\)</span>.</p>
</div>
</div>
<div id="sec-priestley-chao-kernel-estimator" class="section level3" number="10.3.3">
<h3>
<span class="header-section-number">10.3.3</span> Priestley–Chao Kernel Estimator<a class="anchor" aria-label="anchor" href="#sec-priestley-chao-kernel-estimator"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Priestley–Chao kernel estimator</strong> <span class="citation">(<a href="references.html#ref-priestley1972non">Priestley and Chao 1972</a>)</span> is an early kernel-based regression estimator designed to estimate the regression function <span class="math inline">\(m(x)\)</span> from observed data <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span>. Unlike the <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya–Watson estimator</a>, which uses pointwise kernel weighting, the Priestley–Chao estimator incorporates <strong>differences in the predictor variable</strong> to approximate integrals more accurately.</p>
<p>The estimator is defined as:</p>
<p><span class="math display">\[
\hat{m}_h(x) = \frac{1}{h} \sum_{i=1}^{n-1} K\!\left(\frac{x - x_i}{h}\right) \cdot (x_{i+1} - x_i) \cdot y_i,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(K(\cdot)\)</span> is a kernel function,</p></li>
<li><p><span class="math inline">\(h &gt; 0\)</span> is the <strong>bandwidth parameter</strong>,</p></li>
<li><p><span class="math inline">\((x_{i+1} - x_i)\)</span> represents the <strong>spacing between consecutive observations</strong>.</p></li>
</ul>
<div id="interpretation-1" class="section level4" number="10.3.3.1">
<h4>
<span class="header-section-number">10.3.3.1</span> <strong>Interpretation</strong><a class="anchor" aria-label="anchor" href="#interpretation-1"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>The estimator can be viewed as a <strong>Riemann sum approximation</strong> of an integral, where the kernel-weighted <span class="math inline">\(y_i\)</span> values are scaled by the spacing <span class="math inline">\((x_{i+1} - x_i)\)</span>.</li>
<li>Observations where <span class="math inline">\(x_i\)</span> is <strong>close to</strong> <span class="math inline">\(x\)</span> receive more weight due to the kernel function.</li>
<li>The inclusion of <span class="math inline">\((x_{i+1} - x_i)\)</span> accounts for <strong>non-uniform spacing</strong> in the data, making the estimator more accurate when the predictor values are irregularly spaced.</li>
</ul>
<p>This estimator is particularly useful when the design points <span class="math inline">\(\{x_i\}\)</span> are <strong>unevenly distributed</strong>.</p>
</div>
<div id="weights-representation-1" class="section level4" number="10.3.3.2">
<h4>
<span class="header-section-number">10.3.3.2</span> <strong>Weights Representation</strong><a class="anchor" aria-label="anchor" href="#weights-representation-1"><i class="fas fa-link"></i></a>
</h4>
<p>We can express the estimator as a <strong>weighted sum</strong> of the observed responses <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[
\hat{m}_h(x) = \sum_{i=1}^{n-1} w_i(x) \, y_i,
\]</span></p>
<p>where the weights are defined as:</p>
<p><span class="math display">\[
w_i(x) = \frac{1}{h} \cdot K\!\left(\frac{x - x_i}{h}\right) \cdot (x_{i+1} - x_i).
\]</span></p>
<p><strong>Properties of the weights:</strong></p>
<ul>
<li><p><strong>Non-negativity:</strong> If <span class="math inline">\(K(u) \ge 0\)</span>, then <span class="math inline">\(w_i(x) \ge 0\)</span>.</p></li>
<li><p><strong>Adaptation to spacing:</strong> Larger gaps <span class="math inline">\((x_{i+1} - x_i)\)</span> increase the corresponding weight.</p></li>
<li><p>Unlike <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya–Watson</a>, the weights <strong>do not sum to 1</strong>, as they approximate an integral rather than a normalized average.</p></li>
</ul>
<hr>
</div>
</div>
<div id="sec-gasser-mueller-kernel-estimator" class="section level3" number="10.3.4">
<h3>
<span class="header-section-number">10.3.4</span> Gasser–Müller Kernel Estimator<a class="anchor" aria-label="anchor" href="#sec-gasser-mueller-kernel-estimator"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Gasser–Müller kernel estimator</strong> <span class="citation">(<a href="references.html#ref-gasser1979kernel">Gasser and Müller 1979</a>)</span> improves upon the <a href="sec-nonparametric-regression.html#sec-priestley-chao-kernel-estimator">Priestley–Chao estimator</a> by using a <strong>cumulative kernel function</strong> to smooth over the predictor space. This estimator is particularly effective for <strong>irregularly spaced data</strong> and aims to reduce bias at the boundaries.</p>
<p>The estimator is defined as:</p>
<p><span class="math display">\[
\hat{m}_h(x) = \frac{1}{h} \sum_{i=1}^{n-1} \left[ K^*\!\left(\frac{x - x_i}{h}\right) - K^*\!\left(\frac{x - x_{i+1}}{h}\right) \right] \cdot y_i,
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(K^*(u) = \int_{-\infty}^{u} K(v) \, dv\)</span> is the <strong>cumulative distribution function (CDF)</strong> of the kernel <span class="math inline">\(K\)</span>,</p></li>
<li><p><span class="math inline">\(h &gt; 0\)</span> is the <strong>bandwidth parameter</strong>.</p></li>
</ul>
<div id="interpretation-2" class="section level4" number="10.3.4.1">
<h4>
<span class="header-section-number">10.3.4.1</span> <strong>Interpretation</strong><a class="anchor" aria-label="anchor" href="#interpretation-2"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>The estimator computes the <strong>difference of cumulative kernel functions</strong> at two consecutive design points, effectively assigning weight to the interval between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_{i+1}\)</span>.</li>
<li>Observations contribute more to <span class="math inline">\(\hat{m}_h(x)\)</span> when <span class="math inline">\(x\)</span> lies <strong>between</strong> <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_{i+1}\)</span>, with the contribution decreasing as the distance from <span class="math inline">\(x\)</span> increases.</li>
<li>This method smooths over <strong>intervals</strong> rather than just at points, reducing bias near the boundaries and improving performance with unevenly spaced data.</li>
</ul>
</div>
<div id="weights-representation-2" class="section level4" number="10.3.4.2">
<h4>
<span class="header-section-number">10.3.4.2</span> <strong>Weights Representation</strong><a class="anchor" aria-label="anchor" href="#weights-representation-2"><i class="fas fa-link"></i></a>
</h4>
<p>The <a href="sec-nonparametric-regression.html#sec-gasser-mueller-kernel-estimator">Gasser–Müller estimator</a> can also be expressed as a <strong>weighted sum</strong>:</p>
<p><span class="math display">\[
\hat{m}_h(x) = \sum_{i=1}^{n-1} w_i(x) \, y_i,
\]</span></p>
<p>where the weights are:</p>
<p><span class="math display">\[
w_i(x) = \frac{1}{h} \left[ K^*\!\left(\frac{x - x_i}{h}\right) - K^*\!\left(\frac{x - x_{i+1}}{h}\right) \right].
\]</span></p>
<p><strong>Properties of the weights:</strong></p>
<ul>
<li><p><strong>Non-negativity:</strong> The weights are non-negative if <span class="math inline">\(K^*\)</span> is non-decreasing (which holds if <span class="math inline">\(K\)</span> is non-negative).</p></li>
<li><p><strong>Adaptation to spacing:</strong> The weights account for the spacing between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_{i+1}\)</span>.</p></li>
<li><p>Similar to the <a href="sec-nonparametric-regression.html#sec-priestley-chao-kernel-estimator">Priestley–Chao estimator</a>, the weights <strong>do not sum to 1</strong> because the estimator approximates an integral rather than a normalized sum.</p></li>
</ul>
<hr>
</div>
</div>
<div id="comparison-of-kernel-based-estimators" class="section level3" number="10.3.5">
<h3>
<span class="header-section-number">10.3.5</span> Comparison of Kernel-Based Estimators<a class="anchor" aria-label="anchor" href="#comparison-of-kernel-based-estimators"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="9%">
<col width="63%">
<col width="16%">
<col width="10%">
</colgroup>
<thead><tr class="header">
<th><strong>Estimator</strong></th>
<th><strong>Formula</strong></th>
<th><strong>Key Feature</strong></th>
<th><strong>Weights Sum to 1?</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Nadaraya–Watson</strong></td>
<td><span class="math inline">\(\displaystyle \hat{m}_h(x) = \frac{\sum K\left(\frac{x - x_i}{h}\right) y_i}{\sum K\left(\frac{x - x_i}{h}\right)}\)</span></td>
<td>Weighted average of <span class="math inline">\(y_i\)</span>
</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><strong>Priestley–Chao</strong></td>
<td><span class="math inline">\(\displaystyle \hat{m}_h(x) = \frac{1}{h} \sum K\left(\frac{x - x_i}{h}\right)(x_{i+1} - x_i) y_i\)</span></td>
<td>Incorporates data spacing</td>
<td>No</td>
</tr>
<tr class="odd">
<td><strong>Gasser–Müller</strong></td>
<td><span class="math inline">\(\displaystyle \hat{m}_h(x) = \frac{1}{h} \sum \left[K^*\left(\frac{x - x_i}{h}\right) - K^*\left(\frac{x - x_{i+1}}{h}\right)\right] y_i\)</span></td>
<td>Uses cumulative kernel differences</td>
<td>No</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="bandwidth-selection" class="section level3" number="10.3.6">
<h3>
<span class="header-section-number">10.3.6</span> Bandwidth Selection<a class="anchor" aria-label="anchor" href="#bandwidth-selection"><i class="fas fa-link"></i></a>
</h3>
<p>The choice of <strong>bandwidth</strong> <span class="math inline">\(h\)</span> is crucial because it controls the trade-off between bias and variance:</p>
<ul>
<li>If <span class="math inline">\(h\)</span> is <strong>too large</strong>, the estimator becomes <strong>overly smooth</strong>, incorporating too many distant data points. This leads to <strong>high bias</strong> but <strong>low variance</strong>.</li>
<li>If <span class="math inline">\(h\)</span> is <strong>too small</strong>, the estimator becomes <strong>noisy</strong> and sensitive to fluctuations in the data, resulting in <strong>low bias</strong> but <strong>high variance</strong>.</li>
</ul>
<div id="mean-squared-error-and-optimal-bandwidth" class="section level4" number="10.3.6.1">
<h4>
<span class="header-section-number">10.3.6.1</span> Mean Squared Error and Optimal Bandwidth<a class="anchor" aria-label="anchor" href="#mean-squared-error-and-optimal-bandwidth"><i class="fas fa-link"></i></a>
</h4>
<p>To analyze the performance of kernel estimators, we often examine the <strong>mean integrated squared error</strong> (MISE):</p>
<p><span class="math display">\[
\text{MISE}(\hat{m}_h) = \mathbb{E}\left[\int \left\{\hat{m}_h(x) - m(x)\right\}^2 dx \right].
\]</span></p>
<p>As <span class="math inline">\(n \to \infty\)</span>, under smoothness assumptions on <span class="math inline">\(m(x)\)</span> and regularity conditions on the kernel <span class="math inline">\(K\)</span>, the MISE has the following asymptotic expansion:</p>
<p><span class="math display">\[
\text{MISE}(\hat{m}_h) \approx \frac{R(K)}{n h} \, \sigma^2 + \frac{1}{4} \mu_2^2(K) \, h^4 \int \left\{m''(x)\right\}^2 dx,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(R(K) = \int_{-\infty}^{\infty} K(u)^2 du\)</span> measures the <strong>roughness</strong> of the kernel.</li>
<li>
<span class="math inline">\(\mu_2(K) = \int_{-\infty}^{\infty} u^2 K(u) du\)</span> is the <strong>second moment</strong> of the kernel (related to its spread).</li>
<li>
<span class="math inline">\(\sigma^2\)</span> is the <strong>variance of the noise</strong>, assuming <span class="math inline">\(\operatorname{Var}(\varepsilon \mid X = x) = \sigma^2\)</span>.</li>
<li>
<span class="math inline">\(m''(x)\)</span> is the <strong>second derivative</strong> of the true regression function <span class="math inline">\(m(x)\)</span>.</li>
</ul>
<p>To find the <strong>asymptotically optimal bandwidth</strong>, we differentiate the MISE with respect to <span class="math inline">\(h\)</span>, set the derivative to zero, and solve for <span class="math inline">\(h\)</span>:</p>
<p><span class="math display">\[
h_{\mathrm{opt}} = \left(\frac{R(K) \, \sigma^2}{\mu_2^2(K) \int \left\{m''(x)\right\}^2 dx} \cdot \frac{1}{n}\right)^{1/5}.
\]</span></p>
<p>In practice, <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\int \{m''(x)\}^2 dx\)</span> are unknown and must be estimated from data. A common data-driven approach is <a href="sec-nonparametric-regression.html#sec-cross-validation-kernel-regression">cross-validation</a>.</p>
</div>
<div id="sec-cross-validation-kernel-regression" class="section level4" number="10.3.6.2">
<h4>
<span class="header-section-number">10.3.6.2</span> Cross-Validation<a class="anchor" aria-label="anchor" href="#sec-cross-validation-kernel-regression"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>leave-one-out cross-validation</strong> (LOOCV) method is widely used for bandwidth selection:</p>
<ol style="list-style-type: decimal">
<li>For each <span class="math inline">\(i = 1, \dots, n\)</span>, fit the kernel estimator <span class="math inline">\(\hat{m}_{h,-i}(x)\)</span> using all data <strong>except</strong> the <span class="math inline">\(i\)</span>-th observation <span class="math inline">\((x_i, y_i)\)</span>.</li>
<li>Compute the squared prediction error for the left-out point: <span class="math inline">\((y_i - \hat{m}_{h,-i}(x_i))^2\)</span>.</li>
<li>Average these errors across all observations:</li>
</ol>
<p><span class="math display">\[
\mathrm{CV}(h) = \frac{1}{n} \sum_{i=1}^n \left\{y_i - \hat{m}_{h,-i}(x_i)\right\}^2.
\]</span></p>
<p>The bandwidth <span class="math inline">\(h\)</span> that minimizes <span class="math inline">\(\mathrm{CV}(h)\)</span> is selected as the optimal bandwidth.</p>
</div>
</div>
<div id="asymptotic-properties" class="section level3" number="10.3.7">
<h3>
<span class="header-section-number">10.3.7</span> Asymptotic Properties<a class="anchor" aria-label="anchor" href="#asymptotic-properties"><i class="fas fa-link"></i></a>
</h3>
<p>For the <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson estimator</a>, under regularity conditions and assuming <span class="math inline">\(h \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span> (but not too fast), we have:</p>
<ul>
<li><p><strong>Consistency:</strong> <span class="math display">\[
\hat{m}_h(x) \overset{p}{\longrightarrow} m(x),
\]</span> meaning the estimator converges in probability to the true regression function.</p></li>
<li><p><strong>Rate of Convergence:</strong> The mean squared error (MSE) decreases at the rate: <span class="math display">\[
\text{MSE}(\hat{m}_h(x)) = O\left(n^{-4/5}\right)
\]</span> in the one-dimensional case. This rate results from balancing the variance term (<span class="math inline">\(O(1/(nh))\)</span>) and the squared bias term (<span class="math inline">\(O(h^4)\)</span>).</p></li>
</ul>
</div>
<div id="derivation-of-the-nadaraya-watson-estimator" class="section level3" number="10.3.8">
<h3>
<span class="header-section-number">10.3.8</span> Derivation of the Nadaraya-Watson Estimator<a class="anchor" aria-label="anchor" href="#derivation-of-the-nadaraya-watson-estimator"><i class="fas fa-link"></i></a>
</h3>
<p>The <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson estimator</a> can be derived from a <strong>density-based perspective</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>By the definition of conditional expectation: <span class="math display">\[
m(x) = \mathbb{E}[Y \mid X = x] = \frac{\int y \, f_{X,Y}(x, y) \, dy}{f_X(x)},
\]</span> where <span class="math inline">\(f_{X,Y}(x, y)\)</span> is the joint density of <span class="math inline">\((X, Y)\)</span>, and <span class="math inline">\(f_X(x)\)</span> is the marginal density of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Estimate <span class="math inline">\(f_X(x)\)</span> using a kernel density estimator: <span class="math display">\[
\hat{f}_X(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h} K\!\left(\frac{x - x_i}{h}\right).
\]</span></p></li>
<li><p>Estimate the joint density <span class="math inline">\(f_{X,Y}(x, y)\)</span>: <span class="math display">\[
\hat{f}_{X,Y}(x, y) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h} K\!\left(\frac{x - x_i}{h}\right) \delta_{y_i}(y),
\]</span> where <span class="math inline">\(\delta_{y_i}(y)\)</span> is the Dirac delta function (a point mass at <span class="math inline">\(y_i\)</span>).</p></li>
<li><p>The kernel regression estimator becomes: <span class="math display">\[
\hat{m}_h(x) = \frac{\int y \, \hat{f}_{X,Y}(x, y) \, dy}{\hat{f}_X(x)}
= \frac{\sum_{i=1}^n K\!\left(\frac{x - x_i}{h}\right) y_i}{\sum_{i=1}^n K\!\left(\frac{x - x_i}{h}\right)},
\]</span> which is exactly the <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson estimator</a>.</p></li>
</ol>
<hr>
<div class="sourceCode" id="cb363"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># 1. Simulate Data </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Generate predictor x and response y</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Sorted for Priestley–Chao and Gasser–Müller</span></span>
<span><span class="va">true_function</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span>  <span class="co"># True regression function</span></span>
<span></span>
<span><span class="co"># Add Gaussian noise</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">true_function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span>              </span>
<span></span>
<span><span class="co"># Visualization of the data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"darkblue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu">true_function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Simulated Data with True Regression Function"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"x"</span>, y <span class="op">=</span> <span class="st">"y"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb364"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Gaussian Kernel Function</span></span>
<span><span class="va">gaussian_kernel</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">u</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="va">u</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Epanechnikov Kernel Function</span></span>
<span><span class="va">epanechnikov_kernel</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">u</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">u</span><span class="op">)</span> <span class="op">&lt;=</span> <span class="fl">1</span>, <span class="fl">0.75</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">u</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Cumulative Kernel for Gasser–Müller (CDF of Gaussian Kernel)</span></span>
<span><span class="va">gaussian_cdf_kernel</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">u</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="va">u</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="sourceCode" id="cb365"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Nadaraya-Watson Estimator</span></span>
<span><span class="va">nadaraya_watson</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x_eval</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">h</span>, <span class="va">kernel</span> <span class="op">=</span> <span class="va">gaussian_kernel</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">x_eval</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x0</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/kernel.html">kernel</a></span><span class="op">(</span><span class="op">(</span><span class="va">x0</span> <span class="op">-</span> <span class="va">x</span><span class="op">)</span> <span class="op">/</span> <span class="va">h</span><span class="op">)</span></span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">weights</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">weights</span><span class="op">)</span></span>
<span>        <span class="op">}</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span><span class="co"># Bandwidth Selection (fixed for simplicity)</span></span>
<span><span class="va">h_nw</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># Bandwidth for Nadaraya–Watson</span></span>
<span></span>
<span><span class="co"># Apply Nadaraya–Watson Estimator</span></span>
<span><span class="va">x_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">10</span>, length.out <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span><span class="va">nw_estimate</span> <span class="op">&lt;-</span> <span class="fu">nadaraya_watson</span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">h_nw</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot Nadaraya–Watson Estimate</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">nw_estimate</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>              linewidth <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Nadaraya–Watson Kernel Estimator"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Bandwidth (h) ="</span>, <span class="va">h_nw</span><span class="op">)</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Nadaraya%E2%80%93Watson%20Kernel%20Estimator-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>The <strong>green curve</strong> is the <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya–Watson estimate</a>.</p></li>
<li><p>The <strong>dashed red line</strong> is the true regression function.</p></li>
<li><p>The <strong>blue dots</strong> are the observed noisy data.</p></li>
<li><p>The estimator <strong>smooths the data</strong>, assigning more weight to points close to each evaluation point based on the Gaussian kernel.</p></li>
</ul>
<div class="sourceCode" id="cb366"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Priestley–Chao Estimator</span></span>
<span><span class="va">priestley_chao</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x_eval</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">h</span>, <span class="va">kernel</span> <span class="op">=</span> <span class="va">gaussian_kernel</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">x_eval</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x0</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/kernel.html">kernel</a></span><span class="op">(</span><span class="op">(</span><span class="va">x0</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="op">/</span> <span class="va">h</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">weights</span> <span class="op">*</span> <span class="va">y</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="op">/</span> <span class="va">h</span></span>
<span>        <span class="op">}</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span><span class="co"># Apply Priestley–Chao Estimator</span></span>
<span><span class="va">h_pc</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">pc_estimate</span> <span class="op">&lt;-</span> <span class="fu">priestley_chao</span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">h_pc</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot Priestley–Chao Estimate</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">pc_estimate</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"orange"</span>,</span>
<span>              size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Priestley–Chao Kernel Estimator"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Bandwidth (h) ="</span>, <span class="va">h_pc</span><span class="op">)</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Priestley%E2%80%93Chao%20Kernel%20Estimator-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>The <strong>orange curve</strong> is the <a href="sec-nonparametric-regression.html#sec-priestley-chao-kernel-estimator">Priestley–Chao</a> estimate.</p></li>
<li><p>This estimator incorporates the <strong>spacing between consecutive data points</strong> (<code>diff(x)</code>), making it more sensitive to <strong>non-uniform data spacing</strong>.</p></li>
<li><p>It performs similarly to <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya–Watson</a> when data are evenly spaced.</p></li>
</ul>
<div class="sourceCode" id="cb367"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Gasser–Müller Estimator</span></span>
<span><span class="va">gasser_mueller</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x_eval</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">h</span>, <span class="va">cdf_kernel</span> <span class="op">=</span> <span class="va">gaussian_cdf_kernel</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">x_eval</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x0</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">weights</span> <span class="op">&lt;-</span></span>
<span>                <span class="op">(</span><span class="fu">cdf_kernel</span><span class="op">(</span><span class="op">(</span><span class="va">x0</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="op">/</span> <span class="va">h</span><span class="op">)</span> <span class="op">-</span> <span class="fu">cdf_kernel</span><span class="op">(</span><span class="op">(</span><span class="va">x0</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">/</span> <span class="va">h</span><span class="op">)</span><span class="op">)</span></span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">weights</span> <span class="op">*</span> <span class="va">y</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> <span class="op">/</span> <span class="va">h</span></span>
<span>        <span class="op">}</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span><span class="co"># Apply Gasser–Müller Estimator</span></span>
<span><span class="va">h_gm</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span><span class="va">gm_estimate</span> <span class="op">&lt;-</span> <span class="fu">gasser_mueller</span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">h_gm</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot Gasser–Müller Estimate</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">gm_estimate</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>              size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Gasser–Müller Kernel Estimator"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Bandwidth (h) ="</span>, <span class="va">h_gm</span><span class="op">)</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Gasser%E2%80%93M%C3%BCller%20Kernel%20Estimator-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>The <strong>purple curve</strong> is the Gasser–Müller estimate.</p></li>
<li><p>This estimator uses <strong>cumulative kernel functions</strong> to handle <strong>irregular data spacing</strong> and <strong>reduce boundary bias</strong>.</p></li>
<li><p>It performs well when data are unevenly distributed.</p></li>
</ul>
<div class="sourceCode" id="cb368"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Combine all estimates for comparison</span></span>
<span><span class="va">estimates_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">x_grid</span>,</span>
<span>    Nadaraya_Watson <span class="op">=</span> <span class="va">nw_estimate</span>,</span>
<span>    Priestley_Chao <span class="op">=</span> <span class="va">pc_estimate</span>,</span>
<span>    Gasser_Mueller <span class="op">=</span> <span class="va">gm_estimate</span>,</span>
<span>    True_Function <span class="op">=</span> <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot all estimators together</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"gray60"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">Nadaraya_Watson</span>, color <span class="op">=</span> <span class="st">"Nadaraya–Watson"</span><span class="op">)</span>,</span>
<span>            data <span class="op">=</span> <span class="va">estimates_df</span>, size <span class="op">=</span> <span class="fl">1.1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">Priestley_Chao</span>, color <span class="op">=</span> <span class="st">"Priestley–Chao"</span><span class="op">)</span>,</span>
<span>            data <span class="op">=</span> <span class="va">estimates_df</span>, size <span class="op">=</span> <span class="fl">1.1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">Gasser_Mueller</span>, color <span class="op">=</span> <span class="st">"Gasser–Müller"</span><span class="op">)</span>,</span>
<span>            data <span class="op">=</span> <span class="va">estimates_df</span>, size <span class="op">=</span> <span class="fl">1.1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">True_Function</span>, color <span class="op">=</span> <span class="st">"True Function"</span><span class="op">)</span>,</span>
<span>            data <span class="op">=</span> <span class="va">estimates_df</span>, linetype <span class="op">=</span> <span class="st">"dashed"</span>, size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span></span>
<span>    name <span class="op">=</span> <span class="st">"Estimator"</span>,</span>
<span>    values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Nadaraya–Watson"</span> <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>               <span class="st">"Priestley–Chao"</span>   <span class="op">=</span> <span class="st">"orange"</span>,</span>
<span>               <span class="st">"Gasser–Müller"</span>    <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>               <span class="st">"True Function"</span>    <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Comparison of Kernel-Based Regression Estimators"</span>,</span>
<span>    x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Comparing%20All%20Estimators-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p><strong>All estimators</strong> approximate the true function well when the bandwidth is appropriately chosen.</p></li>
<li><p>The <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya–Watson estimator</a> is sensitive to bandwidth selection and assumes uniform data spacing.</p></li>
<li><p>The <a href="sec-nonparametric-regression.html#sec-priestley-chao-kernel-estimator">Priestley–Chao estimator</a> accounts for <strong>data spacing</strong>, making it more flexible with uneven data.</p></li>
<li><p>The <a href="sec-nonparametric-regression.html#sec-gasser-mueller-kernel-estimator">Gasser–Müller estimator</a> reduces <strong>boundary bias</strong> and handles <strong>irregular data</strong> effectively.</p></li>
</ul>
<div class="sourceCode" id="cb369"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Cross-validation for bandwidth selection (for Nadaraya–Watson)</span></span>
<span><span class="va">cv_bandwidth</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">h</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">kernel</span> <span class="op">=</span> <span class="va">gaussian_kernel</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span>  <span class="va">cv_error</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>    <span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>    <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="fu">nadaraya_watson</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">x_train</span>, <span class="va">y_train</span>, <span class="va">h</span>, <span class="va">kernel</span><span class="op">)</span></span>
<span>    <span class="va">cv_error</span> <span class="op">&lt;-</span> <span class="va">cv_error</span> <span class="op">+</span> <span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">-</span> <span class="va">y_pred</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">cv_error</span> <span class="op">/</span> <span class="va">n</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Optimize bandwidth</span></span>
<span><span class="va">bandwidths</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">2</span>, by <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="va">cv_errors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">bandwidths</span>, <span class="va">cv_bandwidth</span>, x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Optimal bandwidth</span></span>
<span><span class="va">optimal_h</span> <span class="op">&lt;-</span> <span class="va">bandwidths</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="va">cv_errors</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">optimal_h</span></span>
<span><span class="co">#&gt; [1] 0.3</span></span>
<span></span>
<span><span class="co"># Plot CV errors</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">bandwidths</span>, <span class="va">cv_errors</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">bandwidths</span>, <span class="va">cv_errors</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">optimal_h</span>, y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">cv_errors</span><span class="op">)</span><span class="op">)</span>,</span>
<span>               color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>               size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Cross-Validation for Bandwidth Selection"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Bandwidth (h)"</span>, y <span class="op">=</span> <span class="st">"CV Error"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Cross-Validation%20for%20Bandwidth%20Selection%20(Optional)-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>The <strong>red point</strong> indicates the <strong>optimal bandwidth</strong> that minimizes the cross-validation error.</p></li>
<li><p>Selecting the <strong>right bandwidth</strong> is critical, as it balances <strong>bias and variance</strong> in the estimator.</p></li>
</ul>
<hr>
</div>
</div>
<div id="sec-local-polynomial-regression" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> Local Polynomial Regression<a class="anchor" aria-label="anchor" href="#sec-local-polynomial-regression"><i class="fas fa-link"></i></a>
</h2>
<p>While the <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson estimator</a> is effectively a <strong>local constant</strong> estimator (it approximates <span class="math inline">\(m(x)\)</span> by a constant in a small neighborhood of <span class="math inline">\(x\)</span>), <strong>local polynomial regression</strong> extends this idea by fitting a <strong>local polynomial</strong> around each point <span class="math inline">\(x\)</span>. The advantage of local polynomials is that they can better handle boundary bias and can capture local curvature more effectively.</p>
<div id="local-polynomial-fitting" class="section level3" number="10.4.1">
<h3>
<span class="header-section-number">10.4.1</span> Local Polynomial Fitting<a class="anchor" aria-label="anchor" href="#local-polynomial-fitting"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>local polynomial regression</strong> of degree <span class="math inline">\(p\)</span> at point <span class="math inline">\(x\)</span> estimates a polynomial function:</p>
<p><span class="math display">\[
m_x(t) = \beta_0 + \beta_1 (t - x) + \beta_2 (t - x)^2 + \cdots + \beta_p (t - x)^p
\]</span></p>
<p>that best fits the data <span class="math inline">\(\{(x_i, y_i)\}\)</span> within a neighborhood of <span class="math inline">\(x\)</span>, <strong>weighted by a kernel</strong>. Specifically, we solve:</p>
<p><span class="math display">\[
(\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p) = \underset{\beta_0, \ldots, \beta_p}{\arg\min} \sum_{i=1}^n \left[y_i - \left\{\beta_0 + \beta_1 (x_i - x) + \cdots + \beta_p (x_i - x)^p\right\}\right]^2 \, K\!\left(\frac{x_i - x}{h}\right).
\]</span></p>
<p>We then estimate:</p>
<p><span class="math display">\[
\hat{m}(x) = \hat{\beta}_0,
\]</span></p>
<p>because <span class="math inline">\(\beta_0\)</span> is the constant term of the local polynomial expansion around <span class="math inline">\(x\)</span>, which represents the estimated value at that point.</p>
<p><strong>Why center the polynomial at</strong> <span class="math inline">\(x\)</span> rather than 0?<br>
Centering at <span class="math inline">\(x\)</span> ensures that the fitted polynomial provides the best approximation <em>around</em> <span class="math inline">\(x\)</span>. This is conceptually similar to a Taylor expansion, where local approximations are most accurate near the point of expansion.</p>
</div>
<div id="mathematical-form-of-the-solution" class="section level3" number="10.4.2">
<h3>
<span class="header-section-number">10.4.2</span> Mathematical Form of the Solution<a class="anchor" aria-label="anchor" href="#mathematical-form-of-the-solution"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(\mathbf{X}_x\)</span> be the design matrix for the local polynomial expansion at point <span class="math inline">\(x\)</span>. For a polynomial of degree <span class="math inline">\(p\)</span>, each row <span class="math inline">\(i\)</span> of <span class="math inline">\(\mathbf{X}_x\)</span> is:</p>
<p><span class="math display">\[
\bigl(1,\; x_i - x,\; (x_i - x)^2,\; \ldots,\; (x_i - x)^p \bigr).
\]</span></p>
<p>Define <span class="math inline">\(\mathbf{W}_x\)</span> as the diagonal matrix with entries:</p>
<p><span class="math display">\[
(\mathbf{W}_x)_{ii} = K\!\left(\frac{x_i - x}{h}\right),
\]</span></p>
<p>representing the kernel weights. The parameter vector <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)^T\)</span> is estimated via <a href="linear-regression.html#weighted-least-squares">weighted least squares</a>:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}(x) = \left(\mathbf{X}_x^T \mathbf{W}_x \mathbf{X}_x\right)^{-1} \mathbf{X}_x^T \mathbf{W}_x \mathbf{y},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{y} = (y_1, y_2, \ldots, y_n)^T\)</span>. The local polynomial estimator of <span class="math inline">\(m(x)\)</span> is given by:</p>
<p><span class="math display">\[
\hat{m}(x) = \hat{\beta}_0(x).
\]</span></p>
<p>Alternatively, we can express this concisely using a selection vector:</p>
<p><span class="math display">\[
\hat{m}(x) = \mathbf{e}_1^T \left(\mathbf{X}_x^T \mathbf{W}_x \mathbf{X}_x\right)^{-1} \mathbf{X}_x^T \mathbf{W}_x \mathbf{y},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{e}_1 = (1, 0, \ldots, 0)^T\)</span> picks out the intercept term.</p>
</div>
<div id="bias-variance-and-asymptotics" class="section level3" number="10.4.3">
<h3>
<span class="header-section-number">10.4.3</span> Bias, Variance, and Asymptotics<a class="anchor" aria-label="anchor" href="#bias-variance-and-asymptotics"><i class="fas fa-link"></i></a>
</h3>
<p>Local polynomial estimators have well-characterized bias and variance properties, which depend on the polynomial degree <span class="math inline">\(p\)</span>, the bandwidth <span class="math inline">\(h\)</span>, and the smoothness of the true regression function <span class="math inline">\(m(x)\)</span>.</p>
<div id="bias" class="section level4" number="10.4.3.1">
<h4>
<span class="header-section-number">10.4.3.1</span> Bias<a class="anchor" aria-label="anchor" href="#bias"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<p>The leading bias term is proportional to <span class="math inline">\(h^{p+1}\)</span> and involves the <span class="math inline">\((p+1)\)</span>-th derivative of <span class="math inline">\(m(x)\)</span>:</p>
<p><span class="math display">\[
\operatorname{Bias}\left[\hat{m}(x)\right] \approx \frac{h^{p+1}}{(p+1)!} m^{(p+1)}(x) \cdot B(K, p),
\]</span></p>
<p>where <span class="math inline">\(B(K, p)\)</span> is a constant depending on the kernel and the polynomial degree.</p>
</li>
<li><p>For <a href="sec-nonparametric-regression.html#sec-special-case-local-linear-regression">local linear regression</a> (<span class="math inline">\(p=1\)</span>), the bias is of order <span class="math inline">\(O(h^2)\)</span>, while for <strong>local quadratic regression</strong> (<span class="math inline">\(p=2\)</span>), it’s of order <span class="math inline">\(O(h^3)\)</span>.</p></li>
</ul>
</div>
<div id="variance" class="section level4" number="10.4.3.2">
<h4>
<span class="header-section-number">10.4.3.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<p>The variance is approximately:</p>
<p><span class="math display">\[
\operatorname{Var}\left[\hat{m}(x)\right] \approx \frac{\sigma^2}{n h} \cdot V(K, p),
\]</span></p>
<p>where <span class="math inline">\(\sigma^2\)</span> is the error variance, and <span class="math inline">\(V(K, p)\)</span> is another kernel-dependent constant.</p>
</li>
<li><p>The variance decreases with larger <span class="math inline">\(n\)</span> and larger <span class="math inline">\(h\)</span>, but increasing <span class="math inline">\(h\)</span> also increases bias, illustrating the classic <strong>bias-variance trade-off</strong>.</p></li>
</ul>
</div>
<div id="boundary-issues" class="section level4" number="10.4.3.3">
<h4>
<span class="header-section-number">10.4.3.3</span> Boundary Issues<a class="anchor" aria-label="anchor" href="#boundary-issues"><i class="fas fa-link"></i></a>
</h4>
<p>One of the key advantages of local polynomial regression—especially <a href="sec-nonparametric-regression.html#sec-special-case-local-linear-regression">local linear regression</a>—is its ability to reduce <strong>boundary bias</strong>, which is a major issue for the <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson estimator</a>. This is because the linear term allows the fit to adjust for slope changes near the boundaries, where the kernel becomes asymmetric due to fewer data points on one side.</p>
<hr>
</div>
</div>
<div id="sec-special-case-local-linear-regression" class="section level3" number="10.4.4">
<h3>
<span class="header-section-number">10.4.4</span> Special Case: Local Linear Regression<a class="anchor" aria-label="anchor" href="#sec-special-case-local-linear-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Local linear regression (often called a local polynomial fit of degree 1) is particularly popular because:</p>
<ul>
<li>It mitigates boundary bias effectively, making it superior to <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson</a> near the edges of the data.</li>
<li>It remains computationally simple yet provides better performance than local-constant (degree 0) models.</li>
<li>It is robust to heteroscedasticity, as it adapts to varying data densities.</li>
</ul>
<p>The resulting estimator for <span class="math inline">\(\hat{m}(x)\)</span> simplifies to:</p>
<p><span class="math display">\[
\hat{m}(x)
= \frac{S_{2}(x)\,S_{0y}(x) \;-\; S_{1}(x)\,S_{1y}(x)}
       {S_{0}(x)\,S_{2}(x) \;-\; S_{1}^2(x)},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
S_{k}(x)
\;=\; \sum_{i=1}^n (x_i - x)^k \, K\!\Bigl(\tfrac{x_i - x}{h}\Bigr),
\quad
S_{k y}(x)
\;=\; \sum_{i=1}^n (x_i - x)^k \, y_i \, K\!\Bigl(\tfrac{x_i - x}{h}\Bigr).
\]</span></p>
<hr>
<p>To see why the local linear fit helps reduce bias, consider approximating <span class="math inline">\(m\)</span> around the point <span class="math inline">\(x\)</span> via a first-order Taylor expansion:</p>
<p><span class="math display">\[
m(t) \;\approx\; m(x) \;+\; m'(x)\,(t - x).
\]</span></p>
<p>When we perform local linear regression, we solve the weighted least squares problem</p>
<p><span class="math display">\[
\min_{\beta_0, \beta_1}
\;\sum_{i=1}^n \Bigl[y_i \;-\; \bigl\{\beta_0 + \beta_1 \,(x_i - x)\bigr\}\Bigr]^2
\, K\!\Bigl(\tfrac{x_i - x}{h}\Bigr).
\]</span></p>
<p>If we assume <span class="math inline">\(y_i = m(x_i) + \varepsilon_i\)</span>, then expanding <span class="math inline">\(m(x_i)\)</span> in a Taylor series around <span class="math inline">\(x\)</span> gives:</p>
<p><span class="math display">\[
m(x_i)
\;=\; m(x) \;+\; m'(x)\,(x_i - x)
       \;+\; \tfrac{1}{2}\,m''(x)\,(x_i - x)^2
       \;+\; \cdots.
\]</span></p>
<p>For <span class="math inline">\(x_i\)</span> close to <span class="math inline">\(x\)</span>, the higher-order terms may be small, but they contribute to the bias if we truncate at the linear term.</p>
<hr>
<p>Let us denote:</p>
<p><span class="math display">\[
S_0(x)
\;=\; \sum_{i=1}^n K\!\Bigl(\tfrac{x_i - x}{h}\Bigr),
\quad
S_1(x)
\;=\; \sum_{i=1}^n (x_i - x)\,K\!\Bigl(\tfrac{x_i - x}{h}\Bigr),
\quad
S_2(x)
\;=\; \sum_{i=1}^n (x_i - x)^2\,K\!\Bigl(\tfrac{x_i - x}{h}\Bigr).
\]</span></p>
<p>Similarly, define <span class="math display">\[
\sum_{i=1}^n y_i\,K\!\bigl(\tfrac{x_i - x}{h}\bigr)
\quad\text{and}\quad
\sum_{i=1}^n (x_i - x)\,y_i\,K\!\bigl(\tfrac{x_i - x}{h}\bigr)
\]</span> for the right-hand sides. The estimated coefficients <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are found by solving:</p>
<p><span class="math display">\[
\begin{pmatrix}
S_0(x) &amp; S_1(x)\\[6pt]
S_1(x) &amp; S_2(x)
\end{pmatrix}
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix}
=
\begin{pmatrix}
\sum_{i=1}^n y_i \,K\!\bigl(\tfrac{x_i - x}{h}\bigr)\\[6pt]
\sum_{i=1}^n (x_i - x)\,y_i \,K\!\bigl(\tfrac{x_i - x}{h}\bigr)
\end{pmatrix}.
\]</span></p>
<p>Once <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are found, we identify <span class="math inline">\(\hat{m}(x) = \hat{\beta}_0\)</span>.</p>
<p>By substituting the Taylor expansion <span class="math inline">\(y_i = m(x_i) + \varepsilon_i\)</span> and taking expectations, one can derive how the “extra” <span class="math inline">\(\tfrac12\,m''(x)\,(x_i - x)^2\)</span> terms feed into the local fit’s bias.</p>
<p>From these expansions and associated algebra, one finds:</p>
<ul>
<li>
<strong>Bias:</strong> The leading bias term for local linear regression is typically on the order of <span class="math inline">\(h^2\)</span>, often written as <span class="math inline">\(\tfrac12\,m''(x)\,\mu_2(K)\,h^2\)</span> for some constant <span class="math inline">\(\mu_2(K)\)</span> depending on the kernel’s second moment.</li>
<li>
<strong>Variance:</strong> The leading variance term at a single point <span class="math inline">\(x\)</span> is on the order of <span class="math inline">\(\tfrac{1}{n\,h}\)</span>.</li>
</ul>
<p>Balancing these two orders of magnitude—i.e., setting <span class="math inline">\(h^2 \sim \tfrac{1}{n\,h}\)</span>—gives <span class="math inline">\(h \sim n^{-1/3}\)</span>. Consequently, the mean squared error at <span class="math inline">\(x\)</span> then behaves like</p>
<p><span class="math display">\[
\bigl(\hat{m}(x) - m(x)\bigr)^2 \;=\; O_p\!\bigl(n^{-2/3}\bigr).
\]</span></p>
<p>While local constant (<a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya–Watson</a>) and local linear estimators often have the same <em>interior</em> rate, the local linear approach can eliminate leading-order bias near the boundaries, making it preferable in many practical settings.</p>
<hr>
</div>
<div id="bandwidth-selection-1" class="section level3" number="10.4.5">
<h3>
<span class="header-section-number">10.4.5</span> Bandwidth Selection<a class="anchor" aria-label="anchor" href="#bandwidth-selection-1"><i class="fas fa-link"></i></a>
</h3>
<p>Just like in <a href="sec-nonparametric-regression.html#sec-kernel-regression">kernel regression</a>, the <strong>bandwidth</strong> <span class="math inline">\(h\)</span> controls the smoothness of the local polynomial estimator.</p>
<ul>
<li>
<strong>Small</strong> <span class="math inline">\(h\)</span>: Captures fine local details but increases variance (potential overfitting).</li>
<li>
<strong>Large</strong> <span class="math inline">\(h\)</span>: Smooths out noise but may miss important local structure (potential underfitting).</li>
</ul>
<div id="cross-validation-for-local-polynomial-regression" class="section level4" number="10.4.5.1">
<h4>
<span class="header-section-number">10.4.5.1</span> Cross-Validation for Local Polynomial Regression<a class="anchor" aria-label="anchor" href="#cross-validation-for-local-polynomial-regression"><i class="fas fa-link"></i></a>
</h4>
<p>Bandwidth selection via <strong>cross-validation</strong> is also common here. The leave-one-out CV criterion is:</p>
<p><span class="math display">\[
\mathrm{CV}(h) = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{m}_{-i,h}(x_i)\right)^2,
\]</span></p>
<p>where <span class="math inline">\(\hat{m}_{-i,h}(x_i)\)</span> is the estimate at <span class="math inline">\(x_i\)</span> obtained by leaving out the <span class="math inline">\(i\)</span>-th observation.</p>
<p>Alternatively, for <a href="sec-nonparametric-regression.html#sec-special-case-local-linear-regression">local linear regression</a>, computational shortcuts (like <strong>generalized cross-validation</strong>) can significantly speed up bandwidth selection.</p>
<hr>
<p>Comparison: <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson</a> vs. <a href="sec-nonparametric-regression.html#sec-local-polynomial-regression">Local Polynomial Regression</a></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="32%">
<col width="29%">
<col width="38%">
</colgroup>
<thead><tr class="header">
<th>Aspect</th>
<th>Nadaraya-Watson (Local Constant)</th>
<th>Local Polynomial Regression</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Bias at boundaries</strong></td>
<td>High</td>
<td>Reduced (especially for <span class="math inline">\(p=1\)</span>)</td>
</tr>
<tr class="even">
<td><strong>Flexibility</strong></td>
<td>Limited (constant fit)</td>
<td>Captures local trends (linear/quadratic)</td>
</tr>
<tr class="odd">
<td><strong>Complexity</strong></td>
<td>Simpler</td>
<td>Slightly more complex (matrix operations)</td>
</tr>
<tr class="even">
<td><strong>Robustness to heteroscedasticity</strong></td>
<td>Lower</td>
<td>Higher (adapts better to varying densities)</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="asymptotic-properties-summary" class="section level3" number="10.4.6">
<h3>
<span class="header-section-number">10.4.6</span> Asymptotic Properties Summary<a class="anchor" aria-label="anchor" href="#asymptotic-properties-summary"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<strong>Consistency:</strong> <span class="math inline">\(\hat{m}(x) \overset{p}{\longrightarrow} m(x)\)</span> as <span class="math inline">\(n \to \infty\)</span>, under mild conditions.</li>
<li>
<strong>Rate of Convergence:</strong> For local linear regression, the MSE converges at rate <span class="math inline">\(O(n^{-4/5})\)</span>, similar to kernel regression, but with better performance at boundaries.</li>
<li>
<strong>Optimal Bandwidth:</strong> Balances bias (<span class="math inline">\(O(h^{p+1})\)</span>) and variance (<span class="math inline">\(O(1/(nh))\)</span>), with cross-validation as a practical selection method.</li>
</ul>
<hr>
<div class="sourceCode" id="cb370"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 1. Simulate Data </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate predictor x and response y</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Sorted for local regression</span></span>
<span><span class="va">true_function</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span>  <span class="co"># True regression function</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">true_function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span> <span class="co"># Add Gaussian noise</span></span>
<span></span>
<span><span class="co"># Visualization of the data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"darkblue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu">true_function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Simulated Data with True Regression Function"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"x"</span>, y <span class="op">=</span> <span class="st">"y"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb371"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Gaussian Kernel Function</span></span>
<span><span class="va">gaussian_kernel</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">u</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="va">u</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Local Polynomial Regression Function</span></span>
<span><span class="va">local_polynomial_regression</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x_eval</span>,</span>
<span>             <span class="va">x</span>,</span>
<span>             <span class="va">y</span>,</span>
<span>             <span class="va">h</span>,</span>
<span>             <span class="va">p</span> <span class="op">=</span> <span class="fl">1</span>,</span>
<span>             <span class="va">kernel</span> <span class="op">=</span> <span class="va">gaussian_kernel</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">x_eval</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x0</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="co"># Design matrix for polynomial of degree p</span></span>
<span>            <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="va">p</span>, <span class="kw">function</span><span class="op">(</span><span class="va">k</span><span class="op">)</span></span>
<span>                <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">x0</span><span class="op">)</span> <span class="op">^</span> <span class="va">k</span><span class="op">)</span></span>
<span>            </span>
<span>            <span class="co"># Kernel weights</span></span>
<span>            <span class="va">W</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/kernel.html">kernel</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">x0</span><span class="op">)</span> <span class="op">/</span> <span class="va">h</span><span class="op">)</span><span class="op">)</span></span>
<span>            </span>
<span>            <span class="co"># Weighted least squares estimation</span></span>
<span>            <span class="va">beta_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">W</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">X</span>, <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">W</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">y</span><span class="op">)</span></span>
<span>            </span>
<span>            <span class="co"># Estimated value at x0 (intercept term)</span></span>
<span>            <span class="va">beta_hat</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>        <span class="op">}</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span><span class="co"># Evaluation grid</span></span>
<span><span class="va">x_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">10</span>, length.out <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply Local Linear Regression (p = 1)</span></span>
<span><span class="va">h_linear</span> <span class="op">&lt;-</span> <span class="fl">0.8</span></span>
<span><span class="va">llr_estimate</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">local_polynomial_regression</span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">x</span>, <span class="va">y</span>, h <span class="op">=</span> <span class="va">h_linear</span>, p <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply Local Quadratic Regression (p = 2)</span></span>
<span><span class="va">h_quadratic</span> <span class="op">&lt;-</span> <span class="fl">0.8</span></span>
<span><span class="va">lqr_estimate</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">local_polynomial_regression</span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">x</span>, <span class="va">y</span>, h <span class="op">=</span> <span class="va">h_quadratic</span>, p <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb372"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot Local Linear Regression</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">llr_estimate</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>              size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Local Linear Regression (p = 1)"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Bandwidth (h) ="</span>, <span class="va">h_linear</span><span class="op">)</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot Local Quadratic Regression</span></span>
<span><span class="va">p2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">lqr_estimate</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"orange"</span>,</span>
<span>              size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Local Quadratic Regression (p = 2)"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Bandwidth (h) ="</span>, <span class="va">h_quadratic</span><span class="op">)</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display plots side by side</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">p1</span>, <span class="va">p2</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Visualization%20of%20Local%20Polynomial%20Fits-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>The green curve represents the <a href="sec-nonparametric-regression.html#sec-special-case-local-linear-regression">local linear regression</a> estimate.</p></li>
<li><p>The orange curve represents the local quadratic regression estimate.</p></li>
<li><p>The dashed red line is the true regression function.</p></li>
<li><p>Boundary effects are better handled by local polynomial methods, especially with quadratic fits that capture curvature more effectively.</p></li>
</ul>
<div class="sourceCode" id="cb373"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Leave-One-Out Cross-Validation for Bandwidth Selection</span></span>
<span><span class="va">cv_bandwidth_lp</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">h</span>, <span class="va">x</span>, <span class="va">y</span>, <span class="va">p</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">kernel</span> <span class="op">=</span> <span class="va">gaussian_kernel</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span>  <span class="va">cv_error</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  </span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Leave-one-out data</span></span>
<span>    <span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>    <span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>    </span>
<span>    <span class="co"># Predict the left-out point</span></span>
<span>    <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="fu">local_polynomial_regression</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">x_train</span>, <span class="va">y_train</span>, h <span class="op">=</span> <span class="va">h</span>, p <span class="op">=</span> <span class="va">p</span>, kernel <span class="op">=</span> <span class="va">kernel</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Accumulate squared error</span></span>
<span>    <span class="va">cv_error</span> <span class="op">&lt;-</span> <span class="va">cv_error</span> <span class="op">+</span> <span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">-</span> <span class="va">y_pred</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">cv_error</span> <span class="op">/</span> <span class="va">n</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Bandwidth grid for optimization</span></span>
<span><span class="va">bandwidths</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">2</span>, by <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Cross-validation errors for local linear regression</span></span>
<span><span class="va">cv_errors_linear</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">bandwidths</span>, <span class="va">cv_bandwidth_lp</span>, x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, p <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Cross-validation errors for local quadratic regression</span></span>
<span><span class="va">cv_errors_quadratic</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">bandwidths</span>, <span class="va">cv_bandwidth_lp</span>, x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, p <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Optimal bandwidths</span></span>
<span><span class="va">optimal_h_linear</span> <span class="op">&lt;-</span> <span class="va">bandwidths</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="va">cv_errors_linear</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">optimal_h_quadratic</span> <span class="op">&lt;-</span> <span class="va">bandwidths</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="va">cv_errors_quadratic</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Display optimal bandwidths</span></span>
<span><span class="va">optimal_h_linear</span></span>
<span><span class="co">#&gt; [1] 0.4</span></span>
<span><span class="va">optimal_h_quadratic</span></span>
<span><span class="co">#&gt; [1] 0.7</span></span></code></pre></div>
<div class="sourceCode" id="cb374"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># CV Error Plot for Linear and Quadratic Fits</span></span>
<span><span class="va">cv_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Bandwidth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">bandwidths</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>    CV_Error <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">cv_errors_linear</span>, <span class="va">cv_errors_quadratic</span><span class="op">)</span>,</span>
<span>    Degree <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Linear (p=1)"</span>, <span class="st">"Quadratic (p=2)"</span><span class="op">)</span>, each <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">bandwidths</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">cv_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Bandwidth</span>, y <span class="op">=</span> <span class="va">CV_Error</span>, color <span class="op">=</span> <span class="va">Degree</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span></span>
<span>        data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span></span>
<span>            <span class="va">cv_data</span>,</span>
<span>            <span class="va">Bandwidth</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">optimal_h_linear</span>, <span class="va">optimal_h_quadratic</span><span class="op">)</span></span>
<span>        <span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Bandwidth</span>, y <span class="op">=</span> <span class="va">CV_Error</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">3</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Cross-Validation for Bandwidth Selection"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="st">"Red points indicate optimal bandwidths"</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"Bandwidth (h)"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"CV Error"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"green"</span>, <span class="st">"orange"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Plotting%20Cross-Validation%20Results-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>The optimal bandwidth minimizes the cross-validation error.</p></li>
<li><p>The red points mark the bandwidths that yield the lowest errors for linear and quadratic fits.</p></li>
<li><p>Smaller bandwidths can overfit, while larger bandwidths may oversmooth.</p></li>
</ul>
<div class="sourceCode" id="cb375"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Apply Local Polynomial Regression with Optimal Bandwidths</span></span>
<span><span class="va">final_llr_estimate</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">local_polynomial_regression</span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">x</span>, <span class="va">y</span>, h <span class="op">=</span> <span class="va">optimal_h_linear</span>, p <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">final_lqr_estimate</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">local_polynomial_regression</span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">x</span>, <span class="va">y</span>, h <span class="op">=</span> <span class="va">optimal_h_quadratic</span>, p <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot final fits</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"gray60"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">final_llr_estimate</span>, color <span class="op">=</span> <span class="st">"Linear Estimate"</span><span class="op">)</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"solid"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">final_lqr_estimate</span>, color <span class="op">=</span> <span class="st">"Quadratic Estimate"</span><span class="op">)</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"solid"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"True Function"</span><span class="op">)</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"Legend"</span>  <span class="co"># Add a legend title</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span></span>
<span>        values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>            <span class="st">"Linear Estimate"</span> <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>            <span class="st">"Quadratic Estimate"</span> <span class="op">=</span> <span class="st">"orange"</span>,</span>
<span>            <span class="st">"True Function"</span> <span class="op">=</span> <span class="st">"red"</span></span>
<span>        <span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Final%20Fit%20with%20Optimal%20Bandwidths-1.png" width="90%" style="display: block; margin: auto;"></div>
<hr>
</div>
</div>
<div id="sec-smoothing-splines" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Smoothing Splines<a class="anchor" aria-label="anchor" href="#sec-smoothing-splines"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>spline</strong> is a piecewise polynomial function that is smooth at the junction points (called <strong>knots</strong>). <strong>Smoothing splines</strong> provide a flexible nonparametric regression technique by balancing the trade-off between closely fitting the data and maintaining smoothness. This is achieved through a penalty on the function’s curvature.</p>
<p>In the univariate case, suppose we have data <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span> with <span class="math inline">\(0 \le x_1 &lt; x_2 &lt; \cdots &lt; x_n \le 1\)</span> (rescaling is always possible if needed). The <strong>smoothing spline estimator</strong> <span class="math inline">\(\hat{m}(x)\)</span> is defined as the solution to the following optimization problem:</p>
<p><span class="math display">\[
\hat{m}(x) = \underset{f \in \mathcal{H}}{\arg\min} \left\{ \sum_{i=1}^n \bigl(y_i - f(x_i)\bigr)^2 + \lambda \int_{0}^{1} \bigl(f''(t)\bigr)^2 \, dt \right\},
\]</span></p>
<p>where:</p>
<ul>
<li>The <strong>first term</strong> measures the <strong>lack of fit</strong> (residual sum of squares).</li>
<li>The <strong>second term</strong> is a <strong>roughness penalty</strong> that discourages excessive curvature in <span class="math inline">\(f\)</span>, controlled by the <strong>smoothing parameter</strong> <span class="math inline">\(\lambda \ge 0\)</span>.</li>
<li>The space <span class="math inline">\(\mathcal{H}\)</span> denotes the set of all twice-differentiable functions on <span class="math inline">\([0,1]\)</span>.</li>
</ul>
<p><strong>Special Cases</strong>:</p>
<ul>
<li>When <span class="math inline">\(\lambda = 0\)</span>: No penalty is applied, and the solution interpolates the data exactly (an interpolating spline).</li>
<li>As <span class="math inline">\(\lambda \to \infty\)</span>: The penalty dominates, forcing the solution to be as smooth as possible—reducing to a linear regression (since the second derivative of a straight line is zero).</li>
</ul>
<div id="properties-and-form-of-the-smoothing-spline" class="section level3" number="10.5.1">
<h3>
<span class="header-section-number">10.5.1</span> Properties and Form of the Smoothing Spline<a class="anchor" aria-label="anchor" href="#properties-and-form-of-the-smoothing-spline"><i class="fas fa-link"></i></a>
</h3>
<p>A key result from spline theory is that the minimizer <span class="math inline">\(\hat{m}(x)\)</span> is a <strong>natural cubic spline</strong> with knots at the observed data points <span class="math inline">\(\{x_1, \ldots, x_n\}\)</span>. This result holds despite the fact that we are minimizing over an infinite-dimensional space of functions.</p>
<p>The solution can be expressed as:</p>
<p><span class="math display">\[
\hat{m}(x) = a_0 + a_1 x + \sum_{j=1}^n b_j \, (x - x_j)_+^3,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\((u)_+ = \max(u, 0)\)</span> is the <strong>positive part function</strong> (the cubic spline basis function),</li>
<li>The coefficients <span class="math inline">\(\{a_0, a_1, b_1, \ldots, b_n\}\)</span> are determined by solving a system of linear equations derived from the optimization problem.</li>
</ul>
<p>This form implies that the spline is a cubic polynomial within each interval between data points, with smooth transitions at the knots. The smoothness conditions ensure continuity of the function and its first and second derivatives at each knot.</p>
</div>
<div id="choice-of-lambda" class="section level3" number="10.5.2">
<h3>
<span class="header-section-number">10.5.2</span> Choice of <span class="math inline">\(\lambda\)</span><a class="anchor" aria-label="anchor" href="#choice-of-lambda"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>smoothing parameter</strong> <span class="math inline">\(\lambda\)</span> plays a crucial role in controlling the trade-off between goodness-of-fit and smoothness:</p>
<ul>
<li>
<strong>Large</strong> <span class="math inline">\(\lambda\)</span>: Imposes a strong penalty on the roughness, leading to a smoother (potentially underfitted) function that captures broad trends.</li>
<li>
<strong>Small</strong> <span class="math inline">\(\lambda\)</span>: Allows the function to closely follow the data, possibly resulting in overfitting if the data are noisy.</li>
</ul>
<p>A common approach to selecting <span class="math inline">\(\lambda\)</span> is <strong>generalized cross-validation (GCV)</strong>, which provides an efficient approximation to leave-one-out cross-validation:</p>
<p><span class="math display">\[
\mathrm{GCV}(\lambda) = \frac{\frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{m}_{\lambda}(x_i)\right)^2}{\left[1 - \frac{\operatorname{tr}(\mathbf{S}_\lambda)}{n}\right]^2},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\hat{m}_{\lambda}(x_i)\)</span> is the fitted value at <span class="math inline">\(x_i\)</span> for a given <span class="math inline">\(\lambda\)</span>,</li>
<li>
<span class="math inline">\(\mathbf{S}_\lambda\)</span> is the smoothing matrix (or influence matrix) such that <span class="math inline">\(\hat{\mathbf{y}} = \mathbf{S}_\lambda \mathbf{y}\)</span>,</li>
<li>
<span class="math inline">\(\operatorname{tr}(\mathbf{S}_\lambda)\)</span> is the effective degrees of freedom, reflecting the model’s flexibility.</li>
</ul>
<p>The optimal <span class="math inline">\(\lambda\)</span> minimizes the GCV score, balancing fit and complexity without the need to refit the model multiple times (as in traditional cross-validation).</p>
<hr>
</div>
<div id="connection-to-reproducing-kernel-hilbert-spaces" class="section level3" number="10.5.3">
<h3>
<span class="header-section-number">10.5.3</span> Connection to Reproducing Kernel Hilbert Spaces<a class="anchor" aria-label="anchor" href="#connection-to-reproducing-kernel-hilbert-spaces"><i class="fas fa-link"></i></a>
</h3>
<p>Smoothing splines can be viewed through the lens of <strong>reproducing kernel Hilbert spaces (RKHS)</strong>. The penalty term:</p>
<p><span class="math display">\[
\int_{0}^{1} \bigl(f''(t)\bigr)^2 \, dt
\]</span></p>
<p>defines a <strong>semi-norm</strong> that corresponds to the squared norm of <span class="math inline">\(f\)</span> in a particular RKHS associated with the cubic spline kernel. This interpretation reveals that smoothing splines are equivalent to solving a <strong>regularization problem in an RKHS</strong>, where the penalty controls the smoothness of the solution.</p>
<p>This connection extends naturally to more general kernel-based methods (e.g., Gaussian process regression, kernel ridge regression) and higher-dimensional spline models.</p>
<hr>
<div class="sourceCode" id="cb376"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 1. Simulate Data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Generate predictor x and response y</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Sorted for smoother visualization</span></span>
<span><span class="va">true_function</span> <span class="op">&lt;-</span></span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span>  <span class="co"># True regression function</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">true_function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span> <span class="co"># Add Gaussian noise</span></span>
<span></span>
<span><span class="co"># Visualization of the data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"darkblue"</span>, alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu">true_function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>              linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Simulated Data with True Regression Function"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"x"</span>, y <span class="op">=</span> <span class="st">"y"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb377"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Apply Smoothing Spline with Default Lambda </span></span>
<span><span class="co"># (automatically selected using GCV)</span></span>
<span><span class="va">spline_fit_default</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/smooth.spline.html">smooth.spline</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply Smoothing Spline with Manual Lambda </span></span>
<span><span class="co"># (via smoothing parameter 'spar')</span></span>
<span><span class="va">spline_fit_smooth</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/smooth.spline.html">smooth.spline</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, spar <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span>  <span class="co"># Smoother fit</span></span>
<span><span class="va">spline_fit_flexible</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/smooth.spline.html">smooth.spline</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, spar <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span>  <span class="co"># More flexible fit</span></span>
<span></span>
<span><span class="co"># Create grid for prediction</span></span>
<span><span class="va">x_grid</span>               <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">10</span>, length.out <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span><span class="va">spline_pred_default</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">spline_fit_default</span>, <span class="va">x_grid</span><span class="op">)</span></span>
<span><span class="va">spline_pred_smooth</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">spline_fit_smooth</span>, <span class="va">x_grid</span><span class="op">)</span></span>
<span><span class="va">spline_pred_flexible</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">spline_fit_flexible</span>, <span class="va">x_grid</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb378"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot Smoothing Splines with Different Smoothness Levels</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"darkblue"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">spline_pred_default</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>              size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">spline_pred_smooth</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"orange"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dotted"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">spline_pred_flexible</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"solid"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Smoothing Spline Fits"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="st">"Green: GCV-selected | Orange: Smooth (spar=0.8) | Purple: Flexible (spar=0.4)"</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Visualization%20of%20Smoothing%20Spline%20Fits-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>The green curve is the fit with the optimal <span class="math inline">\(\lambda\)</span> selected automatically via GCV.</li>
<li>The orange curve (with <code>spar = 0.8</code>) is smoother, capturing broad trends but missing finer details.</li>
<li>The purple curve (with <code>spar = 0.4</code>) is more flexible, fitting the data closely, potentially overfitting noise.</li>
<li>The red solid line represents the true regression function.</li>
</ul>
<div class="sourceCode" id="cb379"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract Generalized Cross-Validation (GCV) Scores</span></span>
<span><span class="va">spline_fit_default</span><span class="op">$</span><span class="va">cv.crit</span>  <span class="co"># GCV criterion for the default fit</span></span>
<span><span class="co">#&gt; [1] 0.09698728</span></span>
<span></span>
<span><span class="co"># Compare GCV for different spar values</span></span>
<span><span class="va">spar_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">1.5</span>, by <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span></span>
<span><span class="va">gcv_scores</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">spar_values</span>, <span class="kw">function</span><span class="op">(</span><span class="va">spar</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/smooth.spline.html">smooth.spline</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, spar <span class="op">=</span> <span class="va">spar</span><span class="op">)</span></span>
<span>    <span class="va">fit</span><span class="op">$</span><span class="va">cv.crit</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Optimal spar corresponding to the minimum GCV score</span></span>
<span><span class="va">optimal_spar</span> <span class="op">&lt;-</span> <span class="va">spar_values</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="va">gcv_scores</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">optimal_spar</span></span>
<span><span class="co">#&gt; [1] 0.7</span></span></code></pre></div>
<div class="sourceCode" id="cb380"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># GCV Plot</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">spar_values</span>, <span class="va">gcv_scores</span><span class="op">)</span>,</span>
<span>       <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">spar_values</span>, y <span class="op">=</span> <span class="va">gcv_scores</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span>, size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">optimal_spar</span>, y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">gcv_scores</span><span class="op">)</span><span class="op">)</span>,</span>
<span>               color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>               size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"GCV for Smoothing Parameter Selection"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Optimal spar ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">optimal_spar</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"Smoothing Parameter (spar)"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"GCV Score"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Plotting%20GCV%20Scores-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>The blue curve shows how the GCV score changes with different smoothing parameters (<code>spar</code>).</li>
<li>The red point indicates the optimal smoothing parameter that minimizes the GCV score.</li>
<li>Low <code>spar</code> values correspond to flexible fits (risking overfitting), while high <code>spar</code> values produce smoother fits (risking underfitting).</li>
</ul>
<div class="sourceCode" id="cb381"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Apply Smoothing Spline with Optimal spar</span></span>
<span><span class="va">spline_fit_optimal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/smooth.spline.html">smooth.spline</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, spar <span class="op">=</span> <span class="va">optimal_spar</span><span class="op">)</span></span>
<span><span class="va">spline_pred_optimal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">spline_fit_optimal</span>, <span class="va">x_grid</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot Final Fit</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"gray60"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="va">spline_pred_optimal</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>              size <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_grid</span>, <span class="fu">true_function</span><span class="op">(</span><span class="va">x_grid</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1.2</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Smoothing Spline with Optimal Smoothing Parameter"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Optimal spar ="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">optimal_spar</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Estimated m(x)"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Final%20Smoothing%20Spline%20Fit%20with%20Optimal%20lambda-1.png" width="90%" style="display: block; margin: auto;"></div>
<hr>
</div>
</div>
<div id="confidence-intervals-in-nonparametric-regression" class="section level2" number="10.6">
<h2>
<span class="header-section-number">10.6</span> Confidence Intervals in Nonparametric Regression<a class="anchor" aria-label="anchor" href="#confidence-intervals-in-nonparametric-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Constructing <strong>confidence intervals</strong> (or bands) for nonparametric regression estimators like <a href="sec-nonparametric-regression.html#sec-kernel-regression">kernel smoothers</a>, <a href="sec-nonparametric-regression.html#sec-local-polynomial-regression">local polynomials</a>, and <a href="sec-nonparametric-regression.html#sec-smoothing-splines">smoothing splines</a> is more complex than in parametric models. The key challenges arise from the flexible nature of the models and the dependence of bias and variance on the local data density and smoothing parameters.</p>
<div id="asymptotic-normality" class="section level3" number="10.6.1">
<h3>
<span class="header-section-number">10.6.1</span> Asymptotic Normality<a class="anchor" aria-label="anchor" href="#asymptotic-normality"><i class="fas fa-link"></i></a>
</h3>
<p>Under regularity conditions, nonparametric estimators are <strong>asymptotically normal</strong>. For a given point <span class="math inline">\(x\)</span>, we have:</p>
<p><span class="math display">\[
\sqrt{n h} \left\{\hat{m}(x) - m(x)\right\} \overset{\mathcal{D}}{\longrightarrow} \mathcal{N}\left(0, \sigma^2 \, \nu(x)\right),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span> is the sample size,</li>
<li>
<span class="math inline">\(h\)</span> is the bandwidth (for kernel or local polynomial estimators) or a function of <span class="math inline">\(\lambda\)</span> (for smoothing splines),</li>
<li>
<span class="math inline">\(\sigma^2\)</span> is the variance of the errors,</li>
<li>
<span class="math inline">\(\nu(x)\)</span> is a function that depends on the estimator, kernel, and local data density.</li>
</ul>
<p>An approximate <span class="math inline">\((1 - \alpha)\)</span> pointwise confidence interval for <span class="math inline">\(m(x)\)</span> is given by:</p>
<p><span class="math display">\[
\hat{m}(x) \pm z_{\alpha/2} \cdot \sqrt{\widehat{\operatorname{Var}}[\hat{m}(x)]},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(z_{\alpha/2}\)</span> is the <span class="math inline">\((1 - \alpha/2)\)</span> quantile of the standard normal distribution,</li>
<li>
<span class="math inline">\(\widehat{\operatorname{Var}}[\hat{m}(x)]\)</span> is an estimate of the variance, which can be obtained using plug-in methods, sandwich estimators, or resampling techniques.</li>
</ul>
</div>
<div id="bootstrap-methods" class="section level3" number="10.6.2">
<h3>
<span class="header-section-number">10.6.2</span> Bootstrap Methods<a class="anchor" aria-label="anchor" href="#bootstrap-methods"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>bootstrap</strong> provides a powerful alternative for constructing confidence intervals and bands, particularly when asymptotic approximations are unreliable (e.g., small sample sizes or near boundaries).</p>
<div id="bootstrap-approaches" class="section level4" number="10.6.2.1">
<h4>
<span class="header-section-number">10.6.2.1</span> Bootstrap Approaches<a class="anchor" aria-label="anchor" href="#bootstrap-approaches"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<strong>Residual Bootstrap:</strong>
<ol style="list-style-type: decimal">
<li>Fit the nonparametric model to obtain residuals <span class="math inline">\(\hat{\varepsilon}_i = y_i - \hat{m}(x_i)\)</span>.</li>
<li>Generate bootstrap samples <span class="math inline">\(y_i^* = \hat{m}(x_i) + \varepsilon_i^*\)</span>, where <span class="math inline">\(\varepsilon_i^*\)</span> are resampled (with replacement) from <span class="math inline">\(\{\hat{\varepsilon}_i\}\)</span>.</li>
<li>Refit the model to each bootstrap sample to obtain <span class="math inline">\(\hat{m}^*(x)\)</span>.</li>
<li>Repeat many times to build an empirical distribution of <span class="math inline">\(\hat{m}^*(x)\)</span>.</li>
</ol>
</li>
<li>
<strong>Wild Bootstrap:</strong><br>
Particularly useful for heteroscedastic data. Instead of resampling residuals directly, we multiply them by random variables with mean zero and unit variance to preserve the variance structure.</li>
</ul>
</div>
<div id="bootstrap-confidence-bands" class="section level4" number="10.6.2.2">
<h4>
<span class="header-section-number">10.6.2.2</span> Bootstrap Confidence Bands<a class="anchor" aria-label="anchor" href="#bootstrap-confidence-bands"><i class="fas fa-link"></i></a>
</h4>
<p>While <strong>pointwise confidence intervals</strong> cover the true function at a specific <span class="math inline">\(x\)</span> with probability <span class="math inline">\((1 - \alpha)\)</span>, <strong>simultaneous confidence bands</strong> cover the entire function over an interval with the desired confidence level. Bootstrap methods can be adapted to estimate these bands by capturing the distribution of the <strong>maximum deviation</strong> between <span class="math inline">\(\hat{m}(x)\)</span> and <span class="math inline">\(m(x)\)</span> over the range of <span class="math inline">\(x\)</span>.</p>
</div>
</div>
<div id="practical-considerations-4" class="section level3" number="10.6.3">
<h3>
<span class="header-section-number">10.6.3</span> Practical Considerations<a class="anchor" aria-label="anchor" href="#practical-considerations-4"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p><strong>Bias Correction:</strong><br>
Nonparametric estimators often have non-negligible bias, especially near boundaries. Bias correction techniques or undersmoothing (choosing a smaller bandwidth) are sometimes used to improve interval coverage.</p></li>
<li><p><strong>Effective Degrees of Freedom:</strong><br>
For smoothing splines, the <strong>effective degrees of freedom</strong> (related to <span class="math inline">\(\operatorname{tr}(\mathbf{S}_\lambda)\)</span>) provide insight into model complexity and influence confidence interval construction.</p></li>
</ul>
<hr>
</div>
</div>
<div id="sec-generalized-additive-models" class="section level2" number="10.7">
<h2>
<span class="header-section-number">10.7</span> Generalized Additive Models<a class="anchor" aria-label="anchor" href="#sec-generalized-additive-models"><i class="fas fa-link"></i></a>
</h2>
<p>A <a href="sec-nonparametric-regression.html#sec-generalized-additive-models">generalized additive model</a> (GAM) extends <a href="generalized-linear-models.html#generalized-linear-models">generalized linear models</a> by allowing <strong>additive smooth terms</strong>:</p>
<p><span class="math display">\[
g(\mathbb{E}[Y]) = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(g(\cdot)\)</span> is a <strong>link function</strong> (as in GLMs),</li>
<li>
<span class="math inline">\(\beta_0\)</span> is the intercept,</li>
<li>Each <span class="math inline">\(f_j\)</span> is a smooth, potentially nonparametric function (e.g., a spline, kernel smoother, or local polynomial smoother),</li>
<li>
<span class="math inline">\(p\)</span> is the number of predictors, with <span class="math inline">\(p \ge 2\)</span> highlighting the flexibility of GAMs in handling multivariate data.</li>
</ul>
<p>This structure allows for <strong>nonlinear relationships</strong> between each predictor <span class="math inline">\(X_j\)</span> and the response <span class="math inline">\(Y\)</span>, while maintaining <strong>additivity</strong>, which simplifies interpretation compared to fully nonparametric models.</p>
<p>Traditional linear models assume a strictly linear relationship:</p>
<p><span class="math display">\[
g(\mathbb{E}[Y]) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p.
\]</span></p>
<p>However, real-world data often exhibit complex, nonlinear patterns. While <a href="generalized-linear-models.html#generalized-linear-models">generalized linear models</a> extend linear models to non-Gaussian responses, they still rely on linear predictors. GAMs address this by replacing linear terms with smooth functions:</p>
<ul>
<li>
<strong>GLMs:</strong> Linear effects (e.g., <span class="math inline">\(\beta_1 X_1\)</span>)</li>
<li>
<strong>GAMs:</strong> Nonlinear smooth effects (e.g., <span class="math inline">\(f_1(X_1)\)</span>)</li>
</ul>
<hr>
<p>The general form of a GAM is:</p>
<p><span class="math display">\[
g(\mathbb{E}[Y \mid \mathbf{X}]) = \beta_0 + \sum_{j=1}^p f_j(X_j),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Y\)</span> is the response variable,</li>
<li>
<span class="math inline">\(\mathbf{X} = (X_1, X_2, \ldots, X_p)\)</span> are the predictors,</li>
<li>
<span class="math inline">\(f_j\)</span> are smooth functions capturing potentially nonlinear effects,</li>
<li>The link function <span class="math inline">\(g(\cdot)\)</span> connects the mean of <span class="math inline">\(Y\)</span> to the additive predictor.</li>
</ul>
<p><strong>Special Cases:</strong></p>
<ul>
<li>
<strong>When</strong> <span class="math inline">\(g\)</span> is the identity function and <span class="math inline">\(Y\)</span> is continuous: This reduces to an <strong>additive model</strong> (a special case of GAM).</li>
<li>
<strong>When</strong> <span class="math inline">\(g\)</span> is the logit function and <span class="math inline">\(Y\)</span> is binary: We have a <strong>logistic GAM</strong> for classification tasks.</li>
<li>
<strong>When</strong> <span class="math inline">\(g\)</span> is the log function and <span class="math inline">\(Y\)</span> follows a Poisson distribution: This is a <strong>Poisson GAM</strong> for count data.</li>
</ul>
<hr>
<div id="estimation-via-penalized-likelihood" class="section level3" number="10.7.1">
<h3>
<span class="header-section-number">10.7.1</span> Estimation via Penalized Likelihood<a class="anchor" aria-label="anchor" href="#estimation-via-penalized-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>GAMs are typically estimated using <strong>penalized likelihood methods</strong> to balance model fit and smoothness. The objective function is:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{pen}} = \ell(\beta_0, f_1, \ldots, f_p) - \frac{1}{2} \sum_{j=1}^p \lambda_j \int \left(f_j''(x)\right)^2 dx,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\ell(\beta_0, f_1, \ldots, f_p)\)</span> is the (log-)likelihood of the data,</li>
<li>
<span class="math inline">\(\lambda_j \ge 0\)</span> are <strong>smoothing parameters</strong> controlling the smoothness of each <span class="math inline">\(f_j\)</span>,</li>
<li>The penalty term <span class="math inline">\(\int (f_j'')^2 dx\)</span> discourages excessive curvature, similar to smoothing splines.</li>
</ul>
<div id="backfitting-algorithm" class="section level4" number="10.7.1.1">
<h4>
<span class="header-section-number">10.7.1.1</span> Backfitting Algorithm<a class="anchor" aria-label="anchor" href="#backfitting-algorithm"><i class="fas fa-link"></i></a>
</h4>
<p>For continuous responses, the classic <strong>backfitting algorithm</strong> is often used:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Initialize</strong>: Start with an initial guess for each <span class="math inline">\(f_j\)</span>, typically zero.</li>
<li>
<strong>Iterate</strong>: For each <span class="math inline">\(j = 1, \dots, p\)</span>:
<ul>
<li>Compute the <strong>partial residuals</strong>: <span class="math display">\[
r_j = y - \beta_0 - \sum_{k \neq j} f_k(X_k)
\]</span>
</li>
<li>Update <span class="math inline">\(f_j\)</span> by fitting a smoother to <span class="math inline">\((X_j, r_j)\)</span>.</li>
</ul>
</li>
<li>
<strong>Convergence</strong>: Repeat until the functions <span class="math inline">\(f_j\)</span> stabilize.</li>
</ol>
<p>This approach works because of the additive structure, which allows each smooth term to be updated conditionally on the others.</p>
</div>
<div id="generalized-additive-model-estimation-for-glms" class="section level4" number="10.7.1.2">
<h4>
<span class="header-section-number">10.7.1.2</span> Generalized Additive Model Estimation (for GLMs)<a class="anchor" aria-label="anchor" href="#generalized-additive-model-estimation-for-glms"><i class="fas fa-link"></i></a>
</h4>
<p>When <span class="math inline">\(Y\)</span> is non-Gaussian (e.g., binary, count data), we use iteratively reweighted least squares (IRLS) in combination with backfitting. Popular implementations, such as in the <code>mgcv</code> package in R, use penalized likelihood estimation with efficient computational algorithms (e.g., penalized iteratively reweighted least squares).</p>
<hr>
</div>
</div>
<div id="interpretation-of-gams" class="section level3" number="10.7.2">
<h3>
<span class="header-section-number">10.7.2</span> Interpretation of GAMs<a class="anchor" aria-label="anchor" href="#interpretation-of-gams"><i class="fas fa-link"></i></a>
</h3>
<p>One of the key advantages of GAMs is their <strong>interpretability</strong>, especially compared to fully nonparametric or black-box machine learning models.</p>
<ul>
<li>
<strong>Additive Structure:</strong> Each predictor’s effect is modeled separately via <span class="math inline">\(f_j(X_j)\)</span>, making it easy to interpret marginal effects.</li>
<li>
<strong>Partial Dependence Plots:</strong> Visualization of <span class="math inline">\(f_j(X_j)\)</span> shows how each predictor affects the response, holding other variables constant.</li>
</ul>
<p><strong>Example:</strong></p>
<p>For a marketing dataset predicting customer purchase probability:</p>
<p><span class="math display">\[
\log\left(\frac{\mathbb{P}(\text{Purchase})}{1 - \mathbb{P}(\text{Purchase})}\right) = \beta_0 + f_1(\text{Age}) + f_2(\text{Income}) + f_3(\text{Ad Exposure})
\]</span></p>
<ul>
<li>
<span class="math inline">\(f_1(\text{Age})\)</span> might show a peak in purchase likelihood for middle-aged customers.</li>
<li>
<span class="math inline">\(f_2(\text{Income})\)</span> could reveal a threshold effect where purchases increase beyond a certain income level.</li>
<li>
<span class="math inline">\(f_3(\text{Ad Exposure})\)</span> might show diminishing returns after repeated exposures.</li>
</ul>
<hr>
</div>
<div id="model-selection-and-smoothing-parameter-estimation" class="section level3" number="10.7.3">
<h3>
<span class="header-section-number">10.7.3</span> Model Selection and Smoothing Parameter Estimation<a class="anchor" aria-label="anchor" href="#model-selection-and-smoothing-parameter-estimation"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>smoothing parameters</strong> <span class="math inline">\(\lambda_j\)</span> control the complexity of each smooth term:</p>
<ul>
<li>
<strong>Large</strong> <span class="math inline">\(\lambda_j\)</span>: Strong smoothing, leading to nearly linear fits.</li>
<li>
<strong>Small</strong> <span class="math inline">\(\lambda_j\)</span>: Flexible, wiggly fits that may overfit if <span class="math inline">\(\lambda_j\)</span> is too small.</li>
</ul>
<p><strong>Methods for Choosing</strong> <span class="math inline">\(\lambda_j\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Generalized Cross-Validation (GCV):</strong> <span class="math display">\[
\mathrm{GCV} = \frac{1}{n} \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\left(1 - \frac{\operatorname{tr}(\mathbf{S})}{n}\right)^2}
\]</span> where <span class="math inline">\(\mathbf{S}\)</span> is the smoother matrix. GCV is a popular method for selecting the smoothing parameter <span class="math inline">\(\lambda_j\)</span> because it approximates leave-one-out cross-validation without requiring explicit refitting of the model. The term <span class="math inline">\(\text{tr}(\mathbf{S})\)</span> represents the effective degrees of freedom of the smoother, and the denominator penalizes overfitting.</p></li>
<li><p><strong>Unbiased Risk Estimation:</strong> This method extends the idea of GCV to non-Gaussian families (e.g., Poisson, binomial). It aims to minimize an unbiased estimate of the risk (expected prediction error). For Gaussian models, it often reduces to a form similar to GCV, but for other distributions, it incorporates the appropriate likelihood or deviance.</p></li>
<li><p><strong>Akaike Information Criterion (AIC):</strong> <span class="math display">\[
AIC=−2\log⁡(L)+2tr⁡(S)
\]</span> where <span class="math inline">\(L\)</span> is the likelihood of the model. AIC balances model fit (measured by the likelihood) and complexity (measured by the effective degrees of freedom <span class="math inline">\(tr⁡(S)\)</span>). The smoothing parameter <span class="math inline">\(\lambda_j\)</span> is chosen to minimize the AIC.</p></li>
<li><p><strong>Bayesian Information Criterion (BIC):</strong> <span class="math display">\[
BIC=−2\log⁡(L)+\log⁡(n)tr⁡(S)
\]</span> where <span class="math inline">\(n\)</span> is the sample size. BIC is similar to AIC but imposes a stronger penalty for model complexity, making it more suitable for larger datasets.</p></li>
<li><p><strong>Leave-One-Out Cross-Validation (LOOCV):</strong> <span class="math display">\[
LOOCV = \frac{1}{n}\sum_{i = 1}^n ( y_i - \hat{y}_i^{(-i)})^2,
\]</span> where <span class="math inline">\(y_i^{(−i)}\)</span> is the predicted value for the ii-th observation when the model is fitted without it. LOOCV is computationally intensive but provides a direct estimate of prediction error.</p></li>
<li><p><strong>Empirical Risk Minimization:</strong><br>
For some non-parametric regression methods, <span class="math inline">\(\lambda_j\)</span> can be chosen by minimizing the empirical risk (e.g., mean squared error) on a validation set or via resampling techniques like <span class="math inline">\(k\)</span>-fold cross-validation.</p></li>
</ol>
<hr>
</div>
<div id="extensions-of-gams" class="section level3" number="10.7.4">
<h3>
<span class="header-section-number">10.7.4</span> Extensions of GAMs<a class="anchor" aria-label="anchor" href="#extensions-of-gams"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p><strong>GAM with Interaction Terms:</strong> <span class="math display">\[
g(\mathbb{E}[Y]) = \beta_0 + f_1(X_1) + f_2(X_2) + f_{12}(X_1, X_2)
\]</span> where <span class="math inline">\(f_{12}\)</span> captures the interaction between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> (using tensor product smooths).</p></li>
<li><p><strong>GAMMs (Generalized Additive Mixed Models):</strong> Incorporate random effects to handle hierarchical or grouped data.</p></li>
<li><p><strong>Varying Coefficient Models:</strong> Allow regression coefficients to vary smoothly with another variable, e.g., <span class="math display">\[
Y = \beta_0 + f_1(Z) \cdot X + \varepsilon
\]</span></p></li>
</ul>
<hr>
<div class="sourceCode" id="cb382"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">mgcv</span><span class="op">)</span>    <span class="co"># For fitting GAMs</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate Data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">x3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># True nonlinear functions</span></span>
<span><span class="va">f1</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>                 <span class="co"># Nonlinear effect of x1</span></span>
<span><span class="va">f2</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">x</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span>             <span class="co"># Nonlinear effect of x2</span></span>
<span><span class="va">f3</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="fl">0.5</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fl">5</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span>        <span class="co"># Quadratic effect for x3</span></span>
<span></span>
<span><span class="co"># Generate response variable with noise</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">3</span> <span class="op">+</span> <span class="fu">f1</span><span class="op">(</span><span class="va">x1</span><span class="op">)</span> <span class="op">+</span> <span class="fu">f2</span><span class="op">(</span><span class="va">x2</span><span class="op">)</span> <span class="op">-</span> <span class="fu">f3</span><span class="op">(</span><span class="va">x3</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Data frame for analysis</span></span>
<span><span class="va">data_gam</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plotting the true functions with simulated data</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_gam</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x1 (sin(x1))"</span><span class="op">)</span></span>
<span><span class="va">p2</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_gam</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x2</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x2 (log(x2+1))"</span><span class="op">)</span></span>
<span><span class="va">p3</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_gam</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x3</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x3 (quadratic)"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display plots side by side</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">p1</span>, <span class="va">p2</span>, <span class="va">p3</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Simulate%20Data%20for%20GAM-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb383"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit a GAM using mgcv</span></span>
<span><span class="va">gam_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x3</span><span class="op">)</span>,</span>
<span>        data <span class="op">=</span> <span class="va">data_gam</span>, method <span class="op">=</span> <span class="st">"REML"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gam_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Family: gaussian </span></span>
<span><span class="co">#&gt; Link function: identity </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Formula:</span></span>
<span><span class="co">#&gt; y ~ s(x1) + s(x2) + s(x3)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parametric coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  2.63937    0.09511   27.75   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Approximate significance of smooth terms:</span></span>
<span><span class="co">#&gt;         edf Ref.df       F p-value    </span></span>
<span><span class="co">#&gt; s(x1) 5.997  7.165   7.966   5e-07 ***</span></span>
<span><span class="co">#&gt; s(x2) 1.000  1.000  10.249 0.00192 ** </span></span>
<span><span class="co">#&gt; s(x3) 6.239  7.343 105.551 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; R-sq.(adj) =   0.91   Deviance explained = 92.2%</span></span>
<span><span class="co">#&gt; -REML = 155.23  Scale est. = 0.90463   n = 100</span></span></code></pre></div>
<div class="sourceCode" id="cb384"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot smooth terms</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Arrange plots in one row</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">gam_model</span>, shade <span class="op">=</span> <span class="cn">TRUE</span>, seWithMean <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Visualizing%20Smooth%20Functions%20(Partial%20Dependence%20Plots)-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb385"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Reset plotting layout</span></span></code></pre></div>
<div class="sourceCode" id="cb386"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Using ggplot2 with mgcv's predict function</span></span>
<span><span class="va">pred_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">data_gam</span>, <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span></span>
<span>    x1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>,</span>
<span>    x2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x2</span><span class="op">)</span>,</span>
<span>    x3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x3</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictions for x1 effect</span></span>
<span><span class="va">pred_data</span><span class="op">$</span><span class="va">pred_x1</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">gam_model</span>, newdata <span class="op">=</span> <span class="va">pred_data</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">pred_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">pred_x1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span>, size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Partial Effect of x1"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"x1"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Effect on y"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/unnamed-chunk-4-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb387"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Check AIC and GCV score</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">gam_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 289.8201</span></span>
<span><span class="va">gam_model</span><span class="op">$</span><span class="va">gcv.ubre</span>  <span class="co"># GCV/UBRE score</span></span>
<span><span class="co">#&gt;     REML </span></span>
<span><span class="co">#&gt; 155.2314 </span></span>
<span><span class="co">#&gt; attr(,"Dp")</span></span>
<span><span class="co">#&gt; [1] 47.99998</span></span>
<span></span>
<span></span>
<span><span class="co"># Compare models with different smoothness</span></span>
<span><span class="va">gam_model_simple</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x1</span>, k <span class="op">=</span> <span class="fl">4</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x2</span>, k <span class="op">=</span> <span class="fl">4</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x3</span>, k <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>,</span>
<span>        data <span class="op">=</span> <span class="va">data_gam</span><span class="op">)</span></span>
<span><span class="va">gam_model_complex</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x1</span>, k <span class="op">=</span> <span class="fl">20</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x2</span>, k <span class="op">=</span> <span class="fl">20</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x3</span>, k <span class="op">=</span> <span class="fl">20</span><span class="op">)</span>,</span>
<span>        data <span class="op">=</span> <span class="va">data_gam</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare models using AIC</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">gam_model</span>, <span class="va">gam_model_simple</span>, <span class="va">gam_model_complex</span><span class="op">)</span></span>
<span><span class="co">#&gt;                          df      AIC</span></span>
<span><span class="co">#&gt; gam_model         15.706428 289.8201</span></span>
<span><span class="co">#&gt; gam_model_simple   8.429889 322.1502</span></span>
<span><span class="co">#&gt; gam_model_complex 13.571165 287.4171</span></span></code></pre></div>
<ul>
<li>Lower AIC indicates a better model balancing fit and complexity.</li>
<li>GCV score helps select the optimal level of smoothness.</li>
<li>Compare models to prevent overfitting (too flexible) or underfitting (too simple).</li>
</ul>
<div class="sourceCode" id="cb388"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># GAM with interaction using tensor product smooths</span></span>
<span><span class="va">gam_interaction</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/te.html">te</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x3</span><span class="op">)</span>,</span>
<span>                       data <span class="op">=</span> <span class="va">data_gam</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the interaction model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gam_interaction</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Family: gaussian </span></span>
<span><span class="co">#&gt; Link function: identity </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Formula:</span></span>
<span><span class="co">#&gt; y ~ te(x1, x2) + s(x3)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parametric coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  2.63937    0.09364   28.19   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Approximate significance of smooth terms:</span></span>
<span><span class="co">#&gt;             edf Ref.df       F p-value    </span></span>
<span><span class="co">#&gt; te(x1,x2) 8.545  8.923   9.218  &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; s(x3)     4.766  5.834 147.595  &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; R-sq.(adj) =  0.912   Deviance explained = 92.4%</span></span>
<span><span class="co">#&gt; GCV = 1.0233  Scale est. = 0.87688   n = 100</span></span>
<span></span>
<span><span class="co"># Visualization of interaction effect</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/vis.gam.html">vis.gam</a></span><span class="op">(</span></span>
<span>    <span class="va">gam_interaction</span>,</span>
<span>    view <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x1"</span>, <span class="st">"x2"</span><span class="op">)</span>,</span>
<span>    plot.type <span class="op">=</span> <span class="st">"contour"</span>,</span>
<span>    color <span class="op">=</span> <span class="st">"terrain"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Adding%20Interaction%20Terms-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>The tensor product smooth <code>te(x1, x2)</code> captures nonlinear interactions between <code>x1</code> and <code>x2</code>.</li>
<li>The contour plot visualizes how their joint effect influences the response.</li>
</ul>
<div class="sourceCode" id="cb389"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate binary response</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="fu">f1</span><span class="op">(</span><span class="va">x1</span><span class="op">)</span> <span class="op">-</span> <span class="fu">f2</span><span class="op">(</span><span class="va">x2</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x3</span><span class="op">)</span>  <span class="co"># Logistic function</span></span>
<span><span class="va">y_bin</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, <span class="va">prob</span><span class="op">)</span>                     <span class="co"># Binary outcome</span></span>
<span></span>
<span><span class="co"># Fit GAM for binary classification</span></span>
<span><span class="va">gam_logistic</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y_bin</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x3</span><span class="op">)</span>,</span>
<span>        family <span class="op">=</span> <span class="va">binomial</span>,</span>
<span>        data <span class="op">=</span> <span class="va">data_gam</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary and visualization</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gam_logistic</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Family: binomial </span></span>
<span><span class="co">#&gt; Link function: logit </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Formula:</span></span>
<span><span class="co">#&gt; y_bin ~ s(x1) + s(x2) + s(x3)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Parametric coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)</span></span>
<span><span class="co">#&gt; (Intercept)    22.30      32.18   0.693    0.488</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Approximate significance of smooth terms:</span></span>
<span><span class="co">#&gt;         edf Ref.df Chi.sq p-value</span></span>
<span><span class="co">#&gt; s(x1) 4.472  5.313  2.645   0.775</span></span>
<span><span class="co">#&gt; s(x2) 1.000  1.000  1.925   0.165</span></span>
<span><span class="co">#&gt; s(x3) 1.000  1.000  1.390   0.238</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; R-sq.(adj) =      1   Deviance explained = 99.8%</span></span>
<span><span class="co">#&gt; UBRE = -0.84802  Scale est. = 1         n = 100</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">gam_logistic</span>, shade <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/GAM%20for%20Logistic%20Regression%20(Binary%20Outcome)-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb390"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>The logistic GAM models nonlinear effects on the log-odds of the binary outcome.</li>
<li>Smooth plots indicate predictors’ influence on probability of success.</li>
</ul>
<div class="sourceCode" id="cb391"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Diagnostic plots</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.check.html">gam.check</a></span><span class="op">(</span><span class="va">gam_model</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Model%20Diagnostics-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; 
#&gt; Method: REML   Optimizer: outer newton
#&gt; full convergence after 9 iterations.
#&gt; Gradient range [-5.387854e-05,2.006026e-05]
#&gt; (score 155.2314 &amp; scale 0.9046299).
#&gt; Hessian positive definite, eigenvalue range [5.387409e-05,48.28647].
#&gt; Model rank =  28 / 28 
#&gt; 
#&gt; Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
#&gt; indicate that k is too low, especially if edf is close to k'.
#&gt; 
#&gt;         k'  edf k-index p-value
#&gt; s(x1) 9.00 6.00    1.01    0.46
#&gt; s(x2) 9.00 1.00    1.16    0.92
#&gt; s(x3) 9.00 6.24    1.07    0.72
par(mfrow = c(1, 1))</code></pre>
<ul>
<li>Residual plots assess model fit.</li>
<li>QQ plot checks for normality of residuals.</li>
<li>K-index evaluates the adequacy of smoothness selection.</li>
</ul>
<hr>
</div>
</div>
<div id="regression-trees-and-random-forests" class="section level2" number="10.8">
<h2>
<span class="header-section-number">10.8</span> Regression Trees and Random Forests<a class="anchor" aria-label="anchor" href="#regression-trees-and-random-forests"><i class="fas fa-link"></i></a>
</h2>
<p>Though not typically framed as “kernel” or “spline,” <strong>tree-based methods</strong>—such as <strong>Classification and Regression Trees (CART)</strong> and <strong>random forests</strong>—are also <strong>nonparametric</strong> models. They do not assume a predetermined functional form for the relationship between predictors and the response. Instead, they <strong>adaptively partition the predictor space</strong> into regions, fitting simple models (usually constants or linear models) within each region.</p>
<hr>
<div id="sec-regression-trees" class="section level3" number="10.8.1">
<h3>
<span class="header-section-number">10.8.1</span> Regression Trees<a class="anchor" aria-label="anchor" href="#sec-regression-trees"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Classification and Regression Trees (CART)</strong> algorithm is the foundation of tree-based models <span class="citation">(<a href="references.html#ref-breiman2017classification">Breiman 2017</a>)</span>. In regression settings, CART models the response variable as a <strong>piecewise constant function</strong>.</p>
<p>A <strong>regression tree</strong> recursively partitions the predictor space into disjoint regions, <span class="math inline">\(R_1, R_2, \ldots, R_M\)</span>, and predicts the response as a constant within each region:</p>
<p><span class="math display">\[
\hat{m}(x) = \sum_{m=1}^{M} c_m \cdot \mathbb{I}(x \in R_m),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(c_m\)</span> is the <strong>predicted value</strong> (usually the mean of <span class="math inline">\(y_i\)</span>) for all observations in region <span class="math inline">\(R_m\)</span>,</li>
<li>
<span class="math inline">\(\mathbb{I}(\cdot)\)</span> is the indicator function.</li>
</ul>
<p><strong>Tree-Building Algorithm (Greedy Recursive Partitioning):</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Start with the full dataset</strong> as a single region.</li>
<li>
<strong>Find the best split</strong>:
<ul>
<li>Consider all possible splits of all predictors (e.g., <span class="math inline">\(X_j &lt; s\)</span>).</li>
<li>Choose the split that minimizes the <strong>residual sum of squares (RSS)</strong>: <span class="math display">\[
\text{RSS} = \sum_{i \in R_1} (y_i - \bar{y}_{R_1})^2 + \sum_{i \in R_2} (y_i - \bar{y}_{R_2})^2,
\]</span> where <span class="math inline">\(\bar{y}_{R_k}\)</span> is the mean response in region <span class="math inline">\(R_k\)</span>.</li>
</ul>
</li>
<li>
<strong>Recursively repeat</strong> the splitting process for each new region (node) until a stopping criterion is met (e.g., minimum number of observations per leaf, maximum tree depth).</li>
<li>
<strong>Assign a constant prediction</strong> to each terminal node (leaf) based on the average response of observations within that node.</li>
</ol>
<p><strong>Stopping Criteria and Pruning:</strong></p>
<ul>
<li>
<strong>Pre-pruning (early stopping):</strong> Halt the tree growth when a predefined condition is met (e.g., minimal node size, maximal depth).</li>
<li>
<strong>Post-pruning (cost-complexity pruning):</strong> Grow a large tree first, then prune back to avoid overfitting. The <strong>cost-complexity criterion</strong> is: <span class="math display">\[
C_\alpha(T) = \text{RSS}(T) + \alpha |T|,
\]</span> where <span class="math inline">\(|T|\)</span> is the number of terminal nodes (leaves) and <span class="math inline">\(\alpha\)</span> controls the penalty for complexity.</li>
</ul>
<p><strong>Advantages of Regression Trees:</strong></p>
<ul>
<li>
<strong>Interpretability:</strong> Easy to visualize and understand.</li>
<li>
<strong>Handling of different data types:</strong> Can naturally handle numerical and categorical variables.</li>
<li>
<strong>Nonlinear relationships and interactions:</strong> Captures complex patterns without explicit modeling.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>
<strong>High variance:</strong> Trees are sensitive to small changes in data (unstable).</li>
<li>
<strong>Overfitting risk:</strong> Without pruning or regularization, deep trees can overfit noise.</li>
</ul>
<hr>
</div>
<div id="sec-random-forests" class="section level3" number="10.8.2">
<h3>
<span class="header-section-number">10.8.2</span> Random Forests<a class="anchor" aria-label="anchor" href="#sec-random-forests"><i class="fas fa-link"></i></a>
</h3>
<p>To address the high variance of <a href="sec-nonparametric-regression.html#sec-regression-trees">single trees</a>, <strong>random forests</strong> combine many regression trees to create an ensemble model with improved predictive performance and stability <span class="citation">(<a href="references.html#ref-breiman2001random">Breiman 2001</a>)</span>.</p>
<p>A <strong>random forest</strong> builds multiple decision trees and aggregates their predictions to reduce variance. For regression, the final prediction is the <strong>average</strong> of the predictions from individual trees:</p>
<p><span class="math display">\[
\hat{m}_{\text{RF}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{m}^{(b)}(x),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(B\)</span> is the number of trees in the forest,</li>
<li>
<span class="math inline">\(\hat{m}^{(b)}(x)\)</span> is the prediction from the <span class="math inline">\(b\)</span>-th tree.</li>
</ul>
<p><strong>Random Forest Algorithm:</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Bootstrap Sampling:</strong> For each tree, draw a bootstrap sample from the training data (sampling with replacement).</li>
<li>
<strong>Random Feature Selection:</strong> At each split in the tree:
<ul>
<li>Randomly select a subset of predictors (usually <span class="math inline">\(\sqrt{p}\)</span> for classification or <span class="math inline">\(p/3\)</span> for regression).</li>
<li>Find the best split <strong>only</strong> among the selected features.</li>
</ul>
</li>
<li>
<strong>Tree Growth:</strong> Grow each tree to full depth without pruning.</li>
<li>
<strong>Aggregation:</strong> For regression, average the predictions from all trees. For classification, use majority voting.</li>
</ol>
<p><strong>Why Does Random Forest Work?</strong></p>
<ul>
<li>
<strong>Bagging (Bootstrap Aggregating):</strong> Reduces variance by averaging over multiple models.</li>
<li>
<strong>Random Feature Selection:</strong> Decorrelates trees, further reducing variance.</li>
</ul>
<hr>
</div>
<div id="theoretical-insights" class="section level3" number="10.8.3">
<h3>
<span class="header-section-number">10.8.3</span> Theoretical Insights<a class="anchor" aria-label="anchor" href="#theoretical-insights"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Bias-Variance Trade-off</strong></p>
<ul>
<li>
<strong>Regression Trees:</strong> Low bias but high variance.</li>
<li>
<strong>Random Forests:</strong> Slightly higher bias than a single tree (due to randomization) but significantly reduced variance, leading to lower overall prediction error.</li>
</ul>
<p><strong>Out-of-Bag (OOB) Error</strong></p>
<p>Random forests provide an internal estimate of prediction error using <strong>out-of-bag</strong> samples (the data not included in the bootstrap sample for a given tree). The OOB error is computed by:</p>
<ol style="list-style-type: decimal">
<li>For each observation, predict its response using only the trees where it was <strong>not</strong> included in the bootstrap sample.</li>
<li>Calculate the error by comparing the OOB predictions to the true responses.</li>
</ol>
<p>OOB error serves as an efficient, unbiased estimate of test error without the need for cross-validation.</p>
<hr>
</div>
<div id="feature-importance-in-random-forests" class="section level3" number="10.8.4">
<h3>
<span class="header-section-number">10.8.4</span> Feature Importance in Random Forests<a class="anchor" aria-label="anchor" href="#feature-importance-in-random-forests"><i class="fas fa-link"></i></a>
</h3>
<p>Random forests naturally provide measures of <strong>variable importance</strong>, helping identify which predictors contribute most to the model.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Mean Decrease in Impurity (MDI):</strong> Measures the total reduction in impurity (e.g., RSS) attributed to each variable across all trees.</li>
<li>
<strong>Permutation Importance:</strong> Measures the increase in prediction error when the values of a predictor are randomly permuted, breaking its relationship with the response.</li>
</ol>
</div>
<div id="advantages-and-limitations-of-tree-based-methods" class="section level3" number="10.8.5">
<h3>
<span class="header-section-number">10.8.5</span> Advantages and Limitations of Tree-Based Methods<a class="anchor" aria-label="anchor" href="#advantages-and-limitations-of-tree-based-methods"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="26%">
<col width="28%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Regression Trees</strong></th>
<th><strong>Random Forests</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>High (easy to visualize)</td>
<td>Moderate (difficult to interpret individual trees)</td>
</tr>
<tr class="even">
<td><strong>Variance</strong></td>
<td>High (prone to overfitting)</td>
<td>Low (averaging reduces variance)</td>
</tr>
<tr class="odd">
<td><strong>Bias</strong></td>
<td>Low (flexible to data patterns)</td>
<td>Slightly higher than a single tree</td>
</tr>
<tr class="even">
<td><strong>Feature Importance</strong></td>
<td>Basic (via tree splits)</td>
<td>Advanced (permutation-based measures)</td>
</tr>
<tr class="odd">
<td><strong>Handling of Missing Data</strong></td>
<td>Handles with surrogate splits</td>
<td>Handles naturally in ensemble averaging</td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td>Low (fast for small datasets)</td>
<td>High (especially with many trees)</td>
</tr>
</tbody>
</table></div>
<div class="sourceCode" id="cb393"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bethatkinson/rpart">rpart</a></span><span class="op">)</span>           <span class="co"># For regression trees</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.milbo.org/rpart-plot/index.html">rpart.plot</a></span><span class="op">)</span>       <span class="co"># For visualizing trees</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span>     <span class="co"># For random forests</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate Data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">x3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Nonlinear functions</span></span>
<span><span class="va">f1</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">f2</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">x</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">f3</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="fl">0.5</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fl">5</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span></span>
<span></span>
<span><span class="co"># Generate response variable with noise</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">3</span> <span class="op">+</span> <span class="fu">f1</span><span class="op">(</span><span class="va">x1</span><span class="op">)</span> <span class="op">+</span> <span class="fu">f2</span><span class="op">(</span><span class="va">x2</span><span class="op">)</span> <span class="op">-</span> <span class="fu">f3</span><span class="op">(</span><span class="va">x3</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Data frame</span></span>
<span><span class="va">data_tree</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Quick visualization of data</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_tree</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x1"</span><span class="op">)</span></span>
<span><span class="va">p2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_tree</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x2</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x2"</span><span class="op">)</span></span>
<span><span class="va">p3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_tree</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x3</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x3"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">p1</span>, <span class="va">p2</span>, <span class="va">p3</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/unnamed-chunk-5-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb394"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit a Regression Tree using rpart</span></span>
<span><span class="va">tree_model</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span></span>
<span>        <span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span>,</span>
<span>        data <span class="op">=</span> <span class="va">data_tree</span>,</span>
<span>        method <span class="op">=</span> <span class="st">"anova"</span>,</span>
<span>        control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span>    <span class="op">)</span>  <span class="co"># cp = complexity parameter</span></span>
<span></span>
<span><span class="co"># Summary of the tree</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">tree_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; rpart(formula = y ~ x1 + x2 + x3, data = data_tree, method = "anova", </span></span>
<span><span class="co">#&gt;     control = rpart.control(cp = 0.01))</span></span>
<span><span class="co">#&gt;   n= 100 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;           CP nsplit rel error    xerror      xstd</span></span>
<span><span class="co">#&gt; 1 0.39895879      0 1.0000000 1.0134781 0.3406703</span></span>
<span><span class="co">#&gt; 2 0.17470339      1 0.6010412 0.8649973 0.3336272</span></span>
<span><span class="co">#&gt; 3 0.04607373      2 0.4263378 0.5707932 0.1880333</span></span>
<span><span class="co">#&gt; 4 0.02754858      3 0.3802641 0.5287366 0.1866728</span></span>
<span><span class="co">#&gt; 5 0.01584638      4 0.3527155 0.5061104 0.1867491</span></span>
<span><span class="co">#&gt; 6 0.01032524      5 0.3368691 0.5136765 0.1861020</span></span>
<span><span class="co">#&gt; 7 0.01000000      7 0.3162187 0.4847072 0.1861849</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Variable importance</span></span>
<span><span class="co">#&gt; x3 x2 x1 </span></span>
<span><span class="co">#&gt; 91  6  3 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 1: 100 observations,    complexity param=0.3989588</span></span>
<span><span class="co">#&gt;   mean=2.639375, MSE=9.897038 </span></span>
<span><span class="co">#&gt;   left son=2 (7 obs) right son=3 (93 obs)</span></span>
<span><span class="co">#&gt;   Primary splits:</span></span>
<span><span class="co">#&gt;       x3 &lt; 7.707736  to the right, improve=0.39895880, (0 missing)</span></span>
<span><span class="co">#&gt;       x1 &lt; 6.84138   to the left,  improve=0.07685517, (0 missing)</span></span>
<span><span class="co">#&gt;       x2 &lt; 2.627429  to the left,  improve=0.04029839, (0 missing)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 2: 7 observations</span></span>
<span><span class="co">#&gt;   mean=-4.603469, MSE=24.47372 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 3: 93 observations,    complexity param=0.1747034</span></span>
<span><span class="co">#&gt;   mean=3.184535, MSE=4.554158 </span></span>
<span><span class="co">#&gt;   left son=6 (18 obs) right son=7 (75 obs)</span></span>
<span><span class="co">#&gt;   Primary splits:</span></span>
<span><span class="co">#&gt;       x3 &lt; 2.967495  to the left,  improve=0.40823990, (0 missing)</span></span>
<span><span class="co">#&gt;       x2 &lt; 1.001856  to the left,  improve=0.07353453, (0 missing)</span></span>
<span><span class="co">#&gt;       x1 &lt; 6.84138   to the left,  improve=0.07049507, (0 missing)</span></span>
<span><span class="co">#&gt;   Surrogate splits:</span></span>
<span><span class="co">#&gt;       x2 &lt; 0.3435293 to the left,  agree=0.828, adj=0.111, (0 split)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 6: 18 observations</span></span>
<span><span class="co">#&gt;   mean=0.4012593, MSE=3.4521 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 7: 75 observations,    complexity param=0.04607373</span></span>
<span><span class="co">#&gt;   mean=3.852521, MSE=2.513258 </span></span>
<span><span class="co">#&gt;   left son=14 (12 obs) right son=15 (63 obs)</span></span>
<span><span class="co">#&gt;   Primary splits:</span></span>
<span><span class="co">#&gt;       x3 &lt; 6.324486  to the right, improve=0.24191360, (0 missing)</span></span>
<span><span class="co">#&gt;       x2 &lt; 1.603258  to the left,  improve=0.10759280, (0 missing)</span></span>
<span><span class="co">#&gt;       x1 &lt; 6.793804  to the left,  improve=0.09106168, (0 missing)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 14: 12 observations</span></span>
<span><span class="co">#&gt;   mean=2.065917, MSE=2.252311 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 15: 63 observations,    complexity param=0.02754858</span></span>
<span><span class="co">#&gt;   mean=4.192826, MSE=1.839163 </span></span>
<span><span class="co">#&gt;   left son=30 (9 obs) right son=31 (54 obs)</span></span>
<span><span class="co">#&gt;   Primary splits:</span></span>
<span><span class="co">#&gt;       x3 &lt; 3.548257  to the left,  improve=0.2353119, (0 missing)</span></span>
<span><span class="co">#&gt;       x2 &lt; 1.349633  to the left,  improve=0.1103019, (0 missing)</span></span>
<span><span class="co">#&gt;       x1 &lt; 7.006669  to the left,  improve=0.1019295, (0 missing)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 30: 9 observations</span></span>
<span><span class="co">#&gt;   mean=2.581411, MSE=0.3669647 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 31: 54 observations,    complexity param=0.01584638</span></span>
<span><span class="co">#&gt;   mean=4.461396, MSE=1.579623 </span></span>
<span><span class="co">#&gt;   left son=62 (10 obs) right son=63 (44 obs)</span></span>
<span><span class="co">#&gt;   Primary splits:</span></span>
<span><span class="co">#&gt;       x2 &lt; 1.130662  to the left,  improve=0.18386040, (0 missing)</span></span>
<span><span class="co">#&gt;       x1 &lt; 6.209961  to the left,  improve=0.14561510, (0 missing)</span></span>
<span><span class="co">#&gt;       x3 &lt; 4.517029  to the left,  improve=0.01044883, (0 missing)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 62: 10 observations</span></span>
<span><span class="co">#&gt;   mean=3.330957, MSE=2.001022 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 63: 44 observations,    complexity param=0.01032524</span></span>
<span><span class="co">#&gt;   mean=4.718314, MSE=1.127413 </span></span>
<span><span class="co">#&gt;   left son=126 (27 obs) right son=127 (17 obs)</span></span>
<span><span class="co">#&gt;   Primary splits:</span></span>
<span><span class="co">#&gt;       x1 &lt; 6.468044  to the left,  improve=0.16079230, (0 missing)</span></span>
<span><span class="co">#&gt;       x3 &lt; 5.608708  to the right, improve=0.05277854, (0 missing)</span></span>
<span><span class="co">#&gt;       x2 &lt; 2.784688  to the left,  improve=0.03145241, (0 missing)</span></span>
<span><span class="co">#&gt;   Surrogate splits:</span></span>
<span><span class="co">#&gt;       x2 &lt; 3.074905  to the left,  agree=0.636, adj=0.059, (0 split)</span></span>
<span><span class="co">#&gt;       x3 &lt; 5.888028  to the left,  agree=0.636, adj=0.059, (0 split)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 126: 27 observations,    complexity param=0.01032524</span></span>
<span><span class="co">#&gt;   mean=4.380469, MSE=1.04313 </span></span>
<span><span class="co">#&gt;   left son=252 (12 obs) right son=253 (15 obs)</span></span>
<span><span class="co">#&gt;   Primary splits:</span></span>
<span><span class="co">#&gt;       x1 &lt; 3.658072  to the right, improve=0.4424566, (0 missing)</span></span>
<span><span class="co">#&gt;       x3 &lt; 4.270123  to the right, improve=0.1430466, (0 missing)</span></span>
<span><span class="co">#&gt;       x2 &lt; 2.658809  to the left,  improve=0.1121999, (0 missing)</span></span>
<span><span class="co">#&gt;   Surrogate splits:</span></span>
<span><span class="co">#&gt;       x2 &lt; 2.707432  to the left,  agree=0.815, adj=0.583, (0 split)</span></span>
<span><span class="co">#&gt;       x3 &lt; 4.010151  to the right, agree=0.593, adj=0.083, (0 split)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 127: 17 observations</span></span>
<span><span class="co">#&gt;   mean=5.25489, MSE=0.7920812 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 252: 12 observations</span></span>
<span><span class="co">#&gt;   mean=3.620914, MSE=0.6204645 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Node number 253: 15 observations</span></span>
<span><span class="co">#&gt;   mean=4.988114, MSE=0.5504908</span></span>
<span></span>
<span><span class="co"># Visualize the Regression Tree</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span></span>
<span>    <span class="va">tree_model</span>,</span>
<span>    type <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    extra <span class="op">=</span> <span class="fl">101</span>,</span>
<span>    fallen.leaves <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Regression Tree"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Fitting%20a%20Regression%20Tree-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>Splits are made based on conditions (e.g., x1 &lt; 4.2), partitioning the predictor space.</li>
<li>Terminal nodes (leaves) show the predicted value (mean response in that region).</li>
<li>The tree depth affects interpretability and overfitting risk.</li>
</ul>
<div class="sourceCode" id="cb395"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Optimal pruning based on cross-validation error</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rpart/man/printcp.html">printcp</a></span><span class="op">(</span><span class="va">tree_model</span><span class="op">)</span> <span class="co"># Displays CP table with cross-validation error</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Regression tree:</span></span>
<span><span class="co">#&gt; rpart(formula = y ~ x1 + x2 + x3, data = data_tree, method = "anova", </span></span>
<span><span class="co">#&gt;     control = rpart.control(cp = 0.01))</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Variables actually used in tree construction:</span></span>
<span><span class="co">#&gt; [1] x1 x2 x3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Root node error: 989.7/100 = 9.897</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; n= 100 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;         CP nsplit rel error  xerror    xstd</span></span>
<span><span class="co">#&gt; 1 0.398959      0   1.00000 1.01348 0.34067</span></span>
<span><span class="co">#&gt; 2 0.174703      1   0.60104 0.86500 0.33363</span></span>
<span><span class="co">#&gt; 3 0.046074      2   0.42634 0.57079 0.18803</span></span>
<span><span class="co">#&gt; 4 0.027549      3   0.38026 0.52874 0.18667</span></span>
<span><span class="co">#&gt; 5 0.015846      4   0.35272 0.50611 0.18675</span></span>
<span><span class="co">#&gt; 6 0.010325      5   0.33687 0.51368 0.18610</span></span>
<span><span class="co">#&gt; 7 0.010000      7   0.31622 0.48471 0.18618</span></span>
<span><span class="va">optimal_cp</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">tree_model</span><span class="op">$</span><span class="va">cptable</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="va">tree_model</span><span class="op">$</span><span class="va">cptable</span><span class="op">[</span>, <span class="st">"xerror"</span><span class="op">]</span><span class="op">)</span>, <span class="st">"CP"</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Prune the tree</span></span>
<span><span class="va">pruned_tree</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/prune.rpart.html">prune</a></span><span class="op">(</span><span class="va">tree_model</span>, cp <span class="op">=</span> <span class="va">optimal_cp</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualize the pruned tree</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span></span>
<span>    <span class="va">pruned_tree</span>,</span>
<span>    type <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    extra <span class="op">=</span> <span class="fl">101</span>,</span>
<span>    fallen.leaves <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Pruned Regression Tree"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Pruning%20the%20Regression%20Tree-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>Pruning reduces overfitting by simplifying the tree.</li>
<li>The optimal CP minimizes cross-validation error, balancing complexity and fit.</li>
<li>A shallower tree improves generalization on unseen data.</li>
</ul>
<div class="sourceCode" id="cb396"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit a Random Forest</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">rf_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span></span>
<span>    <span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">+</span> <span class="va">x2</span> <span class="op">+</span> <span class="va">x3</span>,</span>
<span>    data <span class="op">=</span> <span class="va">data_tree</span>,</span>
<span>    ntree <span class="op">=</span> <span class="fl">500</span>,</span>
<span>    mtry <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    importance <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary of the Random Forest</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">rf_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt;  randomForest(formula = y ~ x1 + x2 + x3, data = data_tree, ntree = 500,      mtry = 2, importance = TRUE) </span></span>
<span><span class="co">#&gt;                Type of random forest: regression</span></span>
<span><span class="co">#&gt;                      Number of trees: 500</span></span>
<span><span class="co">#&gt; No. of variables tried at each split: 2</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;           Mean of squared residuals: 3.031589</span></span>
<span><span class="co">#&gt;                     % Var explained: 69.37</span></span></code></pre></div>
<ul>
<li>MSE decreases as more trees are added.</li>
<li>% Variance Explained reflects predictive performance.</li>
<li>
<code>mtry = 2</code> indicates 2 random predictors are considered at each split.</li>
</ul>
<div class="sourceCode" id="cb397"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot OOB Error vs. Number of Trees</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">rf_model</span>, main <span class="op">=</span> <span class="st">"Out-of-Bag Error for Random Forest"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Out-of-Bag%20(OOB)%20Error%20Plot-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>OOB error stabilizes as more trees are added, providing an unbiased estimate of test error.</li>
<li>Helps determine if more trees improve performance or if the model has converged.</li>
</ul>
<div class="sourceCode" id="cb398"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Variable Importance</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/importance.html">importance</a></span><span class="op">(</span><span class="va">rf_model</span><span class="op">)</span>                <span class="co"># Numerical importance measures</span></span>
<span><span class="co">#&gt;      %IncMSE IncNodePurity</span></span>
<span><span class="co">#&gt; x1 10.145674     137.09918</span></span>
<span><span class="co">#&gt; x2  1.472662      77.41256</span></span>
<span><span class="co">#&gt; x3 44.232816     718.49567</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/varImpPlot.html">varImpPlot</a></span><span class="op">(</span><span class="va">rf_model</span>, main <span class="op">=</span> <span class="st">"Variable Importance (Random Forest)"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Feature%20Importance%20in%20Random%20Forests-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>Mean Decrease in MSE indicates how much the model’s error increases when a variable is permuted.</li>
<li>Mean Decrease in Node Impurity reflects how much each variable reduces variance in splits.</li>
<li>Variables with higher importance are more influential in the model.</li>
</ul>
<div class="sourceCode" id="cb399"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Predictions on new data</span></span>
<span><span class="va">x_new</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">10</span>, length.out <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span><span class="va">test_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x1 <span class="op">=</span> <span class="va">x_new</span>,</span>
<span>                        x2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x2</span><span class="op">)</span>,</span>
<span>                        x3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictions</span></span>
<span><span class="va">tree_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">pruned_tree</span>, newdata <span class="op">=</span> <span class="va">test_data</span><span class="op">)</span></span>
<span><span class="va">rf_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">rf_model</span>, newdata <span class="op">=</span> <span class="va">test_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualization</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">y</span><span class="op">)</span>,</span>
<span>               data <span class="op">=</span> <span class="va">data_tree</span>,</span>
<span>               alpha <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>               color <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_new</span>, <span class="va">tree_pred</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x_new</span>, <span class="va">rf_pred</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"green"</span>, size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Regression Tree vs. Random Forest"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="st">"Blue: Pruned Tree | Green: Random Forest"</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x1"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Predicted y"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Model%20Comparison:%20Regression%20Tree%20vs.%20Random%20Forest-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>The pruned regression tree (blue) shows step-like predictions, characteristic of piecewise constant fits.</li>
<li>The random forest (green) provides a smoother fit by averaging across many trees, reducing variance.</li>
</ul>
<div class="sourceCode" id="cb400"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># OOB Error (Random Forest)</span></span>
<span><span class="va">oob_mse</span> <span class="op">&lt;-</span> <span class="va">rf_model</span><span class="op">$</span><span class="va">mse</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">rf_model</span><span class="op">$</span><span class="va">mse</span><span class="op">)</span><span class="op">]</span>  <span class="co"># Final OOB MSE</span></span>
<span></span>
<span><span class="co"># Cross-Validation Error (Regression Tree)</span></span>
<span><span class="va">cv_mse_tree</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">tree_model</span><span class="op">$</span><span class="va">cptable</span><span class="op">[</span>, <span class="st">"xerror"</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">data_tree</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare OOB and CV errors</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Model <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Pruned Regression Tree"</span>, <span class="st">"Random Forest"</span><span class="op">)</span>,</span>
<span>    MSE <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">cv_mse_tree</span>, <span class="va">oob_mse</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;                    Model      MSE</span></span>
<span><span class="co">#&gt; 1 Pruned Regression Tree 4.845622</span></span>
<span><span class="co">#&gt; 2          Random Forest 3.031589</span></span></code></pre></div>
<ul>
<li>OOB error (Random Forest) provides an efficient, unbiased estimate without cross-validation.</li>
<li>Cross-validation error (Regression Tree) evaluates generalization through resampling.</li>
<li>Random Forest often shows lower MSE due to reduced variance.</li>
</ul>
<hr>
</div>
</div>
<div id="sec-wavelet-regression" class="section level2" number="10.9">
<h2>
<span class="header-section-number">10.9</span> Wavelet Regression<a class="anchor" aria-label="anchor" href="#sec-wavelet-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Wavelet regression is a nonparametric regression technique that represents the target function as a combination of <strong>wavelet basis functions</strong>. Unlike traditional basis functions (e.g., <a href="sec-nonparametric-regression.html#sec-local-polynomial-regression">polynomials</a> or <a href="sec-nonparametric-regression.html#sec-smoothing-splines">splines</a>), wavelets have excellent <strong>localization properties</strong> in both the <strong>time (or space)</strong> and <strong>frequency</strong> domains. This makes wavelet regression particularly effective for capturing <strong>local features</strong>, such as sharp changes, discontinuities, and transient patterns.</p>
<hr>
<p>A wavelet is a function <span class="math inline">\(\psi(t)\)</span> that oscillates (like a wave) but is localized in both time and frequency. The key idea is to represent a function as a linear combination of shifted and scaled versions of a mother wavelet <span class="math inline">\(\psi(t)\)</span>.</p>
<p>Wavelet basis functions are generated by scaling and translating the mother wavelet:</p>
<p><span class="math display">\[
\psi_{j,k}(t) = 2^{j/2} \, \psi(2^j t - k),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(j\)</span> (scale parameter): Controls the <strong>frequency</strong>—larger <span class="math inline">\(j\)</span> captures finer details (high-frequency components).</li>
<li>
<span class="math inline">\(k\)</span> (translation parameter): Controls the <strong>location</strong>—shifting the wavelet along the time or space axis.</li>
<li>The factor <span class="math inline">\(2^{j/2}\)</span> ensures that the wavelet basis functions are <strong>orthonormal</strong>.</li>
</ul>
<p>In addition to the mother wavelet <span class="math inline">\(\psi(t)\)</span>, there’s also a <strong>scaling function</strong> <span class="math inline">\(\phi(t)\)</span>, which captures the low-frequency (smooth) components of the data.</p>
<hr>
<div id="wavelet-series-expansion" class="section level3" number="10.9.1">
<h3>
<span class="header-section-number">10.9.1</span> Wavelet Series Expansion<a class="anchor" aria-label="anchor" href="#wavelet-series-expansion"><i class="fas fa-link"></i></a>
</h3>
<p>Just as Fourier analysis represents functions as sums of sines and cosines, <strong>wavelet analysis</strong> represents a function as a sum of wavelet basis functions:</p>
<p><span class="math display">\[
f(t) = \sum_{k} c_{J_0, k} \, \phi_{J_0, k}(t) + \sum_{j = J_0}^{J_{\max}} \sum_{k} d_{j, k} \, \psi_{j, k}(t),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(c_{J_0, k}\)</span> are the <strong>approximation coefficients</strong> at the coarsest scale <span class="math inline">\(J_0\)</span>, capturing smooth trends,</li>
<li>
<span class="math inline">\(d_{j, k}\)</span> are the <strong>detail coefficients</strong> at scale <span class="math inline">\(j\)</span>, capturing finer details,</li>
<li>
<span class="math inline">\(\phi_{J_0, k}(t)\)</span> are the scaling functions, and <span class="math inline">\(\psi_{j, k}(t)\)</span> are the wavelet functions.</li>
</ul>
<p>The goal of <a href="sec-nonparametric-regression.html#sec-wavelet-regression">wavelet regression</a> is to estimate these coefficients based on observed data.</p>
<hr>
</div>
<div id="wavelet-regression-model" class="section level3" number="10.9.2">
<h3>
<span class="header-section-number">10.9.2</span> Wavelet Regression Model<a class="anchor" aria-label="anchor" href="#wavelet-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p>Given data <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span>, the wavelet regression model assumes:</p>
<p><span class="math display">\[
y_i = f(x_i) + \varepsilon_i,
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(f(x)\)</span> is the unknown regression function,</li>
<li>
<span class="math inline">\(\varepsilon_i\)</span> are i.i.d. errors with mean zero and variance <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>We approximate <span class="math inline">\(f(x)\)</span> using a finite number of wavelet basis functions:</p>
<p><span class="math display">\[
\hat{f}(x) = \sum_{k} \hat{c}_{J_0, k} \, \phi_{J_0, k}(x) + \sum_{j = J_0}^{J_{\max}} \sum_{k} \hat{d}_{j, k} \, \psi_{j, k}(x),
\]</span></p>
<p>where the coefficients <span class="math inline">\(\hat{c}_{J_0, k}\)</span> and <span class="math inline">\(\hat{d}_{j, k}\)</span> are estimated from the data.</p>
<p><strong>Coefficient Estimation:</strong></p>
<ul>
<li>
<strong>Linear Wavelet Regression:</strong> Estimate coefficients via least squares, projecting the data onto the wavelet basis.</li>
<li>
<strong>Nonlinear Wavelet Regression (Thresholding):</strong> Apply <strong>shrinkage</strong> or <strong>thresholding</strong> (e.g., hard or soft thresholding) to the detail coefficients <span class="math inline">\(d_{j, k}\)</span> to reduce noise and prevent overfitting. This is especially useful when the true signal is sparse in the wavelet domain.</li>
</ul>
<hr>
</div>
<div id="wavelet-shrinkage-and-thresholding" class="section level3" number="10.9.3">
<h3>
<span class="header-section-number">10.9.3</span> Wavelet Shrinkage and Thresholding<a class="anchor" aria-label="anchor" href="#wavelet-shrinkage-and-thresholding"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Wavelet shrinkage</strong> is a powerful denoising technique introduced by <span class="citation">Donoho and Johnstone (<a href="references.html#ref-donoho1995adapting">1995</a>)</span>. The idea is to suppress small coefficients (likely to be noise) while retaining large coefficients (likely to contain the true signal).</p>
<p><strong>Thresholding Rules:</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Hard Thresholding:</strong></li>
</ol>
<p><span class="math display">\[
\hat{d}_{j, k}^{\text{(hard)}} =
\begin{cases}
d_{j, k}, &amp; \text{if } |d_{j, k}| &gt; \tau, \\
0, &amp; \text{otherwise},
\end{cases}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Soft Thresholding:</strong></li>
</ol>
<p><span class="math display">\[
\hat{d}_{j, k}^{\text{(soft)}} =
\operatorname{sign}(d_{j, k}) \cdot \max\left(|d_{j, k}| - \tau, \, 0\right),
\]</span></p>
<p>where <span class="math inline">\(\tau\)</span> is the <strong>threshold parameter</strong>, often chosen based on the noise level (e.g., via cross-validation or universal thresholding).</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="12%">
<col width="47%">
<col width="19%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th><strong>Aspect</strong></th>
<th>
<a href="sec-nonparametric-regression.html#sec-kernel-regression"><strong>Kernel</strong></a><strong>/<a href="sec-nonparametric-regression.html#sec-local-polynomial-regression">Local Polynomial</a></strong>
</th>
<th><a href="sec-nonparametric-regression.html#sec-smoothing-splines"><strong>Splines</strong></a></th>
<th><a href="sec-nonparametric-regression.html#sec-wavelet-regression"><strong>Wavelets</strong></a></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Smoothness</strong></td>
<td>Smooth, localized</td>
<td>Globally smooth with knots</td>
<td>Captures sharp discontinuities</td>
</tr>
<tr class="even">
<td><strong>Basis Functions</strong></td>
<td>Kernel functions</td>
<td>Piecewise polynomials</td>
<td>Compact, oscillatory wavelets</td>
</tr>
<tr class="odd">
<td><strong>Handling of Noise</strong></td>
<td>Prone to overfitting without smoothing</td>
<td>Controlled via penalties</td>
<td>Excellent via thresholding</td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td>Moderate</td>
<td>High (for large knots)</td>
<td>Efficient (fast wavelet transform)</td>
</tr>
<tr class="odd">
<td><strong>Applications</strong></td>
<td>Curve fitting, density estimation</td>
<td>Smoothing trends</td>
<td>Signal processing, time series</td>
</tr>
</tbody>
</table></div>
<hr>
<div class="sourceCode" id="cb401"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulated data: Noisy signal with discontinuities</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">96</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="va">n</span><span class="op">)</span></span>
<span><span class="va">signal</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="fl">4</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>  <span class="co"># Discontinuity at x = 0.5</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">signal</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Wavelet Regression using Discrete Wavelet Transform (DWT)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://waveslim.blogspot.com">waveslim</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply DWT with the correct parameter name</span></span>
<span><span class="va">dwt_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/waveslim/man/dwt.html">dwt</a></span><span class="op">(</span><span class="va">y</span>, wf <span class="op">=</span> <span class="st">"haar"</span>, n.levels <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Thresholding (currently hard thresholding)</span></span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">0.2</span></span>
<span><span class="va">dwt_result_thresh</span> <span class="op">&lt;-</span> <span class="va">dwt_result</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">dwt_result_thresh</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">dwt_result</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="op">&gt;</span> <span class="va">threshold</span>, <span class="va">dwt_result</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># for soft thresholding</span></span>
<span><span class="co"># for (i in 1:4) {</span></span>
<span><span class="co">#     dwt_result_thresh[[i]] &lt;-</span></span>
<span><span class="co">#         sign(dwt_result[[i]]) * pmax(abs(dwt_result[[i]]) - threshold, 0)</span></span>
<span><span class="co"># }</span></span>
<span></span>
<span></span>
<span><span class="co"># Inverse DWT to reconstruct the signal</span></span>
<span><span class="va">y_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/waveslim/man/dwt.html">idwt</a></span><span class="op">(</span><span class="va">dwt_result_thresh</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plotting</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="va">x</span>,</span>
<span>    <span class="va">y</span>,</span>
<span>    type <span class="op">=</span> <span class="st">"l"</span>,</span>
<span>    col <span class="op">=</span> <span class="st">"gray"</span>,</span>
<span>    lwd <span class="op">=</span> <span class="fl">1</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Wavelet Regression (Denoising)"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x</span>,</span>
<span>      <span class="va">signal</span>,</span>
<span>      col <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>      lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>      lty <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>  <span class="co"># True signal</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y_hat</span>, col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>             <span class="co"># Denoised estimate</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span></span>
<span>    <span class="st">"topright"</span>,</span>
<span>    legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Noisy Data"</span>, <span class="st">"True Signal"</span>, <span class="st">"Wavelet Estimate"</span><span class="op">)</span>,</span>
<span>    col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gray"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span><span class="op">)</span>,</span>
<span>    lty <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>    lwd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>Advantages:</strong></p>
<ul>
<li><p><strong>Local Feature Detection:</strong> Excellent for capturing sharp changes, discontinuities, and localized phenomena.</p></li>
<li><p><strong>Multiresolution Analysis:</strong> Analyzes data at multiple scales, making it effective for both global trends and fine details.</p></li>
<li><p><strong>Denoising Capability:</strong> Powerful noise reduction through thresholding in the wavelet domain.</p></li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><p><strong>Complexity:</strong> Requires careful selection of wavelet basis, decomposition levels, and thresholding methods.</p></li>
<li><p><strong>Less Interpretability:</strong> Coefficients are less interpretable compared to spline or tree-based methods.</p></li>
<li><p><strong>Boundary Effects:</strong> May suffer from artifacts near data boundaries without proper treatment.</p></li>
</ul>
<div class="sourceCode" id="cb402"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://waveslim.blogspot.com">waveslim</a></span><span class="op">)</span>   <span class="co"># For Discrete Wavelet Transform (DWT)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Simulate Data: Noisy signal with discontinuities</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">96</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># True signal: Sinusoidal with a discontinuity at x = 0.5</span></span>
<span><span class="va">signal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="fl">4</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Add Gaussian noise</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">signal</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot the noisy data and true signal</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"gray"</span>,</span>
<span>              size <span class="op">=</span> <span class="fl">0.8</span>,</span>
<span>              alpha <span class="op">=</span> <span class="fl">0.7</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">signal</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">1</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Noisy Data with Underlying True Signal"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"x"</span>, y <span class="op">=</span> <span class="st">"Signal"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/unnamed-chunk-8-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb403"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Apply Discrete Wavelet Transform (DWT) using Haar wavelet</span></span>
<span><span class="va">dwt_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/waveslim/man/dwt.html">dwt</a></span><span class="op">(</span><span class="va">y</span>, wf <span class="op">=</span> <span class="st">"haar"</span>, n.levels <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># View DWT structure</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">dwt_result</span><span class="op">)</span></span>
<span><span class="co">#&gt; List of 5</span></span>
<span><span class="co">#&gt;  $ d1: num [1:48] 0.14 -0.1221 0.3017 -0.1831 0.0745 ...</span></span>
<span><span class="co">#&gt;  $ d2: num [1:24] 0.50003 -0.06828 0.35298 0.01582 -0.00853 ...</span></span>
<span><span class="co">#&gt;  $ d3: num [1:12] 0.669 0.128 -0.746 -0.532 -0.258 ...</span></span>
<span><span class="co">#&gt;  $ d4: num [1:6] 1.07 -1.766 0.752 0.663 -1.963 ...</span></span>
<span><span class="co">#&gt;  $ s4: num [1:6] 2.963 -0.159 -2.758 6.945 3.845 ...</span></span>
<span><span class="co">#&gt;  - attr(*, "class")= chr "dwt"</span></span>
<span><span class="co">#&gt;  - attr(*, "wavelet")= chr "haar"</span></span>
<span><span class="co">#&gt;  - attr(*, "boundary")= chr "periodic"</span></span></code></pre></div>
<ul>
<li>
<code>wf = "haar"</code>: Haar wavelet, simple and effective for detecting discontinuities.</li>
<li>
<code>n.levels = 4</code>: Number of decomposition levels (captures details at different scales).</li>
</ul>
<p>The DWT output contains approximation coefficients and detail coefficients for each level.</p>
<div class="sourceCode" id="cb404"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Hard Thresholding</span></span>
<span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="fl">0.2</span>  <span class="co"># Chosen threshold for demonstration</span></span>
<span><span class="va">dwt_hard_thresh</span> <span class="op">&lt;-</span> <span class="va">dwt_result</span></span>
<span></span>
<span><span class="co"># Apply hard thresholding to detail coefficients</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">dwt_hard_thresh</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">dwt_result</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="op">&gt;</span> <span class="va">threshold</span>, <span class="va">dwt_result</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="sourceCode" id="cb405"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Soft Thresholding</span></span>
<span><span class="va">dwt_soft_thresh</span> <span class="op">&lt;-</span> <span class="va">dwt_result</span></span>
<span></span>
<span><span class="co"># Apply soft thresholding to detail coefficients</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">dwt_soft_thresh</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sign.html">sign</a></span><span class="op">(</span><span class="va">dwt_result</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">pmax</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">dwt_result</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="va">threshold</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<ul>
<li>Hard Thresholding: Keeps coefficients above the threshold, sets others to zero.</li>
<li>Soft Thresholding: Shrinks coefficients toward zero, reducing potential noise smoothly.</li>
</ul>
<div class="sourceCode" id="cb406"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Reconstruct the denoised signals using Inverse DWT</span></span>
<span><span class="va">y_hat_hard</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/waveslim/man/dwt.html">idwt</a></span><span class="op">(</span><span class="va">dwt_hard_thresh</span><span class="op">)</span></span>
<span><span class="va">y_hat_soft</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/waveslim/man/dwt.html">idwt</a></span><span class="op">(</span><span class="va">dwt_soft_thresh</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb407"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Combine data for ggplot</span></span>
<span><span class="va">df_plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">signal</span>, <span class="va">y_hat_hard</span>, <span class="va">y_hat_soft</span><span class="op">)</span>,</span>
<span>    Type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>            <span class="st">"Noisy Data"</span>,</span>
<span>            <span class="st">"True Signal"</span>,</span>
<span>            <span class="st">"Hard Thresholding"</span>,</span>
<span>            <span class="st">"Soft Thresholding"</span></span>
<span>        <span class="op">)</span>,</span>
<span>        each <span class="op">=</span> <span class="va">n</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plotting</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">df_plot</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, color <span class="op">=</span> <span class="va">Type</span>, linetype <span class="op">=</span> <span class="va">Type</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gray"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"green"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_linetype_manual</a></span><span class="op">(</span>values <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"solid"</span>, <span class="st">"dashed"</span>, <span class="st">"solid"</span>, <span class="st">"solid"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>        title <span class="op">=</span> <span class="st">"Wavelet Regression (Denoising)"</span>,</span>
<span>        subtitle <span class="op">=</span> <span class="st">"Comparison of Hard vs. Soft Thresholding"</span>,</span>
<span>        x <span class="op">=</span> <span class="st">"x"</span>,</span>
<span>        y <span class="op">=</span> <span class="st">"Signal"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"top"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Visualization%20of%20Wavelet%20Denoising-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb408"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute Mean Squared Error (MSE) for each method</span></span>
<span><span class="va">mse_noisy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">signal</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mse_hard</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">y_hat_hard</span> <span class="op">-</span> <span class="va">signal</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mse_soft</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">y_hat_soft</span> <span class="op">-</span> <span class="va">signal</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display MSE comparison</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Noisy Data"</span>, <span class="st">"Hard Thresholding"</span>, <span class="st">"Soft Thresholding"</span><span class="op">)</span>,</span>
<span>    MSE <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">mse_noisy</span>, <span class="va">mse_hard</span>, <span class="va">mse_soft</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;              Method        MSE</span></span>
<span><span class="co">#&gt; 1        Noisy Data 0.03127707</span></span>
<span><span class="co">#&gt; 2 Hard Thresholding 0.02814465</span></span>
<span><span class="co">#&gt; 3 Soft Thresholding 0.02267171</span></span></code></pre></div>
<ul>
<li>Lower MSE indicates better denoising performance.</li>
<li>Soft thresholding often achieves smoother results with lower MSE.</li>
</ul>
<hr>
</div>
</div>
<div id="multivariate-nonparametric-regression" class="section level2" number="10.10">
<h2>
<span class="header-section-number">10.10</span> Multivariate Nonparametric Regression<a class="anchor" aria-label="anchor" href="#multivariate-nonparametric-regression"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Nonparametric regression in higher dimensions</strong> (<span class="math inline">\(p &gt; 1\)</span>) presents significant challenges compared to the univariate case. This difficulty arises primarily from the <strong>curse of dimensionality</strong>, which refers to the exponential growth of data requirements as the number of predictors increases.</p>
<hr>
<div id="the-curse-of-dimensionality" class="section level3" number="10.10.1">
<h3>
<span class="header-section-number">10.10.1</span> The Curse of Dimensionality<a class="anchor" aria-label="anchor" href="#the-curse-of-dimensionality"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>curse of dimensionality</strong> refers to various phenomena that occur when analyzing and organizing data in high-dimensional spaces. In the context of nonparametric regression:</p>
<ul>
<li>
<strong>Data sparsity:</strong> As the number of dimensions increases, data become sparse. Even large datasets may not adequately cover the predictor space.</li>
<li>
<strong>Exponential sample size growth:</strong> To achieve the same level of accuracy, the required sample size grows exponentially with the number of dimensions. For example, to maintain the same density of points when moving from 1D to 2D, you need roughly the square of the sample size.</li>
</ul>
<p><strong>Illustration:</strong></p>
<p>Consider estimating a function on the unit cube <span class="math inline">\([0,1]^p\)</span>. If we need 10 points per dimension to capture the structure:</p>
<ul>
<li>In 1D: 10 points suffice.</li>
<li>In 2D: <span class="math inline">\(10^2 = 100\)</span> points are needed.</li>
<li>In 10D: <span class="math inline">\(10^{10} = 10,000,000,000\)</span> points are required.</li>
</ul>
<p>This makes traditional nonparametric methods, like kernel smoothing, impractical in high dimensions without additional structure or assumptions.</p>
<hr>
</div>
<div id="multivariate-kernel-regression" class="section level3" number="10.10.2">
<h3>
<span class="header-section-number">10.10.2</span> Multivariate Kernel Regression<a class="anchor" aria-label="anchor" href="#multivariate-kernel-regression"><i class="fas fa-link"></i></a>
</h3>
<p>A straightforward extension of kernel regression to higher dimensions is the multivariate <a href="sec-nonparametric-regression.html#sec-nadaraya-watson-kernel-estimator">Nadaraya-Watson estimator</a>:</p>
<p><span class="math display">\[
\hat{m}_h(\mathbf{x}) = \frac{\sum_{i=1}^n K_h(\mathbf{x} - \mathbf{x}_i) \, y_i}{\sum_{i=1}^n K_h(\mathbf{x} - \mathbf{x}_i)},
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_p)\)</span> is the predictor vector,</p></li>
<li>
<p><span class="math inline">\(K_h(\cdot)\)</span> is a <strong>multivariate kernel function</strong>, often a product of univariate kernels:</p>
<p><span class="math display">\[
K_h(\mathbf{x} - \mathbf{x}_i) = \prod_{j=1}^p K\left(\frac{x_j - x_{ij}}{h_j}\right),
\]</span></p>
</li>
<li><p><span class="math inline">\(h_j\)</span> is the bandwidth for the <span class="math inline">\(j\)</span>-th predictor.</p></li>
</ul>
<p><strong>Challenges:</strong></p>
<ul>
<li>The product kernel suffers from <strong>inefficiency</strong> in high dimensions because most data points are far from any given target point <span class="math inline">\(\mathbf{x}\)</span>, resulting in very small kernel weights.</li>
<li>Selecting an optimal <strong>multivariate bandwidth matrix</strong> is complex and computationally intensive.</li>
</ul>
<hr>
</div>
<div id="multivariate-splines" class="section level3" number="10.10.3">
<h3>
<span class="header-section-number">10.10.3</span> Multivariate Splines<a class="anchor" aria-label="anchor" href="#multivariate-splines"><i class="fas fa-link"></i></a>
</h3>
<p>Extending splines to multiple dimensions involves more sophisticated techniques, as the simplicity of piecewise polynomials in 1D does not generalize easily.</p>
<div id="thin-plate-splines" class="section level4" number="10.10.3.1">
<h4>
<span class="header-section-number">10.10.3.1</span> Thin-Plate Splines<a class="anchor" aria-label="anchor" href="#thin-plate-splines"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Thin-plate splines</strong> generalize cubic splines to higher dimensions. They minimize a smoothness penalty that depends on the derivatives of the function:</p>
<p><span class="math display">\[
\hat{m}(\mathbf{x}) = \underset{f}{\arg\min} \left\{ \sum_{i=1}^n (y_i - f(\mathbf{x}_i))^2 + \lambda \int \|\nabla^2 f(\mathbf{x})\|^2 d\mathbf{x} \right\},
\]</span></p>
<p>where <span class="math inline">\(\nabla^2 f(\mathbf{x})\)</span> is the <strong>Hessian matrix</strong> of second derivatives, and <span class="math inline">\(\|\cdot\|^2\)</span> represents the sum of squared elements.</p>
<p>Thin-plate splines are <strong>rotation-invariant</strong> and do not require explicit placement of knots, but they become computationally expensive as the number of dimensions increases.</p>
</div>
<div id="tensor-product-splines" class="section level4" number="10.10.3.2">
<h4>
<span class="header-section-number">10.10.3.2</span> Tensor Product Splines<a class="anchor" aria-label="anchor" href="#tensor-product-splines"><i class="fas fa-link"></i></a>
</h4>
<p>For structured multivariate data, <strong>tensor product splines</strong> are commonly used. They construct a basis for each predictor and form the multivariate basis via tensor products:</p>
<p><span class="math display">\[
\hat{m}(\mathbf{x}) = \sum_{i=1}^{K_1} \sum_{j=1}^{K_2} \beta_{ij} \, B_i(x_1) \, B_j(x_2),
\]</span></p>
<p>where <span class="math inline">\(B_i(x_1)\)</span> and <span class="math inline">\(B_j(x_2)\)</span> are spline basis functions for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, respectively.</p>
<p>Tensor products allow flexible modeling of <strong>interactions</strong> between predictors but can lead to large numbers of parameters as the number of dimensions increases.</p>
<hr>
</div>
</div>
<div id="additive-models-gams" class="section level3" number="10.10.4">
<h3>
<span class="header-section-number">10.10.4</span> Additive Models (GAMs)<a class="anchor" aria-label="anchor" href="#additive-models-gams"><i class="fas fa-link"></i></a>
</h3>
<p>A powerful approach to mitigating the curse of dimensionality is to assume an <strong>additive structure</strong> for the regression function:</p>
<p><span class="math display">\[
m(\mathbf{x}) = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p),
\]</span></p>
<p>where each <span class="math inline">\(f_j\)</span> is a univariate smooth function estimated nonparametrically.</p>
<div id="why-additive-models-help" class="section level4" number="10.10.4.1">
<h4>
<span class="header-section-number">10.10.4.1</span> Why Additive Models Help:<a class="anchor" aria-label="anchor" href="#why-additive-models-help"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<strong>Dimensionality Reduction:</strong> Instead of estimating a full <span class="math inline">\(p\)</span>-dimensional surface, we estimate <span class="math inline">\(p\)</span> separate functions in 1D.</li>
<li>
<strong>Interpretability:</strong> Each function <span class="math inline">\(f_j(x_j)\)</span> represents the effect of predictor <span class="math inline">\(X_j\)</span> on the response, holding other variables constant.</li>
</ul>
<p><strong>Extensions</strong>:</p>
<ul>
<li>
<p><strong>Interactions:</strong> Additive models can be extended to include interactions:</p>
<p><span class="math display">\[
m(\mathbf{x}) = \beta_0 + \sum_{j=1}^p f_j(x_j) + \sum_{j &lt; k} f_{jk}(x_j, x_k),
\]</span></p>
<p>where <span class="math inline">\(f_{jk}\)</span> are bivariate smooth functions capturing interaction effects.</p>
</li>
</ul>
<hr>
</div>
</div>
<div id="radial-basis-functions" class="section level3" number="10.10.5">
<h3>
<span class="header-section-number">10.10.5</span> Radial Basis Functions<a class="anchor" aria-label="anchor" href="#radial-basis-functions"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Radial basis functions</strong> <strong>(RBF)</strong> are another approach to multivariate nonparametric regression, particularly effective in scattered data interpolation.</p>
<p>A typical RBF model is:</p>
<p><span class="math display">\[
\hat{m}(\mathbf{x}) = \sum_{i=1}^n \alpha_i \, \phi(\|\mathbf{x} - \mathbf{x}_i\|),
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\phi(\cdot)\)</span> is a radial basis function (e.g., Gaussian: <span class="math inline">\(\phi(r) = e^{-\gamma r^2}\)</span>),</li>
<li>
<span class="math inline">\(\|\mathbf{x} - \mathbf{x}_i\|\)</span> is the Euclidean distance between <span class="math inline">\(\mathbf{x}\)</span> and the data point <span class="math inline">\(\mathbf{x}_i\)</span>,</li>
<li>
<span class="math inline">\(\alpha_i\)</span> are coefficients estimated from the data.</li>
</ul>
<hr>
<div class="sourceCode" id="cb409"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/JeffreyRacine/R-Package-np">np</a></span><span class="op">)</span>       <span class="co"># For multivariate kernel regression</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">mgcv</span><span class="op">)</span>     <span class="co"># For thin-plate and tensor product splines</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/dnychka/fieldsRPackage">fields</a></span><span class="op">)</span>   <span class="co"># For radial basis functions (RBF)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/hadley/reshape">reshape2</a></span><span class="op">)</span> <span class="co"># For data manipulation</span></span>
<span></span>
<span></span>
<span><span class="co"># Simulate Multivariate Data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">x3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># True nonlinear multivariate function with interaction effects</span></span>
<span><span class="va">true_function</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">pi</span> <span class="op">*</span> <span class="va">x1</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="va">pi</span> <span class="op">*</span> <span class="va">x2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="op">(</span><span class="va">x3</span> <span class="op">-</span> <span class="fl">2.5</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">x1</span> <span class="op">*</span> <span class="va">x3</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Response with noise</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">true_function</span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Data frame</span></span>
<span><span class="va">data_multi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualization of marginal relationships</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_multi</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x1"</span><span class="op">)</span></span>
<span><span class="va">p2</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_multi</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x2</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x2"</span><span class="op">)</span></span>
<span><span class="va">p3</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_multi</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x3</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Effect of x3"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">gridExtra</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">p1</span>, <span class="va">p2</span>, <span class="va">p3</span>, ncol <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Simulate%20Multivariate%20Data-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="sec-nonparametric-regression.html#cb410-1" tabindex="-1"></a><span class="co"># Multivariate Kernel Regression using np package</span></span>
<span id="cb410-2"><a href="sec-nonparametric-regression.html#cb410-2" tabindex="-1"></a>bw <span class="ot">&lt;-</span></span>
<span id="cb410-3"><a href="sec-nonparametric-regression.html#cb410-3" tabindex="-1"></a>    <span class="fu">npregbw</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">+</span> x3, <span class="at">data =</span> data_multi)  <span class="co"># Bandwidth selection</span></span>
<span id="cb410-4"><a href="sec-nonparametric-regression.html#cb410-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb410-5"><a href="sec-nonparametric-regression.html#cb410-5" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-6"><a href="sec-nonparametric-regression.html#cb410-6" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-7"><a href="sec-nonparametric-regression.html#cb410-7" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-8"><a href="sec-nonparametric-regression.html#cb410-8" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">/</span></span>
<span id="cb410-9"><a href="sec-nonparametric-regression.html#cb410-9" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">-</span></span>
<span id="cb410-10"><a href="sec-nonparametric-regression.html#cb410-10" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-11"><a href="sec-nonparametric-regression.html#cb410-11" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-12"><a href="sec-nonparametric-regression.html#cb410-12" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">/</span></span>
<span id="cb410-13"><a href="sec-nonparametric-regression.html#cb410-13" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> <span class="sc">-</span></span>
<span id="cb410-14"><a href="sec-nonparametric-regression.html#cb410-14" tabindex="-1"></a>Multistart <span class="dv">1</span> of <span class="dv">3</span> \</span>
<span id="cb410-15"><a href="sec-nonparametric-regression.html#cb410-15" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-16"><a href="sec-nonparametric-regression.html#cb410-16" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-17"><a href="sec-nonparametric-regression.html#cb410-17" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">/</span></span>
<span id="cb410-18"><a href="sec-nonparametric-regression.html#cb410-18" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">-</span></span>
<span id="cb410-19"><a href="sec-nonparametric-regression.html#cb410-19" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> \</span>
<span id="cb410-20"><a href="sec-nonparametric-regression.html#cb410-20" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-21"><a href="sec-nonparametric-regression.html#cb410-21" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-22"><a href="sec-nonparametric-regression.html#cb410-22" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-23"><a href="sec-nonparametric-regression.html#cb410-23" tabindex="-1"></a>Multistart <span class="dv">2</span> of <span class="dv">3</span> <span class="sc">/</span></span>
<span id="cb410-24"><a href="sec-nonparametric-regression.html#cb410-24" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-25"><a href="sec-nonparametric-regression.html#cb410-25" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-26"><a href="sec-nonparametric-regression.html#cb410-26" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">/</span></span>
<span id="cb410-27"><a href="sec-nonparametric-regression.html#cb410-27" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">-</span></span>
<span id="cb410-28"><a href="sec-nonparametric-regression.html#cb410-28" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> \</span>
<span id="cb410-29"><a href="sec-nonparametric-regression.html#cb410-29" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-30"><a href="sec-nonparametric-regression.html#cb410-30" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-31"><a href="sec-nonparametric-regression.html#cb410-31" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">|</span></span>
<span id="cb410-32"><a href="sec-nonparametric-regression.html#cb410-32" tabindex="-1"></a>Multistart <span class="dv">3</span> of <span class="dv">3</span> <span class="sc">/</span></span>
<span id="cb410-33"><a href="sec-nonparametric-regression.html#cb410-33" tabindex="-1"></a>                   </span>
<span id="cb410-34"><a href="sec-nonparametric-regression.html#cb410-34" tabindex="-1"></a>kernel_model <span class="ot">&lt;-</span> <span class="fu">npreg</span>(<span class="at">bws =</span> bw)</span>
<span id="cb410-35"><a href="sec-nonparametric-regression.html#cb410-35" tabindex="-1"></a></span>
<span id="cb410-36"><a href="sec-nonparametric-regression.html#cb410-36" tabindex="-1"></a><span class="co"># Predict on a grid for visualization</span></span>
<span id="cb410-37"><a href="sec-nonparametric-regression.html#cb410-37" tabindex="-1"></a>grid_data <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb410-38"><a href="sec-nonparametric-regression.html#cb410-38" tabindex="-1"></a>    <span class="at">x1 =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">50</span>),</span>
<span id="cb410-39"><a href="sec-nonparametric-regression.html#cb410-39" tabindex="-1"></a>    <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">50</span>),</span>
<span id="cb410-40"><a href="sec-nonparametric-regression.html#cb410-40" tabindex="-1"></a>    <span class="at">x3 =</span> <span class="fu">mean</span>(data_multi<span class="sc">$</span>x3)</span>
<span id="cb410-41"><a href="sec-nonparametric-regression.html#cb410-41" tabindex="-1"></a>)</span>
<span id="cb410-42"><a href="sec-nonparametric-regression.html#cb410-42" tabindex="-1"></a>pred_kernel <span class="ot">&lt;-</span> <span class="fu">predict</span>(kernel_model, <span class="at">newdata =</span> grid_data)</span>
<span id="cb410-43"><a href="sec-nonparametric-regression.html#cb410-43" tabindex="-1"></a></span>
<span id="cb410-44"><a href="sec-nonparametric-regression.html#cb410-44" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb410-45"><a href="sec-nonparametric-regression.html#cb410-45" tabindex="-1"></a>grid_data<span class="sc">$</span>pred <span class="ot">&lt;-</span> pred_kernel</span>
<span id="cb410-46"><a href="sec-nonparametric-regression.html#cb410-46" tabindex="-1"></a><span class="fu">ggplot</span>(grid_data, <span class="fu">aes</span>(x1, x2, <span class="at">fill =</span> pred)) <span class="sc">+</span></span>
<span id="cb410-47"><a href="sec-nonparametric-regression.html#cb410-47" tabindex="-1"></a>    <span class="fu">geom_raster</span>() <span class="sc">+</span></span>
<span id="cb410-48"><a href="sec-nonparametric-regression.html#cb410-48" tabindex="-1"></a>    <span class="fu">scale_fill_viridis_c</span>() <span class="sc">+</span></span>
<span id="cb410-49"><a href="sec-nonparametric-regression.html#cb410-49" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Multivariate Kernel Regression (x3 fixed)"</span>,</span>
<span id="cb410-50"><a href="sec-nonparametric-regression.html#cb410-50" tabindex="-1"></a>         <span class="at">x =</span> <span class="st">"x1"</span>,</span>
<span id="cb410-51"><a href="sec-nonparametric-regression.html#cb410-51" tabindex="-1"></a>         <span class="at">y =</span> <span class="st">"x2"</span>) <span class="sc">+</span></span>
<span id="cb410-52"><a href="sec-nonparametric-regression.html#cb410-52" tabindex="-1"></a>    <span class="fu">theme_minimal</span>()</span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Multivariate%20Kernel%20Regression%20(Nadaraya-Watson)-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>Kernel regression captures nonlinear interactions but suffers from data sparsity in high dimensions (curse of dimensionality).</li>
<li>Bandwidth selection is critical for performance.</li>
</ul>
<div class="sourceCode" id="cb411"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit Thin-Plate Spline</span></span>
<span><span class="va">tps_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span>, bs <span class="op">=</span> <span class="st">"tp"</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data_multi</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictions</span></span>
<span><span class="va">grid_data</span><span class="op">$</span><span class="va">pred_tps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tps_model</span>, newdata <span class="op">=</span> <span class="va">grid_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualization</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">grid_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, fill <span class="op">=</span> <span class="va">pred_tps</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_raster</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_fill_viridis_c</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Thin-Plate Spline (x3 fixed)"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"x1"</span>, y <span class="op">=</span> <span class="st">"x2"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Thin-Plate%20Splines%20(with%20mgcv)-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>Thin-plate splines handle smooth surfaces well and are rotation-invariant.</li>
<li>Computational cost increases with higher dimensions due to matrix operations.</li>
</ul>
<div class="sourceCode" id="cb412"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit Tensor Product Spline</span></span>
<span><span class="va">tensor_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/te.html">te</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data_multi</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictions</span></span>
<span><span class="va">grid_data</span><span class="op">$</span><span class="va">pred_tensor</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tensor_model</span>, newdata <span class="op">=</span> <span class="va">grid_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualization</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">grid_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, fill <span class="op">=</span> <span class="va">pred_tensor</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_raster</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_fill_viridis_c</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Tensor Product Spline (x3 fixed)"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"x1"</span>, y <span class="op">=</span> <span class="st">"x2"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Tensor%20Product%20Splines%20(for%20Structured%20Interactions)-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>Tensor product splines model interactions explicitly between variables.</li>
<li>Suitable when data have structured dependencies but can lead to many parameters in higher dimensions.</li>
</ul>
<div class="sourceCode" id="cb413"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Additive Model (GAM)</span></span>
<span><span class="va">gam_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/gam.html">gam</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">x3</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data_multi</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictions</span></span>
<span><span class="va">grid_data</span><span class="op">$</span><span class="va">pred_gam</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">gam_model</span>, newdata <span class="op">=</span> <span class="va">grid_data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualization</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">grid_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, fill <span class="op">=</span> <span class="va">pred_gam</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_raster</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_fill_viridis_c</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Additive Model (GAM, x3 fixed)"</span>, </span>
<span>         x <span class="op">=</span> <span class="st">"x1"</span>, y <span class="op">=</span> <span class="st">"x2"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Additive%20Models%20(GAMs)%20to%20Mitigate%20Curse%20of%20Dimensionality-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Why GAMs Help:</p>
<ul>
<li>Dimensionality reduction: Instead of estimating a full multivariate function, GAM estimates separate 1D functions for each predictor.</li>
<li>Interpretability: Easy to understand individual effects.</li>
<li>Limitations: Cannot capture complex interactions unless explicitly added.</li>
</ul>
<div class="sourceCode" id="cb414"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Radial Basis Function Model</span></span>
<span><span class="va">rbf_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/fields/man/Tps.html">Tps</a></span><span class="op">(</span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span>, <span class="va">y</span><span class="op">)</span>  <span class="co"># Thin-plate spline RBF</span></span>
<span><span class="co">#&gt; Warning: </span></span>
<span><span class="co">#&gt; Grid searches over lambda (nugget and sill variances) with  minima at the endpoints: </span></span>
<span><span class="co">#&gt;   (GCV) Generalized Cross-Validation </span></span>
<span><span class="co">#&gt;    minimum at  right endpoint  lambda  =  0.0002622876 (eff. df= 95.00001 )</span></span>
<span></span>
<span><span class="co"># Predictions</span></span>
<span><span class="va">grid_data</span><span class="op">$</span><span class="va">pred_rbf</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">rbf_model</span>, <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">grid_data</span><span class="op">$</span><span class="va">x1</span>, <span class="va">grid_data</span><span class="op">$</span><span class="va">x2</span>, <span class="va">grid_data</span><span class="op">$</span><span class="va">x3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualization</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">grid_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, fill <span class="op">=</span> <span class="va">pred_rbf</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_raster</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_fill_viridis_c</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Radial Basis Function Regression (x3 fixed)"</span>, </span>
<span>         x <span class="op">=</span> <span class="st">"x1"</span>, y <span class="op">=</span> <span class="st">"x2"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="10-nonparam-regression_files/figure-html/Radial%20Basis%20Functions%20(RBFs)%20for%20Multivariate%20Regression-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>RBFs capture complex, smooth surfaces and interactions based on distance.</li>
<li>Perform well for scattered data but can be computationally expensive for large datasets.</li>
</ul>
<div class="sourceCode" id="cb415"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute Mean Squared Error for each model</span></span>
<span><span class="va">mse_kernel</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">kernel_model</span><span class="op">)</span> <span class="op">-</span> <span class="va">data_multi</span><span class="op">$</span><span class="va">y</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mse_tps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tps_model</span><span class="op">)</span> <span class="op">-</span> <span class="va">data_multi</span><span class="op">$</span><span class="va">y</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mse_tensor</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tensor_model</span><span class="op">)</span> <span class="op">-</span> <span class="va">data_multi</span><span class="op">$</span><span class="va">y</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mse_gam</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">gam_model</span><span class="op">)</span> <span class="op">-</span> <span class="va">data_multi</span><span class="op">$</span><span class="va">y</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mse_rbf</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">rbf_model</span>, <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">data_multi</span><span class="op">$</span><span class="va">y</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display MSE comparison</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    Model <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>        <span class="st">"Kernel Regression"</span>,</span>
<span>        <span class="st">"Thin-Plate Spline"</span>,</span>
<span>        <span class="st">"Tensor Product Spline"</span>,</span>
<span>        <span class="st">"Additive Model (GAM)"</span>,</span>
<span>        <span class="st">"Radial Basis Functions"</span></span>
<span>    <span class="op">)</span>,</span>
<span>    MSE <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">mse_kernel</span>, <span class="va">mse_tps</span>, <span class="va">mse_tensor</span>, <span class="va">mse_gam</span>, <span class="va">mse_rbf</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;                    Model         MSE</span></span>
<span><span class="co">#&gt; 1      Kernel Regression 0.253019836</span></span>
<span><span class="co">#&gt; 2      Thin-Plate Spline 0.449096723</span></span>
<span><span class="co">#&gt; 3  Tensor Product Spline 0.304680406</span></span>
<span><span class="co">#&gt; 4   Additive Model (GAM) 1.595616092</span></span>
<span><span class="co">#&gt; 5 Radial Basis Functions 0.001361915</span></span></code></pre></div>
<hr>
</div>
</div>
<div id="conclusion-the-evolving-landscape-of-regression-analysis" class="section level2" number="10.11">
<h2>
<span class="header-section-number">10.11</span> Conclusion: The Evolving Landscape of Regression Analysis<a class="anchor" aria-label="anchor" href="#conclusion-the-evolving-landscape-of-regression-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>As we conclude this exploration of regression analysis, we reflect on the vast landscape we have navigated—spanning from the foundational principles of <a href="linear-regression.html#linear-regression"><strong>linear regression</strong></a> to the intricate complexities of <a href="generalized-linear-models.html#generalized-linear-models"><strong>generalized linear models</strong></a>, <a href="sec-linear-mixed-models.html#sec-linear-mixed-models"><strong>linear mixed models</strong></a>, <a href="sec-nonlinear-and-generalized-linear-mixed-models.html#sec-nonlinear-mixed-models"><strong>nonlinear mixed models</strong></a>, and now, the flexible world of <a href="sec-nonparametric-regression.html#sec-nonparametric-regression"><strong>nonparametric regression</strong></a>.</p>
<p>Regression analysis is more than just a statistical tool; it is a <strong>versatile framework</strong> that underpins decision-making across disciplines—from marketing and finance to healthcare, engineering, and beyond. This journey has shown how regression serves not only as a method for modeling relationships but also as a lens through which we interpret complex data in an ever-changing world.</p>
<hr>
<div id="key-takeaways-1" class="section level3" number="10.11.1">
<h3>
<span class="header-section-number">10.11.1</span> Key Takeaways<a class="anchor" aria-label="anchor" href="#key-takeaways-1"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p><strong>The Power of Simplicity:</strong> At its core, simple <a href="linear-regression.html#linear-regression"><strong>linear regression</strong></a> illustrates how relationships between variables can be modeled with clarity and elegance. Mastering these fundamentals lays the groundwork for more complex techniques.</p></li>
<li><p><strong>Beyond Linearity:</strong> <a href="non-linear-regression.html#non-linear-regression"><strong>Nonlinear regression</strong></a> and <a href="generalized-linear-models.html#generalized-linear-models"><strong>generalized linear models</strong></a> extend our capabilities to handle data that defy linear assumptions—capturing curved relationships, non-normal error structures, and diverse outcome distributions.</p></li>
<li><p><strong>Accounting for Hierarchies and Dependencies:</strong> Real-world data often exhibit structures such as nested observations or repeated measures. <a href="sec-linear-mixed-models.html#sec-linear-mixed-models"><strong>Linear mixed models</strong></a> and <a href="sec-nonlinear-and-generalized-linear-mixed-models.html#sec-generalized-linear-mixed-models"><strong>generalized linear mixed models</strong></a> enable us to account for both fixed effects and random variability, ensuring robust and nuanced inferences.</p></li>
<li><p><strong>Complex Systems, Flexible Models:</strong> <a href="sec-nonlinear-and-generalized-linear-mixed-models.html#sec-nonlinear-mixed-models"><strong>Nonlinear mixed models</strong></a> allow us to capture dynamic, non-linear processes with hierarchical structures, bridging the gap between theoretical models and real-world complexity.</p></li>
<li><p><strong>The Flexibility of Nonparametric Regression:</strong> <a href="sec-nonparametric-regression.html#sec-nonparametric-regression"><strong>Nonparametric methods</strong></a>, such as <a href="sec-nonparametric-regression.html#sec-kernel-regression"><strong>kernel regression</strong></a>, <a href="sec-nonparametric-regression.html#sec-local-polynomial-regression"><strong>local polynomial regression</strong></a>, <a href="sec-nonparametric-regression.html#sec-smoothing-splines"><strong>smoothing splines</strong></a>, <a href="sec-nonparametric-regression.html#sec-wavelet-regression"><strong>wavelet regression</strong></a>, and <a href="sec-nonparametric-regression.html#sec-regression-trees"><strong>regression trees</strong></a>, provide powerful tools when parametric assumptions are too restrictive. These models excel at capturing complex, nonlinear patterns without assuming a specific functional form, offering greater adaptability in diverse applications.</p></li>
</ul>
<hr>
</div>
<div id="the-art-and-science-of-regression" class="section level3" number="10.11.2">
<h3>
<span class="header-section-number">10.11.2</span> The Art and Science of Regression<a class="anchor" aria-label="anchor" href="#the-art-and-science-of-regression"><i class="fas fa-link"></i></a>
</h3>
<p>While statistical formulas and algorithms form the backbone of regression analysis, the <strong>true art</strong> lies in <strong>model selection</strong>, <strong>diagnostic evaluation</strong>, and <strong>interpretation</strong>. No model is inherently perfect; each is an approximation of reality, shaped by the assumptions we make and the data we collect. The most effective analysts are those who approach models critically—testing assumptions, validating results, and recognizing the limitations of their analyses.</p>
<p>Nonparametric methods remind us that <strong>flexibility</strong> often comes at the cost of <strong>interpretability</strong> and <strong>efficiency</strong>, just as parametric models offer simplicity but may risk oversimplification. The key is not to choose between these paradigms, but to understand when each is most appropriate.</p>
<hr>
</div>
<div id="looking-forward" class="section level3" number="10.11.3">
<h3>
<span class="header-section-number">10.11.3</span> Looking Forward<a class="anchor" aria-label="anchor" href="#looking-forward"><i class="fas fa-link"></i></a>
</h3>
<p>The field of regression continues to evolve, driven by rapid advancements in <strong>computational power</strong>, <strong>data availability</strong>, and <strong>methodological innovation</strong>. This evolution has given rise to a wide range of modern techniques that extend beyond traditional frameworks:</p>
<ul>
<li>
<strong>Machine Learning Algorithms:</strong> While methods like random forests, support vector machines, and gradient boosting are well-established, recent developments include:
<ul>
<li>
<strong>Extreme Gradient Boosting (XGBoost)</strong> and <strong>LightGBM</strong>, optimized for speed and performance in large-scale data environments.</li>
<li>
<strong>CatBoost</strong>, which handles categorical features more effectively without extensive preprocessing.</li>
</ul>
</li>
<li>
<strong>Bayesian Regression Techniques:</strong> Modern Bayesian approaches go beyond simple hierarchical models to include:
<ul>
<li>
<strong>Bayesian Additive Regression Trees (BART):</strong> A flexible, nonparametric Bayesian method that combines the power of regression trees with probabilistic inference.</li>
<li>
<strong>Bayesian Neural Networks (BNNs):</strong> Extending deep learning with uncertainty quantification, enabling robust decision-making in high-stakes applications.</li>
</ul>
</li>
<li>
<strong>High-Dimensional Data Analysis:</strong> Regularization methods like LASSO and ridge regression have paved the way for more advanced techniques, such as:
<ul>
<li>
<strong>Graphical Models and Sparse Precision Matrices:</strong> For capturing complex dependency structures in high-dimensional data.</li>
</ul>
</li>
<li>
<strong>Deep Learning for Regression:</strong> Deep neural networks (DNNs) are increasingly used for regression tasks, particularly when dealing with:
<ul>
<li>
<strong>Structured Data (e.g., tabular datasets):</strong> Through architectures like <strong>TabNet</strong>.</li>
<li>
<strong>Unstructured Data (e.g., images, text):</strong> Using <strong>convolutional neural networks (CNNs)</strong> and <strong>transformer-based models</strong>.</li>
</ul>
</li>
<li>
<strong>Causal Inference in Regression:</strong> The integration of causal modeling techniques into regression frameworks has advanced significantly:
<ul>
<li>
<strong>Double Machine Learning (DML):</strong> Combining machine learning with econometric methods for robust causal effect estimation.</li>
<li>
<strong>Causal Forests:</strong> An extension of random forests designed to estimate heterogeneous treatment effects.</li>
</ul>
</li>
<li>
<strong>Functional Data Analysis (FDA):</strong> For analyzing data where predictors or responses are functions (e.g., curves, time series), using methods like:
<ul>
<li>
<strong>Functional Linear Models (FLM)</strong> and <strong>Functional Additive Models (FAM)</strong>.</li>
<li>
<strong>Dynamic Regression Models</strong> for real-time prediction in streaming data environments.</li>
</ul>
</li>
</ul>
<p>While these modern approaches differ in implementation, many are rooted in the fundamental concepts covered in this book. Whether through parametric precision or nonparametric flexibility, the principles of regression remain central to data-driven inquiry.</p>
<hr>
</div>
<div id="final-thoughts" class="section level3" number="10.11.4">
<h3>
<span class="header-section-number">10.11.4</span> Final Thoughts<a class="anchor" aria-label="anchor" href="#final-thoughts"><i class="fas fa-link"></i></a>
</h3>
<p>As you apply these techniques in your own work, remember that regression is not just about fitting models—it’s about:</p>
<ul>
<li><p><strong>Asking the right questions</strong></p></li>
<li><p><strong>Interpreting results thoughtfully</strong></p></li>
<li><p><strong>Using data to generate meaningful insights</strong></p></li>
</ul>
<p>Whether you’re developing marketing strategies, forecasting financial trends, optimizing healthcare interventions, or conducting academic research, the tools you’ve gained here will serve as a strong foundation.</p>
<blockquote>
<p>In the words of <strong>George E.P. Box</strong>,<br><em>“All models are wrong, but some are useful.”</em></p>
<p>Our goal as analysts is to find models that are not only useful but also <strong>enlightening</strong>—models that reveal patterns, guide decisions, and deepen our understanding of the world.</p>
</blockquote>

</div>
</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></div>
<div class="next"><a href="data.html"><span class="header-section-number">11</span> Data</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-nonparametric-regression"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li>
<a class="nav-link" href="#why-nonparametric"><span class="header-section-number">10.1</span> Why Nonparametric?</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#flexibility"><span class="header-section-number">10.1.1</span> Flexibility</a></li>
<li><a class="nav-link" href="#fewer-assumptions"><span class="header-section-number">10.1.2</span> Fewer Assumptions</a></li>
<li><a class="nav-link" href="#interpretability"><span class="header-section-number">10.1.3</span> Interpretability</a></li>
<li><a class="nav-link" href="#practical-considerations-3"><span class="header-section-number">10.1.4</span> Practical Considerations</a></li>
<li><a class="nav-link" href="#balancing-parametric-and-nonparametric-approaches"><span class="header-section-number">10.1.5</span> Balancing Parametric and Nonparametric Approaches</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#basic-concepts-in-nonparametric-estimation"><span class="header-section-number">10.2</span> Basic Concepts in Nonparametric Estimation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#bias-variance-trade-off"><span class="header-section-number">10.2.1</span> Bias-Variance Trade-Off</a></li>
<li><a class="nav-link" href="#kernel-smoothing-and-local-averages"><span class="header-section-number">10.2.2</span> Kernel Smoothing and Local Averages</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-kernel-regression"><span class="header-section-number">10.3</span> Kernel Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#basic-setup"><span class="header-section-number">10.3.1</span> Basic Setup</a></li>
<li><a class="nav-link" href="#sec-nadaraya-watson-kernel-estimator"><span class="header-section-number">10.3.2</span> Nadaraya-Watson Kernel Estimator</a></li>
<li><a class="nav-link" href="#sec-priestley-chao-kernel-estimator"><span class="header-section-number">10.3.3</span> Priestley–Chao Kernel Estimator</a></li>
<li><a class="nav-link" href="#sec-gasser-mueller-kernel-estimator"><span class="header-section-number">10.3.4</span> Gasser–Müller Kernel Estimator</a></li>
<li><a class="nav-link" href="#comparison-of-kernel-based-estimators"><span class="header-section-number">10.3.5</span> Comparison of Kernel-Based Estimators</a></li>
<li><a class="nav-link" href="#bandwidth-selection"><span class="header-section-number">10.3.6</span> Bandwidth Selection</a></li>
<li><a class="nav-link" href="#asymptotic-properties"><span class="header-section-number">10.3.7</span> Asymptotic Properties</a></li>
<li><a class="nav-link" href="#derivation-of-the-nadaraya-watson-estimator"><span class="header-section-number">10.3.8</span> Derivation of the Nadaraya-Watson Estimator</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-local-polynomial-regression"><span class="header-section-number">10.4</span> Local Polynomial Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#local-polynomial-fitting"><span class="header-section-number">10.4.1</span> Local Polynomial Fitting</a></li>
<li><a class="nav-link" href="#mathematical-form-of-the-solution"><span class="header-section-number">10.4.2</span> Mathematical Form of the Solution</a></li>
<li><a class="nav-link" href="#bias-variance-and-asymptotics"><span class="header-section-number">10.4.3</span> Bias, Variance, and Asymptotics</a></li>
<li><a class="nav-link" href="#sec-special-case-local-linear-regression"><span class="header-section-number">10.4.4</span> Special Case: Local Linear Regression</a></li>
<li><a class="nav-link" href="#bandwidth-selection-1"><span class="header-section-number">10.4.5</span> Bandwidth Selection</a></li>
<li><a class="nav-link" href="#asymptotic-properties-summary"><span class="header-section-number">10.4.6</span> Asymptotic Properties Summary</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-smoothing-splines"><span class="header-section-number">10.5</span> Smoothing Splines</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#properties-and-form-of-the-smoothing-spline"><span class="header-section-number">10.5.1</span> Properties and Form of the Smoothing Spline</a></li>
<li><a class="nav-link" href="#choice-of-lambda"><span class="header-section-number">10.5.2</span> Choice of \(\lambda\)</a></li>
<li><a class="nav-link" href="#connection-to-reproducing-kernel-hilbert-spaces"><span class="header-section-number">10.5.3</span> Connection to Reproducing Kernel Hilbert Spaces</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#confidence-intervals-in-nonparametric-regression"><span class="header-section-number">10.6</span> Confidence Intervals in Nonparametric Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#asymptotic-normality"><span class="header-section-number">10.6.1</span> Asymptotic Normality</a></li>
<li><a class="nav-link" href="#bootstrap-methods"><span class="header-section-number">10.6.2</span> Bootstrap Methods</a></li>
<li><a class="nav-link" href="#practical-considerations-4"><span class="header-section-number">10.6.3</span> Practical Considerations</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-generalized-additive-models"><span class="header-section-number">10.7</span> Generalized Additive Models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimation-via-penalized-likelihood"><span class="header-section-number">10.7.1</span> Estimation via Penalized Likelihood</a></li>
<li><a class="nav-link" href="#interpretation-of-gams"><span class="header-section-number">10.7.2</span> Interpretation of GAMs</a></li>
<li><a class="nav-link" href="#model-selection-and-smoothing-parameter-estimation"><span class="header-section-number">10.7.3</span> Model Selection and Smoothing Parameter Estimation</a></li>
<li><a class="nav-link" href="#extensions-of-gams"><span class="header-section-number">10.7.4</span> Extensions of GAMs</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#regression-trees-and-random-forests"><span class="header-section-number">10.8</span> Regression Trees and Random Forests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-regression-trees"><span class="header-section-number">10.8.1</span> Regression Trees</a></li>
<li><a class="nav-link" href="#sec-random-forests"><span class="header-section-number">10.8.2</span> Random Forests</a></li>
<li><a class="nav-link" href="#theoretical-insights"><span class="header-section-number">10.8.3</span> Theoretical Insights</a></li>
<li><a class="nav-link" href="#feature-importance-in-random-forests"><span class="header-section-number">10.8.4</span> Feature Importance in Random Forests</a></li>
<li><a class="nav-link" href="#advantages-and-limitations-of-tree-based-methods"><span class="header-section-number">10.8.5</span> Advantages and Limitations of Tree-Based Methods</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-wavelet-regression"><span class="header-section-number">10.9</span> Wavelet Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#wavelet-series-expansion"><span class="header-section-number">10.9.1</span> Wavelet Series Expansion</a></li>
<li><a class="nav-link" href="#wavelet-regression-model"><span class="header-section-number">10.9.2</span> Wavelet Regression Model</a></li>
<li><a class="nav-link" href="#wavelet-shrinkage-and-thresholding"><span class="header-section-number">10.9.3</span> Wavelet Shrinkage and Thresholding</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#multivariate-nonparametric-regression"><span class="header-section-number">10.10</span> Multivariate Nonparametric Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-curse-of-dimensionality"><span class="header-section-number">10.10.1</span> The Curse of Dimensionality</a></li>
<li><a class="nav-link" href="#multivariate-kernel-regression"><span class="header-section-number">10.10.2</span> Multivariate Kernel Regression</a></li>
<li><a class="nav-link" href="#multivariate-splines"><span class="header-section-number">10.10.3</span> Multivariate Splines</a></li>
<li><a class="nav-link" href="#additive-models-gams"><span class="header-section-number">10.10.4</span> Additive Models (GAMs)</a></li>
<li><a class="nav-link" href="#radial-basis-functions"><span class="header-section-number">10.10.5</span> Radial Basis Functions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#conclusion-the-evolving-landscape-of-regression-analysis"><span class="header-section-number">10.11</span> Conclusion: The Evolving Landscape of Regression Analysis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#key-takeaways-1"><span class="header-section-number">10.11.1</span> Key Takeaways</a></li>
<li><a class="nav-link" href="#the-art-and-science-of-regression"><span class="header-section-number">10.11.2</span> The Art and Science of Regression</a></li>
<li><a class="nav-link" href="#looking-forward"><span class="header-section-number">10.11.3</span> Looking Forward</a></li>
<li><a class="nav-link" href="#final-thoughts"><span class="header-section-number">10.11.4</span> Final Thoughts</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/10-nonparam-regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/10-nonparam-regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-03-28.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
