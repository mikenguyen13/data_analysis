<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Prerequisites | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="This chapter serves as a concise review of fundamental concepts in Matrix Theory and Probability Theory. If you are confident in your understanding of these topics, you can proceed directly to the...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 2 Prerequisites | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/prerequisites.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="This chapter serves as a concise review of fundamental concepts in Matrix Theory and Probability Theory. If you are confident in your understanding of these topics, you can proceed directly to the...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2 Prerequisites | A Guide on Data Analysis">
<meta name="twitter:description" content="This chapter serves as a concise review of fundamental concepts in Matrix Theory and Probability Theory. If you are confident in your understanding of these topics, you can proceed directly to the...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="active" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="synthetic-difference-in-differences.html"><span class="header-section-number">28</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">29</span> Difference-in-differences</a></li>
<li><a class="" href="changes-in-changes.html"><span class="header-section-number">30</span> Changes-in-Changes</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">31</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">32</span> Event Studies</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">33</span> Instrumental Variables</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">34</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">35</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">39</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="prerequisites" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Prerequisites<a class="anchor" aria-label="anchor" href="#prerequisites"><i class="fas fa-link"></i></a>
</h1>
<p>This chapter serves as a concise review of fundamental concepts in <a href="prerequisites.html#matrix-theory">Matrix Theory</a> and <a href="prerequisites.html#probability-theory">Probability Theory</a>.</p>
<p>If you are confident in your understanding of these topics, you can proceed directly to the <a href="descriptive-statistics.html#descriptive-statistics">Descriptive Statistics</a> section to begin exploring applied data analysis.</p>
<div class="inline-figure"><img src="images/Famous-internet-meme-displaying-the-relation-between-statistics-Machine-Learning-and.png" style="width:90.0%"></div>
<div id="matrix-theory" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Matrix Theory<a class="anchor" aria-label="anchor" href="#matrix-theory"><i class="fas fa-link"></i></a>
</h2>
<p>Matrix <span class="math inline">\(A\)</span> represents the original matrix. It’s a 2x2 matrix with elements <span class="math inline">\(a_{ij}\)</span>, where <span class="math inline">\(i\)</span> represents the row and <span class="math inline">\(j\)</span> represents the column.</p>
<p><span class="math display">\[
A =
\begin{bmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22}
\end{bmatrix}
\]</span> <span class="math inline">\(A'\)</span> is the transpose of <span class="math inline">\(A\)</span>. The transpose of a matrix flips its rows and columns.</p>
<p><span class="math display">\[
A' =
\begin{bmatrix}
a_{11} &amp; a_{21} \\
a_{12} &amp; a_{22}
\end{bmatrix}
\]</span></p>
<p>Fundamental properties and rules of matrices, essential for understanding operations in linear algebra:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{(ABC)'}   &amp; = \mathbf{C'B'A'} \quad &amp;\text{(Transpose reverses order in a product)} \\
\mathbf{A(B+C)}   &amp; = \mathbf{AB + AC} \quad &amp;\text{(Distributive property)} \\
\mathbf{AB}       &amp; \neq \mathbf{BA} \quad &amp;\text{(Multiplication is not commutative)} \\
\mathbf{(A')'}    &amp; = \mathbf{A} \quad &amp;\text{(Double transpose is the original matrix)} \\
\mathbf{(A+B)'}   &amp; = \mathbf{A' + B'} \quad &amp;\text{(Transpose of a sum is the sum of transposes)} \\
\mathbf{(AB)'}    &amp; = \mathbf{B'A'} \quad &amp;\text{(Transpose reverses order in a product)} \\
\mathbf{(AB)^{-1}} &amp; = \mathbf{B^{-1}A^{-1}} \quad &amp;\text{(Inverse reverses order in a product)} \\
\mathbf{A+B}      &amp; = \mathbf{B + A} \quad &amp;\text{(Addition is commutative)} \\
\mathbf{AA^{-1}}  &amp; = \mathbf{I} \quad &amp;\text{(Matrix times its inverse is identity)}
\end{aligned}
\]</span> These properties are critical in solving systems of equations, optimizing models, and performing data transformations.</p>
<p>If a matrix <span class="math inline">\(\mathbf{A}\)</span> has an inverse, it is called <strong>invertible</strong>. If <span class="math inline">\(\mathbf{A}\)</span> does not have an inverse, it is referred to as <strong>singular</strong>.</p>
<p>The product of two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is computed as:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{A} &amp;=
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23}
\end{bmatrix}
\begin{bmatrix}
b_{11} &amp; b_{12} &amp; b_{13} \\
b_{21} &amp; b_{22} &amp; b_{23} \\
b_{31} &amp; b_{32} &amp; b_{33}
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} &amp; \sum_{i=1}^{3}a_{1i}b_{i2} &amp; \sum_{i=1}^{3}a_{1i}b_{i3} \\
\sum_{i=1}^{3}a_{2i}b_{i1} &amp; \sum_{i=1}^{3}a_{2i}b_{i2} &amp; \sum_{i=1}^{3}a_{2i}b_{i3}
\end{bmatrix}
\end{aligned}
\]</span></p>
<p><strong>Quadratic Form</strong></p>
<p>Let <span class="math inline">\(\mathbf{a}\)</span> be a <span class="math inline">\(3 \times 1\)</span> vector. The quadratic form involving a matrix <span class="math inline">\(\mathbf{B}\)</span> is given by:</p>
<p><span class="math display">\[
\mathbf{a'Ba} = \sum_{i=1}^{3}\sum_{j=1}^{3}a_i b_{ij} a_{j}
\]</span></p>
<p><strong>Length of a Vector</strong></p>
<p>The <strong>length</strong> (or 2-norm) of a vector <span class="math inline">\(\mathbf{a}\)</span>, denoted as <span class="math inline">\(||\mathbf{a}||\)</span>, is defined as the square root of the inner product of the vector with itself:</p>
<p><span class="math display">\[
||\mathbf{a}|| = \sqrt{\mathbf{a'a}}
\]</span></p>
<div id="rank-of-a-matrix" class="section level3" number="2.1.1">
<h3>
<span class="header-section-number">2.1.1</span> Rank of a Matrix<a class="anchor" aria-label="anchor" href="#rank-of-a-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>rank</strong> of a matrix refers to:</p>
<ul>
<li>The dimension of the space spanned by its columns (or rows).</li>
<li>The number of linearly independent columns or rows.</li>
</ul>
<p>For an <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> and a <span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(\mathbf{B}\)</span>, the following properties hold:</p>
<ul>
<li><span class="math inline">\(\text{rank}(\mathbf{A}) \leq \min(n, k)\)</span></li>
<li><span class="math inline">\(\text{rank}(\mathbf{A}) = \text{rank}(\mathbf{A'}) = \text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf{AA'})\)</span></li>
<li><span class="math inline">\(\text{rank}(\mathbf{AB}) = \min(\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}))\)</span></li>
<li>
<span class="math inline">\(\mathbf{B}\)</span> is invertible (non-singular) if and only if <span class="math inline">\(\text{rank}(\mathbf{B}) = k\)</span>.</li>
</ul>
</div>
<div id="inverse-of-a-matrix" class="section level3" number="2.1.2">
<h3>
<span class="header-section-number">2.1.2</span> Inverse of a Matrix<a class="anchor" aria-label="anchor" href="#inverse-of-a-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>In scalar algebra, if <span class="math inline">\(a = 0\)</span>, then <span class="math inline">\(1/a\)</span> does not exist.</p>
<p>In matrix algebra, a matrix is invertible if it is <strong>non-singular</strong>, meaning it has a non-zero determinant and its inverse exists. A square matrix <span class="math inline">\(\mathbf{A}\)</span> is invertible if there exists another square matrix <span class="math inline">\(\mathbf{B}\)</span> such that:</p>
<p><span class="math display">\[
\mathbf{AB} = \mathbf{I} \quad \text{(Identity Matrix)}.
\]</span></p>
<p>In this case, <span class="math inline">\(\mathbf{A}^{-1} = \mathbf{B}\)</span>.</p>
<p>For a <span class="math inline">\(2 \times 2\)</span> matrix:</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{bmatrix}
a &amp; b \\
c &amp; d
\end{bmatrix}
\]</span></p>
<p>The inverse is:</p>
<p><span class="math display">\[
\mathbf{A}^{-1} =
\frac{1}{ad-bc}
\begin{bmatrix}
d &amp; -b \\
-c &amp; a
\end{bmatrix}
\]</span></p>
<p>This inverse exists only if <span class="math inline">\(ad - bc \neq 0\)</span>, where <span class="math inline">\(ad - bc\)</span> is the determinant of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>For a partitioned block matrix:</p>
<p><span class="math display">\[
\begin{bmatrix}
A &amp; B \\
C &amp; D
\end{bmatrix}^{-1}
=
\begin{bmatrix}
\mathbf{(A-BD^{-1}C)^{-1}} &amp; \mathbf{-(A-BD^{-1}C)^{-1}BD^{-1}} \\
\mathbf{-D^{-1}C(A-BD^{-1}C)^{-1}} &amp; \mathbf{D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}}
\end{bmatrix}
\]</span></p>
<p>This formula assumes that <span class="math inline">\(\mathbf{D}\)</span> and <span class="math inline">\(\mathbf{A - BD^{-1}C}\)</span> are invertible.</p>
<p>Properties of the Inverse for Non-Singular Matrices</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\mathbf{(A^{-1})^{-1}} = \mathbf{A}\)</span><br>
</li>
<li>For a non-zero scalar <span class="math inline">\(b\)</span>, <span class="math inline">\(\mathbf{(bA)^{-1} = b^{-1}A^{-1}}\)</span><br>
</li>
<li>For a matrix <span class="math inline">\(\mathbf{B}\)</span>, <span class="math inline">\(\mathbf{(BA)^{-1} = B^{-1}A^{-1}}\)</span> (only if <span class="math inline">\(\mathbf{B}\)</span> is non-singular).<br>
</li>
<li>
<span class="math inline">\(\mathbf{(A^{-1})' = (A')^{-1}}\)</span> (the transpose of the inverse equals the inverse of the transpose).<br>
</li>
<li>Never notate <span class="math inline">\(\mathbf{1/A}\)</span>; use <span class="math inline">\(\mathbf{A^{-1}}\)</span> instead.</li>
</ol>
<p><strong>Notes</strong>: - The determinant of a matrix determines whether it is invertible. For square matrices, a determinant of <span class="math inline">\(0\)</span> means the matrix is <strong>singular</strong> and has no inverse.<br>
- Always verify the conditions for invertibility, particularly when dealing with partitioned or block matrices.</p>
</div>
<div id="definiteness-of-a-matrix" class="section level3" number="2.1.3">
<h3>
<span class="header-section-number">2.1.3</span> Definiteness of a Matrix<a class="anchor" aria-label="anchor" href="#definiteness-of-a-matrix"><i class="fas fa-link"></i></a>
</h3>
<p>A symmetric square <span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> is classified based on the following conditions:</p>
<ul>
<li><p><strong>Positive Semi-Definite (PSD)</strong>: <span class="math inline">\(\mathbf{A}\)</span> is PSD if, for any non-zero <span class="math inline">\(k \times 1\)</span> vector <span class="math inline">\(\mathbf{x}\)</span>: <span class="math display">\[
\mathbf{x'Ax \geq 0}.
\]</span></p></li>
<li><p><strong>Negative Semi-Definite (NSD)</strong>: <span class="math inline">\(\mathbf{A}\)</span> is NSD if, for any non-zero <span class="math inline">\(k \times 1\)</span> vector <span class="math inline">\(\mathbf{x}\)</span>: <span class="math display">\[
\mathbf{x'Ax \leq 0}.
\]</span></p></li>
<li><p><strong>Indefinite</strong>: <span class="math inline">\(\mathbf{A}\)</span> is indefinite if it is neither PSD nor NSD.</p></li>
</ul>
<p>The <strong>identity matrix</strong> is always positive definite (PD).</p>
<p>Example</p>
<p>Let <span class="math inline">\(\mathbf{x} = (x_1, x_2)'\)</span>, and consider a <span class="math inline">\(2 \times 2\)</span> identity matrix <span class="math inline">\(\mathbf{I}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{x'Ix}
&amp;= (x_1, x_2)
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \\
&amp;=
(x_1, x_2)
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \\
&amp;=
x_1^2 + x_2^2 \geq 0.
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(\mathbf{I}\)</span> is PD because <span class="math inline">\(\mathbf{x'Ix} &gt; 0\)</span> for all non-zero <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p><strong>Properties of Definiteness</strong></p>
<ol style="list-style-type: decimal">
<li>Any variance-covariance matrix is PSD.</li>
<li>A matrix <span class="math inline">\(\mathbf{A}\)</span> is PSD if and only if there exists a matrix <span class="math inline">\(\mathbf{B}\)</span> such that: <span class="math display">\[
\mathbf{A = B'B}.
\]</span>
</li>
<li>If <span class="math inline">\(\mathbf{A}\)</span> is PSD, then <span class="math inline">\(\mathbf{B'AB}\)</span> is also PSD for any conformable matrix <span class="math inline">\(\mathbf{B}\)</span>.</li>
<li>If <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{C}\)</span> are non-singular, then <span class="math inline">\(\mathbf{A - C}\)</span> is PSD if and only if <span class="math inline">\(\mathbf{C^{-1} - A^{-1}}\)</span> is PSD.</li>
<li>If <span class="math inline">\(\mathbf{A}\)</span> is PD (or ND), then <span class="math inline">\(\mathbf{A^{-1}}\)</span> is also PD (or ND).</li>
</ol>
<p><strong>Notes</strong></p>
<ul>
<li>An <strong>indefinite</strong> matrix <span class="math inline">\(\mathbf{A}\)</span> is neither PSD nor NSD. This concept does not have a direct counterpart in scalar algebra.</li>
<li>If a square matrix is PSD and invertible, then it is PD.</li>
</ul>
<p>Examples of Definiteness</p>
<ol style="list-style-type: decimal">
<li><p><strong>Invertible / Indefinite</strong>: <span class="math display">\[
\begin{bmatrix}
-1 &amp; 0 \\
0 &amp; 10
\end{bmatrix}
\]</span></p></li>
<li><p><strong>Non-Invertible / Indefinite</strong>: <span class="math display">\[
\begin{bmatrix}
0 &amp; 1 \\
0 &amp; 0
\end{bmatrix}
\]</span></p></li>
<li><p><strong>Invertible / PSD</strong>: <span class="math display">\[
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\]</span></p></li>
<li><p><strong>Non-Invertible / PSD</strong>: <span class="math display">\[
\begin{bmatrix}
0 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\]</span></p></li>
</ol>
</div>
<div id="matrix-calculus" class="section level3" number="2.1.4">
<h3>
<span class="header-section-number">2.1.4</span> Matrix Calculus<a class="anchor" aria-label="anchor" href="#matrix-calculus"><i class="fas fa-link"></i></a>
</h3>
<p>Consider a scalar function <span class="math inline">\(y = f(x_1, x_2, \dots, x_k) = f(x)\)</span>, where <span class="math inline">\(x\)</span> is a <span class="math inline">\(1 \times k\)</span> row vector.</p>
<div id="gradient-first-order-derivative" class="section level4" number="2.1.4.1">
<h4>
<span class="header-section-number">2.1.4.1</span> Gradient (First-Order Derivative)<a class="anchor" aria-label="anchor" href="#gradient-first-order-derivative"><i class="fas fa-link"></i></a>
</h4>
<p>The gradient, or the first-order derivative of <span class="math inline">\(f(x)\)</span> with respect to the vector <span class="math inline">\(x\)</span>, is given by:</p>
<p><span class="math display">\[
\frac{\partial f(x)}{\partial x} =
\begin{bmatrix}
\frac{\partial f(x)}{\partial x_1} \\
\frac{\partial f(x)}{\partial x_2} \\
\vdots \\
\frac{\partial f(x)}{\partial x_k}
\end{bmatrix}
\]</span></p>
</div>
<div id="hessian-second-order-derivative" class="section level4" number="2.1.4.2">
<h4>
<span class="header-section-number">2.1.4.2</span> Hessian (Second-Order Derivative)<a class="anchor" aria-label="anchor" href="#hessian-second-order-derivative"><i class="fas fa-link"></i></a>
</h4>
<p>The Hessian, or the second-order derivative of <span class="math inline">\(f(x)\)</span> with respect to <span class="math inline">\(x\)</span>, is a symmetric matrix defined as:</p>
<p><span class="math display">\[
\frac{\partial^2 f(x)}{\partial x \partial x'} =
\begin{bmatrix}
\frac{\partial^2 f(x)}{\partial x_1^2} &amp; \frac{\partial^2 f(x)}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_1 \partial x_k} \\
\frac{\partial^2 f(x)}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f(x)}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_2 \partial x_k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f(x)}{\partial x_k \partial x_1} &amp; \frac{\partial^2 f(x)}{\partial x_k \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_k^2}
\end{bmatrix}
\]</span></p>
</div>
<div id="derivative-of-a-scalar-function-with-respect-to-a-matrix" class="section level4" number="2.1.4.3">
<h4>
<span class="header-section-number">2.1.4.3</span> Derivative of a Scalar Function with Respect to a Matrix<a class="anchor" aria-label="anchor" href="#derivative-of-a-scalar-function-with-respect-to-a-matrix"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\(f(\mathbf{X})\)</span> be a scalar function, where <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times p\)</span> matrix. The derivative is:</p>
<p><span class="math display">\[
\frac{\partial f(\mathbf{X})}{\partial \mathbf{X}} =
\begin{bmatrix}
\frac{\partial f(\mathbf{X})}{\partial x_{11}} &amp; \cdots &amp; \frac{\partial f(\mathbf{X})}{\partial x_{1p}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f(\mathbf{X})}{\partial x_{n1}} &amp; \cdots &amp; \frac{\partial f(\mathbf{X})}{\partial x_{np}}
\end{bmatrix}
\]</span></p>
</div>
<div id="common-matrix-derivatives" class="section level4" number="2.1.4.4">
<h4>
<span class="header-section-number">2.1.4.4</span> Common Matrix Derivatives<a class="anchor" aria-label="anchor" href="#common-matrix-derivatives"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\mathbf{a}\)</span> is a vector and <span class="math inline">\(\mathbf{A}\)</span> is a matrix independent of <span class="math inline">\(\mathbf{y}\)</span>:
<ul>
<li><span class="math inline">\(\frac{\partial \mathbf{a'y}}{\partial \mathbf{y}} = \mathbf{a}\)</span></li>
<li><span class="math inline">\(\frac{\partial \mathbf{y'y}}{\partial \mathbf{y}} = 2\mathbf{y}\)</span></li>
<li><span class="math inline">\(\frac{\partial \mathbf{y'Ay}}{\partial \mathbf{y}} = (\mathbf{A} + \mathbf{A'})\mathbf{y}\)</span></li>
</ul>
</li>
<li>If <span class="math inline">\(\mathbf{X}\)</span> is symmetric:
<ul>
<li>
<span class="math inline">\(\frac{\partial |\mathbf{X}|}{\partial x_{ij}} = \begin{cases} X_{ii}, &amp; i = j \\ X_{ij}, &amp; i \neq j \end{cases}\)</span> where <span class="math inline">\(X_{ij}\)</span> is the <span class="math inline">\((i,j)\)</span>-th cofactor of <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul>
</li>
<li>If <span class="math inline">\(\mathbf{X}\)</span> is symmetric and <span class="math inline">\(\mathbf{A}\)</span> is a matrix independent of <span class="math inline">\(\mathbf{X}\)</span>:
<ul>
<li>
<span class="math inline">\(\frac{\partial \text{tr}(\mathbf{XA})}{\partial \mathbf{X}} = \mathbf{A} + \mathbf{A'} - \text{diag}(\mathbf{A})\)</span>.</li>
</ul>
</li>
<li>If <span class="math inline">\(\mathbf{X}\)</span> is symmetric, let <span class="math inline">\(\mathbf{J}_{ij}\)</span> be a matrix with 1 at the <span class="math inline">\((i,j)\)</span>-th position and 0 elsewhere:
<ul>
<li><span class="math inline">\(\frac{\partial \mathbf{X}^{-1}}{\partial x_{ij}} = \begin{cases} -\mathbf{X}^{-1}\mathbf{J}_{ii}\mathbf{X}^{-1}, &amp; i = j \\ -\mathbf{X}^{-1}(\mathbf{J}_{ij} + \mathbf{J}_{ji})\mathbf{X}^{-1}, &amp; i \neq j \end{cases}.\)</span></li>
</ul>
</li>
</ol>
</div>
</div>
<div id="optimization-in-scalar-and-vector-spaces" class="section level3" number="2.1.5">
<h3>
<span class="header-section-number">2.1.5</span> Optimization in Scalar and Vector Spaces<a class="anchor" aria-label="anchor" href="#optimization-in-scalar-and-vector-spaces"><i class="fas fa-link"></i></a>
</h3>
<p>Optimization is the process of finding the minimum or maximum of a function. The conditions for optimization differ depending on whether the function involves a scalar or a vector. Below is a comparison of scalar and vector optimization:</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="28%">
<col width="24%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th>Condition</th>
<th><strong>Scalar Optimization</strong></th>
<th><strong>Vector Optimization</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>First-Order Condition</strong></td>
<td><span class="math display">\[\frac{\partial f(x_0)}{\partial x} = 0\]</span></td>
<td><span class="math display">\[\frac{\partial f(x_0)}{\partial x} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}\]</span></td>
</tr>
<tr class="even">
<td>
<p><strong>Second-Order Condition</strong></p>
<p>For <strong>convex</strong> functions, this implies a <strong>minimum</strong>.</p>
</td>
<td><span class="math display">\[\frac{\partial^2 f(x_0)}{\partial x^2} &gt; 0\]</span></td>
<td><span class="math display">\[\frac{\partial^2 f(x_0)}{\partial x \partial x'} &gt; 0\]</span></td>
</tr>
<tr class="odd">
<td>For <strong>concave</strong> functions, this implies a <strong>maximum</strong>.</td>
<td><span class="math display">\[\frac{\partial^2 f(x_0)}{\partial x^2} &lt; 0\]</span></td>
<td><span class="math display">\[\frac{\partial^2 f(x_0)}{\partial x \partial x'} &lt; 0\]</span></td>
</tr>
</tbody>
</table></div>
<p><strong>Key Concepts</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>First-Order Condition</strong>:
<ul>
<li>The <strong>first-order derivative</strong> of the function must equal zero at a critical point. This holds for both scalar and vector functions:
<ul>
<li>In the scalar case, <span class="math inline">\(\frac{\partial f(x)}{\partial x} = 0\)</span> identifies critical points.</li>
<li>In the vector case, <span class="math inline">\(\frac{\partial f(x)}{\partial x}\)</span> is a <strong>gradient vector</strong>, and the condition is satisfied when all elements of the gradient are zero.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Second-Order Condition</strong>:
<ul>
<li>The <strong>second-order derivative</strong> determines whether the critical point is a minimum, maximum, or saddle point:
<ul>
<li>For scalar functions, <span class="math inline">\(\frac{\partial^2 f(x)}{\partial x^2} &gt; 0\)</span> implies a <strong>local minimum</strong>, while <span class="math inline">\(\frac{\partial^2 f(x)}{\partial x^2} &lt; 0\)</span> implies a <strong>local maximum</strong>.</li>
<li>For vector functions, the Hessian matrix <span class="math inline">\(\frac{\partial^2 f(x)}{\partial x \partial x'}\)</span> must be:
<ul>
<li>
<strong>Positive Definite</strong>: For a <strong>minimum</strong> (convex function).</li>
<li>
<strong>Negative Definite</strong>: For a <strong>maximum</strong> (concave function).</li>
<li>
<strong>Indefinite</strong>: For a <strong>saddle point</strong> (neither minimum nor maximum).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Convex and Concave Functions</strong>:
<ul>
<li>A function <span class="math inline">\(f(x)\)</span> is:
<ul>
<li>
<strong>Convex</strong> if <span class="math inline">\(\frac{\partial^2 f(x)}{\partial x^2} &gt; 0\)</span> or the Hessian <span class="math inline">\(\frac{\partial^2 f(x)}{\partial x \partial x'}\)</span> is positive definite.</li>
<li>
<strong>Concave</strong> if <span class="math inline">\(\frac{\partial^2 f(x)}{\partial x^2} &lt; 0\)</span> or the Hessian is negative definite.</li>
</ul>
</li>
<li>Convexity ensures global optimization for minimization problems, while concavity ensures global optimization for maximization problems.</li>
</ul>
</li>
<li>
<strong>Hessian Matrix</strong>:
<ul>
<li>In vector optimization, the Hessian <span class="math inline">\(\frac{\partial^2 f(x)}{\partial x \partial x'}\)</span> plays a crucial role in determining the nature of critical points:
<ul>
<li>Positive definite Hessian: All eigenvalues are positive.</li>
<li>Negative definite Hessian: All eigenvalues are negative.</li>
<li>Indefinite Hessian: Eigenvalues have mixed signs.</li>
</ul>
</li>
</ul>
</li>
</ol>
</div>
<div id="cholesky-decomposition" class="section level3" number="2.1.6">
<h3>
<span class="header-section-number">2.1.6</span> Cholesky Decomposition<a class="anchor" aria-label="anchor" href="#cholesky-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>In statistical analysis and numerical linear algebra, decomposing matrices into more tractable forms is crucial for efficient computation. One such important factorization is the <strong>Cholesky Decomposition</strong>. It applies to <strong>Hermitian</strong> (in the complex case) or <strong>symmetric</strong> (in the real case), <strong>positive-definite</strong> matrices.</p>
<p>Given an <span class="math inline">\(n \times n\)</span> positive-definite matrix <span class="math inline">\(A\)</span>, the Cholesky Decomposition states:</p>
<p><span class="math display">\[
A = L L^{*},
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(L\)</span> is a lower-triangular matrix with strictly positive diagonal entries.</li>
<li>
<span class="math inline">\(L^{*}\)</span> denotes the <em>conjugate transpose</em> of <span class="math inline">\(L\)</span> (simply the transpose <span class="math inline">\(L^{T}\)</span> for real matrices).</li>
</ul>
<p>Cholesky Decomposition is both computationally efficient and numerically stable, making it a go-to technique for many applications—particularly in statistics where we deal extensively with covariance matrices, linear systems, and probability distributions.</p>
<p>Before diving into how we compute a Cholesky Decomposition, we need to clarify what it means for a matrix to be <em>positive-definite</em>. For a real symmetric matrix <span class="math inline">\(A\)</span>:</p>
<ul>
<li>
<span class="math inline">\(A\)</span> is <em>positive-definite</em> if for every nonzero vector <span class="math inline">\(x\)</span>, we have <span class="math display">\[
x^T A \, x &gt; 0.
\]</span>
</li>
<li>Alternatively, you can characterize positive-definiteness by noting that all eigenvalues of <span class="math inline">\(A\)</span> are strictly positive.</li>
</ul>
<p>Many important matrices in statistics—particularly <em>covariance</em> or <em>precision</em> matrices—are both symmetric and positive-definite.</p>
<hr>
<div id="existence" class="section level4" number="2.1.6.1">
<h4>
<span class="header-section-number">2.1.6.1</span> Existence<a class="anchor" aria-label="anchor" href="#existence"><i class="fas fa-link"></i></a>
</h4>
<p>A real <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> that is symmetric and positive-definite always admits a Cholesky Decomposition <span class="math inline">\(A = L L^T\)</span>. This theorem guarantees that for any covariance matrix in statistics—assuming it is valid (i.e., positive-definite)—we can decompose it via Cholesky.</p>
</div>
<div id="uniqueness" class="section level4" number="2.1.6.2">
<h4>
<span class="header-section-number">2.1.6.2</span> Uniqueness<a class="anchor" aria-label="anchor" href="#uniqueness"><i class="fas fa-link"></i></a>
</h4>
<p>If we additionally require that the diagonal entries of <span class="math inline">\(L\)</span> are <em>strictly positive</em>, then <span class="math inline">\(L\)</span> is <em>unique</em>. That is, no other lower-triangular matrix with strictly positive diagonal entries will produce the same factorization. This uniqueness is helpful for ensuring consistent numerical outputs in software implementations.</p>
</div>
<div id="constructing-the-cholesky-factor-l" class="section level4" number="2.1.6.3">
<h4>
<span class="header-section-number">2.1.6.3</span> Constructing the Cholesky Factor <span class="math inline">\(L\)</span><a class="anchor" aria-label="anchor" href="#constructing-the-cholesky-factor-l"><i class="fas fa-link"></i></a>
</h4>
<p>Given a real, symmetric, positive-definite matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, we want to find the lower-triangular matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(A = LL^T\)</span>. One way to do this is by using a simple step-by-step procedure (often part of standard linear algebra libraries):</p>
<ol style="list-style-type: decimal">
<li>
<strong>Initialize</strong> <span class="math inline">\(L\)</span> to be an <span class="math inline">\(n \times n\)</span> zero matrix.</li>
<li>
<strong>Iterate</strong> through the rows <span class="math inline">\(i = 1, 2, \dots, n\)</span>:
<ol style="list-style-type: decimal">
<li>For each row <span class="math inline">\(i\)</span>, compute <span class="math display">\[
L_{ii} = \sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2}.
\]</span>
</li>
<li>For each column <span class="math inline">\(j = i+1, i+2, \dots, n\)</span>: <span class="math display">\[
L_{ji} = \frac{1}{L_{ii}}
          \Bigl(A_{ji} - \sum_{k=1}^{i-1} L_{jk} L_{ik}\Bigr).
\]</span>
</li>
<li>All other entries of <span class="math inline">\(L\)</span> remain zero or are computed in subsequent steps.</li>
</ol>
</li>
<li>
<strong>Result</strong>: <span class="math inline">\(L\)</span> is lower-triangular, and <span class="math inline">\(L^T\)</span> is its transpose.</li>
</ol>
<p>Cholesky Decomposition is roughly half the computational cost of a more general LU Decomposition. Specifically, it requires on the order of <span class="math inline">\(\frac{1}{3} n^3\)</span> floating-point operations (flops), making it significantly more efficient in practice than other decompositions for positive-definite systems.</p>
<hr>
</div>
<div id="illustrative-example" class="section level4" number="2.1.6.4">
<h4>
<span class="header-section-number">2.1.6.4</span> Illustrative Example<a class="anchor" aria-label="anchor" href="#illustrative-example"><i class="fas fa-link"></i></a>
</h4>
<p>Consider a small <span class="math inline">\(3 \times 3\)</span> positive-definite matrix:</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
4 &amp; 2 &amp; 4 \\
2 &amp; 5 &amp; 6 \\
4 &amp; 6 &amp; 20
\end{pmatrix}.
\]</span></p>
<p>We claim <span class="math inline">\(A\)</span> is positive-definite (one could check by calculating principal minors or verifying <span class="math inline">\(x^T A x &gt; 0\)</span> for all <span class="math inline">\(x \neq 0\)</span>). We find <span class="math inline">\(L\)</span> step-by-step:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Compute</strong> <span class="math inline">\(L_{11}\)</span>:<br><span class="math display">\[
L_{11} = \sqrt{A_{11}} = \sqrt{4} = 2.
\]</span>
</li>
<li>
<strong>Compute</strong> <span class="math inline">\(L_{21}\)</span> and <span class="math inline">\(L_{31}\)</span>:
<ul>
<li>
<span class="math inline">\(L_{21} = \frac{A_{21}}{L_{11}} = \frac{2}{2} = 1.\)</span><br>
</li>
<li><span class="math inline">\(L_{31} = \frac{A_{31}}{L_{11}} = \frac{4}{2} = 2.\)</span></li>
</ul>
</li>
<li>
<strong>Compute</strong> <span class="math inline">\(L_{22}\)</span>:<br><span class="math display">\[
L_{22} = \sqrt{A_{22} - L_{21}^2}
        = \sqrt{5 - 1^2}
        = \sqrt{4} = 2.
\]</span>
</li>
<li>
<strong>Compute</strong> <span class="math inline">\(L_{32}\)</span>:<br><span class="math display">\[
L_{32} = \frac{A_{32} - L_{31} L_{21}}{L_{22}}
        = \frac{6 - (2)(1)}{2}
        = \frac{4}{2} = 2.
\]</span>
</li>
<li>
<strong>Compute</strong> <span class="math inline">\(L_{33}\)</span>:<br><span class="math display">\[
L_{33} = \sqrt{A_{33} - (L_{31}^2 + L_{32}^2)}
        = \sqrt{20 - (2^2 + 2^2)}
        = \sqrt{20 - 8}
        = \sqrt{12}
        = 2\sqrt{3}.
\]</span>
</li>
</ol>
<p>Thus,</p>
<p><span class="math display">\[
L =
\begin{pmatrix}
2 &amp; 0 &amp; 0 \\
1 &amp; 2 &amp; 0 \\
2 &amp; 2 &amp; 2\sqrt{3}
\end{pmatrix}.
\]</span></p>
<p>One can verify <span class="math inline">\(L L^T = A\)</span>.</p>
<hr>
</div>
<div id="applications-in-statistics" class="section level4" number="2.1.6.5">
<h4>
<span class="header-section-number">2.1.6.5</span> Applications in Statistics<a class="anchor" aria-label="anchor" href="#applications-in-statistics"><i class="fas fa-link"></i></a>
</h4>
<div id="solving-linear-systems" class="section level5" number="2.1.6.5.1">
<h5>
<span class="header-section-number">2.1.6.5.1</span> Solving Linear Systems<a class="anchor" aria-label="anchor" href="#solving-linear-systems"><i class="fas fa-link"></i></a>
</h5>
<p>A common statistical problem is solving <span class="math inline">\(A x = b\)</span> for <span class="math inline">\(x\)</span>. For instance, in regression or in computing Bayesian posterior modes, we often need to solve linear equations with covariance or precision matrices. With <span class="math inline">\(A = LL^T\)</span>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Forward Substitution</strong>: Solve <span class="math inline">\(L \, y = b\)</span>.</li>
<li>
<strong>Backward Substitution</strong>: Solve <span class="math inline">\(L^T x = y\)</span>.</li>
</ol>
<p>This two-step process is more stable and efficient than directly inverting <span class="math inline">\(A\)</span> (which is typically discouraged due to numerical issues).</p>
</div>
<div id="generating-correlated-random-vectors" class="section level5" number="2.1.6.5.2">
<h5>
<span class="header-section-number">2.1.6.5.2</span> Generating Correlated Random Vectors<a class="anchor" aria-label="anchor" href="#generating-correlated-random-vectors"><i class="fas fa-link"></i></a>
</h5>
<p>In simulation-based statistics (e.g., Monte Carlo methods), we often need to generate random draws from a <strong>multivariate normal</strong> distribution <span class="math inline">\(\mathcal{N}(\mu, \Sigma)\)</span>, where <span class="math inline">\(\Sigma\)</span> is the covariance matrix. The steps are:</p>
<ol style="list-style-type: decimal">
<li>Generate a vector <span class="math inline">\(z \sim \mathcal{N}(0, I)\)</span> of independent standard normal variables.</li>
<li>Compute <span class="math inline">\(x = \mu + Lz\)</span>, where <span class="math inline">\(\Sigma = LL^T\)</span>.</li>
</ol>
<p>Then <span class="math inline">\(x\)</span> has the desired covariance structure <span class="math inline">\(\Sigma\)</span>. This technique is widely used in Bayesian statistics (e.g., MCMC sampling) and financial modeling (e.g., portfolio simulations).</p>
</div>
<div id="gaussian-processes-and-kriging" class="section level5" number="2.1.6.5.3">
<h5>
<span class="header-section-number">2.1.6.5.3</span> Gaussian Processes and Kriging<a class="anchor" aria-label="anchor" href="#gaussian-processes-and-kriging"><i class="fas fa-link"></i></a>
</h5>
<p>In Gaussian Process modeling (common in spatial statistics, machine learning, and geostatistics), we frequently work with large covariance matrices that describe the correlations between observed data points:</p>
<p><span class="math display">\[
\Sigma =
\begin{pmatrix}
k(x_1, x_1) &amp; k(x_1, x_2) &amp; \cdots &amp; k(x_1, x_n) \\
k(x_2, x_1) &amp; k(x_2, x_2) &amp; \cdots &amp; k(x_2, x_n) \\
\vdots      &amp; \vdots      &amp; \ddots &amp; \vdots      \\
k(x_n, x_1) &amp; k(x_n, x_2) &amp; \cdots &amp; k(x_n, x_n)
\end{pmatrix},
\]</span></p>
<p>where <span class="math inline">\(k(\cdot, \cdot)\)</span> is a covariance (kernel) function. We may need to invert or factorize <span class="math inline">\(\Sigma\)</span> repeatedly to evaluate the log-likelihood:</p>
<p><span class="math display">\[
\log \mathcal{L}(\theta) \sim
- \tfrac{1}{2} \bigl( y - m(\theta) \bigr)^T \Sigma^{-1} \bigl( y - m(\theta) \bigr)
- \tfrac{1}{2} \log \bigl| \Sigma \bigr|,
\]</span></p>
<p>where <span class="math inline">\(m(\theta)\)</span> is the mean function and <span class="math inline">\(\theta\)</span> are parameters. Using the Cholesky factor <span class="math inline">\(L\)</span> of <span class="math inline">\(\Sigma\)</span> helps:</p>
<ul>
<li>
<span class="math inline">\(\Sigma^{-1}\)</span> can be implied by solving systems with <span class="math inline">\(L\)</span> instead of explicitly computing the inverse.</li>
<li>
<span class="math inline">\(\log|\Sigma|\)</span> can be computed as <span class="math inline">\(2 \sum_{i=1}^n \log L_{ii}\)</span>.</li>
</ul>
<p>Hence, Cholesky Decomposition becomes the backbone of Gaussian Process computations.</p>
</div>
<div id="bayesian-inference-with-covariance-matrices" class="section level5" number="2.1.6.5.4">
<h5>
<span class="header-section-number">2.1.6.5.4</span> Bayesian Inference with Covariance Matrices<a class="anchor" aria-label="anchor" href="#bayesian-inference-with-covariance-matrices"><i class="fas fa-link"></i></a>
</h5>
<p>Many Bayesian models—especially hierarchical models—assume a multivariate normal prior on parameters. Cholesky Decomposition is used to:</p>
<ul>
<li>Sample from these priors or from posterior distributions.</li>
<li>Regularize large covariance matrices.</li>
<li>Speed up Markov Chain Monte Carlo (MCMC) computations by factorizing covariance structures.</li>
</ul>
<hr>
</div>
</div>
<div id="other-notes" class="section level4" number="2.1.6.6">
<h4>
<span class="header-section-number">2.1.6.6</span> Other Notes<a class="anchor" aria-label="anchor" href="#other-notes"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>Numerical Stability Considerations</li>
</ol>
<p>Cholesky Decomposition is considered more stable than a general LU Decomposition when applied to positive-definite matrices. Since no row or column pivots are required, rounding errors can be smaller. Of course, in practice, software implementations can vary, and extremely ill-conditioned matrices can still pose numerical challenges.</p>
<ol start="2" style="list-style-type: decimal">
<li>Why We <em>Don’t</em> Usually Compute <span class="math inline">\(\mathbf{A}^{-1}\)</span>
</li>
</ol>
<p>It is common in statistics (especially in older texts) to see formulas involving <span class="math inline">\(\Sigma^{-1}\)</span>. However, computing an inverse explicitly is often discouraged because:</p>
<ul>
<li>It is numerically less stable.</li>
<li>It requires more computations.</li>
<li>Many tasks that <em>appear</em> to need <span class="math inline">\(\Sigma^{-1}\)</span> can be done more efficiently by solving systems via the Cholesky factor <span class="math inline">\(L\)</span>.</li>
</ul>
<p>Hence, “<strong>solve, don’t invert</strong>” is a common mantra. If you see an expression like <span class="math inline">\(\Sigma^{-1} b\)</span>, you can use the Cholesky factors <span class="math inline">\(L\)</span> and <span class="math inline">\(L^T\)</span> to solve <span class="math inline">\(\Sigma x = b\)</span> by forward and backward substitution, bypassing the direct inverse calculation.</p>
<ol start="3" style="list-style-type: decimal">
<li>Further Variants and Extensions</li>
</ol>
<ul>
<li>
<strong>Incomplete Cholesky</strong>: Sometimes used in iterative solvers where a full Cholesky factorization is too expensive, especially for large sparse systems.</li>
<li>
<strong>LDL^T Decomposition</strong>: A variant that avoids taking square roots; used for positive <em>semi</em>-definite or indefinite systems, but with caution about pivoting strategies.</li>
</ul>
</div>
</div>
</div>
<div id="probability-theory" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Probability Theory<a class="anchor" aria-label="anchor" href="#probability-theory"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="images/meme-stat.jpg" style="width:80.0%"></div>
<div id="axioms-and-theorems-of-probability" class="section level3" number="2.2.1">
<h3>
<span class="header-section-number">2.2.1</span> Axioms and Theorems of Probability<a class="anchor" aria-label="anchor" href="#axioms-and-theorems-of-probability"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(S\)</span> denote the sample space of an experiment. Then: <span class="math display">\[
P[S] = 1
\]</span> (The probability of the sample space is always 1.)</p></li>
<li><p>For any event <span class="math inline">\(A\)</span>: <span class="math display">\[
P[A] \geq 0
\]</span> (Probabilities are always non-negative.)</p></li>
<li><p>Let <span class="math inline">\(A_1, A_2, A_3, \dots\)</span> be a finite or infinite collection of mutually exclusive events. Then: <span class="math display">\[
P[A_1 \cup A_2 \cup A_3 \dots] = P[A_1] + P[A_2] + P[A_3] + \dots
\]</span> (The probability of the union of mutually exclusive events is the sum of their probabilities.)</p></li>
<li><p>The probability of the empty set is: <span class="math display">\[
P[\emptyset] = 0
\]</span></p></li>
<li><p>The complement rule: <span class="math display">\[
P[A'] = 1 - P[A]
\]</span></p></li>
<li><p>The probability of the union of two events: <span class="math display">\[
P[A_1 \cup A_2] = P[A_1] + P[A_2] - P[A_1 \cap A_2]
\]</span></p></li>
</ol>
<div id="conditional-probability" class="section level4" number="2.2.1.1">
<h4>
<span class="header-section-number">2.2.1.1</span> Conditional Probability<a class="anchor" aria-label="anchor" href="#conditional-probability"><i class="fas fa-link"></i></a>
</h4>
<p>The conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is defined as:</p>
<p><span class="math display">\[
P[A|B] = \frac{P[A \cap B]}{P[B]}, \quad \text{provided } P[B] \neq 0.
\]</span></p>
</div>
<div id="independent-events" class="section level4" number="2.2.1.2">
<h4>
<span class="header-section-number">2.2.1.2</span> Independent Events<a class="anchor" aria-label="anchor" href="#independent-events"><i class="fas fa-link"></i></a>
</h4>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if and only if:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P[A \cap B] = P[A]P[B]\)</span></li>
<li><span class="math inline">\(P[A|B] = P[A]\)</span></li>
<li><span class="math inline">\(P[B|A] = P[B]\)</span></li>
</ol>
<p>A collection of events <span class="math inline">\(A_1, A_2, \dots, A_n\)</span> is independent if and only if every subcollection is independent.</p>
</div>
<div id="multiplication-rule" class="section level4" number="2.2.1.3">
<h4>
<span class="header-section-number">2.2.1.3</span> Multiplication Rule<a class="anchor" aria-label="anchor" href="#multiplication-rule"><i class="fas fa-link"></i></a>
</h4>
<p>The probability of the intersection of two events can be calculated as: <span class="math display">\[
P[A \cap B] = P[A|B]P[B] = P[B|A]P[A].
\]</span></p>
</div>
<div id="bayes-theorem" class="section level4" number="2.2.1.4">
<h4>
<span class="header-section-number">2.2.1.4</span> Bayes’ Theorem<a class="anchor" aria-label="anchor" href="#bayes-theorem"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\(A_1, A_2, \dots, A_n\)</span> be a collection of mutually exclusive events whose union is <span class="math inline">\(S\)</span>, and let <span class="math inline">\(B\)</span> be an event with <span class="math inline">\(P[B] \neq 0\)</span>. Then, for any event <span class="math inline">\(A_j\)</span> (<span class="math inline">\(j = 1, 2, \dots, n\)</span>): <span class="math display">\[
P[A_j|B] = \frac{P[B|A_j]P[A_j]}{\sum_{i=1}^n P[B|A_i]P[A_i]}.
\]</span></p>
</div>
<div id="jensens-inequality" class="section level4" number="2.2.1.5">
<h4>
<span class="header-section-number">2.2.1.5</span> Jensen’s Inequality<a class="anchor" aria-label="anchor" href="#jensens-inequality"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p>If <span class="math inline">\(g(x)\)</span> is convex, then: <span class="math display">\[
E[g(X)] \geq g(E[X])
\]</span></p></li>
<li><p>If <span class="math inline">\(g(x)\)</span> is concave, then: <span class="math display">\[
E[g(X)] \leq g(E[X]).
\]</span></p></li>
</ul>
<p>Jensen’s inequality provides a useful way to demonstrate why the standard error calculated using the sample standard deviation (<span class="math inline">\(s\)</span>) as a proxy for the population standard deviation (<span class="math inline">\(\sigma\)</span>) is a biased estimator.</p>
<ul>
<li><p>The population standard deviation <span class="math inline">\(\sigma\)</span> is defined as: <span class="math display">\[
\sigma = \sqrt{\mathbb{E}[(X - \mu)^2]},
\]</span> where <span class="math inline">\(\mu = \mathbb{E}[X]\)</span> is the population mean.</p></li>
<li><p>The sample standard deviation <span class="math inline">\(s\)</span> is given by: <span class="math display">\[
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2},
\]</span> where <span class="math inline">\(\bar{X}\)</span> is the sample mean.</p></li>
<li><p>When <span class="math inline">\(s\)</span> is used as an estimator for <span class="math inline">\(\sigma\)</span>, the expectation involves the square root function, which is concave.</p></li>
</ul>
<p><strong>Applying Jensen’s Inequality</strong></p>
<p>The standard error formula involves the square root: <span class="math display">\[
\sqrt{\mathbb{E}[s^2]}.
\]</span></p>
<p>However, because the square root function is concave, Jensen’s inequality implies: <span class="math display">\[
\sqrt{\mathbb{E}[s^2]} \leq \mathbb{E}[\sqrt{s^2}] = \mathbb{E}[s].
\]</span></p>
<p>This inequality shows that the expected value of <span class="math inline">\(s\)</span> (the sample standard deviation) systematically underestimates the population standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p><strong>Quantifying the Bias</strong></p>
<p>The bias arises because: <span class="math display">\[
\mathbb{E}[s] \neq \sigma.
\]</span></p>
<p>To correct this bias, we note that the sample standard deviation is related to the population standard deviation by: <span class="math display">\[
\mathbb{E}[s] = \sigma \cdot \sqrt{\frac{n-1}{n}},
\]</span> where <span class="math inline">\(n\)</span> is the sample size. This bias decreases as <span class="math inline">\(n\)</span> increases, and the estimator becomes asymptotically unbiased.</p>
<p>By leveraging Jensen’s inequality, we observe that the concavity of the square root function ensures that <span class="math inline">\(s\)</span> is a biased estimator of <span class="math inline">\(\sigma\)</span>, systematically underestimating the population standard deviation.</p>
</div>
<div id="law-of-iterated-expectation" class="section level4" number="2.2.1.6">
<h4>
<span class="header-section-number">2.2.1.6</span> Law of Iterated Expectation<a class="anchor" aria-label="anchor" href="#law-of-iterated-expectation"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Law of Iterated Expectation</strong> states that for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
E(X) = E(E(X|Y)).
\]</span></p>
<p>This means the expected value of <span class="math inline">\(X\)</span> can be obtained by first calculating the conditional expectation <span class="math inline">\(E(X|Y)\)</span> and then taking the expectation of this quantity over the distribution of <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="correlation-and-independence" class="section level4" number="2.2.1.7">
<h4>
<span class="header-section-number">2.2.1.7</span> Correlation and Independence<a class="anchor" aria-label="anchor" href="#correlation-and-independence"><i class="fas fa-link"></i></a>
</h4>
<p>The strength of the relationship between random variables can be ranked from strongest to weakest as:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Independence</strong>:
<ul>
<li><span class="math inline">\(f(x, y) = f_X(x)f_Y(y)\)</span></li>
<li>
<span class="math inline">\(f_{Y|X}(y|x) = f_Y(y)\)</span> and <span class="math inline">\(f_{X|Y}(x|y) = f_X(x)\)</span>
</li>
<li><span class="math inline">\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\)</span></li>
</ul>
</li>
<li>
<strong>Mean Independence</strong> (implied by independence):
<ul>
<li>
<span class="math inline">\(Y\)</span> is mean independent of <span class="math inline">\(X\)</span> if: <span class="math display">\[
E[Y|X] = E[Y].
\]</span>
</li>
<li><span class="math inline">\(E[Xg(Y)] = E[X]E[g(Y)]\)</span></li>
</ul>
</li>
<li>
<strong>Uncorrelatedness</strong> (implied by independence and mean independence):
<ul>
<li><span class="math inline">\(\text{Cov}(X, Y) = 0\)</span></li>
<li><span class="math inline">\(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)\)</span></li>
<li><span class="math inline">\(E[XY] = E[X]E[Y]\)</span></li>
</ul>
</li>
</ol>
</div>
</div>
<div id="central-limit-theorem" class="section level3" number="2.2.2">
<h3>
<span class="header-section-number">2.2.2</span> Central Limit Theorem<a class="anchor" aria-label="anchor" href="#central-limit-theorem"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Central Limit Theorem</strong> states that for a sufficiently large sample size (<span class="math inline">\(n \geq 25\)</span>), the sampling distribution of the sample mean or proportion approaches a normal distribution, regardless of the population’s original distribution.</p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a distribution <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Then, for large <span class="math inline">\(n\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>The sample mean <span class="math inline">\(\bar{X}\)</span> is approximately normal: <span class="math display">\[
\mu_{\bar{X}} = \mu, \quad \sigma^2_{\bar{X}} = \frac{\sigma^2}{n}.
\]</span></p></li>
<li><p>The sample proportion <span class="math inline">\(\hat{p}\)</span> is approximately normal: <span class="math display">\[
\mu_{\hat{p}} = p, \quad \sigma^2_{\hat{p}} = \frac{p(1-p)}{n}.
\]</span></p></li>
<li><p>The difference in sample proportions <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is approximately normal: <span class="math display">\[
\mu_{\hat{p}_1 - \hat{p}_2} = p_1 - p_2, \quad \sigma^2_{\hat{p}_1 - \hat{p}_2} = \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}.
\]</span></p></li>
<li><p>The difference in sample means <span class="math inline">\(\bar{X}_1 - \bar{X}_2\)</span> is approximately normal: <span class="math display">\[
\mu_{\bar{X}_1 - \bar{X}_2} = \mu_1 - \mu_2, \quad \sigma^2_{\bar{X}_1 - \bar{X}_2} = \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}.
\]</span></p></li>
<li>
<p>The following random variables are approximately standard normal:</p>
<ul>
<li><span class="math inline">\(\frac{\bar{X} - \mu}{\sigma / \sqrt{n}}\)</span></li>
<li><span class="math inline">\(\frac{\hat{p} - p}{\sqrt{\frac{p(1-p)}{n}}}\)</span></li>
<li><span class="math inline">\(\frac{(\hat{p}_1 - \hat{p}_2) - (p_1 - p_2)}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}\)</span></li>
<li><span class="math inline">\(\frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}\)</span></li>
</ul>
</li>
</ol>
<div id="limiting-distribution-of-the-sample-mean" class="section level4" number="2.2.2.1">
<h4>
<span class="header-section-number">2.2.2.1</span> Limiting Distribution of the Sample Mean<a class="anchor" aria-label="anchor" href="#limiting-distribution-of-the-sample-mean"><i class="fas fa-link"></i></a>
</h4>
<p>If <span class="math inline">\(\{X_i\}_{i=1}^{n}\)</span> is an iid random sample from a distribution with finite mean <span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>, the sample mean <span class="math inline">\(\bar{X}\)</span> scaled by <span class="math inline">\(\sqrt{n}\)</span> has the following limiting distribution:</p>
<p><span class="math display">\[
\sqrt{n}(\bar{X} - \mu) \xrightarrow{d} N(0, \sigma^2).
\]</span></p>
<p>Standardizing the sample mean gives: <span class="math display">\[
\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \xrightarrow{d} N(0, 1).
\]</span></p>
<p><strong>Notes</strong>:</p>
<ul>
<li>The CLT holds for most random samples from any distribution (continuous, discrete, or unknown).</li>
<li>It extends to the multivariate case: A random sample of a random vector converges to a multivariate normal distribution.</li>
</ul>
</div>
<div id="asymptotic-variance-and-limiting-variance" class="section level4" number="2.2.2.2">
<h4>
<span class="header-section-number">2.2.2.2</span> Asymptotic Variance and Limiting Variance<a class="anchor" aria-label="anchor" href="#asymptotic-variance-and-limiting-variance"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Asymptotic Variance</strong> (Avar): <span class="math display">\[
Avar(\sqrt{n}(\bar{X} - \mu)) = \sigma^2.
\]</span></p>
<ul>
<li><p>Refers to the variance of the limiting distribution of an estimator as the sample size (<span class="math inline">\(n\)</span>) approaches infinity.</p></li>
<li><p>It characterizes the variability of the scaled estimator <span class="math inline">\(\sqrt{n}(\bar{x} - \mu)\)</span> in its asymptotic distribution (e.g., normal distribution).</p></li>
</ul>
<p><strong>Limiting Variance</strong> (<span class="math inline">\(\lim_{n \to \infty} Var\)</span>)</p>
<p><span class="math display">\[
\lim_{n \to \infty} Var(\sqrt{n}(\bar{x}-\mu)) = \sigma^2
\]</span></p>
<ul>
<li>Represents the value that the actual variance of <span class="math inline">\(\sqrt{n}(\bar{x} - \mu)\)</span> converges to as <span class="math inline">\(n \to \infty\)</span>.</li>
</ul>
<p>For a well-behaved estimator,</p>
<p><span class="math display">\[
Avar(\sqrt{n}(\bar{X} - \mu)) = \lim_{n \to \infty} Var(\sqrt{n}(\bar{x}-\mu)) = \sigma^2.
\]</span></p>
<p>However, <strong>asymptotic variance is not necessarily equal to the limiting value of the variance</strong> because asymptotic variance is derived from the limiting distribution, while limiting variance is a convergence result of the sequence of variances.</p>
<p><span class="math display">\[
Avar(.) \neq lim_{n \to \infty} Var(.)
\]</span></p>
<ul>
<li><p>Both the asymptotic variance <span class="math inline">\(Avar\)</span> and the limiting variance <span class="math inline">\(\lim_{n \to \infty} Var\)</span> are numerically equal to <span class="math inline">\(\sigma^2\)</span>, but their conceptual definitions differ.</p></li>
<li>
<p><span class="math inline">\(Avar(\cdot) \neq \lim_{n \to \infty} Var(\cdot)\)</span>. This emphasizes that while the numerical result may match, their derivation and meaning differ:</p>
<ul>
<li><p><span class="math inline">\(Avar\)</span> depends on the asymptotic (large-sample) distribution of the estimator.</p></li>
<li><p><span class="math inline">\(\lim_{n \to \infty} Var(\cdot)\)</span> involves the sequence of variances as <span class="math inline">\(n\)</span> grows.</p></li>
</ul>
</li>
</ul>
<p>Cases where the two do not match:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Sample Quantiles</strong>: Consider the sample quantile of order <span class="math inline">\(p\)</span>, for some <span class="math inline">\(0 &lt; p &lt; 1\)</span>. Under regularity conditions, the asymptotic distribution of the sample quantile is normal, with a variance that depends on <span class="math inline">\(p\)</span> and the density of the distribution at the <span class="math inline">\(p\)</span>-th quantile. However, the variance of the sample quantile itself does not necessarily converge to this limit as the sample size grows.</li>
<li>
<strong>Bootstrap Methods</strong>: When using bootstrapping techniques to estimate the distribution of a statistic, the bootstrap distribution might converge to a different limiting distribution than the original statistic. In these cases, the variance of the bootstrap distribution (or the bootstrap variance) might differ from the limiting variance of the original statistic.</li>
<li>
<strong>Statistics with Randomly Varying Asymptotic Behavior</strong>: In some cases, the asymptotic behavior of a statistic can vary randomly depending on the sample path. For such statistics, the asymptotic variance might not provide a consistent estimate of the limiting variance.</li>
<li>
<strong>M-estimators with Varying Asymptotic Behavior</strong>: M-estimators can sometimes have different asymptotic behaviors depending on the tail behavior of the underlying distribution. For heavy-tailed distributions, the variance of the estimator might not stabilize even as the sample size grows large, making the asymptotic variance different from the variance of any limiting distribution.</li>
</ol>
</div>
</div>
<div id="random-variable" class="section level3" number="2.2.3">
<h3>
<span class="header-section-number">2.2.3</span> Random Variable<a class="anchor" aria-label="anchor" href="#random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>Random variables can be categorized as either <strong>discrete</strong> or <strong>continuous</strong>, with distinct properties and functions defining each type.</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="14%">
<col width="38%">
<col width="46%">
</colgroup>
<thead><tr class="header">
<th></th>
<th><strong>Discrete Variable</strong></th>
<th><strong>Continuous Variable</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Definition</strong></td>
<td>A random variable is discrete if it can assume at most a finite or countably infinite number of values.</td>
<td>A random variable is continuous if it can assume any value in some interval or intervals of real numbers, with <span class="math inline">\(P(X=x) = 0\)</span>.</td>
</tr>
<tr class="even">
<td><strong>Density Function</strong></td>
<td>A function <span class="math inline">\(f\)</span> is called a density for <span class="math inline">\(X\)</span> if:</td>
<td>A function <span class="math inline">\(f\)</span> is called a density for <span class="math inline">\(X\)</span> if:</td>
</tr>
<tr class="odd">
<td></td>
<td>1. <span class="math inline">\(f(x) \geq 0\)</span>
</td>
<td>1. <span class="math inline">\(f(x) \geq 0\)</span> for <span class="math inline">\(x\)</span> real</td>
</tr>
<tr class="even">
<td></td>
<td>2. <span class="math inline">\(\sum_{x} f(x) = 1\)</span>
</td>
<td>2. <span class="math inline">\(\int_{-\infty}^{\infty} f(x) \, dx = 1\)</span>
</td>
</tr>
<tr class="odd">
<td></td>
<td>3. <span class="math inline">\(f(x) = P(X = x)\)</span> for <span class="math inline">\(x\)</span> real</td>
<td>3. <span class="math inline">\(P[a \leq X \leq b] = \int_{a}^{b} f(x) \, dx\)</span> for <span class="math inline">\(a, b\)</span> real</td>
</tr>
<tr class="even">
<td><strong>Cumulative Distribution Function</strong></td>
<td><span class="math inline">\(F(x) = P(X \leq x)\)</span></td>
<td><span class="math inline">\(F(x) = P(X \leq x) = \int_{-\infty}^{x} f(t) \, dt\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(E[H(X)]\)</span></td>
<td><span class="math inline">\(\sum_{x} H(x) f(x)\)</span></td>
<td><span class="math inline">\(\int_{-\infty}^{\infty} H(x) f(x) \, dx\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu = E[X]\)</span></td>
<td><span class="math inline">\(\sum_{x} x f(x)\)</span></td>
<td><span class="math inline">\(\int_{-\infty}^{\infty} x f(x) \, dx\)</span></td>
</tr>
<tr class="odd">
<td><strong>Ordinary Moments</strong></td>
<td><span class="math inline">\(\sum_{x} x^k f(x)\)</span></td>
<td><span class="math inline">\(\int_{-\infty}^{\infty} x^k f(x) \, dx\)</span></td>
</tr>
<tr class="even">
<td><strong>Moment Generating Function</strong></td>
<td><span class="math inline">\(m_X(t) = E[e^{tX}] = \sum_{x} e^{tx} f(x)\)</span></td>
<td><span class="math inline">\(m_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} f(x) \, dx\)</span></td>
</tr>
</tbody>
</table></div>
<p><strong>Expected Value Properties</strong></p>
<ul>
<li>
<span class="math inline">\(E[c] = c\)</span> for any constant <span class="math inline">\(c\)</span>.</li>
<li>
<span class="math inline">\(E[cX] = cE[X]\)</span> for any constant <span class="math inline">\(c\)</span>.</li>
<li>
<span class="math inline">\(E[X + Y] = E[X] + E[Y]\)</span>.</li>
<li>
<span class="math inline">\(E[XY] = E[X]E[Y]\)</span> (if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent).</li>
</ul>
<p><strong>Variance Properties</strong></p>
<ul>
<li>
<span class="math inline">\(\text{Var}(c) = 0\)</span> for any constant <span class="math inline">\(c\)</span>.</li>
<li>
<span class="math inline">\(\text{Var}(cX) = c^2 \text{Var}(X)\)</span> for any constant <span class="math inline">\(c\)</span>.</li>
<li>
<span class="math inline">\(\text{Var}(X) \geq 0\)</span>.</li>
<li>
<span class="math inline">\(\text{Var}(X) = E[X^2] - (E[X])^2\)</span>.</li>
<li>
<span class="math inline">\(\text{Var}(X + c) = \text{Var}(X)\)</span>.</li>
<li>
<span class="math inline">\(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)\)</span> (if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent).</li>
</ul>
<p>The standard deviation <span class="math inline">\(\sigma\)</span> is given by: <span class="math display">\[
\sigma = \sqrt{\sigma^2} = \sqrt{\text{Var}(X)}.
\]</span></p>
<div id="multivariate-random-variables" class="section level4" number="2.2.3.1">
<h4>
<span class="header-section-number">2.2.3.1</span> Multivariate Random Variables<a class="anchor" aria-label="anchor" href="#multivariate-random-variables"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose <span class="math inline">\(y_1, \dots, y_p\)</span> are random variables with means <span class="math inline">\(\mu_1, \dots, \mu_p\)</span>. Then:</p>
<p><span class="math display">\[
\mathbf{y} = \begin{bmatrix}
y_1 \\
\vdots \\
y_p
\end{bmatrix}, \quad E[\mathbf{y}] = \begin{bmatrix}
\mu_1 \\
\vdots \\
\mu_p
\end{bmatrix} = \boldsymbol{\mu}.
\]</span></p>
<p>The covariance between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> is <span class="math inline">\(\sigma_{ij} = \text{Cov}(y_i, y_j)\)</span>. The variance-covariance (or dispersion) matrix is:</p>
<p><span class="math display">\[
\mathbf{\Sigma} = (\sigma_{ij})= \begin{bmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \dots &amp; \sigma_{1p} \\
\sigma_{21} &amp; \sigma_{22} &amp; \dots &amp; \sigma_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{p1} &amp; \sigma_{p2} &amp; \dots &amp; \sigma_{pp}
\end{bmatrix}.
\]</span></p>
<p>And <span class="math inline">\(\mathbf{\Sigma}\)</span> is symmetric with <span class="math inline">\((p+1)p/2\)</span> unique parameters.</p>
<p>Alternatively, let <span class="math inline">\(u_{p \times 1}\)</span> and <span class="math inline">\(v_{v \times 1}\)</span> be random vectors with means <span class="math inline">\(\mathbf{\mu_u}\)</span> and <span class="math inline">\(\mathbf{\mu_v}\)</span>. then</p>
<p><span class="math display">\[ \mathbf{\Sigma_{uv}} = cov(\mathbf{u,v}) = E[\mathbf{(u-\mu_u)(v-\mu_v)'}] \]</span></p>
<p><span class="math inline">\(\Sigma_{uv} \neq \Sigma_{vu}\)</span> (but <span class="math inline">\(\Sigma_{uv} = \Sigma_{vu}'\)</span>)</p>
<p><strong>Properties of Covariance Matrices</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Symmetry</strong>: <span class="math inline">\(\mathbf{\Sigma}' = \mathbf{\Sigma}\)</span>.</li>
<li>
<strong>Eigen-Decomposition</strong> (spectral decomposition,symmetric decomposition): <span class="math inline">\(\mathbf{\Sigma = \Phi \Lambda \Phi}\)</span>, where <span class="math inline">\(\mathbf{\Phi}\)</span> is a matrix of eigenvectors such that <span class="math inline">\(\mathbf{\Phi \Phi' = I}\)</span> (orthonormal), and <span class="math inline">\(\mathbf{\Lambda}\)</span> is a diagonal matrix with eigenvalues <span class="math inline">\((\lambda_1,...,\lambda_p)\)</span> on the diagonal.</li>
<li>
<strong>Non-Negative Definiteness</strong>: <span class="math inline">\(\mathbf{a \Sigma a} \ge 0\)</span> for any <span class="math inline">\(\mathbf{a} \in R^p\)</span>. Equivalently, the eigenvalues of <span class="math inline">\(\mathbf{\Sigma}\)</span>, <span class="math inline">\(\lambda_1 \ge ... \ge \lambda_p \ge 0\)</span>
</li>
<li>
<strong>Generalized Variance</strong>: <span class="math inline">\(|\mathbf{\Sigma}| = \lambda_1 \dots \lambda_p \geq 0\)</span>.</li>
<li>
<strong>Trace</strong>: <span class="math inline">\(\text{tr}(\mathbf{\Sigma}) = \lambda_1 + \dots + \lambda_p = \sigma_{11} + \dots+ \sigma_{pp} = \sum \sigma_{ii}\)</span> = sum of variances (total variance).</li>
</ol>
<p><strong>Note</strong>: <span class="math inline">\(\mathbf{\Sigma}\)</span> is required to be positive definite. This implies that all eigenvalues are positive, and <span class="math inline">\(\mathbf{\Sigma}\)</span> has an inverse <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>, such that <span class="math inline">\(\mathbf{\Sigma}^{-1}\mathbf{\Sigma}= \mathbf{I}_{p \times p} = \mathbf{\Sigma}\mathbf{\Sigma}^{-1}\)</span></p>
</div>
<div id="correlation-matrices" class="section level4" number="2.2.3.2">
<h4>
<span class="header-section-number">2.2.3.2</span> Correlation Matrices<a class="anchor" aria-label="anchor" href="#correlation-matrices"><i class="fas fa-link"></i></a>
</h4>
<p>The correlation coefficient <span class="math inline">\(\rho_{ij}\)</span> and correlation matrix <span class="math inline">\(\mathbf{R}\)</span> are defined as:</p>
<p><span class="math display">\[
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}, \quad \mathbf{R} = \begin{bmatrix}
1 &amp; \rho_{12} &amp; \dots &amp; \rho_{1p} \\
\rho_{21} &amp; 1 &amp; \dots &amp; \rho_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\rho_{p1} &amp; \rho_{p2} &amp; \dots &amp; 1
\end{bmatrix}.
\]</span></p>
<p>where <span class="math inline">\(\rho_{ii} = 1 \forall i\)</span></p>
</div>
<div id="linear-transformations" class="section level4" number="2.2.3.3">
<h4>
<span class="header-section-number">2.2.3.3</span> Linear Transformations<a class="anchor" aria-label="anchor" href="#linear-transformations"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> be matrices of constants, and <span class="math inline">\(\mathbf{c}\)</span> and <span class="math inline">\(\mathbf{d}\)</span> be vectors of constants. Then:</p>
<ul>
<li>
<span class="math inline">\(E[\mathbf{Ay + c}] = \mathbf{A \mu_y + c}\)</span>.</li>
<li>
<span class="math inline">\(\text{Var}(\mathbf{Ay + c}) = \mathbf{A \Sigma_y A'}\)</span>.</li>
<li>
<span class="math inline">\(\text{Cov}(\mathbf{Ay + c, By + d}) = \mathbf{A \Sigma_y B'}\)</span>.</li>
</ul>
</div>
</div>
<div id="moment-generating-function" class="section level3" number="2.2.4">
<h3>
<span class="header-section-number">2.2.4</span> Moment Generating Function<a class="anchor" aria-label="anchor" href="#moment-generating-function"><i class="fas fa-link"></i></a>
</h3>
<div id="properties-of-the-moment-generating-function" class="section level4" number="2.2.4.1">
<h4>
<span class="header-section-number">2.2.4.1</span> Properties of the Moment Generating Function<a class="anchor" aria-label="anchor" href="#properties-of-the-moment-generating-function"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\frac{d^k(m_X(t))}{dt^k} \bigg|_{t=0} = E[X^k]\)</span> (The <span class="math inline">\(k\)</span>-th derivative at <span class="math inline">\(t=0\)</span> gives the <span class="math inline">\(k\)</span>-th moment of <span class="math inline">\(X\)</span>).</li>
<li>
<span class="math inline">\(\mu = E[X] = m_X'(0)\)</span> (The first derivative at <span class="math inline">\(t=0\)</span> gives the mean).</li>
<li>
<span class="math inline">\(E[X^2] = m_X''(0)\)</span> (The second derivative at <span class="math inline">\(t=0\)</span> gives the second moment).</li>
</ol>
</div>
<div id="theorems-involving-mgfs" class="section level4" number="2.2.4.2">
<h4>
<span class="header-section-number">2.2.4.2</span> Theorems Involving MGFs<a class="anchor" aria-label="anchor" href="#theorems-involving-mgfs"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n, Y\)</span> be random variables with MGFs <span class="math inline">\(m_{X_1}(t), m_{X_2}(t), \dots, m_{X_n}(t), m_Y(t)\)</span>:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(m_{X_1}(t) = m_{X_2}(t)\)</span> for all <span class="math inline">\(t\)</span> in some open interval about 0, then <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the same distribution.</li>
<li>If <span class="math inline">\(Y = \alpha + \beta X_1\)</span>, then: <span class="math display">\[
m_Y(t) = e^{\alpha t}m_{X_1}(\beta t).
\]</span>
</li>
<li>If <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent and <span class="math inline">\(Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + \dots + \alpha_n X_n\)</span>, where <span class="math inline">\(\alpha_0, \alpha_1, \dots, \alpha_n\)</span> are constants, then: <span class="math display">\[
m_Y(t) = e^{\alpha_0 t} m_{X_1}(\alpha_1 t) m_{X_2}(\alpha_2 t) \dots m_{X_n}(\alpha_n t).
\]</span>
</li>
<li>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent normal random variables with means <span class="math inline">\(\mu_1, \mu_2, \dots, \mu_n\)</span> and variances <span class="math inline">\(\sigma_1^2, \sigma_2^2, \dots, \sigma_n^2\)</span>. If <span class="math inline">\(Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + \dots + \alpha_n X_n\)</span>, then:
<ul>
<li>
<span class="math inline">\(Y\)</span> is normally distributed.</li>
<li>Mean: <span class="math inline">\(\mu_Y = \alpha_0 + \alpha_1 \mu_1 + \alpha_2 \mu_2 + \dots + \alpha_n \mu_n\)</span>.</li>
<li>Variance: <span class="math inline">\(\sigma_Y^2 = \alpha_1^2 \sigma_1^2 + \alpha_2^2 \sigma_2^2 + \dots + \alpha_n^2 \sigma_n^2\)</span>.</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="moments" class="section level3" number="2.2.5">
<h3>
<span class="header-section-number">2.2.5</span> Moments<a class="anchor" aria-label="anchor" href="#moments"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="16%">
<col width="34%">
<col width="47%">
</colgroup>
<thead><tr class="header">
<th>Moment</th>
<th>Uncentered</th>
<th>Centered</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1st</td>
<td><span class="math inline">\(E[X] = \mu = \text{Mean}(X)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>2nd</td>
<td><span class="math inline">\(E[X^2]\)</span></td>
<td><span class="math inline">\(E[(X-\mu)^2] = \text{Var}(X) = \sigma^2\)</span></td>
</tr>
<tr class="odd">
<td>3rd</td>
<td><span class="math inline">\(E[X^3]\)</span></td>
<td><span class="math inline">\(E[(X-\mu)^3]\)</span></td>
</tr>
<tr class="even">
<td>4th</td>
<td><span class="math inline">\(E[X^4]\)</span></td>
<td><span class="math inline">\(E[(X-\mu)^4]\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>
<p><strong>Skewness</strong>: <span class="math inline">\(\text{Skewness}(X) = \frac{E[(X-\mu)^3]}{\sigma^3}\)</span></p>
<ul>
<li>
<strong>Definition</strong>: Skewness measures the asymmetry of a probability distribution around its mean.</li>
<li>
<strong>Interpretation</strong>:
<ul>
<li><p><strong>Positive skewness</strong>: The right tail (higher values) is longer or heavier than the left tail.</p></li>
<li><p><strong>Negative skewness</strong>: The left tail (lower values) is longer or heavier than the right tail.</p></li>
<li><p><strong>Zero skewness</strong>: The data is symmetric.</p></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Kurtosis</strong>: <span class="math inline">\(\text{Kurtosis}(X) = \frac{E[(X-\mu)^4]}{\sigma^4}\)</span></p>
<ul>
<li><p><strong>Definition</strong>: Kurtosis measures the “tailedness” or the heaviness of the tails of a probability distribution.</p></li>
<li><p>Excess kurtosis (often reported) is the kurtosis minus 3 (to compare against the normal distribution’s kurtosis of 3).</p></li>
<li>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><p><strong>High kurtosis (&gt;3)</strong>: Heavy tails, more extreme outliers.</p></li>
<li><p><strong>Low kurtosis (&lt;3)</strong>: Light tails, fewer outliers.</p></li>
<li><p><strong>Normal distribution kurtosis = 3</strong>: Benchmark for comparison.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div id="skewness" class="section level3" number="2.2.6">
<h3>
<span class="header-section-number">2.2.6</span> Skewness<a class="anchor" aria-label="anchor" href="#skewness"><i class="fas fa-link"></i></a>
</h3>
<p>Skewness measures the asymmetry of the distribution:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Positive skew</strong>: The right side (high values) is stretched out.</p>
<ul>
<li><p>Positive skew occurs when the right tail (higher values) of the distribution is longer or heavier.</p></li>
<li>
<p><strong>Examples</strong>:</p>
<ul>
<li><p><strong>Income Distribution</strong>: In many countries, most people earn a moderate income, but a small fraction of ultra-high earners stretches the distribution’s right tail.</p></li>
<li><p><strong>Housing Prices</strong>: Most homes may be around an affordable price, but a few extravagant mansions create a very long (and expensive) upper tail.</p></li>
</ul>
</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load required libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate data for positive skew</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">positive_skew_income</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">rbeta</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">100</span>  <span class="co"># Income distribution example</span></span>
<span><span class="va">positive_skew_housing</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">rbeta</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">1000</span>  <span class="co"># Housing prices example</span></span>
<span></span>
<span><span class="co"># Combine data</span></span>
<span><span class="va">data_positive_skew</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">positive_skew_income</span>, <span class="va">positive_skew_housing</span><span class="op">)</span>,</span>
<span>    example <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Income Distribution"</span>, <span class="fl">1000</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Housing Prices"</span>, <span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot positive skew</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_positive_skew</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, fill <span class="op">=</span> <span class="va">example</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">30</span>,</span>
<span>                   alpha <span class="op">=</span> <span class="fl">0.7</span>,</span>
<span>                   position <span class="op">=</span> <span class="st">"identity"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span> <span class="op">~</span> <span class="va">example</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Visualization of Positive Skew"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Frequency"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li>In the <strong>Income Distribution</strong> example, most people earn moderate incomes, but a few high earners stretch the right tail.</li>
<li>In the <strong>Housing Prices</strong> example, most homes are reasonably priced, but a few mansions create a long, expensive right tail.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Negative Skew (Left Skew)</li>
</ol>
<ul>
<li><p>Negative skew occurs when the left tail (lower values) of the distribution is longer or heavier.</p></li>
<li>
<p><strong>Examples</strong>:</p>
<ul>
<li><p><strong>Scores on an Easy Test</strong>: If an exam is very easy, most students score quite high, and only a few students score low, creating a left tail.</p></li>
<li><p><strong>Age of Retirement</strong>: Most people might retire around a common age (say 65+), with fewer retiring very early (stretching the left tail).</p></li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate data for negative skew</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">negative_skew_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fl">10</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">rbeta</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">10</span>  <span class="co"># Easy test scores example</span></span>
<span><span class="va">negative_skew_retirement</span> <span class="op">&lt;-</span></span>
<span>    <span class="fl">80</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">rbeta</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">20</span>  <span class="co"># Retirement age example</span></span>
<span></span>
<span><span class="co"># Combine data</span></span>
<span><span class="va">data_negative_skew</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">negative_skew_test</span>, <span class="va">negative_skew_retirement</span><span class="op">)</span>,</span>
<span>    example <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Easy Test Scores"</span>, <span class="fl">1000</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Retirement Age"</span>, <span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot negative skew</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_negative_skew</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, fill <span class="op">=</span> <span class="va">example</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">30</span>,</span>
<span>                   alpha <span class="op">=</span> <span class="fl">0.7</span>,</span>
<span>                   position <span class="op">=</span> <span class="st">"identity"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span> <span class="op">~</span> <span class="va">example</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Visualization of Negative Skew"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Frequency"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>In the <strong>Easy Test Scores</strong> example, most students perform well, but a few low scores stretch the left tail.</p></li>
<li><p>In the <strong>Retirement Age</strong> example, most people retire around the same age, but a small number of individuals retire very early, stretching the left tail.</p></li>
</ul>
</div>
<div id="kurtosis" class="section level3" number="2.2.7">
<h3>
<span class="header-section-number">2.2.7</span> Kurtosis<a class="anchor" aria-label="anchor" href="#kurtosis"><i class="fas fa-link"></i></a>
</h3>
<p>Kurtosis measures the “peakedness” or heaviness of the tails:</p>
<ul>
<li>
<p><strong>High kurtosis</strong>: Tall, sharp peak with heavy tails.</p>
<ul>
<li>
<strong>Example</strong>: Financial market returns during a crisis (extreme losses or gains).</li>
</ul>
</li>
<li>
<p><strong>Low kurtosis</strong>: Flatter peak with thinner tails.</p>
<ul>
<li>
<strong>Example</strong>: Human height distribution (fewer extreme deviations from the mean).</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate data for kurtosis</span></span>
<span><span class="va">low_kurtosis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span>  <span class="co"># Low kurtosis</span></span>
<span><span class="va">high_kurtosis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">900</span>, <span class="fl">5</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span>  <span class="co"># High kurtosis</span></span>
<span></span>
<span><span class="co"># Combine data</span></span>
<span><span class="va">data_kurtosis</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">low_kurtosis</span>, <span class="va">high_kurtosis</span><span class="op">)</span>,</span>
<span>  kurtosis_type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"Low Kurtosis (Height Distribution)"</span>, <span class="fl">1000</span><span class="op">)</span>, </span>
<span>                    <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="st">"High Kurtosis (Market Returns)"</span>, <span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot kurtosis</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">data_kurtosis</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, fill <span class="op">=</span> <span class="va">kurtosis_type</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">30</span>, alpha <span class="op">=</span> <span class="fl">0.7</span>, position <span class="op">=</span> <span class="st">"identity"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">kurtosis_type</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Visualization of Kurtosis"</span>,</span>
<span>    x <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>The left panel shows <strong>low kurtosis</strong>, similar to the distribution of human height, which has a flatter peak and thinner tails.</p></li>
<li><p>The right panel shows <strong>high kurtosis</strong>, reflecting financial market returns, where there are more extreme outliers in gains or losses.</p></li>
</ul>
<hr>
<div id="conditional-moments" class="section level4" number="2.2.7.1">
<h4>
<span class="header-section-number">2.2.7.1</span> Conditional Moments<a class="anchor" aria-label="anchor" href="#conditional-moments"><i class="fas fa-link"></i></a>
</h4>
<p>For a random variable <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Expected Value</strong>: <span class="math display">\[
E[Y|X=x] =
\begin{cases}
\sum_y y f_Y(y|x) &amp; \text{for discrete RV}, \\
\int_y y f_Y(y|x) \, dy &amp; \text{for continuous RV}.
\end{cases}
\]</span></p></li>
<li><p><strong>Variance</strong>: <span class="math display">\[
\text{Var}(Y|X=x) =
\begin{cases}
\sum_y (y - E[Y|X=x])^2 f_Y(y|x) &amp; \text{for discrete RV}, \\
\int_y (y - E[Y|X=x])^2 f_Y(y|x) \, dy &amp; \text{for continuous RV}.
\end{cases}
\]</span></p></li>
</ol>
<hr>
</div>
<div id="multivariate-moments" class="section level4" number="2.2.7.2">
<h4>
<span class="header-section-number">2.2.7.2</span> Multivariate Moments<a class="anchor" aria-label="anchor" href="#multivariate-moments"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><p><strong>Expected Value</strong>: <span class="math display">\[
E
\begin{bmatrix}
X \\
Y
\end{bmatrix}
=
\begin{bmatrix}
E[X] \\
E[Y]
\end{bmatrix}
=
\begin{bmatrix}
\mu_X \\
\mu_Y
\end{bmatrix}
\]</span></p></li>
<li><p><strong>Variance-Covariance Matrix</strong>: <span class="math display">\[
\begin{aligned}
\text{Var}
\begin{bmatrix}
X \\
Y
\end{bmatrix}
&amp;=
\begin{bmatrix}
\text{Var}(X) &amp; \text{Cov}(X, Y) \\
\text{Cov}(X, Y) &amp; \text{Var}(Y)
\end{bmatrix}
\\
&amp;=
\begin{bmatrix}
E[(X-\mu_X)^2] &amp; E[(X-\mu_X)(Y-\mu_Y)] \\
E[(X-\mu_X)(Y-\mu_Y)] &amp; E[(Y-\mu_Y)^2]
\end{bmatrix}
\end{aligned}
\]</span></p></li>
</ol>
<hr>
</div>
<div id="properties-of-moments" class="section level4" number="2.2.7.3">
<h4>
<span class="header-section-number">2.2.7.3</span> Properties of Moments<a class="anchor" aria-label="anchor" href="#properties-of-moments"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E[aX + bY + c] = aE[X] + bE[Y] + c\)</span></li>
<li><span class="math inline">\(\text{Var}(aX + bY + c) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(X, Y)\)</span></li>
<li><span class="math inline">\(\text{Cov}(aX + bY, cX + dY) = ac \text{Var}(X) + bd \text{Var}(Y) + (ad + bc) \text{Cov}(X, Y)\)</span></li>
<li>Correlation: <span class="math inline">\(\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\)</span>
</li>
</ol>
</div>
</div>
<div id="distributions" class="section level3" number="2.2.8">
<h3>
<span class="header-section-number">2.2.8</span> Distributions<a class="anchor" aria-label="anchor" href="#distributions"><i class="fas fa-link"></i></a>
</h3>
<div id="conditional-distributions" class="section level4" number="2.2.8.1">
<h4>
<span class="header-section-number">2.2.8.1</span> Conditional Distributions<a class="anchor" aria-label="anchor" href="#conditional-distributions"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)}
\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent:</p>
<p><span class="math display">\[
f_{X|Y}(x|y) = f_X(x).
\]</span></p>
<hr>
</div>
<div id="discrete-distributions" class="section level4" number="2.2.8.2">
<h4>
<span class="header-section-number">2.2.8.2</span> Discrete Distributions<a class="anchor" aria-label="anchor" href="#discrete-distributions"><i class="fas fa-link"></i></a>
</h4>
<div id="bernoulli-distribution" class="section level5" number="2.2.8.2.1">
<h5>
<span class="header-section-number">2.2.8.2.1</span> Bernoulli Distribution<a class="anchor" aria-label="anchor" href="#bernoulli-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>A random variable <span class="math inline">\(X\)</span> follows a Bernoulli distribution, denoted as <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>, if it represents a single trial with:</p>
<ul>
<li><p>Success probability <span class="math inline">\(p\)</span></p></li>
<li><p>Failure probability <span class="math inline">\(q = 1-p\)</span>.</p></li>
</ul>
<p><strong>Density Function</strong><span class="math display">\[
f(x) = p^x (1-p)^{1-x}, \quad x \in \{0, 1\}
\]</span></p>
<p><strong>CDF</strong>: Use table or manual computation.</p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu">mc2d</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/mc2d/man/bernoulli.html">rbern</a></span><span class="op">(</span><span class="fl">1000</span>, prob <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Bernoulli Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-4-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = E[X] = p
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = p(1-p)
\]</span></p>
<hr>
</div>
<div id="binomial-distribution" class="section level5" number="2.2.8.2.2">
<h5>
<span class="header-section-number">2.2.8.2.2</span> Binomial Distribution<a class="anchor" aria-label="anchor" href="#binomial-distribution"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math inline">\(X \sim B(n, p)\)</span> is the number of successes in <span class="math inline">\(n\)</span> independent Bernoulli trials, where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> is the number of trials</p></li>
<li><p><span class="math inline">\(p\)</span> is the success probability.</p></li>
<li><p>The trials are identical and independent, and probability of success (<span class="math inline">\(p\)</span>) and probability of failure (<span class="math inline">\(q = 1 - p\)</span>) remains the same for all trials.</p></li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x) = \binom{n}{x} p^x (1-p)^{n-x}, \quad x = 0, 1, \dots, n
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1000</span>, size <span class="op">=</span> <span class="fl">100</span>, prob <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Binomial Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-5-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_X(t) = (1 - p + p e^t)^n
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = np
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = np(1-p)
\]</span></p>
<hr>
</div>
<div id="poisson-distribution" class="section level5" number="2.2.8.2.3">
<h5>
<span class="header-section-number">2.2.8.2.3</span> Poisson Distribution<a class="anchor" aria-label="anchor" href="#poisson-distribution"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span> models the number of occurrences of an event in a fixed interval, with average rate <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><p>Arises with Poisson process, which involves observing discrete events in a continuous “interval” of time, length, or space.</p></li>
<li><p>The random variable <span class="math inline">\(X\)</span> is the number of occurrences of the event within an interval of <span class="math inline">\(s\)</span> units.</p></li>
<li><p>The parameter <span class="math inline">\(\lambda\)</span> is the average number of occurrences of the event in question per measurement unit. For the distribution, we use the parameter <span class="math inline">\(k = \lambda s\)</span>.</p></li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x) = \frac{e^{-k} k^x}{x!}, \quad x = 0, 1, 2, \dots
\]</span></p>
<p><strong>CDF</strong></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">1000</span>, lambda <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Histogram of Poisson Distribution"</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Frequency"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_X(t) = e^{k (e^t - 1)}
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = E(X) = k
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = Var(X) = k
\]</span></p>
<hr>
</div>
<div id="geometric-distribution" class="section level5" number="2.2.8.2.4">
<h5>
<span class="header-section-number">2.2.8.2.4</span> Geometric Distribution<a class="anchor" aria-label="anchor" href="#geometric-distribution"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math inline">\(X \sim \text{G}(p)\)</span> models the number of trials needed to obtain the first success, with:</p>
<ul>
<li><p><span class="math inline">\(p\)</span>: probability of success</p></li>
<li><p><span class="math inline">\(q = 1-p\)</span>: probability of failure.</p></li>
<li><p>The experiment consists of a series of trails. The outcome of each trial can be classed as being either a “success” (s) or “failure” (f). (i.e., Bernoulli trial).</p></li>
<li><p>The trials are identical and independent in the sense that the outcome of one trial has no effect on the outcome of any other (i..e, lack of memory - momerylessness). The probability of success (<span class="math inline">\(p\)</span>) and probability of failure (<span class="math inline">\(q = 1- p\)</span>) remains the same from trial to trial.</p></li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x) = p(1-p)^{x-1}, \quad x = 1, 2, \dots
\]</span></p>
<p><strong>CDF</strong><span class="math display">\[
F(x) = 1 - (1-p)^x
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Geometric.html">rgeom</a></span><span class="op">(</span><span class="fl">1000</span>, prob <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Histogram of Geometric Distribution"</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Frequency"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_X(t) = \frac{p e^t}{1 - (1-p)e^t}, \quad t &lt; -\ln(1-p)
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = \frac{1}{p}
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \frac{1-p}{p^2}
\]</span></p>
<hr>
</div>
<div id="hypergeometric-distribution" class="section level5" number="2.2.8.2.5">
<h5>
<span class="header-section-number">2.2.8.2.5</span> Hypergeometric Distribution<a class="anchor" aria-label="anchor" href="#hypergeometric-distribution"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math inline">\(X \sim \text{H}(N, r, n)\)</span> models the number of successes in a sample of size <span class="math inline">\(n\)</span> drawn without replacement from a population of size <span class="math inline">\(N\)</span>, where:</p>
<ul>
<li><p><span class="math inline">\(r\)</span> objects have the trait of interest</p></li>
<li><p><span class="math inline">\(N-r\)</span> do not have the trait.</p></li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x) = \frac{\binom{r}{x} \binom{N-r}{n-x}}{\binom{N}{n}}, \quad \max(0, n-(N-r)) \leq x \leq \min(n, r)
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Hypergeometric.html">rhyper</a></span><span class="op">(</span><span class="fl">1000</span>, m <span class="op">=</span> <span class="fl">50</span>, n <span class="op">=</span> <span class="fl">20</span>, k <span class="op">=</span> <span class="fl">30</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Hypergeometric Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-8-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>Mean</strong><span class="math display">\[
\mu = E[X] = \frac{n r}{N}
\]</span></p>
<p><strong>Variance</strong><span class="math display">\[
\sigma^2 = \text{Var}(X) = n \frac{r}{N} \frac{N-r}{N} \frac{N-n}{N-1}
\]</span></p>
<p><strong>Note</strong>: For large <span class="math inline">\(N\)</span> (when <span class="math inline">\(\frac{n}{N} \leq 0.05\)</span>), the hypergeometric distribution can be approximated by a binomial distribution with <span class="math inline">\(p = \frac{r}{N}\)</span>.</p>
</div>
</div>
<div id="continuous-distributions" class="section level4" number="2.2.8.3">
<h4>
<span class="header-section-number">2.2.8.3</span> Continuous Distributions<a class="anchor" aria-label="anchor" href="#continuous-distributions"><i class="fas fa-link"></i></a>
</h4>
<div id="uniform-distribution" class="section level5" number="2.2.8.3.1">
<h5>
<span class="header-section-number">2.2.8.3.1</span> Uniform Distribution<a class="anchor" aria-label="anchor" href="#uniform-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>Defined over an interval <span class="math inline">\((a, b)\)</span>, where the probabilities are “equally likely” for subintervals of equal length.</p>
<p><strong>Density Function</strong>: <span class="math display">\[
f(x) = \frac{1}{b-a}, \quad a &lt; x &lt; b
\]</span></p>
<p><strong>CDF</strong><span class="math display">\[
F(x) =
\begin{cases}
0 &amp; \text{if } x &lt; a \\
\frac{x-a}{b-a} &amp; a \le x \le b \\
1 &amp; \text{if } x &gt; b
\end{cases}
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Uniform Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong><span class="math display">\[
m_X(t) =
\begin{cases}
\frac{e^{tb} - e^{ta}}{t(b-a)} &amp; \text{if } t \neq 0 \\
1 &amp; \text{if } t = 0
\end{cases}
\]</span></p>
<p><strong>Mean</strong><span class="math display">\[
\mu = E[X] = \frac{a + b}{2}
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = \frac{(b-a)^2}{12}
\]</span></p>
<hr>
</div>
<div id="gamma-distribution" class="section level5" number="2.2.8.3.2">
<h5>
<span class="header-section-number">2.2.8.3.2</span> Gamma Distribution<a class="anchor" aria-label="anchor" href="#gamma-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The gamma distribution is used to define the exponential and <span class="math inline">\(\chi^2\)</span> distributions.</p>
<p>The <strong>gamma function</strong> is defined as: <span class="math display">\[
\Gamma(\alpha) = \int_0^{\infty} z^{\alpha-1}e^{-z}dz, \quad \alpha &gt; 0
\]</span></p>
<p><strong>Properties of the Gamma Function</strong>:</p>
<ul>
<li><p><span class="math inline">\(\Gamma(1) = 1\)</span></p></li>
<li><p>For <span class="math inline">\(\alpha &gt; 1\)</span>, <span class="math inline">\(\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)\)</span></p></li>
<li><p>If <span class="math inline">\(n\)</span> is an integer and <span class="math inline">\(n &gt; 1\)</span>, then <span class="math inline">\(\Gamma(n) = (n-1)!\)</span></p></li>
</ul>
<p><strong>Density Function</strong>:</p>
<p><span class="math display">\[
f(x) = \frac{1}{\Gamma(\alpha)\beta^{\alpha}} x^{\alpha-1} e^{-x/\beta}, \quad x &gt; 0
\]</span></p>
<p><strong>CDF</strong> (for <span class="math inline">\(\alpha = n\)</span>, and <span class="math inline">\(x&gt;0\)</span> a positive integer):</p>
<p><span class="math display">\[
F(x, n, \beta) = 1 - \sum_{k=0}^{n-1} \frac{(\frac{x}{\beta})^k e^{-x/\beta}}{k!}
\]</span></p>
<p><strong>PDF</strong>:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html">rgamma</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>, shape <span class="op">=</span> <span class="fl">5</span>, rate <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Gamma Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-10-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_X(t) = (1 - \beta t)^{-\alpha}, \quad t &lt; \frac{1}{\beta}
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = E[X] = \alpha \beta
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = \alpha \beta^2
\]</span></p>
<hr>
</div>
<div id="normal-distribution" class="section level5" number="2.2.8.3.3">
<h5>
<span class="header-section-number">2.2.8.3.3</span> Normal Distribution<a class="anchor" aria-label="anchor" href="#normal-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The normal distribution, denoted as <span class="math inline">\(N(\mu, \sigma^2)\)</span>, is symmetric and bell-shaped with parameters <span class="math inline">\(\mu\)</span> (mean) and <span class="math inline">\(\sigma^2\)</span> (variance). It is also known as the Gaussian distribution.</p>
<p><strong>Density Function</strong>:</p>
<p><span class="math display">\[
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}, \quad -\infty &lt; x &lt; \infty, \; \sigma &gt; 0
\]</span></p>
<p><strong>CDF</strong>: Use table or numerical methods.</p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Normal Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-11-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = E[X]
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X)
\]</span></p>
<p><strong>Standard Normal Random Variable</strong>:</p>
<ul>
<li><p>The normal random variable <span class="math inline">\(Z\)</span> with mean <span class="math inline">\(\mu = 0\)</span> and standard deviation <span class="math inline">\(\sigma = 1\)</span> is called a standard normal random variable.</p></li>
<li><p>Any normal random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> can be converted to the standard normal random variable <span class="math inline">\(Z\)</span>: <span class="math display">\[
Z = \frac{X - \mu}{\sigma}
\]</span></p></li>
</ul>
<p><strong>Normal Approximation to the Binomial Distribution</strong>:</p>
<p>Let <span class="math inline">\(X\)</span> be binomial with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. For large <span class="math inline">\(n\)</span>:</p>
<ul>
<li><p>If <span class="math inline">\(p \le 0.5\)</span> and <span class="math inline">\(np &gt; 5\)</span>, or</p></li>
<li><p>If <span class="math inline">\(p &gt; 0.5\)</span> and <span class="math inline">\(n(1-p) &gt; 5\)</span>,</p></li>
</ul>
<p><span class="math inline">\(X\)</span> is approximately normally distributed with mean <span class="math inline">\(\mu = np\)</span> and standard deviation <span class="math inline">\(\sigma = \sqrt{np(1-p)}\)</span>.</p>
<p>When using the normal approximation, add or subtract 0.5 as needed for the continuity correction.</p>
<p><strong>Discrete Approximate Normal (Corrected)</strong>:</p>
<div class="inline-table"><table class="table table-sm">
<caption><strong>Normal Probability Rule</strong></caption>
<thead><tr class="header">
<th align="center">Discrete</th>
<th align="center">Approximate Normal (corrected)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X = c)\)</span></td>
<td align="center"><span class="math inline">\(P(c -0.5 &lt; Y &lt; c + 0.5)\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(P(X &lt; c)\)</span></td>
<td align="center"><span class="math inline">\(P(Y &lt; c - 0.5)\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X \le c)\)</span></td>
<td align="center"><span class="math inline">\(P(Y &lt; c + 0.5)\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(P(X &gt; c)\)</span></td>
<td align="center"><span class="math inline">\(P(Y &gt; c + 0.5)\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X \ge c)\)</span></td>
<td align="center"><span class="math inline">\(P(Y &gt; c - 0.5)\)</span></td>
</tr>
</tbody>
</table></div>
<p>If X is normally distributed with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, then</p>
<ul>
<li><span class="math inline">\(P(-\sigma &lt; X - \mu &lt; \sigma) \approx .68\)</span></li>
<li><span class="math inline">\(P(-2\sigma &lt; X - \mu &lt; 2\sigma) \approx .95\)</span></li>
<li><span class="math inline">\(P(-3\sigma &lt; X - \mu &lt; 3\sigma) \approx .997\)</span></li>
</ul>
<hr>
</div>
<div id="logistic-distribution" class="section level5" number="2.2.8.3.4">
<h5>
<span class="header-section-number">2.2.8.3.4</span> Logistic Distribution<a class="anchor" aria-label="anchor" href="#logistic-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The logistic distribution is a continuous probability distribution commonly used in logistic regression and other types of statistical modeling. It resembles the normal distribution but has heavier tails, allowing for more extreme values. - The logistic distribution is symmetric around <span class="math inline">\(\mu\)</span>. - Its heavier tails make it useful for modeling outcomes with occasional extreme values.</p>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x; \mu, s) = \frac{e^{-(x-\mu)/s}}{s \left(1 + e^{-(x-\mu)/s}\right)^2}, \quad -\infty &lt; x &lt; \infty
\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the location parameter (mean) and <span class="math inline">\(s &gt; 0\)</span> is the scale parameter.</p>
<p><strong>CDF</strong></p>
<p><span class="math display">\[
F(x; \mu, s) = \frac{1}{1 + e^{-(x-\mu)/s}}, \quad -\infty &lt; x &lt; \infty
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">rlogis</a></span><span class="op">(</span><span class="fl">1000</span>, location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Logistic Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-12-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p>The MGF of the logistic distribution does not exist because its expected value diverges for most <span class="math inline">\(t\)</span>.</p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = E[X] = \mu
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = \frac{\pi^2 s^2}{3}
\]</span></p>
<hr>
</div>
<div id="laplace-distribution" class="section level5" number="2.2.8.3.5">
<h5>
<span class="header-section-number">2.2.8.3.5</span> Laplace Distribution<a class="anchor" aria-label="anchor" href="#laplace-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The Laplace distribution, also known as the double exponential distribution, is a continuous probability distribution often used in economics, finance, and engineering. It is characterized by a peak at its mean and heavier tails compared to the normal distribution.</p>
<ul>
<li>The Laplace distribution is symmetric around <span class="math inline">\(\mu\)</span>.</li>
<li>It has heavier tails than the normal distribution, making it suitable for modeling data with more extreme outliers.</li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x; \mu, b) = \frac{1}{2b} e^{-|x-\mu|/b}, \quad -\infty &lt; x &lt; \infty
\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the location parameter (mean) and <span class="math inline">\(b &gt; 0\)</span> is the scale parameter.</p>
<p><strong>CDF</strong></p>
<p><span class="math display">\[
F(x; \mu, b) =
\begin{cases}
    \frac{1}{2} e^{(x-\mu)/b} &amp; \text{if } x &lt; \mu \\
    1 - \frac{1}{2} e^{-(x-\mu)/b} &amp; \text{if } x \ge \mu
\end{cases}
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu">VGAM</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/VGAM/man/laplaceUC.html">rlaplace</a></span><span class="op">(</span><span class="fl">1000</span>, location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Laplace Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-13-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_X(t) = \frac{e^{\mu t}}{1 - b^2 t^2}, \quad |t| &lt; \frac{1}{b}
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
\mu = E[X] = \mu
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = 2b^2
\]</span></p>
<hr>
</div>
<div id="log-normal-distribution" class="section level5" number="2.2.8.3.6">
<h5>
<span class="header-section-number">2.2.8.3.6</span> Log-normal Distribution<a class="anchor" aria-label="anchor" href="#log-normal-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The log-normal distribution is denoted as <span class="math inline">\(\text{Lognormal}(\mu, \sigma^2)\)</span>.</p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Lognormal.html">rlnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>, meanlog <span class="op">=</span> <span class="fl">0</span>, sdlog <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>, main<span class="op">=</span><span class="st">"Histogram of Log-normal Distribution"</span>, xlab<span class="op">=</span><span class="st">"Value"</span>, ylab<span class="op">=</span><span class="st">"Frequency"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-14-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="lognormal-distribution" class="section level5" number="2.2.8.3.7">
<h5>
<span class="header-section-number">2.2.8.3.7</span> Lognormal Distribution<a class="anchor" aria-label="anchor" href="#lognormal-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The lognormal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. It is often used to model variables that are positively skewed, such as income or biological measurements.</p>
<ul>
<li>The lognormal distribution is positively skewed.</li>
<li>It is useful for modeling data that cannot take negative values and is often used in finance and environmental studies.</li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x; \mu, \sigma) = \frac{1}{x \sigma \sqrt{2\pi}} e^{-(\ln(x) - \mu)^2 / (2\sigma^2)}, \quad x &gt; 0
\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean of the underlying normal distribution and <span class="math inline">\(\sigma &gt; 0\)</span> is the standard deviation.</p>
<p><strong>CDF</strong></p>
<p>The cumulative distribution function of the lognormal distribution is given by:</p>
<p><span class="math display">\[
F(x; \mu, \sigma) = \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{\ln(x) - \mu}{\sigma \sqrt{2}} \right) \right], \quad x &gt; 0
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Lognormal.html">rlnorm</a></span><span class="op">(</span><span class="fl">1000</span>, meanlog <span class="op">=</span> <span class="fl">0</span>, sdlog <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Lognormal Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-15-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p>The moment generating function (MGF) of the lognormal distribution does not exist in a simple closed form.</p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
E[X] = e^{\mu + \sigma^2 / 2}
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = \left( e^{\sigma^2} - 1 \right) e^{2\mu + \sigma^2}
\]</span></p>
<hr>
</div>
<div id="exponential-distribution" class="section level5" number="2.2.8.3.8">
<h5>
<span class="header-section-number">2.2.8.3.8</span> Exponential Distribution<a class="anchor" aria-label="anchor" href="#exponential-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The exponential distribution, denoted as <span class="math inline">\(\text{Exp}(\lambda)\)</span>, is a special case of the gamma distribution with <span class="math inline">\(\alpha = 1\)</span>.</p>
<ul>
<li><p>It is commonly used to model the time between independent events that occur at a constant rate. It is often applied in reliability analysis and queuing theory.</p></li>
<li><p>The exponential distribution is memoryless, meaning the probability of an event occurring in the future is independent of the past.</p></li>
<li><p>It is commonly used to model waiting times, such as the time until the next customer arrives or the time until a radioactive particle decays.</p></li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x) = \frac{1}{\beta} e^{-x/\beta}, \quad x, \beta &gt; 0
\]</span></p>
<p><strong>CDF</strong><span class="math display">\[
F(x) =
\begin{cases}
0 &amp; \text{if } x \le 0 \\
1 - e^{-x/\beta} &amp; \text{if } x &gt; 0
\end{cases}
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Exponential.html">rexp</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>, rate <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Histogram of Exponential Distribution"</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Frequency"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-16-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong><span class="math display">\[
m_X(t) = (1-\beta t)^{-1}, \quad t &lt; 1/\beta
\]</span></p>
<p><strong>Mean</strong><span class="math display">\[
\mu = E[X] = \beta
\]</span></p>
<p><strong>Variance</strong><span class="math display">\[
\sigma^2 = \text{Var}(X) = \beta^2
\]</span></p>
<hr>
</div>
<div id="chi-squared-distribution" class="section level5" number="2.2.8.3.9">
<h5>
<span class="header-section-number">2.2.8.3.9</span> Chi-Squared Distribution<a class="anchor" aria-label="anchor" href="#chi-squared-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The chi-squared distribution is a continuous probability distribution commonly used in statistical inference, particularly in hypothesis testing and construction of confidence intervals for variance. It is also used in goodness-of-fit tests.</p>
<ul>
<li>The chi-squared distribution is defined only for positive values.</li>
<li>It is often used to model the distribution of the sum of the squares of <span class="math inline">\(k\)</span> independent standard normal random variables.</li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x; k) = \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \quad x \ge 0
\]</span></p>
<p>where <span class="math inline">\(k\)</span> is the degrees of freedom and <span class="math inline">\(\Gamma\)</span> is the gamma function.</p>
<p><strong>CDF</strong></p>
<p>The cumulative distribution function of the chi-squared distribution is given by:</p>
<p><span class="math display">\[
F(x; k) = \frac{\gamma(k/2, x/2)}{\Gamma(k/2)}, \quad x \ge 0
\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is the lower incomplete gamma function.</p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">rchisq</a></span><span class="op">(</span><span class="fl">1000</span>, df <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Chi-Squared Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_X(t) = (1 - 2t)^{-k/2}, \quad t &lt; \frac{1}{2}
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
E[X] = k
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = 2k
\]</span></p>
<hr>
</div>
<div id="students-t-distribution" class="section level5" number="2.2.8.3.10">
<h5>
<span class="header-section-number">2.2.8.3.10</span> Student’s T Distribution<a class="anchor" aria-label="anchor" href="#students-t-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>Student’s t-distribution</strong> is named after <strong>William Sealy Gosset</strong>, a statistician at Guinness Brewery in the early 20th century. Gosset developed the t-distribution to address small-sample problems in quality control. Since Guinness prohibited employees from publishing under their names, Gosset used the pseudonym <strong>“Student”</strong> when he published his work in 1908 <span class="citation">(<a href="references.html#ref-student1908probable">Student 1908</a>)</span>. The name has stuck ever since, honoring his contribution to statistics.</p>
<p>The Student’s t-distribution, denoted as <span class="math inline">\(T(v)\)</span>, is defined by: <span class="math display">\[
T = \frac{Z}{\sqrt{\chi^2_v / v}},
\]</span> where <span class="math inline">\(Z\)</span> is a standard normal random variable and <span class="math inline">\(\chi^2_v\)</span> follows a chi-squared distribution with <span class="math inline">\(v\)</span> degrees of freedom.</p>
<p>The Student’s T distribution is a continuous probability distribution used in statistical inference, particularly for estimating population parameters when the sample size is small and/or the population variance is unknown. It is similar to the normal distribution but has heavier tails, which makes it more robust for small sample sizes.</p>
<ul>
<li>The Student’s T distribution is symmetric around 0.</li>
<li>It has heavier tails than the normal distribution, making it useful for dealing with outliers or small sample sizes.</li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x;v) = \frac{\Gamma((v + 1)/2)}{\sqrt{v \pi} \Gamma(v/2)} \left( 1 + \frac{x^2}{v} \right)^{-(v + 1)/2}
\]</span></p>
<p>where <span class="math inline">\(v\)</span> is the degrees of freedom and <span class="math inline">\(\Gamma(x)\)</span> is the Gamma function.</p>
<p><strong>CDF</strong></p>
<p>The cumulative distribution function of the Student’s T distribution is more complex and typically evaluated using numerical methods.</p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="fl">1000</span>, df <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Student's T Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-18-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p>The moment generating function (MGF) of the Student’s T distribution does not exist in a simple closed form.</p>
<p><strong>Mean</strong></p>
<p>For <span class="math inline">\(v &gt; 1\)</span>:</p>
<p><span class="math display">\[
E[X] = 0
\]</span></p>
<p><strong>Variance</strong></p>
<p>For <span class="math inline">\(v &gt; 2\)</span>:</p>
<p><span class="math display">\[
\sigma^2 =  \text{Var}(X) = \frac{v}{v - 2}
\]</span></p>
<hr>
</div>
<div id="non-central-t-distribution" class="section level5" number="2.2.8.3.11">
<h5>
<span class="header-section-number">2.2.8.3.11</span> Non-central T Distribution<a class="anchor" aria-label="anchor" href="#non-central-t-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The non-central t-distribution, denoted as <span class="math inline">\(T(v, \lambda)\)</span>, is a generalization of the Student’s t-distribution. It is defined as: <span class="math display">\[
T = \frac{Z + \lambda}{\sqrt{\chi^2_v / v}},
\]</span> where <span class="math inline">\(Z\)</span> is a standard normal random variable, <span class="math inline">\(\chi^2_v\)</span> follows a chi-squared distribution with <span class="math inline">\(v\)</span> degrees of freedom, and <span class="math inline">\(\lambda\)</span> is the <strong>non-centrality parameter</strong>. This additional parameter introduces asymmetry to the distribution.</p>
<p>The non-central t-distribution arises in scenarios where the null hypothesis does not hold, such as under the alternative hypothesis in hypothesis testing. The non-centrality parameter <span class="math inline">\(\lambda\)</span> represents the degree to which the mean deviates from zero.</p>
<ul>
<li>For <span class="math inline">\(\lambda = 0\)</span>, the non-central t-distribution reduces to the Student’s t-distribution.</li>
<li>The distribution is skewed for <span class="math inline">\(\lambda \neq 0\)</span>, with the skewness increasing as <span class="math inline">\(\lambda\)</span> grows.</li>
</ul>
<p><strong>Density Function</strong></p>
<p>The density function of the non-central t-distribution is more complex and depends on <span class="math inline">\(v\)</span> and <span class="math inline">\(\lambda\)</span>. It can be expressed in terms of an infinite sum:</p>
<p><span class="math display">\[
f(x; v, \lambda) = \sum_{k=0}^\infty \frac{e^{-\lambda^2/2}(\lambda^2/2)^k}{k!} \cdot \frac{\Gamma((v + k + 1)/2)}{\sqrt{v \pi} \Gamma((v + k)/2)} \left( 1 + \frac{x^2}{v} \right)^{-(v + k + 1)/2}.
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span>  <span class="co"># Number of samples</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fl">5</span>    <span class="co"># Degrees of freedom</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="fl">2</span>  <span class="co"># Non-centrality parameter</span></span>
<span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span><span class="va">n</span>, df <span class="op">=</span> <span class="va">df</span>, ncp <span class="op">=</span> <span class="va">lambda</span><span class="op">)</span>,</span>
<span>  main <span class="op">=</span> <span class="st">"Histogram of Non-central T Distribution"</span>,</span>
<span>  xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>  ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-19-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>CDF</strong></p>
<p>The cumulative distribution function of the non-central t-distribution is typically computed using numerical methods due to its complexity.</p>
<p><strong>Mean</strong></p>
<p>For <span class="math inline">\(v &gt; 1\)</span>:</p>
<p><span class="math display">\[
E[T] = \lambda \sqrt{\frac{v}{2}} \cdot \frac{\Gamma((v - 1)/2)}{\Gamma(v/2)}.
\]</span></p>
<p><strong>Variance</strong></p>
<p>For <span class="math inline">\(v &gt; 2\)</span>:</p>
<p><span class="math display">\[
\text{Var}(T) = \frac{v}{v - 2} + \lambda^2.
\]</span></p>
<hr>
<p><strong>Comparison: Student’s T vs. Non-central T</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="23%">
<col width="33%">
<col width="42%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Student’s t-distribution</th>
<th>Non-central t-distribution</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Definition</td>
<td><span class="math inline">\(T = \frac{Z}{\sqrt{\chi^2_v / v}}\)</span></td>
<td><span class="math inline">\(T = \frac{Z + \lambda}{\sqrt{\chi^2_v / v}}\)</span></td>
</tr>
<tr class="even">
<td>Centered at</td>
<td>0</td>
<td><span class="math inline">\(\lambda\)</span></td>
</tr>
<tr class="odd">
<td>Symmetry</td>
<td>Symmetric</td>
<td>Skewed for <span class="math inline">\(\lambda \neq 0\)</span>
</td>
</tr>
<tr class="even">
<td>Parameters</td>
<td>Degrees of freedom (<span class="math inline">\(v\)</span>)</td>
<td>
<span class="math inline">\(v\)</span> and <span class="math inline">\(\lambda\)</span>
</td>
</tr>
<tr class="odd">
<td>
<p>Shape as <span class="math inline">\(v \to \infty\)</span></p>
<p>(df <span class="math inline">\(\to \infty\)</span>)</p>
</td>
<td>Normal(0, 1)</td>
<td>Normal(<span class="math inline">\(\lambda\)</span>, 1)</td>
</tr>
<tr class="even">
<td>Applications</td>
<td>Hypothesis testing under null</td>
<td>Power analysis, alternative testing</td>
</tr>
</tbody>
</table></div>
<p>While the Student’s t-distribution is used for standard hypothesis testing and confidence intervals, the non-central t-distribution finds its applications in scenarios involving non-null hypotheses, such as power and sample size calculations.</p>
</div>
<div id="f-distribution" class="section level5" number="2.2.8.3.12">
<h5>
<span class="header-section-number">2.2.8.3.12</span> F Distribution<a class="anchor" aria-label="anchor" href="#f-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The F-distribution, denoted as <span class="math inline">\(F(d_1, d_2)\)</span>, is strictly positive and used to compare variances.</p>
<p><strong>Definition</strong>: <span class="math display">\[
F = \frac{\chi^2_{d_1} / d_1}{\chi^2_{d_2} / d_2},
\]</span> where <span class="math inline">\(\chi^2_{d_1}\)</span> and <span class="math inline">\(\chi^2_{d_2}\)</span> are independent chi-squared random variables with degrees of freedom <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>, respectively.</p>
<p>The distribution is asymmetric and never negative.</p>
<p>The F distribution arises frequently as the null distribution of a test statistic, especially in the context of comparing variances, such as in analysis of variance (ANOVA).</p>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x; d_1, d_2) = \frac{\sqrt{\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}}}{x B\left( \frac{d_1}{2}, \frac{d_2}{2} \right)}, \quad x &gt; 0
\]</span></p>
<p>where <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> are the degrees of freedom and <span class="math inline">\(B\)</span> is the beta function.</p>
<p><strong>CDF</strong></p>
<p>The cumulative distribution function of the F distribution is typically evaluated using numerical methods.</p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Fdist.html">rf</a></span><span class="op">(</span><span class="fl">1000</span>, df1 <span class="op">=</span> <span class="fl">5</span>, df2 <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of F Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-20-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p>The moment generating function (MGF) of the F distribution does not exist in a simple closed form.</p>
<p><strong>Mean</strong></p>
<p>For <span class="math inline">\(d_2 &gt; 2\)</span>:</p>
<p><span class="math display">\[
E[X] = \frac{d_2}{d_2 - 2}
\]</span></p>
<p><strong>Variance</strong></p>
<p>For <span class="math inline">\(d_2 &gt; 4\)</span>:</p>
<p><span class="math display">\[
\sigma^2 = \text{Var}(X) = \frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}
\]</span></p>
<hr>
</div>
<div id="cauchy-distribution" class="section level5" number="2.2.8.3.13">
<h5>
<span class="header-section-number">2.2.8.3.13</span> Cauchy Distribution<a class="anchor" aria-label="anchor" href="#cauchy-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>The Cauchy distribution is a continuous probability distribution that is often used in physics and has heavier tails than the normal distribution. It is notable because it does not have a finite mean or variance.</p>
<ul>
<li>The Cauchy distribution does not have a finite mean or variance.</li>
<li>The <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a> and Weak Law of Large Numbers do not apply to the Cauchy distribution.</li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(x; x_0, \gamma) = \frac{1}{\pi \gamma \left[ 1 + \left( \frac{x - x_0}{\gamma}
\right)^2 \right]}
\]</span></p>
<p>where <span class="math inline">\(x_0\)</span> is the location parameter and <span class="math inline">\(\gamma &gt; 0\)</span> is the scale parameter.</p>
<p><strong>CDF</strong></p>
<p>The cumulative distribution function of the Cauchy distribution is given by:</p>
<p><span class="math display">\[
F(x; x_0, \gamma) = \frac{1}{\pi} \arctan \left( \frac{x - x_0}{\gamma}  \right) + \frac{1}{2}
\]</span></p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Cauchy.html">rcauchy</a></span><span class="op">(</span><span class="fl">1000</span>, location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of Cauchy Distribution"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-21-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p>The MGF of the Cauchy distribution does not exist.</p>
<p><strong>Mean</strong></p>
<p>The mean of the Cauchy distribution is undefined.</p>
<p><strong>Variance</strong></p>
<p>The variance of the Cauchy distribution is undefined.</p>
<hr>
</div>
<div id="multivariate-normal-distribution" class="section level5" number="2.2.8.3.14">
<h5>
<span class="header-section-number">2.2.8.3.14</span> Multivariate Normal Distribution<a class="anchor" aria-label="anchor" href="#multivariate-normal-distribution"><i class="fas fa-link"></i></a>
</h5>
<p>Let <span class="math inline">\(y\)</span> be a <span class="math inline">\(p\)</span>-dimensional multivariate normal (MVN) random variable with mean <span class="math inline">\(\mu\)</span> and variance-covariance matrix <span class="math inline">\(\Sigma\)</span>. The density function of <span class="math inline">\(y\)</span> is given by:</p>
<p><span class="math display">\[ f(\mathbf{y}) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{y}-\mu)' \Sigma^{-1} (\mathbf{y}-\mu)\right) \]</span></p>
<p>where <span class="math inline">\(|\mathbf{\Sigma}|\)</span> represents the determinant of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(\mathbf{y} \sim N_p(\mathbf{\mu}, \mathbf{\Sigma})\)</span>.</p>
<p><strong>Properties</strong>:</p>
<ul>
<li>Let <span class="math inline">\(\mathbf{A}_{r \times p}\)</span> be a fixed matrix. Then <span class="math inline">\(\mathbf{A y} \sim N_r(\mathbf{A \mu}, \mathbf{A \Sigma A'})\)</span>. Note that <span class="math inline">\(r \le p\)</span>, and all rows of <span class="math inline">\(\mathbf{A}\)</span> must be linearly independent to guarantee that <span class="math inline">\(\mathbf{A \Sigma A'}\)</span> is non-singular.</li>
<li>Let <span class="math inline">\(\mathbf{G}\)</span> be a matrix such that <span class="math inline">\(\mathbf{\Sigma^{-1} = G G'}\)</span>. Then <span class="math inline">\(\mathbf{G'y} \sim N_p(\mathbf{G'\mu}, \mathbf{I})\)</span> and <span class="math inline">\(\mathbf{G'(y - \mu)} \sim N_p(\mathbf{0}, \mathbf{I})\)</span>.</li>
<li>Any fixed linear combination of <span class="math inline">\(y_1, \dots, y_p\)</span>, say <span class="math inline">\(\mathbf{c'y}\)</span>, follows <span class="math inline">\(\mathbf{c'y} \sim N_1(\mathbf{c'\mu}, \mathbf{c'\Sigma c})\)</span>.</li>
</ul>
<p><strong>Large Sample Properties</strong></p>
<p>Suppose that <span class="math inline">\(y_1, \dots, y_n\)</span> are a random sample from some population with mean <span class="math inline">\(\mu\)</span> and variance-covariance matrix <span class="math inline">\(\Sigma\)</span>:</p>
<p><span class="math display">\[ \mathbf{Y} \sim MVN(\mathbf{\mu}, \mathbf{\Sigma}) \]</span></p>
<p>Then:</p>
<ul>
<li>
<span class="math inline">\(\bar{\mathbf{y}} = \frac{1}{n} \sum_{i=1}^n \mathbf{y}_i\)</span> is a consistent estimator for <span class="math inline">\(\mathbf{\mu}\)</span>.</li>
<li>
<span class="math inline">\(\mathbf{S} = \frac{1}{n-1} \sum_{i=1}^n (\mathbf{y}_i - \bar{\mathbf{y}})(\mathbf{y}_i - \bar{\mathbf{y}})'\)</span> is a consistent estimator for <span class="math inline">\(\mathbf{\Sigma}\)</span>.</li>
<li>
<strong>Multivariate Central Limit Theorem</strong>: Similar to the univariate case, <span class="math inline">\(\sqrt{n}(\bar{\mathbf{y}} - \mu) \sim N_p(\mathbf{0}, \mathbf{\Sigma})\)</span> when <span class="math inline">\(n\)</span> is large relative to <span class="math inline">\(p\)</span> (e.g., <span class="math inline">\(n \ge 25p\)</span>), which is equivalent to <span class="math inline">\(\bar{\mathbf{y}} \sim N_p(\mathbf{\mu}, \mathbf{\Sigma/n})\)</span>.</li>
<li>
<strong>Wald’s Theorem</strong>: <span class="math inline">\(n(\bar{\mathbf{y}} - \mu)' \mathbf{S^{-1}} (\bar{\mathbf{y}} - \mu) \sim \chi^2_{(p)}\)</span> when <span class="math inline">\(n\)</span> is large relative to <span class="math inline">\(p\)</span>.</li>
</ul>
<p><strong>Density Function</strong></p>
<p><span class="math display">\[
f(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{k/2} | \boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})
\right)
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}\)</span> is the mean vector, <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the covariance matrix, <span class="math inline">\(\mathbf{x} \in \mathbb{R}^k\)</span> and <span class="math inline">\(k\)</span> is the number of variables.</p>
<p><strong>CDF</strong></p>
<p>The cumulative distribution function of the multivariate normal distribution does not have a simple closed form and is typically evaluated using numerical methods.</p>
<p><strong>PDF</strong></p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">2</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="va">k</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span><span class="va">n</span>, mu <span class="op">=</span> <span class="va">mu</span>, Sigma <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,</span>
<span>    main <span class="op">=</span> <span class="st">"Histogram of MVN Distribution (1st Var)"</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Value"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Frequency"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.1-prerequisites_files/figure-html/unnamed-chunk-22-1.png" width="90%" style="display: block; margin: auto;"></div>
<p><strong>MGF</strong></p>
<p><span class="math display">\[
m_{\mathbf{X}}(\mathbf{t}) = \exp\left(\boldsymbol{\mu}^T \mathbf{t} + \frac{1}{2} \mathbf{t}^T \boldsymbol{\Sigma} \mathbf{t}
\right)
\]</span></p>
<p><strong>Mean</strong></p>
<p><span class="math display">\[
E[\mathbf{X}] = \boldsymbol{\mu}
\]</span></p>
<p><strong>Variance</strong></p>
<p><span class="math display">\[
\text{Var}(\mathbf{X}) = \boldsymbol{\Sigma}
\]</span></p>

</div>
</div>
</div>
</div>
<div id="general-math" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> General Math<a class="anchor" aria-label="anchor" href="#general-math"><i class="fas fa-link"></i></a>
</h2>
<div id="number-sets" class="section level3" number="2.3.1">
<h3>
<span class="header-section-number">2.3.1</span> Number Sets<a class="anchor" aria-label="anchor" href="#number-sets"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="20%">
<col width="58%">
</colgroup>
<thead><tr class="header">
<th>Notation</th>
<th>Denotes</th>
<th>Examples</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\emptyset\)</span></td>
<td>Empty set</td>
<td>No members</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbb{N}\)</span></td>
<td>Natural numbers</td>
<td><span class="math inline">\(\{1, 2, \ldots\}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbb{Z}\)</span></td>
<td>Integers</td>
<td><span class="math inline">\(\{\ldots, -1, 0, 1, \ldots\}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbb{Q}\)</span></td>
<td>Rational numbers</td>
<td>Including fractions</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbb{R}\)</span></td>
<td>Real numbers</td>
<td>Including all finite decimals, irrational numbers</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbb{C}\)</span></td>
<td>Complex numbers</td>
<td>Including numbers of the form <span class="math inline">\(a + bi\)</span> where <span class="math inline">\(i^2 = -1\)</span>
</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="summation-notation-and-series" class="section level3" number="2.3.2">
<h3>
<span class="header-section-number">2.3.2</span> Summation Notation and Series<a class="anchor" aria-label="anchor" href="#summation-notation-and-series"><i class="fas fa-link"></i></a>
</h3>
<div id="chebyshevs-inequality" class="section level4" number="2.3.2.1">
<h4>
<span class="header-section-number">2.3.2.1</span> Chebyshev’s Inequality<a class="anchor" aria-label="anchor" href="#chebyshevs-inequality"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\(X\)</span> be a random variable with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. For any positive number <span class="math inline">\(k\)</span>, Chebyshev’s Inequality states:</p>
<p><span class="math display">\[
P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}
\]</span></p>
<p>This provides a probabilistic bound on the deviation of <span class="math inline">\(X\)</span> from its mean and does not require <span class="math inline">\(X\)</span> to follow a normal distribution.</p>
<hr>
</div>
<div id="geometric-sum" class="section level4" number="2.3.2.2">
<h4>
<span class="header-section-number">2.3.2.2</span> Geometric Sum<a class="anchor" aria-label="anchor" href="#geometric-sum"><i class="fas fa-link"></i></a>
</h4>
<p>For a geometric series of the form <span class="math inline">\(\sum_{k=0}^{n-1} ar^k\)</span>, the sum is given by:</p>
<p><span class="math display">\[
\sum_{k=0}^{n-1} ar^k = a\frac{1-r^n}{1-r} \quad \text{where } r \neq 1
\]</span></p>
</div>
<div id="infinite-geometric-series" class="section level4" number="2.3.2.3">
<h4>
<span class="header-section-number">2.3.2.3</span> Infinite Geometric Series<a class="anchor" aria-label="anchor" href="#infinite-geometric-series"><i class="fas fa-link"></i></a>
</h4>
<p>When <span class="math inline">\(|r| &lt; 1\)</span>, the geometric series converges to:</p>
<p><span class="math display">\[
\sum_{k=0}^\infty ar^k = \frac{a}{1-r}
\]</span></p>
<hr>
</div>
<div id="binomial-theorem" class="section level4" number="2.3.2.4">
<h4>
<span class="header-section-number">2.3.2.4</span> Binomial Theorem<a class="anchor" aria-label="anchor" href="#binomial-theorem"><i class="fas fa-link"></i></a>
</h4>
<p>The binomial expansion for <span class="math inline">\((x + y)^n\)</span> is:</p>
<p><span class="math display">\[
(x + y)^n = \sum_{k=0}^n \binom{n}{k} x^{n-k} y^k \quad \text{where } n \geq 0
\]</span></p>
</div>
<div id="binomial-series" class="section level4" number="2.3.2.5">
<h4>
<span class="header-section-number">2.3.2.5</span> Binomial Series<a class="anchor" aria-label="anchor" href="#binomial-series"><i class="fas fa-link"></i></a>
</h4>
<p>For non-integer exponents <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[
\sum_{k=0}^\infty \binom{\alpha}{k} x^k = (1 + x)^\alpha \quad \text{where } |x| &lt; 1
\]</span></p>
<hr>
</div>
<div id="telescoping-sum" class="section level4" number="2.3.2.6">
<h4>
<span class="header-section-number">2.3.2.6</span> Telescoping Sum<a class="anchor" aria-label="anchor" href="#telescoping-sum"><i class="fas fa-link"></i></a>
</h4>
<p>A telescoping sum simplifies as intermediate terms cancel, leaving:</p>
<p><span class="math display">\[
\sum_{a \leq k &lt; b} \Delta F(k) = F(b) - F(a) \quad \text{where } a, b \in \mathbb{Z}, a \leq b
\]</span></p>
<hr>
</div>
<div id="vandermonde-convolution" class="section level4" number="2.3.2.7">
<h4>
<span class="header-section-number">2.3.2.7</span> Vandermonde Convolution<a class="anchor" aria-label="anchor" href="#vandermonde-convolution"><i class="fas fa-link"></i></a>
</h4>
<p>The Vandermonde convolution identity is:</p>
<p><span class="math display">\[
\sum_{k=0}^n \binom{r}{k} \binom{s}{n-k} = \binom{r+s}{n} \quad \text{where } n \in \mathbb{Z}
\]</span></p>
<hr>
</div>
<div id="exponential-series" class="section level4" number="2.3.2.8">
<h4>
<span class="header-section-number">2.3.2.8</span> Exponential Series<a class="anchor" aria-label="anchor" href="#exponential-series"><i class="fas fa-link"></i></a>
</h4>
<p>The exponential function <span class="math inline">\(e^x\)</span> can be represented as:</p>
<p><span class="math display">\[
\sum_{k=0}^\infty \frac{x^k}{k!} = e^x \quad \text{where } x \in \mathbb{C}
\]</span></p>
<hr>
</div>
<div id="taylor-series" class="section level4" number="2.3.2.9">
<h4>
<span class="header-section-number">2.3.2.9</span> Taylor Series<a class="anchor" aria-label="anchor" href="#taylor-series"><i class="fas fa-link"></i></a>
</h4>
<p>The Taylor series expansion for a function <span class="math inline">\(f(x)\)</span> about <span class="math inline">\(x=a\)</span> is:</p>
<p><span class="math display">\[
\sum_{k=0}^\infty \frac{f^{(k)}(a)}{k!} (x-a)^k = f(x)
\]</span></p>
<p>For <span class="math inline">\(a = 0\)</span>, this becomes the <strong>Maclaurin series</strong>.</p>
<hr>
</div>
<div id="maclaurin-series-for-ez" class="section level4" number="2.3.2.10">
<h4>
<span class="header-section-number">2.3.2.10</span> Maclaurin Series for <span class="math inline">\(e^z\)</span><a class="anchor" aria-label="anchor" href="#maclaurin-series-for-ez"><i class="fas fa-link"></i></a>
</h4>
<p>A special case of the Taylor series, the Maclaurin expansion for <span class="math inline">\(e^z\)</span> is:</p>
<p><span class="math display">\[
e^z = 1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} + \cdots
\]</span></p>
<hr>
</div>
<div id="eulers-summation-formula" class="section level4" number="2.3.2.11">
<h4>
<span class="header-section-number">2.3.2.11</span> Euler’s Summation Formula<a class="anchor" aria-label="anchor" href="#eulers-summation-formula"><i class="fas fa-link"></i></a>
</h4>
<p>Euler’s summation formula connects sums and integrals:</p>
<p><span class="math display">\[
\sum_{a \leq k &lt; b} f(k) = \int_a^b f(x) \, dx + \sum_{k=1}^m \frac{B_k}{k!} \left[f^{(k-1)}(x)\right]_a^b
+ (-1)^{m+1} \int_a^b \frac{B_m(x-\lfloor x \rfloor)}{m!} f^{(m)}(x) \, dx
\]</span></p>
<p>Here, <span class="math inline">\(B_k\)</span> are Bernoulli numbers.</p>
<ul>
<li>
<strong>For</strong> <span class="math inline">\(m=1\)</span> (Trapezoidal Rule):</li>
</ul>
<p><span class="math display">\[
\sum_{a \leq k &lt; b} f(k) \approx \int_a^b f(x) \, dx - \frac{1}{2}(f(b) - f(a))
\]</span></p>
</div>
</div>
<div id="taylor-expansion" class="section level3" number="2.3.3">
<h3>
<span class="header-section-number">2.3.3</span> Taylor Expansion<a class="anchor" aria-label="anchor" href="#taylor-expansion"><i class="fas fa-link"></i></a>
</h3>
<p>A differentiable function, <span class="math inline">\(G(x)\)</span>, can be written as an infinite sum of its derivatives. More specifically, if <span class="math inline">\(G(x)\)</span> is infinitely differentiable and evaluated at <span class="math inline">\(a\)</span>, its Taylor expansion is:</p>
<p><span class="math display">\[
G(x) = G(a) + \frac{G'(a)}{1!} (x-a) + \frac{G''(a)}{2!}(x-a)^2 + \frac{G'''(a)}{3!}(x-a)^3 + \dots
\]</span></p>
<p>This expansion is valid within the radius of convergence.</p>
<hr>
</div>
<div id="law-of-large-numbers" class="section level3" number="2.3.4">
<h3>
<span class="header-section-number">2.3.4</span> Law of Large Numbers<a class="anchor" aria-label="anchor" href="#law-of-large-numbers"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(X_1, X_2, \ldots\)</span> be an infinite sequence of independent and identically distributed (i.i.d.) random variables with finite mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The <strong>Law of Large Numbers (LLN)</strong> states that the sample average:</p>
<p><span class="math display">\[
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>converges to the expected value <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. This can be expressed as:</p>
<p><span class="math display">\[
\bar{X}_n \rightarrow \mu \quad \text{(as $n \rightarrow \infty$)}.
\]</span></p>
<div id="variance-of-the-sample-mean" class="section level4" number="2.3.4.1">
<h4>
<span class="header-section-number">2.3.4.1</span> Variance of the Sample Mean<a class="anchor" aria-label="anchor" href="#variance-of-the-sample-mean"><i class="fas fa-link"></i></a>
</h4>
<p>The variance of the sample mean decreases as the sample size increases:</p>
<p><span class="math display">\[
Var(\bar{X}_n) = Var\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{\sigma^2}{n}.
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
Var(\bar{X}_n) &amp;= Var(\frac{1}{n}(X_1 + ... + X_n)) =Var\left(\frac{1}{n} \sum_{i=1}^n X_i\right) \\
&amp;= \frac{1}{n^2}Var(X_1 + ... + X_n) \\
&amp;=\frac{n\sigma^2}{n^2}=\frac{\sigma^2}{n}
\end{aligned}
\]</span></p>
<p><strong>Note</strong>: The connection between the <a href="prerequisites.html#law-of-large-numbers">Law of Large Numbers</a> and the <a href="prerequisites.html#normal-distribution">Normal Distribution</a> lies in the <a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a>. The CLT states that, regardless of the original distribution of a dataset, the distribution of the sample means will tend to follow a normal distribution as the sample size becomes larger.</p>
<p>The difference between [Weak Law] and [Strong Law] regards the mode of convergence.</p>
<hr>
</div>
<div id="weak-law-of-large-numbers" class="section level4" number="2.3.4.2">
<h4>
<span class="header-section-number">2.3.4.2</span> Weak Law of Large Numbers<a class="anchor" aria-label="anchor" href="#weak-law-of-large-numbers"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Weak Law of Large Numbers</strong> states that the sample average converges in probability to the expected value:</p>
<p><span class="math display">\[
\bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \rightarrow \infty.
\]</span></p>
<p>Formally, for any <span class="math inline">\(\epsilon &gt; 0\)</span>:</p>
<p><span class="math display">\[
\lim_{n \to \infty} P(|\bar{X}_n - \mu| &gt; \epsilon) = 0.
\]</span></p>
<p>Additionally, the sample mean of an i.i.d. random sample (<span class="math inline">\(\{ X_i \}_{i=1}^n\)</span>) from any population with a finite mean and variance is a consistent estimator of the population mean <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
plim(\bar{X}_n) = plim\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \mu.
\]</span></p>
<hr>
</div>
<div id="strong-law-of-large-numbers" class="section level4" number="2.3.4.3">
<h4>
<span class="header-section-number">2.3.4.3</span> Strong Law of Large Numbers<a class="anchor" aria-label="anchor" href="#strong-law-of-large-numbers"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>Strong Law of Large Numbers</strong> states that the sample average converges almost surely to the expected value:</p>
<p><span class="math display">\[
\bar{X}_n \xrightarrow{a.s.} \mu \quad \text{as } n \rightarrow \infty.
\]</span></p>
<p>Equivalently, this can be expressed as:</p>
<p><span class="math display">\[
P\left(\lim_{n \to \infty} \bar{X}_n = \mu\right) = 1.
\]</span></p>
<hr>
</div>
</div>
<div id="convergence" class="section level3" number="2.3.5">
<h3>
<span class="header-section-number">2.3.5</span> Convergence<a class="anchor" aria-label="anchor" href="#convergence"><i class="fas fa-link"></i></a>
</h3>
<div id="convergence-in-probability" class="section level4" number="2.3.5.1">
<h4>
<span class="header-section-number">2.3.5.1</span> Convergence in Probability<a class="anchor" aria-label="anchor" href="#convergence-in-probability"><i class="fas fa-link"></i></a>
</h4>
<p>As <span class="math inline">\(n \rightarrow \infty\)</span>, an estimator (random variable) <span class="math inline">\(\theta_n\)</span> is said to converge in probability to a constant <span class="math inline">\(c\)</span> if:</p>
<p><span class="math display">\[
\lim_{n \to \infty} P(|\theta_n - c| \geq \epsilon) = 0 \quad \text{for any } \epsilon &gt; 0.
\]</span></p>
<p>This is denoted as:</p>
<p><span class="math display">\[
plim(\theta_n) = c \quad \text{or equivalently, } \theta_n \xrightarrow{p} c.
\]</span></p>
<hr>
<p><strong>Properties of Convergence in Probability:</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Slutsky’s Theorem</strong>: For a continuous function <span class="math inline">\(g(\cdot)\)</span>, if <span class="math inline">\(plim(\theta_n) = \theta\)</span>, then:</p>
<p><span class="math display">\[
plim(g(\theta_n)) = g(\theta)
\]</span></p>
</li>
<li>
<p>If <span class="math inline">\(\gamma_n \xrightarrow{p} \gamma\)</span>, then:</p>
<ul>
<li>
<span class="math inline">\(plim(\theta_n + \gamma_n) = \theta + \gamma\)</span>,</li>
<li>
<span class="math inline">\(plim(\theta_n \gamma_n) = \theta \gamma\)</span>,</li>
<li>
<span class="math inline">\(plim(\theta_n / \gamma_n) = \theta / \gamma\)</span> (if <span class="math inline">\(\gamma \neq 0\)</span>).</li>
</ul>
</li>
<li><p>These properties extend to random vectors and matrices.</p></li>
</ol>
<hr>
</div>
<div id="convergence-in-distribution" class="section level4" number="2.3.5.2">
<h4>
<span class="header-section-number">2.3.5.2</span> Convergence in Distribution<a class="anchor" aria-label="anchor" href="#convergence-in-distribution"><i class="fas fa-link"></i></a>
</h4>
<p>As <span class="math inline">\(n \rightarrow \infty\)</span>, the distribution of a random variable <span class="math inline">\(X_n\)</span> may converge to another (“fixed”) distribution. Formally, <span class="math inline">\(X_n\)</span> with CDF <span class="math inline">\(F_n(x)\)</span> converges in distribution to <span class="math inline">\(X\)</span> with CDF <span class="math inline">\(F(x)\)</span> if:</p>
<p><span class="math display">\[
\lim_{n \to \infty} |F_n(x) - F(x)| = 0
\]</span></p>
<p>at all points of continuity of <span class="math inline">\(F(x)\)</span>. This is denoted as:</p>
<p><span class="math display">\[
X_n \xrightarrow{d} X \quad \text{or equivalently, } F(x) \text{ is the limiting distribution of } X_n.
\]</span></p>
<p><strong>Asymptotic Properties:</strong></p>
<ul>
<li>
<span class="math inline">\(E(X)\)</span>: Limiting mean (asymptotic mean).</li>
<li>
<span class="math inline">\(Var(X)\)</span>: Limiting variance (asymptotic variance).</li>
</ul>
<p><strong>Note:</strong> Limiting expectations and variances do not necessarily match the expectations and variances of <span class="math inline">\(X_n\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E(X) &amp;\neq \lim_{n \to \infty} E(X_n), \\
Avar(X_n) &amp;\neq \lim_{n \to \infty} Var(X_n).
\end{aligned}
\]</span></p>
<hr>
<p><strong>Properties of Convergence in Distribution:</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Continuous Mapping Theorem</strong>: For a continuous function <span class="math inline">\(g(\cdot)\)</span>, if <span class="math inline">\(X_n \xrightarrow{d} X\)</span>, then:</p>
<p><span class="math display">\[
g(X_n) \xrightarrow{d} g(X).
\]</span></p>
</li>
<li>
<p>If <span class="math inline">\(Y_n \xrightarrow{d} c\)</span> (a constant), then:</p>
<ul>
<li>
<span class="math inline">\(X_n + Y_n \xrightarrow{d} X + c\)</span>,</li>
<li>
<span class="math inline">\(Y_n X_n \xrightarrow{d} c X\)</span>,</li>
<li>
<span class="math inline">\(X_n / Y_n \xrightarrow{d} X / c\)</span> (if <span class="math inline">\(c \neq 0\)</span>).</li>
</ul>
</li>
<li><p>These properties also extend to random vectors and matrices.</p></li>
</ol>
<hr>
</div>
<div id="summary-properties-of-convergence" class="section level4" number="2.3.5.3">
<h4>
<span class="header-section-number">2.3.5.3</span> Summary: Properties of Convergence<a class="anchor" aria-label="anchor" href="#summary-properties-of-convergence"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="48%">
<col width="51%">
</colgroup>
<thead><tr class="header">
<th>Convergence in Probability</th>
<th>Convergence in Distribution</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Slutsky’s Theorem: For a continuous <span class="math inline">\(g(\cdot)\)</span>, if <span class="math inline">\(plim(\theta_n) = \theta\)</span>, then <span class="math inline">\(plim(g(\theta_n)) = g(\theta)\)</span>
</td>
<td>Continuous Mapping Theorem: For a continuous <span class="math inline">\(g(\cdot)\)</span>, if <span class="math inline">\(X_n \xrightarrow{d} X\)</span>, then <span class="math inline">\(g(X_n) \xrightarrow{d} g(X)\)</span>
</td>
</tr>
<tr class="even">
<td>If <span class="math inline">\(\gamma_n \xrightarrow{p} \gamma\)</span>, then:</td>
<td>If <span class="math inline">\(Y_n \xrightarrow{d} c\)</span>, then:</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(plim(\theta_n + \gamma_n) = \theta + \gamma\)</span></td>
<td><span class="math inline">\(X_n + Y_n \xrightarrow{d} X + c\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(plim(\theta_n \gamma_n) = \theta \gamma\)</span></td>
<td><span class="math inline">\(Y_n X_n \xrightarrow{d} c X\)</span></td>
</tr>
<tr class="odd">
<td>
<span class="math inline">\(plim(\theta_n / \gamma_n) = \theta / \gamma\)</span> (if <span class="math inline">\(\gamma \neq 0\)</span>)</td>
<td>
<span class="math inline">\(X_n / Y_n \xrightarrow{d} X / c\)</span> (if <span class="math inline">\(c \neq 0\)</span>)</td>
</tr>
</tbody>
</table></div>
<p><strong>Relationship between Convergence Types:</strong></p>
<p><a href="prerequisites.html#convergence-in-probability">Convergence in Probability</a> is stronger than <a href="prerequisites.html#convergence-in-distribution">Convergence in Distribution</a>. Therefore:</p>
<ul>
<li>
<a href="prerequisites.html#convergence-in-distribution">Convergence in Distribution</a> does not guarantee <a href="prerequisites.html#convergence-in-probability">Convergence in Probability</a>.</li>
</ul>
</div>
</div>
<div id="sufficient-statistics-and-likelihood" class="section level3" number="2.3.6">
<h3>
<span class="header-section-number">2.3.6</span> Sufficient Statistics and Likelihood<a class="anchor" aria-label="anchor" href="#sufficient-statistics-and-likelihood"><i class="fas fa-link"></i></a>
</h3>
<div id="likelihood" class="section level4" number="2.3.6.1">
<h4>
<span class="header-section-number">2.3.6.1</span> Likelihood<a class="anchor" aria-label="anchor" href="#likelihood"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>likelihood</strong> describes the degree to which the observed data supports a particular value of a parameter <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>The exact value of the likelihood is <strong>not meaningful</strong>; only relative comparisons matter.</li>
<li>Likelihood is <strong>informative</strong> when comparing parameter values, helping identify which values of <span class="math inline">\(\theta\)</span> are more plausible given the data.</li>
</ul>
<p>For a single observation <span class="math inline">\(Y = y\)</span>, the likelihood function is defined as:</p>
<p><span class="math display">\[
L(\theta_0; y) = P(Y = y \mid \theta = \theta_0) = f_Y(y; \theta_0),
\]</span></p>
<p>where <span class="math inline">\(f_Y(y; \theta_0)\)</span> is the probability density (or mass) function of <span class="math inline">\(Y\)</span> for the parameter <span class="math inline">\(\theta_0\)</span>.</p>
<p><strong>Key Insight</strong>: The likelihood tells us how plausible <span class="math inline">\(\theta\)</span> is, given the data we observed. It is <strong>not a probability</strong>, but it is proportional to the probability of observing the data under a given parameter value.</p>
<p><strong>Example</strong>: Suppose <span class="math inline">\(Y\)</span> follows a binomial distribution with <span class="math inline">\(n=10\)</span> trials and probability of success <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
P(Y = y \mid p) = \binom{10}{y} p^y (1-p)^{10-y}.
\]</span></p>
<p>For <span class="math inline">\(y=7\)</span> observed successes, the likelihood function becomes:</p>
<p><span class="math display">\[
L(p; y=7) = \binom{10}{7} p^7 (1-p)^3.
\]</span></p>
<p>We can use this to compare how well different values of <span class="math inline">\(p\)</span> explain the observed data.</p>
<hr>
</div>
<div id="likelihood-ratio" class="section level4" number="2.3.6.2">
<h4>
<span class="header-section-number">2.3.6.2</span> Likelihood Ratio<a class="anchor" aria-label="anchor" href="#likelihood-ratio"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>likelihood ratio</strong> compares the relative likelihood of two parameter values <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> given the observed data:</p>
<p><span class="math display">\[
\text{Likelihood Ratio} = \frac{L(\theta_0; y)}{L(\theta_1; y)}.
\]</span></p>
<ul>
<li>A likelihood ratio greater than 1 implies that <span class="math inline">\(\theta_0\)</span> is more likely than <span class="math inline">\(\theta_1\)</span>, given the observed data.</li>
<li>Likelihood ratios are widely used in hypothesis testing and model comparison to evaluate the evidence against a null hypothesis.</li>
</ul>
<p><strong>Example</strong>: For the binomial example above, consider <span class="math inline">\(p_0 = 0.7\)</span> and <span class="math inline">\(p_1 = 0.5\)</span>. The likelihood ratio is:</p>
<p><span class="math display">\[
\frac{L(p_0; y=7)}{L(p_1; y=7)} = \frac{\binom{10}{7} (0.7)^7 (0.3)^3}{\binom{10}{7} (0.5)^7 (0.5)^3}.
\]</span></p>
<p>This simplifies to:</p>
<p><span class="math display">\[
\frac{(0.7)^7 (0.3)^3}{(0.5)^7 (0.5)^3}.
\]</span></p>
<p>The likelihood ratio quantifies how much more likely <span class="math inline">\(p_0\)</span> is compared to <span class="math inline">\(p_1\)</span> given the observed data.</p>
<hr>
</div>
<div id="likelihood-function" class="section level4" number="2.3.6.3">
<h4>
<span class="header-section-number">2.3.6.3</span> Likelihood Function<a class="anchor" aria-label="anchor" href="#likelihood-function"><i class="fas fa-link"></i></a>
</h4>
<p>For a given sample, the likelihood for all possible values of <span class="math inline">\(\theta\)</span> forms the <strong>likelihood function</strong>:</p>
<p><span class="math display">\[
L(\theta) = L(\theta; y) = f_Y(y; \theta).
\]</span></p>
<p>For a sample of size <span class="math inline">\(n\)</span>, assuming independence among observations:</p>
<p><span class="math display">\[
L(\theta) = \prod_{i=1}^{n} f_Y(y_i; \theta).
\]</span></p>
<p>Taking the natural logarithm of the likelihood gives the <strong>log-likelihood function</strong>:</p>
<p><span class="math display">\[
l(\theta) = \sum_{i=1}^{n} \log f_Y(y_i; \theta).
\]</span></p>
<p><strong>Why Log-Likelihood?</strong></p>
<ul>
<li>The log-likelihood simplifies computation by turning products into sums.</li>
<li>It is particularly useful for optimization, as many numerical methods (e.g., gradient-based algorithms) perform better with sums than products.</li>
</ul>
<p><strong>Example</strong>: For <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span> i.i.d. observations from a normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>, the likelihood is:</p>
<p><span class="math display">\[
L(\mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right).
\]</span></p>
<p>The log-likelihood is:</p>
<p><span class="math display">\[
l(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2.
\]</span></p>
<hr>
</div>
<div id="sufficient-statistics" class="section level4" number="2.3.6.4">
<h4>
<span class="header-section-number">2.3.6.4</span> Sufficient Statistics<a class="anchor" aria-label="anchor" href="#sufficient-statistics"><i class="fas fa-link"></i></a>
</h4>
<p>A <strong>sufficient statistic</strong> <span class="math inline">\(T(y)\)</span> is a summary of the data that retains all information about a parameter <span class="math inline">\(\theta\)</span>. It allows us to focus on this condensed statistic without losing any inferential power regarding <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Formal Definition:</strong></p>
<p>A statistic <span class="math inline">\(T(y)\)</span> is sufficient for a parameter <span class="math inline">\(\theta\)</span> if the conditional probability distribution of the data <span class="math inline">\(y\)</span>, given <span class="math inline">\(T(y)\)</span> and <span class="math inline">\(\theta\)</span>, does not depend on <span class="math inline">\(\theta\)</span>. Mathematically:</p>
<p><span class="math display">\[ P(Y = y \mid T(y), \theta) = P(Y = y \mid T(y)). \]</span></p>
<p>Alternatively, by the <strong>Factorization Theorem</strong>, <span class="math inline">\(T(y)\)</span> is sufficient if the likelihood can be written as:</p>
<p><span class="math display">\[ L(\theta; y) = c(y) L^*(\theta; T(y)), \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(c(y)\)</span> is a function of the data independent of <span class="math inline">\(\theta\)</span>.</li>
<li>
<span class="math inline">\(L^*(\theta; T(y))\)</span> is a function that depends on <span class="math inline">\(\theta\)</span> and <span class="math inline">\(T(y)\)</span>.</li>
</ul>
<p>In other words, the likelihood function can be rewritten in terms of <span class="math inline">\(T(y)\)</span> alone, without loss of information about <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Why Sufficient Statistics Matter:</strong></p>
<ul>
<li>They allow us to simplify the analysis by reducing the data without losing inferential power.</li>
<li>Many inferential procedures (e.g., Maximum Likelihood Estimation, Bayesian methods) are simplified by working with sufficient statistics.</li>
</ul>
<p><strong>Example:</strong></p>
<p>Consider a sample of i.i.d. observations <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span> from a normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>. Here:</p>
<ol style="list-style-type: decimal">
<li>The sample mean <span class="math inline">\(\bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i\)</span> is sufficient for <span class="math inline">\(\mu\)</span>.</li>
<li>The sample variance <span class="math inline">\(S^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \bar{Y})^2\)</span> is sufficient for <span class="math inline">\(\sigma^2\)</span>.</li>
</ol>
<p><strong>Verification:</strong> The joint density of <span class="math inline">\(y_1, y_2, \dots, y_n\)</span> can be factored as:</p>
<p><span class="math display">\[
f(y_1, \dots, y_n; \mu, \sigma^2) = \underbrace{\frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \bar{y})^2\right)}_{L^*(\mu, \sigma^2; \bar{y}, s^2)}
\cdot \underbrace{\text{[independent of $\mu$, $\sigma^2$]}}_{c(y)}.
\]</span></p>
<p>This shows <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(S^2\)</span> are sufficient.</p>
<hr>
<p><strong>Usage of Sufficient Statistics</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Maximum Likelihood Estimation (MLE):</strong> In MLE, sufficient statistics simplify the optimization problem by reducing the data without losing information.</p>
<p>Example: In the normal distribution case, <span class="math inline">\(\mu\)</span> can be estimated using the sufficient statistic <span class="math inline">\(\bar{Y}\)</span>: <span class="math display">\[
\hat{\mu}_{MLE} = \bar{Y}.
\]</span></p>
</li>
<li><p><strong>Bayesian Inference:</strong> In Bayesian analysis, the posterior distribution depends on the sufficient statistic rather than the entire data set. For the normal case: <span class="math display">\[
P(\mu \mid \bar{Y}) \propto P(\mu) L(\mu; \bar{Y}).
\]</span></p></li>
<li><p><strong>Data Compression:</strong> In practice, sufficient statistics reduce the complexity of data storage and analysis by condensing all relevant information into a smaller representation.</p></li>
</ol>
<hr>
</div>
<div id="nuisance-parameters" class="section level4" number="2.3.6.5">
<h4>
<span class="header-section-number">2.3.6.5</span> Nuisance Parameters<a class="anchor" aria-label="anchor" href="#nuisance-parameters"><i class="fas fa-link"></i></a>
</h4>
<p>Parameters that are not of direct interest in the analysis but are necessary to model the data are called <strong>nuisance parameters</strong>.</p>
<p><strong>Profile Likelihood</strong>: To handle nuisance parameters, replace them with their maximum likelihood estimates (MLEs) in the likelihood function, creating a <strong>profile likelihood</strong> for the parameter of interest.</p>
<p><strong>Example of Profile Likelihood</strong>:</p>
<p>In a regression model with parameters <span class="math inline">\(\beta\)</span> (coefficients) and <span class="math inline">\(\sigma^2\)</span> (error variance), <span class="math inline">\(\sigma^2\)</span> is often a nuisance parameter. The profile likelihood for <span class="math inline">\(\beta\)</span> is obtained by substituting the MLE of <span class="math inline">\(\sigma^2\)</span> into the likelihood:</p>
<p><span class="math display">\[
L_p(\beta) = L(\beta, \hat{\sigma}^2),
\]</span></p>
<p>where <span class="math inline">\(\hat{\sigma}^2\)</span> is the MLE of <span class="math inline">\(\sigma^2\)</span> given <span class="math inline">\(\beta\)</span>.</p>
<p>This simplifies the problem to focus only on the parameter of interest, <span class="math inline">\(\beta\)</span>.</p>
<hr>
</div>
</div>
<div id="parameter-transformations" class="section level3" number="2.3.7">
<h3>
<span class="header-section-number">2.3.7</span> Parameter Transformations<a class="anchor" aria-label="anchor" href="#parameter-transformations"><i class="fas fa-link"></i></a>
</h3>
<p>Transformations of parameters are often used to improve interpretability or statistical properties of models.</p>
<div id="log-odds-transformation" class="section level4" number="2.3.7.1">
<h4>
<span class="header-section-number">2.3.7.1</span> Log-Odds Transformation<a class="anchor" aria-label="anchor" href="#log-odds-transformation"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>log-odds transformation</strong> is commonly used in logistic regression and binary classification problems. It transforms probabilities (which are bounded between 0 and 1) to the real line:</p>
<p><span class="math display">\[
\text{Log odds} = g(\theta) = \ln\left(\frac{\theta}{1-\theta}\right),
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> represents a probability (e.g., the success probability in a Bernoulli trial).</p>
<hr>
</div>
<div id="general-parameter-transformations" class="section level4" number="2.3.7.2">
<h4>
<span class="header-section-number">2.3.7.2</span> General Parameter Transformations<a class="anchor" aria-label="anchor" href="#general-parameter-transformations"><i class="fas fa-link"></i></a>
</h4>
<p>For a parameter <span class="math inline">\(\theta\)</span> and a transformation <span class="math inline">\(g(\cdot)\)</span>:</p>
<ul>
<li>If <span class="math inline">\(\theta \in (a, b)\)</span>, <span class="math inline">\(g(\theta)\)</span> may map <span class="math inline">\(\theta\)</span> to a different range (e.g., <span class="math inline">\(\mathbb{R}\)</span>).</li>
<li>Useful transformations include:
<ul>
<li>Logarithmic: <span class="math inline">\(g(\theta) = \ln(\theta)\)</span> for <span class="math inline">\(\theta &gt; 0\)</span>.</li>
<li>Exponential: <span class="math inline">\(g(\theta) = e^{\theta}\)</span> for unconstrained <span class="math inline">\(\theta\)</span>.</li>
<li>Square root: <span class="math inline">\(g(\theta) = \sqrt{\theta}\)</span> for <span class="math inline">\(\theta \geq 0\)</span>.</li>
</ul>
</li>
</ul>
<p><strong>Jacobian Adjustment for Transformations</strong>: If transforming a parameter in Bayesian inference, the Jacobian of the transformation must be included to ensure proper posterior scaling.</p>
<hr>
</div>
<div id="applications-of-parameter-transformations" class="section level4" number="2.3.7.3">
<h4>
<span class="header-section-number">2.3.7.3</span> Applications of Parameter Transformations<a class="anchor" aria-label="anchor" href="#applications-of-parameter-transformations"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Improving Interpretability</strong>:
<ul>
<li>Probabilities can be transformed to odds or log-odds for logistic models.</li>
<li>Rates can be transformed logarithmically for multiplicative effects.</li>
</ul>
</li>
<li>
<strong>Statistical Modeling</strong>:
<ul>
<li>Variance-stabilizing transformations (e.g., log for Poisson data or arcsine for proportions).</li>
<li>Regularization or simplification of complex relationships.</li>
</ul>
</li>
<li>
<strong>Optimization</strong>:
<ul>
<li>Transforming constrained parameters (e.g., probabilities or positive scales) to unconstrained scales simplifies optimization algorithms.</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
<div id="data-importexport" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Data Import/Export<a class="anchor" aria-label="anchor" href="#data-importexport"><i class="fas fa-link"></i></a>
</h2>
<p><a href="https://cran.r-project.org/doc/manuals/r-release/R-data.html">Extended Manual by R</a></p>
<div class="inline-table"><table class="table table-sm">
<caption>Table by <a href="https://cran.r-project.org/web/packages/rio/vignettes/rio.html">Rio Vignette</a>
</caption>
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th align="left">Format</th>
<th align="left">Typical Extension</th>
<th align="left">Import Package</th>
<th align="left">Export Package</th>
<th align="left">Installed by Default</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Comma-separated data</td>
<td align="left">.csv</td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Pipe-separated data</td>
<td align="left">.psv</td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Tab-separated data</td>
<td align="left">.tsv</td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">CSVY (CSV + YAML metadata header)</td>
<td align="left">.csvy</td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=data.table"><strong>data.table</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">SAS</td>
<td align="left">.sas7bdat</td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">SPSS</td>
<td align="left">.sav</td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">SPSS (compressed)</td>
<td align="left">.zsav</td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Stata</td>
<td align="left">.dta</td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">SAS XPORT</td>
<td align="left">.xpt</td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">SPSS Portable</td>
<td align="left">.por</td>
<td align="left"><a href="https://cran.r-project.org/package=haven"><strong>haven</strong></a></td>
<td align="left"></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Excel</td>
<td align="left">.xls</td>
<td align="left"><a href="https://cran.r-project.org/package=readxl"><strong>readxl</strong></a></td>
<td align="left"></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Excel</td>
<td align="left">.xlsx</td>
<td align="left"><a href="https://cran.r-project.org/package=readxl"><strong>readxl</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=openxlsx"><strong>openxlsx</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">R syntax</td>
<td align="left">.R</td>
<td align="left"><strong>base</strong></td>
<td align="left"><strong>base</strong></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Saved R objects</td>
<td align="left">.RData, .rda</td>
<td align="left"><strong>base</strong></td>
<td align="left"><strong>base</strong></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Serialized R objects</td>
<td align="left">.rds</td>
<td align="left"><strong>base</strong></td>
<td align="left"><strong>base</strong></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Epiinfo</td>
<td align="left">.rec</td>
<td align="left"><a href="https://cran.r-project.org/package=foreign"><strong>foreign</strong></a></td>
<td align="left"></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Minitab</td>
<td align="left">.mtp</td>
<td align="left"><a href="https://cran.r-project.org/package=foreign"><strong>foreign</strong></a></td>
<td align="left"></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Systat</td>
<td align="left">.syd</td>
<td align="left"><a href="https://cran.r-project.org/package=foreign"><strong>foreign</strong></a></td>
<td align="left"></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">“XBASE” database files</td>
<td align="left">.dbf</td>
<td align="left"><a href="https://cran.r-project.org/package=foreign"><strong>foreign</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=foreign"><strong>foreign</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Weka Attribute-Relation File Format</td>
<td align="left">.arff</td>
<td align="left"><a href="https://cran.r-project.org/package=foreign"><strong>foreign</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=foreign"><strong>foreign</strong></a></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Data Interchange Format</td>
<td align="left">.dif</td>
<td align="left"><strong>utils</strong></td>
<td align="left"></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Fortran data</td>
<td align="left">no recognized extension</td>
<td align="left"><strong>utils</strong></td>
<td align="left"></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Fixed-width format data</td>
<td align="left">.fwf</td>
<td align="left"><strong>utils</strong></td>
<td align="left"><strong>utils</strong></td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">gzip comma-separated data</td>
<td align="left">.csv.gz</td>
<td align="left"><strong>utils</strong></td>
<td align="left"><strong>utils</strong></td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Apache Arrow (Parquet)</td>
<td align="left">.parquet</td>
<td align="left"><a href="https://cran.r-project.org/package=arrow"><strong>arrow</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=arrow"><strong>arrow</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">EViews</td>
<td align="left">.wf1</td>
<td align="left"><a href="https://cran.r-project.org/package=hexView"><strong>hexView</strong></a></td>
<td align="left"></td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Feather R/Python interchange format</td>
<td align="left">.feather</td>
<td align="left"><a href="https://cran.r-project.org/package=feather"><strong>feather</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=feather"><strong>feather</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Fast Storage</td>
<td align="left">.fst</td>
<td align="left"><a href="https://cran.r-project.org/package=fst"><strong>fst</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=fst"><strong>fst</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">JSON</td>
<td align="left">.json</td>
<td align="left"><a href="https://cran.r-project.org/package=jsonlite"><strong>jsonlite</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=jsonlite"><strong>jsonlite</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Matlab</td>
<td align="left">.mat</td>
<td align="left"><a href="https://cran.r-project.org/package=rmatio"><strong>rmatio</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=rmatio"><strong>rmatio</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">OpenDocument Spreadsheet</td>
<td align="left">.ods</td>
<td align="left"><a href="https://cran.r-project.org/package=readODS"><strong>readODS</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=readODS"><strong>readODS</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">HTML Tables</td>
<td align="left">.html</td>
<td align="left"><a href="https://cran.r-project.org/package=xml2"><strong>xml2</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=xml2"><strong>xml2</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Shallow XML documents</td>
<td align="left">.xml</td>
<td align="left"><a href="https://cran.r-project.org/package=xml2"><strong>xml2</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=xml2"><strong>xml2</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">YAML</td>
<td align="left">.yml</td>
<td align="left"><a href="https://cran.r-project.org/package=yaml"><strong>yaml</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=yaml"><strong>yaml</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Clipboard</td>
<td align="left">default is tsv</td>
<td align="left"><a href="https://cran.r-project.org/package=clipr"><strong>clipr</strong></a></td>
<td align="left"><a href="https://cran.r-project.org/package=clipr"><strong>clipr</strong></a></td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><a href="https://www.google.com/sheets/about/">Google Sheets</a></td>
<td align="left">as Comma-separated data</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table></div>
<p>R limitations:</p>
<ul>
<li><p>By default, R use 1 core in CPU</p></li>
<li><p>R puts data into memory (limit around 2-4 GB), while SAS uses data from files on demand</p></li>
<li>
<p>Categorization</p>
<ul>
<li><p>Medium-size file: within RAM limit, around 1-2 GB</p></li>
<li><p>Large file: 2-10 GB, there might be some workaround solution</p></li>
<li><p>Very large file &gt; 10 GB, you have to use distributed or parallel computing</p></li>
</ul>
</li>
</ul>
<p>Solutions:</p>
<ul>
<li><p>buy more RAM</p></li>
<li>
<p>HPC packages</p>
<ul>
<li><p>Explicit Parallelism</p></li>
<li><p>Implicit Parallelism</p></li>
<li><p>Large Memory</p></li>
<li><p>Map/Reduce</p></li>
</ul>
</li>
<li><p>specify number of rows and columns, typically including command <code>nrow =</code></p></li>
<li>
<p>Use packages that store data differently</p>
<ul>
<li><p><code>bigmemory</code>, <code>biganalytics</code>, <code>bigtabulate</code> , <code>synchronicity</code>, <code>bigalgebra</code>, <code>bigvideo</code> use C++ to store matrices, but also support one class type</p></li>
<li><p>For multiple class types, use <code>ff</code> package</p></li>
</ul>
</li>
<li>
<p>Very Large datasets use</p>
<ul>
<li>
<code>RHaddop</code> package</li>
<li><code>HadoopStreaming</code></li>
<li><code>Rhipe</code></li>
</ul>
</li>
</ul>
<div id="medium-size" class="section level3" number="2.4.1">
<h3>
<span class="header-section-number">2.4.1</span> Medium size<a class="anchor" aria-label="anchor" href="#medium-size"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/gesistsa/rio">"rio"</a></span><span class="op">)</span></span></code></pre></div>
<p>To import multiple files in a directory</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/import_list.html">import_list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.files.html">dir</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, which <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>To export a single data file</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>, <span class="st">"data.csv"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>,<span class="st">"data.dta"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>,<span class="st">"data.txt"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>,<span class="st">"data_cyl.rds"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>,<span class="st">"data.rdata"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>,<span class="st">"data.R"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>,<span class="st">"data.csv.zip"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="va">data</span>,<span class="st">"list.json"</span><span class="op">)</span></span></code></pre></div>
<p>To export multiple data files</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/export.html">export</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>mtcars <span class="op">=</span> <span class="va">mtcars</span>, iris <span class="op">=</span> <span class="va">iris</span><span class="op">)</span>, <span class="st">"data_file_type"</span><span class="op">)</span> </span>
<span><span class="co"># where data_file_type should substituted with the extension listed above</span></span></code></pre></div>
<p>To convert between data file types</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># convert Stata to SPSS</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/convert.html">convert</a></span><span class="op">(</span><span class="st">"data.dta"</span>, <span class="st">"data.sav"</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="large-size" class="section level3" number="2.4.2">
<h3>
<span class="header-section-number">2.4.2</span> Large size<a class="anchor" aria-label="anchor" href="#large-size"><i class="fas fa-link"></i></a>
</h3>
<div id="cloud-computing-using-aws-for-big-data" class="section level4" number="2.4.2.1">
<h4>
<span class="header-section-number">2.4.2.1</span> Cloud Computing: Using AWS for Big Data<a class="anchor" aria-label="anchor" href="#cloud-computing-using-aws-for-big-data"><i class="fas fa-link"></i></a>
</h4>
<p>Amazon Web Service (AWS): Compute resources can be rented at approximately $1/hr. Use AWS to process large datasets without overwhelming your local machine.</p>
</div>
<div id="importing-large-files-as-chunks" class="section level4" number="2.4.2.2">
<h4>
<span class="header-section-number">2.4.2.2</span> Importing Large Files as Chunks<a class="anchor" aria-label="anchor" href="#importing-large-files-as-chunks"><i class="fas fa-link"></i></a>
</h4>
<div id="using-base-r" class="section level5" number="2.4.2.2.1">
<h5>
<span class="header-section-number">2.4.2.2.1</span> Using Base R<a class="anchor" aria-label="anchor" href="#using-base-r"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">file_in</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/connections.html">file</a></span><span class="op">(</span><span class="st">"in.csv"</span>, <span class="st">"r"</span><span class="op">)</span>  <span class="co"># Open a connection to the file</span></span>
<span><span class="va">chunk_size</span> <span class="op">&lt;-</span> <span class="fl">100000</span>            <span class="co"># Define chunk size</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/readLines.html">readLines</a></span><span class="op">(</span><span class="va">file_in</span>, n <span class="op">=</span> <span class="va">chunk_size</span><span class="op">)</span>  <span class="co"># Read data in chunks</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/connections.html">close</a></span><span class="op">(</span><span class="va">file_in</span><span class="op">)</span>                  <span class="co"># Close the file connection</span></span></code></pre></div>
</div>
<div id="using-the-data.table-package" class="section level5" number="2.4.2.2.2">
<h5>
<span class="header-section-number">2.4.2.2.2</span> Using the <code>data.table</code> Package<a class="anchor" aria-label="anchor" href="#using-the-data.table-package"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com">data.table</a></span><span class="op">)</span></span>
<span><span class="va">mydata</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/data.table/man/fread.html">fread</a></span><span class="op">(</span><span class="st">"in.csv"</span>, header <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>  <span class="co"># Fast and memory-efficient</span></span></code></pre></div>
</div>
<div id="using-the-ff-package" class="section level5" number="2.4.2.2.3">
<h5>
<span class="header-section-number">2.4.2.2.3</span> Using the <code>ff</code> Package<a class="anchor" aria-label="anchor" href="#using-the-ff-package"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/truecluster/ff">ff</a></span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">read.csv.ffdf</span><span class="op">(</span></span>
<span>  file <span class="op">=</span> <span class="st">"file.csv"</span>,</span>
<span>  nrow <span class="op">=</span> <span class="fl">10</span>,          <span class="co"># Total rows</span></span>
<span>  header <span class="op">=</span> <span class="cn">TRUE</span>,      <span class="co"># Include headers</span></span>
<span>  VERBOSE <span class="op">=</span> <span class="cn">TRUE</span>,     <span class="co"># Display progress</span></span>
<span>  first.rows <span class="op">=</span> <span class="fl">10000</span>, <span class="co"># Initial chunk</span></span>
<span>  next.rows <span class="op">=</span> <span class="fl">50000</span>,  <span class="co"># Subsequent chunks</span></span>
<span>  colClasses <span class="op">=</span> <span class="cn">NA</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div id="using-the-bigmemory-package" class="section level5" number="2.4.2.2.4">
<h5>
<span class="header-section-number">2.4.2.2.4</span> Using the <code>bigmemory</code> Package<a class="anchor" aria-label="anchor" href="#using-the-bigmemory-package"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/kaneplusplus/bigmemory">bigmemory</a></span><span class="op">)</span></span>
<span><span class="va">my_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/bigmemory/man/write.big.matrix.html">read.big.matrix</a></span><span class="op">(</span><span class="st">'in.csv'</span>, header <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="using-the-sqldf-package" class="section level5" number="2.4.2.2.5">
<h5>
<span class="header-section-number">2.4.2.2.5</span> Using the <code>sqldf</code> Package<a class="anchor" aria-label="anchor" href="#using-the-sqldf-package"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/ggrothendieck/sqldf">sqldf</a></span><span class="op">)</span></span>
<span><span class="va">my_data</span> <span class="op">&lt;-</span> <span class="fu">read.csv.sql</span><span class="op">(</span><span class="st">'in.csv'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example: Filtering during import</span></span>
<span><span class="va">iris2</span> <span class="op">&lt;-</span> <span class="fu">read.csv.sql</span><span class="op">(</span><span class="st">"iris.csv"</span>, </span>
<span>    sql <span class="op">=</span> <span class="st">"SELECT * FROM file WHERE Species = 'setosa'"</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="using-the-rmysql-package" class="section level5" number="2.4.2.2.6">
<h5>
<span class="header-section-number">2.4.2.2.6</span> Using the <code>RMySQL</code> Package<a class="anchor" aria-label="anchor" href="#using-the-rmysql-package"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://downloads.mariadb.org/connector-c/">RMySQL</a></span><span class="op">)</span></span></code></pre></div>
<p><code>RQLite</code> package</p>
<ul>
<li>
<a href="https://sqlite.org/download.html">Download SQLite</a>, pick “A bundle of command-line tools for managing SQLite database files” for Window 10</li>
<li>Unzip file, and open <code>sqlite3.exe.</code>
</li>
<li>Type in the prompt
<ul>
<li>
<code>sqlite&gt; .cd 'C:\Users\data'</code> specify path to your desired directory</li>
<li>
<code>sqlite&gt; .open database_name.db</code> to open a database</li>
<li>To import the CSV file into the database
<ul>
<li>
<code>sqlite&gt; .mode csv</code> specify to SQLite that the next file is .csv file</li>
<li>
<code>sqlite&gt; .import file_name.csv datbase_name</code> to import the csv file to the database</li>
</ul>
</li>
<li>
<code>sqlite&gt; .exit</code> After you’re done, exit the sqlite program</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dbi.r-dbi.org">DBI</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://rsqlite.r-dbi.org">"RSQLite"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/getwd.html">setwd</a></span><span class="op">(</span><span class="st">""</span><span class="op">)</span></span>
<span><span class="va">con</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dbi.r-dbi.org/reference/dbConnect.html">dbConnect</a></span><span class="op">(</span><span class="fu">RSQLite</span><span class="fu">::</span><span class="fu"><a href="https://rsqlite.r-dbi.org/reference/SQLite.html">SQLite</a></span><span class="op">(</span><span class="op">)</span>, <span class="st">"data_base.db"</span><span class="op">)</span></span>
<span><span class="va">tbl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/tbl.html">tbl</a></span><span class="op">(</span><span class="va">con</span>, <span class="st">"data_table"</span><span class="op">)</span></span>
<span><span class="va">tbl</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/compute.html">collect</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># to actually pull the data into the workspace</span></span>
<span><span class="fu"><a href="https://dbi.r-dbi.org/reference/dbDisconnect.html">dbDisconnect</a></span><span class="op">(</span><span class="va">con</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="using-the-arrow-package" class="section level5" number="2.4.2.2.7">
<h5>
<span class="header-section-number">2.4.2.2.7</span> Using the <code>arrow</code> Package<a class="anchor" aria-label="anchor" href="#using-the-arrow-package"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/apache/arrow/">arrow</a></span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://arrow.apache.org/docs/r/reference/read_delim_arrow.html">read_csv_arrow</a></span><span class="op">(</span><span class="st">"file.csv"</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="using-the-vroom-package" class="section level5" number="2.4.2.2.8">
<h5>
<span class="header-section-number">2.4.2.2.8</span> Using the <code>vroom</code> Package<a class="anchor" aria-label="anchor" href="#using-the-vroom-package"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://vroom.r-lib.org">vroom</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Import a compressed CSV file</span></span>
<span><span class="va">compressed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://vroom.r-lib.org/reference/vroom_example.html">vroom_example</a></span><span class="op">(</span><span class="st">"mtcars.csv.zip"</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://vroom.r-lib.org/reference/vroom.html">vroom</a></span><span class="op">(</span><span class="va">compressed</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="using-the-data.table-package-1" class="section level5" number="2.4.2.2.9">
<h5>
<span class="header-section-number">2.4.2.2.9</span> Using the <code>data.table</code> Package<a class="anchor" aria-label="anchor" href="#using-the-data.table-package-1"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">s</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/data.table/man/fread.html">fread</a></span><span class="op">(</span><span class="st">"sample.csv"</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="comparisons-regarding-storage-space" class="section level5" number="2.4.2.2.10">
<h5>
<span class="header-section-number">2.4.2.2.10</span> Comparisons Regarding Storage Space<a class="anchor" aria-label="anchor" href="#comparisons-regarding-storage-space"><i class="fas fa-link"></i></a>
</h5>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test</span> <span class="op">=</span> <span class="fu">ff</span><span class="fu">::</span><span class="fu">read.csv.ffdf</span><span class="op">(</span>file <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/object.size.html">object.size</a></span><span class="op">(</span><span class="va">test</span><span class="op">)</span> <span class="co"># Highest memory usage</span></span>
<span></span>
<span><span class="va">test1</span> <span class="op">=</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/data.table/man/fread.html">fread</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/object.size.html">object.size</a></span><span class="op">(</span><span class="va">test1</span><span class="op">)</span> <span class="co"># Lowest memory usage</span></span>
<span></span>
<span><span class="va">test2</span> <span class="op">=</span> <span class="fu">readr</span><span class="fu">::</span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/object.size.html">object.size</a></span><span class="op">(</span><span class="va">test2</span><span class="op">)</span> <span class="co"># Second lowest memory usage</span></span>
<span></span>
<span><span class="va">test3</span> <span class="op">=</span> <span class="fu">vroom</span><span class="fu">::</span><span class="fu"><a href="https://vroom.r-lib.org/reference/vroom.html">vroom</a></span><span class="op">(</span>file <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/object.size.html">object.size</a></span><span class="op">(</span><span class="va">test3</span><span class="op">)</span> <span class="co"># Similar to read_csv</span></span></code></pre></div>
<p>To work with large datasets, you can compress them into <code>csv.gz</code> format. However, typically, R requires loading the entire dataset before exporting it, which can be impractical for data over 10 GB. In such cases, processing the data sequentially becomes necessary. Although <code>read.csv</code> is slower compared to <code><a href="https://readr.tidyverse.org/reference/read_delim.html">readr::read_csv</a></code>, it can handle connections and allows for sequential looping, making it useful for large files.</p>
<p>Currently, <code><a href="https://readr.tidyverse.org/reference/read_delim.html">readr::read_csv</a></code> does not support the <code>skip</code> argument efficiently for large data. Even if you specify <code>skip</code>, the function reads all preceding lines again. For instance, if you run <code>read_csv(file, n_max = 100, skip = 0)</code> followed by <code>read_csv(file, n_max = 200, skip = 100)</code>, the first 100 rows are re-read. In contrast, <code>read.csv</code> can continue from where it left off without re-reading previous rows.</p>
<p>If you encounter an error such as:</p>
<p>“Error in (function (con, what, n = 1L, size = NA_integer_, signed = TRUE): can only read from a binary connection”,</p>
<p>you can modify the connection mode from <code>"r"</code> to <code>"rb"</code> (read binary). Although the <code>file</code> function is designed to detect the appropriate format automatically, this workaround can help resolve the issue when it does not behave as expected.</p>
</div>
</div>
<div id="sequential-processing-for-large-data" class="section level4" number="2.4.2.3">
<h4>
<span class="header-section-number">2.4.2.3</span> Sequential Processing for Large Data<a class="anchor" aria-label="anchor" href="#sequential-processing-for-large-data"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Open file for sequential reading</span></span>
<span><span class="va">file_conn</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/connections.html">file</a></span><span class="op">(</span><span class="st">"file.csv"</span>, open <span class="op">=</span> <span class="st">"r"</span><span class="op">)</span></span>
<span><span class="kw">while</span> <span class="op">(</span><span class="cn">TRUE</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Read a chunk of data</span></span>
<span>  <span class="va">data_chunk</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="va">file_conn</span>, nrows <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">data_chunk</span><span class="op">)</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="kw">break</span>  <span class="co"># Stop if no more rows</span></span>
<span>  <span class="co"># Process the chunk here</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/connections.html">close</a></span><span class="op">(</span><span class="va">file_conn</span><span class="op">)</span>  <span class="co"># Close connection</span></span></code></pre></div>
</div>
</div>
</div>
<div id="data-manipulation" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Data Manipulation<a class="anchor" aria-label="anchor" href="#data-manipulation"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load required packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lubridate.tidyverse.org">lubridate</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># -----------------------------</span></span>
<span><span class="co"># Data Structures in R</span></span>
<span><span class="co"># -----------------------------</span></span>
<span></span>
<span><span class="co"># Create vectors</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">23</span>, <span class="fl">4</span>, <span class="fl">45</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">g</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"M"</span>, <span class="st">"M"</span>, <span class="st">"F"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a data frame</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">g</span><span class="op">)</span></span>
<span><span class="va">df</span>  <span class="co"># View the data frame</span></span>
<span><span class="co">#&gt;   n g</span></span>
<span><span class="co">#&gt; 1 1 M</span></span>
<span><span class="co">#&gt; 2 3 M</span></span>
<span><span class="co">#&gt; 3 5 F</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span>  <span class="co"># Check its structure</span></span>
<span><span class="co">#&gt; 'data.frame':    3 obs. of  2 variables:</span></span>
<span><span class="co">#&gt;  $ n: num  1 3 5</span></span>
<span><span class="co">#&gt;  $ g: chr  "M" "M" "F"</span></span>
<span></span>
<span><span class="co"># Using tibble for cleaner outputs</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">g</span><span class="op">)</span></span>
<span><span class="va">df</span>  <span class="co"># View the tibble</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;       n g    </span></span>
<span><span class="co">#&gt;   &lt;dbl&gt; &lt;chr&gt;</span></span>
<span><span class="co">#&gt; 1     1 M    </span></span>
<span><span class="co">#&gt; 2     3 M    </span></span>
<span><span class="co">#&gt; 3     5 F</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span></span>
<span><span class="co">#&gt; tibble [3 × 2] (S3: tbl_df/tbl/data.frame)</span></span>
<span><span class="co">#&gt;  $ n: num [1:3] 1 3 5</span></span>
<span><span class="co">#&gt;  $ g: chr [1:3] "M" "M" "F"</span></span>
<span></span>
<span><span class="co"># Create a list</span></span>
<span><span class="va">lst</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">g</span>, <span class="va">df</span><span class="op">)</span></span>
<span><span class="va">lst</span>  <span class="co"># Display the list</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; [1]  1  4 23  4 45</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; [1] 1 3 5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[3]]</span></span>
<span><span class="co">#&gt; [1] "M" "M" "F"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[4]]</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;       n g    </span></span>
<span><span class="co">#&gt;   &lt;dbl&gt; &lt;chr&gt;</span></span>
<span><span class="co">#&gt; 1     1 M    </span></span>
<span><span class="co">#&gt; 2     3 M    </span></span>
<span><span class="co">#&gt; 3     5 F</span></span>
<span></span>
<span><span class="co"># Name list elements</span></span>
<span><span class="va">lst2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>num <span class="op">=</span> <span class="va">x</span>, size <span class="op">=</span> <span class="va">n</span>, sex <span class="op">=</span> <span class="va">g</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span><span class="va">lst2</span>  <span class="co"># Named list elements are easier to reference</span></span>
<span><span class="co">#&gt; $num</span></span>
<span><span class="co">#&gt; [1]  1  4 23  4 45</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $size</span></span>
<span><span class="co">#&gt; [1] 1 3 5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $sex</span></span>
<span><span class="co">#&gt; [1] "M" "M" "F"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $data</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;       n g    </span></span>
<span><span class="co">#&gt;   &lt;dbl&gt; &lt;chr&gt;</span></span>
<span><span class="co">#&gt; 1     1 M    </span></span>
<span><span class="co">#&gt; 2     3 M    </span></span>
<span><span class="co">#&gt; 3     5 F</span></span>
<span></span>
<span><span class="co"># Another list example with numeric vectors</span></span>
<span><span class="va">lst3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">5</span>, <span class="fl">7</span><span class="op">)</span>,</span>
<span>  y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">4</span>, <span class="fl">5</span>, <span class="fl">5</span>, <span class="fl">5</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">22</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">5</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">lst3</span></span>
<span><span class="co">#&gt; $x</span></span>
<span><span class="co">#&gt; [1] 1 3 5 7</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $y</span></span>
<span><span class="co">#&gt; [1] 2 2 2 4 5 5 5 6</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $z</span></span>
<span><span class="co">#&gt; [1] 22  3  3  3  5 10</span></span>
<span></span>
<span><span class="co"># Find means of list elements</span></span>
<span><span class="co"># One at a time</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">lst3</span><span class="op">$</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 4</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">lst3</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 3.875</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">lst3</span><span class="op">$</span><span class="va">z</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 7.666667</span></span>
<span></span>
<span><span class="co"># Using lapply to calculate means</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">lst3</span>, <span class="va">mean</span><span class="op">)</span></span>
<span><span class="co">#&gt; $x</span></span>
<span><span class="co">#&gt; [1] 4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $y</span></span>
<span><span class="co">#&gt; [1] 3.875</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $z</span></span>
<span><span class="co">#&gt; [1] 7.666667</span></span>
<span></span>
<span><span class="co"># Simplified output with sapply</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">lst3</span>, <span class="va">mean</span><span class="op">)</span></span>
<span><span class="co">#&gt;        x        y        z </span></span>
<span><span class="co">#&gt; 4.000000 3.875000 7.666667</span></span>
<span></span>
<span><span class="co"># Tidyverse alternative: map() function</span></span>
<span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span><span class="va">lst3</span>, <span class="va">mean</span><span class="op">)</span></span>
<span><span class="co">#&gt; $x</span></span>
<span><span class="co">#&gt; [1] 4</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $y</span></span>
<span><span class="co">#&gt; [1] 3.875</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $z</span></span>
<span><span class="co">#&gt; [1] 7.666667</span></span>
<span></span>
<span><span class="co"># Tidyverse with numeric output: map_dbl()</span></span>
<span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">lst3</span>, <span class="va">mean</span><span class="op">)</span></span>
<span><span class="co">#&gt;        x        y        z </span></span>
<span><span class="co">#&gt; 4.000000 3.875000 7.666667</span></span>
<span></span>
<span><span class="co"># -----------------------------</span></span>
<span><span class="co"># Binding Data Frames</span></span>
<span><span class="co"># -----------------------------</span></span>
<span></span>
<span><span class="co"># Create tibbles for demonstration</span></span>
<span><span class="va">dat01</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, y <span class="op">=</span> <span class="fl">5</span><span class="op">:</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">dat02</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fl">10</span><span class="op">:</span><span class="fl">16</span>, y <span class="op">=</span> <span class="va">x</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">dat03</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span><span class="op">)</span>  <span class="co"># 5 random numbers from (0, 1)</span></span>
<span></span>
<span><span class="co"># Row binding</span></span>
<span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span><span class="va">dat01</span>, <span class="va">dat02</span>, <span class="va">dat01</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 17 × 2</span></span>
<span><span class="co">#&gt;        x     y</span></span>
<span><span class="co">#&gt;    &lt;int&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt;  1     1   5  </span></span>
<span><span class="co">#&gt;  2     2   4  </span></span>
<span><span class="co">#&gt;  3     3   3  </span></span>
<span><span class="co">#&gt;  4     4   2  </span></span>
<span><span class="co">#&gt;  5     5   1  </span></span>
<span><span class="co">#&gt;  6    10   5  </span></span>
<span><span class="co">#&gt;  7    11   5.5</span></span>
<span><span class="co">#&gt;  8    12   6  </span></span>
<span><span class="co">#&gt;  9    13   6.5</span></span>
<span><span class="co">#&gt; 10    14   7  </span></span>
<span><span class="co">#&gt; 11    15   7.5</span></span>
<span><span class="co">#&gt; 12    16   8  </span></span>
<span><span class="co">#&gt; 13     1   5  </span></span>
<span><span class="co">#&gt; 14     2   4  </span></span>
<span><span class="co">#&gt; 15     3   3  </span></span>
<span><span class="co">#&gt; 16     4   2  </span></span>
<span><span class="co">#&gt; 17     5   1</span></span>
<span></span>
<span><span class="co"># Add a new identifier column with .id</span></span>
<span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span><span class="va">dat01</span>, <span class="va">dat02</span>, .id <span class="op">=</span> <span class="st">"id"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 12 × 3</span></span>
<span><span class="co">#&gt;    id        x     y</span></span>
<span><span class="co">#&gt;    &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt;  1 1         1   5  </span></span>
<span><span class="co">#&gt;  2 1         2   4  </span></span>
<span><span class="co">#&gt;  3 1         3   3  </span></span>
<span><span class="co">#&gt;  4 1         4   2  </span></span>
<span><span class="co">#&gt;  5 1         5   1  </span></span>
<span><span class="co">#&gt;  6 2        10   5  </span></span>
<span><span class="co">#&gt;  7 2        11   5.5</span></span>
<span><span class="co">#&gt;  8 2        12   6  </span></span>
<span><span class="co">#&gt;  9 2        13   6.5</span></span>
<span><span class="co">#&gt; 10 2        14   7  </span></span>
<span><span class="co">#&gt; 11 2        15   7.5</span></span>
<span><span class="co">#&gt; 12 2        16   8</span></span>
<span></span>
<span><span class="co"># Use named inputs for better identification</span></span>
<span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span><span class="st">"dat01"</span> <span class="op">=</span> <span class="va">dat01</span>, <span class="st">"dat02"</span> <span class="op">=</span> <span class="va">dat02</span>, .id <span class="op">=</span> <span class="st">"id"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 12 × 3</span></span>
<span><span class="co">#&gt;    id        x     y</span></span>
<span><span class="co">#&gt;    &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt;  1 dat01     1   5  </span></span>
<span><span class="co">#&gt;  2 dat01     2   4  </span></span>
<span><span class="co">#&gt;  3 dat01     3   3  </span></span>
<span><span class="co">#&gt;  4 dat01     4   2  </span></span>
<span><span class="co">#&gt;  5 dat01     5   1  </span></span>
<span><span class="co">#&gt;  6 dat02    10   5  </span></span>
<span><span class="co">#&gt;  7 dat02    11   5.5</span></span>
<span><span class="co">#&gt;  8 dat02    12   6  </span></span>
<span><span class="co">#&gt;  9 dat02    13   6.5</span></span>
<span><span class="co">#&gt; 10 dat02    14   7  </span></span>
<span><span class="co">#&gt; 11 dat02    15   7.5</span></span>
<span><span class="co">#&gt; 12 dat02    16   8</span></span>
<span></span>
<span><span class="co"># Bind a list of data frames</span></span>
<span><span class="va">list01</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"dat01"</span> <span class="op">=</span> <span class="va">dat01</span>, <span class="st">"dat02"</span> <span class="op">=</span> <span class="va">dat02</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span><span class="va">list01</span>, .id <span class="op">=</span> <span class="st">"source"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 12 × 3</span></span>
<span><span class="co">#&gt;    source     x     y</span></span>
<span><span class="co">#&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt;  1 dat01      1   5  </span></span>
<span><span class="co">#&gt;  2 dat01      2   4  </span></span>
<span><span class="co">#&gt;  3 dat01      3   3  </span></span>
<span><span class="co">#&gt;  4 dat01      4   2  </span></span>
<span><span class="co">#&gt;  5 dat01      5   1  </span></span>
<span><span class="co">#&gt;  6 dat02     10   5  </span></span>
<span><span class="co">#&gt;  7 dat02     11   5.5</span></span>
<span><span class="co">#&gt;  8 dat02     12   6  </span></span>
<span><span class="co">#&gt;  9 dat02     13   6.5</span></span>
<span><span class="co">#&gt; 10 dat02     14   7  </span></span>
<span><span class="co">#&gt; 11 dat02     15   7.5</span></span>
<span><span class="co">#&gt; 12 dat02     16   8</span></span>
<span></span>
<span><span class="co"># Column binding</span></span>
<span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_cols.html">bind_cols</a></span><span class="op">(</span><span class="va">dat01</span>, <span class="va">dat03</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 5 × 3</span></span>
<span><span class="co">#&gt;       x     y     z</span></span>
<span><span class="co">#&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1     1     5 0.265</span></span>
<span><span class="co">#&gt; 2     2     4 0.410</span></span>
<span><span class="co">#&gt; 3     3     3 0.780</span></span>
<span><span class="co">#&gt; 4     4     2 0.926</span></span>
<span><span class="co">#&gt; 5     5     1 0.501</span></span>
<span></span>
<span><span class="co"># -----------------------------</span></span>
<span><span class="co"># String Manipulation</span></span>
<span><span class="co"># -----------------------------</span></span>
<span></span>
<span><span class="va">names</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Ford, MS"</span>, <span class="st">"Jones, PhD"</span>, <span class="st">"Martin, Phd"</span>, <span class="st">"Huck, MA, MLS"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Remove everything after the first comma</span></span>
<span><span class="fu"><a href="https://stringr.tidyverse.org/reference/str_remove.html">str_remove</a></span><span class="op">(</span><span class="va">names</span>, pattern <span class="op">=</span> <span class="st">", [[:print:]]+"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Ford"   "Jones"  "Martin" "Huck"</span></span>
<span></span>
<span><span class="co"># Explanation: [[:print:]]+ matches one or more printable characters</span></span>
<span></span>
<span><span class="co"># -----------------------------</span></span>
<span><span class="co"># Reshaping Data</span></span>
<span><span class="co"># -----------------------------</span></span>
<span></span>
<span><span class="co"># Wide format data</span></span>
<span><span class="va">wide</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Clay"</span>, <span class="st">"Garrett"</span>, <span class="st">"Addison"</span><span class="op">)</span>,</span>
<span>  test1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">78</span>, <span class="fl">93</span>, <span class="fl">90</span><span class="op">)</span>,</span>
<span>  test2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">87</span>, <span class="fl">91</span>, <span class="fl">97</span><span class="op">)</span>,</span>
<span>  test3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">88</span>, <span class="fl">99</span>, <span class="fl">91</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Long format data</span></span>
<span><span class="va">long</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Clay"</span>, <span class="st">"Garrett"</span>, <span class="st">"Addison"</span><span class="op">)</span>, each <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  score <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">78</span>, <span class="fl">87</span>, <span class="fl">88</span>, <span class="fl">93</span>, <span class="fl">91</span>, <span class="fl">99</span>, <span class="fl">90</span>, <span class="fl">97</span>, <span class="fl">91</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary statistics</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/aggregate.html">aggregate</a></span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">name</span>, data <span class="op">=</span> <span class="va">long</span>, <span class="va">mean</span><span class="op">)</span>  <span class="co"># Mean score per student</span></span>
<span><span class="co">#&gt;      name    score</span></span>
<span><span class="co">#&gt; 1 Addison 92.66667</span></span>
<span><span class="co">#&gt; 2    Clay 84.33333</span></span>
<span><span class="co">#&gt; 3 Garrett 94.33333</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/aggregate.html">aggregate</a></span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">test</span>, data <span class="op">=</span> <span class="va">long</span>, <span class="va">mean</span><span class="op">)</span>  <span class="co"># Mean score per test</span></span>
<span><span class="co">#&gt;   test    score</span></span>
<span><span class="co">#&gt; 1    1 87.00000</span></span>
<span><span class="co">#&gt; 2    2 91.66667</span></span>
<span><span class="co">#&gt; 3    3 92.66667</span></span>
<span></span>
<span><span class="co"># Line plot of scores over tests</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">long</span>,</span>
<span>       <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>           x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">test</span><span class="op">)</span>,</span>
<span>           y <span class="op">=</span> <span class="va">score</span>,</span>
<span>           color <span class="op">=</span> <span class="va">name</span>,</span>
<span>           group <span class="op">=</span> <span class="va">name</span></span>
<span>       <span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"Test"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"Test Scores by Student"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="02.3-data-mani_files/figure-html/unnamed-chunk-18-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Reshape wide to long</span></span>
<span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="va">wide</span>, <span class="va">test1</span><span class="op">:</span><span class="va">test3</span>, names_to <span class="op">=</span> <span class="st">"test"</span>, values_to <span class="op">=</span> <span class="st">"score"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 9 × 3</span></span>
<span><span class="co">#&gt;   name    test  score</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 Clay    test1    78</span></span>
<span><span class="co">#&gt; 2 Clay    test2    87</span></span>
<span><span class="co">#&gt; 3 Clay    test3    88</span></span>
<span><span class="co">#&gt; 4 Garrett test1    93</span></span>
<span><span class="co">#&gt; 5 Garrett test2    91</span></span>
<span><span class="co">#&gt; 6 Garrett test3    99</span></span>
<span><span class="co">#&gt; 7 Addison test1    90</span></span>
<span><span class="co">#&gt; 8 Addison test2    97</span></span>
<span><span class="co">#&gt; 9 Addison test3    91</span></span>
<span></span>
<span><span class="co"># Use names_prefix to clean column names</span></span>
<span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span></span>
<span>    <span class="va">wide</span>,</span>
<span>    <span class="op">-</span><span class="va">name</span>,</span>
<span>    names_to <span class="op">=</span> <span class="st">"test"</span>,</span>
<span>    values_to <span class="op">=</span> <span class="st">"score"</span>,</span>
<span>    names_prefix <span class="op">=</span> <span class="st">"test"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 9 × 3</span></span>
<span><span class="co">#&gt;   name    test  score</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 Clay    1        78</span></span>
<span><span class="co">#&gt; 2 Clay    2        87</span></span>
<span><span class="co">#&gt; 3 Clay    3        88</span></span>
<span><span class="co">#&gt; 4 Garrett 1        93</span></span>
<span><span class="co">#&gt; 5 Garrett 2        91</span></span>
<span><span class="co">#&gt; 6 Garrett 3        99</span></span>
<span><span class="co">#&gt; 7 Addison 1        90</span></span>
<span><span class="co">#&gt; 8 Addison 2        97</span></span>
<span><span class="co">#&gt; 9 Addison 3        91</span></span>
<span></span>
<span><span class="co"># Reshape long to wide with explicit id_cols argument</span></span>
<span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_wider.html">pivot_wider</a></span><span class="op">(</span></span>
<span>  <span class="va">long</span>,</span>
<span>  id_cols <span class="op">=</span> <span class="va">name</span>, </span>
<span>  names_from <span class="op">=</span> <span class="va">test</span>,</span>
<span>  values_from <span class="op">=</span> <span class="va">score</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 4</span></span>
<span><span class="co">#&gt;   name      `1`   `2`   `3`</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 Clay       78    87    88</span></span>
<span><span class="co">#&gt; 2 Garrett    93    91    99</span></span>
<span><span class="co">#&gt; 3 Addison    90    97    91</span></span>
<span></span>
<span><span class="co"># Add a prefix to the resulting columns</span></span>
<span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_wider.html">pivot_wider</a></span><span class="op">(</span></span>
<span>  <span class="va">long</span>,</span>
<span>  id_cols <span class="op">=</span> <span class="va">name</span>,  </span>
<span>  names_from <span class="op">=</span> <span class="va">test</span>,</span>
<span>  values_from <span class="op">=</span> <span class="va">score</span>,</span>
<span>  names_prefix <span class="op">=</span> <span class="st">"test"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 4</span></span>
<span><span class="co">#&gt;   name    test1 test2 test3</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 Clay       78    87    88</span></span>
<span><span class="co">#&gt; 2 Garrett    93    91    99</span></span>
<span><span class="co">#&gt; 3 Addison    90    97    91</span></span></code></pre></div>
<p>The verbs of data manipulation</p>
<ul>
<li>
<code>select</code>: selecting (or not selecting) columns based on their names (eg: select columns Q1 through Q25)</li>
<li>
<code>slice</code>: selecting (or not selecting) rows based on their position (eg: select rows 1:10)</li>
<li>
<code>mutate</code>: add or derive new columns (or variables) based on existing columns (eg: create a new column that expresses measurement in cm based on existing measure in inches)</li>
<li>
<code>rename</code>: rename variables or change column names (eg: change “GraduationRate100” to “grad100”)</li>
<li>
<code>filter</code>: selecting rows based on a condition (eg: all rows where gender = Male)</li>
<li>
<code>arrange</code>: ordering rows based on variable(s) numeric or alphabetical order (eg: sort in descending order of Income)</li>
<li>
<code>sample</code>: take random samples of data (eg: sample 80% of data to create a “training” set)</li>
<li>
<code>summarize</code>: condense or aggregate multiple values into single summary values (eg: calculate median income by age group)</li>
<li>
<code>group_by</code>: convert a tbl into a grouped tbl so that operations are performed “by group”; allows us to summarize data or apply verbs to data by groups (eg, by gender or treatment)</li>
<li>the pipe: <code>%&gt;%</code>
<ul>
<li><p>Use Ctrl + Shift + M (Win) or Cmd + Shift + M (Mac) to enter in RStudio</p></li>
<li><p>The pipe takes the output of a function and “pipes” into the first argument of the next function.</p></li>
<li><p>new pipe is <code>|&gt;</code> It should be identical to the old one, except for certain special cases.</p></li>
</ul>
</li>
<li>
<code>:=</code> (Walrus operator): similar to <code>=</code> , but for cases where you want to use the <code>glue</code> package (i.e., dynamic changes in the variable name in the left-hand side)</li>
</ul>
<p>Writing function in R</p>
<p>Tunneling</p>
<p><code>{{</code> (called curly-curly) allows you to tunnel data-variables through arg-variables (i.e., function arguments)</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="co"># -----------------------------</span></span>
<span><span class="co"># Writing Functions with {{ }}</span></span>
<span><span class="co"># -----------------------------</span></span>
<span></span>
<span><span class="co"># Define a custom function using {{ }}</span></span>
<span><span class="va">get_mean</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span>, <span class="va">group_var</span>, <span class="va">var_to_mean</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="op">{</span><span class="op">{</span><span class="va">group_var</span><span class="op">}</span><span class="op">}</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">{</span><span class="op">{</span><span class="va">var_to_mean</span><span class="op">}</span><span class="op">}</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Apply the function</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"mtcars"</span><span class="op">)</span></span>
<span><span class="va">mtcars</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">get_mean</span><span class="op">(</span>group_var <span class="op">=</span> <span class="va">cyl</span>, var_to_mean <span class="op">=</span> <span class="va">mpg</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;     cyl  mean</span></span>
<span><span class="co">#&gt;   &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1     4  26.7</span></span>
<span><span class="co">#&gt; 2     6  19.7</span></span>
<span><span class="co">#&gt; 3     8  15.1</span></span>
<span></span>
<span><span class="co"># Dynamically name the resulting variable</span></span>
<span><span class="va">get_mean</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span>, <span class="va">group_var</span>, <span class="va">var_to_mean</span>, <span class="va">prefix</span> <span class="op">=</span> <span class="st">"mean_of"</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="op">{</span><span class="op">{</span><span class="va">group_var</span><span class="op">}</span><span class="op">}</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span><span class="st">"{prefix}_{{var_to_mean}}"</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">{</span><span class="op">{</span><span class="va">var_to_mean</span><span class="op">}</span><span class="op">}</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Apply the modified function</span></span>
<span><span class="va">mtcars</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">get_mean</span><span class="op">(</span>group_var <span class="op">=</span> <span class="va">cyl</span>, var_to_mean <span class="op">=</span> <span class="va">mpg</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;     cyl mean_of_mpg</span></span>
<span><span class="co">#&gt;   &lt;dbl&gt;       &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1     4        26.7</span></span>
<span><span class="co">#&gt; 2     6        19.7</span></span>
<span><span class="co">#&gt; 3     8        15.1</span></span></code></pre></div>

</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="introduction.html"><span class="header-section-number">1</span> Introduction</a></div>
<div class="next"><a href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#prerequisites"><span class="header-section-number">2</span> Prerequisites</a></li>
<li>
<a class="nav-link" href="#matrix-theory"><span class="header-section-number">2.1</span> Matrix Theory</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#rank-of-a-matrix"><span class="header-section-number">2.1.1</span> Rank of a Matrix</a></li>
<li><a class="nav-link" href="#inverse-of-a-matrix"><span class="header-section-number">2.1.2</span> Inverse of a Matrix</a></li>
<li><a class="nav-link" href="#definiteness-of-a-matrix"><span class="header-section-number">2.1.3</span> Definiteness of a Matrix</a></li>
<li><a class="nav-link" href="#matrix-calculus"><span class="header-section-number">2.1.4</span> Matrix Calculus</a></li>
<li><a class="nav-link" href="#optimization-in-scalar-and-vector-spaces"><span class="header-section-number">2.1.5</span> Optimization in Scalar and Vector Spaces</a></li>
<li><a class="nav-link" href="#cholesky-decomposition"><span class="header-section-number">2.1.6</span> Cholesky Decomposition</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#probability-theory"><span class="header-section-number">2.2</span> Probability Theory</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#axioms-and-theorems-of-probability"><span class="header-section-number">2.2.1</span> Axioms and Theorems of Probability</a></li>
<li><a class="nav-link" href="#central-limit-theorem"><span class="header-section-number">2.2.2</span> Central Limit Theorem</a></li>
<li><a class="nav-link" href="#random-variable"><span class="header-section-number">2.2.3</span> Random Variable</a></li>
<li><a class="nav-link" href="#moment-generating-function"><span class="header-section-number">2.2.4</span> Moment Generating Function</a></li>
<li><a class="nav-link" href="#moments"><span class="header-section-number">2.2.5</span> Moments</a></li>
<li><a class="nav-link" href="#skewness"><span class="header-section-number">2.2.6</span> Skewness</a></li>
<li><a class="nav-link" href="#kurtosis"><span class="header-section-number">2.2.7</span> Kurtosis</a></li>
<li><a class="nav-link" href="#distributions"><span class="header-section-number">2.2.8</span> Distributions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#general-math"><span class="header-section-number">2.3</span> General Math</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#number-sets"><span class="header-section-number">2.3.1</span> Number Sets</a></li>
<li><a class="nav-link" href="#summation-notation-and-series"><span class="header-section-number">2.3.2</span> Summation Notation and Series</a></li>
<li><a class="nav-link" href="#taylor-expansion"><span class="header-section-number">2.3.3</span> Taylor Expansion</a></li>
<li><a class="nav-link" href="#law-of-large-numbers"><span class="header-section-number">2.3.4</span> Law of Large Numbers</a></li>
<li><a class="nav-link" href="#convergence"><span class="header-section-number">2.3.5</span> Convergence</a></li>
<li><a class="nav-link" href="#sufficient-statistics-and-likelihood"><span class="header-section-number">2.3.6</span> Sufficient Statistics and Likelihood</a></li>
<li><a class="nav-link" href="#parameter-transformations"><span class="header-section-number">2.3.7</span> Parameter Transformations</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#data-importexport"><span class="header-section-number">2.4</span> Data Import/Export</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#medium-size"><span class="header-section-number">2.4.1</span> Medium size</a></li>
<li><a class="nav-link" href="#large-size"><span class="header-section-number">2.4.2</span> Large size</a></li>
</ul>
</li>
<li><a class="nav-link" href="#data-manipulation"><span class="header-section-number">2.5</span> Data Manipulation</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/02.1-prerequisites.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/02.1-prerequisites.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-02-18.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
