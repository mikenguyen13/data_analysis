<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 24 Analysis of Variance | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Analysis of Variance (ANOVA) shares its underlying mechanism with linear regression. However, ANOVA approaches the analysis from a different perspective, making it particularly useful for studying...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 24 Analysis of Variance | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/sec-analysis-of-variance-anova.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Analysis of Variance (ANOVA) shares its underlying mechanism with linear regression. However, ANOVA approaches the analysis from a different perspective, making it particularly useful for studying...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 24 Analysis of Variance | A Guide on Data Analysis">
<meta name="twitter:description" content="Analysis of Variance (ANOVA) shares its underlying mechanism with linear regression. However, ANOVA approaches the analysis from a different perspective, making it particularly useful for studying...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="active" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">39</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="sec-analysis-of-variance-anova" class="section level1" number="24">
<h1>
<span class="header-section-number">24</span> Analysis of Variance<a class="anchor" aria-label="anchor" href="#sec-analysis-of-variance-anova"><i class="fas fa-link"></i></a>
</h1>
<p>Analysis of Variance (ANOVA) shares its underlying mechanism with linear regression. However, ANOVA approaches the analysis from a different perspective, making it particularly useful for studying <strong>qualitative variables</strong> and <strong>designed experiments</strong>.</p>
<p><strong>Key Terminology</strong></p>
<ul>
<li>
<strong>Factor</strong>: An explanatory or predictor variable studied in an experiment.</li>
<li>
<strong>Treatment (Factor Level)</strong>: A specific value or category of a factor applied to an experimental unit.</li>
<li>
<strong>Experimental Unit</strong>: The entity (e.g., person, animal, material) subjected to treatments and providing a response.</li>
<li>
<strong>Single-Factor Experiment</strong>: An experiment with only one explanatory variable.</li>
<li>
<strong>Multifactor Experiment</strong>: An experiment involving multiple explanatory variables.</li>
<li>
<strong>Classification Factor</strong>: A factor that is not controlled by the experimenter (common in observational studies).</li>
<li>
<strong>Experimental Factor</strong>: A factor that is directly assigned by the experimenter.</li>
</ul>
<p>A well-designed experiment requires careful planning in the following areas:</p>
<ul>
<li>
<strong>Choice of treatments</strong>: Selecting factor levels to be tested.</li>
<li>
<strong>Selection of experimental units</strong>: Ensuring an appropriate sample.</li>
<li>
<strong>Treatment assignment</strong>: Avoiding selection bias through proper randomization.</li>
<li>
<strong>Measurement</strong>: Minimizing measurement bias and considering blinding when necessary.</li>
</ul>
<p>Advancements in Experimental Design</p>
<ol style="list-style-type: decimal">
<li>
<strong>Factorial Experiments</strong>:
<ul>
<li>Investigate multiple factors simultaneously.</li>
<li>Allow for the study of <strong>interactions</strong> between factors.</li>
</ul>
</li>
<li>
<strong>Replication</strong>:
<ul>
<li>Repeating experiments increases statistical power.</li>
<li>Helps estimate <strong>mean squared error</strong>.</li>
</ul>
</li>
<li>
<strong>Randomization</strong>:
<ul>
<li>Introduced formally by R.A. Fisher in the early 1900s.</li>
<li>Ensures that treatment assignment is not systematically biased.</li>
<li>Helps eliminate confounding effects due to time, space, or other lurking variables.</li>
</ul>
</li>
<li>
<strong>Local Control (Blocking/Stratification)</strong>:
<ul>
<li>Reduces experimental error by controlling for known sources of variability.</li>
<li>Increases power by grouping similar experimental units together before randomizing treatments.</li>
</ul>
</li>
</ol>
<p>Randomization also helps eliminate correlations due to time and space.</p>
<hr>
<div id="sec-completely-randomized-design" class="section level2" number="24.1">
<h2>
<span class="header-section-number">24.1</span> Completely Randomized Design<a class="anchor" aria-label="anchor" href="#sec-completely-randomized-design"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>Completely Randomized Design (CRD)</strong> is the simplest type of experimental design, where experimental units are randomly assigned to treatments.</p>
<p>Consider a treatment factor <span class="math inline">\(A\)</span> with <span class="math inline">\(a \geq 2\)</span> treatment levels. Each experimental unit is randomly assigned to one of these levels. The number of units in each group can be:</p>
<ul>
<li>
<strong>Balanced</strong>: All groups have equal sample sizes <span class="math inline">\(n\)</span>.</li>
<li>
<strong>Unbalanced</strong>: Groups have different sample sizes <span class="math inline">\(n_i\)</span> (for <span class="math inline">\(i = 1, ..., a\)</span>).</li>
</ul>
<p>The total sample size is given by:</p>
<p><span class="math display">\[
N = \sum_{i=1}^{a} n_i
\]</span></p>
<p>The number of possible assignments of units to treatments is:</p>
<p><span class="math display">\[
k = \frac{N!}{n_1! n_2! \dots n_a!}
\]</span></p>
<p>Each assignment has an equal probability of being selected: <span class="math inline">\(1/k\)</span>. The response of each experimental unit is denoted as <span class="math inline">\(Y_{ij}\)</span>, where:</p>
<ul>
<li>
<span class="math inline">\(i\)</span> indexes the treatment group.</li>
<li>
<span class="math inline">\(j\)</span> indexes the individual unit within treatment <span class="math inline">\(i\)</span>.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<caption>Treatment Response Table</caption>
<thead><tr class="header">
<th>Treatment</th>
<th>1</th>
<th>2</th>
<th>…</th>
<th>a</th>
</tr></thead>
<tbody>
<tr class="odd">
<td></td>
<td><span class="math inline">\(Y_{11}\)</span></td>
<td><span class="math inline">\(Y_{21}\)</span></td>
<td>…</td>
<td><span class="math inline">\(Y_{a1}\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(Y_{12}\)</span></td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td></td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>Sample Mean</td>
<td><span class="math inline">\(\bar{Y_{1.}}\)</span></td>
<td><span class="math inline">\(\bar{Y_{2.}}\)</span></td>
<td>…</td>
<td><span class="math inline">\(\bar{Y_{a.}}\)</span></td>
</tr>
<tr class="odd">
<td>Sample SD</td>
<td><span class="math inline">\(s_1\)</span></td>
<td><span class="math inline">\(s_2\)</span></td>
<td>…</td>
<td><span class="math inline">\(s_a\)</span></td>
</tr>
</tbody>
</table></div>
<p>Where:</p>
<p><span class="math display">\[
\bar{Y_{i.}} = \frac{1}{n_i} \sum_{j=1}^{n_i} Y_{ij}
\]</span></p>
<p><span class="math display">\[
s_i^2 = \frac{1}{n_i - 1} \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y_{i.}})^2
\]</span></p>
<p>The <strong>grand mean</strong> is:</p>
<p><span class="math display">\[
\bar{Y_{..}} = \frac{1}{N} \sum_{i} \sum_{j} Y_{ij}
\]</span></p>
<hr>
<div id="sec-single-factor-fixed-effects-model" class="section level3" number="24.1.1">
<h3>
<span class="header-section-number">24.1.1</span> Single-Factor Fixed Effects ANOVA<a class="anchor" aria-label="anchor" href="#sec-single-factor-fixed-effects-model"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as <strong>One-Way ANOVA</strong> or <strong>ANOVA Type I Model</strong>.</p>
<p>The total variability in the response variable <span class="math inline">\(Y_{ij}\)</span> can be decomposed as follows:</p>
<p><span class="math display">\[
\begin{aligned}
Y_{ij} - \bar{Y_{..}} &amp;= Y_{ij} - \bar{Y}_{..} + \bar{Y}_{i.} - \bar{Y}_{i.} \\
&amp; = (\bar{Y_{i.}} - \bar{Y_{..}}) + (Y_{ij} - \bar{Y_{i.}})
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>The first term represents <strong>between-treatment variability</strong> (deviation of treatment means from the grand mean).</li>
<li>The second term represents <strong>within-treatment variability</strong> (deviation of observations from their treatment mean).</li>
</ul>
<p>Thus, we partition the <strong>total sum of squares (SSTO)</strong> as:</p>
<p><span class="math display">\[
\sum_{i} \sum_{j} (Y_{ij} - \bar{Y_{..}})^2 = \sum_{i} n_i (\bar{Y_{i.}} - \bar{Y_{..}})^2 + \sum_{i} \sum_{j} (Y_{ij} - \bar{Y_{i.}})^2
\]</span></p>
<p>Or equivalently:</p>
<p><span class="math display">\[
SSTO = SSTR + SSE
\]</span></p>
<p>Where:</p>
<ul>
<li>
<strong>SSTO (Total SS)</strong>: Total variability in the data.</li>
<li>
<strong>SSTR (Treatment SS)</strong>: Variability due to differences between treatment means.</li>
<li>
<strong>SSE (Error SS)</strong>: Variability within treatments (unexplained variance).</li>
</ul>
<p>Degrees of freedom (d.f.):</p>
<p><span class="math display">\[
(N-1) = (a-1) + (N-a)
\]</span></p>
<p>where we lose a degree of freedom for the total corrected SSTO because of the estimation of the mean (<span class="math inline">\(\sum_i \sum_j (Y_{ij} - \bar{Y}_{..} )= 0\)</span>) and for the SSTR (<span class="math inline">\(\sum_i n_i (\bar{Y}_{i.} - \bar{Y}_{..}) = 0\)</span>)</p>
<p>Mean squares:</p>
<p><span class="math display">\[
MSTR = \frac{SSTR}{a-1}, \quad MSR = \frac{SSE}{N-a}
\]</span></p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<caption>ANOVA Table</caption>
<colgroup>
<col width="26%">
<col width="44%">
<col width="12%">
<col width="14%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Between Treatments</td>
<td><span class="math inline">\(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\)</span></td>
<td><span class="math inline">\(a-1\)</span></td>
<td><span class="math inline">\(SSTR/(a-1)\)</span></td>
</tr>
<tr class="even">
<td>Error (within treatments)</td>
<td><span class="math inline">\(\sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2\)</span></td>
<td><span class="math inline">\(N-a\)</span></td>
<td><span class="math inline">\(SSE/(N-a)\)</span></td>
</tr>
<tr class="odd">
<td>Total (corrected)</td>
<td><span class="math inline">\(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\)</span></td>
<td><span class="math inline">\(N-1\)</span></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>For a linear model interpretation of ANOVA, we have either</p>
<ol style="list-style-type: decimal">
<li>Cell Means Model</li>
<li>Treatment Effect (Factor Effects Model)</li>
</ol>
<hr>
<div id="sec-cell-means-model" class="section level4" number="24.1.1.1">
<h4>
<span class="header-section-number">24.1.1.1</span> Cell Means Model<a class="anchor" aria-label="anchor" href="#sec-cell-means-model"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>cell means model</strong> describes the response as:</p>
<p><span class="math display">\[
Y_{ij} = \mu_i + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Y_{ij}\)</span>: Response for unit <span class="math inline">\(j\)</span> in treatment <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(\mu_i\)</span>: Fixed population mean for treatment <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2)\)</span>: Independent errors.</li>
<li>
<span class="math inline">\(E(Y_{ij}) = \mu_i\)</span>, <span class="math inline">\(\text{Var}(Y_{ij}) = \sigma^2\)</span>.</li>
</ul>
<p>All observations are assumed to have <strong>equal variance</strong> across treatments.</p>
<hr>
<p>Example: ANOVA with <span class="math inline">\(a = 3\)</span> Treatments</p>
<p>Consider a case with <strong>three treatments</strong> (<span class="math inline">\(a = 3\)</span>), where each treatment has <strong>two replicates</strong> (<span class="math inline">\(n_1 = n_2 = n_3 = 2\)</span>). The response vector can be expressed in matrix form as:</p>
<p><span class="math display">\[
\begin{aligned}
\left(\begin{array}{c}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &amp;=
\left(\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 \\
\end{array}\right)
\left(\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &amp;= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(X_{k,ij} = 1\)</span> if the <span class="math inline">\(k\)</span>-th treatment is applied to unit <span class="math inline">\((i,j)\)</span>.</li>
<li>
<span class="math inline">\(X_{k,ij} = 0\)</span> otherwise.</li>
</ul>
<p><strong>Note:</strong> There is <strong>no intercept</strong> term in this model.</p>
<p>The least squares estimator for <span class="math inline">\(\beta\)</span> is given by:</p>
<span class="math display" id="eq:betaorigin">\[\begin{equation}
\begin{aligned}
\mathbf{b}= \left[\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right] &amp;=
(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y} \\
&amp; =
\left[\begin{array}{ccc}
n_1 &amp; 0 &amp; 0\\
0 &amp; n_2 &amp; 0\\
0 &amp; 0 &amp; n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_1\\
Y_2\\
Y_3\\
\end{array}\right] \\
&amp; =
\left[\begin{array}{c}
\bar{Y_1}\\
\bar{Y_2}\\
\bar{Y_3}\\
\end{array}\right]
\end{aligned}
\tag{24.1}
\end{equation}\]</span>
<p>Thus, the estimated treatment means are:</p>
<p><span class="math display">\[
\hat{\mu}_i = \bar{Y_i}, \quad i = 1,2,3
\]</span></p>
<p>This estimator <span class="math inline">\(\mathbf{b} = [\bar{Y_1}, \bar{Y_2}, \bar{Y_3}]'\)</span> is the <strong>best linear unbiased estimator (BLUE)</strong> for <span class="math inline">\(\beta\)</span> (i.e., <span class="math inline">\(E(\mathbf{b}) = \beta\)</span>)</p>
<p>Since <span class="math inline">\(\mathbf{b} \sim N(\beta, \sigma^2 (\mathbf{X'X})^{-1})\)</span>, the variance of the estimated treatment means is:</p>
<p><span class="math display">\[
var(\mathbf{b}) = \sigma^2(\mathbf{X'X})^{-1} = \sigma^2
\left[\begin{array}{ccc}
1/n_1 &amp; 0 &amp; 0\\
0 &amp; 1/n_2 &amp; 0\\
0 &amp; 0 &amp; 1/n_3\\
\end{array}\right]
\]</span></p>
<p>Thus, the variance of each estimated treatment mean is:</p>
<p><span class="math display">\[
var(b_i) = var(\hat{\mu}_i) = \frac{\sigma^2}{n_i}, \quad i = 1,2,3
\]</span></p>
<p>The <strong>mean squared error (MSE)</strong> is given by:</p>
<p><span class="math display">\[
\begin{aligned}
MSE
&amp;= \frac{1}{N - a} \sum_{i=1}^a \sum_{j=1}^{n_i} \bigl(Y_{ij} - \overline{Y}_{i\cdot}\bigr)^2
\\[6pt]
&amp;= \frac{1}{N - a}
   \sum_{i=1}^a
   \Bigl[
     (n_i - 1) \;
     \underbrace{
       \frac{1}{n_i - 1}
       \sum_{j=1}^{n_i}
         \bigl(Y_{ij} - \overline{Y}_{i\cdot}\bigr)^2
     }_{=\,s_i^2}
   \Bigr]
\\[6pt]
&amp;= \frac{1}{N - a} \sum_{i=1}^a (n_i - 1)\, s_i^2.
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(s_i^2\)</span> is the sample variance within the <span class="math inline">\(i\)</span>-th treatment group.</p>
<p>Since <span class="math inline">\(E(s_i^2) = \sigma^2\)</span>, we get:</p>
<p><span class="math display">\[
E(MSE) = \frac{1}{N-a} \sum_{i} (n_i-1) \sigma^2 = \sigma^2
\]</span></p>
<p>Thus, <strong>MSE is an unbiased estimator of</strong> <span class="math inline">\(\sigma^2\)</span>, regardless of whether the treatment means are equal.</p>
<hr>
<p>The expected mean square for treatments (MSTR) is:</p>
<p><span class="math display">\[
E(MSTR) = \sigma^2 + \frac{\sum_{i} n_i (\mu_i - \mu_.)^2}{a-1}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mu_. = \frac{\sum_{i=1}^{a} n_i \mu_i}{\sum_{i=1}^{a} n_i}
\]</span></p>
<p>If all treatment means are equal (<span class="math inline">\(\mu_1 = \mu_2 = \dots = \mu_a = \mu_.\)</span>), then:</p>
<p><span class="math display">\[
E(MSTR) = \sigma^2
\]</span></p>
<hr>
<p><span class="math inline">\(F\)</span>-Test for Equality of Treatment Means</p>
<p>We test the null hypothesis:</p>
<p><span class="math display">\[
H_0: \mu_1 = \mu_2 = \dots = \mu_a
\]</span></p>
<p>against the alternative:</p>
<p><span class="math display">\[
H_a: \text{at least one } \mu_i \text{ differs}
\]</span></p>
<p>The <strong>test statistic</strong> is:</p>
<p><span class="math display">\[
F = \frac{MSTR}{MSE}
\]</span></p>
<ul>
<li>Large values of <span class="math inline">\(F\)</span> suggest rejecting <span class="math inline">\(H_0\)</span> (since MSTR will be larger than MSE when <span class="math inline">\(H_a\)</span> is true).</li>
<li>Values of <span class="math inline">\(F\)</span> near 1 suggest that we fail to reject <span class="math inline">\(H_0\)</span>.</li>
</ul>
<p>Since <span class="math inline">\(MSTR\)</span> and <span class="math inline">\(MSE\)</span> are independent chi-square random variables scaled by their degrees of freedom, under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
F \sim F_{(a-1, N-a)}
\]</span></p>
<p>Decision Rule:</p>
<ul>
<li>If <span class="math inline">\(F \leq F_{(a-1, N-a;1-\alpha)}\)</span>, <strong>fail to reject</strong> <span class="math inline">\(H_0\)</span>.</li>
<li>If <span class="math inline">\(F \geq F_{(a-1, N-a;1-\alpha)}\)</span>, <strong>reject</strong> <span class="math inline">\(H_0\)</span>.</li>
</ul>
<hr>
<p>If there are only two treatments (<span class="math inline">\(a = 2\)</span>), the ANOVA <span class="math inline">\(F\)</span>-test <strong>reduces to the two-sample</strong> <span class="math inline">\(t\)</span>-test:</p>
<p><span class="math display">\[
F = t^2
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
t = \frac{\bar{Y_1} - \bar{Y_2}}{\sqrt{MSE \left(\frac{1}{n_1} + \frac{1}{n_2} \right)}}
\]</span></p>
<hr>
</div>
<div id="treatment-effects-factor-effects" class="section level4" number="24.1.1.2">
<h4>
<span class="header-section-number">24.1.1.2</span> Treatment Effects (Factor Effects)<a class="anchor" aria-label="anchor" href="#treatment-effects-factor-effects"><i class="fas fa-link"></i></a>
</h4>
<p>Besides <a href="sec-analysis-of-variance-anova.html#sec-cell-means-model">cell means model</a>, we have another way to formalize one-way ANOVA:</p>
<p><span class="math display">\[Y_{ij} = \mu + \tau_i + \epsilon_{ij}\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Y_{ij}\)</span> is the <span class="math inline">\(j\)</span>-th response for the <span class="math inline">\(i\)</span>-th treatment.</li>
<li>
<span class="math inline">\(\tau_i\)</span> is the <span class="math inline">\(i\)</span>-th treatment effect.</li>
<li>
<span class="math inline">\(\mu\)</span> is the constant component common to all observations.</li>
<li>
<span class="math inline">\(\epsilon_{ij}\)</span> are independent random errors, assumed to be normally distributed: <span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2)\)</span>.</li>
</ul>
<p>For example, if we have <span class="math inline">\(a = 3\)</span> treatments and <span class="math inline">\(n_1 = n_2 = n_3 = 2\)</span> observations per treatment, the model representation is:</p>
<span class="math display" id="eq:unsolvable">\[\begin{equation}
\begin{aligned}
\left(\begin{array}{c}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &amp;=
\left(\begin{array}{cccc}
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 1 \\
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\tau_3\\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &amp;= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\tag{24.2}
\end{equation}\]</span>
<p>However, the matrix:</p>
<p><span class="math display">\[
\mathbf{X'X} =
\left(
\begin{array}
{cccc}
\sum_{i}n_i &amp; n_1 &amp; n_2 &amp; n_3 \\
n_1 &amp; n_1 &amp; 0 &amp; 0 \\
n_2 &amp; 0 &amp; n_2 &amp; 0 \\
n_3 &amp; 0 &amp; 0 &amp; n_3 \\
\end{array}
\right)
\]</span></p>
<p>is <strong>singular</strong>, meaning <span class="math inline">\(\mathbf{X'X}\)</span> is not invertible. This results in an infinite number of possible solutions for <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p>To resolve this, we impose restrictions on the parameters to ensure that <span class="math inline">\(\mathbf{X}\)</span> has full rank. Regardless of the restriction used, the expected value remains:</p>
<p><span class="math display">\[
E(Y_{ij}) = \mu + \tau_i = \mu_i = \text{mean response for the $i$-th treatment}
\]</span></p>
<hr>
<div id="restriction-on-sum-of-tau" class="section level5" number="24.1.1.2.1">
<h5>
<span class="header-section-number">24.1.1.2.1</span> Restriction on Sum of Treatment Effects<a class="anchor" aria-label="anchor" href="#restriction-on-sum-of-tau"><i class="fas fa-link"></i></a>
</h5>
<p>One common restriction is:</p>
<p><span class="math display">\[
\sum_{i=1}^{a} \tau_i = 0
\]</span></p>
<p>which implies that:</p>
<p><span class="math display">\[
\mu = \frac{1}{a} \sum_{i=1}^{a} (\mu + \tau_i)
\]</span></p>
<p>meaning that <span class="math inline">\(\mu\)</span> represents the grand mean (the overall mean response across treatments).</p>
<p>Each treatment effect can then be expressed as:</p>
<p><span class="math display">\[
\begin{aligned}
\tau_i &amp;= \mu_i - \mu \\
&amp;= \text{treatment mean} - \text{grand mean}
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\sum_{i} \tau_i = 0\)</span>, we can solve for <span class="math inline">\(\tau_a\)</span> as:</p>
<p><span class="math display">\[
\tau_a = -(\tau_1 + \tau_2 + \dots + \tau_{a-1})
\]</span></p>
<p>Thus, the mean for the <span class="math inline">\(a\)</span>-th treatment is:</p>
<p><span class="math display">\[
\mu_a = \mu + \tau_a = \mu - (\tau_1 + \tau_2 + \dots + \tau_{a-1})
\]</span></p>
<p>This reduces the number of parameters from <span class="math inline">\(a + 1\)</span> to just <span class="math inline">\(a\)</span>, meaning we estimate:</p>
<p><span class="math display">\[
\mu, \tau_1, \tau_2, ..., \tau_{a-1}
\]</span></p>
<p>Rewriting Equation <a href="sec-analysis-of-variance-anova.html#eq:unsolvable">(24.2)</a>:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\left(\begin{array}{c}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &amp;=
\left(\begin{array}{ccc}
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; -1 &amp; -1 \\
1 &amp; -1 &amp; -1 \\
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &amp;= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\end{equation}\]</span>
<p>where <span class="math inline">\(\beta = [\mu, \tau_1, \tau_2]'\)</span>.</p>
<hr>
</div>
<div id="restriction-on-first-tau" class="section level5" number="24.1.1.2.2">
<h5>
<span class="header-section-number">24.1.1.2.2</span> Restriction on the First <span class="math inline">\(\tau\)</span><a class="anchor" aria-label="anchor" href="#restriction-on-first-tau"><i class="fas fa-link"></i></a>
</h5>
<p>In <strong>R</strong>, the default parameterization in <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> for a one-way ANOVA model sets <span class="math inline">\(\tau_1 = 0\)</span>. This effectively chooses the first treatment (or group) as a baseline or reference, making its treatment effect <span class="math inline">\(\tau_1\)</span> equal to zero.</p>
<p>Consider the last example with three treatments, each having two observations, <span class="math inline">\(\,n_1 = n_2 = n_3 = 2\)</span>. Under the restriction <span class="math inline">\(\tau_1 = 0\)</span>, the treatment means can be expressed as:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_1 &amp;= \mu + \tau_1 \;=\; \mu + 0 \;=\; \mu, \\
\mu_2 &amp;= \mu + \tau_2, \\
\mu_3 &amp;= \mu + \tau_3.
\end{aligned}
\]</span></p>
<p>Hence, <span class="math inline">\(\mu\)</span> becomes the mean response for the <strong>first</strong> treatment.</p>
<p>We write the observations in vector form:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y}
&amp;= \begin{pmatrix}
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{pmatrix}
=
\underbrace{
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
\end{pmatrix}
}_{\mathbf{X}}
\begin{pmatrix}
\mu \\
\tau_2 \\
\tau_3 \\
\end{pmatrix}
+
\begin{pmatrix}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{pmatrix} \\
&amp;= \mathbf{X\beta} + \mathbf{\epsilon},
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\beta =
\begin{pmatrix}
\mu \\
\tau_2 \\
\tau_3
\end{pmatrix}.
\]</span></p>
<p>The OLS estimator is:</p>
<p><span class="math display">\[
\mathbf{b}
=
\begin{pmatrix}
\hat{\mu} \\
\hat{\tau_2} \\
\hat{\tau_3}
\end{pmatrix}
= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{y}.
\]</span></p>
<p>In our specific case with equal sample sizes (<span class="math inline">\(n_1=n_2=n_3=2\)</span>), the <span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}\)</span> calculation yields:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{b}
&amp; =     \left[\begin{array}{ccc}          \sum_{i}n_i &amp; n_2 &amp; n_3\\          n_2 &amp; n_2 &amp; 0\\          n_3 &amp; 0 &amp; n_3 \\          \end{array}\right]^{-1}\left[\begin{array}{c}      Y_{..}\\      Y_{2.}\\      Y_{3.}\\      \end{array}\right] \\
&amp;=
\begin{pmatrix}
\bar{Y}_{1\cdot} \\
\bar{Y}_{2\cdot} - \bar{Y}_{1\cdot} \\
\bar{Y}_{3\cdot} - \bar{Y}_{1\cdot}
\end{pmatrix}
\end{aligned}
\]</span> where <span class="math inline">\(\bar{Y}_{1\cdot}\)</span>, <span class="math inline">\(\bar{Y}_{2\cdot}\)</span>, and <span class="math inline">\(\bar{Y}_{3\cdot}\)</span> are the sample means for treatments 1, 2, and 3, respectively.</p>
<p>Taking the expectation of <span class="math inline">\(\mathbf{b}\)</span> confirms:</p>
<p><span class="math display">\[
E(\mathbf{b})
=
\beta
=
\begin{pmatrix}
\mu \\
\tau_2 \\
\tau_3
\end{pmatrix}
=
\begin{pmatrix}
\mu_1 \\
\mu_2 - \mu_1 \\
\mu_3 - \mu_1
\end{pmatrix}.
\]</span></p>
<p>Recall that:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{b})
=
\sigma^2\,(\mathbf{X}'\mathbf{X})^{-1}.
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\mu})
&amp;= \text{Var}(\bar{Y}_{1\cdot})
= \frac{\sigma^2}{n_1}, \\[6pt]
\text{Var}(\hat{\tau_2})
&amp;= \text{Var}\bigl(\bar{Y}_{2\cdot}-\bar{Y}_{1\cdot}\bigr)
= \frac{\sigma^2}{n_2} + \frac{\sigma^2}{n_1}, \\[6pt]
\text{Var}(\hat{\tau_3})
&amp;= \text{Var}\bigl(\bar{Y}_{3\cdot}-\bar{Y}_{1\cdot}\bigr)
= \frac{\sigma^2}{n_3} + \frac{\sigma^2}{n_1}.
\end{aligned}
\]</span></p>
<hr>
</div>
</div>
<div id="equivalence-of-parameterizations" class="section level4" number="24.1.1.3">
<h4>
<span class="header-section-number">24.1.1.3</span> Equivalence of Parameterizations<a class="anchor" aria-label="anchor" href="#equivalence-of-parameterizations"><i class="fas fa-link"></i></a>
</h4>
<p>Despite having different ways of writing the model, all three parameterizations yield the <strong>same</strong> ANOVA table:</p>
<ol style="list-style-type: decimal">
<li>
<a href="sec-analysis-of-variance-anova.html#sec-cell-means-model">Model 1</a>: <span class="math inline">\(Y_{ij} = \mu_i + \epsilon_{ij}\)</span>.</li>
<li>
<a href="sec-analysis-of-variance-anova.html#restriction-on-sum-of-tau">Model 2</a>: <span class="math inline">\(Y_{ij} = \mu + \tau_i + \epsilon_{ij}\)</span> where <span class="math inline">\(\sum_i \tau_i = 0\)</span>.</li>
<li>
<a href="sec-analysis-of-variance-anova.html#restriction-on-first-tau">Model 3</a>: <span class="math inline">\(Y_{ij} = \mu + \tau_i + \epsilon_{ij}\)</span> where <span class="math inline">\(\tau_1 = 0\)</span>.</li>
</ol>
<p>All three lead to the same fitted values, because</p>
<p><span class="math display">\[
\mathbf{\hat{Y}} = \mathbf{X}\bigl(\mathbf{X}'\mathbf{X}\bigr)^{-1}\mathbf{X}'\mathbf{Y}
= \mathbf{P\,Y}
= \mathbf{X\,b}.
\]</span></p>
<hr>
</div>
<div id="anova-table" class="section level4" number="24.1.1.4">
<h4>
<span class="header-section-number">24.1.1.4</span> ANOVA Table<a class="anchor" aria-label="anchor" href="#anova-table"><i class="fas fa-link"></i></a>
</h4>
<p>The generic form of the ANOVA table is:</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="11%">
<col width="63%">
<col width="3%">
<col width="9%">
<col width="9%">
</colgroup>
<thead><tr class="header">
<th>Source of Variation</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>F</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Between Treatments</strong></td>
<td><span class="math inline">\(\sum_{i} n_i (\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot})^2 \;=\; \mathbf{Y}'(\mathbf{P} - \mathbf{P}_1)\mathbf{Y}\)</span></td>
<td><span class="math inline">\(a-1\)</span></td>
<td><span class="math inline">\(\frac{SSTR}{a-1}\)</span></td>
<td><span class="math inline">\(\frac{MSTR}{MSE}\)</span></td>
</tr>
<tr class="even">
<td>
<p><strong>Error</strong></p>
<p><strong>(within treatments)</strong></p>
</td>
<td><span class="math inline">\(\sum_{i}\sum_{j}\bigl(Y_{ij} - \overline{Y}_{i\cdot}\bigr)^2 \;=\; \mathbf{e}'\mathbf{e}\)</span></td>
<td><span class="math inline">\(N-a\)</span></td>
<td><span class="math inline">\(\frac{SSE}{N-a}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Total (corrected)</strong></td>
<td><span class="math inline">\(\sum_{i} n_i(\overline{Y}_{i\cdot} - \overline{Y}_{\cdot\cdot})^2 \;=\; \mathbf{Y}'\mathbf{Y} \;-\; \mathbf{Y}'\mathbf{P}_1\mathbf{Y}\)</span></td>
<td><span class="math inline">\(N-1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>where <span class="math inline">\(\mathbf{P}_1 = \frac{1}{n}\mathbf{J}\)</span>, <span class="math inline">\(n = \sum_i n_i\)</span>, and <span class="math inline">\(\mathbf{J}\)</span> is the all-ones matrix.</p>
<p>The <span class="math inline">\(F\)</span>-statistic has <span class="math inline">\((a-1, N-a)\)</span> degrees of freedom and the numeric value is unchanged under any of the three parameterizations. The slight difference lies in how we state the null hypothesis:</p>
<p><span class="math display">\[
\begin{aligned}
H_0 &amp;: \mu_1 = \mu_2 = \dots = \mu_a, \\
H_0 &amp;: \mu + \tau_1 = \mu + \tau_2 = \dots = \mu + \tau_a, \\
H_0 &amp;: \tau_1 = \tau_2 = \dots = \tau_a.
\end{aligned}
\]</span></p>
<p>The <span class="math inline">\(F\)</span>-test here serves as a preliminary analysis, to see if there is any difference at different factors. For more in-depth analysis, we consider different testing of treatment effects.</p>
</div>
<div id="testing-of-treatment-effects" class="section level4" number="24.1.1.5">
<h4>
<span class="header-section-number">24.1.1.5</span> Testing of Treatment Effects<a class="anchor" aria-label="anchor" href="#testing-of-treatment-effects"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<a href="sec-analysis-of-variance-anova.html#sec-single-treatment-mean-anova">Single Treatment Mean</a> <span class="math inline">\(\mu_i\)</span>
</li>
<li><a href="sec-analysis-of-variance-anova.html#sec-differences-between-treatment-means-anova">Differences Between Treatment Means</a></li>
<li><a href="sec-analysis-of-variance-anova.html#sec-contrast-among-treatment-means-anova">Contrast Among Treatment Means</a></li>
<li><a href="sec-analysis-of-variance-anova.html#sec-linear-combination-of-treatment-means-anova">Linear Combination of Treatment Means</a></li>
</ul>
<div id="sec-single-treatment-mean-anova" class="section level5" number="24.1.1.5.1">
<h5>
<span class="header-section-number">24.1.1.5.1</span> Single Treatment Mean<a class="anchor" aria-label="anchor" href="#sec-single-treatment-mean-anova"><i class="fas fa-link"></i></a>
</h5>
<p>For a single treatment group, the sample mean serves as an estimate of the population mean:</p>
<p><span class="math display">\[
\hat{\mu_i} = \bar{Y}_{i.}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(E(\bar{Y}_{i.}) = \mu_i\)</span>, indicating unbiasedness.</li>
<li>
<span class="math inline">\(var(\bar{Y}_{i.}) = \frac{\sigma^2}{n_i}\)</span>, estimated by <span class="math inline">\(s^2(\bar{Y}_{i.}) = \frac{MSE}{n_i}\)</span>.</li>
</ul>
<p>Since the standardized test statistic</p>
<p><span class="math display">\[
T = \frac{\bar{Y}_{i.} - \mu_i}{s(\bar{Y}_{i.})}
\]</span></p>
<p>follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(N-a\)</span> degrees of freedom (<span class="math inline">\(t_{N-a}\)</span>), a <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(\mu_i\)</span> is:</p>
<p><span class="math display">\[
\bar{Y}_{i.} \pm t_{1-\alpha/2;N-a} s(\bar{Y}_{i.})
\]</span></p>
<p>To test whether <span class="math inline">\(\mu_i\)</span> is equal to some constant <span class="math inline">\(c\)</span>, we set up the hypothesis:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \mu_i = c \\
&amp;H_1: \mu_i \neq c
\end{aligned}
\]</span></p>
<p>The test statistic:</p>
<p><span class="math display">\[
T = \frac{\bar{Y}_{i.} - c}{s(\bar{Y}_{i.})} \sim t_{N-a}
\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>, we reject <span class="math inline">\(H_0\)</span> at the <span class="math inline">\(\alpha\)</span> level if:</p>
<p><span class="math display">\[
|T| &gt; t_{1-\alpha/2;N-a}
\]</span></p>
</div>
<div id="sec-differences-between-treatment-means-anova" class="section level5" number="24.1.1.5.2">
<h5>
<span class="header-section-number">24.1.1.5.2</span> Differences Between Treatment Means<a class="anchor" aria-label="anchor" href="#sec-differences-between-treatment-means-anova"><i class="fas fa-link"></i></a>
</h5>
<p>The difference between two treatment means, also called a <strong>pairwise comparison</strong>, is given by:</p>
<p><span class="math display">\[
D = \mu_i - \mu_{i'}
\]</span></p>
<p>which is estimated by:</p>
<p><span class="math display">\[
\hat{D} = \bar{Y}_{i.} - \bar{Y}_{i'.}
\]</span></p>
<p>This estimate is unbiased since:</p>
<p><span class="math display">\[
E(\hat{D}) = \mu_i - \mu_{i'}
\]</span></p>
<p>Since <span class="math inline">\(\bar{Y}_{i.}\)</span> and <span class="math inline">\(\bar{Y}_{i'.}\)</span> are independent, the variance of <span class="math inline">\(\hat{D}\)</span> is:</p>
<p><span class="math display">\[
var(\hat{D}) = var(\bar{Y}_{i.}) + var(\bar{Y}_{i'.}) = \sigma^2 \left(\frac{1}{n_i} + \frac{1}{n_{i'}}\right)
\]</span></p>
<p>which is estimated by:</p>
<p><span class="math display">\[
s^2(\hat{D}) = MSE \left(\frac{1}{n_i} + \frac{1}{n_{i'}}\right)
\]</span></p>
<p>Using the same inference structure as the <a href="sec-analysis-of-variance-anova.html#sec-single-treatment-mean-anova">single treatment mean</a>:</p>
<p><span class="math display">\[
\frac{\hat{D} - D}{s(\hat{D})} \sim t_{N-a}
\]</span></p>
<p>A <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(D\)</span> is:</p>
<p><span class="math display">\[
\hat{D} \pm t_{1-\alpha/2;N-a} s(\hat{D})
\]</span></p>
<p>For hypothesis testing:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \mu_i = \mu_{i'} \\
&amp;H_a: \mu_i \neq \mu_{i'}
\end{aligned}
\]</span></p>
<p>we use the test statistic:</p>
<p><span class="math display">\[
T = \frac{\hat{D}}{s(\hat{D})} \sim t_{N-a}
\]</span></p>
<p>We reject <span class="math inline">\(H_0\)</span> at the <span class="math inline">\(\alpha\)</span> level if:</p>
<p><span class="math display">\[
|T| &gt; t_{1-\alpha/2;N-a}
\]</span></p>
</div>
<div id="sec-contrast-among-treatment-means-anova" class="section level5" number="24.1.1.5.3">
<h5>
<span class="header-section-number">24.1.1.5.3</span> Contrast Among Treatment Means<a class="anchor" aria-label="anchor" href="#sec-contrast-among-treatment-means-anova"><i class="fas fa-link"></i></a>
</h5>
<p>To generalize the comparison of two means, we introduce <strong>contrasts</strong>.</p>
<p>A <strong>contrast</strong> is a linear combination of treatment means:</p>
<p><span class="math display">\[
L = \sum_{i=1}^{a} c_i \mu_i
\]</span></p>
<p>where the coefficients <span class="math inline">\(c_i\)</span> are non-random constants that satisfy the constraint:</p>
<p><span class="math display">\[
\sum_{i=1}^{a} c_i = 0
\]</span></p>
<p>This ensures that contrasts focus on relative comparisons rather than absolute magnitudes.</p>
<p>An unbiased estimator of <span class="math inline">\(L\)</span> is given by:</p>
<p><span class="math display">\[
\hat{L} = \sum_{i=1}^{a} c_i \bar{Y}_{i.}
\]</span></p>
<p>Since expectation is a linear operator:</p>
<p><span class="math display">\[
E(\hat{L}) = \sum_{i=1}^{a} c_i E(\bar{Y}_{i.}) = \sum_{i=1}^{a} c_i \mu_i = L
\]</span></p>
<p>Thus, <span class="math inline">\(\hat{L}\)</span> is an unbiased estimator of <span class="math inline">\(L\)</span>.</p>
<p>Since the sample means <span class="math inline">\(\bar{Y}_{i.}\)</span> are independent, the variance of <span class="math inline">\(\hat{L}\)</span> is:</p>
<p><span class="math display">\[
\begin{aligned}
var(\hat{L}) &amp;= var\left(\sum_{i=1}^a c_i \bar{Y}_{i.} \right) \\
&amp;= \sum_{i=1}^a c_i^2 var(\bar{Y}_{i.}) \\
&amp;= \sum_{i=1}^a c_i^2 \frac{\sigma^2}{n_i} \\
&amp;= \sigma^2 \sum_{i=1}^{a} \frac{c_i^2}{n_i}
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\sigma^2\)</span> is unknown, we estimate it using the mean squared error:</p>
<p><span class="math display">\[
s^2(\hat{L}) = MSE \sum_{i=1}^{a} \frac{c_i^2}{n_i}
\]</span></p>
<p>Since <span class="math inline">\(\hat{L}\)</span> is a linear combination of independent normal random variables, it follows a normal distribution:</p>
<p><span class="math display">\[
\hat{L} \sim N\left(L, \sigma^2 \sum_{i=1}^{a} \frac{c_i^2}{n_i} \right)
\]</span></p>
<p>Since <span class="math inline">\(SSE/\sigma^2 \sim \chi^2_{N-a}\)</span> and <span class="math inline">\(MSE = SSE/(N-a)\)</span>, we use the <span class="math inline">\(t\)</span>-distribution:</p>
<p><span class="math display">\[
\frac{\hat{L} - L}{s(\hat{L})} \sim t_{N-a}
\]</span></p>
<p>Thus, a <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(L\)</span> is:</p>
<p><span class="math display">\[
\hat{L} \pm t_{1-\alpha/2; N-a} s(\hat{L})
\]</span></p>
<p>To test whether a specific contrast equals zero:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: L = 0 \quad \text{(no difference in the contrast)} \\
&amp;H_a: L \neq 0 \quad \text{(significant contrast)}
\end{aligned}
\]</span></p>
<p>We use the test statistic:</p>
<p><span class="math display">\[
T = \frac{\hat{L}}{s(\hat{L})} \sim t_{N-a}
\]</span></p>
<p>We reject <span class="math inline">\(H_0\)</span> at the <span class="math inline">\(\alpha\)</span> level if:</p>
<p><span class="math display">\[
|T| &gt; t_{1-\alpha/2;N-a}
\]</span></p>
</div>
<div id="sec-linear-combination-of-treatment-means-anova" class="section level5" number="24.1.1.5.4">
<h5>
<span class="header-section-number">24.1.1.5.4</span> Linear Combination of Treatment Means<a class="anchor" aria-label="anchor" href="#sec-linear-combination-of-treatment-means-anova"><i class="fas fa-link"></i></a>
</h5>
<p>A <strong>linear combination</strong> of treatment means extends the idea of a contrast:</p>
<p><span class="math display">\[
L = \sum_{i=1}^{a} c_i \mu_i
\]</span></p>
<p>Unlike contrasts, there are <strong>no restrictions</strong> on the coefficients <span class="math inline">\(c_i\)</span> (i.e., they do not need to sum to zero).</p>
<p>Since tests on a <a href="sec-analysis-of-variance-anova.html#sec-single-treatment-mean-anova">single treatment mean</a>, <a href="sec-analysis-of-variance-anova.html#sec-differences-between-treatment-means-anova">pairwise differences</a>, and <a href="sec-analysis-of-variance-anova.html#sec-contrast-among-treatment-means-anova">contrasts</a> are all special cases of this general form, we can express the hypothesis test as:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \sum_{i=1}^{a} c_i \mu_i = c \\
&amp;H_a: \sum_{i=1}^{a} c_i \mu_i \neq c
\end{aligned}
\]</span></p>
<p>The test statistic follows a <span class="math inline">\(t\)</span>-distribution:</p>
<p><span class="math display">\[
T = \frac{\hat{L} - c}{s(\hat{L})} \sim t_{N-a}
\]</span></p>
<p>Since squaring a <span class="math inline">\(t\)</span>-distributed variable results in an <span class="math inline">\(F\)</span>-distributed variable,</p>
<p><span class="math display">\[
F = T^2 \sim F_{1,N-a}
\]</span></p>
<p>This means that all such tests can be viewed as <strong>single-degree-of-freedom</strong> <span class="math inline">\(F\)</span>-tests, since the numerator degrees of freedom is always 1.</p>
<hr>
<p><strong>Multiple Contrasts</strong></p>
<p>When testing <span class="math inline">\(k \geq 2\)</span> contrasts simultaneously, the test statistics <span class="math inline">\(T_1, T_2, ..., T_k\)</span> follow a multivariate <span class="math inline">\(t\)</span>-distribution, since they are dependent (as they are based on the same data).</p>
<p><strong>Limitations of Multiple Comparisons</strong></p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Inflation of Type I Error</strong>:<br>
The confidence coefficient <span class="math inline">\((1-\alpha)\)</span> applies to a single estimate, not a series of estimates. Similarly, the Type I error rate <span class="math inline">\(\alpha\)</span> applies to an individual test, not a collection of tests.</p>
<p><strong>Example:</strong> If three <span class="math inline">\(t\)</span>-tests are performed at <span class="math inline">\(\alpha = 0.05\)</span>, and if they were independent (which they are not), then:</p>
<p><span class="math display">\[
(1 - 0.05)^3 = 0.857
\]</span></p>
<p>meaning the overall Type I error rate would be approximately <span class="math inline">\(0.143\)</span>, <strong>not</strong> <span class="math inline">\(0.05\)</span>.</p>
</li>
<li>
<p><strong>Data Snooping Concern</strong>:<br>
The significance level <span class="math inline">\(\alpha\)</span> is valid <strong>only if</strong> the test was planned <strong>before</strong> examining the data.</p>
<ul>
<li>Often, an experiment suggests relationships to investigate.</li>
<li>Exploring effects based on observed data is known as <strong>data snooping</strong>.</li>
</ul>
</li>
</ol>
<p>To address these issues, we use <strong>Multiple Comparison Procedures</strong>, such as:</p>
<ul>
<li>Tukey – for all pairwise comparisons of treatment means.</li>
<li>Scheffé – for all possible contrasts.</li>
<li>Bonferroni – for a fixed number of planned comparisons.</li>
</ul>
<hr>
<div id="sec-tukeys-anova" class="section level6" number="24.1.1.5.4.1">
<h6>
<span class="header-section-number">24.1.1.5.4.1</span> Tukey<a class="anchor" aria-label="anchor" href="#sec-tukeys-anova"><i class="fas fa-link"></i></a>
</h6>
<p>Used for all pairwise comparisons of treatment means:</p>
<p><span class="math display">\[
D = \mu_i - \mu_{i'}
\]</span></p>
<p>Hypothesis test:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \mu_i - \mu_{i'} = 0 \\
&amp;H_a: \mu_i - \mu_{i'} \neq 0
\end{aligned}
\]</span></p>
<p>Properties:</p>
<ul>
<li>When sample sizes are equal (<span class="math inline">\(n_1 = n_2 = ... = n_a\)</span>), the family confidence coefficient is exactly <span class="math inline">\((1-\alpha)\)</span>.</li>
<li>When sample sizes are unequal, the method is conservative (i.e., the actual significance level is less than <span class="math inline">\(\alpha\)</span>).</li>
</ul>
<p>The Tukey test is based on the <strong>studentized range</strong>:</p>
<p><span class="math display">\[
w = \max(Y_i) - \min(Y_i)
\]</span></p>
<p>If <span class="math inline">\(Y_1, ..., Y_r\)</span> are observations from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the statistic:</p>
<p><span class="math display">\[
q(r, v) = \frac{w}{s}
\]</span></p>
<p>follows the studentized range distribution, which requires a special table.</p>
<p><strong>Notes:</strong></p>
<ul>
<li><p>When testing only a subset of pairwise comparisons, the confidence coefficient exceeds <span class="math inline">\((1-\alpha)\)</span>, making the test more conservative.</p></li>
<li><p>Tukey’s method can be used for data snooping, as long as the investigated effects are pairwise comparisons.</p></li>
</ul>
<hr>
</div>
<div id="sec-scheffe-anova" class="section level6" number="24.1.1.5.4.2">
<h6>
<span class="header-section-number">24.1.1.5.4.2</span> Scheffé<a class="anchor" aria-label="anchor" href="#sec-scheffe-anova"><i class="fas fa-link"></i></a>
</h6>
<p>Scheffé’s method is used for testing <strong>all possible contrasts</strong>:</p>
<p><span class="math display">\[
L = \sum_{i=1}^{a} c_i \mu_i, \quad \text{where} \quad \sum_{i=1}^{a} c_i = 0
\]</span></p>
<p>Hypothesis test:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: L = 0 \\
&amp;H_a: L \neq 0
\end{aligned}
\]</span></p>
<p>Properties:</p>
<ul>
<li>Valid for any set of contrasts, making it the most general multiple comparison procedure.</li>
<li>The family confidence level is exactly <span class="math inline">\((1-\alpha)\)</span>, regardless of sample sizes.</li>
</ul>
<p>Simultaneous Confidence Intervals:</p>
<p><span class="math display">\[
\hat{L} \pm S s(\hat{L})
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat{L} = \sum c_i \bar{Y}_{i.}\)</span></li>
<li><span class="math inline">\(s^2(\hat{L}) = MSE \sum \frac{c_i^2}{n_i}\)</span></li>
<li><span class="math inline">\(S^2 = (a-1) f_{1-\alpha; a-1, N-a}\)</span></li>
</ul>
<p>Test Statistic:</p>
<p><span class="math display">\[
F = \frac{\hat{L}^2}{(a-1) s^2(\hat{L})}
\]</span></p>
<p>We reject <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
F &gt; f_{1-\alpha; a-1, N-a}
\]</span></p>
<p>Notes:</p>
<ul>
<li>Finite Family Correction: Since we never test all possible contrasts in practice, the actual confidence coefficient is greater than <span class="math inline">\((1-\alpha)\)</span>. Thus, some researchers use a higher <span class="math inline">\(\alpha\)</span> (e.g., a 90% confidence level instead of 95%).</li>
<li>Scheffé is useful for data snooping, since it applies to any contrast.</li>
<li>If only pairwise comparisons are needed, Tukey’s method gives narrower confidence intervals than Scheffé.</li>
</ul>
<hr>
</div>
<div id="sec-bonferroni-anova" class="section level6" number="24.1.1.5.4.3">
<h6>
<span class="header-section-number">24.1.1.5.4.3</span> Bonferroni<a class="anchor" aria-label="anchor" href="#sec-bonferroni-anova"><i class="fas fa-link"></i></a>
</h6>
<p>The Bonferroni correction is applicable regardless of whether sample sizes are equal or unequal. It is particularly useful when a small number of planned comparisons are of interest.</p>
<p>A <span class="math inline">\((1-\alpha)100\%\)</span> simultaneous confidence interval for a set of <span class="math inline">\(g\)</span> comparisons is:</p>
<p><span class="math display">\[
\hat{L} \pm B s(\hat{L})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
B = t_{1-\alpha/(2g), N-a}
\]</span></p>
<p>and <span class="math inline">\(g\)</span> is the <strong>number of comparisons</strong> in the family.</p>
<p>To test:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: L = 0 \\
&amp;H_a: L \neq 0
\end{aligned}
\]</span></p>
<p>we use the test statistic:</p>
<p><span class="math display">\[
T = \frac{\hat{L}}{s(\hat{L})}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
|T| &gt; t_{1-\alpha/(2g),N-a}
\]</span></p>
<p>Notes:</p>
<ul>
<li>If all pairwise comparisons are needed, Tukey’s method is superior, as it provides narrower confidence intervals.</li>
<li>Bonferroni is better than Scheffé when the number of contrasts is similar to or smaller than the number of treatment levels.</li>
<li>Practical recommendation: Compute Tukey, Scheffé, and Bonferroni and use the method with the smallest confidence intervals.</li>
<li>Bonferroni cannot be used for data snooping, as it assumes the comparisons were planned before examining the data.</li>
</ul>
<hr>
</div>
<div id="fishers-least-significant-difference" class="section level6" number="24.1.1.5.4.4">
<h6>
<span class="header-section-number">24.1.1.5.4.4</span> Fisher’s Least Significant Difference<a class="anchor" aria-label="anchor" href="#fishers-least-significant-difference"><i class="fas fa-link"></i></a>
</h6>
<p>The Fisher LSD method does not control the family-wise error rate (refer to <a href="hypothesis-testing.html#sec-false-discovery-rate">16.3</a>), meaning it does not correct for multiple comparisons. However, it can be useful for exploratory analysis when a preliminary ANOVA is significant.</p>
<p>The hypothesis test for comparing two treatment means:</p>
<p><span class="math display">\[
H_0: \mu_i = \mu_j
\]</span></p>
<p>uses the <span class="math inline">\(t\)</span>-statistic:</p>
<p><span class="math display">\[
t = \frac{\bar{Y}_i - \bar{Y}_j}{\sqrt{MSE \left(\frac{1}{n_i} + \frac{1}{n_j}\right)}}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\bar{Y}_i\)</span> and <span class="math inline">\(\bar{Y}_j\)</span> are the sample means for treatments <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p></li>
<li><p><span class="math inline">\(MSE\)</span> is the mean squared error from ANOVA.</p></li>
<li><p><span class="math inline">\(n_i, n_j\)</span> are the sample sizes for groups <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p></li>
</ul>
<p>Notes:</p>
<ul>
<li>The LSD method does not adjust for multiple comparisons, which increases the Type I error rate.</li>
<li>It is only valid if the overall ANOVA is significant (i.e., the global null hypothesis of no treatment effect is rejected).</li>
<li>Tukey and Bonferroni methods are preferred when many comparisons are made.</li>
</ul>
<hr>
</div>
<div id="newman-keuls" class="section level6" number="24.1.1.5.4.5">
<h6>
<span class="header-section-number">24.1.1.5.4.5</span> Newman-Keuls<a class="anchor" aria-label="anchor" href="#newman-keuls"><i class="fas fa-link"></i></a>
</h6>
<p>The Newman-Keuls procedure is a stepwise multiple comparison test similar to Tukey’s method but less rigorous.</p>
<p>Key Issues:</p>
<ul>
<li>Unlike Tukey, Newman-Keuls does not control the family-wise error rate.</li>
<li>It has less power than ANOVA.</li>
<li>It is rarely recommended in modern statistical practice.</li>
<li>
<strong>Do not recommend using the Newman-Keuls test</strong>.</li>
</ul>
</div>
<div id="summary-of-multiple-comparison-procedures" class="section level6" number="24.1.1.5.4.6">
<h6>
<span class="header-section-number">24.1.1.5.4.6</span> Summary of Multiple Comparison Procedures<a class="anchor" aria-label="anchor" href="#summary-of-multiple-comparison-procedures"><i class="fas fa-link"></i></a>
</h6>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="4%">
<col width="11%">
<col width="10%">
<col width="19%">
<col width="29%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th>Method</th>
<th>Type of Comparisons</th>
<th>Controls Family-Wise Error Rate?</th>
<th>Best Used For</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Tukey</strong></td>
<td>All pairwise comparisons</td>
<td>Yes</td>
<td>Comparing all treatment means</td>
<td>Exact confidence level when sample sizes are equal; more powerful than Scheffé for pairwise tests</td>
<td>Conservative if sample sizes are unequal</td>
</tr>
<tr class="even">
<td>Scheffé</td>
<td>All possible contrasts</td>
<td>Yes</td>
<td>Exploratory analysis, especially when interested in any contrast</td>
<td>Valid for any contrast; can be used for data snooping</td>
<td>Confidence intervals wider than Tukey for pairwise comparisons</td>
</tr>
<tr class="odd">
<td>Bonferroni</td>
<td>Fixed number of planned comparisons</td>
<td>Yes</td>
<td>A small number of pre-specified tests</td>
<td>Simple and flexible; better than Scheffé for few comparisons</td>
<td>Less powerful than Tukey for many pairwise tests; cannot be used for data snooping</td>
</tr>
<tr class="even">
<td>Fisher’s LSD</td>
<td>Pairwise comparisons</td>
<td>No</td>
<td>Exploratory comparisons after significant ANOVA</td>
<td>Most powerful for pairwise comparisons when ANOVA is significant</td>
<td>Inflates Type I error rate; not valid without a significant ANOVA</td>
</tr>
<tr class="odd">
<td>Newman-Keuls</td>
<td>Pairwise comparisons</td>
<td>No</td>
<td>-</td>
<td>-</td>
<td>Less power than ANOVA; generally not recommended</td>
</tr>
</tbody>
</table></div>
</div>
<div id="dunnetts-test" class="section level6" number="24.1.1.5.4.7">
<h6>
<span class="header-section-number">24.1.1.5.4.7</span> Dunnett’s Test<a class="anchor" aria-label="anchor" href="#dunnetts-test"><i class="fas fa-link"></i></a>
</h6>
<p>In some experiments, instead of comparing all treatment groups against each other, we are specifically interested in comparing each treatment to a control. This is common in clinical trials or A/B testing, where one group serves as a baseline.</p>
<p>Dunnett’s test is designed for experiments with <span class="math inline">\(a\)</span> groups, where:</p>
<ul>
<li><p>One group is the control (e.g., placebo or standard treatment).</p></li>
<li><p>The remaining <span class="math inline">\(a-1\)</span> groups are treatment groups.</p></li>
</ul>
<p>Thus, we perform <span class="math inline">\(a-1\)</span> pairwise comparisons:</p>
<p><span class="math display">\[
D_i = \mu_i - \mu_c, \quad i = 1, \dots, a-1
\]</span></p>
<p>where <span class="math inline">\(\mu_c\)</span> is the mean of the control group.</p>
<p><strong>Dunnett’s Test vs. Other Methods</strong></p>
<ul>
<li>Unlike Tukey’s method (which compares all pairs), Dunnett’s method only compares treatments to the control.</li>
<li>Dunnett’s test controls the family-wise error rate, making it more powerful than Bonferroni for this scenario.</li>
<li>If the goal is to compare treatments against each other as well, Tukey’s method is preferable.</li>
</ul>
<hr>
</div>
</div>
</div>
</div>
<div id="sec-single-factor-random-effects-model" class="section level3" number="24.1.2">
<h3>
<span class="header-section-number">24.1.2</span> Single Factor Random Effects ANOVA<a class="anchor" aria-label="anchor" href="#sec-single-factor-random-effects-model"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as an <strong>ANOVA Type II model</strong>, the single factor random effects model assumes that treatments are randomly selected from a larger population. Thus, inference extends beyond the observed treatments to the entire population of treatments.</p>
<hr>
<div id="random-cell-means-model" class="section level4" number="24.1.2.1">
<h4>
<span class="header-section-number">24.1.2.1</span> Random Cell Means Model<a class="anchor" aria-label="anchor" href="#random-cell-means-model"><i class="fas fa-link"></i></a>
</h4>
<p>The model is given by:</p>
<p><span class="math display">\[
Y_{ij} = \mu_i + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mu_i \sim N(\mu, \sigma^2_{\mu})\)</span>, independent across treatments.</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2)\)</span>, independent across observations.</li>
<li>
<span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(\epsilon_{ij}\)</span> are mutually independent for <span class="math inline">\(i = 1, \dots, a\)</span> and <span class="math inline">\(j = 1, \dots, n\)</span>.</li>
</ul>
<p>When all treatment sample sizes are equal:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_{ij}) &amp;= E(\mu_i) = \mu \\
var(Y_{ij}) &amp;= var(\mu_i) + var(\epsilon_{ij}) = \sigma^2_{\mu} + \sigma^2
\end{aligned}
\]</span></p>
<hr>
<div id="covariance-structure" class="section level5" number="24.1.2.1.1">
<h5>
<span class="header-section-number">24.1.2.1.1</span> Covariance Structure<a class="anchor" aria-label="anchor" href="#covariance-structure"><i class="fas fa-link"></i></a>
</h5>
<p>Since <span class="math inline">\(Y_{ij}\)</span> are not independent, we calculate their covariances:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Same treatment group (</strong><span class="math inline">\(i\)</span> fixed, <span class="math inline">\(j \neq j'\)</span>):</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
cov(Y_{ij}, Y_{ij'}) &amp;= E(Y_{ij} Y_{ij'}) - E(Y_{ij}) E(Y_{ij'}) \\
&amp;= E(\mu_i^2 + \mu_i \epsilon_{ij'} + \mu_i \epsilon_{ij} + \epsilon_{ij} \epsilon_{ij'}) - \mu^2 \\
&amp;= \sigma^2_{\mu} + \mu^2 - \mu^2 \\
&amp;= \sigma^2_{\mu}
\end{aligned}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Different treatment groups (</strong><span class="math inline">\(i \neq i'\)</span>):</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
cov(Y_{ij}, Y_{i'j'}) &amp;= E(\mu_i \mu_{i'} + \mu_i \epsilon_{i'j'} + \mu_{i'} \epsilon_{ij} + \epsilon_{ij} \epsilon_{i'j'}) - \mu^2 \\
&amp;= \mu^2 - \mu^2 = 0
\end{aligned}
\]</span></p>
<p>Thus:</p>
<ul>
<li>All observations have the same variance: <span class="math inline">\(var(Y_{ij}) = \sigma^2_{\mu} + \sigma^2\)</span>.</li>
<li>Observations from the same treatment have covariance: <span class="math inline">\(\sigma^2_{\mu}\)</span>.</li>
<li>Observations from different treatments are uncorrelated.</li>
</ul>
<p>The <strong>intraclass correlation</strong> between two responses from the same treatment:</p>
<p><span class="math display">\[
\rho(Y_{ij}, Y_{ij'}) = \frac{\sigma^2_{\mu}}{\sigma^2_{\mu} + \sigma^2}, \quad j \neq j'
\]</span></p>
<hr>
</div>
<div id="inference-for-random-effects-model" class="section level5" number="24.1.2.1.2">
<h5>
<span class="header-section-number">24.1.2.1.2</span> Inference for Random Effects Model<a class="anchor" aria-label="anchor" href="#inference-for-random-effects-model"><i class="fas fa-link"></i></a>
</h5>
<p>The <strong>Intraclass Correlation Coefficient</strong>:</p>
<p><span class="math display">\[
\frac{\sigma^2_{\mu}}{\sigma^2 + \sigma^2_{\mu}}
\]</span></p>
<p>measures the proportion of total variability in <span class="math inline">\(Y_{ij}\)</span> that is accounted for by treatment differences.</p>
<p>To test whether treatments contribute significantly to variance:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \sigma_{\mu}^2 = 0 \quad \text{(No treatment effect, all $\mu_i = \mu$)} \\
&amp;H_a: \sigma_{\mu}^2 \neq 0
\end{aligned}
\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>, an ANOVA F-test is used:</p>
<p><span class="math display">\[
F = \frac{MSTR}{MSE}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(MSTR\)</span> (Mean Square for Treatments) captures variation <strong>between treatments</strong>.</li>
<li>
<span class="math inline">\(MSE\)</span> (Mean Square Error) captures variation <strong>within treatments</strong>.</li>
</ul>
<p>If <span class="math inline">\(H_0\)</span> is true, then:</p>
<p><span class="math display">\[
F \sim F_{(a-1, a(n-1))}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
F &gt; f_{(1-\alpha; a-1, a(n-1))}
\]</span></p>
<hr>
</div>
<div id="comparison-fixed-effects-vs.-random-effects-models" class="section level5" number="24.1.2.1.3">
<h5>
<span class="header-section-number">24.1.2.1.3</span> Comparison: Fixed Effects vs. Random Effects Models<a class="anchor" aria-label="anchor" href="#comparison-fixed-effects-vs.-random-effects-models"><i class="fas fa-link"></i></a>
</h5>
<p>Although ANOVA calculations are the same for <a href="sec-analysis-of-variance-anova.html#sec-single-factor-fixed-effects-model">fixed</a> and <a href="sec-analysis-of-variance-anova.html#sec-single-factor-random-effects-model">random effects</a> models, the interpretation of results differs.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="39%">
<col width="60%">
</colgroup>
<thead><tr class="header">
<th><strong>Random Effects Model</strong></th>
<th><strong>Fixed Effects Model</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(E(MSE) = \sigma^2\)</span></td>
<td><span class="math inline">\(E(MSE) = \sigma^2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(E(MSTR) = \sigma^2 + n \sigma^2_{\mu}\)</span></td>
<td><span class="math inline">\(E(MSTR) = \sigma^2 + \frac{ \sum_i n_i (\mu_i - \mu)^2}{a-1}\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>If <span class="math inline">\(\sigma^2_{\mu} = 0\)</span>, then <span class="math inline">\(E(MSTR) = E(MSE)\)</span>, implying no treatment effect.</li>
<li>Otherwise, <span class="math inline">\(E(MSTR) &gt; E(MSE)\)</span>, suggesting significant treatment variation.</li>
</ul>
<p>When sample sizes <strong>are not equal</strong>, the <span class="math inline">\(F\)</span>-test remains valid, but the degrees of freedom change to:</p>
<p><span class="math display">\[
F \sim F_{(a-1, N-a)}
\]</span></p>
<hr>
</div>
<div id="estimation-of-mu" class="section level5" number="24.1.2.1.4">
<h5>
<span class="header-section-number">24.1.2.1.4</span> Estimation of <span class="math inline">\(\mu\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-mu"><i class="fas fa-link"></i></a>
</h5>
<p>An unbiased estimator of <span class="math inline">\(E(Y_{ij}) = \mu\)</span> is the <strong>grand mean</strong>:</p>
<p><span class="math display">\[
\hat{\mu} = \bar{Y}_{..} = \frac{1}{a n} \sum_{i=1}^{a} \sum_{j=1}^{n} Y_{ij}
\]</span></p>
<p>The variance of this estimator is:</p>
<p><span class="math display">\[
\begin{aligned}
var(\bar{Y}_{..}) &amp;= var\left(\frac{1}{a} \sum_{i=1}^{a} \bar{Y}_{i.} \right) \\
&amp;= \frac{1}{a^2} \sum_{i=1}^{a} var(\bar{Y}_{i.}) \\
&amp;= \frac{1}{a^2} \sum_{i=1}^{a} \left(\sigma^2_\mu + \frac{\sigma^2}{n} \right) \\
&amp;= \frac{n \sigma^2_{\mu} + \sigma^2}{a n}
\end{aligned}
\]</span></p>
<p>An unbiased estimator of this variance is:</p>
<p><span class="math display">\[
s^2(\bar{Y}_{..}) = \frac{MSTR}{a n}
\]</span></p>
<p>Since:</p>
<p><span class="math display">\[
\frac{\bar{Y}_{..} - \mu}{s(\bar{Y}_{..})} \sim t_{a-1}
\]</span></p>
<p>A <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(\mu\)</span> is:</p>
<p><span class="math display">\[
\bar{Y}_{..} \pm t_{1-\alpha/2; a-1} s(\bar{Y}_{..})
\]</span></p>
<hr>
</div>
<div id="estimation-of-intraclass-correlation-coefficient-fracsigma2_musigma2_musigma2" class="section level5" number="24.1.2.1.5">
<h5>
<span class="header-section-number">24.1.2.1.5</span> Estimation of Intraclass Correlation Coefficient <span class="math inline">\(\frac{\sigma^2_\mu}{\sigma^2_{\mu}+\sigma^2}\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-intraclass-correlation-coefficient-fracsigma2_musigma2_musigma2"><i class="fas fa-link"></i></a>
</h5>
<p>In both <a href="sec-analysis-of-variance-anova.html#sec-single-factor-random-effects-model">random</a> and <a href="sec-analysis-of-variance-anova.html#sec-single-factor-fixed-effects-model">fixed</a> effects models, <span class="math inline">\(MSTR\)</span> and <span class="math inline">\(MSE\)</span> are <strong>independent</strong>.</p>
<p>When sample sizes are equal (<span class="math inline">\(n_i = n\)</span> for all <span class="math inline">\(i\)</span>), the test statistic:</p>
<p><span class="math display">\[
\frac{\frac{MSTR}{n\sigma^2_\mu + \sigma^2}}{\frac{MSE}{\sigma^2}} \sim F_{a-1, a(n-1)}
\]</span></p>
<p>A <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(\frac{\sigma^2_\mu}{\sigma^2_\mu + \sigma^2}\)</span> follows from:</p>
<p><span class="math display">\[
P\left(f_{\alpha/2; a-1, a(n-1)} \leq \frac{\frac{MSTR}{n\sigma^2_\mu + \sigma^2}}{\frac{MSE}{\sigma^2}} \leq f_{1-\alpha/2; a-1, a(n-1)} \right) = 1 - \alpha
\]</span></p>
<p>Defining:</p>
<p><span class="math display">\[
\begin{aligned}
L &amp;= \frac{1}{n} \left( \frac{MSTR}{MSE} \times \frac{1}{f_{1-\alpha/2; a-1, a(n-1)}} - 1 \right) \\
U &amp;= \frac{1}{n} \left( \frac{MSTR}{MSE} \times \frac{1}{f_{\alpha/2; a-1, a(n-1)}} - 1 \right)
\end{aligned}
\]</span></p>
<p>The lower and upper confidence limits for <span class="math inline">\(\frac{\sigma^2_\mu}{\sigma^2_\mu + \sigma^2}\)</span> are:</p>
<p><span class="math display">\[
\begin{aligned}
L^* &amp;= \frac{L}{1+L} \\
U^* &amp;= \frac{U}{1+U}
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(L^*\)</span> is negative, we customarily set it to 0.</p>
<hr>
</div>
<div id="estimation-of-sigma2" class="section level5" number="24.1.2.1.6">
<h5>
<span class="header-section-number">24.1.2.1.6</span> Estimation of <span class="math inline">\(\sigma^2\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-sigma2"><i class="fas fa-link"></i></a>
</h5>
<p>Since:</p>
<p><span class="math display">\[
\frac{a(n-1) MSE}{\sigma^2} \sim \chi^2_{a(n-1)}
\]</span></p>
<p>A <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(\sigma^2\)</span> is:</p>
<p><span class="math display">\[
\frac{a(n-1) MSE}{\chi^2_{1-\alpha/2; a(n-1)}} \leq \sigma^2 \leq \frac{a(n-1) MSE}{\chi^2_{\alpha/2; a(n-1)}}
\]</span></p>
<p>If sample sizes are unequal, the same formula applies, but the degrees of freedom change to:</p>
<p><span class="math display">\[
df = N - a
\]</span></p>
<hr>
</div>
<div id="estimation-of-sigma2_mu" class="section level5" number="24.1.2.1.7">
<h5>
<span class="header-section-number">24.1.2.1.7</span> Estimation of <span class="math inline">\(\sigma^2_\mu\)</span><a class="anchor" aria-label="anchor" href="#estimation-of-sigma2_mu"><i class="fas fa-link"></i></a>
</h5>
<p>From the expectations:</p>
<p><span class="math display">\[
E(MSE) = \sigma^2, \quad E(MSTR) = \sigma^2 + n\sigma^2_\mu
\]</span></p>
<p>we solve for <span class="math inline">\(\sigma^2_{\mu}\)</span>:</p>
<p><span class="math display">\[
\sigma^2_{\mu} = \frac{E(MSTR) - E(MSE)}{n}
\]</span></p>
<p>An unbiased estimator of <span class="math inline">\(\sigma^2_\mu\)</span> is:</p>
<p><span class="math display">\[
s^2_\mu = \frac{MSTR - MSE}{n}
\]</span></p>
<p>If <span class="math inline">\(s^2_\mu &lt; 0\)</span>, we set <span class="math inline">\(s^2_\mu = 0\)</span> (since variances cannot be negative).</p>
<p>If sample sizes are unequal, we replace <span class="math inline">\(n\)</span> with an effective sample size <span class="math inline">\(n'\)</span>:</p>
<p><span class="math display">\[
s^2_\mu = \frac{MSTR - MSE}{n'}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
n' = \frac{1}{a-1} \left(\sum_i n_i - \frac{\sum_i n_i^2}{\sum_i n_i} \right)
\]</span></p>
<hr>
<p>There are no exact confidence intervals for <span class="math inline">\(\sigma^2_\mu\)</span>, but we can approximate them using the Satterthwaite procedure.</p>
<div id="sec-satterthwaite-approximation" class="section level6" number="24.1.2.1.7.1">
<h6>
<span class="header-section-number">24.1.2.1.7.1</span> Satterthwaite Approximation<a class="anchor" aria-label="anchor" href="#sec-satterthwaite-approximation"><i class="fas fa-link"></i></a>
</h6>
<p>A linear combination of expected mean squares:</p>
<p><span class="math display">\[
\sigma^2_\mu = \frac{1}{n} E(MSTR) + \left(-\frac{1}{n}\right) E(MSE)
\]</span></p>
<p>For a general linear combination:</p>
<p><span class="math display">\[
S = d_1 E(MS_1) + \dots + d_h E(MS_h)
\]</span></p>
<p>where <span class="math inline">\(d_i\)</span> are coefficients, an unbiased estimator of <span class="math inline">\(S\)</span> is:</p>
<p><span class="math display">\[
\hat{S} = d_1 MS_1 + \dots + d_h MS_h
\]</span></p>
<p>Let <span class="math inline">\(df_i\)</span> be the degrees of freedom associated with each mean square <span class="math inline">\(MS_i\)</span>. The Satterthwaite approximation states:</p>
<p><span class="math display">\[
\frac{(df) \hat{S}}{S} \sim \chi^2_{df}
\]</span></p>
<p>where the degrees of freedom are approximated as:</p>
<p><span class="math display">\[
df = \frac{(d_1 MS_1 + \dots + d_h MS_h)^2}{\sum_{i=1}^{h} \frac{(d_i MS_i)^2}{df_i}}
\]</span></p>
<hr>
<p>Applying the Satterthwaite method to the <a href="sec-analysis-of-variance-anova.html#sec-single-factor-random-effects-model">single factor random effects model</a>:</p>
<p><span class="math display">\[
\frac{(df) s^2_\mu}{\chi^2_{1-\alpha/2; df}} \leq \sigma^2_\mu \leq \frac{(df) s^2_\mu}{\chi^2_{\alpha/2; df}}
\]</span></p>
<p>where the approximate degrees of freedom are:</p>
<p><span class="math display">\[
df = \frac{(s^2_\mu)^2}{\frac{(MSTR)^2}{a-1} + \frac{(MSE)^2}{a(n-1)}}
\]</span></p>
<hr>
</div>
</div>
</div>
<div id="random-treatment-effects-model" class="section level4" number="24.1.2.2">
<h4>
<span class="header-section-number">24.1.2.2</span> Random Treatment Effects Model<a class="anchor" aria-label="anchor" href="#random-treatment-effects-model"><i class="fas fa-link"></i></a>
</h4>
<p>In a random effects model, treatment levels are considered random samples from a larger population of possible treatments. The model accounts for variability across all potential treatments, not just those observed in the study.</p>
<p>We define the <strong>random treatment effect</strong> as:</p>
<p><span class="math display">\[
\tau_i = \mu_i - E(\mu_i) = \mu_i - \mu
\]</span></p>
<p>where <span class="math inline">\(\tau_i\)</span> represents the deviation of treatment mean <span class="math inline">\(\mu_i\)</span> from the overall mean <span class="math inline">\(\mu\)</span>.</p>
<p>Thus, we rewrite treatment means as:</p>
<p><span class="math display">\[
\mu_i = \mu + \tau_i
\]</span></p>
<p>Substituting this into the response model:</p>
<p><span class="math display">\[
Y_{ij} = \mu + \tau_i + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mu\)</span> = common mean across all observations.</li>
<li>
<span class="math inline">\(\tau_i \sim N(0, \sigma^2_\tau)\)</span>, random treatment effects, assumed independent.</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2)\)</span>, random error terms, also independent.</li>
<li>
<span class="math inline">\(\tau_{i}\)</span> and <span class="math inline">\(\epsilon_{ij}\)</span> are mutually independent for <span class="math inline">\(i = 1, \dots, a\)</span> and <span class="math inline">\(j = 1, \dots, n\)</span>.</li>
<li>We consider only balanced single-factor ANOVA (equal sample sizes across treatments).</li>
</ul>
<hr>
</div>
<div id="diagnostic-measures-for-model-assumptions" class="section level4" number="24.1.2.3">
<h4>
<span class="header-section-number">24.1.2.3</span> Diagnostic Measures for Model Assumptions<a class="anchor" aria-label="anchor" href="#diagnostic-measures-for-model-assumptions"><i class="fas fa-link"></i></a>
</h4>
<p>Checking assumptions is crucial for valid inference. Common issues include:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="40%">
<col width="59%">
</colgroup>
<thead><tr class="header">
<th><strong>Issue</strong></th>
<th><strong>Diagnostic Tools</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Non-constant error variance (heteroscedasticity)</strong></td>
<td>Residual plots, Levene’s test, Hartley’s test</td>
</tr>
<tr class="even">
<td><strong>Non-independence of errors</strong></td>
<td>Residual plots, Durbin-Watson test (for autocorrelation)</td>
</tr>
<tr class="odd">
<td><strong>Outliers</strong></td>
<td>Boxplots, residual plots, regression influence measures (e.g., Cook’s distance)</td>
</tr>
<tr class="even">
<td><strong>Non-normality of errors</strong></td>
<td>Histogram, Q-Q plot, Shapiro-Wilk test, Anderson-Darling test</td>
</tr>
<tr class="odd">
<td><strong>Omitted variable bias</strong></td>
<td>Residual plots, checking for unaccounted sources of variation</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="remedial-measures" class="section level4" number="24.1.2.4">
<h4>
<span class="header-section-number">24.1.2.4</span> Remedial Measures<a class="anchor" aria-label="anchor" href="#remedial-measures"><i class="fas fa-link"></i></a>
</h4>
<p>If diagnostic checks indicate violations of assumptions, possible solutions include:</p>
<ul>
<li>
<a href="linear-regression.html#weighted-least-squares">Weighted Least Squares</a> – Adjusts for heteroscedasticity.</li>
<li>
<a href="variable-transformation.html#variable-transformation">Variable Transformation</a> – Log or Box-Cox transformations may improve normality or stabilize variance.</li>
<li>
<a href="sec-analysis-of-variance-anova.html#sec-nonparametric-anova">Non-Parametric Procedures</a> – Kruskal-Wallis test or bootstrapping when normality assumptions fail.</li>
</ul>
<hr>
</div>
<div id="key-notes-on-robustness" class="section level4" number="24.1.2.5">
<h4>
<span class="header-section-number">24.1.2.5</span> Key Notes on Robustness<a class="anchor" aria-label="anchor" href="#key-notes-on-robustness"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>Fixed effects ANOVA is relatively robust to:
<ul>
<li>Non-normality, particularly when sample sizes are moderate to large.</li>
<li>Unequal variances when sample sizes are roughly equal.</li>
<li>F-test and multiple comparisons remain valid under mild violations.</li>
</ul>
</li>
<li>Random effects ANOVA is sensitive to:
<ul>
<li>Lack of independence, which severely affects both fixed and random effects models.</li>
<li>Unequal variances, particularly when estimating variance components.</li>
</ul>
</li>
</ul>
<hr>
</div>
</div>
<div id="sec-two-factor-fixed-effects-anova" class="section level3" number="24.1.3">
<h3>
<span class="header-section-number">24.1.3</span> Two-Factor Fixed Effects ANOVA<a class="anchor" aria-label="anchor" href="#sec-two-factor-fixed-effects-anova"><i class="fas fa-link"></i></a>
</h3>
<p>A multi-factor experiment offers several advantages:</p>
<ul>
<li>Higher efficiency – More precise estimates with fewer observations.</li>
<li>Increased information – Allows for testing interactions between factors.</li>
<li>Greater validity – Reduces confounding by controlling additional sources of variation.</li>
</ul>
<p><strong>Balanced Two-Factor ANOVA: Assumptions</strong></p>
<ul>
<li>Equal sample sizes for all treatment combinations.</li>
<li>All treatment means are of equal importance (no weighting).</li>
<li>Factors are categorical and chosen purposefully.</li>
</ul>
<p>We assume:</p>
<ul>
<li>Factor A has <span class="math inline">\(a\)</span> levels and Factor B has <span class="math inline">\(b\)</span> levels.</li>
<li>All <span class="math inline">\(a \times b\)</span> factor level combinations are included.</li>
<li>Each treatment combination has <span class="math inline">\(n\)</span> replications.</li>
<li>The total number of observations:<br><span class="math display">\[ N = abn \]</span>
</li>
</ul>
<hr>
<div id="sec-cell-means-model-two-factor-anova" class="section level4" number="24.1.3.1">
<h4>
<span class="header-section-number">24.1.3.1</span> Cell Means Model<a class="anchor" aria-label="anchor" href="#sec-cell-means-model-two-factor-anova"><i class="fas fa-link"></i></a>
</h4>
<p>The response is modeled as:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mu_{ij}\)</span> are fixed parameters (cell means).</li>
<li>
<span class="math inline">\(i = 1, \dots, a\)</span> represents levels of Factor A.</li>
<li>
<span class="math inline">\(j = 1, \dots, b\)</span> represents levels of Factor B.</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim \text{independent } N(0, \sigma^2)\)</span> for all <span class="math inline">\(i, j, k\)</span>.</li>
</ul>
<p><strong>Expected values and variance</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_{ijk}) &amp;= \mu_{ij} \\
var(Y_{ijk}) &amp;= var(\epsilon_{ijk}) = \sigma^2
\end{aligned}
\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[
Y_{ijk} \sim \text{independent } N(\mu_{ij}, \sigma^2)
\]</span></p>
<p>This can be expressed in matrix notation:</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X} \beta + \epsilon
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\begin{aligned}
E(\mathbf{Y}) &amp;= \mathbf{X} \beta \\
var(\mathbf{Y}) &amp;= \sigma^2 \mathbf{I}
\end{aligned}
\]</span></p>
<hr>
<div id="interaction-effects" class="section level5" number="24.1.3.1.1">
<h5>
<span class="header-section-number">24.1.3.1.1</span> Interaction Effects<a class="anchor" aria-label="anchor" href="#interaction-effects"><i class="fas fa-link"></i></a>
</h5>
<p>Interaction measures whether the effect of one factor depends on the level of the other factor. It is defined as:</p>
<p><span class="math display">\[
(\alpha \beta)_{ij} = \mu_{ij} - (\mu_{..} + \alpha_i + \beta_j)
\]</span></p>
<p>where:</p>
<ul>
<li>
<strong>Grand mean</strong>:<br><span class="math display">\[ \mu_{..} = \frac{1}{ab} \sum_i \sum_j \mu_{ij} \]</span>
</li>
<li>
<strong>Main effect for Factor A</strong> (average effect of level <span class="math inline">\(i\)</span>):<br><span class="math display">\[ \alpha_i = \mu_{i.} - \mu_{..} \]</span>
</li>
<li>
<strong>Main effect for Factor B</strong> (average effect of level <span class="math inline">\(j\)</span>):<br><span class="math display">\[ \beta_j = \mu_{.j} - \mu_{..} \]</span>
</li>
<li>
<strong>Interaction effect</strong>:<br><span class="math display">\[ (\alpha \beta)_{ij} = \mu_{ij} - \mu_{i.} - \mu_{.j} + \mu_{..} \]</span>
</li>
</ul>
<hr>
<p>To determine whether interactions exist:</p>
<ol style="list-style-type: decimal">
<li>Check if all <span class="math inline">\(\mu_{ij}\)</span> can be written as sums <span class="math inline">\(\mu_{..} + \alpha_i + \beta_j\)</span><br>
(i.e., check if interaction terms are zero).</li>
<li>Compare mean differences across levels of Factor B at each level of Factor A.</li>
<li>Compare mean differences across levels of Factor A at each level of Factor B.</li>
<li>Graphical method:
<ul>
<li>Plot treatment means for each level of Factor B.</li>
<li>If lines are not parallel, an interaction exists.</li>
</ul>
</li>
</ol>
<hr>
<p>The interaction terms satisfy:</p>
<p>For each level of <strong>Factor B</strong>:</p>
<p><span class="math display">\[
\sum_i (\alpha \beta)_{ij} = \sum_i \left(\mu_{ij} - \mu_{..} - \alpha_i - \beta_j \right)
\]</span></p>
<p>Expanding:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_i (\alpha \beta)_{ij} &amp;= \sum_i \mu_{ij} - a \mu_{..} - \sum_i \alpha_i - a \beta_j \\
&amp;= a \mu_{.j} - a \mu_{..} - \sum_i (\mu_{i.} - \mu_{..}) - a (\mu_{.j} - \mu_{..}) \\
&amp;= a \mu_{.j} - a \mu_{..} - a \mu_{..}+ a \mu_{..} - a (\mu_{.j} - \mu_{..})  \\
&amp;= 0
\end{aligned}
\]</span></p>
<p>Similarly:</p>
<p><span class="math display">\[
\sum_j (\alpha \beta)_{ij} = 0, \quad i = 1, \dots, a
\]</span></p>
<p>and:</p>
<p><span class="math display">\[
\sum_i \sum_j (\alpha \beta)_{ij} = 0, \quad \sum_i \alpha_i = 0, \quad \sum_j \beta_j = 0
\]</span></p>
<hr>
</div>
</div>
<div id="sec-factor-effects-model-two-factor-anova" class="section level4" number="24.1.3.2">
<h4>
<span class="header-section-number">24.1.3.2</span> Factor Effects Model<a class="anchor" aria-label="anchor" href="#sec-factor-effects-model-two-factor-anova"><i class="fas fa-link"></i></a>
</h4>
<p>In the Factor Effects Model, we express the response as:</p>
<p><span class="math display">\[
\begin{aligned}
\mu_{ij} &amp;= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} \\
Y_{ijk} &amp;= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span> is the grand mean.</li>
<li>
<span class="math inline">\(\alpha_i\)</span> are main effects for Factor A, subject to:<br><span class="math display">\[ \sum_i \alpha_i = 0 \]</span>
</li>
<li>
<span class="math inline">\(\beta_j\)</span> are main effects for Factor B, subject to:<br><span class="math display">\[ \sum_j \beta_j = 0 \]</span>
</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij}\)</span> are interaction effects, subject to:<br><span class="math display">\[ \sum_i (\alpha \beta)_{ij} = 0, \quad j = 1, \dots, b \]</span><br><span class="math display">\[ \sum_j (\alpha \beta)_{ij} = 0, \quad i = 1, \dots, a \]</span>
</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim \text{independent } N(0, \sigma^2)\)</span> for <span class="math inline">\(k = 1, \dots, n\)</span>.</li>
</ul>
<p>Thus, we have:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_{ijk}) &amp;= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} \\
var(Y_{ijk}) &amp;= \sigma^2 \\
Y_{ijk} &amp;\sim N (\mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij}, \sigma^2)
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="parameter-counting-and-restrictions" class="section level4" number="24.1.3.3">
<h4>
<span class="header-section-number">24.1.3.3</span> Parameter Counting and Restrictions<a class="anchor" aria-label="anchor" href="#parameter-counting-and-restrictions"><i class="fas fa-link"></i></a>
</h4>
<p>The <a href="sec-analysis-of-variance-anova.html#sec-cell-means-model-two-factor-anova">Cell Means Model</a> has <span class="math inline">\(ab\)</span> parameters corresponding to each combination of factor levels.<br>
In the <a href="sec-analysis-of-variance-anova.html#sec-factor-effects-model-two-factor-anova">Factor Effects Model</a>, the imposed constraints reduce the number of estimable parameters:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="47%">
<col width="52%">
</colgroup>
<thead><tr class="header">
<th>Parameter</th>
<th>Count</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu_{..}\)</span></td>
<td><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(\alpha_i\)</span> (Main effects for A)</td>
<td>
<span class="math inline">\(a-1\)</span> (due to constraint <span class="math inline">\(\sum_i \alpha_i = 0\)</span>)</td>
</tr>
<tr class="odd">
<td>
<span class="math inline">\(\beta_j\)</span> (Main effects for B)</td>
<td>
<span class="math inline">\(b-1\)</span> (due to constraint <span class="math inline">\(\sum_j \beta_j = 0\)</span>)</td>
</tr>
<tr class="even">
<td>
<span class="math inline">\((\alpha \beta)_{ij}\)</span> (Interaction effects)</td>
<td>
<span class="math inline">\((a-1)(b-1)\)</span> (due to two constraints)</td>
</tr>
</tbody>
</table></div>
<p>Thus, the total number of parameters:</p>
<p><span class="math display">\[
1 + (a-1) + (b-1) + (a-1)(b-1) = ab
\]</span></p>
<p>which matches the number of parameters in the <a href="sec-analysis-of-variance-anova.html#sec-cell-means-model-two-factor-anova">Cell Means Model</a>.</p>
<hr>
<p>To uniquely estimate parameters, we apply constraints:</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_a  &amp;= -(\alpha_1 + \alpha_2 + \dots + \alpha_{a-1}) \\
\beta_b &amp;= -(\beta_1 + \beta_2 + \dots + \beta_{b-1}) \\
(\alpha \beta)_{ib} &amp;= -(\alpha \beta)_{i1} - (\alpha \beta)_{i2} - \dots - (\alpha \beta)_{i,b-1}, \quad i = 1, \dots, a \\
(\alpha \beta)_{aj} &amp;= -(\alpha \beta)_{1j} - (\alpha \beta)_{2j} - \dots - (\alpha \beta)_{a-1,j}, \quad j = 1, \dots, b
\end{aligned}
\]</span></p>
<p>The model can be fitted using least squares or maximum likelihood estimation.</p>
<hr>
<div id="cell-means-model-estimation" class="section level5" number="24.1.3.3.1">
<h5>
<span class="header-section-number">24.1.3.3.1</span> Cell Means Model Estimation<a class="anchor" aria-label="anchor" href="#cell-means-model-estimation"><i class="fas fa-link"></i></a>
</h5>
<p>Minimizing:</p>
<p><span class="math display">\[
Q = \sum_i \sum_j \sum_k (Y_{ijk} - \mu_{ij})^2
\]</span></p>
<p>yields estimators:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\mu}_{ij} &amp;= \bar{Y}_{ij} \\
\hat{Y}_{ijk} &amp;= \bar{Y}_{ij} \\
e_{ijk} &amp;= Y_{ijk} - \hat{Y}_{ijk} = Y_{ijk} - \bar{Y}_{ij}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(e_{ijk} \sim \text{independent } N(0, \sigma^2)\)</span>.</p>
<hr>
</div>
<div id="factor-effects-model-estimation" class="section level5" number="24.1.3.3.2">
<h5>
<span class="header-section-number">24.1.3.3.2</span> Factor Effects Model Estimation<a class="anchor" aria-label="anchor" href="#factor-effects-model-estimation"><i class="fas fa-link"></i></a>
</h5>
<p>Minimizing:</p>
<p><span class="math display">\[
Q = \sum_i \sum_j \sum_k (Y_{ijk} - \mu_{..} - \alpha_i - \beta_j - (\alpha \beta)_{ij})^2
\]</span></p>
<p>subject to the constraints:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_i \alpha_i &amp;= 0 \\
\sum_j \beta_j &amp;= 0 \\
\sum_i (\alpha \beta)_{ij} &amp;= 0, \quad j = 1, \dots, b \\
\sum_j (\alpha \beta)_{ij} &amp;= 0, \quad i = 1, \dots, a
\end{aligned}
\]</span></p>
<p>yields estimators:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\mu}_{..} &amp;= \bar{Y}_{...} \\
\hat{\alpha}_i &amp;= \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\beta}_j &amp;= \bar{Y}_{.j.} - \bar{Y}_{...} \\
(\hat{\alpha \beta})_{ij} &amp;= \bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.} + \bar{Y}_{...}
\end{aligned}
\]</span></p>
<hr>
<p>The fitted values are:</p>
<p><span class="math display">\[
\hat{Y}_{ijk} = \bar{Y}_{...} + (\bar{Y}_{i..} - \bar{Y}_{...}) + (\bar{Y}_{.j.} - \bar{Y}_{...}) + (\bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.} + \bar{Y}_{...})
\]</span></p>
<p>which simplifies to:</p>
<p><span class="math display">\[
\hat{Y}_{ijk} = \bar{Y}_{ij.}
\]</span></p>
<p>The residuals are:</p>
<p><span class="math display">\[
e_{ijk} = Y_{ijk} - \bar{Y}_{ij.}
\]</span></p>
<p>and follow:</p>
<p><span class="math display">\[
e_{ijk} \sim \text{independent } N(0, \sigma^2)
\]</span></p>
<hr>
<p>The variances of the estimated effects are:</p>
<p><span class="math display">\[
\begin{aligned}
s^2_{\hat{\mu}_{..}} &amp;= \frac{MSE}{nab} \\
s^2_{\hat{\alpha}_i} &amp;= MSE \left(\frac{1}{nb} - \frac{1}{nab} \right) \\
s^2_{\hat{\beta}_j} &amp;= MSE \left(\frac{1}{na} - \frac{1}{nab} \right) \\
s^2_{(\hat{\alpha\beta})_{ij}} &amp;= MSE \left(\frac{1}{n} - \frac{1}{na} - \frac{1}{nb} + \frac{1}{nab} \right)
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="partitioning-the-total-sum-of-squares" class="section level5" number="24.1.3.3.3">
<h5>
<span class="header-section-number">24.1.3.3.3</span> Partitioning the Total Sum of Squares<a class="anchor" aria-label="anchor" href="#partitioning-the-total-sum-of-squares"><i class="fas fa-link"></i></a>
</h5>
<p>The total deviation of an observation from the overall mean can be decomposed as:</p>
<p><span class="math display">\[
Y_{ijk} - \bar{Y}_{...} = (\bar{Y}_{ij.} - \bar{Y}_{...}) + (Y_{ijk} - \bar{Y}_{ij.})
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Y_{ijk} - \bar{Y}_{...}\)</span>: Total deviation of an observation.</li>
<li>
<span class="math inline">\(\bar{Y}_{ij.} - \bar{Y}_{...}\)</span>: Deviation of treatment mean from the overall mean.</li>
<li>
<span class="math inline">\(Y_{ijk} - \bar{Y}_{ij.}\)</span>: Residual deviation of an observation from the treatment mean.</li>
</ul>
<p>Summing over all observations:</p>
<p><span class="math display">\[
\sum_i \sum_j \sum_k (Y_{ijk} - \bar{Y}_{...})^2 = n \sum_i \sum_j (\bar{Y}_{ij.} - \bar{Y}_{...})^2 + \sum_i \sum_j \sum_k (Y_{ijk} - \bar{Y}_{ij.})^2
\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[
SSTO = SSTR + SSE
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(SSTO\)</span> = Total Sum of Squares (Total variation).</li>
<li>
<span class="math inline">\(SSTR\)</span> = Treatment Sum of Squares (Variation due to factor effects).</li>
<li>
<span class="math inline">\(SSE\)</span> = Error Sum of Squares (Residual variation).</li>
</ul>
<p>Since the cross-product terms are 0, the model naturally partitions the variance.</p>
<p>From the <a href="sec-analysis-of-variance-anova.html#sec-factor-effects-model-two-factor-anova">factor effects model</a>:</p>
<p><span class="math display">\[
\bar{Y}_{ij.} - \bar{Y}_{...} = (\bar{Y}_{i..} - \bar{Y}_{...}) + (\bar{Y}_{.j.} - \bar{Y}_{...}) + (\bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.} + \bar{Y}_{...})
\]</span></p>
<p>Squaring and summing:</p>
<p><span class="math display">\[
\begin{aligned}
n\sum_i \sum_j (\bar{Y}_{ij.} - \bar{Y}_{...})^2 &amp;= nb\sum_i (\bar{Y}_{i..} - \bar{Y}_{...})^2 + na\sum_j (\bar{Y}_{.j.} - \bar{Y}_{...})^2 \\
&amp;+ n\sum_i \sum_j (\bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.} + \bar{Y}_{...})^2
\end{aligned}
\]</span></p>
<p>Thus, treatment sum of squares can be further partitioned as:</p>
<p><span class="math display">\[
SSTR = SSA + SSB + SSAB
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(SSA\)</span>: Sum of Squares for Factor A.</li>
<li>
<span class="math inline">\(SSB\)</span>: Sum of Squares for Factor B.</li>
<li>
<span class="math inline">\(SSAB\)</span>: Sum of Squares for Interaction.</li>
</ul>
<p>The interaction term can also be expressed as:</p>
<p><span class="math display">\[
SSAB = SSTO - SSE - SSA - SSB
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
SSAB = SSTR - SSA - SSB
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(SSA\)</span> measures the variability of the estimated factor <strong>A</strong> level means (<span class="math inline">\(\bar{Y}_{i..}\)</span>). The more variable these means, the larger <span class="math inline">\(SSA\)</span>.</li>
<li>
<span class="math inline">\(SSB\)</span> measures the variability of the estimated factor <strong>B</strong> level means (<span class="math inline">\(\bar{Y}_{.j.}\)</span>).</li>
<li>
<span class="math inline">\(SSAB\)</span> measures the variability in <strong>interaction effects</strong>.</li>
</ul>
<hr>
<p>For <a href="sec-analysis-of-variance-anova.html#sec-two-factor-fixed-effects-anova">Two-Factor ANOVA</a>, the degrees of freedom partitioning follows:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><strong>Sum of Squares</strong></th>
<th><strong>Degrees of Freedom (df)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<span class="math inline">\(SSTO\)</span> (Total)</td>
<td><span class="math inline">\(N - 1 = abn - 1\)</span></td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(SSTR\)</span> (Treatments)</td>
<td><span class="math inline">\(ab - 1\)</span></td>
</tr>
<tr class="odd">
<td>
<span class="math inline">\(SSE\)</span> (Error)</td>
<td><span class="math inline">\(N - ab = ab(n - 1)\)</span></td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(SSA\)</span> (Factor A)</td>
<td><span class="math inline">\(a - 1\)</span></td>
</tr>
<tr class="odd">
<td>
<span class="math inline">\(SSB\)</span> (Factor B)</td>
<td><span class="math inline">\(b - 1\)</span></td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(SSAB\)</span> (Interaction)</td>
<td><span class="math inline">\((a-1)(b-1)\)</span></td>
</tr>
</tbody>
</table></div>
<p>Since:</p>
<p><span class="math display">\[
SSTR = SSA + SSB + SSAB
\]</span></p>
<p>the treatment degrees of freedom also partition as:</p>
<p><span class="math display">\[
ab - 1 = (a - 1) + (b - 1) + (a - 1)(b - 1)
\]</span></p>
<ul>
<li>
<span class="math inline">\(df_{SSA} = a - 1\)</span><br>
(One degree of freedom lost due to the constraint <span class="math inline">\(\sum (\bar{Y}_{i..} - \bar{Y}_{...}) = 0\)</span>).</li>
<li>
<span class="math inline">\(df_{SSB} = b - 1\)</span><br>
(One degree of freedom lost due to the constraint <span class="math inline">\(\sum (\bar{Y}_{.j.} - \bar{Y}_{...}) = 0\)</span>).</li>
<li>
<span class="math inline">\(df_{SSAB} = (a - 1)(b - 1)\)</span><br>
(Due to interaction constraints).</li>
</ul>
<hr>
<p>The Mean Squares are obtained by dividing Sum of Squares by the corresponding degrees of freedom:</p>
<p><span class="math display">\[
\begin{aligned}
MSA &amp;= \frac{SSA}{a - 1} \\
MSB &amp;= \frac{SSB}{b - 1} \\
MSAB &amp;= \frac{SSAB}{(a - 1)(b - 1)}
\end{aligned}
\]</span></p>
<p>The expectations of the mean squares are:</p>
<p><span class="math display">\[
\begin{aligned}
E(MSE) &amp;= \sigma^2 \\
E(MSA) &amp;= \sigma^2 + nb \frac{\sum \alpha_i^2}{a - 1} = \sigma^2 + nb \frac{\sum (\mu_{i..} - \mu_{..})^2}{a - 1} \\
E(MSB) &amp;= \sigma^2 + na \frac{\sum \beta_j^2}{b - 1} = \sigma^2 + na \frac{\sum (\mu_{.j.} - \mu_{..})^2}{b - 1} \\
E(MSAB) &amp;= \sigma^2 + n \frac{\sum \sum (\alpha \beta)^2_{ij}}{(a-1)(b-1)} = \sigma^2 + n \frac{\sum (\mu_{ij} - \mu_{i..} - \mu_{.j.} + \mu_{..})^2}{(a - 1)(b - 1)}
\end{aligned}
\]</span></p>
<p>If Factor A has no effect (<span class="math inline">\(\mu_{i..} = \mu_{..}\)</span>), then <span class="math inline">\(MSA\)</span> and <span class="math inline">\(MSE\)</span> have the same expectation.<br>
Similarly, if Factor B has no effect, then <span class="math inline">\(MSB = MSE\)</span>.</p>
<p>Thus, MSA &gt; MSE and MSB &gt; MSE suggest the presence of factor effects.</p>
<hr>
</div>
</div>
<div id="testing-for-interaction" class="section level4" number="24.1.3.4">
<h4>
<span class="header-section-number">24.1.3.4</span> Testing for Interaction<a class="anchor" aria-label="anchor" href="#testing-for-interaction"><i class="fas fa-link"></i></a>
</h4>
<p>Hypotheses:</p>
<p><span class="math display">\[
\begin{aligned}
H_0: \mu_{ij} - \mu_{i..} - \mu_{.j.} + \mu_{..} = 0 &amp;\quad \text{(No interaction)} \\
H_a: \mu_{ij} - \mu_{i..} - \mu_{.j.} + \mu_{..} \neq 0 &amp;\quad \text{(Interaction present)}
\end{aligned}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \text{All } (\alpha \beta)_{ij} = 0 \\
&amp;H_a: \text{Not all } (\alpha \beta)_{ij} = 0
\end{aligned}
\]</span></p>
<p>The F-statistic is:</p>
<p><span class="math display">\[
F = \frac{MSAB}{MSE}
\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(F \sim F_{(a-1)(b-1), ab(n-1)}\)</span>. Reject <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
F &gt; F_{1-\alpha; (a-1)(b-1), ab(n-1)}
\]</span></p>
<hr>
</div>
<div id="two-way-anova-summary-table" class="section level4" number="24.1.3.5">
<h4>
<span class="header-section-number">24.1.3.5</span> Two-Way ANOVA Summary Table<a class="anchor" aria-label="anchor" href="#two-way-anova-summary-table"><i class="fas fa-link"></i></a>
</h4>
<p>The Two-Way ANOVA table partitions the total variation into its components:</p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="17%">
<col width="17%">
<col width="20%">
<col width="23%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th><strong>Source of Variation</strong></th>
<th><strong>Sum of Squares (SS)</strong></th>
<th><strong>Degrees of Freedom (df)</strong></th>
<th><strong>Mean Square (MS)</strong></th>
<th><strong>F-Statistic</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Factor A</strong></td>
<td><span class="math inline">\(SSA\)</span></td>
<td><span class="math inline">\(a-1\)</span></td>
<td><span class="math inline">\(MSA = \frac{SSA}{a-1}\)</span></td>
<td><span class="math inline">\(F_A = \frac{MSA}{MSE}\)</span></td>
</tr>
<tr class="even">
<td><strong>Factor B</strong></td>
<td><span class="math inline">\(SSB\)</span></td>
<td><span class="math inline">\(b-1\)</span></td>
<td><span class="math inline">\(MSB = \frac{SSB}{b-1}\)</span></td>
<td><span class="math inline">\(F_B = \frac{MSB}{MSE}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Interaction (A × B)</strong></td>
<td><span class="math inline">\(SSAB\)</span></td>
<td><span class="math inline">\((a-1)(b-1)\)</span></td>
<td><span class="math inline">\(MSAB = \frac{SSAB}{(a-1)(b-1)}\)</span></td>
<td><span class="math inline">\(F_{AB} = \frac{MSAB}{MSE}\)</span></td>
</tr>
<tr class="even">
<td><strong>Error</strong></td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(ab(n-1)\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{ab(n-1)}\)</span></td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>Total (corrected)</strong></td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(abn - 1\)</span></td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table></div>
<hr>
<p><strong>Interpreting Two-Way ANOVA Results</strong></p>
<p>When conducting a Two-Way ANOVA, always check interaction effects first:</p>
<ol style="list-style-type: decimal">
<li>If the interaction (<span class="math inline">\(A \times B\)</span>) is significant:
<ul>
<li>The effect of one factor depends on the level of the other factor.</li>
<li>Main effects are not interpretable alone because their impact varies across levels of the second factor.</li>
</ul>
</li>
<li>If the interaction is NOT significant:
<ul>
<li>The factors have independent (additive) effects.</li>
<li>Main effects can be tested individually.</li>
</ul>
</li>
</ol>
<p><strong>Post-Hoc Comparisons</strong></p>
<ul>
<li>If interaction is not significant, proceed with main effect comparisons using:
<ul>
<li><a href="sec-analysis-of-variance-anova.html#sec-tukeys-anova">Tukey</a></li>
<li><a href="sec-analysis-of-variance-anova.html#sec-scheffe-anova">Scheffé</a></li>
<li><a href="sec-analysis-of-variance-anova.html#sec-bonferroni-anova">Bonferroni</a></li>
</ul>
</li>
<li>If interaction is significant, post-hoc tests should examine simple effects (comparisons within each level of a factor).</li>
</ul>
<hr>
<div id="contrasts-in-two-way-anova" class="section level5" number="24.1.3.5.1">
<h5>
<span class="header-section-number">24.1.3.5.1</span> Contrasts in Two-Way ANOVA<a class="anchor" aria-label="anchor" href="#contrasts-in-two-way-anova"><i class="fas fa-link"></i></a>
</h5>
<p>In Two-Way ANOVA, we can define contrasts to test specific hypotheses:</p>
<p><span class="math display">\[
L = \sum c_i \mu_i, \quad \text{where } \sum c_i = 0
\]</span></p>
<p>An unbiased estimator of <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[
\hat{L} = \sum c_i \bar{Y}_{i..}
\]</span></p>
<p>with variance:</p>
<p><span class="math display">\[
\sigma^2(\hat{L}) = \frac{\sigma^2}{bn} \sum c_i^2
\]</span></p>
<p>and variance estimate:</p>
<p><span class="math display">\[
\frac{MSE}{bn} \sum c_i^2
\]</span></p>
<hr>
<div id="orthogonal-contrasts-in-two-way-anova" class="section level6" number="24.1.3.5.1.1">
<h6>
<span class="header-section-number">24.1.3.5.1.1</span> Orthogonal Contrasts in Two-Way ANOVA<a class="anchor" aria-label="anchor" href="#orthogonal-contrasts-in-two-way-anova"><i class="fas fa-link"></i></a>
</h6>
<p>For two contrasts:</p>
<p><span class="math display">\[
\begin{aligned}
L_1 &amp;= \sum c_i \mu_i, \quad \sum c_i = 0 \\
L_2 &amp;= \sum d_i \mu_i, \quad \sum d_i = 0
\end{aligned}
\]</span></p>
<p>They are orthogonal if:</p>
<p><span class="math display">\[
\sum \frac{c_i d_i}{n_i} = 0
\]</span></p>
<p>For balanced designs (<span class="math inline">\(n_i = n\)</span>):</p>
<p><span class="math display">\[
\sum c_i d_i = 0
\]</span></p>
<p>This ensures that orthogonal contrasts are uncorrelated:</p>
<p><span class="math display">\[
\begin{aligned}
cov(\hat{L}_1, \hat{L}_2) &amp;= cov\left(\sum_i c_i \bar{Y}_{i..}, \sum_l d_l \bar{Y}_{l..}\right) \\
&amp;= \sum_i \sum_l c_i d_l cov(\bar{Y}_{i..},\bar{Y}_{l..}) \\
&amp;= \sum_i c_i d_i \frac{\sigma^2}{bn} = 0
\end{aligned}
\]</span></p>
<p>Thus, orthogonal contrasts allow us to partition the sum of squares further.</p>
<hr>
</div>
<div id="orthogonal-polynomial-contrasts" class="section level6" number="24.1.3.5.1.2">
<h6>
<span class="header-section-number">24.1.3.5.1.2</span> Orthogonal Polynomial Contrasts<a class="anchor" aria-label="anchor" href="#orthogonal-polynomial-contrasts"><i class="fas fa-link"></i></a>
</h6>
<ul>
<li>Used when factor levels are equally spaced (e.g., dose levels: 0, 15, 30, 45, 60).</li>
<li>Requires equal sample sizes across factor levels.</li>
</ul>
<p>The Sum of Squares (SS) for a given contrast:</p>
<p><span class="math display">\[
SS_L = \frac{\hat{L}^2}{\sum_{i=1}^a \frac{c^2_i}{bn_i}}
\]</span></p>
<p>The <span class="math inline">\(t\)</span>-statistic for testing contrasts:</p>
<p><span class="math display">\[
T = \frac{\hat{L}}{\sqrt{MSE \sum_{i=1}^a \frac{c_i^2}{bn_i}}} \sim t
\]</span></p>
<p>Since:</p>
<p><span class="math display">\[
t^2_{(1-\alpha/2; df)} = F_{(1-\alpha; 1, df)}
\]</span></p>
<p>we can equivalently test:</p>
<p><span class="math display">\[
\frac{SS_L}{MSE} \sim F_{(1-\alpha;1,df_{MSE})}
\]</span></p>
<p>All contrasts have <span class="math inline">\(df = 1\)</span>.</p>
<hr>
</div>
</div>
</div>
<div id="unbalanced-two-way-anova" class="section level4" number="24.1.3.6">
<h4>
<span class="header-section-number">24.1.3.6</span> Unbalanced Two-Way ANOVA<a class="anchor" aria-label="anchor" href="#unbalanced-two-way-anova"><i class="fas fa-link"></i></a>
</h4>
<p>In many practical situations, sample sizes may be unequal across factor combinations, such as in:</p>
<ul>
<li>Observational studies (e.g., real-world data with missing values).</li>
<li>Dropouts in designed studies (e.g., clinical trials with subject attrition).</li>
<li>Larger sample sizes for inexpensive treatments.</li>
<li>Sample sizes chosen to match population proportions.</li>
</ul>
<p>We assume the standard Two-Way ANOVA model:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where sample sizes vary:</p>
<p><span class="math display">\[
\begin{aligned}
n_{i.} &amp;= \sum_j n_{ij} \quad \text{(Total for factor level } i) \\
n_{.j} &amp;= \sum_i n_{ij} \quad \text{(Total for factor level } j) \\
n_T &amp;= \sum_i \sum_j n_{ij} \quad \text{(Total sample size)}
\end{aligned}
\]</span></p>
<p>However, for unbalanced designs, a major issue arises:</p>
<p><span class="math display">\[
SSTO \neq SSA + SSB + SSAB + SSE
\]</span></p>
<p>Unlike the balanced case, the design is non-orthogonal, meaning sum-of-squares partitions do not add up cleanly.</p>
<hr>
<div id="indicator-variables-for-factor-levels" class="section level5" number="24.1.3.6.1">
<h5>
<span class="header-section-number">24.1.3.6.1</span> Indicator Variables for Factor Levels<a class="anchor" aria-label="anchor" href="#indicator-variables-for-factor-levels"><i class="fas fa-link"></i></a>
</h5>
<p>To handle unbalanced data, we use indicator (dummy) variables as predictors.</p>
<p>For Factor A (<span class="math inline">\(i = 1, \dots, a-1\)</span>):</p>
<p><span class="math display">\[
u_i =
\begin{cases}
+1 &amp; \text{if observation is from level } i \text{ of Factor A} \\
-1 &amp; \text{if observation is from the reference level (level } a \text{)} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>For Factor B (<span class="math inline">\(j = 1, \dots, b-1\)</span>):</p>
<p><span class="math display">\[
v_j =
\begin{cases}
+1 &amp; \text{if observation is from level } j \text{ of Factor B} \\
-1 &amp; \text{if observation is from the reference level (level } b \text{)} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Rewriting the ANOVA model using indicator variables:</p>
<p><span class="math display">\[
Y = \mu_{..} + \sum_{i=1}^{a-1} \alpha_i u_i + \sum_{j=1}^{b-1} \beta_j v_j + \sum_{i=1}^{a-1} \sum_{j=1}^{b-1}(\alpha \beta)_{ij} u_i v_j + \epsilon
\]</span></p>
<p>Here, the unknown parameters are:</p>
<ul>
<li><p><span class="math inline">\(\mu_{..}\)</span> (grand mean),</p></li>
<li><p><span class="math inline">\(\alpha_i\)</span> (main effects for Factor A),</p></li>
<li><p><span class="math inline">\(\beta_j\)</span> (main effects for Factor B),</p></li>
<li><p><span class="math inline">\((\alpha \beta)_{ij}\)</span> (interaction effects).</p></li>
</ul>
<hr>
</div>
<div id="hypothesis-testing-using-extra-sum-of-squares" class="section level5" number="24.1.3.6.2">
<h5>
<span class="header-section-number">24.1.3.6.2</span> Hypothesis Testing Using Extra Sum of Squares<a class="anchor" aria-label="anchor" href="#hypothesis-testing-using-extra-sum-of-squares"><i class="fas fa-link"></i></a>
</h5>
<p>For unbalanced designs, we use sequential (type I) or adjusted (type III) sum of squares to test hypotheses.</p>
<p>To test for interaction effects, we test:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \text{All } (\alpha \beta)_{ij} = 0 \quad \text{(No interaction)} \\
&amp;H_a: \text{Not all } (\alpha \beta)_{ij} = 0 \quad \text{(Interaction present)}
\end{aligned}
\]</span></p>
<p>To test whether Factor B has an effect:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \beta_1 = \beta_2 = \dots = \beta_b = 0 \\
&amp;H_a: \text{At least one } \beta_j \neq 0
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="factor-mean-analysis-and-contrasts" class="section level5" number="24.1.3.6.3">
<h5>
<span class="header-section-number">24.1.3.6.3</span> Factor Mean Analysis and Contrasts<a class="anchor" aria-label="anchor" href="#factor-mean-analysis-and-contrasts"><i class="fas fa-link"></i></a>
</h5>
<p>Factor means and contrasts (e.g., pairwise comparisons) work similarly to the balanced case but require adjustments due to unequal sample sizes.</p>
<p>The variance estimate for a contrast:</p>
<p><span class="math display">\[
\sigma^2(\hat{L}) = \frac{\sigma^2}{\sum n_{ij}} \sum c_i^2
\]</span></p>
<p>is modified to:</p>
<p><span class="math display">\[
\frac{MSE}{\sum n_{ij}} \sum c_i^2
\]</span></p>
<p>Orthogonal contrasts are harder to define because unequal sample sizes break orthogonality.</p>
<hr>
</div>
<div id="regression-approach-to-unbalanced-anova" class="section level5" number="24.1.3.6.4">
<h5>
<span class="header-section-number">24.1.3.6.4</span> Regression Approach to Unbalanced ANOVA<a class="anchor" aria-label="anchor" href="#regression-approach-to-unbalanced-anova"><i class="fas fa-link"></i></a>
</h5>
<p>An alternative is to fit the cell means model as a regression model:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{ij} + \epsilon_{ij}
\]</span></p>
<p>which allows us to analyze each treatment mean separately.</p>
<p>However, if there are empty cells (some factor combinations have no observations), the regression approach fails, and only partial analyses can be conducted.</p>
<hr>
</div>
</div>
</div>
<div id="sec-two-way-random-effects-anova" class="section level3" number="24.1.4">
<h3>
<span class="header-section-number">24.1.4</span> Two-Way Random Effects ANOVA<a class="anchor" aria-label="anchor" href="#sec-two-way-random-effects-anova"><i class="fas fa-link"></i></a>
</h3>
<p>The Two-Way Random Effects ANOVA assumes that both Factor A and Factor B levels are randomly sampled from larger populations.</p>
<p>The model is:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span>: Overall mean (constant).</li>
<li>
<span class="math inline">\(\alpha_i \sim N(0, \sigma^2_{\alpha})\)</span> for <span class="math inline">\(i = 1, \dots, a\)</span> (random effects for Factor A, independently distributed).</li>
<li>
<span class="math inline">\(\beta_j \sim N(0, \sigma^2_{\beta})\)</span> for <span class="math inline">\(j = 1, \dots, b\)</span> (random effects for Factor B, independently distributed).</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij} \sim N(0, \sigma^2_{\alpha \beta})\)</span> for <span class="math inline">\(i = 1, \dots, a\)</span>, <span class="math inline">\(j = 1, \dots, b\)</span> (random interaction effects, independently distributed).</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2)\)</span> (random error, independently distributed).</li>
</ul>
<p>Additionally, all random effects (<span class="math inline">\(\alpha_i, \beta_j, (\alpha \beta)_{ij}\)</span>) and error terms (<span class="math inline">\(\epsilon_{ijk}\)</span>) are mutually independent.</p>
<hr>
<div id="expectation" class="section level4" number="24.1.4.1">
<h4>
<span class="header-section-number">24.1.4.1</span> Expectation<a class="anchor" aria-label="anchor" href="#expectation"><i class="fas fa-link"></i></a>
</h4>
<p>Taking expectations on both sides:</p>
<p><span class="math display">\[
E(Y_{ijk}) = E(\mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk})
\]</span></p>
<p>Since all random effects have mean zero:</p>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{..}
\]</span></p>
<p>Thus, the mean response across all factor levels is <span class="math inline">\(\mu_{..}\)</span>.</p>
<hr>
</div>
<div id="variance-1" class="section level4" number="24.1.4.2">
<h4>
<span class="header-section-number">24.1.4.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance-1"><i class="fas fa-link"></i></a>
</h4>
<p>The total variance of observations is the sum of all variance components:</p>
<p><span class="math display">\[
\begin{aligned}
var(Y_{ijk}) &amp;= var(\alpha_i) + var(\beta_j) + var((\alpha \beta)_{ij}) + var(\epsilon_{ijk}) \\
&amp;= \sigma^2_{\alpha} + \sigma^2_{\beta} + \sigma^2_{\alpha \beta} + \sigma^2
\end{aligned}
\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[
Y_{ijk} \sim N(\mu_{..}, \sigma^2_{\alpha} + \sigma^2_{\beta} + \sigma^2_{\alpha \beta} + \sigma^2)
\]</span></p>
<hr>
</div>
<div id="covariance-structure-1" class="section level4" number="24.1.4.3">
<h4>
<span class="header-section-number">24.1.4.3</span> Covariance Structure<a class="anchor" aria-label="anchor" href="#covariance-structure-1"><i class="fas fa-link"></i></a>
</h4>
<p>In random effects models, observations are correlated if they share the same factor levels.</p>
<p><strong>Case 1: Same factor A, different factor B</strong></p>
<p>If <span class="math inline">\(i\)</span> is the same but <span class="math inline">\(j \neq j'\)</span>, then:</p>
<p><span class="math display">\[
cov(Y_{ijk}, Y_{ij'k'}) = var(\alpha_i) = \sigma^2_{\alpha}
\]</span></p>
<p><strong>Case 2: Same factor B, different factor A</strong></p>
<p>If <span class="math inline">\(j\)</span> is the same but <span class="math inline">\(i \neq i'\)</span>, then:</p>
<p><span class="math display">\[
cov(Y_{ijk}, Y_{i'jk'}) = var(\beta_j) = \sigma^2_{\beta}
\]</span></p>
<p><strong>Case 3: Same factor A and B, different replication</strong></p>
<p>If both factor levels are the same (<span class="math inline">\(i, j\)</span> fixed), but different replication (<span class="math inline">\(k \neq k'\)</span>):</p>
<p><span class="math display">\[
cov(Y_{ijk}, Y_{ijk'}) = var(\alpha_i) + var(\beta_j) + var((\alpha \beta)_{ij}) = \sigma^2_{\alpha} + \sigma^2_{\beta} + \sigma^2_{\alpha \beta}
\]</span></p>
<p><strong>Case 4: Completely different factor levels</strong></p>
<p>If neither factor A nor B is the same (<span class="math inline">\(i \neq i'\)</span>, <span class="math inline">\(j \neq j'\)</span>), then:</p>
<p><span class="math display">\[
cov(Y_{ijk}, Y_{i'j'k'}) = 0
\]</span></p>
<p>since all random effects are independent across different factor levels.</p>
<hr>
<p><strong>Summary of Variance-Covariance Structure</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="32%">
<col width="22%">
<col width="44%">
</colgroup>
<thead><tr class="header">
<th><strong>Case</strong></th>
<th><strong>Condition</strong></th>
<th><strong>Covariance</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Same factor A, different factor B</strong></td>
<td>
<span class="math inline">\(i\)</span> same, <span class="math inline">\(j \neq j'\)</span>
</td>
<td><span class="math inline">\(\sigma^2_{\alpha}\)</span></td>
</tr>
<tr class="even">
<td><strong>Same factor B, different factor A</strong></td>
<td>
<span class="math inline">\(j\)</span> same, <span class="math inline">\(i \neq i'\)</span>
</td>
<td><span class="math inline">\(\sigma^2_{\beta}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Same factor levels, different replications</strong></td>
<td>
<span class="math inline">\(i\)</span> same, <span class="math inline">\(j\)</span> same, <span class="math inline">\(k \neq k'\)</span>
</td>
<td><span class="math inline">\(\sigma^2_{\alpha} + \sigma^2_{\beta} + \sigma^2_{\alpha \beta}\)</span></td>
</tr>
<tr class="even">
<td><strong>Different factor levels</strong></td>
<td>
<span class="math inline">\(i \neq i'\)</span>, <span class="math inline">\(j \neq j'\)</span>
</td>
<td><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="sec-two-way-mixed-effects-anova" class="section level3" number="24.1.5">
<h3>
<span class="header-section-number">24.1.5</span> Two-Way Mixed Effects ANOVA<a class="anchor" aria-label="anchor" href="#sec-two-way-mixed-effects-anova"><i class="fas fa-link"></i></a>
</h3>
<p>In a Two-Way Mixed Effects Model, one factor is fixed, while the other is random.<br>
This is often referred to as a mixed effects model or simply a mixed model.</p>
<div id="balanced" class="section level4" number="24.1.5.1">
<h4>
<span class="header-section-number">24.1.5.1</span> Balanced<a class="anchor" aria-label="anchor" href="#balanced"><i class="fas fa-link"></i></a>
</h4>
<p>For a balanced design, the <strong>restricted</strong> mixed model is:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span>: Overall mean (constant).</li>
<li>
<span class="math inline">\(\alpha_i\)</span>: Fixed effects for Factor A, subject to the constraint <span class="math inline">\(\sum \alpha_i = 0\)</span>.</li>
<li>
<span class="math inline">\(\beta_j \sim N(0, \sigma^2_\beta)\)</span> (random effects for Factor B).</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij} \sim N(0, \frac{a-1}{a} \sigma^2_{\alpha \beta})\)</span><br>
(interaction effects, constrained so that <span class="math inline">\(\sum_i (\alpha \beta)_{ij} = 0\)</span> for all <span class="math inline">\(j\)</span>). The variance is written as the proportion for convenience, it makes the expected mean squares simpler.</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2)\)</span> (random error).</li>
<li>
<span class="math inline">\(\beta_j, (\alpha \beta)_{ij}, \epsilon_{ijk}\)</span> are pairwise independent.</li>
</ul>
<p>The restriction on interaction variance (<span class="math inline">\(\frac{a-1}{a} \sigma^2_{\alpha \beta}\)</span>) simplifies the expected mean squares, though some sources assume <span class="math inline">\(var((\alpha \beta)_{ij}) = \sigma^2_{\alpha \beta}\)</span>.</p>
<hr>
<p>An <strong>unrestricted</strong> version of the model removes constraints on interaction terms.</p>
<p>Define:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_j &amp;= \beta_j^* + (\bar{\alpha \beta})_{ij}^* \\
(\alpha \beta)_{ij} &amp;= (\alpha \beta)_{ij}^* - (\bar{\alpha \beta})_{ij}^*
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\beta^*\)</span> and <span class="math inline">\((\alpha \beta)^*_{ij}\)</span> are unrestricted random effects.</p>
<p>Some consider the <strong>restricted model more general</strong>, but we use the <strong>restricted form</strong> for simplicity.</p>
<hr>
<p>Taking expectations:</p>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{..} + \alpha_i
\]</span></p>
<p>The total variance of responses:</p>
<p><span class="math display">\[
var(Y_{ijk}) = \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta} + \sigma^2
\]</span></p>
<hr>
<p><strong>Covariance Structure</strong></p>
<p>Observations sharing the same <strong>random factor (B)</strong> level are correlated.</p>
<p><strong>Covariances for Different Cases</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="37%">
<col width="62%">
</colgroup>
<thead><tr class="header">
<th><strong>Condition</strong></th>
<th><strong>Covariance</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Same <span class="math inline">\(i, j\)</span>, different replications (<span class="math inline">\(k \neq k'\)</span>)</td>
<td><span class="math inline">\(cov (Y_{ijk}, Y_{ijk'}) = \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta}\)</span></td>
</tr>
<tr class="even">
<td>Same <span class="math inline">\(j\)</span>, different <span class="math inline">\(i\)</span> (<span class="math inline">\(i \neq i'\)</span>)</td>
<td><span class="math inline">\(cov(Y_{ijk}, Y_{i'jk'}) = \sigma^2_\beta - \frac{1}{a} \sigma^2_{\alpha \beta}\)</span></td>
</tr>
<tr class="odd">
<td>Different <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> (<span class="math inline">\(i \neq i'\)</span>, <span class="math inline">\(j \neq j'\)</span>)</td>
<td><span class="math inline">\(cov(Y_{ijk}, Y_{i'j'k'}) = 0\)</span></td>
</tr>
</tbody>
</table></div>
<p>Thus, observations only become independent when they do not share the same random effect.</p>
<p>An advantage of the <strong>restricted mixed model</strong> is that 2 observations from the same random factor (B) level can be positively or negatively correlated. In the <strong>unrestricted</strong> model, they can only be positively correlated.</p>
<hr>
<p><strong>Comparison of Fixed, Random, and Mixed Effects Models</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="8%">
<col width="30%">
<col width="19%">
<col width="40%">
</colgroup>
<thead><tr class="header">
<th><strong>Mean Square</strong></th>
<th><strong>Fixed ANOVA (A, B fixed)</strong></th>
<th><strong>Random ANOVA (A, B random)</strong></th>
<th><strong>Mixed ANOVA (A fixed, B random)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>MSA</strong></td>
<td><span class="math inline">\(\sigma^2 + n b \frac{\sum \alpha_i^2}{a-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n b \sigma^2_{\alpha}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n b \frac{\sum_{i = 1}^a \alpha_i^2}{a-1} + n \sigma^2_{\alpha \beta}\)</span></td>
</tr>
<tr class="even">
<td><strong>MSB</strong></td>
<td><span class="math inline">\(\sigma^2 + n a \frac{\sum \beta_j^2}{b-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n a \sigma^2_{\beta}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n a \sigma^2_{\beta} + n \sigma^2_{\alpha \beta}\)</span></td>
</tr>
<tr class="odd">
<td><strong>MSAB</strong></td>
<td><span class="math inline">\(\sigma^2 + n \frac{\sum (\alpha \beta)_{ij}^2}{(a-1)(b-1)}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_{\alpha \beta}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_{\alpha \beta}\)</span></td>
</tr>
<tr class="even">
<td><strong>MSE</strong></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table></div>
<p>While SS and df are identical across models, the expected mean squares differ, affecting test statistics.</p>
<hr>
<div id="hypothesis-testing-in-mixed-anova" class="section level5" number="24.1.5.1.1">
<h5>
<span class="header-section-number">24.1.5.1.1</span> Hypothesis Testing in Mixed ANOVA<a class="anchor" aria-label="anchor" href="#hypothesis-testing-in-mixed-anova"><i class="fas fa-link"></i></a>
</h5>
<p>In <a href="sec-analysis-of-variance-anova.html#sec-two-way-random-effects-anova">random ANOVA</a>, we test:</p>
<p><span class="math display">\[
\begin{aligned}
H_0: \sigma^2 = 0 \quad vs. \quad H_a: \sigma^2 &gt; 0
\end{aligned}
\]</span></p>
<p>using:</p>
<p><span class="math display">\[
F = \frac{MSA}{MSAB} \sim F_{a-1, (a-1)(b-1)}
\]</span></p>
<p>For <a href="sec-analysis-of-variance-anova.html#sec-two-way-mixed-effects-anova">mixed models</a>, the same test statistic is used for:</p>
<p><span class="math display">\[
H_0: \alpha_i = 0, \quad \forall i
\]</span></p>
<p>However, for <a href="sec-analysis-of-variance-anova.html#sec-two-factor-fixed-effects-anova">fixed effects models</a>, the test statistic differs.</p>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="20%">
<col width="23%">
<col width="25%">
<col width="29%">
</colgroup>
<thead><tr class="header">
<th><strong>Test for Effect of</strong></th>
<th><strong>Fixed ANOVA (A, B fixed)</strong></th>
<th><strong>Random ANOVA (A, B random)</strong></th>
<th><strong>Mixed ANOVA (A fixed, B random)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Factor A</strong></td>
<td><span class="math inline">\(\frac{MSA}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSA}{MSAB}\)</span></td>
<td><span class="math inline">\(\frac{MSA}{MSAB}\)</span></td>
</tr>
<tr class="even">
<td><strong>Factor B</strong></td>
<td><span class="math inline">\(\frac{MSB}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSB}{MSAB}\)</span></td>
<td><span class="math inline">\(\frac{MSB}{MSE}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Interaction (A × B)</strong></td>
<td><span class="math inline">\(\frac{MSAB}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSAB}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSAB}{MSE}\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="variance-component-estimation" class="section level5" number="24.1.5.1.2">
<h5>
<span class="header-section-number">24.1.5.1.2</span> Variance Component Estimation<a class="anchor" aria-label="anchor" href="#variance-component-estimation"><i class="fas fa-link"></i></a>
</h5>
<p>In random and mixed effects models, we are interested in estimating variance components.</p>
<p>To estimate <span class="math inline">\(\sigma^2_\beta\)</span>:</p>
<p><span class="math display">\[
E(\sigma^2_\beta) = \frac{E(MSB) - E(MSE)}{na} = \frac{\sigma^2 + na \sigma^2_\beta - \sigma^2}{na} = \sigma^2_\beta
\]</span></p>
<p>which is estimated by:</p>
<p><span class="math display">\[
\hat{\sigma}^2_\beta = \frac{MSB - MSE}{na}
\]</span></p>
<p>Confidence intervals for variance components can be approximated using:</p>
<ul>
<li>
<a href="sec-analysis-of-variance-anova.html#sec-satterthwaite-approximation">Satterthwaite procedure</a>.</li>
<li>Modified large-sample (MLS) method</li>
</ul>
<hr>
</div>
<div id="estimating-fixed-effects-in-mixed-models" class="section level5" number="24.1.5.1.3">
<h5>
<span class="header-section-number">24.1.5.1.3</span> Estimating Fixed Effects in Mixed Models<a class="anchor" aria-label="anchor" href="#estimating-fixed-effects-in-mixed-models"><i class="fas fa-link"></i></a>
</h5>
<p>Fixed effects <span class="math inline">\(\alpha_i\)</span> are estimated by:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\alpha}_i &amp;= \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\mu}_{i.} &amp;= \bar{Y}_{...} + (\bar{Y}_{i..} - \bar{Y}_{...}) = \bar{Y}_{i..}
\end{aligned}
\]</span></p>
<p>Their variances:</p>
<p><span class="math display">\[
\begin{aligned}
\sigma^2(\hat{\alpha}_i) &amp;= \frac{\sigma^2 + n \sigma^2_{\alpha \beta}}{bn} = \frac{E(MSAB)}{bn} \\
s^2(\hat{\alpha}_i) &amp;= \frac{MSAB}{bn}
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="contrasts-on-fixed-effects" class="section level5" number="24.1.5.1.4">
<h5>
<span class="header-section-number">24.1.5.1.4</span> Contrasts on Fixed Effects<a class="anchor" aria-label="anchor" href="#contrasts-on-fixed-effects"><i class="fas fa-link"></i></a>
</h5>
<p>For a contrast:</p>
<p><span class="math display">\[
L = \sum c_i \alpha_i, \quad \text{where } \sum c_i = 0
\]</span></p>
<p>Estimate:</p>
<p><span class="math display">\[
\hat{L} = \sum c_i \hat{\alpha}_i
\]</span></p>
<p>Variance:</p>
<p><span class="math display">\[
\sigma^2(\hat{L}) = \sum c^2_i \sigma^2(\hat{\alpha}_i), \quad s^2(\hat{L}) = \frac{MSAB}{bn} \sum c^2_i
\]</span></p>
<hr>
</div>
</div>
<div id="unbalanced-two-way-mixed-effects-anova" class="section level4" number="24.1.5.2">
<h4>
<span class="header-section-number">24.1.5.2</span> Unbalanced Two-Way Mixed Effects ANOVA<a class="anchor" aria-label="anchor" href="#unbalanced-two-way-mixed-effects-anova"><i class="fas fa-link"></i></a>
</h4>
<p>In an <strong>unbalanced</strong> two-way mixed model (e.g., <span class="math inline">\(a = 2, b = 4\)</span>), the model remains:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\alpha_i\)</span>: Fixed effects for Factor A.</li>
<li>
<span class="math inline">\(\beta_j \sim N(0, \sigma^2_\beta)\)</span>: Random effects for Factor B.</li>
<li>
<span class="math inline">\((\alpha \beta)_{ij} \sim N(0, \frac{\sigma^2_{\alpha \beta}}{2})\)</span>: Interaction effects.</li>
<li>
<span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2)\)</span>: Residual error.</li>
</ul>
<hr>
<div id="variance-components" class="section level5" number="24.1.5.2.1">
<h5>
<span class="header-section-number">24.1.5.2.1</span> Variance Components<a class="anchor" aria-label="anchor" href="#variance-components"><i class="fas fa-link"></i></a>
</h5>
<p>The variance components are:</p>
<p><span class="math display">\[
\begin{aligned}
var(\beta_j) &amp;= \sigma^2_\beta \\
var((\alpha \beta)_{ij}) &amp;= \frac{2-1}{2} \sigma^2_{\alpha \beta} = \frac{\sigma^2_{\alpha \beta}}{2} \\
var(\epsilon_{ijk}) &amp;= \sigma^2
\end{aligned}
\]</span></p>
</div>
<div id="expectation-and-variance" class="section level5" number="24.1.5.2.2">
<h5>
<span class="header-section-number">24.1.5.2.2</span> Expectation and Variance<a class="anchor" aria-label="anchor" href="#expectation-and-variance"><i class="fas fa-link"></i></a>
</h5>
<p>Taking expectations:</p>
<p><span class="math display">\[
E(Y_{ijk}) = \mu_{..} + \alpha_i
\]</span></p>
<p>Total variance:</p>
<p><span class="math display">\[
var(Y_{ijk}) = \sigma^2_{\beta} + \frac{\sigma^2_{\alpha \beta}}{2} + \sigma^2
\]</span></p>
<hr>
</div>
<div id="covariance-structure-2" class="section level5" number="24.1.5.2.3">
<h5>
<span class="header-section-number">24.1.5.2.3</span> Covariance Structure<a class="anchor" aria-label="anchor" href="#covariance-structure-2"><i class="fas fa-link"></i></a>
</h5>
<p>Observations sharing <strong>Factor B (random effect)</strong> are correlated.</p>
<p><strong>Covariances for Different Cases</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="40%">
<col width="60%">
</colgroup>
<thead><tr class="header">
<th><strong>Condition</strong></th>
<th><strong>Covariance</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<strong>Same</strong> <span class="math inline">\(i, j\)</span>, different replications (<span class="math inline">\(k \neq k'\)</span>)</td>
<td><span class="math inline">\(cov(Y_{ijk}, Y_{ijk'}) = \sigma^2 + \frac{\sigma^2_{\alpha \beta}}{2}\)</span></td>
</tr>
<tr class="even">
<td>
<strong>Same</strong> <span class="math inline">\(j\)</span>, different <span class="math inline">\(i\)</span> (<span class="math inline">\(i \neq i'\)</span>)</td>
<td><span class="math inline">\(cov (Y_{ijk}, Y_{i'jk'}) = \sigma^2_{\beta} - \frac{\sigma^2_{\alpha \beta}}{2}\)</span></td>
</tr>
<tr class="odd">
<td>
<strong>Different</strong> <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> (<span class="math inline">\(i \neq i'\)</span>, <span class="math inline">\(j \neq j'\)</span>)</td>
<td><span class="math inline">\(cov(Y_{ijk}, Y_{i'j'k'}) = 0\)</span></td>
</tr>
</tbody>
</table></div>
<p>Thus, only observations within the same random factor level share dependence.</p>
<hr>
</div>
<div id="matrix-representation" class="section level5" number="24.1.5.2.4">
<h5>
<span class="header-section-number">24.1.5.2.4</span> Matrix Representation<a class="anchor" aria-label="anchor" href="#matrix-representation"><i class="fas fa-link"></i></a>
</h5>
<p>Assume:</p>
<p><span class="math display">\[
\mathbf{Y} \sim N(\mathbf{X} \beta, M)
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{X}\)</span>: Fixed effects design matrix.</li>
<li>
<span class="math inline">\(\beta\)</span>: Fixed effect coefficients.</li>
<li>
<span class="math inline">\(M\)</span>: Block diagonal covariance matrix containing variance components.</li>
</ul>
<p>The density function of <span class="math inline">\(\mathbf{Y}\)</span> is:</p>
<p><span class="math display">\[
f(\mathbf{Y}) = \frac{1}{(2\pi)^{N/2} |M|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{Y} - \mathbf{X} \beta)' M^{-1} (\mathbf{Y} - \mathbf{X} \beta) \right)
\]</span></p>
<p>If variance components were known, we could use Generalized Least Squares:</p>
<p><span class="math display">\[
\hat{\beta}_{GLS} = (\mathbf{X}' M^{-1} \mathbf{X})^{-1} \mathbf{X}' M^{-1} \mathbf{Y}
\]</span></p>
<p>However, since variance components (<span class="math inline">\(\sigma^2, \sigma^2_\beta, \sigma^2_{\alpha \beta}\)</span>) are unknown, we estimate them using:</p>
<ul>
<li><a href="linear-regression.html#maximum-likelihood-estimator">Maximum Likelihood</a></li>
<li><a href="sec-linear-mixed-models.html#restricted-maximum-likelihood-lmm">Restricted Maximum Likelihood</a></li>
</ul>
<p>Maximizing the likelihood:</p>
<p><span class="math display">\[
\ln L = - \frac{N}{2} \ln (2\pi) - \frac{1}{2} \ln |M| - \frac{1}{2} (\mathbf{Y} - \mathbf{X} \beta)' M^{-1} (\mathbf{Y} - \mathbf{X} \beta)
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(|M|\)</span>: Determinant of the variance-covariance matrix.</li>
<li>
<span class="math inline">\((\mathbf{Y} - \mathbf{X} \beta)' M^{-1} (\mathbf{Y} - \mathbf{X} \beta)\)</span>: Quadratic form in the likelihood.</li>
</ul>
<hr>
</div>
</div>
</div>
</div>
<div id="sec-nonparametric-anova" class="section level2" number="24.2">
<h2>
<span class="header-section-number">24.2</span> Nonparametric ANOVA<a class="anchor" aria-label="anchor" href="#sec-nonparametric-anova"><i class="fas fa-link"></i></a>
</h2>
<p>When assumptions of <strong>normality</strong> and <strong>equal variance</strong> are not satisfied, we use nonparametric ANOVA tests, which rank the data instead of using raw values.</p>
<hr>
<div id="kruskal-wallis-test-one-way-nonparametric-anova" class="section level3" number="24.2.1">
<h3>
<span class="header-section-number">24.2.1</span> Kruskal-Wallis Test (One-Way Nonparametric ANOVA)<a class="anchor" aria-label="anchor" href="#kruskal-wallis-test-one-way-nonparametric-anova"><i class="fas fa-link"></i></a>
</h3>
<p>The Kruskal-Wallis test is a generalization of the Wilcoxon rank-sum test to more than two independent samples. It is an alternative to one-way ANOVA when normality is not assumed.</p>
<p>Setup</p>
<ul>
<li>
<span class="math inline">\(a \geq 2\)</span> independent treatments.</li>
<li>
<span class="math inline">\(n_i\)</span> is the sample size for the <span class="math inline">\(i\)</span>-th treatment.</li>
<li>
<span class="math inline">\(Y_{ij}\)</span> is the <span class="math inline">\(j\)</span>-th observation from the <span class="math inline">\(i\)</span>-th treatment.</li>
<li>No assumption of normality.</li>
<li>Assume observations are independent random samples from continuous CDFs <span class="math inline">\(F_1, F_2, \dots, F_a\)</span>.</li>
</ul>
<p>Hypotheses</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: F_1 = F_2 = \dots = F_a \quad \text{(All distributions are identical)} \\
&amp;H_a: F_i &lt; F_j \text{ for some } i \neq j
\end{aligned}
\]</span> If the data come from a location-scale family, the hypothesis simplifies to:</p>
<p><span class="math display">\[
H_0: \theta_1 = \theta_2 = \dots = \theta_a
\]</span></p>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>
<p>Rank all <span class="math inline">\(N = \sum_{i=1}^a n_i\)</span> observations in ascending order.<br>
Let <span class="math inline">\(r_{ij} = rank(Y_{ij})\)</span><br>
The sum of ranks must satisfy:</p>
<p><span class="math display">\[
\sum_i \sum_j r_{ij} = \frac{N(N+1)}{2}
\]</span></p>
</li>
<li><p>Compute rank sums and averages: <span class="math display">\[
r_{i.} = \sum_{j=1}^{n_i} r_{ij}, \quad \bar{r}_{i.} = \frac{r_{i.}}{n_i}
\]</span></p></li>
<li>
<p>Calculate the test statistic:</p>
<p><span class="math display">\[
\chi_{KW}^2 = \frac{SSTR}{\frac{SSTO}{N-1}}
\]</span></p>
<p>where:</p>
<ul>
<li>Treatment Sum of Squares: <span class="math display">\[
SSTR = \sum n_i (\bar{r}_{i.} - \bar{r}_{..})^2
\]</span>
</li>
<li>Total Sum of Squares: <span class="math display">\[
SSTO = \sum_i \sum_j (r_{ij} - \bar{r}_{..})^2
\]</span>
</li>
<li>Overall Mean Rank: <span class="math display">\[
\bar{r}_{..} = \frac{N+1}{2}
\]</span>
</li>
</ul>
</li>
<li>
<p>Compare to a chi-square distribution:</p>
<ul>
<li>For large <span class="math inline">\(n_i\)</span> (<span class="math inline">\(\geq 5\)</span>), <span class="math inline">\(\chi^2_{KW} \sim \chi^2_{a-1}\)</span>.</li>
<li>Reject <span class="math inline">\(H_0\)</span> if: <span class="math display">\[
\chi^2_{KW} &gt; \chi^2_{(1-\alpha; a-1)}
\]</span>
</li>
</ul>
</li>
<li>
<p>Exact Test for Small Samples:</p>
<ul>
<li>Compute all possible rank assignments:<br><span class="math display">\[
\frac{N!}{n_1! n_2! \dots n_a!}
\]</span>
</li>
<li>Evaluate each Kruskal-Wallis statistic and determine the empirical p-value.</li>
</ul>
</li>
</ol>
<hr>
</div>
<div id="friedman-test-nonparametric-two-way-anova" class="section level3" number="24.2.2">
<h3>
<span class="header-section-number">24.2.2</span> Friedman Test (Nonparametric Two-Way ANOVA)<a class="anchor" aria-label="anchor" href="#friedman-test-nonparametric-two-way-anova"><i class="fas fa-link"></i></a>
</h3>
<p>The Friedman test is a distribution-free alternative to two-way ANOVA when data are measured in a randomized complete block design and normality cannot be assumed.</p>
<p>Setup</p>
<ul>
<li>
<span class="math inline">\(Y_{ij}\)</span> represents responses from <span class="math inline">\(n\)</span> blocks and <span class="math inline">\(r\)</span> treatments.</li>
<li>Assume no normality or homogeneity of variance.</li>
<li>Let <span class="math inline">\(F_{ij}\)</span> be the CDF of <span class="math inline">\(Y_{ij}\)</span>, corresponding to observed values.</li>
</ul>
<p>Hypotheses</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: F_{i1} = F_{i2} = \dots = F_{ir} \quad \forall i \quad \text{(Identical distributions within each block)} \\
&amp;H_a: F_{ij} &lt; F_{ij'} \text{ for some } j \neq j' \quad \forall i
\end{aligned}
\]</span></p>
<p>For location-scale families, the hypothesis simplifies to:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \tau_1 = \tau_2 = \dots = \tau_r \\
&amp;H_a: \tau_j &gt; \tau_{j'} \text{ for some } j \neq j'
\end{aligned}
\]</span></p>
<hr>
<p><strong>Procedure</strong></p>
<ol style="list-style-type: decimal">
<li>
<p>Rank observations within each block separately (ascending order).</p>
<ul>
<li>If there are ties, assign average ranks.</li>
</ul>
</li>
<li>
<p>Compute test statistic:</p>
<p><span class="math display">\[
\chi^2_F = \frac{SSTR}{\frac{SSTR + SSE}{n(r-1)}}
\]</span></p>
<p>where:</p>
<ul>
<li>Treatment Sum of Squares: <span class="math display">\[
SSTR = n \sum (\bar{r}_{.j} - \bar{r}_{..})^2
\]</span>
</li>
<li>Error Sum of Squares: <span class="math display">\[
SSE = \sum_i \sum_j (r_{ij} - \bar{r}_{.j})^2
\]</span>
</li>
<li>Mean Ranks: <span class="math display">\[
\bar{r}_{.j} = \frac{\sum_i r_{ij}}{n}, \quad \bar{r}_{..} = \frac{r+1}{2}
\]</span>
</li>
</ul>
</li>
<li>
<p>Alternative Formula for Large Samples (No Ties):</p>
<p>If no ties, Friedman’s statistic simplifies to:</p>
<p><span class="math display">\[
\chi^2_F = \left[\frac{12}{nr(n+1)} \sum_j r_{.j}^2\right] - 3n(r+1)
\]</span></p>
</li>
<li>
<p>Compare to a chi-square distribution:</p>
<ul>
<li>For large <span class="math inline">\(n\)</span>, <span class="math inline">\(\chi^2_F \sim \chi^2_{r-1}\)</span>.</li>
<li>Reject <span class="math inline">\(H_0\)</span> if: <span class="math display">\[
\chi^2_F &gt; \chi^2_{(1-\alpha; r-1)}
\]</span>
</li>
</ul>
</li>
<li>
<p>Exact Test for Small Samples:</p>
<ul>
<li>Compute all possible ranking permutations: <span class="math display">\[
(r!)^n
\]</span>
</li>
<li>Evaluate each Friedman statistic and determine the empirical p-value.</li>
</ul>
</li>
</ol>
<hr>
</div>
</div>
<div id="sec-randomized-block-designs" class="section level2" number="24.3">
<h2>
<span class="header-section-number">24.3</span> Randomized Block Designs<a class="anchor" aria-label="anchor" href="#sec-randomized-block-designs"><i class="fas fa-link"></i></a>
</h2>
<p>To improve the precision of treatment comparisons, we can <strong>reduce variability</strong> among experimental units by grouping them into <strong>blocks</strong>.<br>
Each block contains <strong>homogeneous units</strong>, reducing the impact of nuisance variation.</p>
<p><strong>Key Principles of Blocking</strong></p>
<ul>
<li>Within each block, treatments are randomly assigned to units.</li>
<li>The number of units per block is a multiple of the number of factor combinations.</li>
<li>Commonly, each treatment appears once per block.</li>
</ul>
<hr>
<p><strong>Benefits of Blocking</strong></p>
<ul>
<li>
<p>Reduction in variability of treatment effect estimates</p>
<ul>
<li><p>Improved power for t-tests and F-tests.</p></li>
<li><p>Narrower confidence intervals.</p></li>
<li><p>Smaller mean square error (MSE).</p></li>
</ul>
</li>
<li><p>Allows comparison of treatments across different conditions (captured by blocks).</p></li>
</ul>
<p><strong>Potential Downsides of Blocking</strong></p>
<ul>
<li><p>If blocks are not chosen well, degrees of freedom are wasted on negligible block effects.</p></li>
<li><p>This reduces df for t-tests and F-tests without reducing MSE, causing a small loss of power.</p></li>
</ul>
<hr>
<div id="sec-random-block-effects-with-additive-effects" class="section level4" number="24.3.0.1">
<h4>
<span class="header-section-number">24.3.0.1</span> Random Block Effects with Additive Effects<a class="anchor" aria-label="anchor" href="#sec-random-block-effects-with-additive-effects"><i class="fas fa-link"></i></a>
</h4>
<p>The statistical model for a randomized block design:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(i = 1, 2, \dots, n\)</span> (Blocks)</p></li>
<li><p><span class="math inline">\(j = 1, 2, \dots, r\)</span> (Treatments)</p></li>
<li><p><span class="math inline">\(\mu_{..}\)</span>: Overall mean response (averaged across all blocks and treatments).</p></li>
<li>
<p><span class="math inline">\(\rho_i\)</span>: Block effect (average difference for the <span class="math inline">\(i\)</span>-th block), constrained such that:</p>
<p><span class="math display">\[
\sum_i \rho_i = 0
\]</span></p>
</li>
<li>
<p><span class="math inline">\(\tau_j\)</span>: Treatment effect (average across blocks), constrained such that:</p>
<p><span class="math display">\[
\sum_j \tau_j = 0
\]</span></p>
</li>
<li><p><span class="math inline">\(\epsilon_{ij} \sim iid N(0, \sigma^2)\)</span>: Random experimental error.</p></li>
</ul>
<p><strong>Interpretation of the Model</strong></p>
<ul>
<li><p>Block and treatment effects are additive.</p></li>
<li>
<p>The difference in average response between any two treatments is the same within each block:</p>
<p><span class="math display">\[
(\mu_{..} + \rho_i + \tau_j) - (\mu_{..} + \rho_i + \tau_j') = \tau_j - \tau_j'
\]</span></p>
</li>
<li><p>This ensures that blocking only affects variability, not treatment comparisons.</p></li>
</ul>
<hr>
<p><strong>Estimators of Model Parameters</strong></p>
<ul>
<li>
<p><strong>Overall Mean</strong>:</p>
<p><span class="math display">\[
\hat{\mu} = \bar{Y}_{..}
\]</span></p>
</li>
<li>
<p><strong>Block Effects</strong>:</p>
<p><span class="math display">\[
\hat{\rho}_i = \bar{Y}_{i.} - \bar{Y}_{..}
\]</span></p>
</li>
<li>
<p><strong>Treatment Effects</strong>:</p>
<p><span class="math display">\[
\hat{\tau}_j = \bar{Y}_{.j} - \bar{Y}_{..}
\]</span></p>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>Fitted Response:</strong> <span class="math display">\[
\hat{Y}_{ij} = \bar{Y}_{..} + (\bar{Y}_{i.} - \bar{Y}_{..}) + (\bar{Y}_{.j} - \bar{Y}_{..})
\]</span></p>
<p>Simplifies to:</p>
<p><span class="math display">\[
\hat{Y}_{ij} = \bar{Y}_{i.} + \bar{Y}_{.j} - \bar{Y}_{..}
\]</span></p>
</li>
<li>
<p><strong>Residuals</strong>:</p>
<p><span class="math display">\[
e_{ij} = Y_{ij} - \hat{Y}_{ij} = Y_{ij} - \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..}
\]</span></p>
</li>
</ul>
<hr>
</div>
<div id="anova-table-for-randomized-block-design" class="section level4" number="24.3.0.2">
<h4>
<span class="header-section-number">24.3.0.2</span> ANOVA Table for Randomized Block Design<a class="anchor" aria-label="anchor" href="#anova-table-for-randomized-block-design"><i class="fas fa-link"></i></a>
</h4>
<p>The ANOVA decomposition partitions total variability into contributions from blocks, treatments, and residual error.</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="11%">
<col width="33%">
<col width="13%">
<col width="19%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th><strong>Source of Variation</strong></th>
<th><strong>Sum of Squares (SS)</strong></th>
<th><strong>Degrees of Freedom (df)</strong></th>
<th><strong>Fixed Treatments (E(MS))</strong></th>
<th><strong>Random Treatments (E(MS))</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Blocks</strong></td>
<td><span class="math inline">\(r \sum_i (\bar{Y}_{i.} - \bar{Y}_{..})^2\)</span></td>
<td><span class="math inline">\(n-1\)</span></td>
<td><span class="math inline">\(\sigma^2 + r \frac{\sum \rho_i^2}{n-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + r \frac{\sum \rho_i^2}{n-1}\)</span></td>
</tr>
<tr class="even">
<td><strong>Treatments</strong></td>
<td><span class="math inline">\(n \sum_j (\bar{Y}_{.j} - \bar{Y}_{..})^2\)</span></td>
<td><span class="math inline">\(r-1\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \frac{\sum \tau_j^2}{r-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_{\tau}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Error</strong></td>
<td><span class="math inline">\(\sum_i \sum_j (Y_{ij} - \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..})^2\)</span></td>
<td><span class="math inline">\((n-1)(r-1)\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="even">
<td><strong>Total</strong></td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(nr-1\)</span></td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table></div>
</div>
<div id="f-tests-in-randomized-block-designs" class="section level4" number="24.3.0.3">
<h4>
<span class="header-section-number">24.3.0.3</span> F-tests in Randomized Block Designs<a class="anchor" aria-label="anchor" href="#f-tests-in-randomized-block-designs"><i class="fas fa-link"></i></a>
</h4>
<p>To test for treatment effects, we use an F-test:</p>
<p>For <strong>fixed treatment effects</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
H_0: \tau_1 = \tau_2 = \dots = \tau_r = 0 \quad \text{(No treatment effect)} \\
H_a: \text{Not all } \tau_j = 0
\end{aligned}
\]</span></p>
<p>For <strong>random treatment effects</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
H_0: \sigma^2_{\tau} = 0 \quad \text{(No variance in treatment effects)} \\
H_a: \sigma^2_{\tau} \neq 0
\end{aligned}
\]</span></p>
<p>In both cases, the test statistic is:</p>
<p><span class="math display">\[
F = \frac{MSTR}{MSE}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
F &gt; f_{(1-\alpha; r-1, (n-1)(r-1))}
\]</span></p>
<hr>
<p><strong>Why Not Use an F-Test for Blocks?</strong></p>
<p>We do not test for block effects because:</p>
<ol style="list-style-type: decimal">
<li>Blocks are assumed to be different a priori.</li>
<li>Randomization occurs within each block, ensuring treatments are comparable.</li>
</ol>
<hr>
<p><strong>Efficiency Gain from Blocking</strong></p>
<p>To measure the improvement in precision, compare the mean square error (MSE) in a <a href="sec-analysis-of-variance-anova.html#sec-completely-randomized-design">completely randomized design</a> vs. a <a href="sec-analysis-of-variance-anova.html#sec-randomized-block-designs">randomized block design</a>.</p>
<p>Estimated variance in a <a href="sec-analysis-of-variance-anova.html#sec-completely-randomized-design">CRD</a>:</p>
<p><span class="math display">\[
\hat{\sigma}^2_{CR} = \frac{(n-1)MSBL + n(r-1)MSE}{nr-1}
\]</span></p>
<p>Estimated variance in an <a href="sec-analysis-of-variance-anova.html#sec-randomized-block-designs">RBD</a>:</p>
<p><span class="math display">\[
\hat{\sigma}^2_{RB} = MSE
\]</span></p>
<p>Relative efficiency:</p>
<p><span class="math display">\[
\frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}}
\]</span></p>
<ul>
<li><p>If greater than 1, blocking reduces experimental error.</p></li>
<li><p>The percentage reduction in required sample size for an RBD:</p></li>
</ul>
<p><span class="math display">\[
\left( \frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}} - 1 \right) \times 100\%
\]</span></p>
<hr>
<p><strong>Random Blocks and Mixed Models</strong></p>
<p>If blocks are randomly selected, they are treated as random effects.<br>
That is, if the experiment were repeated, a new set of blocks would be selected, with:</p>
<p><span class="math display">\[
\rho_1, \rho_2, \dots, \rho_i \sim N(0, \sigma^2_\rho)
\]</span></p>
<p>The model remains:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mu_{..}\)</span> is fixed.</li>
<li>
<span class="math inline">\(\rho_i \sim iid N(0, \sigma^2_\rho)\)</span> (random block effects).</li>
<li>
<span class="math inline">\(\tau_j\)</span> is fixed (or random, with <span class="math inline">\(\sum \tau_j = 0\)</span>).</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim iid N(0, \sigma^2)\)</span>.</li>
</ul>
<hr>
</div>
<div id="variance-and-covariance-structure" class="section level4" number="24.3.0.4">
<h4>
<span class="header-section-number">24.3.0.4</span> Variance and Covariance Structure<a class="anchor" aria-label="anchor" href="#variance-and-covariance-structure"><i class="fas fa-link"></i></a>
</h4>
<p>For <strong>fixed treatment effects</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_{ij}) &amp;= \mu_{..} + \tau_j \\
var(Y_{ij}) &amp;= \sigma^2_{\rho} + \sigma^2
\end{aligned}
\]</span></p>
<p>Observations <strong>within the same block</strong> are <strong>correlated</strong>:</p>
<p><span class="math display">\[
cov(Y_{ij}, Y_{ij'}) = \sigma^2_{\rho}, \quad j \neq j'
\]</span></p>
<p>Observations <strong>from different blocks</strong> are <strong>independent</strong>:</p>
<p><span class="math display">\[
cov(Y_{ij}, Y_{i'j'}) = 0, \quad i \neq i', j \neq j'
\]</span></p>
<p>The <strong>intra-block correlation</strong>:</p>
<p><span class="math display">\[
\frac{\sigma^2_{\rho}}{\sigma^2 + \sigma^2_{\rho}}
\]</span></p>
<hr>
<p><strong>Expected Mean Squares for Fixed Treatments</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Source</th>
<th>SS</th>
<th>E(MS)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Blocks</strong></td>
<td>SSBL</td>
<td><span class="math inline">\(\sigma^2 + r \sigma^2_\rho\)</span></td>
</tr>
<tr class="even">
<td><strong>Treatments</strong></td>
<td>SSTR</td>
<td><span class="math inline">\(\sigma^2 + n \frac{\sum \tau^2_j}{r-1}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Error</strong></td>
<td>SSE</td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="sec-random-block-effects-with-interaction" class="section level4" number="24.3.0.5">
<h4>
<span class="header-section-number">24.3.0.5</span> Random Block Effects with Interaction<a class="anchor" aria-label="anchor" href="#sec-random-block-effects-with-interaction"><i class="fas fa-link"></i></a>
</h4>
<p>When block-treatment interaction exists, we modify the model:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + (\rho \tau)_{ij} + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\rho_i \sim iid N(0, \sigma^2_{\rho})\)</span> (random).</p></li>
<li><p><span class="math inline">\(\tau_j\)</span> is fixed (<span class="math inline">\(\sum \tau_j = 0\)</span>).</p></li>
<li>
<p><span class="math inline">\((\rho \tau)_{ij} \sim N(0, \frac{r-1}{r} \sigma^2_{\rho \tau})\)</span>, constrained such that:</p>
<p><span class="math display">\[
\sum_j (\rho \tau)_{ij} = 0, \quad \forall i
\]</span></p>
</li>
<li>
<p>Covariance between interaction terms:</p>
<p><span class="math display">\[
cov((\rho \tau)_{ij}, (\rho \tau)_{ij'}) = -\frac{1}{r} \sigma^2_{\rho \tau}, \quad j \neq j'
\]</span></p>
</li>
<li><p><span class="math inline">\(\epsilon_{ij} \sim iid N(0, \sigma^2)\)</span>.</p></li>
</ul>
<hr>
<p><strong>Variance and Covariance with Interaction</strong></p>
<ul>
<li>
<p><strong>Expectation</strong>:</p>
<p><span class="math display">\[
E(Y_{ij}) = \mu_{..} + \tau_j
\]</span></p>
</li>
<li>
<p><strong>Total variance</strong>:</p>
<p><span class="math display">\[
var(Y_{ij}) = \sigma^2_\rho + \frac{r-1}{r} \sigma^2_{\rho \tau} + \sigma^2
\]</span></p>
</li>
<li>
<p><strong>Within-block covariance</strong>:</p>
<p><span class="math display">\[
cov(Y_{ij}, Y_{ij'}) = \sigma^2_\rho - \frac{1}{r} \sigma^2_{\rho \tau}, \quad j \neq j'
\]</span></p>
</li>
<li>
<p><strong>Between-block covariance</strong>:</p>
<p><span class="math display">\[
cov(Y_{ij}, Y_{i'j'}) = 0, \quad i \neq i', j \neq j'
\]</span></p>
</li>
</ul>
<p>The sum of squares and degrees of freedom for <a href="sec-analysis-of-variance-anova.html#sec-random-block-effects-with-interaction">interaction model</a> are the same as those for the <a href="sec-analysis-of-variance-anova.html#sec-random-block-effects-with-additive-effects">additive model</a>. The difference exists only in the expected mean squares.</p>
<hr>
</div>
<div id="anova-table-with-interaction-effects" class="section level4" number="24.3.0.6">
<h4>
<span class="header-section-number">24.3.0.6</span> ANOVA Table with Interaction Effects<a class="anchor" aria-label="anchor" href="#anova-table-with-interaction-effects"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table style="width:98%;" class="table table-sm">
<colgroup>
<col width="15%">
<col width="9%">
<col width="13%">
<col width="59%">
</colgroup>
<thead><tr class="header">
<th>Source</th>
<th>SS</th>
<th>df</th>
<th>E(MS)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Blocks</strong></td>
<td><span class="math inline">\(SSBL\)</span></td>
<td><span class="math inline">\(n-1\)</span></td>
<td><span class="math inline">\(\sigma^2 + r \sigma^2_\rho\)</span></td>
</tr>
<tr class="even">
<td><strong>Treatments</strong></td>
<td><span class="math inline">\(SSTR\)</span></td>
<td><span class="math inline">\(r-1\)</span></td>
<td><span class="math inline">\(\sigma^2 + \sigma^2_{\rho \tau} + n \frac{\sum \tau_j^2}{r-1}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Error</strong></td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\((n-1)(r-1)\)</span></td>
<td><span class="math inline">\(\sigma^2 + \sigma^2_{\rho \tau}\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
<ul>
<li>No exact test is possible for block effects when interaction is present (Not important if blocks are used primarily to reduce experimental error variability)</li>
<li>
<span class="math inline">\(E(MSE) = \sigma^2 + \sigma^2_{\rho \tau}\)</span> the error term variance and interaction variance <span class="math inline">\(\sigma^2_{\rho \tau}\)</span>. We can’t estimate these components separately with this model. The two are <strong>confounded</strong>.</li>
<li>If more than one observation per treatment block combination, one can consider interaction with fixed block effects, which is called <strong>generalized randomized block designs</strong> (multifactor analysis).</li>
</ul>
<hr>
</div>
<div id="tukey-test-of-additivity" class="section level4" number="24.3.0.7">
<h4>
<span class="header-section-number">24.3.0.7</span> Tukey Test of Additivity<a class="anchor" aria-label="anchor" href="#tukey-test-of-additivity"><i class="fas fa-link"></i></a>
</h4>
<p>Tukey’s 1-degree-of-freedom test for additivity provides a formal test for interaction effects between blocks and treatments in a randomized block design.</p>
<p>This test can also be used in two-way ANOVA when there is only one observation per cell.</p>
<hr>
<p>In a <a href="sec-analysis-of-variance-anova.html#sec-randomized-block-designs">randomized block design</a>, an additive model assumes:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\mu_{..}\)</span> = overall mean</p></li>
<li><p><span class="math inline">\(\rho_i\)</span> = block effect</p></li>
<li><p><span class="math inline">\(\tau_j\)</span> = treatment effect</p></li>
<li><p><span class="math inline">\(\epsilon_{ij}\)</span> = random error, <span class="math inline">\(iid N(0, \sigma^2)\)</span></p></li>
</ul>
<p>To test for interaction, we introduce a less restricted interaction term:</p>
<p><span class="math display">\[
(\rho \tau)_{ij} = D \rho_i \tau_j
\]</span></p>
<p>where <span class="math inline">\(D\)</span> is a constant measuring interaction strength.</p>
<p>Thus, the interaction model becomes:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + D\rho_i \tau_j + \epsilon_{ij}
\]</span></p>
<p>The least squares estimate (or MLE) of <span class="math inline">\(D\)</span> is:</p>
<p><span class="math display">\[
\hat{D} = \frac{\sum_i \sum_j \rho_i \tau_j Y_{ij}}{\sum_i \rho_i^2 \sum_j \tau_j^2}
\]</span></p>
<p>Replacing <span class="math inline">\(\rho_i\)</span> and <span class="math inline">\(\tau_j\)</span> with their estimates:</p>
<p><span class="math display">\[
\hat{D} = \frac{\sum_i \sum_j (\bar{Y}_{i.} - \bar{Y}_{..})(\bar{Y}_{.j} - \bar{Y}_{..}) Y_{ij}}{\sum_i (\bar{Y}_{i.} - \bar{Y}_{..})^2 \sum_j (\bar{Y}_{.j} - \bar{Y}_{..})^2}
\]</span></p>
<p>The sum of squares for interaction is:</p>
<p><span class="math display">\[
SS_{int} = \sum_i \sum_j \hat{D}^2 (\bar{Y}_{i.} - \bar{Y}_{..})^2 (\bar{Y}_{.j} - \bar{Y}_{..})^2
\]</span></p>
<hr>
<p><strong>ANOVA Decomposition</strong></p>
<p>The total sum of squares (SSTO) is decomposed as:</p>
<p><span class="math display">\[
SSTO = SSBL + SSTR + SS_{int} + SS_{Rem}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(SSBL\)</span> = Sum of squares due to blocks</p></li>
<li><p><span class="math inline">\(SSTR\)</span> = Sum of squares due to treatments</p></li>
<li><p><span class="math inline">\(SS_{int}\)</span> = Interaction sum of squares</p></li>
<li><p><span class="math inline">\(SS_{Rem}\)</span> = Remainder sum of squares, computed as:</p></li>
</ul>
<p><span class="math display">\[
SS_{Rem} = SSTO - SSBL - SSTR - SS_{int}
\]</span></p>
<hr>
<p>We test:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: D = 0 \quad \text{(No interaction present)} \\
&amp;H_a: D \neq 0 \quad \text{(Interaction of form $D \rho_i \tau_j$ present)}
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(D = 0\)</span>, then <span class="math inline">\(SS_{int}\)</span> and <span class="math inline">\(SS_{Rem}\)</span> are independent and follow:</p>
<p><span class="math display">\[
SS_{int} \sim \chi^2_1, \quad SS_{Rem} \sim \chi^2_{(rn-r-n)}
\]</span></p>
<p>Thus, the F-statistic for testing interaction is:</p>
<p><span class="math display">\[
F = \frac{SS_{int} / 1}{SS_{Rem} / (rn - r - n)}
\]</span></p>
<p>which follows an <span class="math inline">\(F\)</span>-distribution:</p>
<p><span class="math display">\[
F \sim F_{(1, nr - r - n)}
\]</span></p>
<p>We reject <span class="math inline">\(H_0\)</span> if:</p>
<p><span class="math display">\[
F &gt; f_{(1-\alpha; 1, nr - r - n)}
\]</span></p>
<hr>
</div>
</div>
<div id="nested-designs" class="section level2" number="24.4">
<h2>
<span class="header-section-number">24.4</span> Nested Designs<a class="anchor" aria-label="anchor" href="#nested-designs"><i class="fas fa-link"></i></a>
</h2>
<p>A nested design occurs when one factor is entirely contained within another. This differs from a crossed design, where all levels of one factor are present across all levels of another factor.</p>
<ul>
<li>
<strong>Crossed Design</strong>: If Factor B is crossed with Factor A, then each level of Factor B appears at every level of Factor A.</li>
<li>
<strong>Nested Design</strong>: If Factor B is nested within Factor A, then each level of Factor B is unique to a particular level of Factor A.</li>
</ul>
<p>Thus, if Factor B is nested within Factor A:</p>
<ul>
<li><p>Level 1 of B within A = 1 has nothing in common with</p></li>
<li><p>Level 1 of B within A = 2.</p></li>
</ul>
<p><strong>Types of Factors</strong></p>
<ul>
<li>
<strong>Classification Factors</strong>: Factors that <strong>cannot be manipulated</strong> (e.g., geographical regions, subjects).</li>
<li>
<strong>Experimental Factors</strong>: Factors that are <strong>randomly assigned</strong> in an experiment.</li>
</ul>
<hr>
<div id="two-factor-nested-design" class="section level3" number="24.4.1">
<h3>
<span class="header-section-number">24.4.1</span> Two-Factor Nested Design<a class="anchor" aria-label="anchor" href="#two-factor-nested-design"><i class="fas fa-link"></i></a>
</h3>
<p>We consider a nested two-factor model where:</p>
<ul>
<li><p>Factor A has <span class="math inline">\(a\)</span> levels.</p></li>
<li><p>Factor B is nested within Factor A, with <span class="math inline">\(b\)</span> levels per level of A.</p></li>
<li><p>Both factors are fixed.</p></li>
<li><p>All treatment means are equally important.</p></li>
</ul>
<hr>
<p>The mean response at level <span class="math inline">\(i\)</span> of Factor A:</p>
<p><span class="math display">\[
\mu_{i.} = \frac{1}{b} \sum_j \mu_{ij}
\]</span></p>
<p>The main effect of Factor A:</p>
<p><span class="math display">\[
\alpha_i = \mu_{i.} - \mu_{..}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mu_{..} = \frac{1}{ab} \sum_i \sum_j \mu_{ij} = \frac{1}{a} \sum_i \mu_{i.}
\]</span></p>
<p>with the constraint:</p>
<p><span class="math display">\[
\sum_i \alpha_i = 0
\]</span></p>
<p>The nested effect of Factor B within A is denoted as <span class="math inline">\(\beta_{j(i)}\)</span>, where:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_{j(i)} &amp;= \mu_{ij} - \mu_{i.} \\
&amp;= \mu_{ij} - \alpha_i - \mu_{..}
\end{aligned}
\]</span></p>
<p>with the restriction:</p>
<p><span class="math display">\[
\sum_j \beta_{j(i)} = 0, \quad \forall i = 1, \dots, a
\]</span></p>
<p>Since <span class="math inline">\(\beta_{j(i)}\)</span> is the <strong>specific effect</strong> of the <span class="math inline">\(j\)</span>-th level of factor <span class="math inline">\(B\)</span> nested within the <span class="math inline">\(i\)</span>-th level of factor <span class="math inline">\(A\)</span>, the full model can be written as:</p>
<p><span class="math display">\[
\mu_{ij} = \mu_{..} + \alpha_i + \beta_{j(i)}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\mu_{ij} = \mu_{..} + (\mu_{i.} - \mu_{..}) + (\mu_{ij} - \mu_{i.})
\]</span></p>
<hr>
<p>The statistical model for a two-factor nested design is:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk}
\]</span></p>
<p>where:</p>
<ul>
<li>
<p><span class="math inline">\(Y_{ijk}\)</span> = response for the <span class="math inline">\(k\)</span>-th observation when:</p>
<ul>
<li><p><strong>Factor A</strong> is at level <span class="math inline">\(i\)</span>.</p></li>
<li><p><strong>Factor B</strong> (nested within A) is at level <span class="math inline">\(j\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math inline">\(\mu_{..}\)</span> = overall mean.</p></li>
<li><p><span class="math inline">\(\alpha_i\)</span> = main effect of <strong>Factor A</strong> (subject to: <span class="math inline">\(\sum_i \alpha_i = 0\)</span>).</p></li>
<li><p><span class="math inline">\(\beta_{j(i)}\)</span> = nested effect of <strong>Factor B within A</strong> (subject to: <span class="math inline">\(\sum_j \beta_{j(i)} = 0\)</span> for all <span class="math inline">\(i\)</span>).</p></li>
<li><p><span class="math inline">\(\epsilon_{ijk} \sim iid N(0, \sigma^2)\)</span> = random error.</p></li>
</ul>
<p>Thus, the expected value and variance are:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_{ijk}) &amp;= \mu_{..} + \alpha_i + \beta_{j(i)} \\
var(Y_{ijk}) &amp;= \sigma^2
\end{aligned}
\]</span></p>
<p><strong>Note</strong>: There is no interaction term in a nested model, because Factor B levels are unique within each level of A.</p>
<hr>
<p>The least squares and maximum likelihood estimates:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th><strong>Parameter</strong></th>
<th><strong>Estimator</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu_{..}\)</span></td>
<td><span class="math inline">\(\bar{Y}_{...}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_i\)</span></td>
<td><span class="math inline">\(\bar{Y}_{i..} - \bar{Y}_{...}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_{j(i)}\)</span></td>
<td><span class="math inline">\(\bar{Y}_{ij.} - \bar{Y}_{i..}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y}_{ijk}\)</span></td>
<td><span class="math inline">\(\bar{Y}_{ij.}\)</span></td>
</tr>
</tbody>
</table></div>
<p>The residual error:</p>
<p><span class="math display">\[
e_{ijk} = Y_{ijk} - \bar{Y}_{ij.}
\]</span></p>
<hr>
<p>The total sum of squares (SSTO) is partitioned as:</p>
<p><span class="math display">\[
SSTO = SSA + SSB(A) + SSE
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_i \sum_j \sum_k (Y_{ijk} - \bar{Y}_{...})^2
&amp;= bn \sum_i (\bar{Y}_{i..} - \bar{Y}_{...})^2
+ n \sum_i \sum_j (\bar{Y}_{ij.} - \bar{Y}_{i..})^2 \\
&amp;+ \sum_i \sum_j \sum_k (Y_{ijk} - \bar{Y}_{ij.})^2
\end{aligned}
\]</span></p>
<hr>
<div id="anova-table-for-nested-designs" class="section level4" number="24.4.1.1">
<h4>
<span class="header-section-number">24.4.1.1</span> ANOVA Table for Nested Designs<a class="anchor" aria-label="anchor" href="#anova-table-for-nested-designs"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table style="width:97%;" class="table table-sm">
<colgroup>
<col width="22%">
<col width="9%">
<col width="10%">
<col width="9%">
<col width="45%">
</colgroup>
<thead><tr class="header">
<th><strong>Source of Variation</strong></th>
<th><strong>SS</strong></th>
<th><strong>df</strong></th>
<th><strong>MS</strong></th>
<th><strong>E(MS)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Factor A</strong></td>
<td><span class="math inline">\(SSA\)</span></td>
<td><span class="math inline">\(a-1\)</span></td>
<td><span class="math inline">\(MSA\)</span></td>
<td><span class="math inline">\(\sigma^2 + bn \frac{\sum \alpha_i^2}{a-1}\)</span></td>
</tr>
<tr class="even">
<td><strong>Factor B (A)</strong></td>
<td><span class="math inline">\(SSB(A)\)</span></td>
<td><span class="math inline">\(a(b-1)\)</span></td>
<td><span class="math inline">\(MSB(A)\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \frac{\sum \beta_{j(i)}^2}{a(b-1)}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Error</strong></td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(ab(n-1)\)</span></td>
<td><span class="math inline">\(MSE\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="even">
<td><strong>Total</strong></td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(abn -1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="tests-for-factor-effects" class="section level4" number="24.4.1.2">
<h4>
<span class="header-section-number">24.4.1.2</span> Tests For Factor Effects<a class="anchor" aria-label="anchor" href="#tests-for-factor-effects"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<p>Factor A:</p>
<p><span class="math display">\[
F = \frac{MSA}{MSB(A)} \sim F_{(a-1, a(b-1))}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F &gt; f_{(1-\alpha; a-1, a(b-1))}\)</span>.</p>
</li>
<li>
<p>Factor B within A:</p>
<p><span class="math display">\[
F = \frac{MSB(A)}{MSE} \sim F_{(a(b-1), ab(n-1))}
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(F &gt; f_{(1-\alpha; a(b-1), ab(n-1))}\)</span>.</p>
</li>
</ul>
<hr>
</div>
<div id="testing-factor-effect-contrasts" class="section level4" number="24.4.1.3">
<h4>
<span class="header-section-number">24.4.1.3</span> Testing Factor Effect Contrasts<a class="anchor" aria-label="anchor" href="#testing-factor-effect-contrasts"><i class="fas fa-link"></i></a>
</h4>
<p>A contrast is a linear combination of factor level means:</p>
<p><span class="math display">\[
L = \sum c_i \mu_i, \quad \text{where} \quad \sum c_i = 0
\]</span></p>
<p>The estimated contrast:</p>
<p><span class="math display">\[
\hat{L} = \sum c_i \bar{Y}_{i..}
\]</span></p>
<p>The confidence interval for <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[
\hat{L} \pm t_{(1-\alpha/2; df)} s(\hat{L})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
s^2(\hat{L}) = \sum c_i^2 s^2(\bar{Y}_{i..}), \quad \text{where} \quad s^2(\bar{Y}_{i..}) = \frac{MSE}{bn}, \quad df = ab(n-1)
\]</span></p>
<hr>
</div>
<div id="testing-treatment-means" class="section level4" number="24.4.1.4">
<h4>
<span class="header-section-number">24.4.1.4</span> Testing Treatment Means<a class="anchor" aria-label="anchor" href="#testing-treatment-means"><i class="fas fa-link"></i></a>
</h4>
<p>For treatment means, a similar approach applies:</p>
<p><span class="math display">\[
L = \sum c_i \mu_{.j}, \quad \hat{L} = \sum c_i \bar{Y}_{ij}
\]</span></p>
<p>The confidence limits for <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[
\hat{L} \pm t_{(1-\alpha/2; (n-1)ab)} s(\hat{L})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
s^2(\hat{L}) = \frac{MSE}{n} \sum c_i^2
\]</span></p>
<hr>
</div>
</div>
<div id="unbalanced-nested-two-factor-designs" class="section level3" number="24.4.2">
<h3>
<span class="header-section-number">24.4.2</span> Unbalanced Nested Two-Factor Designs<a class="anchor" aria-label="anchor" href="#unbalanced-nested-two-factor-designs"><i class="fas fa-link"></i></a>
</h3>
<p>When Factor B has different levels for different levels of Factor A, the design is unbalanced.</p>
<p><span class="math display">\[
\begin{aligned}
Y_{ijk} &amp;= \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk} \\
\sum_{i=1}^2 \alpha_i &amp;= 0, \quad
\sum_{j=1}^3 \beta_{j(1)} = 0, \quad
\sum_{j=1}^2 \beta_{j(2)} = 0
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li><p>Factor A: <span class="math inline">\(i = 1, 2\)</span>.</p></li>
<li><p>Factor B (nested in A): <span class="math inline">\(j = 1, \dots, b_i\)</span>.</p></li>
<li><p>Observations: <span class="math inline">\(k = 1, \dots, n_{ij}\)</span>.</p></li>
</ul>
<p>Example case:</p>
<ul>
<li><p><span class="math inline">\(b_1 = 3, b_2 = 2\)</span> (Factor B has different levels for A).</p></li>
<li><p><span class="math inline">\(n_{11} = n_{13} = 2, n_{12} = 1, n_{21} = n_{22} = 2\)</span>.</p></li>
<li><p>Parameters: <span class="math inline">\(\alpha_1, \beta_{1(1)}, \beta_{2(1)}, \beta_{1(2)}\)</span>.</p></li>
</ul>
<p>Constraints:</p>
<p><span class="math display">\[
\alpha_2 = -\alpha_1, \quad
\beta_{3(1)} = -\beta_{1(1)} - \beta_{2(1)}, \quad
\beta_{2(2)} = -\beta_{1(2)}
\]</span></p>
<hr>
<p>The unbalanced design can be modeled using <strong>indicator variables</strong>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Factor A (School Level):</strong> <span class="math display">\[
X_1 =
\begin{cases}
1 &amp; \text{if observation from school 1} \\
-1 &amp; \text{if observation from school 2}
\end{cases}
\]</span></p></li>
<li><p><strong>Factor B (Instructor within School 1):</strong> <span class="math display">\[
X_2 =
\begin{cases}
1 &amp; \text{if observation from instructor 1 in school 1} \\
-1 &amp; \text{if observation from instructor 3 in school 1} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p></li>
<li><p><strong>Factor B (Instructor within School 1):</strong> <span class="math display">\[
X_3 =
\begin{cases}
1 &amp; \text{if observation from instructor 2 in school 1} \\
-1 &amp; \text{if observation from instructor 3 in school 1} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p></li>
<li><p><strong>Factor B (Instructor within School 1):</strong> <span class="math display">\[
X_4 =
\begin{cases}
1 &amp; \text{if observation from instructor 1 in school 1} \\
-1 &amp; \text{if observation from instructor 2 in school 1} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p></li>
</ol>
<p>Using these indicator variables, the <strong>full regression model</strong> is:</p>
<p><span class="math display">\[
Y_{ijk} = \mu_{..} + \alpha_1 X_{ijk1} + \beta_{1(1)} X_{ijk2} + \beta_{2(1)} X_{ijk3} + \beta_{1(2)} X_{ijk4} + \epsilon_{ijk}
\]</span></p>
<p>where <span class="math inline">\(X_1, X_2, X_3, X_4\)</span> represent different factor effects.</p>
<hr>
</div>
<div id="random-factor-effects" class="section level3" number="24.4.3">
<h3>
<span class="header-section-number">24.4.3</span> Random Factor Effects<a class="anchor" aria-label="anchor" href="#random-factor-effects"><i class="fas fa-link"></i></a>
</h3>
<p>If factors are <strong>random</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_1 &amp;\sim iid N(0, \sigma^2_\alpha) \\
\beta_{j(i)} &amp;\sim iid N(0, \sigma^2_\beta)
\end{aligned}
\]</span></p>
<hr>
<p><strong>Expected Mean Squares for Random Effects</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="12%">
<col width="46%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th><strong>Mean Square</strong></th>
<th><strong>Expected Mean Squares (A Fixed, B Random)</strong></th>
<th><strong>Expected Mean Squares (A Random, B Random)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>MSA</strong></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_\beta + bn \frac{\sum \alpha_i^2}{a-1}\)</span></td>
<td><span class="math inline">\(\sigma^2 + bn \sigma^2_{\alpha} + n \sigma^2_\beta\)</span></td>
</tr>
<tr class="even">
<td><strong>MSB(A)</strong></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_\beta\)</span></td>
<td><span class="math inline">\(\sigma^2 + n \sigma^2_\beta\)</span></td>
</tr>
<tr class="odd">
<td><strong>MSE</strong></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
<p><strong>F-Tests for Factor Effects</strong></p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="20%">
<col width="38%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th><strong>Factor</strong></th>
<th><strong>F-Test (A Fixed, B Random)</strong></th>
<th><strong>F-Test (A Random, B Random)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Factor A</strong></td>
<td><span class="math inline">\(\frac{MSA}{MSB(A)}\)</span></td>
<td><span class="math inline">\(\frac{MSA}{MSB(A)}\)</span></td>
</tr>
<tr class="even">
<td><strong>Factor B(A)</strong></td>
<td><span class="math inline">\(\frac{MSB(A)}{MSE}\)</span></td>
<td><span class="math inline">\(\frac{MSB(A)}{MSE}\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
<p>Another way to increase precision in treatment comparisons is by adjusting for covariates using regression models. This is called Analysis of Covariance (ANCOVA).</p>
<p><strong>Why use ANCOVA?</strong></p>
<ul>
<li><p>Reduces variability by accounting for covariate effects.</p></li>
<li><p>Increases statistical power by removing nuisance variation.</p></li>
<li><p>Combines ANOVA and regression for more precise comparisons.</p></li>
</ul>
<hr>
</div>
</div>
<div id="sample-size-planning-for-anova" class="section level2" number="24.5">
<h2>
<span class="header-section-number">24.5</span> Sample Size Planning for ANOVA<a class="anchor" aria-label="anchor" href="#sample-size-planning-for-anova"><i class="fas fa-link"></i></a>
</h2>
<div id="balanced-designs" class="section level3" number="24.5.1">
<h3>
<span class="header-section-number">24.5.1</span> Balanced Designs<a class="anchor" aria-label="anchor" href="#balanced-designs"><i class="fas fa-link"></i></a>
</h3>
<p>Choosing an appropriate sample size for an ANOVA study requires ensuring sufficient power while balancing practical constraints.</p>
</div>
<div id="single-factor-studies" class="section level3" number="24.5.2">
<h3>
<span class="header-section-number">24.5.2</span> Single Factor Studies<a class="anchor" aria-label="anchor" href="#single-factor-studies"><i class="fas fa-link"></i></a>
</h3>
<div id="fixed-cell-means-model" class="section level4" number="24.5.2.1">
<h4>
<span class="header-section-number">24.5.2.1</span> Fixed Cell Means Model<a class="anchor" aria-label="anchor" href="#fixed-cell-means-model"><i class="fas fa-link"></i></a>
</h4>
<p>The probability of rejecting <span class="math inline">\(H_0\)</span> when it is false (power) is given by:</p>
<p><span class="math display">\[
P(F &gt; f_{(1-\alpha; a-1, N-a)} | \phi) = 1 - \beta
\]</span></p>
<p>where:</p>
<ul>
<li>
<p><span class="math inline">\(\phi\)</span> is the non-centrality parameter (measuring the inequality among treatment means <span class="math inline">\(\mu_i\)</span>):</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n}{a} \sum_{i} (\mu_i - \mu_.)^2}, \quad (n_i \equiv n)
\]</span></p>
</li>
<li>
<p><span class="math inline">\(\mu_.\)</span> is the overall mean:</p>
<p><span class="math display">\[
\mu_. = \frac{\sum \mu_i}{a}
\]</span></p>
</li>
</ul>
<p>To determine power, we use the non-central F distribution.</p>
<hr>
<p><strong>Using Power Tables</strong></p>
<p>Power tables can be used directly when:</p>
<ol style="list-style-type: decimal">
<li><p>The effects are fixed.</p></li>
<li><p>The design is balanced.</p></li>
<li>
<p>The minimum range of factor level means <span class="math inline">\(\Delta\)</span> is known:</p>
<p><span class="math display">\[
\Delta = \max(\mu_i) - \min(\mu_i)
\]</span></p>
</li>
</ol>
<p>Thus, the required inputs are:</p>
<ul>
<li>Significance level (<span class="math inline">\(\alpha\)</span>)</li>
<li>Minimum range of means (<span class="math inline">\(\Delta\)</span>)</li>
<li>Error standard deviation (<span class="math inline">\(\sigma\)</span>)</li>
<li>Power (<span class="math inline">\(1 - \beta\)</span>)</li>
</ul>
<p>Notes on Sample Size Sensitivity</p>
<ul>
<li>When <span class="math inline">\(\Delta/\sigma\)</span> is small, sample size requirements increase dramatically.</li>
<li>Lowering <span class="math inline">\(\alpha\)</span> or <span class="math inline">\(\beta\)</span> increases required sample sizes.</li>
<li>Errors in estimating <span class="math inline">\(\sigma\)</span> can significantly impact sample size calculations.</li>
</ul>
<hr>
</div>
</div>
<div id="multi-factor-studies" class="section level3" number="24.5.3">
<h3>
<span class="header-section-number">24.5.3</span> Multi-Factor Studies<a class="anchor" aria-label="anchor" href="#multi-factor-studies"><i class="fas fa-link"></i></a>
</h3>
<p>The same noncentral <span class="math inline">\(F\)</span> tables apply for multi-factor models.</p>
<div id="two-factor-fixed-effects-model" class="section level4" number="24.5.3.1">
<h4>
<span class="header-section-number">24.5.3.1</span> Two-Factor Fixed Effects Model<a class="anchor" aria-label="anchor" href="#two-factor-fixed-effects-model"><i class="fas fa-link"></i></a>
</h4>
<div id="test-for-interaction-effects" class="section level5" number="24.5.3.1.1">
<h5>
<span class="header-section-number">24.5.3.1.1</span> Test for Interaction Effects<a class="anchor" aria-label="anchor" href="#test-for-interaction-effects"><i class="fas fa-link"></i></a>
</h5>
<p>The non-centrality parameter:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n \sum_i \sum_j (\alpha \beta)_{ij}^2}{(a-1)(b-1)+1}}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n \sum_i \sum_j (\mu_{ij} - \mu_{i.} - \mu_{.j} + \mu_{..})^2}{(a-1)(b-1)+1}}
\]</span></p>
<p>where degrees of freedom are:</p>
<p><span class="math display">\[
\begin{aligned}
\upsilon_1 &amp;= (a-1)(b-1) \\
\upsilon_2 &amp;= ab(n-1)
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="test-for-factor-a-main-effects" class="section level5" number="24.5.3.1.2">
<h5>
<span class="header-section-number">24.5.3.1.2</span> Test for Factor <span class="math inline">\(A\)</span> Main Effects<a class="anchor" aria-label="anchor" href="#test-for-factor-a-main-effects"><i class="fas fa-link"></i></a>
</h5>
<p>The non-centrality parameter:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{nb \sum \alpha_i^2}{a}}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{nb \sum (\mu_{i.} - \mu_{..})^2}{a}}
\]</span></p>
<p>where degrees of freedom are:</p>
<p><span class="math display">\[
\begin{aligned}
\upsilon_1 &amp;= a-1 \\
\upsilon_2 &amp;= ab(n-1)
\end{aligned}
\]</span></p>
<hr>
</div>
<div id="test-for-factor-b-main-effects" class="section level5" number="24.5.3.1.3">
<h5>
<span class="header-section-number">24.5.3.1.3</span> Test for Factor <span class="math inline">\(B\)</span> Main Effects<a class="anchor" aria-label="anchor" href="#test-for-factor-b-main-effects"><i class="fas fa-link"></i></a>
</h5>
<p>The non-centrality parameter:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{na \sum \beta_j^2}{b}}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{na \sum (\mu_{.j} - \mu_{..})^2}{b}}
\]</span></p>
<p>where degrees of freedom are:</p>
<p><span class="math display">\[
\begin{aligned}
\upsilon_1 &amp;= b-1 \\
\upsilon_2 &amp;= ab(n-1)
\end{aligned}
\]</span></p>
<hr>
</div>
</div>
</div>
<div id="procedure-for-sample-size-selection" class="section level3" number="24.5.4">
<h3>
<span class="header-section-number">24.5.4</span> Procedure for Sample Size Selection<a class="anchor" aria-label="anchor" href="#procedure-for-sample-size-selection"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>Specify the minimum range of Factor <span class="math inline">\(A\)</span> means.</li>
<li>Obtain sample size from power tables using <span class="math inline">\(r = a\)</span>.
<ul>
<li>The resulting sample size is <span class="math inline">\(bn\)</span>, from which <span class="math inline">\(n\)</span> can be derived.</li>
</ul>
</li>
<li>Repeat steps 1-2 for Factor <span class="math inline">\(B\)</span>.</li>
<li>Choose the larger sample size from the calculations for Factors <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</li>
</ol>
<hr>
</div>
<div id="randomized-block-experiments" class="section level3" number="24.5.5">
<h3>
<span class="header-section-number">24.5.5</span> Randomized Block Experiments<a class="anchor" aria-label="anchor" href="#randomized-block-experiments"><i class="fas fa-link"></i></a>
</h3>
<p>Analogous to completely randomized designs . The power of the F-test for treatment effects for randomized block design uses the same non-centrality parameter as completely randomized design:</p>
<p><span class="math display">\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n}{r} \sum (\mu_i - \mu_.)^2}
\]</span></p>
<p>However, the power level is different from the randomized block design because</p>
<ul>
<li>error variance <span class="math inline">\(\sigma^2\)</span> is different</li>
<li>df(MSE) is different.</li>
</ul>
<hr>
</div>
</div>
<div id="single-factor-covariance-model" class="section level2" number="24.6">
<h2>
<span class="header-section-number">24.6</span> Single Factor Covariance Model<a class="anchor" aria-label="anchor" href="#single-factor-covariance-model"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>single-factor covariance model</strong> (Analysis of Covariance, ANCOVA) accounts for both treatment effects and a continuous covariate:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{.} + \tau_i + \gamma(X_{ij} - \bar{X}_{..}) + \epsilon_{ij}
\]</span></p>
<p>for <span class="math inline">\(i = 1, \dots, r\)</span> (treatments) and <span class="math inline">\(j = 1, \dots, n_i\)</span> (observations per treatment).</p>
<ul>
<li>
<span class="math inline">\(\mu_{.}\)</span>: Overall mean response.</li>
<li>
<span class="math inline">\(\tau_i\)</span>: Fixed treatment effects (<span class="math inline">\(\sum \tau_i = 0\)</span>).</li>
<li>
<span class="math inline">\(\gamma\)</span>: Fixed regression coefficient (relationship between covariate <span class="math inline">\(X\)</span> and response <span class="math inline">\(Y\)</span>).</li>
<li>
<span class="math inline">\(X_{ij}\)</span>: Observed covariate (fixed, not random).</li>
<li>
<span class="math inline">\(\epsilon_{ij} \sim iid N(0, \sigma^2)\)</span>: Independent random errors.</li>
</ul>
<p>If we use <span class="math inline">\(\gamma X_{ij}\)</span> directly (without centering), then <span class="math inline">\(\mu_{.}\)</span> is no longer the overall mean. <strong>Thus, centering the covariate</strong> is necessary to maintain interpretability.</p>
<p><strong>Expectation and Variance</strong></p>
<p><span class="math display">\[
\begin{aligned}
E(Y_{ij}) &amp;= \mu_. + \tau_i + \gamma(X_{ij}-\bar{X}_{..}) \\
var(Y_{ij}) &amp;= \sigma^2
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(Y_{ij} \sim N(\mu_{ij},\sigma^2)\)</span>, we express:</p>
<p><span class="math display">\[
\mu_{ij} = \mu_. + \tau_i + \gamma(X_{ij} - \bar{X}_{..})
\]</span></p>
<p>where <span class="math inline">\(\sum \tau_i = 0\)</span>. The mean response <span class="math inline">\(\mu_{ij}\)</span> is a regression line with intercept <span class="math inline">\(\mu_. + \tau_i\)</span> and slope <span class="math inline">\(\gamma\)</span> for each treatment <span class="math inline">\(i\)</span>.</p>
<hr>
<p><strong>Key Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>All treatments share the same slope (<span class="math inline">\(\gamma\)</span>).</li>
<li>No interaction between treatment and covariate (parallel regression lines).</li>
<li>If slopes differ, ANCOVA is not appropriate → use separate regressions per treatment.</li>
</ol>
<p>A more general model allows multiple covariates:</p>
<p><span class="math display">\[
Y_{ij} = \mu_. + \tau_i + \gamma_1(X_{ij1}-\bar{X}_{..1}) + \gamma_2(X_{ij2}-\bar{X}_{..2}) + \epsilon_{ij}
\]</span></p>
<hr>
<p>Using indicator variables for treatments:</p>
<p>For treatment <span class="math inline">\(i = 1\)</span>: <span class="math display">\[
l_1 =
\begin{cases}
1 &amp; \text{if case belongs to treatment 1} \\
-1 &amp; \text{if case belongs to treatment $r$} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>For treatment <span class="math inline">\(i = r-1\)</span>: <span class="math display">\[
l_{r-1} =
\begin{cases}
1 &amp; \text{if case belongs to treatment $r-1$} \\
-1 &amp; \text{if case belongs to treatment $r$} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Defining <span class="math inline">\(x_{ij} = X_{ij}- \bar{X}_{..}\)</span>, the regression model is:</p>
<p><span class="math display">\[
Y_{ij} = \mu_. + \tau_1 l_{ij,1} + \dots + \tau_{r-1} l_{ij,r-1} + \gamma x_{ij} + \epsilon_{ij}
\]</span></p>
<p>where <span class="math inline">\(I_{ij,1}\)</span> is the indicator variable <span class="math inline">\(l_1\)</span> for the <span class="math inline">\(j\)</span>-th case in treatment <span class="math inline">\(i\)</span>.</p>
<p>The treatment effects (<span class="math inline">\(\tau_i\)</span>) are simply regression coefficients for the indicator variables.</p>
<hr>
<div id="statistical-inference-for-treatment-effects" class="section level3" number="24.6.1">
<h3>
<span class="header-section-number">24.6.1</span> Statistical Inference for Treatment Effects<a class="anchor" aria-label="anchor" href="#statistical-inference-for-treatment-effects"><i class="fas fa-link"></i></a>
</h3>
<p>To test treatment effects:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \tau_1 = \tau_2 = \dots = 0 \\
&amp;H_a: \text{Not all } \tau_i = 0
\end{aligned}
\]</span></p>
<ol style="list-style-type: decimal">
<li><p><strong>Full Model (with treatment effects):</strong> <span class="math display">\[
Y_{ij} = \mu_. + \tau_i + \gamma X_{ij} + \epsilon_{ij}
\]</span></p></li>
<li><p><strong>Reduced Model (without treatment effects):</strong> <span class="math display">\[
Y_{ij} = \mu_. + \gamma X_{ij} + \epsilon_{ij}
\]</span></p></li>
</ol>
<p><strong>F-Test for Treatment Effects</strong></p>
<p>The test statistic is:</p>
<p><span class="math display">\[
F = \frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} \Big/ \frac{SSE(F)}{N-(r+1)}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(SSE(R)\)</span>: Sum of squared errors for the <strong>reduced model</strong>.</p></li>
<li><p><span class="math inline">\(SSE(F)\)</span>: Sum of squared errors for the <strong>full model</strong>.</p></li>
<li><p><span class="math inline">\(N\)</span>: Total number of observations.</p></li>
<li><p><span class="math inline">\(r\)</span>: Number of treatment groups.</p></li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>, the statistic follows an <span class="math inline">\(F\)</span>-distribution:</p>
<p><span class="math display">\[
F \sim F_{(r-1, N-(r+1))}
\]</span></p>
<hr>
<p><strong>Comparisons of Treatment Effects</strong></p>
<p>For <span class="math inline">\(r = 3\)</span>, we estimate:</p>
<div class="inline-table"><table style="width:99%;" class="table table-sm">
<colgroup>
<col width="15%">
<col width="25%">
<col width="58%">
</colgroup>
<thead><tr class="header">
<th><strong>Comparison</strong></th>
<th><strong>Estimate</strong></th>
<th><strong>Variance of Estimator</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\tau_1 - \tau_2\)</span></td>
<td><span class="math inline">\(\hat{\tau}_1 - \hat{\tau}_2\)</span></td>
<td><span class="math inline">\(var(\hat{\tau}_1) + var(\hat{\tau}_2) - 2cov(\hat{\tau}_1, \hat{\tau}_2)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\tau_1 - \tau_3\)</span></td>
<td><span class="math inline">\(2 \hat{\tau}_1 + \hat{\tau}_2\)</span></td>
<td><span class="math inline">\(4var(\hat{\tau}_1) + var(\hat{\tau}_2) - 4cov(\hat{\tau}_1, \hat{\tau}_2)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\tau_2 - \tau_3\)</span></td>
<td><span class="math inline">\(\hat{\tau}_1 + 2 \hat{\tau}_2\)</span></td>
<td><span class="math inline">\(var(\hat{\tau}_1) + 4var(\hat{\tau}_2) - 4cov(\hat{\tau}_1, \hat{\tau}_2)\)</span></td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="testing-for-parallel-slopes" class="section level3" number="24.6.2">
<h3>
<span class="header-section-number">24.6.2</span> Testing for Parallel Slopes<a class="anchor" aria-label="anchor" href="#testing-for-parallel-slopes"><i class="fas fa-link"></i></a>
</h3>
<p>To check if <strong>slopes differ across treatments</strong>, we use the model:</p>
<p><span class="math display">\[
Y_{ij} = \mu_{.} + \tau_1 I_{ij,1} + \tau_2 I_{ij,2} + \gamma X_{ij} + \beta_1 I_{ij,1}X_{ij} + \beta_2 I_{ij,2}X_{ij} + \epsilon_{ij}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\beta_1, \beta_2\)</span>: Interaction coefficients (slope differences across treatments).</li>
</ul>
<p><strong>Hypothesis Test</strong></p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \beta_1 = \beta_2 = 0 \quad (\text{Slopes are equal}) \\
&amp;H_a: \text{At least one } \beta \neq 0 \quad (\text{Slopes differ})
\end{aligned}
\]</span></p>
<p>If the <span class="math inline">\(F\)</span>-test fails to reject <span class="math inline">\(H_0\)</span>, then <strong>we assume parallel slopes</strong>.</p>
<hr>
</div>
<div id="adjusted-means" class="section level3" number="24.6.3">
<h3>
<span class="header-section-number">24.6.3</span> Adjusted Means<a class="anchor" aria-label="anchor" href="#adjusted-means"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>adjusted treatment means</strong> account for covariate effects:</p>
<p><span class="math display">\[
Y_{i.}(\text{adj}) = \bar{Y}_{i.} - \hat{\gamma}(\bar{X}_{i.} - \bar{X}_{..})
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\bar{Y}_{i.}\)</span>: Observed mean response for treatment <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{\gamma}\)</span>: Estimated regression coefficient.</p></li>
<li><p><span class="math inline">\(\bar{X}_{i.}\)</span>: Mean covariate value for treatment <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(\bar{X}_{..}\)</span>: Overall mean covariate value.</p></li>
</ul>
<p>This provides estimated treatment means after controlling for covariate effects.</p>
<hr>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="sampling.html"><span class="header-section-number">23</span> Sampling</a></div>
<div class="next"><a href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-analysis-of-variance-anova"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li>
<a class="nav-link" href="#sec-completely-randomized-design"><span class="header-section-number">24.1</span> Completely Randomized Design</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-single-factor-fixed-effects-model"><span class="header-section-number">24.1.1</span> Single-Factor Fixed Effects ANOVA</a></li>
<li><a class="nav-link" href="#sec-single-factor-random-effects-model"><span class="header-section-number">24.1.2</span> Single Factor Random Effects ANOVA</a></li>
<li><a class="nav-link" href="#sec-two-factor-fixed-effects-anova"><span class="header-section-number">24.1.3</span> Two-Factor Fixed Effects ANOVA</a></li>
<li><a class="nav-link" href="#sec-two-way-random-effects-anova"><span class="header-section-number">24.1.4</span> Two-Way Random Effects ANOVA</a></li>
<li><a class="nav-link" href="#sec-two-way-mixed-effects-anova"><span class="header-section-number">24.1.5</span> Two-Way Mixed Effects ANOVA</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-nonparametric-anova"><span class="header-section-number">24.2</span> Nonparametric ANOVA</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#kruskal-wallis-test-one-way-nonparametric-anova"><span class="header-section-number">24.2.1</span> Kruskal-Wallis Test (One-Way Nonparametric ANOVA)</a></li>
<li><a class="nav-link" href="#friedman-test-nonparametric-two-way-anova"><span class="header-section-number">24.2.2</span> Friedman Test (Nonparametric Two-Way ANOVA)</a></li>
</ul>
</li>
<li><a class="nav-link" href="#sec-randomized-block-designs"><span class="header-section-number">24.3</span> Randomized Block Designs</a></li>
<li>
<a class="nav-link" href="#nested-designs"><span class="header-section-number">24.4</span> Nested Designs</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#two-factor-nested-design"><span class="header-section-number">24.4.1</span> Two-Factor Nested Design</a></li>
<li><a class="nav-link" href="#unbalanced-nested-two-factor-designs"><span class="header-section-number">24.4.2</span> Unbalanced Nested Two-Factor Designs</a></li>
<li><a class="nav-link" href="#random-factor-effects"><span class="header-section-number">24.4.3</span> Random Factor Effects</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sample-size-planning-for-anova"><span class="header-section-number">24.5</span> Sample Size Planning for ANOVA</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#balanced-designs"><span class="header-section-number">24.5.1</span> Balanced Designs</a></li>
<li><a class="nav-link" href="#single-factor-studies"><span class="header-section-number">24.5.2</span> Single Factor Studies</a></li>
<li><a class="nav-link" href="#multi-factor-studies"><span class="header-section-number">24.5.3</span> Multi-Factor Studies</a></li>
<li><a class="nav-link" href="#procedure-for-sample-size-selection"><span class="header-section-number">24.5.4</span> Procedure for Sample Size Selection</a></li>
<li><a class="nav-link" href="#randomized-block-experiments"><span class="header-section-number">24.5.5</span> Randomized Block Experiments</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#single-factor-covariance-model"><span class="header-section-number">24.6</span> Single Factor Covariance Model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#statistical-inference-for-treatment-effects"><span class="header-section-number">24.6.1</span> Statistical Inference for Treatment Effects</a></li>
<li><a class="nav-link" href="#testing-for-parallel-slopes"><span class="header-section-number">24.6.2</span> Testing for Parallel Slopes</a></li>
<li><a class="nav-link" href="#adjusted-means"><span class="header-section-number">24.6.3</span> Adjusted Means</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/24-ANOVA.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/24-ANOVA.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-03-03.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
