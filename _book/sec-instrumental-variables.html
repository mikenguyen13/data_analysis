<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 34 Instrumental Variables | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="In many empirical settings, we seek to estimate the causal effect of an explanatory variable \(X\) on an outcome variable \(Y\). A common starting point is the Ordinary Least Squares regression:...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 34 Instrumental Variables | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/sec-instrumental-variables.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="In many empirical settings, we seek to estimate the causal effect of an explanatory variable \(X\) on an outcome variable \(Y\). A common starting point is the Ordinary Least Squares regression:...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 34 Instrumental Variables | A Guide on Data Analysis">
<meta name="twitter:description" content="In many empirical settings, we seek to estimate the causal effect of an explanatory variable \(X\) on an outcome variable \(Y\). A common starting point is the Ordinary Least Squares regression:...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="active" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="sec-endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="sec-directed-acyclic-graphs.html"><span class="header-section-number">38</span> Directed Acyclic Graphs</a></li>
<li><a class="" href="sec-controls.html"><span class="header-section-number">39</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="sec-instrumental-variables" class="section level1" number="34">
<h1>
<span class="header-section-number">34</span> Instrumental Variables<a class="anchor" aria-label="anchor" href="#sec-instrumental-variables"><i class="fas fa-link"></i></a>
</h1>
<p>In many empirical settings, we seek to estimate the causal effect of an explanatory variable <span class="math inline">\(X\)</span> on an outcome variable <span class="math inline">\(Y\)</span>. A common starting point is the <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a> regression:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X + \varepsilon.
\]</span></p>
<p>For OLS to provide an unbiased and consistent estimate of <span class="math inline">\(\beta_1\)</span>, the explanatory variable <span class="math inline">\(X\)</span> must satisfy the <em>exogeneity condition</em>:</p>
<p><span class="math display">\[ \mathbb{E}[\varepsilon \mid X] = 0. \]</span></p>
<p>However, when <span class="math inline">\(X\)</span> is correlated with the error term <span class="math inline">\(\varepsilon\)</span>, this assumption is violated, leading to <em>endogeneity</em>. As a result, the OLS estimator is biased and inconsistent. Common causes of endogeneity include:</p>
<ul>
<li>Omitted Variable Bias (OVB): When a relevant variable is omitted from the regression, leading to correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(\varepsilon\)</span>.</li>
<li>Simultaneity: When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are jointly determined, such as in supply-and-demand models.</li>
<li>Measurement Error: Errors in measuring <span class="math inline">\(X\)</span> introduce bias in estimation.
<ul>
<li>Attenuation Bias in Errors-in-Variables: measurement error in the independent variable leads to an underestimate of the true effect (biasing the coefficient toward zero).</li>
</ul>
</li>
</ul>
<p>Instrumental Variables (IV) estimation addresses endogeneity by introducing an instrument <span class="math inline">\(Z\)</span> that affects <span class="math inline">\(Y\)</span> <em>only</em> through <span class="math inline">\(X\)</span>. Similar to <a href="sec-experimental-design.html#sec-experimental-design">RCT</a>, we try to introduce randomization (random assignment to treatment) to our treatment variable by using only variation in the instrument.</p>
<p>Logic of using an instrument:</p>
<ul>
<li><p>Use only exogenous variation to see the variation in treatment (try to exclude all endogenous variation in the treatment)</p></li>
<li><p>Use only exogenous variation to see the variation in outcome (try to exclude all endogenous variation in the outcome)</p></li>
<li><p>See the relationship between treatment and outcome in terms of residual variations that are exogenous to omitted variables.</p></li>
</ul>
<p>For an instrument <span class="math inline">\(Z\)</span> to be valid, it must satisfy two conditions:</p>
<ol style="list-style-type: decimal">
<li><p>Relevance Condition: The instrument <span class="math inline">\(Z\)</span> must be correlated with the endogenous variable <span class="math inline">\(X\)</span>: <span class="math display">\[ \text{Cov}(Z, X) \neq 0. \]</span></p></li>
<li><p>Exogeneity Condition (Exclusion Restriction): The instrument <span class="math inline">\(Z\)</span> must be uncorrelated with the error term <span class="math inline">\(\varepsilon\)</span> and affect <span class="math inline">\(Y\)</span> <em>only</em> through <span class="math inline">\(X\)</span>: <span class="math display">\[ \text{Cov}(Z, \varepsilon) = 0. \]</span></p></li>
</ol>
<p>These conditions ensure that <span class="math inline">\(Z\)</span> provides exogenous variation in <span class="math inline">\(X\)</span>, allowing us to isolate the causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>These conditions ensure that <span class="math inline">\(Z\)</span> provides exogenous variation in <span class="math inline">\(X\)</span>, allowing us to estimate the causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. <a href="sec-experimental-design.html#sec-the-gold-standard-randomized-controlled-trials">Random assignment</a> of <span class="math inline">\(Z\)</span> helps ensure exogeneity, but we must also confirm that <span class="math inline">\(Z\)</span> influences <span class="math inline">\(Y\)</span> <em>only</em> through <span class="math inline">\(X\)</span> to satisfy the exclusion restriction.</p>
<p>The IV approach dates back to early econometric research in the 1920s and 1930s, with a significant role in Cowles Commission studies on simultaneous equations. Key contributions include:</p>
<ul>
<li>
<span class="citation">Wright (<a href="references.html#ref-wright1928tariff">1928</a>)</span>: One of the earliest applications, studying supply and demand for pig iron.</li>
<li>
<span class="citation">J. Angrist and Imbens (<a href="references.html#ref-angrist1991sources">1991</a>)</span>: Popularized IV methods using quarter-of-birth as an instrument for education.</li>
</ul>
<p>The <em>credibility revolution</em> in econometrics (1990s–2000s) led to widespread use of IVs in applied research, particularly in economics, political science, and epidemiology.</p>
<div id="challenges-with-instrumental-variables" class="section level2" number="34.1">
<h2>
<span class="header-section-number">34.1</span> Challenges with Instrumental Variables<a class="anchor" aria-label="anchor" href="#challenges-with-instrumental-variables"><i class="fas fa-link"></i></a>
</h2>
<p>While IVs can provide a solution to endogeneity, several challenges arise:</p>
<ul>
<li>Exclusion Restriction Violations: If <span class="math inline">\(Z\)</span> affects <span class="math inline">\(Y\)</span> through any channel other than <span class="math inline">\(X\)</span>, the IV estimate is biased.</li>
<li>Repeated Use of Instruments: Common instruments, such as weather or policy changes, may be invalid due to their widespread application across studies <span class="citation">(<a href="references.html#ref-gallen2020broken">Gallen 2020</a>)</span>. One needs to test for invalid instruments (Hausman-like test).
<ul>
<li>A notable example is <span class="citation">Mellon (<a href="references.html#ref-mellon2021rain">2021</a>)</span>, who documents that <em>289 social sciences studies</em> have used weather as an instrument for 195 variables, raising concerns about exclusion violations.</li>
</ul>
</li>
<li>Heterogeneous Treatment Effects: The Local Average Treatment Effect (LATE) estimated by IV applies only to <em>compliers</em>—units whose treatment status is affected by the instrument.</li>
<li>Weak Instruments: Too little correlation with the endogenous regressor yields unstable estimates.</li>
<li>Invalid Instruments: If the instrument violates exogeneity, your results are inconsistent.</li>
<li>Interpretation Mistakes: The IV identifies <em>only</em> the effect for those “marginal” units whose treatment status is driven by the instrument.</li>
</ul>
<hr>
</div>
<div id="framework-for-instrumental-variables" class="section level2" number="34.2">
<h2>
<span class="header-section-number">34.2</span> Framework for Instrumental Variables<a class="anchor" aria-label="anchor" href="#framework-for-instrumental-variables"><i class="fas fa-link"></i></a>
</h2>
<p>We consider a binary treatment framework where:</p>
<ul>
<li><p><span class="math inline">\(D_i \sim Bernoulli(p)\)</span> is a dummy treatment variable.</p></li>
<li><p><span class="math inline">\((Y_{0i}, Y_{1i})\)</span> are the potential outcomes under control and treatment.</p></li>
<li><p>The observed outcome is: <span class="math display">\[ Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i. \]</span></p></li>
<li>
<p>We introduce an instrumental variable <span class="math inline">\(Z_i\)</span> satisfying: <span class="math display">\[ Z_i \perp (Y_{0i}, Y_{1i}, D_{0i}, D_{1i}). \]</span></p>
<ul>
<li>This means <span class="math inline">\(Z_i\)</span> is independent of potential outcomes and potential treatment status.</li>
<li>
<span class="math inline">\(Z_i\)</span> must also be correlated with <span class="math inline">\(D_i\)</span> to satisfy the relevance condition.</li>
</ul>
</li>
</ul>
<div id="constant-treatment-effect-model" class="section level3" number="34.2.1">
<h3>
<span class="header-section-number">34.2.1</span> Constant-Treatment-Effect Model<a class="anchor" aria-label="anchor" href="#constant-treatment-effect-model"><i class="fas fa-link"></i></a>
</h3>
<p>Under the constant treatment effect assumption (i.e., the treatment effect is the same for all individuals),</p>
<p><span class="math display">\[
\begin{aligned}
Y_{0i} &amp;= \alpha + \eta_i, \\
Y_{1i} - Y_{0i} &amp;= \rho, \\
Y_i &amp;= Y_{0i} + D_i (Y_{1i} - Y_{0i}) \\
    &amp;= \alpha + \eta_i  + D_i \rho \\
    &amp;= \alpha + \rho D_i + \eta_i.
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\eta_i\)</span> captures individual-level heterogeneity.</li>
<li>
<span class="math inline">\(\rho\)</span> is the constant treatment effect.</li>
</ul>
<p>The problem with OLS estimation is that <span class="math inline">\(D_i\)</span> may be correlated with <span class="math inline">\(\eta_i\)</span>, leading to endogeneity bias.</p>
</div>
<div id="instrumental-variable-solution" class="section level3" number="34.2.2">
<h3>
<span class="header-section-number">34.2.2</span> Instrumental Variable Solution<a class="anchor" aria-label="anchor" href="#instrumental-variable-solution"><i class="fas fa-link"></i></a>
</h3>
<p>A valid instrument <span class="math inline">\(Z_i\)</span> allows us to estimate the causal effect <span class="math inline">\(\rho\)</span> via:</p>
<p><span class="math display">\[
\begin{aligned}
\rho &amp;= \frac{\text{Cov}(Y_i, Z_i)}{\text{Cov}(D_i, Z_i)} \\
     &amp;= \frac{\text{Cov}(Y_i, Z_i) / V(Z_i) }{\text{Cov}(D_i, Z_i) / V(Z_i)} \\
     &amp;= \frac{\text{Reduced form estimate}}{\text{First-stage estimate}} \\
     &amp;= \frac{E[Y_i |Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i | Z_i = 0 ]}.
\end{aligned}
\]</span></p>
<p>This ratio measures the treatment effect <em>only if</em> <span class="math inline">\(Z_i\)</span> is a valid instrument.</p>
</div>
<div id="heterogeneous-treatment-effects-and-the-late-framework" class="section level3" number="34.2.3">
<h3>
<span class="header-section-number">34.2.3</span> Heterogeneous Treatment Effects and the LATE Framework<a class="anchor" aria-label="anchor" href="#heterogeneous-treatment-effects-and-the-late-framework"><i class="fas fa-link"></i></a>
</h3>
<p>In a more general framework where treatment effects vary across individuals,</p>
<ul>
<li><p>Define potential outcomes as: <span class="math display">\[ Y_i(d,z) = \text{outcome for unit } i \text{ given } D_i = d, Z_i = z. \]</span></p></li>
<li>
<p>Define treatment status based on <span class="math inline">\(Z_i\)</span>: <span class="math display">\[ D_i = D_{0i} + Z_i (D_{1i} - D_{0i}). \]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(D_{1i}\)</span> is the treatment status when <span class="math inline">\(Z_i = 1\)</span>.</li>
<li>
<span class="math inline">\(D_{0i}\)</span> is the treatment status when <span class="math inline">\(Z_i = 0\)</span>.</li>
<li>
<span class="math inline">\(D_{1i} - D_{0i}\)</span> is the causal effect of <span class="math inline">\(Z_i\)</span> on <span class="math inline">\(D_i\)</span>.</li>
</ul>
</li>
</ul>
</div>
<div id="assumptions-for-late-identification" class="section level3" number="34.2.4">
<h3>
<span class="header-section-number">34.2.4</span> Assumptions for LATE Identification<a class="anchor" aria-label="anchor" href="#assumptions-for-late-identification"><i class="fas fa-link"></i></a>
</h3>
<div id="independence-instrument-randomization" class="section level4" number="34.2.4.1">
<h4>
<span class="header-section-number">34.2.4.1</span> Independence (Instrument Randomization)<a class="anchor" aria-label="anchor" href="#independence-instrument-randomization"><i class="fas fa-link"></i></a>
</h4>
<p>The instrument must be <em>as good as randomly assigned</em>:</p>
<p><span class="math display">\[ [\{Y_i(d,z); \forall d, z \}, D_{1i}, D_{0i} ] \perp Z_i. \]</span></p>
<p>This ensures that <span class="math inline">\(Z_i\)</span> is uncorrelated with potential outcomes and potential treatment status.</p>
<p>This assumption let the first-stage equation be the average causal effect of <span class="math inline">\(Z_i\)</span> on <span class="math inline">\(D_i\)</span></p>
<p><span class="math display">\[ \begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &amp;= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\ &amp;= E[D_{1i} - D_{0i}] \end{aligned} \]</span></p>
<p>This assumption also is sufficient for a causal interpretation of the reduced form, where we see the effect of the instrument <span class="math inline">\(Z_i\)</span> on the outcome <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[ E[Y_i |Z_i = 1 ] - E[Y_i|Z_i = 0] = E[Y_i (D_{1i}, Z_i = 1) - Y_i (D_{0i} , Z_i = 0)] \]</span></p>
</div>
<div id="exclusion-restriction" class="section level4" number="34.2.4.2">
<h4>
<span class="header-section-number">34.2.4.2</span> Exclusion Restriction<a class="anchor" aria-label="anchor" href="#exclusion-restriction"><i class="fas fa-link"></i></a>
</h4>
<p>This is also known as the existence of the instrument assumption <span class="citation">(<a href="references.html#ref-imbens1994identification">G. W. Imbens and Angrist 1994</a>)</span>. The instrument should only affect <span class="math inline">\(Y_i\)</span> through <span class="math inline">\(D_i\)</span> (i.e., the treatment <span class="math inline">\(D_i\)</span> fully mediates the effect of <span class="math inline">\(Z_i\)</span> on <span class="math inline">\(Y_i\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
Y_{1i} &amp;= Y_i (1,1) = Y_i (1,0)\\
Y_{0i} &amp;= Y_i (0,1) = Y_i (0,0)
\end{aligned}
\]</span></p>
<p>Under this assumption (and assume <span class="math inline">\(Y_{1i, Y_{0i}}\)</span> already satisfy the independence assumption), the observed outcome <span class="math inline">\(Y_i\)</span> can be rewritten as:</p>
<p><span class="math display">\[
\begin{aligned}
  Y_i &amp;= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\
      &amp;= Y_{0i} + (Y_{1i} - Y_{0i}) D_i.
  \end{aligned}
\]</span></p>
<p>This assumption let us go from reduced-form causal effects to treatment effects <span class="citation">(<a href="references.html#ref-angrist1995two">J. D. Angrist and Imbens 1995</a>)</span>.</p>
</div>
<div id="monotonicity-no-defiers" class="section level4" number="34.2.4.3">
<h4>
<span class="header-section-number">34.2.4.3</span> Monotonicity (No Defiers)<a class="anchor" aria-label="anchor" href="#monotonicity-no-defiers"><i class="fas fa-link"></i></a>
</h4>
<p>We assume that <span class="math inline">\(Z_i\)</span> affects <span class="math inline">\(D_i\)</span> in a <em>monotonic</em> way:</p>
<p><span class="math display">\[ D_{1i} \geq D_{0i}, \quad \forall i. \]</span></p>
<ul>
<li>This assumption lets us assume that there is a first stage, in which we examine the proportion of the population that <span class="math inline">\(D_i\)</span> is driven by <span class="math inline">\(Z_i\)</span>. It implies that <span class="math inline">\(Z_i\)</span> only moves individuals <em>toward</em> treatment, but never away. This rules out “defiers” (i.e., individuals who would have taken the treatment when not assigned but refuse when assigned).</li>
<li>This assumption is used to solve to problem of the shifts between participation status back to non-participation status.
<ul>
<li><p>Alternatively, one can solve the same problem by assuming constant (homogeneous) treatment effect <span class="citation">(<a href="references.html#ref-imbens1994identification">G. W. Imbens and Angrist 1994</a>)</span>, but this is rather restrictive.</p></li>
<li><p>A third solution is the assumption that there exists a value of the instrument, where the probability of participation conditional on that value is 0 <span class="citation">J. Angrist and Imbens (<a href="references.html#ref-angrist1991sources">1991</a>)</span>.</p></li>
</ul>
</li>
</ul>
<p>Under monotonicity,</p>
<p><span class="math display">\[
\begin{aligned}
  E[D_{1i} - D_{0i} ] = P[D_{1i} &gt; D_{0i}].
  \end{aligned}
\]</span></p>
</div>
</div>
<div id="local-average-treatment-effect-theorem" class="section level3" number="34.2.5">
<h3>
<span class="header-section-number">34.2.5</span> Local Average Treatment Effect Theorem<a class="anchor" aria-label="anchor" href="#local-average-treatment-effect-theorem"><i class="fas fa-link"></i></a>
</h3>
<p>Given Independence, Exclusion, and Monotonicity, we obtain the LATE result <span class="citation">(<a href="references.html#ref-angrist2009mostly">J. D. Angrist and Pischke 2009, 4.4.1</a>)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{E[Y_i | Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i |Z_i = 0]} = E[Y_{1i} - Y_{0i} | D_{1i} &gt; D_{0i}].
\end{aligned}
\]</span></p>
<p>This states that the IV estimator recovers the causal effect <em>only for compliers</em>—units whose treatment status changes due to <span class="math inline">\(Z_i\)</span>.</p>
<p>IV only identifies treatment effects for switchers (compliers):</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="15%">
<col width="17%">
<col width="67%">
</colgroup>
<thead><tr class="header">
<th>Switcher Type</th>
<th>Compliance Type</th>
<th>Definition</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Switchers</td>
<td>Compliers</td>
<td>
<span class="math inline">\(D_{1i} &gt; D_{0i}\)</span> (take treatment if <span class="math inline">\(Z_i = 1\)</span>, not if <span class="math inline">\(Z_i = 0\)</span>)</td>
</tr>
<tr class="even">
<td>Non-switchers</td>
<td>Always-Takers</td>
<td>
<span class="math inline">\(D_{1i} = D_{0i} = 1\)</span> (always take treatment)</td>
</tr>
<tr class="odd">
<td>Non-switchers</td>
<td>Never-Takers</td>
<td>
<span class="math inline">\(D_{1i} = D_{0i} = 0\)</span> (never take treatment)</td>
</tr>
</tbody>
</table></div>
<ul>
<li>IV estimates nothing for always-takers and never-takers since their treatment status is unaffected by <span class="math inline">\(Z_i\)</span> (Similar to the fixed-effects models).</li>
</ul>
</div>
<div id="iv-in-randomized-trials-noncompliance" class="section level3" number="34.2.6">
<h3>
<span class="header-section-number">34.2.6</span> IV in Randomized Trials (Noncompliance)<a class="anchor" aria-label="anchor" href="#iv-in-randomized-trials-noncompliance"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>In randomized trials, if compliance is imperfect (i.e., compliance is voluntary), where individuals in the treatment group will not always take the treatment (e.g., selection bias), intention-to-treat (ITT) estimates are valid but contaminated by noncompliance.</li>
<li>IV estimation using <a href="sec-experimental-design.html#sec-the-gold-standard-randomized-controlled-trials">random assignment</a> (<span class="math inline">\(Z_i\)</span>) as an instrument for actual treatment received (<span class="math inline">\(D_i\)</span>) recovers the LATE.</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\frac{E[Y_i |Z_i = 1] - E[Y_i |Z_i = 0]}{E[D_i |Z_i = 1]} = \frac{\text{Intent-to-Treat Effect}}{\text{Compliance Rate}} = E[Y_{1i} - Y_{0i} |D_i = 1].
\end{aligned}
\]</span></p>
<p>Under full compliance, LATE = Treatment Effect on the Treated (TOT).</p>
<hr>
</div>
</div>
<div id="sec-estimation" class="section level2" number="34.3">
<h2>
<span class="header-section-number">34.3</span> Estimation<a class="anchor" aria-label="anchor" href="#sec-estimation"><i class="fas fa-link"></i></a>
</h2>
<div id="sec-two-stage-least-squares-estimation" class="section level3" number="34.3.1">
<h3>
<span class="header-section-number">34.3.1</span> Two-Stage Least Squares Estimation<a class="anchor" aria-label="anchor" href="#sec-two-stage-least-squares-estimation"><i class="fas fa-link"></i></a>
</h3>
<p>Two-Stage Least Squares (2SLS) is the most widely used IV estimator It’s a special case of <a href="sec-instrumental-variables.html#iv-gmm">IV-GMM</a>. Consider the structural equation:</p>
<p><span class="math display">\[
Y_i = X_i \beta + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(X_i\)</span> is endogenous. We introduce an instrument <span class="math inline">\(Z_i\)</span> satisfying:</p>
<ol style="list-style-type: decimal">
<li>Relevance: <span class="math inline">\(Z_i\)</span> is correlated with <span class="math inline">\(X_i\)</span>.</li>
<li>Exogeneity: <span class="math inline">\(Z_i\)</span> is uncorrelated with <span class="math inline">\(\varepsilon_i\)</span>.</li>
</ol>
<p>2SLS Steps</p>
<ol style="list-style-type: decimal">
<li>
<p>First-Stage Regression: Predict <span class="math inline">\(X_i\)</span> using the instrument: <span class="math display">\[
X_i = \pi_0 + \pi_1 Z_i + v_i.
\]</span></p>
<ul>
<li>Obtain fitted values <span class="math inline">\(\hat{X}_i = \pi_0 + \pi_1 Z_i\)</span>.</li>
</ul>
</li>
<li>
<p>Second-Stage Regression: Use <span class="math inline">\(\hat{X}_i\)</span> in place of <span class="math inline">\(X_i\)</span>: <span class="math display">\[
Y_i = \beta_0 + \beta_1 \hat{X}_i + \varepsilon_i.
\]</span></p>
<ul>
<li>The estimated <span class="math inline">\(\hat{\beta}_1\)</span> is our IV estimator.</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb896"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lrberge.github.io/fixest/">fixest</a></span><span class="op">)</span></span>
<span><span class="va">base</span> <span class="op">=</span> <span class="va">iris</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">base</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"y"</span>, <span class="st">"x1"</span>, <span class="st">"x_endo_1"</span>, <span class="st">"x_inst_1"</span>, <span class="st">"fe"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">base</span><span class="op">$</span><span class="va">x_inst_2</span> <span class="op">=</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">y</span> <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">x_endo_1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">150</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="va">base</span><span class="op">$</span><span class="va">x_endo_2</span> <span class="op">=</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">y</span> <span class="op">-</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">x_inst_1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">150</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># IV Estimation</span></span>
<span><span class="va">est_iv</span> <span class="op">=</span> <span class="fu"><a href="https://lrberge.github.io/fixest/reference/feols.html">feols</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">|</span> <span class="va">x_endo_1</span> <span class="op">+</span> <span class="va">x_endo_2</span> <span class="op">~</span> <span class="va">x_inst_1</span> <span class="op">+</span> <span class="va">x_inst_2</span>, <span class="va">base</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">est_iv</span><span class="op">)</span></span>
<span><span class="co">#&gt; TSLS estimation - Dep. Var.: y</span></span>
<span><span class="co">#&gt;                   Endo.    : x_endo_1, x_endo_2</span></span>
<span><span class="co">#&gt;                   Instr.   : x_inst_1, x_inst_2</span></span>
<span><span class="co">#&gt; Second stage: Dep. Var.: y</span></span>
<span><span class="co">#&gt; Observations: 150</span></span>
<span><span class="co">#&gt; Standard-errors: IID </span></span>
<span><span class="co">#&gt;              Estimate Std. Error  t value   Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***</span></span>
<span><span class="co">#&gt; fit_x_endo_1 0.444982   0.022086 20.14744  &lt; 2.2e-16 ***</span></span>
<span><span class="co">#&gt; fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  </span></span>
<span><span class="co">#&gt; x1           0.565095   0.084715  6.67051 4.9180e-10 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; RMSE: 0.398842   Adj. R2: 0.761653</span></span>
<span><span class="co">#&gt; F-test (1st stage), x_endo_1: stat = 903.2    , p &lt; 2.2e-16 , on 2 and 146 DoF.</span></span>
<span><span class="co">#&gt; F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.</span></span>
<span><span class="co">#&gt;                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.</span></span></code></pre></div>
<p>Diagnostic Tests</p>
<p>To assess instrument validity:</p>
<div class="sourceCode" id="cb897"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://lrberge.github.io/fixest/reference/fitstat.html">fitstat</a></span><span class="op">(</span><span class="va">est_iv</span>, type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"n"</span>, <span class="st">"f"</span>, <span class="st">"ivf"</span>, <span class="st">"ivf1"</span>, <span class="st">"ivf2"</span>, <span class="st">"ivwald"</span>, <span class="st">"cd"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 Observations: 150</span></span>
<span><span class="co">#&gt;                       F-test: stat = 132.0    , p &lt; 2.2e-16 , on 3 and 146 DoF.</span></span>
<span><span class="co">#&gt; F-test (1st stage), x_endo_1: stat = 903.2    , p &lt; 2.2e-16 , on 2 and 146 DoF.</span></span>
<span><span class="co">#&gt; F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.</span></span>
<span><span class="co">#&gt;           F-test (2nd stage): stat = 194.2    , p &lt; 2.2e-16 , on 2 and 146 DoF.</span></span>
<span><span class="co">#&gt; Wald (1st stage), x_endo_1  : stat = 903.2    , p &lt; 2.2e-16 , on 2 and 146 DoF, VCOV: IID.</span></span>
<span><span class="co">#&gt; Wald (1st stage), x_endo_2  : stat =   3.25828, p = 0.041268, on 2 and 146 DoF, VCOV: IID.</span></span>
<span><span class="co">#&gt;                 Cragg-Donald: 3.11162</span></span></code></pre></div>
<p>To set default printing</p>
<div class="sourceCode" id="cb898"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># always add second-stage Wald test</span></span>
<span><span class="fu"><a href="https://lrberge.github.io/fixest/reference/print.fixest.html">setFixest_print</a></span><span class="op">(</span>fitstat <span class="op">=</span> <span class="op">~</span> <span class="va">.</span> <span class="op">+</span> <span class="va">ivwald2</span><span class="op">)</span></span>
<span><span class="va">est_iv</span></span></code></pre></div>
<p>To see results from different stages</p>
<div class="sourceCode" id="cb899"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># first-stage</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">est_iv</span>, stage <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># second-stage</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">est_iv</span>, stage <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># both stages</span></span>
<span><span class="fu"><a href="https://lrberge.github.io/fixest/reference/etable.html">etable</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">est_iv</span>, stage <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">)</span>, fitstat <span class="op">=</span> <span class="op">~</span> <span class="va">.</span> <span class="op">+</span> <span class="va">ivfall</span> <span class="op">+</span> <span class="va">ivwaldall.p</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://lrberge.github.io/fixest/reference/etable.html">etable</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">est_iv</span>, stage <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">1</span><span class="op">)</span>, fitstat <span class="op">=</span> <span class="op">~</span> <span class="va">.</span> <span class="op">+</span> <span class="va">ivfall</span> <span class="op">+</span> <span class="va">ivwaldall.p</span><span class="op">)</span></span>
<span><span class="co"># .p means p-value, not statistic</span></span>
<span><span class="co"># `all` means IV only</span></span></code></pre></div>
</div>
<div id="iv-gmm" class="section level3" number="34.3.2">
<h3>
<span class="header-section-number">34.3.2</span> IV-GMM<a class="anchor" aria-label="anchor" href="#iv-gmm"><i class="fas fa-link"></i></a>
</h3>
<p>The Generalized Method of Moments (GMM) provides a flexible estimation framework that generalizes the Instrumental Variables (IV) approach, including <a href="sec-instrumental-variables.html#sec-two-stage-least-squares-estimation">2SLS</a> as a special case. The key idea behind GMM is to use moment conditions derived from economic models to estimate parameters efficiently, even in the presence of endogeneity.</p>
<p>Consider the standard linear regression model:</p>
<p><span class="math display">\[
Y = X\beta + u, \quad u \sim (0, \Omega)
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Y\)</span> is an <span class="math inline">\(N \times 1\)</span> vector of the dependent variable.</li>
<li>
<span class="math inline">\(X\)</span> is an <span class="math inline">\(N \times k\)</span> matrix of endogenous regressors.</li>
<li>
<span class="math inline">\(\beta\)</span> is a <span class="math inline">\(k \times 1\)</span> vector of coefficients.</li>
<li>
<span class="math inline">\(u\)</span> is an <span class="math inline">\(N \times 1\)</span> vector of error terms.</li>
<li>
<span class="math inline">\(\Omega\)</span> is the variance-covariance matrix of <span class="math inline">\(u\)</span>.</li>
</ul>
<p>To address endogeneity in <span class="math inline">\(X\)</span>, we introduce an <span class="math inline">\(N \times l\)</span> matrix of instruments, <span class="math inline">\(Z\)</span>, where <span class="math inline">\(l \geq k\)</span>. The moment conditions are then given by:</p>
<p><span class="math display">\[
E[Z_i' u_i] = E[Z_i' (Y_i - X_i \beta)] = 0.
\]</span></p>
<p>In practice, these expectations are replaced by their sample analogs. The empirical moment conditions are given by:</p>
<p><span class="math display">\[
\bar{g}(\beta) = \frac{1}{N} \sum_{i=1}^{N} Z_i' (Y_i - X_i \beta) = \frac{1}{N} Z' (Y - X\beta).
\]</span></p>
<p>GMM estimates <span class="math inline">\(\beta\)</span> by minimizing a quadratic function of these sample moments.</p>
<hr>
<div id="iv-and-gmm-estimators" class="section level4" number="34.3.2.1">
<h4>
<span class="header-section-number">34.3.2.1</span> IV and GMM Estimators<a class="anchor" aria-label="anchor" href="#iv-and-gmm-estimators"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>Exactly Identified Case (<span class="math inline">\(l = k\)</span>)</li>
</ol>
<p>When the number of instruments equals the number of endogenous regressors (<span class="math inline">\(l = k\)</span>), the moment conditions uniquely determine <span class="math inline">\(\beta\)</span>. In this case, the IV estimator is:</p>
<p><span class="math display">\[
\hat{\beta}_{IV} = (Z'X)^{-1}Z'Y.
\]</span></p>
<p>This is equivalent to the classical 2SLS estimator.</p>
<ol start="2" style="list-style-type: decimal">
<li>Overidentified Case (<span class="math inline">\(l &gt; k\)</span>)</li>
</ol>
<p>When there are more instruments than endogenous variables (<span class="math inline">\(l &gt; k\)</span>), the system has more moment conditions than parameters. In this case, we project <span class="math inline">\(X\)</span> onto the instrument space:</p>
<p><span class="math display">\[
\hat{X} = Z(Z'Z)^{-1} Z' X = P_Z X.
\]</span></p>
<p>The 2SLS estimator is then given by:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta}_{2SLS} &amp;= (\hat{X}'X)^{-1} \hat{X}' Y \\
&amp;= (X'P_Z X)^{-1} X' P_Z Y.
\end{aligned}
\]</span></p>
<p>However, 2SLS does not optimally weight the instruments when <span class="math inline">\(l &gt; k\)</span>. The IV-GMM approach resolves this issue.</p>
<hr>
</div>
<div id="iv-gmm-estimation" class="section level4" number="34.3.2.2">
<h4>
<span class="header-section-number">34.3.2.2</span> IV-GMM Estimation<a class="anchor" aria-label="anchor" href="#iv-gmm-estimation"><i class="fas fa-link"></i></a>
</h4>
<p>The GMM estimator is obtained by minimizing the objective function:</p>
<p><span class="math display">\[
J (\hat{\beta}_{GMM} ) = N \bar{g}(\hat{\beta}_{GMM})' W \bar{g} (\hat{\beta}_{GMM}),
\]</span></p>
<p>where <span class="math inline">\(W\)</span> is an <span class="math inline">\(l \times l\)</span> symmetric weighting matrix.</p>
<p>For the IV-GMM estimator, solving the first-order conditions yields:</p>
<p><span class="math display">\[
\hat{\beta}_{GMM} = (X'ZWZ' X)^{-1} X'ZWZ'Y.
\]</span></p>
<p>For any weighting matrix <span class="math inline">\(W\)</span>, this is a consistent estimator. The optimal choice of <span class="math inline">\(W\)</span> is <span class="math inline">\(S^{-1}\)</span>, where <span class="math inline">\(S\)</span> is the covariance matrix of the moment conditions:</p>
<p><span class="math display">\[
S = E[Z' u u' Z] = \lim_{N \to \infty} N^{-1} [Z' \Omega Z].
\]</span></p>
<p>A feasible estimator replaces <span class="math inline">\(S\)</span> with its sample estimate from the 2SLS residuals:</p>
<p><span class="math display">\[
\hat{\beta}_{FEGMM} = (X'Z \hat{S}^{-1} Z' X)^{-1} X'Z \hat{S}^{-1} Z'Y.
\]</span></p>
<p>When <span class="math inline">\(\Omega\)</span> satisfies standard assumptions:</p>
<ol style="list-style-type: decimal">
<li>Errors are independently and identically distributed.</li>
<li>
<span class="math inline">\(S = \sigma_u^2 I_N\)</span>.</li>
<li>The optimal weighting matrix is proportional to the identity matrix.</li>
</ol>
<p>Then, the IV-GMM estimator simplifies to the standard IV (or 2SLS) estimator.</p>
<hr>
<p>Comparison of 2SLS and IV-GMM</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="24%">
<col width="38%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>2SLS</th>
<th>IV-GMM</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Instrument usage</td>
<td>Uses a subset of available instruments</td>
<td>Uses all available instruments</td>
</tr>
<tr class="even">
<td>Weighting</td>
<td>No weighting applied</td>
<td>Weights instruments for efficiency</td>
</tr>
<tr class="odd">
<td>Efficiency</td>
<td>Suboptimal in overidentified cases</td>
<td>Efficient when <span class="math inline">\(W = S^{-1}\)</span>
</td>
</tr>
<tr class="even">
<td>Overidentification test</td>
<td>Not available</td>
<td>Uses Hansen’s <span class="math inline">\(J\)</span>-test (overid test)</td>
</tr>
</tbody>
</table></div>
<p>Key Takeaways:</p>
<ul>
<li>Use IV-GMM whenever overidentification is a concern (i.e., <span class="math inline">\(l &gt; k\)</span>).</li>
<li>2SLS is a special case of IV-GMM when the weighting matrix is proportional to the identity matrix.</li>
<li>IV-GMM improves efficiency by optimally weighting the moment conditions.</li>
</ul>
<div class="sourceCode" id="cb900"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Standard approach</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gmm</span><span class="op">)</span></span>
<span><span class="va">gmm_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/gmm/man/gmm.html">gmm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span>, <span class="op">~</span> <span class="va">x_inst_1</span> <span class="op">+</span> <span class="va">x_inst_2</span>, data <span class="op">=</span> <span class="va">base</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gmm_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; gmm(g = y ~ x1, x = ~x_inst_1 + x_inst_2, data = base)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method:  twoStep </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel:  Quadratic Spectral(with bw =  0.72368 )</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate     Std. Error   t value      Pr(&gt;|t|)   </span></span>
<span><span class="co">#&gt; (Intercept)   1.4385e+01   1.8960e+00   7.5871e+00   3.2715e-14</span></span>
<span><span class="co">#&gt; x1           -2.7506e+00   6.2101e-01  -4.4292e+00   9.4584e-06</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; J-Test: degrees of freedom is 1 </span></span>
<span><span class="co">#&gt;                 J-test     P-value  </span></span>
<span><span class="co">#&gt; Test E(g)=0:    7.9455329  0.0048206</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Initial values of the coefficients</span></span>
<span><span class="co">#&gt; (Intercept)          x1 </span></span>
<span><span class="co">#&gt;   16.117875   -3.360622</span></span></code></pre></div>
<hr>
</div>
<div id="overidentification-test-hansens-j-statistic" class="section level4" number="34.3.2.3">
<h4>
<span class="header-section-number">34.3.2.3</span> Overidentification Test: Hansen’s <span class="math inline">\(J\)</span>-Statistic<a class="anchor" aria-label="anchor" href="#overidentification-test-hansens-j-statistic"><i class="fas fa-link"></i></a>
</h4>
<p>A key advantage of IV-GMM is that it allows testing of instrument validity through the Hansen <span class="math inline">\(J\)</span>-test (also known as the GMM distance test or Hayashi’s C-statistic). The test statistic is:</p>
<p><span class="math display">\[
J = N \bar{g}(\hat{\beta}_{GMM})' \hat{S}^{-1} \bar{g} (\hat{\beta}_{GMM}),
\]</span></p>
<p>which follows a <span class="math inline">\(\chi^2\)</span> distribution with degrees of freedom equal to the number of overidentifying restrictions (<span class="math inline">\(l - k\)</span>). A significant <span class="math inline">\(J\)</span>-statistic suggests that the instruments may not be valid.</p>
<hr>
</div>
<div id="cluster-robust-standard-errors" class="section level4" number="34.3.2.4">
<h4>
<span class="header-section-number">34.3.2.4</span> Cluster-Robust Standard Errors<a class="anchor" aria-label="anchor" href="#cluster-robust-standard-errors"><i class="fas fa-link"></i></a>
</h4>
<p>In empirical applications, errors often exhibit heteroskedasticity or intra-group correlation (clustering), violating the assumption of independently and identically distributed errors. Standard IV-GMM estimators remain consistent but may not be efficient if clustering is ignored.</p>
<p>To address this, we adjust the GMM weighting matrix by incorporating cluster-robust variance estimation. Specifically, the covariance matrix of the moment conditions <span class="math inline">\(S\)</span> is estimated as:</p>
<p><span class="math display">\[
\hat{S} = \frac{1}{N} \sum_{c=1}^{C} \left( \sum_{i \in c} Z_i' u_i \right) \left( \sum_{i \in c} Z_i' u_i \right)',
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(C\)</span> is the number of clusters,</p></li>
<li><p><span class="math inline">\(i \in c\)</span> represents observations belonging to cluster <span class="math inline">\(c\)</span>,</p></li>
<li><p><span class="math inline">\(u_i\)</span> is the residual for observation <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(Z_i\)</span> is the vector of instruments.</p></li>
</ul>
<p>Using this robust weighting matrix, we compute a clustered GMM estimator that remains consistent and improves inference when clustering is present.</p>
<hr>
<div class="sourceCode" id="cb901"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load required packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gmm</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span>  <span class="co"># For generalized inverse if needed</span></span>
<span></span>
<span><span class="co"># General IV-GMM function with clustering</span></span>
<span><span class="va">gmmcl</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">formula</span>, <span class="va">instruments</span>, <span class="va">data</span>, <span class="va">cluster_var</span>, <span class="va">lambda</span> <span class="op">=</span> <span class="fl">1e-6</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Ensure cluster_var exists in data</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="op">(</span><span class="va">cluster_var</span> <span class="op"><a href="https://rdrr.io/pkg/BiocGenerics/man/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/row_colnames.html">colnames</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/stop.html">stop</a></span><span class="op">(</span><span class="st">"Error: Cluster variable not found in data."</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># Step 1: Initial GMM estimation (identity weighting matrix)</span></span>
<span>  <span class="va">initial_gmm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/gmm/man/gmm.html">gmm</a></span><span class="op">(</span><span class="va">formula</span>, <span class="va">instruments</span>, data <span class="op">=</span> <span class="va">data</span>, vcov <span class="op">=</span> <span class="st">"TrueFixed"</span>, </span>
<span>                      weightsMatrix <span class="op">=</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/nrow.html">ncol</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">instruments</span>, <span class="va">data</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Extract residuals</span></span>
<span>  <span class="va">u_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">initial_gmm</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Matrix of instruments</span></span>
<span>  <span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">instruments</span>, <span class="va">data</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Ensure clusters are treated as a factor</span></span>
<span>  <span class="va">data</span><span class="op">[[</span><span class="va">cluster_var</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="va">cluster_var</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Compute clustered weighting matrix</span></span>
<span>  <span class="va">cluster_groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/split.html">split</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">u_hat</span><span class="op">)</span>, <span class="va">data</span><span class="op">[[</span><span class="va">cluster_var</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Remove empty clusters (if any)</span></span>
<span>  <span class="va">cluster_groups</span> <span class="op">&lt;-</span> <span class="va">cluster_groups</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/lengths.html">lengths</a></span><span class="op">(</span><span class="va">cluster_groups</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Initialize cluster-based covariance matrix</span></span>
<span>  <span class="va">S_cluster</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/nrow.html">ncol</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/nrow.html">ncol</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Zero matrix</span></span>
<span>  </span>
<span>  <span class="co"># Compute clustered weight matrix</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">indices</span> <span class="kw">in</span> <span class="va">cluster_groups</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">indices</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span> <span class="op">{</span>  <span class="co"># Ensure valid clusters</span></span>
<span>      <span class="va">u_cluster</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">u_hat</span><span class="op">[</span><span class="va">indices</span><span class="op">]</span>, ncol <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Convert to column matrix</span></span>
<span>      <span class="va">Z_cluster</span> <span class="op">&lt;-</span> <span class="va">Z</span><span class="op">[</span><span class="va">indices</span>, , drop <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span>        <span class="co"># Keep matrix form</span></span>
<span>      <span class="va">S_cluster</span> <span class="op">&lt;-</span> <span class="va">S_cluster</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="va">Z_cluster</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="va">u_cluster</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="va">u_cluster</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Z_cluster</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># Normalize by sample size</span></span>
<span>  <span class="va">S_cluster</span> <span class="op">&lt;-</span> <span class="va">S_cluster</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/nrow.html">nrow</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Ensure S_cluster is invertible</span></span>
<span>  <span class="va">S_cluster</span> <span class="op">&lt;-</span> <span class="va">S_cluster</span> <span class="op">+</span> <span class="va">lambda</span> <span class="op">*</span> <span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/nrow.html">ncol</a></span><span class="op">(</span><span class="va">S_cluster</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Regularization</span></span>
<span></span>
<span>  <span class="co"># Compute inverse or generalized inverse if needed</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/qr-methods.html">qr</a></span><span class="op">(</span><span class="va">S_cluster</span><span class="op">)</span><span class="op">$</span><span class="va">rank</span> <span class="op">&lt;</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/nrow.html">ncol</a></span><span class="op">(</span><span class="va">S_cluster</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">S_cluster_inv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/ginv.html">ginv</a></span><span class="op">(</span><span class="va">S_cluster</span><span class="op">)</span>  <span class="co"># Use generalized inverse (MASS package)</span></span>
<span>  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>    <span class="va">S_cluster_inv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">S_cluster</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># Step 2: GMM estimation using clustered weighting matrix</span></span>
<span>  <span class="va">final_gmm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/gmm/man/gmm.html">gmm</a></span><span class="op">(</span><span class="va">formula</span>, <span class="va">instruments</span>, data <span class="op">=</span> <span class="va">data</span>, vcov <span class="op">=</span> <span class="st">"TrueFixed"</span>, </span>
<span>                    weightsMatrix <span class="op">=</span> <span class="va">S_cluster_inv</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">final_gmm</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Example: Simulated Data for IV-GMM with Clustering</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span>   <span class="co"># Total observations</span></span>
<span><span class="va">C</span> <span class="op">&lt;-</span> <span class="fl">50</span>    <span class="co"># Number of clusters</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  cluster <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">C</span>, each <span class="op">=</span> <span class="va">n</span> <span class="op">/</span> <span class="va">C</span><span class="op">)</span>,  <span class="co"># Cluster variable</span></span>
<span>  z1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>,</span>
<span>  z2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>,</span>
<span>  x1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>,</span>
<span>  y1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">$</span><span class="va">z1</span> <span class="op">+</span> <span class="va">data</span><span class="op">$</span><span class="va">z2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>  <span class="co"># Endogenous regressor</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">y1</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">$</span><span class="va">x1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>            <span class="co"># Outcome variable</span></span>
<span></span>
<span><span class="co"># Run standard IV-GMM (without clustering)</span></span>
<span><span class="va">gmm_results_standard</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/gmm/man/gmm.html">gmm</a></span><span class="op">(</span><span class="va">y1</span> <span class="op">~</span> <span class="va">x1</span>, <span class="op">~</span> <span class="va">z1</span> <span class="op">+</span> <span class="va">z2</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Run IV-GMM with clustering</span></span>
<span><span class="va">gmm_results_clustered</span> <span class="op">&lt;-</span> <span class="fu">gmmcl</span><span class="op">(</span><span class="va">y1</span> <span class="op">~</span> <span class="va">x1</span>, <span class="op">~</span> <span class="va">z1</span> <span class="op">+</span> <span class="va">z2</span>, data <span class="op">=</span> <span class="va">data</span>, cluster_var <span class="op">=</span> <span class="st">"cluster"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Display results for comparison</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gmm_results_standard</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; gmm(g = y1 ~ x1, x = ~z1 + z2, data = data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method:  twoStep </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel:  Quadratic Spectral(with bw =  1.09893 )</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate     Std. Error   t value      Pr(&gt;|t|)   </span></span>
<span><span class="co">#&gt; (Intercept)   4.4919e-02   6.5870e-02   6.8193e-01   4.9528e-01</span></span>
<span><span class="co">#&gt; x1            9.8409e-01   4.4215e-02   2.2257e+01  9.6467e-110</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; J-Test: degrees of freedom is 1 </span></span>
<span><span class="co">#&gt;                 J-test  P-value</span></span>
<span><span class="co">#&gt; Test E(g)=0:    1.6171  0.2035 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Initial values of the coefficients</span></span>
<span><span class="co">#&gt; (Intercept)          x1 </span></span>
<span><span class="co">#&gt;  0.05138658  0.98580796</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">gmm_results_clustered</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; gmm(g = formula, x = instruments, vcov = "TrueFixed", weightsMatrix = S_cluster_inv, </span></span>
<span><span class="co">#&gt;     data = data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Method:  One step GMM with fixed W </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel:  Quadratic Spectral</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate    Std. Error  t value     Pr(&gt;|t|)  </span></span>
<span><span class="co">#&gt; (Intercept)  4.9082e-02  7.0878e-05  6.9249e+02  0.0000e+00</span></span>
<span><span class="co">#&gt; x1           9.8238e-01  5.2798e-05  1.8606e+04  0.0000e+00</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; J-Test: degrees of freedom is 1 </span></span>
<span><span class="co">#&gt;                 J-test   P-value</span></span>
<span><span class="co">#&gt; Test E(g)=0:    1247099        0</span></span></code></pre></div>
<hr>
</div>
</div>
<div id="limited-information-maximum-likelihood" class="section level3" number="34.3.3">
<h3>
<span class="header-section-number">34.3.3</span> Limited Information Maximum Likelihood<a class="anchor" aria-label="anchor" href="#limited-information-maximum-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>LIML is an alternative to 2SLS that performs better when instruments are weak.</p>
<p>It solves: <span class="math display">\[
\min_{\lambda} \left| \begin{bmatrix} Y - X\beta \\ \lambda (D - X\gamma) \end{bmatrix} \right|
\]</span> where <span class="math inline">\(\lambda\)</span> is an eigenvalue.</p>
</div>
<div id="jackknife-iv" class="section level3" number="34.3.4">
<h3>
<span class="header-section-number">34.3.4</span> Jackknife IV<a class="anchor" aria-label="anchor" href="#jackknife-iv"><i class="fas fa-link"></i></a>
</h3>
<p>JIVE reduces small-sample bias by leaving each observation out when estimating first-stage fitted values:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{X}_i^{(-i)} &amp;= Z_i (Z_{-i}'Z_{-i})^{-1} Z_{-i}'X_{-i}. \\
\hat{\beta}_{JIVE} &amp;= (X^{(-i)'}X^{(-i)})^{-1}X^{(-i)'} Y
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb902"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span></span>
<span><span class="va">jive_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x_endo_1</span> <span class="op">|</span> <span class="va">x_inst_1</span>, data <span class="op">=</span> <span class="va">base</span>, method <span class="op">=</span> <span class="st">"jive"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">jive_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivreg(formula = y ~ x_endo_1 | x_inst_1, data = base, method = "jive")</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.2390 -0.3022 -0.0206  0.2772  1.0039 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  4.34586    0.08096   53.68   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x_endo_1     0.39848    0.01964   20.29   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.4075 on 148 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-Squared: 0.7595,  Adjusted R-squared: 0.7578 </span></span>
<span><span class="co">#&gt; Wald test: 411.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<hr>
</div>
<div id="sec-control-function-approach" class="section level3" number="34.3.5">
<h3>
<span class="header-section-number">34.3.5</span> Control Function Approach<a class="anchor" aria-label="anchor" href="#sec-control-function-approach"><i class="fas fa-link"></i></a>
</h3>
<p>The Control Function (CF) approach, also known as two-stage residual inclusion (2SRI), is a method used to address endogeneity in regression models. This approach is particularly suited for models with nonadditive errors, such as discrete choice models or cases where both the endogenous variable and the outcome are binary.</p>
<p>The control function approach is particularly useful in:</p>
<ul>
<li>Binary outcome and binary endogenous variable models:
<ul>
<li>In rare events, the second stage typically uses a logistic model <span class="citation">(<a href="references.html#ref-tchetgen2014note">E. Tchetgen Tchetgen 2014</a>)</span>.</li>
<li>In non-rare events, a risk ratio regression is often more appropriate.</li>
</ul>
</li>
<li>Marketing applications:
<ul>
<li>Used in consumer choice models to account for endogeneity in demand estimation <span class="citation">(<a href="references.html#ref-petrin2010control">Petrin and Train 2010</a>)</span>.</li>
</ul>
</li>
</ul>
<p>The general model setup is:</p>
<p><span class="math display">\[
Y = g(X) + U  
\]</span></p>
<p><span class="math display">\[
X = \pi(Z) + V  
\]</span></p>
<p>with the key assumptions:</p>
<ol style="list-style-type: decimal">
<li><p>Conditional mean independence:<br><span class="math display">\[E(U |Z,V) = E(U|V)\]</span><br>
This implies that once we control for <span class="math inline">\(V\)</span>, the instrumental variable <span class="math inline">\(Z\)</span> does not directly affect <span class="math inline">\(U\)</span>.</p></li>
<li><p>Instrument relevance:<br><span class="math display">\[E(V|Z) = 0\]</span><br>
This ensures that <span class="math inline">\(Z\)</span> is a valid instrument for <span class="math inline">\(X\)</span>.</p></li>
</ol>
<p>Under the control function approach, the expectation of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\((Z,V)\)</span> can be rewritten as:</p>
<p><span class="math display">\[
E(Y|Z,V) = g(X) + E(U|Z,V) = g(X) + E(U|V) = g(X) + h(V).
\]</span></p>
<p>Here, <span class="math inline">\(h(V)\)</span> is the control function that captures endogeneity through the first-stage residuals.</p>
<div id="implementation" class="section level4" number="34.3.5.1">
<h4>
<span class="header-section-number">34.3.5.1</span> Implementation<a class="anchor" aria-label="anchor" href="#implementation"><i class="fas fa-link"></i></a>
</h4>
<p>Rather than replacing the endogenous variable <span class="math inline">\(X_i\)</span> with its predicted value <span class="math inline">\(\hat{X}_i\)</span>, the CF approach explicitly incorporates the residuals from the first-stage regression:</p>
<p>Stage 1: Estimate First-Stage Residuals</p>
<p>Estimate the endogenous variable using its instrumental variables:</p>
<p><span class="math display">\[
X_i = Z_i \pi + v_i.
\]</span></p>
<p>Obtain the residuals:</p>
<p><span class="math display">\[
\hat{v}_i = X_i - Z_i \hat{\pi}.
\]</span></p>
<p>Stage 2: Include Residuals in Outcome Equation</p>
<p>Regress the outcome variable on <span class="math inline">\(X_i\)</span> and the first-stage residuals:</p>
<p><span class="math display">\[
Y_i = X_i \beta + \gamma \hat{v}_i + \varepsilon_i.
\]</span></p>
<p>If endogeneity is present, <span class="math inline">\(\gamma \neq 0\)</span>; otherwise, the endogenous regressor <span class="math inline">\(X\)</span> would be exogenous.</p>
</div>
<div id="comparison-to-two-stage-least-squares" class="section level4" number="34.3.5.2">
<h4>
<span class="header-section-number">34.3.5.2</span> Comparison to Two-Stage Least Squares<a class="anchor" aria-label="anchor" href="#comparison-to-two-stage-least-squares"><i class="fas fa-link"></i></a>
</h4>
<p>The control function method differs from <a href="sec-instrumental-variables.html#sec-two-stage-least-squares-estimation">2SLS</a> depending on whether the model is linear or nonlinear:</p>
<ol style="list-style-type: decimal">
<li>Linear Endogenous Variables:
<ul>
<li>When both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous, the CF approach is equivalent to 2SLS.</li>
</ul>
</li>
<li>Nonlinear Endogenous Variables:
<ul>
<li>If <span class="math inline">\(X\)</span> is nonlinear (e.g., a binary treatment), CF differs from 2SLS and often performs better.</li>
</ul>
</li>
<li>Nonlinear in Parameters:
<ul>
<li>In models where <span class="math inline">\(g(X)\)</span> is nonlinear (e.g., logit/probit models), CF is typically superior to 2SLS because it explicitly models endogeneity via the control function <span class="math inline">\(h(V)\)</span>.</li>
</ul>
</li>
</ol>
<div class="sourceCode" id="cb903"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lrberge.github.io/fixest/">fixest</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://modelsummary.com">modelsummary</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set the seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">=</span> <span class="fl">1000</span></span>
<span><span class="co"># Generate the exogenous variable from a normal distribution</span></span>
<span><span class="va">exogenous</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">5</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate the omitted variable as a function of the exogenous variable</span></span>
<span><span class="va">omitted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">2</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate the endogenous variable as a function of the omitted variable and the exogenous variable</span></span>
<span><span class="va">endogenous</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">omitted</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">exogenous</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># nonlinear endogenous variable</span></span>
<span><span class="va">endogenous_nonlinear</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">omitted</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">exogenous</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">100</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">unrelated</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Exponential.html">rexp</a></span><span class="op">(</span><span class="va">n</span>, rate <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate the response variable as a function of the endogenous variable and the omitted variable</span></span>
<span><span class="va">response</span> <span class="op">&lt;-</span> <span class="fl">4</span> <span class="op">+</span>  <span class="fl">3</span> <span class="op">*</span> <span class="va">endogenous</span> <span class="op">+</span> <span class="fl">6</span> <span class="op">*</span> <span class="va">omitted</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">response_nonlinear</span> <span class="op">&lt;-</span> <span class="fl">4</span> <span class="op">+</span>  <span class="fl">3</span> <span class="op">*</span> <span class="va">endogenous_nonlinear</span> <span class="op">+</span> <span class="fl">6</span> <span class="op">*</span> <span class="va">omitted</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">response_nonlinear_para</span> <span class="op">&lt;-</span> <span class="fl">4</span> <span class="op">+</span>  <span class="fl">3</span> <span class="op">*</span> <span class="va">endogenous</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span> <span class="fl">6</span> <span class="op">*</span> <span class="va">omitted</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Combine the variables into a data frame</span></span>
<span><span class="va">my_data</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>        <span class="va">exogenous</span>,</span>
<span>        <span class="va">omitted</span>,</span>
<span>        <span class="va">endogenous</span>,</span>
<span>        <span class="va">response</span>,</span>
<span>        <span class="va">unrelated</span>,</span>
<span>        <span class="va">response</span>,</span>
<span>        <span class="va">response_nonlinear</span>,</span>
<span>        <span class="va">response_nonlinear_para</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="co"># View the first few rows of the data frame</span></span>
<span><span class="co"># head(my_data)</span></span>
<span></span>
<span><span class="va">wo_omitted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://lrberge.github.io/fixest/reference/feols.html">feols</a></span><span class="op">(</span><span class="va">response</span> <span class="op">~</span> <span class="va">endogenous</span> <span class="op">+</span> <span class="fu">sw0</span><span class="op">(</span><span class="va">unrelated</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">my_data</span><span class="op">)</span></span>
<span><span class="va">w_omitted</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://lrberge.github.io/fixest/reference/feols.html">feols</a></span><span class="op">(</span><span class="va">response</span> <span class="op">~</span> <span class="va">endogenous</span> <span class="op">+</span> <span class="va">omitted</span> <span class="op">+</span> <span class="va">unrelated</span>, data <span class="op">=</span> <span class="va">my_data</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># ivreg::ivreg(response ~ endogenous + unrelated | exogenous, data = my_data)</span></span>
<span><span class="va">iv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://lrberge.github.io/fixest/reference/feols.html">feols</a></span><span class="op">(</span><span class="va">response</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="fu">sw0</span><span class="op">(</span><span class="va">unrelated</span><span class="op">)</span> <span class="op">|</span> <span class="va">endogenous</span> <span class="op">~</span> <span class="va">exogenous</span>, data <span class="op">=</span> <span class="va">my_data</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://lrberge.github.io/fixest/reference/etable.html">etable</a></span><span class="op">(</span></span>
<span>    <span class="va">wo_omitted</span>,</span>
<span>    <span class="va">w_omitted</span>,</span>
<span>    <span class="va">iv</span>, </span>
<span>    digits <span class="op">=</span> <span class="fl">2</span></span>
<span>    <span class="co"># vcov = list("each", "iid", "hetero")</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;                   wo_omitted.1   wo_omitted.2     w_omitted          iv.1</span></span>
<span><span class="co">#&gt; Dependent Var.:       response       response      response      response</span></span>
<span><span class="co">#&gt;                                                                          </span></span>
<span><span class="co">#&gt; Constant        -3.8*** (0.30) -3.6*** (0.31) 3.9*** (0.16) 12.0*** (1.4)</span></span>
<span><span class="co">#&gt; endogenous       4.0*** (0.01)  4.0*** (0.01) 3.0*** (0.01) 3.2*** (0.07)</span></span>
<span><span class="co">#&gt; unrelated                       -0.14. (0.08)  -0.02 (0.03)              </span></span>
<span><span class="co">#&gt; omitted                                       6.0*** (0.08)              </span></span>
<span><span class="co">#&gt; _______________ ______________ ______________ _____________ _____________</span></span>
<span><span class="co">#&gt; S.E. type                  IID            IID           IID           IID</span></span>
<span><span class="co">#&gt; Observations             1,000          1,000         1,000         1,000</span></span>
<span><span class="co">#&gt; R2                     0.98756        0.98760       0.99817       0.94976</span></span>
<span><span class="co">#&gt; Adj. R2                0.98755        0.98757       0.99816       0.94971</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;                          iv.2</span></span>
<span><span class="co">#&gt; Dependent Var.:      response</span></span>
<span><span class="co">#&gt;                              </span></span>
<span><span class="co">#&gt; Constant        12.2*** (1.4)</span></span>
<span><span class="co">#&gt; endogenous      3.2*** (0.07)</span></span>
<span><span class="co">#&gt; unrelated       -0.28. (0.16)</span></span>
<span><span class="co">#&gt; omitted                      </span></span>
<span><span class="co">#&gt; _______________ _____________</span></span>
<span><span class="co">#&gt; S.E. type                 IID</span></span>
<span><span class="co">#&gt; Observations            1,000</span></span>
<span><span class="co">#&gt; R2                    0.95010</span></span>
<span><span class="co">#&gt; Adj. R2               0.95000</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>Linear in parameter and linear in endogenous variable</p>
<div class="sourceCode" id="cb904"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># manual</span></span>
<span><span class="co"># 2SLS</span></span>
<span><span class="va">first_stage</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">endogenous</span> <span class="op">~</span> <span class="va">exogenous</span>, data <span class="op">=</span> <span class="va">my_data</span><span class="op">)</span></span>
<span><span class="va">new_data</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">my_data</span>, new_endogenous <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">first_stage</span>, <span class="va">my_data</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">second_stage</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response</span> <span class="op">~</span> <span class="va">new_endogenous</span>, data <span class="op">=</span> <span class="va">new_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">second_stage</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = response ~ new_endogenous, data = new_data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -68.126 -14.949   0.608  15.099  73.842 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)     11.9910     5.7671   2.079   0.0379 *  </span></span>
<span><span class="co">#&gt; new_endogenous   3.2097     0.2832  11.335   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 21.49 on 998 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.1141, Adjusted R-squared:  0.1132 </span></span>
<span><span class="co">#&gt; F-statistic: 128.5 on 1 and 998 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="va">new_data_cf</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">my_data</span>, residual <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">second_stage_cf</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response</span> <span class="op">~</span> <span class="va">endogenous</span> <span class="op">+</span> <span class="va">residual</span>, data <span class="op">=</span> <span class="va">new_data_cf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">second_stage_cf</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = response ~ endogenous + residual, data = new_data_cf)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -5.1039 -1.0065  0.0247  0.9480  4.2521 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 11.99102    0.39849   30.09   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; endogenous   3.20974    0.01957  164.05   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; residual     0.95036    0.02159   44.02   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.485 on 997 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.9958, Adjusted R-squared:  0.9958 </span></span>
<span><span class="co">#&gt; F-statistic: 1.175e+05 on 2 and 997 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://modelsummary.com/man/modelsummary.html">modelsummary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">second_stage</span>, <span class="va">second_stage_cf</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<!-- preamble start -->

    <script>

      function styleCell_bcyeklf5s9it3st59pct(i, j, css_id) {
          var table = document.getElementById("tinytable_bcyeklf5s9it3st59pct");
          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors
          if (cell) {
              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);
              cell.classList.add(css_id);
          } else {
              console.warn(`Cell at (${i}, ${j}) not found.`);
          }
      }
      function insertSpanRow(i, colspan, content) {
        var table = document.getElementById('tinytable_bcyeklf5s9it3st59pct');
        var newRow = table.insertRow(i);
        var newCell = newRow.insertCell(0);
        newCell.setAttribute("colspan", colspan);
        // newCell.innerText = content;
        // this may be unsafe, but innerText does not interpret <br>
        newCell.innerHTML = content;
      }
      function spanCell_bcyeklf5s9it3st59pct(i, j, rowspan, colspan) {
        var table = document.getElementById("tinytable_bcyeklf5s9it3st59pct");
        const targetRow = table.rows[i];
        const targetCell = targetRow.cells[j];
        for (let r = 0; r < rowspan; r++) {
          // Only start deleting cells to the right for the first row (r == 0)
          if (r === 0) {
            // Delete cells to the right of the target cell in the first row
            for (let c = colspan - 1; c > 0; c--) {
              if (table.rows[i + r].cells[j + c]) {
                table.rows[i + r].deleteCell(j + c);
              }
            }
          }
          // For rows below the first, delete starting from the target column
          if (r > 0) {
            for (let c = colspan - 1; c >= 0; c--) {
              if (table.rows[i + r] && table.rows[i + r].cells[j]) {
                table.rows[i + r].deleteCell(j);
              }
            }
          }
        }
        // Set rowspan and colspan of the target cell
        targetCell.rowSpan = rowspan;
        targetCell.colSpan = colspan;
      }
      // tinytable span after
      window.addEventListener('load', function () {
          var cellsToStyle = [
            // tinytable style arrays after
          { positions: [ { i: 16, j: 1 }, { i: 16, j: 2 },  ], css_id: 'tinytable_css_loedou5fkndbhxj8rx6x',}, 
          { positions: [ { i: 8, j: 1 }, { i: 8, j: 2 },  ], css_id: 'tinytable_css_r26r60ajssspwfmf290a',}, 
          { positions: [ { i: 1, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 5, j: 1 }, { i: 6, j: 1 }, { i: 7, j: 1 }, { i: 4, j: 2 }, { i: 9, j: 1 }, { i: 10, j: 1 }, { i: 11, j: 1 }, { i: 12, j: 1 }, { i: 13, j: 1 }, { i: 14, j: 1 }, { i: 15, j: 1 }, { i: 12, j: 2 }, { i: 4, j: 1 }, { i: 1, j: 2 }, { i: 2, j: 2 }, { i: 3, j: 2 }, { i: 5, j: 2 }, { i: 6, j: 2 }, { i: 7, j: 2 }, { i: 9, j: 2 }, { i: 10, j: 2 }, { i: 11, j: 2 }, { i: 13, j: 2 }, { i: 14, j: 2 }, { i: 15, j: 2 },  ], css_id: 'tinytable_css_1au6c85u0y4nmlayzt6a',}, 
          { positions: [ { i: 0, j: 1 }, { i: 0, j: 2 },  ], css_id: 'tinytable_css_nsd9p1h6lz5yped7732d',}, 
          { positions: [ { i: 16, j: 0 },  ], css_id: 'tinytable_css_mq7w75252rzirjsjyffa',}, 
          { positions: [ { i: 8, j: 0 },  ], css_id: 'tinytable_css_01coubw8i75hkgsgfp9l',}, 
          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 }, { i: 6, j: 0 }, { i: 7, j: 0 }, { i: 12, j: 0 }, { i: 9, j: 0 }, { i: 10, j: 0 }, { i: 11, j: 0 }, { i: 13, j: 0 }, { i: 14, j: 0 }, { i: 15, j: 0 },  ], css_id: 'tinytable_css_0wp6hnlpsdgp0nxz78wo',}, 
          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_piypxjc10jchlu8i8lum',}, 
          ];

          // Loop over the arrays to style the cells
          cellsToStyle.forEach(function (group) {
              group.positions.forEach(function (cell) {
                  styleCell_bcyeklf5s9it3st59pct(cell.i, cell.j, group.css_id);
              });
          });
      });
    </script><style>
      /* tinytable css entries after */
      .table td.tinytable_css_loedou5fkndbhxj8rx6x, .table th.tinytable_css_loedou5fkndbhxj8rx6x { text-align: center; border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_r26r60ajssspwfmf290a, .table th.tinytable_css_r26r60ajssspwfmf290a { text-align: center; border-bottom: solid black 0.05em; }
      .table td.tinytable_css_1au6c85u0y4nmlayzt6a, .table th.tinytable_css_1au6c85u0y4nmlayzt6a { text-align: center; }
      .table td.tinytable_css_nsd9p1h6lz5yped7732d, .table th.tinytable_css_nsd9p1h6lz5yped7732d { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
      .table td.tinytable_css_mq7w75252rzirjsjyffa, .table th.tinytable_css_mq7w75252rzirjsjyffa { text-align: left; border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_01coubw8i75hkgsgfp9l, .table th.tinytable_css_01coubw8i75hkgsgfp9l { text-align: left; border-bottom: solid black 0.05em; }
      .table td.tinytable_css_0wp6hnlpsdgp0nxz78wo, .table th.tinytable_css_0wp6hnlpsdgp0nxz78wo { text-align: left; }
      .table td.tinytable_css_piypxjc10jchlu8i8lum, .table th.tinytable_css_piypxjc10jchlu8i8lum { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
    </style>
<div class="container">
      <div class="inline-table"><table class="table table-borderless" id="tinytable_bcyeklf5s9it3st59pct" style="width: auto; margin-left: auto; margin-right: auto;" data-quarto-disable-processing="true">
<thead><tr>
<th scope="col"> </th>
                <th scope="col">(1)</th>
                <th scope="col">(2)</th>
              </tr></thead>
<tbody>
<tr>
<td>(Intercept)</td>
                  <td>11.991</td>
                  <td>11.991</td>
                </tr>
<tr>
<td></td>
                  <td>(5.767)</td>
                  <td>(0.398)</td>
                </tr>
<tr>
<td>new_endogenous</td>
                  <td>3.210</td>
                  <td></td>
                </tr>
<tr>
<td></td>
                  <td>(0.283)</td>
                  <td></td>
                </tr>
<tr>
<td>endogenous</td>
                  <td></td>
                  <td>3.210</td>
                </tr>
<tr>
<td></td>
                  <td></td>
                  <td>(0.020)</td>
                </tr>
<tr>
<td>residual</td>
                  <td></td>
                  <td>0.950</td>
                </tr>
<tr>
<td></td>
                  <td></td>
                  <td>(0.022)</td>
                </tr>
<tr>
<td>Num.Obs.</td>
                  <td>1000</td>
                  <td>1000</td>
                </tr>
<tr>
<td>R2</td>
                  <td>0.114</td>
                  <td>0.996</td>
                </tr>
<tr>
<td>R2 Adj.</td>
                  <td>0.113</td>
                  <td>0.996</td>
                </tr>
<tr>
<td>AIC</td>
                  <td>8977.0</td>
                  <td>3633.5</td>
                </tr>
<tr>
<td>BIC</td>
                  <td>8991.8</td>
                  <td>3653.2</td>
                </tr>
<tr>
<td>Log.Lik.</td>
                  <td>-4485.516</td>
                  <td>-1812.768</td>
                </tr>
<tr>
<td>F</td>
                  <td>128.483</td>
                  <td>117473.460</td>
                </tr>
<tr>
<td>RMSE</td>
                  <td>21.47</td>
                  <td>1.48</td>
                </tr>
</tbody>
</table></div>
</div>
<!-- hack to avoid NA insertion in last line -->
<p>Nonlinear in endogenous variable</p>
<div class="sourceCode" id="cb905"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 2SLS</span></span>
<span><span class="va">first_stage</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">endogenous_nonlinear</span> <span class="op">~</span> <span class="va">exogenous</span>, data <span class="op">=</span> <span class="va">my_data</span><span class="op">)</span></span>
<span></span>
<span><span class="va">new_data</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">my_data</span>, new_endogenous_nonlinear <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">first_stage</span>, <span class="va">my_data</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">second_stage</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response_nonlinear</span> <span class="op">~</span> <span class="va">new_endogenous_nonlinear</span>, data <span class="op">=</span> <span class="va">new_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">second_stage</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = response_nonlinear ~ new_endogenous_nonlinear, data = new_data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -101.26  -53.01  -13.50   39.33  376.16 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)               11.7539    21.6478   0.543    0.587    </span></span>
<span><span class="co">#&gt; new_endogenous_nonlinear   3.1253     0.5993   5.215 2.23e-07 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 70.89 on 998 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.02653,    Adjusted R-squared:  0.02555 </span></span>
<span><span class="co">#&gt; F-statistic:  27.2 on 1 and 998 DF,  p-value: 2.234e-07</span></span>
<span></span>
<span><span class="va">new_data_cf</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">my_data</span>, residual <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">second_stage_cf</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response_nonlinear</span> <span class="op">~</span> <span class="va">endogenous_nonlinear</span> <span class="op">+</span> <span class="va">residual</span>, data <span class="op">=</span> <span class="va">new_data_cf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">second_stage_cf</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = response_nonlinear ~ endogenous_nonlinear + residual, </span></span>
<span><span class="co">#&gt;     data = new_data_cf)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -12.8559  -0.8337   0.4429   1.3432   4.3147 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)          11.75395    0.67012  17.540  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; endogenous_nonlinear  3.12525    0.01855 168.469  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; residual              0.13577    0.01882   7.213 1.08e-12 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 2.194 on 997 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.9991, Adjusted R-squared:  0.9991 </span></span>
<span><span class="co">#&gt; F-statistic: 5.344e+05 on 2 and 997 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://modelsummary.com/man/modelsummary.html">modelsummary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">second_stage</span>, <span class="va">second_stage_cf</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<!-- preamble start -->

    <script>

      function styleCell_gunj3tvsvj2u9zwprcks(i, j, css_id) {
          var table = document.getElementById("tinytable_gunj3tvsvj2u9zwprcks");
          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors
          if (cell) {
              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);
              cell.classList.add(css_id);
          } else {
              console.warn(`Cell at (${i}, ${j}) not found.`);
          }
      }
      function insertSpanRow(i, colspan, content) {
        var table = document.getElementById('tinytable_gunj3tvsvj2u9zwprcks');
        var newRow = table.insertRow(i);
        var newCell = newRow.insertCell(0);
        newCell.setAttribute("colspan", colspan);
        // newCell.innerText = content;
        // this may be unsafe, but innerText does not interpret <br>
        newCell.innerHTML = content;
      }
      function spanCell_gunj3tvsvj2u9zwprcks(i, j, rowspan, colspan) {
        var table = document.getElementById("tinytable_gunj3tvsvj2u9zwprcks");
        const targetRow = table.rows[i];
        const targetCell = targetRow.cells[j];
        for (let r = 0; r < rowspan; r++) {
          // Only start deleting cells to the right for the first row (r == 0)
          if (r === 0) {
            // Delete cells to the right of the target cell in the first row
            for (let c = colspan - 1; c > 0; c--) {
              if (table.rows[i + r].cells[j + c]) {
                table.rows[i + r].deleteCell(j + c);
              }
            }
          }
          // For rows below the first, delete starting from the target column
          if (r > 0) {
            for (let c = colspan - 1; c >= 0; c--) {
              if (table.rows[i + r] && table.rows[i + r].cells[j]) {
                table.rows[i + r].deleteCell(j);
              }
            }
          }
        }
        // Set rowspan and colspan of the target cell
        targetCell.rowSpan = rowspan;
        targetCell.colSpan = colspan;
      }
      // tinytable span after
      window.addEventListener('load', function () {
          var cellsToStyle = [
            // tinytable style arrays after
          { positions: [ { i: 16, j: 1 }, { i: 16, j: 2 },  ], css_id: 'tinytable_css_joqirgo39649wwe8he46',}, 
          { positions: [ { i: 8, j: 1 }, { i: 8, j: 2 },  ], css_id: 'tinytable_css_1pyfov7i1xroz6e7owob',}, 
          { positions: [ { i: 1, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 5, j: 1 }, { i: 6, j: 1 }, { i: 7, j: 1 }, { i: 4, j: 2 }, { i: 9, j: 1 }, { i: 10, j: 1 }, { i: 11, j: 1 }, { i: 12, j: 1 }, { i: 13, j: 1 }, { i: 14, j: 1 }, { i: 15, j: 1 }, { i: 12, j: 2 }, { i: 4, j: 1 }, { i: 1, j: 2 }, { i: 2, j: 2 }, { i: 3, j: 2 }, { i: 5, j: 2 }, { i: 6, j: 2 }, { i: 7, j: 2 }, { i: 9, j: 2 }, { i: 10, j: 2 }, { i: 11, j: 2 }, { i: 13, j: 2 }, { i: 14, j: 2 }, { i: 15, j: 2 },  ], css_id: 'tinytable_css_5ihhxya5a015v9xlg2cu',}, 
          { positions: [ { i: 0, j: 1 }, { i: 0, j: 2 },  ], css_id: 'tinytable_css_ln1kvxnxdqzz7ajcfv1q',}, 
          { positions: [ { i: 16, j: 0 },  ], css_id: 'tinytable_css_tpgrdq1fihx7tijhvcui',}, 
          { positions: [ { i: 8, j: 0 },  ], css_id: 'tinytable_css_61l5drkaax3f6bc98ola',}, 
          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 }, { i: 6, j: 0 }, { i: 7, j: 0 }, { i: 12, j: 0 }, { i: 9, j: 0 }, { i: 10, j: 0 }, { i: 11, j: 0 }, { i: 13, j: 0 }, { i: 14, j: 0 }, { i: 15, j: 0 },  ], css_id: 'tinytable_css_yy8ma1xswh7egpgnn4hn',}, 
          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_33ogij62iys81ucr3onb',}, 
          ];

          // Loop over the arrays to style the cells
          cellsToStyle.forEach(function (group) {
              group.positions.forEach(function (cell) {
                  styleCell_gunj3tvsvj2u9zwprcks(cell.i, cell.j, group.css_id);
              });
          });
      });
    </script><style>
      /* tinytable css entries after */
      .table td.tinytable_css_joqirgo39649wwe8he46, .table th.tinytable_css_joqirgo39649wwe8he46 { text-align: center; border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_1pyfov7i1xroz6e7owob, .table th.tinytable_css_1pyfov7i1xroz6e7owob { text-align: center; border-bottom: solid black 0.05em; }
      .table td.tinytable_css_5ihhxya5a015v9xlg2cu, .table th.tinytable_css_5ihhxya5a015v9xlg2cu { text-align: center; }
      .table td.tinytable_css_ln1kvxnxdqzz7ajcfv1q, .table th.tinytable_css_ln1kvxnxdqzz7ajcfv1q { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
      .table td.tinytable_css_tpgrdq1fihx7tijhvcui, .table th.tinytable_css_tpgrdq1fihx7tijhvcui { text-align: left; border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_61l5drkaax3f6bc98ola, .table th.tinytable_css_61l5drkaax3f6bc98ola { text-align: left; border-bottom: solid black 0.05em; }
      .table td.tinytable_css_yy8ma1xswh7egpgnn4hn, .table th.tinytable_css_yy8ma1xswh7egpgnn4hn { text-align: left; }
      .table td.tinytable_css_33ogij62iys81ucr3onb, .table th.tinytable_css_33ogij62iys81ucr3onb { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
    </style>
<div class="container">
      <div class="inline-table"><table class="table table-borderless" id="tinytable_gunj3tvsvj2u9zwprcks" style="width: auto; margin-left: auto; margin-right: auto;" data-quarto-disable-processing="true">
<thead><tr>
<th scope="col"> </th>
                <th scope="col">(1)</th>
                <th scope="col">(2)</th>
              </tr></thead>
<tbody>
<tr>
<td>(Intercept)</td>
                  <td>11.754</td>
                  <td>11.754</td>
                </tr>
<tr>
<td></td>
                  <td>(21.648)</td>
                  <td>(0.670)</td>
                </tr>
<tr>
<td>new_endogenous_nonlinear</td>
                  <td>3.125</td>
                  <td></td>
                </tr>
<tr>
<td></td>
                  <td>(0.599)</td>
                  <td></td>
                </tr>
<tr>
<td>endogenous_nonlinear</td>
                  <td></td>
                  <td>3.125</td>
                </tr>
<tr>
<td></td>
                  <td></td>
                  <td>(0.019)</td>
                </tr>
<tr>
<td>residual</td>
                  <td></td>
                  <td>0.136</td>
                </tr>
<tr>
<td></td>
                  <td></td>
                  <td>(0.019)</td>
                </tr>
<tr>
<td>Num.Obs.</td>
                  <td>1000</td>
                  <td>1000</td>
                </tr>
<tr>
<td>R2</td>
                  <td>0.027</td>
                  <td>0.999</td>
                </tr>
<tr>
<td>R2 Adj.</td>
                  <td>0.026</td>
                  <td>0.999</td>
                </tr>
<tr>
<td>AIC</td>
                  <td>11364.2</td>
                  <td>4414.7</td>
                </tr>
<tr>
<td>BIC</td>
                  <td>11378.9</td>
                  <td>4434.4</td>
                </tr>
<tr>
<td>Log.Lik.</td>
                  <td>-5679.079</td>
                  <td>-2203.371</td>
                </tr>
<tr>
<td>F</td>
                  <td>27.196</td>
                  <td>534439.006</td>
                </tr>
<tr>
<td>RMSE</td>
                  <td>70.82</td>
                  <td>2.19</td>
                </tr>
</tbody>
</table></div>
</div>
<!-- hack to avoid NA insertion in last line -->
<p>Nonlinear in parameters</p>
<div class="sourceCode" id="cb906"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 2SLS</span></span>
<span><span class="va">first_stage</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">endogenous</span> <span class="op">~</span> <span class="va">exogenous</span>, data <span class="op">=</span> <span class="va">my_data</span><span class="op">)</span></span>
<span></span>
<span><span class="va">new_data</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">my_data</span>, new_endogenous <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">first_stage</span>, <span class="va">my_data</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">second_stage</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response_nonlinear_para</span> <span class="op">~</span> <span class="va">new_endogenous</span>, data <span class="op">=</span> <span class="va">new_data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">second_stage</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = response_nonlinear_para ~ new_endogenous, data = new_data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -1402.34  -462.21   -64.22   382.35  3090.62 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)    -1137.875    173.811  -6.547  9.4e-11 ***</span></span>
<span><span class="co">#&gt; new_endogenous   122.525      8.534  14.357  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 647.7 on 998 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.1712, Adjusted R-squared:  0.1704 </span></span>
<span><span class="co">#&gt; F-statistic: 206.1 on 1 and 998 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="va">new_data_cf</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">my_data</span>, residual <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">second_stage_cf</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">response_nonlinear_para</span> <span class="op">~</span> <span class="va">endogenous_nonlinear</span> <span class="op">+</span> <span class="va">residual</span>, data <span class="op">=</span> <span class="va">new_data_cf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">second_stage_cf</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = response_nonlinear_para ~ endogenous_nonlinear + </span></span>
<span><span class="co">#&gt;     residual, data = new_data_cf)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -904.77 -154.35  -20.41  143.24  953.04 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)          492.2494    32.3530   15.21  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; endogenous_nonlinear  23.5991     0.8741   27.00  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; residual              30.5914     3.7397    8.18 8.58e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 245.9 on 997 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8806, Adjusted R-squared:  0.8804 </span></span>
<span><span class="co">#&gt; F-statistic:  3676 on 2 and 997 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://modelsummary.com/man/modelsummary.html">modelsummary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span><span class="va">second_stage</span>, <span class="va">second_stage_cf</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<!-- preamble start -->

    <script>

      function styleCell_foncyue7l6n3r3g1l51p(i, j, css_id) {
          var table = document.getElementById("tinytable_foncyue7l6n3r3g1l51p");
          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors
          if (cell) {
              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);
              cell.classList.add(css_id);
          } else {
              console.warn(`Cell at (${i}, ${j}) not found.`);
          }
      }
      function insertSpanRow(i, colspan, content) {
        var table = document.getElementById('tinytable_foncyue7l6n3r3g1l51p');
        var newRow = table.insertRow(i);
        var newCell = newRow.insertCell(0);
        newCell.setAttribute("colspan", colspan);
        // newCell.innerText = content;
        // this may be unsafe, but innerText does not interpret <br>
        newCell.innerHTML = content;
      }
      function spanCell_foncyue7l6n3r3g1l51p(i, j, rowspan, colspan) {
        var table = document.getElementById("tinytable_foncyue7l6n3r3g1l51p");
        const targetRow = table.rows[i];
        const targetCell = targetRow.cells[j];
        for (let r = 0; r < rowspan; r++) {
          // Only start deleting cells to the right for the first row (r == 0)
          if (r === 0) {
            // Delete cells to the right of the target cell in the first row
            for (let c = colspan - 1; c > 0; c--) {
              if (table.rows[i + r].cells[j + c]) {
                table.rows[i + r].deleteCell(j + c);
              }
            }
          }
          // For rows below the first, delete starting from the target column
          if (r > 0) {
            for (let c = colspan - 1; c >= 0; c--) {
              if (table.rows[i + r] && table.rows[i + r].cells[j]) {
                table.rows[i + r].deleteCell(j);
              }
            }
          }
        }
        // Set rowspan and colspan of the target cell
        targetCell.rowSpan = rowspan;
        targetCell.colSpan = colspan;
      }
      // tinytable span after
      window.addEventListener('load', function () {
          var cellsToStyle = [
            // tinytable style arrays after
          { positions: [ { i: 16, j: 1 }, { i: 16, j: 2 },  ], css_id: 'tinytable_css_tihu9oe35xyyz92hstch',}, 
          { positions: [ { i: 8, j: 1 }, { i: 8, j: 2 },  ], css_id: 'tinytable_css_3zrpj2g730jekm7mkx4a',}, 
          { positions: [ { i: 1, j: 1 }, { i: 2, j: 1 }, { i: 3, j: 1 }, { i: 5, j: 1 }, { i: 6, j: 1 }, { i: 7, j: 1 }, { i: 4, j: 2 }, { i: 9, j: 1 }, { i: 10, j: 1 }, { i: 11, j: 1 }, { i: 12, j: 1 }, { i: 13, j: 1 }, { i: 14, j: 1 }, { i: 15, j: 1 }, { i: 12, j: 2 }, { i: 4, j: 1 }, { i: 1, j: 2 }, { i: 2, j: 2 }, { i: 3, j: 2 }, { i: 5, j: 2 }, { i: 6, j: 2 }, { i: 7, j: 2 }, { i: 9, j: 2 }, { i: 10, j: 2 }, { i: 11, j: 2 }, { i: 13, j: 2 }, { i: 14, j: 2 }, { i: 15, j: 2 },  ], css_id: 'tinytable_css_o5jfz0tqf78ebq9js54k',}, 
          { positions: [ { i: 0, j: 1 }, { i: 0, j: 2 },  ], css_id: 'tinytable_css_gkykmnb67lvy8jl1lfoq',}, 
          { positions: [ { i: 16, j: 0 },  ], css_id: 'tinytable_css_i7zp0qw8pja7p06kcigp',}, 
          { positions: [ { i: 8, j: 0 },  ], css_id: 'tinytable_css_0pwmvz6w9h2w2su5ha81',}, 
          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 4, j: 0 }, { i: 5, j: 0 }, { i: 6, j: 0 }, { i: 7, j: 0 }, { i: 12, j: 0 }, { i: 9, j: 0 }, { i: 10, j: 0 }, { i: 11, j: 0 }, { i: 13, j: 0 }, { i: 14, j: 0 }, { i: 15, j: 0 },  ], css_id: 'tinytable_css_o6pc4f1jgrgp52wz2ggn',}, 
          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_n7u3moaxtrudjoxfh383',}, 
          ];

          // Loop over the arrays to style the cells
          cellsToStyle.forEach(function (group) {
              group.positions.forEach(function (cell) {
                  styleCell_foncyue7l6n3r3g1l51p(cell.i, cell.j, group.css_id);
              });
          });
      });
    </script><style>
      /* tinytable css entries after */
      .table td.tinytable_css_tihu9oe35xyyz92hstch, .table th.tinytable_css_tihu9oe35xyyz92hstch { text-align: center; border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_3zrpj2g730jekm7mkx4a, .table th.tinytable_css_3zrpj2g730jekm7mkx4a { text-align: center; border-bottom: solid black 0.05em; }
      .table td.tinytable_css_o5jfz0tqf78ebq9js54k, .table th.tinytable_css_o5jfz0tqf78ebq9js54k { text-align: center; }
      .table td.tinytable_css_gkykmnb67lvy8jl1lfoq, .table th.tinytable_css_gkykmnb67lvy8jl1lfoq { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
      .table td.tinytable_css_i7zp0qw8pja7p06kcigp, .table th.tinytable_css_i7zp0qw8pja7p06kcigp { text-align: left; border-bottom: solid #d3d8dc 0.1em; }
      .table td.tinytable_css_0pwmvz6w9h2w2su5ha81, .table th.tinytable_css_0pwmvz6w9h2w2su5ha81 { text-align: left; border-bottom: solid black 0.05em; }
      .table td.tinytable_css_o6pc4f1jgrgp52wz2ggn, .table th.tinytable_css_o6pc4f1jgrgp52wz2ggn { text-align: left; }
      .table td.tinytable_css_n7u3moaxtrudjoxfh383, .table th.tinytable_css_n7u3moaxtrudjoxfh383 { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }
    </style>
<div class="container">
      <div class="inline-table"><table class="table table-borderless" id="tinytable_foncyue7l6n3r3g1l51p" style="width: auto; margin-left: auto; margin-right: auto;" data-quarto-disable-processing="true">
<thead><tr>
<th scope="col"> </th>
                <th scope="col">(1)</th>
                <th scope="col">(2)</th>
              </tr></thead>
<tbody>
<tr>
<td>(Intercept)</td>
                  <td>-1137.875</td>
                  <td>492.249</td>
                </tr>
<tr>
<td></td>
                  <td>(173.811)</td>
                  <td>(32.353)</td>
                </tr>
<tr>
<td>new_endogenous</td>
                  <td>122.525</td>
                  <td></td>
                </tr>
<tr>
<td></td>
                  <td>(8.534)</td>
                  <td></td>
                </tr>
<tr>
<td>endogenous_nonlinear</td>
                  <td></td>
                  <td>23.599</td>
                </tr>
<tr>
<td></td>
                  <td></td>
                  <td>(0.874)</td>
                </tr>
<tr>
<td>residual</td>
                  <td></td>
                  <td>30.591</td>
                </tr>
<tr>
<td></td>
                  <td></td>
                  <td>(3.740)</td>
                </tr>
<tr>
<td>Num.Obs.</td>
                  <td>1000</td>
                  <td>1000</td>
                </tr>
<tr>
<td>R2</td>
                  <td>0.171</td>
                  <td>0.881</td>
                </tr>
<tr>
<td>R2 Adj.</td>
                  <td>0.170</td>
                  <td>0.880</td>
                </tr>
<tr>
<td>AIC</td>
                  <td>15788.6</td>
                  <td>13853.1</td>
                </tr>
<tr>
<td>BIC</td>
                  <td>15803.3</td>
                  <td>13872.7</td>
                </tr>
<tr>
<td>Log.Lik.</td>
                  <td>-7891.307</td>
                  <td>-6922.553</td>
                </tr>
<tr>
<td>F</td>
                  <td>206.123</td>
                  <td>3676.480</td>
                </tr>
<tr>
<td>RMSE</td>
                  <td>647.01</td>
                  <td>245.58</td>
                </tr>
</tbody>
</table></div>
</div>
<!-- hack to avoid NA insertion in last line -->
</div>
</div>
<div id="fuller-and-bias-reduced-iv" class="section level3" number="34.3.6">
<h3>
<span class="header-section-number">34.3.6</span> Fuller and Bias-Reduced IV<a class="anchor" aria-label="anchor" href="#fuller-and-bias-reduced-iv"><i class="fas fa-link"></i></a>
</h3>
<p>Fuller adjusts LIML for bias reduction.</p>
<div class="sourceCode" id="cb907"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fuller_model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x_endo_1</span> <span class="op">|</span> <span class="va">x_inst_1</span>, data <span class="op">=</span> <span class="va">base</span>, method <span class="op">=</span> <span class="st">"fuller"</span>, k <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fuller_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivreg(formula = y ~ x_endo_1 | x_inst_1, data = base, method = "fuller", </span></span>
<span><span class="co">#&gt;     k = 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -1.2390 -0.3022 -0.0206  0.2772  1.0039 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  4.34586    0.08096   53.68   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x_endo_1     0.39848    0.01964   20.29   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.4075 on 148 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-Squared: 0.7595,  Adjusted R-squared: 0.7578 </span></span>
<span><span class="co">#&gt; Wald test: 411.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<hr>
</div>
</div>
<div id="asymptotic-properties-of-the-iv-estimator" class="section level2" number="34.4">
<h2>
<span class="header-section-number">34.4</span> Asymptotic Properties of the IV Estimator<a class="anchor" aria-label="anchor" href="#asymptotic-properties-of-the-iv-estimator"><i class="fas fa-link"></i></a>
</h2>
<p>IV estimation provides consistent and asymptotically normal estimates of structural parameters under a specific set of assumptions. Understanding the asymptotic properties of the IV estimator requires clarity on the identification conditions and the large-sample behavior of the estimator.</p>
<p>Consider the linear structural model:</p>
<p><span class="math display">\[
Y = X \beta + u
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> is the dependent variable (<span class="math inline">\(n \times 1\)</span>)</p></li>
<li><p><span class="math inline">\(X\)</span> is a matrix of endogenous regressors (<span class="math inline">\(n \times k\)</span>)</p></li>
<li><p><span class="math inline">\(u\)</span> is the error term</p></li>
<li><p><span class="math inline">\(\beta\)</span> is the parameter vector of interest (<span class="math inline">\(k \times 1\)</span>)</p></li>
</ul>
<p>Suppose we have a matrix of instruments <span class="math inline">\(Z\)</span> (<span class="math inline">\(n \times m\)</span>), with <span class="math inline">\(m \ge k\)</span>.</p>
<p>The <strong>IV estimator</strong> of <span class="math inline">\(\beta\)</span> is:</p>
<p><span class="math display">\[
\hat{\beta}_{IV} = (Z'X)^{-1} Z'Y
\]</span></p>
<p>Alternatively, when using <strong>2SLS</strong>, this is equivalent to:</p>
<p><span class="math display">\[
\hat{\beta}_{2SLS} = (X'P_ZX)^{-1} X'P_ZY
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(P_Z = Z (Z'Z)^{-1} Z'\)</span> is the projection matrix onto the column space of <span class="math inline">\(Z\)</span>.</li>
</ul>
<div id="sec-consistency-iv" class="section level3" number="34.4.1">
<h3>
<span class="header-section-number">34.4.1</span> Consistency<a class="anchor" aria-label="anchor" href="#sec-consistency-iv"><i class="fas fa-link"></i></a>
</h3>
<p>For <span class="math inline">\(\hat{\beta}_{IV}\)</span> to be <strong>consistent</strong>, the following conditions must hold as <span class="math inline">\(n \to \infty\)</span>:</p>
<ol style="list-style-type: decimal">
<li><strong>Instrument Exogeneity</strong></li>
</ol>
<p><span class="math display">\[
\mathbb{E}[Z'u] = 0
\]</span></p>
<ul>
<li><p>Instruments must be uncorrelated with the structural error term.</p></li>
<li><p>Guarantees instrument validity.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>Instrument Relevance</strong></li>
</ol>
<p><span class="math display">\[
\mathrm{rank}(\mathbb{E}[Z'X]) = k
\]</span></p>
<ul>
<li><p>Instruments must be correlated with the endogenous regressors.</p></li>
<li><p>Ensures identification of <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>If this fails, the model is underidentified, and <span class="math inline">\(\hat{\beta}_{IV}\)</span> does not converge to the true <span class="math inline">\(\beta\)</span>.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Random Sampling (IID Observations)</strong></li>
</ol>
<ul>
<li>
<span class="math inline">\(\{(Y_i, X_i, Z_i)\}_{i=1}^n\)</span> are independent and identically distributed (i.i.d.).</li>
<li>In more general settings, stationarity and mixing conditions can relax this.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><strong>Finite Moments</strong></li>
</ol>
<ul>
<li>
<span class="math inline">\(\mathbb{E}[||Z||^2] &lt; \infty\)</span> and <span class="math inline">\(\mathbb{E}[||u||^2] &lt; \infty\)</span>
</li>
<li>Ensures <a href="prerequisites.html#law-of-large-numbers">Law of Large Numbers</a> applies to sample moments.</li>
</ul>
<p>If these conditions are satisfied: <span class="math display">\[
\hat{\beta}_{IV} \overset{p}{\to} \beta
\]</span> This means the IV estimator is consistent.</p>
</div>
<div id="asymptotic-normality-1" class="section level3" number="34.4.2">
<h3>
<span class="header-section-number">34.4.2</span> Asymptotic Normality<a class="anchor" aria-label="anchor" href="#asymptotic-normality-1"><i class="fas fa-link"></i></a>
</h3>
<p>In addition to <a href="sec-instrumental-variables.html#sec-consistency-iv">consistency</a> conditions, we require:</p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Homoskedasticity (Optional but Simplifying)</strong></li>
</ol>
<p><span class="math display">\[
\mathbb{E}[u u' | Z] = \sigma^2 I
\]</span></p>
<ul>
<li><p>Simplifies variance estimation.</p></li>
<li><p>If violated, heteroskedasticity-robust variance estimators must be used.</p></li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li><strong><a href="prerequisites.html#central-limit-theorem">Central Limit Theorem</a> Conditions</strong></li>
</ol>
<ul>
<li>Sample moments must satisfy a CLT: <span class="math display">\[
\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n Z_i u_i \right) \overset{d}{\to} N(0, \Omega)
\]</span> Where <span class="math inline">\(\Omega = \mathbb{E}[Z_i Z_i' u_i^2]\)</span>.</li>
</ul>
<p>Under the above conditions: <span class="math display">\[
\sqrt{n}(\hat{\beta}_{IV} - \beta) \overset{d}{\to} N(0, V)
\]</span></p>
<p>Where the asymptotic variance-covariance matrix <span class="math inline">\(V\)</span> is: <span class="math display">\[
V = (Q_{ZX})^{-1} Q_{Zuu} (Q_{ZX}')^{-1}
\]</span> With:</p>
<ul>
<li><p><span class="math inline">\(Q_{ZX} = \mathbb{E}[Z_i X_i']\)</span></p></li>
<li><p><span class="math inline">\(Q_{Zuu} = \mathbb{E}[Z_i Z_i' u_i^2]\)</span></p></li>
</ul>
</div>
<div id="asymptotic-efficiency-1" class="section level3" number="34.4.3">
<h3>
<span class="header-section-number">34.4.3</span> Asymptotic Efficiency<a class="anchor" aria-label="anchor" href="#asymptotic-efficiency-1"><i class="fas fa-link"></i></a>
</h3>
<ol start="7" style="list-style-type: decimal">
<li><strong>Optimal Instrument Choice</strong></li>
</ol>
<ul>
<li>Among all IV estimators, <strong>2SLS</strong> is efficient when the instrument matrix <span class="math inline">\(Z\)</span> contains all relevant information.</li>
<li>Generalized Method of Moments (GMM) can deliver efficiency gains in the presence of heteroskedasticity, by optimally weighting the moment conditions.</li>
</ul>
<p>GMM Estimator</p>
<p><span class="math display">\[
\hat{\beta}_{GMM} = \arg \min_{\beta} \left( \frac{1}{n} \sum_{i=1}^n Z_i (Y_i - X_i' \beta) \right)' W \left( \frac{1}{n} \sum_{i=1}^n Z_i (Y_i - X_i' \beta) \right)
\]</span></p>
<p>Where <span class="math inline">\(W\)</span> is an optimal weighting matrix, typically:</p>
<p><span class="math display">\[
W = \Omega^{-1}
\]</span></p>
<p>Result</p>
<ul>
<li>If <span class="math inline">\(Z\)</span> is overidentified (<span class="math inline">\(m &gt; k\)</span>), GMM can be more efficient than 2SLS.</li>
<li>When instruments are exactly identified (<span class="math inline">\(m = k\)</span>), IV, 2SLS, and GMM coincide.</li>
</ul>
<p>Summary Table of Conditions</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="29%">
<col width="39%">
<col width="30%">
</colgroup>
<thead><tr class="header">
<th><strong>Condition</strong></th>
<th><strong>Requirement</strong></th>
<th><strong>Purpose</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Instrument Exogeneity</td>
<td><span class="math inline">\(\mathbb{E}[Z'u] = 0\)</span></td>
<td>Instrument validity</td>
</tr>
<tr class="even">
<td>Instrument Relevance</td>
<td><span class="math inline">\(\mathrm{rank}(\mathbb{E}[Z'X]) = k\)</span></td>
<td>Model identification</td>
</tr>
<tr class="odd">
<td>Random Sampling</td>
<td>IID (or stationary and mixing)</td>
<td>LLN and CLT applicability</td>
</tr>
<tr class="even">
<td>Finite Second Moments</td>
<td>
<span class="math inline">\(\mathbb{E}[||Z||^2] &lt; \infty\)</span>, etc.</td>
<td>LLN and CLT applicability</td>
</tr>
<tr class="odd">
<td>Homoskedasticity (optional)</td>
<td><span class="math inline">\(\mathbb{E}[u u' | Z] = \sigma^2 I\)</span></td>
<td>Simplifies variance formulas</td>
</tr>
<tr class="even">
<td>Optimal Weighting</td>
<td>
<span class="math inline">\(W = \Omega^{-1}\)</span> in GMM</td>
<td>Asymptotic efficiency</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="sec-inference-iv" class="section level2" number="34.5">
<h2>
<span class="header-section-number">34.5</span> Inference<a class="anchor" aria-label="anchor" href="#sec-inference-iv"><i class="fas fa-link"></i></a>
</h2>
<p>Inference in IV models, particularly when instruments are weak, presents serious challenges that can undermine standard testing and confidence interval procedures. In this section, we explore the core issues of IV inference under weak instruments, discuss the standard and alternative approaches, and outline practical guidelines for applied research.</p>
<p>Consider the just-identified linear IV model:</p>
<p><span class="math display">\[
Y = \beta X + u
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(X\)</span> is endogenous: <span class="math inline">\(\text{Cov}(X, u) \neq 0\)</span>.</p></li>
<li>
<p><span class="math inline">\(Z\)</span> is an instrumental variable satisfying:</p>
<ul>
<li><p>Relevance: <span class="math inline">\(\text{Cov}(Z, X) \neq 0\)</span>.</p></li>
<li><p>Exogeneity: <span class="math inline">\(\text{Cov}(Z, u) = 0\)</span>.</p></li>
</ul>
</li>
</ul>
<p>The IV estimator of <span class="math inline">\(\beta\)</span> is consistent under these assumptions.</p>
<p>A commonly used approach for inference is the t-ratio method, constructing a 95% confidence interval as:</p>
<p><span class="math display">\[
\hat{\beta} \pm 1.96 \sqrt{\hat{V}_N(\hat{\beta})}
\]</span></p>
<p>However, this approach is invalid when instruments are weak. Specifically:</p>
<ul>
<li><p>The t-ratio does not follow a standard normal distribution under weak instruments.</p></li>
<li><p>Confidence intervals based on this method can severely under-cover the true parameter.</p></li>
<li><p>Hypothesis tests can over-reject, even in large samples.</p></li>
</ul>
<p>This problem was first systematically identified by <span class="citation">Staiger and Stock (<a href="references.html#ref-staiger1997instrumental">1997</a>)</span> and <span class="citation">Dufour (<a href="references.html#ref-dufour1997some">1997</a>)</span>. Weak instruments create distortions in the finite-sample distribution of <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>Common Practices and Misinterpretations</p>
<ol style="list-style-type: decimal">
<li>Overreliance on t-Ratio Tests</li>
</ol>
<ul>
<li>Popular but problematic when instruments are weak.</li>
<li>Known to over-reject null hypotheses and under-cover confidence intervals.</li>
<li>Documented extensively by <span class="citation">Nelson and Startz (<a href="references.html#ref-nelson1990distribution">1990</a>)</span>, <span class="citation">Bound, Jaeger, and Baker (<a href="references.html#ref-bound1995problems">1995</a>)</span>, <span class="citation">Dufour (<a href="references.html#ref-dufour1997some">1997</a>)</span>, and <span class="citation">Lee et al. (<a href="references.html#ref-lee2022valid">2022</a>)</span>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Weak Instrument Diagnostics</li>
</ol>
<ul>
<li>First-Stage F-Statistic:
<ul>
<li>Rule of thumb: <span class="math inline">\(F &gt; 10\)</span> often used but simplistic and misleading.</li>
<li>More accurate critical values provided by <span class="citation">Stock and Yogo (<a href="references.html#ref-stock2005testing">2005</a>)</span>.</li>
<li>For 95% coverage, <span class="math inline">\(F &gt; 16.38\)</span> is often cited <span class="citation">(<a href="references.html#ref-staiger1997instrumental">Staiger and Stock 1997</a>)</span>.</li>
</ul>
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Misinterpretations and Pitfalls</li>
</ol>
<ul>
<li>Mistakenly interpreting <span class="math inline">\(\hat{\beta} \pm 1.96 \times \hat{SE}\)</span> as a 95% CI when the instrument is weak, <span class="citation">Staiger and Stock (<a href="references.html#ref-staiger1997instrumental">1997</a>)</span> show that under <span class="math inline">\(F &gt; 16.38\)</span>, the nominal 95% CI may only offer 85% coverage.</li>
<li>Pretesting for weak instruments can exacerbate inference problems <span class="citation">(<a href="references.html#ref-hall1996judging">A. R. Hall, Rudebusch, and Wilcox 1996</a>)</span>.</li>
<li>Selective model specification based on weak instrument diagnostics may introduce additional distortions <span class="citation">(<a href="references.html#ref-andrews2019weak">I. Andrews, Stock, and Sun 2019</a>)</span>.</li>
</ul>
<hr>
<div id="sec-weak-instruments-problem" class="section level3" number="34.5.1">
<h3>
<span class="header-section-number">34.5.1</span> Weak Instruments Problem<a class="anchor" aria-label="anchor" href="#sec-weak-instruments-problem"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative statistic accounts for weak instrument issues by adjusting the standard Anderson-Rubin (AR) test:</p>
<p><span class="math display">\[
\hat{t}^2 = \hat{t}^2_{AR} \times \frac{1}{1 - \hat{\rho} \frac{\hat{t}_{AR}}{\hat{f}} + \frac{\hat{t}^2_{AR}}{\hat{f}^2}}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\hat{t}^2_{AR} \sim \chi^2(1)\)</span> under the null, even with weak instruments <span class="citation">(<a href="references.html#ref-anderson1949estimation">T. W. Anderson and Rubin 1949</a>)</span>.</p></li>
<li><p><span class="math inline">\(\hat{t}_{AR} = \dfrac{\hat{\pi}(\hat{\beta} - \beta_0)}{\sqrt{\hat{V}_N (\hat{\pi} (\hat{\beta} - \beta_0))}} \sim N(0,1)\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{f} = \dfrac{\hat{\pi}}{\sqrt{\hat{V}_N(\hat{\pi})}}\)</span> measures instrument strength (first-stage F-stat).</p></li>
<li><p><span class="math inline">\(\hat{\pi}\)</span> is the coefficient from the first-stage regression of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{\rho} = \text{Cov}(Zv, Zu)\)</span> captures the correlation between first-stage residuals and <span class="math inline">\(u\)</span>.</p></li>
</ul>
<p>Implications</p>
<ul>
<li>Even in large samples, <span class="math inline">\(\hat{t}^2 \neq \hat{t}^2_{AR}\)</span> because the adjustment term does not converge to zero unless instruments are strong and <span class="math inline">\(\rho = 0\)</span>.</li>
<li>The distribution of <span class="math inline">\(\hat{t}\)</span> does not match the standard normal but follows a more complex distribution described by <span class="citation">Staiger and Stock (<a href="references.html#ref-staiger1997instrumental">1997</a>)</span> and <span class="citation">Stock and Yogo (<a href="references.html#ref-stock2005testing">2005</a>)</span>.</li>
</ul>
<hr>
<p>The divergence between <span class="math inline">\(\hat{t}^2\)</span> and <span class="math inline">\(\hat{t}^2_{AR}\)</span> depends on:</p>
<ol style="list-style-type: decimal">
<li>Instrument Strength (<span class="math inline">\(\pi\)</span>): Higher correlation between <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> mitigates the problem.</li>
<li>First-Stage F-statistic (<span class="math inline">\(E(F)\)</span>): A weak first-stage regression increases the bias and distortion.</li>
<li>Endogeneity Level (<span class="math inline">\(|\rho|\)</span>): Greater correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> exacerbates inference errors.</li>
</ol>
<hr>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="11%">
<col width="41%">
<col width="46%">
</colgroup>
<thead><tr class="header">
<th>Scenario</th>
<th>Conditions</th>
<th>Inference Quality</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Worst Case</td>
<td>
<span class="math inline">\(\pi = 0\)</span>, <span class="math inline">\(|\rho| = 1\)</span>
</td>
<td>
<span class="math inline">\(\hat{\beta} \pm 1.96 \times SE\)</span> fails; Type I error = 100%</td>
</tr>
<tr class="even">
<td>Best Case</td>
<td>
<span class="math inline">\(\rho = 0\)</span> (No endogeneity) or very large <span class="math inline">\(\hat{f}\)</span> (strong <span class="math inline">\(Z\)</span>)</td>
<td>Standard inference works; intervals cover <span class="math inline">\(\beta\)</span> with correct rate</td>
</tr>
<tr class="odd">
<td>Intermediate Case</td>
<td>Moderate <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\rho\)</span>, and <span class="math inline">\(F\)</span>
</td>
<td>Coverage and Type I error lie between extremes; standard inference risky</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
<div id="solutions-and-approaches-for-valid-inference" class="section level3" number="34.5.2">
<h3>
<span class="header-section-number">34.5.2</span> Solutions and Approaches for Valid Inference<a class="anchor" aria-label="anchor" href="#solutions-and-approaches-for-valid-inference"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>Assume the Problem Away (Risky Assumptions)
<ol style="list-style-type: decimal">
<li>High First-Stage F-statistic:
<ul>
<li>Require <span class="math inline">\(E(F) &gt; 142.6\)</span> for near-validity <span class="citation">(<a href="references.html#ref-lee2022valid">Lee et al. 2022</a>)</span>.</li>
<li>While the first-stage <span class="math inline">\(F\)</span> is observable, this threshold is high and often impractical.</li>
</ul>
</li>
<li>Low Endogeneity:
<ul>
<li>Assume <span class="math inline">\(|\rho| &lt; 0.565\)</span> <span class="citation">Lee et al. (<a href="references.html#ref-lee2022valid">2022</a>)</span>. In other words, we assume endogeneity to be less than moderat level.</li>
<li>This undermines the motivation for IV in the first place, which exists precisely because of suspected endogeneity.</li>
</ul>
</li>
</ol>
</li>
<li>Confront the Problem Directly (Robust Methods)
<ol style="list-style-type: decimal">
<li>
<a href="sec-instrumental-variables.html#sec-anderson-rubin-approach">Anderson-Rubin (AR) Test</a> <span class="citation">(<a href="references.html#ref-anderson1949estimation">T. W. Anderson and Rubin 1949</a>)</span>:
<ul>
<li>Valid under weak instruments.</li>
<li>Tests whether <span class="math inline">\(Z\)</span> explains variation in <span class="math inline">\(Y - \beta_0 X\)</span>.</li>
</ul>
</li>
<li>
<a href="sec-instrumental-variables.html#sec-tf-procedure">tF Procedure</a> <span class="citation">(<a href="references.html#ref-lee2022valid">Lee et al. 2022</a>)</span>:
<ul>
<li>Combines t-statistics and F-statistics in a unified testing framework.</li>
<li>Offers valid inference in presence of weak instruments.</li>
</ul>
</li>
<li>
<a href="sec-instrumental-variables.html#sec-ak-approach">Andrews-Kolesár (AK) Procedure</a> <span class="citation">(<a href="references.html#ref-angrist2023one">J. Angrist and Kolesár 2023</a>)</span>:
<ul>
<li>Provides uniformly valid confidence intervals for <span class="math inline">\(\beta\)</span>.</li>
<li>Allows for weak instruments and arbitrary heteroskedasticity.</li>
<li>Especially useful in overidentified settings.</li>
</ul>
</li>
</ol>
</li>
</ol>
<hr>
</div>
<div id="sec-anderson-rubin-approach" class="section level3" number="34.5.3">
<h3>
<span class="header-section-number">34.5.3</span> Anderson-Rubin Approach<a class="anchor" aria-label="anchor" href="#sec-anderson-rubin-approach"><i class="fas fa-link"></i></a>
</h3>
<p>The Anderson-Rubin (AR) test, originally proposed by <span class="citation">T. W. Anderson and Rubin (<a href="references.html#ref-anderson1949estimation">1949</a>)</span>, remains one of the most robust inferential tools in the context of instrumental variable estimation, particularly when instruments are weak or endogenous regressors exhibit complex error structures.</p>
<p>The AR test directly evaluates the joint null hypothesis that:</p>
<p><span class="math display">\[
H_0: \beta = \beta_0
\]</span></p>
<p>by testing whether the instruments explain any variation in the residuals <span class="math inline">\(Y - \beta_0 X\)</span>. Under the null, the model becomes:</p>
<p><span class="math display">\[
Y - \beta_0 X = u
\]</span></p>
<p>Given that <span class="math inline">\(\text{Cov}(Z, u) = 0\)</span> (by the IV exogeneity assumption), the test regresses <span class="math inline">\((Y - \beta_0 X)\)</span> on <span class="math inline">\(Z\)</span>. The test statistic is constructed as:</p>
<p><span class="math display">\[
AR(\beta_0) = \frac{(Y - \beta_0 X)' P_Z (Y - \beta_0 X)}{\hat{\sigma}^2}
\]</span></p>
<ul>
<li>
<span class="math inline">\(P_Z\)</span> is the projection matrix onto the column space of <span class="math inline">\(Z\)</span>: <span class="math inline">\(P_Z = Z (Z'Z)^{-1} Z'\)</span>.</li>
<li>
<span class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the error variance (under homoskedasticity).</li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>, the statistic follows a chi-squared distribution:</p>
<p><span class="math display">\[
AR(\beta_0) \sim \chi^2(q)
\]</span></p>
<p>where <span class="math inline">\(q\)</span> is the number of instruments (1 in a just-identified model).</p>
<hr>
<p>Key Properties of the AR Test</p>
<ul>
<li>Robust to Weak Instruments:
<ul>
<li>The AR test does not rely on the strength of the instruments.</li>
<li>Its distribution under the null hypothesis remains valid even when the instruments are weak <span class="citation">(<a href="references.html#ref-staiger1997instrumental">Staiger and Stock 1997</a>)</span>.</li>
</ul>
</li>
<li>Robust to Non-Normality and Homoskedastic Errors:
<ul>
<li>Maintains correct Type I error rates even under non-normal errors <span class="citation">(<a href="references.html#ref-staiger1997instrumental">Staiger and Stock 1997</a>)</span>.</li>
<li>Optimality properties under homoskedastic errors are established in <span class="citation">D. W. Andrews, Moreira, and Stock (<a href="references.html#ref-andrews2006optimal">2006</a>)</span> and <span class="citation">M. J. Moreira (<a href="references.html#ref-moreira2009tests">2009</a>)</span>.</li>
</ul>
</li>
<li>Robust to Heteroskedasticity, Clustering, and Autocorrelation:
<ul>
<li>The AR test has been generalized to account for heteroskedasticity, clustered errors, and autocorrelation <span class="citation">(<a href="references.html#ref-stock2000gmm">Stock and Wright 2000</a>; <a href="references.html#ref-moreira2019optimal">H. Moreira and Moreira 2019</a>)</span>.</li>
<li>Valid inference is possible when combined with heteroskedasticity-robust variance estimators or cluster-robust techniques.</li>
</ul>
</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="23%">
<col width="49%">
<col width="27%">
</colgroup>
<thead><tr class="header">
<th>Setting</th>
<th>Validity</th>
<th>Reference</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Non-Normal, Homoskedastic Errors</td>
<td>Valid without distributional assumptions</td>
<td><span class="citation">(<a href="references.html#ref-staiger1997instrumental">Staiger and Stock 1997</a>)</span></td>
</tr>
<tr class="even">
<td>Heteroskedastic Errors</td>
<td>Generalized AR test remains valid; robust variance estimation recommended</td>
<td><span class="citation">(<a href="references.html#ref-stock2000gmm">Stock and Wright 2000</a>)</span></td>
</tr>
<tr class="odd">
<td>Clustered or Autocorrelated Errors</td>
<td>Extensions available using cluster-robust and HAC variance estimators</td>
<td><span class="citation">(<a href="references.html#ref-moreira2019optimal">H. Moreira and Moreira 2019</a>)</span></td>
</tr>
<tr class="even">
<td>Optimality under Homoskedasticity</td>
<td>AR test minimizes Type II error among invariant tests</td>
<td><span class="citation">(<a href="references.html#ref-andrews2006optimal">D. W. Andrews, Moreira, and Stock 2006</a>; <a href="references.html#ref-moreira2009tests">M. J. Moreira 2009</a>)</span></td>
</tr>
</tbody>
</table></div>
<hr>
<p>The AR test is relatively simple to implement and is available in most econometric software. Here’s an intuitive step-by-step breakdown:</p>
<ol style="list-style-type: decimal">
<li>Specify the null hypothesis value <span class="math inline">\(\beta_0\)</span>.</li>
<li>Compute the residual <span class="math inline">\(u = Y - \beta_0 X\)</span>.</li>
<li>Regress <span class="math inline">\(u\)</span> on <span class="math inline">\(Z\)</span> and obtain the <span class="math inline">\(R^2\)</span> from this regression.</li>
<li>Compute the test statistic:</li>
</ol>
<p><span class="math display">\[
AR(\beta_0) = \frac{R^2 \cdot n}{q}
\]</span></p>
<p>(For a just-identified model with a single instrument, <span class="math inline">\(q=1\)</span>.)</p>
<ol start="5" style="list-style-type: decimal">
<li>Compare <span class="math inline">\(AR(\beta_0)\)</span> to the <span class="math inline">\(\chi^2(q)\)</span> distribution to determine significance.</li>
</ol>
<div class="sourceCode" id="cb908"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://yiqingxu.org/packages/ivDiag/">ivDiag</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># AR test (robust to weak instruments)</span></span>
<span><span class="co"># example by the package's authors</span></span>
<span><span class="fu">ivDiag</span><span class="fu">::</span><span class="fu"><a href="https://yiqingxu.org/packages/ivDiag/reference/AR_test.html">AR_test</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">rueda</span>,</span>
<span>    Y <span class="op">=</span> <span class="st">"e_vote_buying"</span>,</span>
<span>    <span class="co"># treatment</span></span>
<span>    D <span class="op">=</span> <span class="st">"lm_pob_mesa"</span>,</span>
<span>    <span class="co"># instruments</span></span>
<span>    Z <span class="op">=</span> <span class="st">"lz_pob_mesa_f"</span>,</span>
<span>    controls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lpopulation"</span>, <span class="st">"lpotencial"</span><span class="op">)</span>,</span>
<span>    cl <span class="op">=</span> <span class="st">"muni_code"</span>,</span>
<span>    CI <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $Fstat</span></span>
<span><span class="co">#&gt;         F       df1       df2         p </span></span>
<span><span class="co">#&gt;   48.4768    1.0000 4350.0000    0.0000</span></span>
<span></span>
<span><span class="va">g</span> <span class="op">&lt;-</span> <span class="fu">ivDiag</span><span class="fu">::</span><span class="fu"><a href="https://yiqingxu.org/packages/ivDiag/reference/ivDiag.html">ivDiag</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">rueda</span>,</span>
<span>    Y <span class="op">=</span> <span class="st">"e_vote_buying"</span>,</span>
<span>    D <span class="op">=</span> <span class="st">"lm_pob_mesa"</span>,</span>
<span>    Z <span class="op">=</span> <span class="st">"lz_pob_mesa_f"</span>,</span>
<span>    controls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lpopulation"</span>, <span class="st">"lpotencial"</span><span class="op">)</span>,</span>
<span>    cl <span class="op">=</span> <span class="st">"muni_code"</span>,</span>
<span>    cores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    bootstrap <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">g</span><span class="op">$</span><span class="va">AR</span></span>
<span><span class="co">#&gt; $Fstat</span></span>
<span><span class="co">#&gt;         F       df1       df2         p </span></span>
<span><span class="co">#&gt;   48.4768    1.0000 4350.0000    0.0000 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $ci.print</span></span>
<span><span class="co">#&gt; [1] "[-1.2626, -0.7073]"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $ci</span></span>
<span><span class="co">#&gt; [1] -1.2626 -0.7073</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $bounded</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="fu">ivDiag</span><span class="fu">::</span><span class="fu"><a href="https://yiqingxu.org/packages/ivDiag/reference/plot_coef.html">plot_coef</a></span><span class="op">(</span><span class="va">g</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="34-instrumental_var_files/figure-html/unnamed-chunk-13-1.png" width="90%" style="display: block; margin: auto;"></div>
<hr>
</div>
<div id="sec-tf-procedure" class="section level3" number="34.5.4">
<h3>
<span class="header-section-number">34.5.4</span> tF Procedure<a class="anchor" aria-label="anchor" href="#sec-tf-procedure"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">Lee et al. (<a href="references.html#ref-lee2022valid">2022</a>)</span> introduce the tF procedure, an inference method specifically designed for just-identified IV models (single endogenous regressor and single instrument). It addresses the shortcomings of traditional 2SLS <span class="math inline">\(t\)</span>-tests under weak instruments and offers a solution that is conceptually familiar to researchers trained in standard econometric practices.</p>
<p>Unlike the <a href="sec-instrumental-variables.html#sec-anderson-rubin-approach">Anderson-Rubin test</a>, which inverts hypothesis tests to form confidence sets, the tF procedure adjusts standard <span class="math inline">\(t\)</span>-statistics and standard errors directly, making it a more intuitive extension of traditional hypothesis testing.</p>
<p>The tF procedure is widely applicable in settings where just-identified IV models arise, including:</p>
<ul>
<li><p>Randomized controlled trials with imperfect compliance<br>
(e.g., <a href="sec-causal-inference.html#sec-local-average-treatment-effects">Local Average Treatment Effects</a> in <span class="citation">G. W. Imbens and Angrist (<a href="references.html#ref-imbens1994identification">1994</a>)</span>).</p></li>
<li><p><a href="sec-regression-discontinuity.html#sec-fuzzy-regression-discontinuity-design">Fuzzy Regression Discontinuity Designs</a><br>
(e.g., <span class="citation">Lee and Lemieux (<a href="references.html#ref-lee2010regression">2010</a>)</span>).</p></li>
<li><p><a href="sec-regression-discontinuity.html#sec-identification-in-fuzzy-regression-kink-design">Fuzzy Regression Kink Designs</a><br>
(e.g., <span class="citation">(<a href="references.html#ref-card2015inference">Card et al. 2015</a>)</span>).</p></li>
</ul>
<p>A comparison of the <a href="sec-instrumental-variables.html#sec-anderson-rubin-approach">AR approach</a> and the <a href="sec-instrumental-variables.html#sec-tf-procedure">tF procedure</a> can be found in <span class="citation">I. Andrews, Stock, and Sun (<a href="references.html#ref-andrews2019weak">2019</a>)</span>.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="18%">
<col width="41%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Anderson-Rubin</th>
<th>tF Procedure</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Robustness to Weak IV</td>
<td>Yes (valid under weak instruments)</td>
<td>Yes (valid under weak instruments)</td>
</tr>
<tr class="even">
<td>Finite Confidence Intervals</td>
<td>No (interval becomes infinite for <span class="math inline">\(F \le 3.84\)</span>)</td>
<td>Yes (finite intervals for all <span class="math inline">\(F\)</span> values)</td>
</tr>
<tr class="odd">
<td>Interval Length</td>
<td>Often longer, especially when <span class="math inline">\(F\)</span> is moderate (e.g., <span class="math inline">\(F = 16\)</span>)</td>
<td>Typically shorter than AR intervals for <span class="math inline">\(F &gt; 3.84\)</span>
</td>
</tr>
<tr class="even">
<td>Ease of Interpretation</td>
<td>Requires inverting tests; less intuitive</td>
<td>Directly adjusts <span class="math inline">\(t\)</span>-based standard errors; more intuitive</td>
</tr>
<tr class="odd">
<td>Computational Simplicity</td>
<td>Moderate (inversion of hypothesis tests)</td>
<td>Simple (multiplicative adjustment to standard errors)</td>
</tr>
</tbody>
</table></div>
<ul>
<li>With <span class="math inline">\(F &gt; 3.84\)</span>, the AR test’s expected interval length is infinite, whereas the tF procedure guarantees finite intervals, making it superior in practical applications with weak instruments.</li>
</ul>
<p>The tF procedure adjusts the conventional 2SLS <span class="math inline">\(t\)</span>-ratio for the first-stage F-statistic strength. Instead of relying on a pre-testing threshold (e.g., <span class="math inline">\(F &gt; 10\)</span>), the tF approach provides a smooth adjustment to the standard errors.</p>
<p>Key Features:</p>
<ul>
<li>Adjusts the 2SLS <span class="math inline">\(t\)</span>-ratio based on the observed first-stage F-statistic.</li>
<li>Applies different adjustment factors for different significance levels (e.g., 95% and 99%).</li>
<li>Remains valid even when the instrument is weak, offering finite confidence intervals even when the first-stage F-statistic is low.</li>
</ul>
<p>Advantages of the tF Procedure</p>
<ol style="list-style-type: decimal">
<li>Smooth Adjustment for First-Stage Strength</li>
</ol>
<ul>
<li><p>The tF procedure smoothly adjusts inference based on the observed first-stage F-statistic, avoiding the need for arbitrary pre-testing thresholds (e.g., <span class="math inline">\(F &gt; 10\)</span>).</p></li>
<li>
<p>It produces finite and usable confidence intervals even when the first-stage F-statistic is low:</p>
<p><span class="math display">\[
F &gt; 3.84
\]</span></p>
</li>
<li>
<p>This threshold aligns with the critical value of 3.84 for a 95% <a href="sec-instrumental-variables.html#sec-anderson-rubin-approach">Anderson-Rubin</a> confidence interval, but with a crucial advantage:</p>
<ul>
<li>The AR interval becomes unbounded (i.e., infinite length) when <span class="math inline">\(F \le 3.84\)</span>.</li>
<li>The tF procedure, in contrast, still provides a finite confidence interval, making it more practical in weak instrument cases.</li>
</ul>
</li>
</ul>
<hr>
<ol start="2" style="list-style-type: decimal">
<li>Clear and Interpretable Confidence Levels</li>
</ol>
<ul>
<li>
<p>The tF procedure offers transparent confidence intervals that:</p>
<ul>
<li><p>Directly incorporate the impact of first-stage instrument strength on the critical values used for inference.</p></li>
<li><p>Mirror the distortion-free properties of robust methods like the <a href="sec-instrumental-variables.html#sec-anderson-rubin-approach">Anderson-Rubin</a> test, but remain closer in spirit to conventional <span class="math inline">\(t\)</span>-based inference.</p></li>
</ul>
</li>
<li><p>Researchers can interpret tF-based 95% and 99% confidence intervals using familiar econometric tools, without needing to invert hypothesis tests or construct confidence sets.</p></li>
</ul>
<hr>
<ol start="3" style="list-style-type: decimal">
<li>Robustness to Common Error Structures</li>
</ol>
<ul>
<li>
<p>The tF procedure remains robust in the presence of:</p>
<ul>
<li>Heteroskedasticity</li>
<li>Clustering</li>
<li>Autocorrelation</li>
</ul>
</li>
<li>
<p>No additional adjustments are necessary beyond the use of a robust variance estimator for both:</p>
<ul>
<li>The first-stage regression</li>
<li>The second-stage IV regression</li>
</ul>
</li>
<li><p>As long as the same robust variance estimator is applied consistently, the tF adjustment maintains valid inference without imposing additional computational complexity.</p></li>
</ul>
<hr>
<ol start="4" style="list-style-type: decimal">
<li>Applicability to Published Research</li>
</ol>
<ul>
<li>
<p>One of the most powerful features of the tF procedure is its flexibility for re-evaluating published studies:</p>
<ul>
<li><p>Researchers only need the reported first-stage F-statistic and standard errors from the 2SLS estimates.</p></li>
<li><p>No access to the original data is required to recalculate confidence intervals or test statistical significance using the tF adjustment.</p></li>
</ul>
</li>
<li>
<p>This makes the tF procedure particularly valuable for meta-analyses, replications, and robustness checks of published IV studies, where:</p>
<ul>
<li>Raw data may be unavailable, or</li>
<li>Replication costs are high.</li>
</ul>
</li>
</ul>
<hr>
<p>Consider the linear IV model with additional covariates <span class="math inline">\(W\)</span>:</p>
<p><span class="math display">\[
Y = X \beta + W \gamma + u
\]</span></p>
<p><span class="math display">\[
X = Z \pi + W \xi + \nu
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span>: Outcome variable.</p></li>
<li><p><span class="math inline">\(X\)</span>: Endogenous regressor of interest.</p></li>
<li><p><span class="math inline">\(Z\)</span>: Instrumental variable (single instrument case).</p></li>
<li><p><span class="math inline">\(W\)</span>: Vector of exogenous controls, possibly including an intercept.</p></li>
<li><p><span class="math inline">\(u\)</span>, <span class="math inline">\(\nu\)</span>: Error terms.</p></li>
</ul>
<p>Key Statistics:</p>
<ul>
<li>
<p><span class="math inline">\(t\)</span>-ratio for the IV estimator:</p>
<p><span class="math display">\[
\hat{t} = \frac{\hat{\beta} - \beta_0}{\sqrt{\hat{V}_N (\hat{\beta})}}
\]</span></p>
</li>
<li>
<p><span class="math inline">\(t\)</span>-ratio for the first-stage coefficient:</p>
<p><span class="math display">\[
\hat{f} = \frac{\hat{\pi}}{\sqrt{\hat{V}_N (\hat{\pi})}}
\]</span></p>
</li>
<li>
<p>First-stage F-statistic:</p>
<p><span class="math display">\[
\hat{F} = \hat{f}^2
\]</span></p>
</li>
</ul>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}\)</span>: Instrumental variable estimator.</li>
<li>
<span class="math inline">\(\hat{V}_N (\hat{\beta})\)</span>: Estimated variance of <span class="math inline">\(\hat{\beta}\)</span>, possibly robust to deal with non-iid errors.</li>
<li>
<span class="math inline">\(\hat{t}\)</span>: <span class="math inline">\(t\)</span>-ratio under the null hypothesis.</li>
<li>
<span class="math inline">\(\hat{f}\)</span>: <span class="math inline">\(t\)</span>-ratio under the null hypothesis of <span class="math inline">\(\pi=0\)</span>.</li>
</ul>
<hr>
<p>Under traditional asymptotics large samples, the <span class="math inline">\(t\)</span>-ratio statistic follows:</p>
<p><span class="math display">\[
\hat{t}^2 \to^d t^2
\]</span></p>
<p>With critical values:</p>
<ul>
<li><p><span class="math inline">\(\pm 1.96\)</span> for a 5% significance test.</p></li>
<li><p><span class="math inline">\(\pm 2.58\)</span> for a 1% significance test.</p></li>
</ul>
<p>However, in IV settings (particularly with weak instruments):</p>
<ul>
<li><p>The distribution of the <span class="math inline">\(t\)</span>-statistic is distorted (i.e., <span class="math inline">\(t\)</span>-distribution might not be normal), even in large samples.</p></li>
<li><p>The distortion arises because the strength of the instrument (<span class="math inline">\(F\)</span>) and the degree of endogeneity (<span class="math inline">\(\rho\)</span>) affect the <span class="math inline">\(t\)</span>-distribution.</p></li>
</ul>
<p><span class="citation">Stock and Yogo (<a href="references.html#ref-stock2005testing">2005</a>)</span> provide a formula to quantify this distortion (in the just-identified case) for Wald test statistics using 2SLS.:</p>
<p><span class="math display">\[
t^2 = f + t_{AR} + \rho f t_{AR}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\hat{f} \to^d f\)</span></p></li>
<li><p><span class="math inline">\(\bar{f} = \dfrac{\pi}{\sqrt{\dfrac{1}{N} AV(\hat{\pi})}}\)</span> and <span class="math inline">\(AV(\hat{\pi})\)</span> is the asymptotic variance of <span class="math inline">\(\hat{\pi}\)</span></p></li>
<li><p><span class="math inline">\(t_{AR}\)</span> is asymptotically standard normal (<span class="math inline">\(AR = t^2_{AR}\)</span>)</p></li>
<li><p><span class="math inline">\(\rho\)</span> measures the correlation (degree of endogeneity) between <span class="math inline">\(Zu\)</span> and <span class="math inline">\(Z\nu\)</span> (when data are homoskedastic, <span class="math inline">\(\rho\)</span> is the correlation between <span class="math inline">\(u\)</span> and <span class="math inline">\(\nu\)</span>).</p></li>
</ul>
<p>Implications:</p>
<ul>
<li>For low <span class="math inline">\(\rho\)</span> (<span class="math inline">\(\rho \in [0, 0.5]\)</span>), rejection probabilities can be below nominal levels.</li>
<li>For high <span class="math inline">\(\rho\)</span> (<span class="math inline">\(\rho = 0.8\)</span>), rejection rates can be inflated, e.g., 13% rejection at a nominal 5% significance level.</li>
<li>Reliance on standard <span class="math inline">\(t\)</span>-ratios leads to incorrect test sizes and invalid confidence intervals.</li>
</ul>
<hr>
<p>The tF procedure corrects for these distortions by adjusting the standard error of the 2SLS estimator based on the observed first-stage F-statistic.</p>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\hat{\beta}\)</span> and its conventional SE from 2SLS.</li>
<li>Compute the first-stage <span class="math inline">\(\hat{F}\)</span>.</li>
<li>Multiply the conventional SE by an adjustment factor, which depends on <span class="math inline">\(\hat{F}\)</span> and the desired confidence level.</li>
<li>Compute new <span class="math inline">\(t\)</span>-ratios and construct confidence intervals using standard critical values (e.g., <span class="math inline">\(\pm 1.96\)</span> for 95% CI).</li>
</ol>
<p><span class="citation">Lee et al. (<a href="references.html#ref-lee2022valid">2022</a>)</span> refer to the adjusted standard errors as “0.05 tF SE” (for a 5% significance level) and “0.01 tF SE” (for 1%).</p>
<hr>
<p><span class="citation">Lee et al. (<a href="references.html#ref-lee2022valid">2022</a>)</span> conducted a review of recent single-instrument studies in the American Economic Review.</p>
<p>Key Findings:</p>
<ul>
<li>For at least 25% of the examined specifications:
<ul>
<li>tF-adjusted confidence intervals were 49% longer at the 5% level.</li>
<li>tF-adjusted confidence intervals were 136% longer at the 1% level.</li>
</ul>
</li>
<li>Even among specifications with <span class="math inline">\(F &gt; 10\)</span> and <span class="math inline">\(t &gt; 1.96\)</span>:
<ul>
<li>Approximately 25% became statistically insignificant at the 5% level after applying the tF adjustment.</li>
</ul>
</li>
</ul>
<p>Takeaway:</p>
<ul>
<li>The tF procedure can substantially alter inference conclusions.</li>
<li>Published studies can be re-evaluated with the tF method using only the reported first-stage F-statistics, without requiring access to the underlying microdata.</li>
</ul>
<hr>
<div class="sourceCode" id="cb909"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://yiqingxu.org/packages/ivDiag/">ivDiag</a></span><span class="op">)</span></span>
<span><span class="va">g</span> <span class="op">&lt;-</span> <span class="fu">ivDiag</span><span class="fu">::</span><span class="fu"><a href="https://yiqingxu.org/packages/ivDiag/reference/ivDiag.html">ivDiag</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">rueda</span>,</span>
<span>    Y <span class="op">=</span> <span class="st">"e_vote_buying"</span>,</span>
<span>    D <span class="op">=</span> <span class="st">"lm_pob_mesa"</span>,</span>
<span>    Z <span class="op">=</span> <span class="st">"lz_pob_mesa_f"</span>,</span>
<span>    controls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lpopulation"</span>, <span class="st">"lpotencial"</span><span class="op">)</span>,</span>
<span>    cl <span class="op">=</span> <span class="st">"muni_code"</span>,</span>
<span>    cores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    bootstrap <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">g</span><span class="op">$</span><span class="va">tF</span></span>
<span><span class="co">#&gt;         F        cF      Coef        SE         t    CI2.5%   CI97.5%   p-value </span></span>
<span><span class="co">#&gt; 8598.3264    1.9600   -0.9835    0.1424   -6.9071   -1.2626   -0.7044    0.0000</span></span></code></pre></div>
<div class="sourceCode" id="cb910"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># example in fixest package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://lrberge.github.io/fixest/">fixest</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="va">base</span> <span class="op">=</span> <span class="va">iris</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">base</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"y"</span>, <span class="st">"x1"</span>, <span class="st">"x_endo_1"</span>, <span class="st">"x_inst_1"</span>, <span class="st">"fe"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">base</span><span class="op">$</span><span class="va">x_inst_2</span> <span class="op">=</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">y</span> <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">x_endo_1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">150</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="va">base</span><span class="op">$</span><span class="va">x_endo_2</span> <span class="op">=</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">y</span> <span class="op">-</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">base</span><span class="op">$</span><span class="va">x_inst_1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">150</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">est_iv</span> <span class="op">=</span> <span class="fu"><a href="https://lrberge.github.io/fixest/reference/feols.html">feols</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x1</span> <span class="op">|</span> <span class="va">x_endo_1</span> <span class="op">+</span> <span class="va">x_endo_2</span> <span class="op">~</span> <span class="va">x_inst_1</span> <span class="op">+</span> <span class="va">x_inst_2</span>, <span class="va">base</span><span class="op">)</span></span>
<span><span class="va">est_iv</span></span>
<span><span class="co">#&gt; TSLS estimation - Dep. Var.: y</span></span>
<span><span class="co">#&gt;                   Endo.    : x_endo_1, x_endo_2</span></span>
<span><span class="co">#&gt;                   Instr.   : x_inst_1, x_inst_2</span></span>
<span><span class="co">#&gt; Second stage: Dep. Var.: y</span></span>
<span><span class="co">#&gt; Observations: 150</span></span>
<span><span class="co">#&gt; Standard-errors: IID </span></span>
<span><span class="co">#&gt;              Estimate Std. Error  t value   Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  1.831380   0.411435  4.45121 1.6844e-05 ***</span></span>
<span><span class="co">#&gt; fit_x_endo_1 0.444982   0.022086 20.14744  &lt; 2.2e-16 ***</span></span>
<span><span class="co">#&gt; fit_x_endo_2 0.639916   0.307376  2.08186 3.9100e-02 *  </span></span>
<span><span class="co">#&gt; x1           0.565095   0.084715  6.67051 4.9180e-10 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; RMSE: 0.398842   Adj. R2: 0.761653</span></span>
<span><span class="co">#&gt; F-test (1st stage), x_endo_1: stat = 903.2    , p &lt; 2.2e-16 , on 2 and 146 DoF.</span></span>
<span><span class="co">#&gt; F-test (1st stage), x_endo_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.</span></span>
<span><span class="co">#&gt;                   Wu-Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.</span></span>
<span></span>
<span><span class="va">res_est_iv</span> <span class="op">&lt;-</span> <span class="va">est_iv</span><span class="op">$</span><span class="va">coeftable</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="https://tibble.tidyverse.org/reference/rownames.html">rownames_to_column</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">coef_of_interest</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">res_est_iv</span><span class="op">[</span><span class="va">res_est_iv</span><span class="op">$</span><span class="va">rowname</span> <span class="op">==</span> <span class="st">"fit_x_endo_1"</span>, <span class="st">"Estimate"</span><span class="op">]</span></span>
<span><span class="va">se_of_interest</span> <span class="op">&lt;-</span></span>
<span>    <span class="va">res_est_iv</span><span class="op">[</span><span class="va">res_est_iv</span><span class="op">$</span><span class="va">rowname</span> <span class="op">==</span> <span class="st">"fit_x_endo_1"</span>, <span class="st">"Std. Error"</span><span class="op">]</span></span>
<span><span class="va">fstat_1st</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://lrberge.github.io/fixest/reference/fitstat.html">fitstat</a></span><span class="op">(</span><span class="va">est_iv</span>, type <span class="op">=</span> <span class="st">"ivf1"</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">stat</span></span>
<span></span>
<span><span class="co"># To get the correct SE based on 1st-stage F-stat (This result is similar without adjustment since F is large)</span></span>
<span><span class="co"># the results are the new CIS and p.value</span></span>
<span><span class="fu"><a href="https://yiqingxu.org/packages/ivDiag/reference/tF.html">tF</a></span><span class="op">(</span>coef <span class="op">=</span> <span class="va">coef_of_interest</span>, se <span class="op">=</span> <span class="va">se_of_interest</span>, Fstat <span class="op">=</span> <span class="va">fstat_1st</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu">causalverse</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/causalverse/man/nice_tab.html">nice_tab</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt;          F   cF  Coef     SE       t CI2.5. CI97.5. p.value</span></span>
<span><span class="co">#&gt; 1 903.1628 1.96 0.445 0.0221 20.1474 0.4017  0.4883       0</span></span>
<span></span>
<span><span class="co"># We can try to see a different 1st-stage F-stat and how it changes the results</span></span>
<span><span class="fu"><a href="https://yiqingxu.org/packages/ivDiag/reference/tF.html">tF</a></span><span class="op">(</span>coef <span class="op">=</span> <span class="va">coef_of_interest</span>, se <span class="op">=</span> <span class="va">se_of_interest</span>, Fstat <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu">causalverse</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/causalverse/man/nice_tab.html">nice_tab</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt;   F    cF  Coef     SE       t CI2.5. CI97.5. p.value</span></span>
<span><span class="co">#&gt; 1 2 18.66 0.445 0.0221 20.1474 0.0329  0.8571  0.0343</span></span></code></pre></div>
<hr>
</div>
<div id="sec-ak-approach" class="section level3" number="34.5.5">
<h3>
<span class="header-section-number">34.5.5</span> AK Approach<a class="anchor" aria-label="anchor" href="#sec-ak-approach"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">J. Angrist and Kolesár (<a href="references.html#ref-angrist2024one">2024</a>)</span> offer a reappraisal of just-identified IV models, focusing on the finite-sample properties of conventional inference in cases where a single instrument is used for a single endogenous variable. Their findings challenge some of the more pessimistic views about weak instruments and inference distortions in microeconometric applications.</p>
<p>Rather than propose a new estimator or test, Angrist and Kolesár provide a framework and rationale supporting the validity of traditional just-ID IV inference in many practical settings. Their insights clarify when conventional t-tests and confidence intervals can be trusted, and they offer practical guidance on first-stage pretesting, bias reduction, and endogeneity considerations.</p>
<p>AK apply their framework to three canonical studies:</p>
<ol style="list-style-type: decimal">
<li>
<span class="citation">J. D. Angrist and Krueger (<a href="references.html#ref-angrist1991does">1991</a>)</span> - Education returns</li>
<li>
<span class="citation">J. D. Angrist and Evans (<a href="references.html#ref-angrist1998children">1998</a>)</span> - Family size and female labor supply</li>
<li>
<span class="citation">J. D. Angrist and Lavy (<a href="references.html#ref-angrist1999using">1999</a>)</span> - Class size effects</li>
</ol>
<p>Findings:</p>
<ul>
<li><p>Endogeneity (<span class="math inline">\(\rho\)</span>) in these studies is moderate (typically <span class="math inline">\(|\rho| &lt; 0.47\)</span>).</p></li>
<li><p>Conventional t-tests and confidence intervals work reasonably well.</p></li>
<li><p>In many micro applications, theoretical bounds on causal effects and plausible OVB scenarios limit <span class="math inline">\(\rho\)</span>, supporting the validity of conventional inference.</p></li>
</ul>
<hr>
<p>Key Contributions of the AK Approach</p>
<ul>
<li><p>Reassessing Bias and Coverage:<br>
AK demonstrate that conventional IV estimates and t-tests in just-ID IV models often perform better than theory might suggest—provided the degree of endogeneity (<span class="math inline">\(\rho\)</span>) is moderate, and the first-stage F-statistic is not extremely weak.</p></li>
<li>
<p>First-Stage Sign Screening:</p>
<ul>
<li>They propose sign screening as a simple, costless strategy to halve the median bias of IV estimators.</li>
<li>Screening on the sign of the estimated first-stage coefficient (i.e., using only samples where the first-stage estimate has the correct sign) improves the finite-sample performance of just-ID IV estimates without degrading confidence interval coverage.</li>
</ul>
</li>
<li>
<p>Bias-Minimizing Screening Rule:</p>
<ul>
<li>AK show that setting the first-stage t-statistic threshold <span class="math inline">\(c = 0\)</span>, i.e., requiring only the correct sign of the first-stage estimate, minimizes median bias while preserving conventional coverage properties.</li>
</ul>
</li>
<li>
<p>Practical Implication:</p>
<ul>
<li>They argue that conventional just-ID IV inference, including t-tests and confidence intervals, is likely valid in most microeconometric applications, especially where theory or institutional knowledge suggests the direction of the first-stage relationship.</li>
</ul>
</li>
</ul>
<hr>
<div id="model-setup-and-notation" class="section level4" number="34.5.5.1">
<h4>
<span class="header-section-number">34.5.5.1</span> Model Setup and Notation<a class="anchor" aria-label="anchor" href="#model-setup-and-notation"><i class="fas fa-link"></i></a>
</h4>
<p>AK adopt a reduced-form and first-stage specification for just-ID IV models:</p>
<p><span class="math display">\[
Y_i = Z_i \delta + X_i' \psi_1 + u_i \\
D_i = Z_i \pi + X_i' \psi_2 + v_i
\]</span></p>
<ul>
<li>
<span class="math inline">\(Y_i\)</span>: Outcome variable</li>
<li>
<span class="math inline">\(D_i\)</span>: Endogenous treatment variable</li>
<li>
<span class="math inline">\(Z_i\)</span>: Instrumental variable (single instrument)</li>
<li>
<span class="math inline">\(X_i\)</span>: Control variables</li>
<li>
<span class="math inline">\(u_i, v_i\)</span>: Error terms</li>
</ul>
<p>Parameter of Interest:</p>
<p><span class="math display">\[
\beta = \frac{\delta}{\pi}
\]</span></p>
<hr>
</div>
<div id="endogeneity-and-instrument-strength" class="section level4" number="34.5.5.2">
<h4>
<span class="header-section-number">34.5.5.2</span> Endogeneity and Instrument Strength<a class="anchor" aria-label="anchor" href="#endogeneity-and-instrument-strength"><i class="fas fa-link"></i></a>
</h4>
<p>AK characterize the two key parameters governing finite-sample inference:</p>
<ul>
<li><p>Instrument Strength:<br><span class="math display">\[ E[F] = \frac{\pi^2}{\sigma^2_{\hat{\pi}}} + 1 \]</span><br>
(Expected value of the first-stage F-statistic.)</p></li>
<li><p>Endogeneity:<br><span class="math display">\[ \rho = \text{cor}(\hat{\delta} - \hat{\pi} \beta, \hat{\pi}) \]</span><br>
Measures the degree of correlation between reduced-form and first-stage residuals (or between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> under homoskedasticity).</p></li>
</ul>
<p>Key Insight:</p>
<p>For <span class="math inline">\(\rho &lt; 0.76\)</span>, the coverage of conventional 95% confidence intervals is distorted by less than 5%, regardless of the first-stage F-statistic.</p>
<hr>
</div>
<div id="first-stage-sign-screening" class="section level4" number="34.5.5.3">
<h4>
<span class="header-section-number">34.5.5.3</span> First-Stage Sign Screening<a class="anchor" aria-label="anchor" href="#first-stage-sign-screening"><i class="fas fa-link"></i></a>
</h4>
<p>AK argue that pre-screening based on the sign of the first-stage estimate (<span class="math inline">\(\hat{\pi}\)</span>) offers bias reduction without compromising confidence interval coverage.</p>
<p>Screening Rule:</p>
<ul>
<li>Screen if <span class="math inline">\(\hat{\pi} &gt; 0\)</span><br>
(or <span class="math inline">\(\hat{\pi} &lt; 0\)</span> if the theoretical sign is negative).</li>
</ul>
<p>Results:</p>
<ul>
<li>Halves median bias of the IV estimator.</li>
<li>No degradation of confidence interval coverage.</li>
</ul>
<p>This screening approach:</p>
<ul>
<li><p>Avoids the pitfalls of pre-testing based on first-stage F-statistics (which can exacerbate bias and distort inference).</p></li>
<li><p>Provides a “free lunch”: bias reduction with no coverage cost.</p></li>
</ul>
<hr>
</div>
<div id="rejection-rates-and-confidence-interval-coverage" class="section level4" number="34.5.5.4">
<h4>
<span class="header-section-number">34.5.5.4</span> Rejection Rates and Confidence Interval Coverage<a class="anchor" aria-label="anchor" href="#rejection-rates-and-confidence-interval-coverage"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>Rejection rates of conventional t-tests stay close to the nominal level (5%) if <span class="math inline">\(|\rho| &lt; 0.76\)</span>, independent of instrument strength.</li>
<li>For <span class="math inline">\(|\rho| &lt; 0.565\)</span>, conventional t-tests exhibit no over-rejection, aligning with findings from <span class="citation">Lee et al. (<a href="references.html#ref-lee2022valid">2022</a>)</span>.</li>
</ul>
<p>Comparison with AR and tF Procedures:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="18%">
<col width="26%">
<col width="13%">
<col width="41%">
</colgroup>
<thead><tr class="header">
<th>Approach</th>
<th>Bias Reduction</th>
<th>Coverage</th>
<th>CI Length (F &gt; 3.84)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>AK Sign Screening</td>
<td>Halves median bias</td>
<td>Near-nominal</td>
<td>Finite</td>
</tr>
<tr class="even">
<td>AR Test</td>
<td>No bias (inversion method)</td>
<td>Exact</td>
<td>Infinite</td>
</tr>
<tr class="odd">
<td>tF Procedure</td>
<td>Bias adjusted</td>
<td>Near-nominal</td>
<td>Longer than AK (especially for moderate F)</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
</div>
<div id="testing-assumptions" class="section level2" number="34.6">
<h2>
<span class="header-section-number">34.6</span> Testing Assumptions<a class="anchor" aria-label="anchor" href="#testing-assumptions"><i class="fas fa-link"></i></a>
</h2>
<p>We are interested in estimating the causal effect of an endogenous regressor <span class="math inline">\(X_2\)</span> on an outcome variable <span class="math inline">\(Y\)</span>, using instrumental variables <span class="math inline">\(Z\)</span> to address endogeneity.</p>
<p>The structural model of interest is:</p>
<p><span class="math display">\[
Y = \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]</span></p>
<ul>
<li>
<span class="math inline">\(X_1\)</span>: Exogenous regressors</li>
<li>
<span class="math inline">\(X_2\)</span>: Endogenous regressor(s)</li>
<li>
<span class="math inline">\(Z\)</span>: Instrumental variables</li>
</ul>
<p>If <span class="math inline">\(Z\)</span> satisfies the relevance and exogeneity assumptions, we can identify <span class="math inline">\(\beta_2\)</span> as:</p>
<p><span class="math display">\[
\beta_2 = \frac{Cov(Z, Y)}{Cov(Z, X_2)}
\]</span></p>
<p>Alternatively, in terms of reduced form and first stage estimates:</p>
<ul>
<li>Reduced Form (effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y\)</span>):</li>
</ul>
<p><span class="math display">\[
\rho = \frac{Cov(Y, Z)}{Var(Z)}
\]</span></p>
<ul>
<li>First Stage (effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(X_2\)</span>):</li>
</ul>
<p><span class="math display">\[
\pi = \frac{Cov(X_2, Z)}{Var(Z)}
\]</span></p>
<ul>
<li>IV Estimate:</li>
</ul>
<p><span class="math display">\[
\beta_2 = \frac{Cov(Y,Z)}{Cov(X_2, Z)} = \frac{\rho}{\pi}
\]</span></p>
<p>To interpret <span class="math inline">\(\beta_2\)</span> as the causal effect of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(Y\)</span>, the following assumptions must hold:</p>
<div id="sec-relevance-assumption" class="section level3" number="34.6.1">
<h3>
<span class="header-section-number">34.6.1</span> Relevance Assumption<a class="anchor" aria-label="anchor" href="#sec-relevance-assumption"><i class="fas fa-link"></i></a>
</h3>
<p>In IV estimation, instrument relevance ensures that the instrument(s) <span class="math inline">\(Z\)</span> can explain sufficient variation in the endogenous regressor(s) <span class="math inline">\(X_2\)</span> to identify the structural equation:</p>
<p><span class="math display">\[
Y = \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]</span></p>
<p>The relevance condition requires that the instrument(s) <span class="math inline">\(Z\)</span> be correlated with the endogenous variable(s) <span class="math inline">\(X_2\)</span>, conditional on other covariates <span class="math inline">\(X_1\)</span>. Formally:</p>
<p><span class="math display">\[
Cov(Z, X_2) \ne 0
\]</span></p>
<p>Or, in matrix notation for multiple instruments and regressors, the matrix of correlations (or more generally, the projection matrix) between <span class="math inline">\(Z\)</span> and <span class="math inline">\(X_2\)</span> must be of full column rank. This guarantees that <span class="math inline">\(Z\)</span> has non-trivial explanatory power for <span class="math inline">\(X_2\)</span>.</p>
<p>An equivalent condition in terms of population moment conditions is that:</p>
<p><span class="math display">\[
E[Z' (X_2 - E[X_2 | Z])] \ne 0
\]</span></p>
<p>This condition ensures the identification of <span class="math inline">\(\beta_2\)</span>. Without it, the IV estimator would be undefined due to division by zero in its ratio form:</p>
<p><span class="math display">\[
\hat{\beta}_2^{IV} = \frac{Cov(Z, Y)}{Cov(Z, X_2)}
\]</span></p>
<p>The first-stage regression operationalizes the relevance assumption:</p>
<p><span class="math display">\[
X_2 = Z \pi + X_1 \gamma + u
\]</span></p>
<ul>
<li>
<span class="math inline">\(\pi\)</span>: Vector of first-stage coefficients, measuring the effect of instruments on the endogenous regressor(s).</li>
<li>
<span class="math inline">\(u\)</span>: First-stage residual.</li>
</ul>
<p>Identification of <span class="math inline">\(\beta_2\)</span> requires that <span class="math inline">\(\pi \ne 0\)</span>. If <span class="math inline">\(\pi = 0\)</span>, the instrument has no explanatory power for <span class="math inline">\(X_2\)</span>, and the IV procedure collapses.</p>
<div id="weak-instruments" class="section level4" number="34.6.1.1">
<h4>
<span class="header-section-number">34.6.1.1</span> Weak Instruments<a class="anchor" aria-label="anchor" href="#weak-instruments"><i class="fas fa-link"></i></a>
</h4>
<p>Even when <span class="math inline">\(Cov(Z, X_2) \ne 0\)</span>, weak instruments pose a serious problem in finite samples:</p>
<ul>
<li>Bias: The IV estimator becomes biased in the direction of the OLS estimator.</li>
<li>Size distortion: Hypothesis tests can have inflated Type I error rates.</li>
<li>Variance: Estimates become highly variable and unreliable.</li>
</ul>
<p>Asymptotic vs. Finite Sample Problems</p>
<ul>
<li>IV estimators are consistent as <span class="math inline">\(n \to \infty\)</span> if the relevance condition holds.</li>
<li>With weak instruments, convergence can be so slow that finite-sample behavior is practically indistinguishable from inconsistency.</li>
</ul>
<p>Boundaries between relevance and strength are thus critical in applied work.</p>
<hr>
</div>
<div id="sec-first-stage-f-statistic" class="section level4" number="34.6.1.2">
<h4>
<span class="header-section-number">34.6.1.2</span> First-Stage F-Statistic<a class="anchor" aria-label="anchor" href="#sec-first-stage-f-statistic"><i class="fas fa-link"></i></a>
</h4>
<p>In a single endogenous regressor case, the first-stage F-statistic is the standard test for instrument strength.</p>
<p>First-Stage Regression:</p>
<p><span class="math display">\[
X_2 = Z \pi + X_1 \gamma + u
\]</span></p>
<p>We test:</p>
<p><span class="math display">\[
\begin{aligned}
H_0&amp;: \pi = 0 \quad \text{(Instruments have no explanatory power)}  \\
H_1&amp;: \pi \ne 0 \quad \text{(Instruments explain variation in $X_2$)}
\end{aligned}
\]</span></p>
<p>F-Statistic Formula:</p>
<p><span class="math display">\[
F = \frac{(SSR_r - SSR_{ur}) / q}{SSR_{ur} / (n - k - 1)}
\]</span></p>
<ul>
<li>
<span class="math inline">\(SSR_r\)</span>: Sum of squared residuals from the restricted model (no instruments).</li>
<li>
<span class="math inline">\(SSR_{ur}\)</span>: Sum of squared residuals from the unrestricted model (with instruments).</li>
<li>
<span class="math inline">\(q\)</span>: Number of excluded instruments (restrictions tested).</li>
<li>
<span class="math inline">\(n\)</span>: Number of observations.</li>
<li>
<span class="math inline">\(k\)</span>: Number of control variables.</li>
</ul>
<p>Interpretation:</p>
<ul>
<li>A rule of thumb <span class="citation">(<a href="references.html#ref-staiger1997instrumental">Staiger and Stock 1997</a>)</span>: If <span class="math inline">\(F &lt; 10\)</span>, instruments are weak.</li>
<li>However, <span class="citation">Lee et al. (<a href="references.html#ref-lee2022valid">2022</a>)</span> criticizes this threshold, advocating for model-specific diagnostics.</li>
<li>
<span class="citation">M. J. Moreira (<a href="references.html#ref-moreira2003conditional">2003</a>)</span> proposes the Conditional Likelihood Ratio test for inference under weak instruments <span class="citation">(<a href="references.html#ref-andrews2008efficient">D. W. Andrews, Moreira, and Stock 2008</a>)</span>.</li>
</ul>
<p>Use <code><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis()</a></code> in R to test instrument relevance.</p>
</div>
<div id="sec-cragg-donald-test" class="section level4" number="34.6.1.3">
<h4>
<span class="header-section-number">34.6.1.3</span> Cragg-Donald Test<a class="anchor" aria-label="anchor" href="#sec-cragg-donald-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Cragg-Donald statistic is essentially the same as the Wald statistic of the joint significance of the instruments in the first stage <span class="citation">(<a href="references.html#ref-cragg1993testing">Cragg and Donald 1993</a>)</span>, and it’s used specifically when you have multiple endogenous regressors. It’s calculated as:</p>
<p><span class="math display">\[
CD = n \times (R_{ur}^2 - R_r^2)
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(R_{ur}^2\)</span> and <span class="math inline">\(R_r^2\)</span> are the R-squared values from the unrestricted and restricted models respectively.</p></li>
<li><p><span class="math inline">\(n\)</span> is the number of observations.</p></li>
</ul>
<p>For one endogenous variable, the Cragg-Donald test results should align closely with those from Stock and Yogo. The Anderson canonical correlation test, a likelihood ratio test, also works under similar conditions, contrasting with Cragg-Donald’s Wald statistic approach. Both are valid with one endogenous variable and at least one instrument.</p>
<div class="sourceCode" id="cb911"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">cragg</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span> <span class="co"># for dataaset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"WeakInstrument"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/cragg/man/cragg_donald.html">cragg_donald</a></span><span class="op">(</span></span>
<span>    <span class="co"># control variables</span></span>
<span>    X <span class="op">=</span> <span class="op">~</span> <span class="fl">1</span>, </span>
<span>    <span class="co"># endogeneous variables</span></span>
<span>    D <span class="op">=</span> <span class="op">~</span> <span class="va">x</span>, </span>
<span>    <span class="co"># instrument variables </span></span>
<span>    Z <span class="op">=</span> <span class="op">~</span> <span class="va">z</span>, </span>
<span>    data <span class="op">=</span> <span class="va">WeakInstrument</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Cragg-Donald test for weak instruments:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Data:                        WeakInstrument </span></span>
<span><span class="co">#&gt;      Controls:                    ~1 </span></span>
<span><span class="co">#&gt;      Treatments:                  ~x </span></span>
<span><span class="co">#&gt;      Instruments:                 ~z </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Cragg-Donald Statistic:        4.566136 </span></span>
<span><span class="co">#&gt;      Df:                                 198</span></span></code></pre></div>
<p>Large CD statistic implies that the instruments are strong, but not in our case here. But to judge it against some critical value, we have to look at <a href="sec-instrumental-variables.html#stock-yogo">Stock-Yogo</a></p>
</div>
<div id="stock-yogo" class="section level4" number="34.6.1.4">
<h4>
<span class="header-section-number">34.6.1.4</span> Stock-Yogo<a class="anchor" aria-label="anchor" href="#stock-yogo"><i class="fas fa-link"></i></a>
</h4>
<p>The Stock-Yogo test does not directly compute a statistic like the F-test or Cragg-Donald, but rather uses pre-computed critical values to assess the strength of instruments. It often uses the eigenvalues derived from the concentration matrix:</p>
<p><span class="math display">\[
S = \frac{1}{n} (Z' X) (X'Z)
\]</span></p>
<p>where <span class="math inline">\(Z\)</span> is the matrix of instruments and <span class="math inline">\(X\)</span> is the matrix of endogenous regressors.</p>
<p>Stock and Yogo provide critical values for different scenarios (bias, size distortion) for a given number of instruments and endogenous regressors, based on the smallest eigenvalue of <span class="math inline">\(S\)</span>. The test compares these eigenvalues against critical values that correspond to thresholds of permissible bias or size distortion in a 2SLS estimator.</p>
<ul>
<li>Critical Values and Test Conditions: The critical values derived by Stock and Yogo depend on the level of acceptable bias, the number of endogenous regressors, and the number of instruments. For example, with a 5% maximum acceptable bias, one endogenous variable, and three instruments, the critical value for a sufficient first stage F-statistic is 13.91. Note that this framework requires at least two overidentifying degree of freedom. <span class="citation">Stock and Yogo (<a href="references.html#ref-stock2002testing">2002</a>)</span> set the critical values such that the bias is less then 10% (default)</li>
</ul>
<p><span class="math inline">\(H_0:\)</span> Instruments are weak</p>
<p><span class="math inline">\(H_1:\)</span> Instruments are not weak</p>
<div class="sourceCode" id="cb912"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">cragg</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span> <span class="co"># for dataaset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"WeakInstrument"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/cragg/man/stock_yogo_test.html">stock_yogo_test</a></span><span class="op">(</span></span>
<span>    <span class="co"># control variables</span></span>
<span>    X <span class="op">=</span>  <span class="op">~</span> <span class="va">Sepal.Length</span>,</span>
<span>    <span class="co"># endogeneous variables</span></span>
<span>    D <span class="op">=</span>  <span class="op">~</span> <span class="va">Sepal.Width</span>,</span>
<span>    <span class="co"># instrument variables</span></span>
<span>    Z <span class="op">=</span>  <span class="op">~</span> <span class="va">Petal.Length</span> <span class="op">+</span> <span class="va">Petal.Width</span> <span class="op">+</span> <span class="va">Species</span>,</span>
<span>    size_bias <span class="op">=</span> <span class="st">"bias"</span>,</span>
<span>    data <span class="op">=</span> <span class="va">iris</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Results of Stock and Yogo test for weak instruments:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Null Hypothesis:             Instruments are weak </span></span>
<span><span class="co">#&gt;      Alternative Hypothesis:      Instruments are not weak </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Data:                        iris       </span></span>
<span><span class="co">#&gt;      Controls:                    ~Sepal.Length </span></span>
<span><span class="co">#&gt;      Treatments:                  ~Sepal.Width </span></span>
<span><span class="co">#&gt;      Instruments:                 ~Petal.Length + Petal.Width + Species </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Alpha:                             0.05 </span></span>
<span><span class="co">#&gt;      Acceptable level of bias:    5% relative to OLS.</span></span>
<span><span class="co">#&gt;      Critical Value:                   16.85 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Cragg-Donald Statistic:        61.30973 </span></span>
<span><span class="co">#&gt;      Df:                                 144</span></span></code></pre></div>
</div>
<div id="sec-anderson-rubin-test" class="section level4" number="34.6.1.5">
<h4>
<span class="header-section-number">34.6.1.5</span> Anderson-Rubin Test<a class="anchor" aria-label="anchor" href="#sec-anderson-rubin-test"><i class="fas fa-link"></i></a>
</h4>
<p>The Anderson-Rubin (AR) test addresses the issues of weak instruments by providing a test for the structural parameter (<span class="math inline">\(\beta\)</span>) that is robust to weak instruments <span class="citation">(<a href="references.html#ref-anderson1949estimation">T. W. Anderson and Rubin 1949</a>)</span>. It does not rely on the strength of the instruments to control size, making it a valuable tool for inference when instrument relevance is questionable.</p>
<p>Consider the following linear IV model:</p>
<p><span class="math display">\[
Y = X \beta + u
\]</span></p>
<ul>
<li>
<span class="math inline">\(Y\)</span>: Dependent variable (<span class="math inline">\(n \times 1\)</span>)</li>
<li>
<span class="math inline">\(X\)</span>: Endogenous regressor (<span class="math inline">\(n \times k\)</span>)</li>
<li>
<span class="math inline">\(Z\)</span>: Instrument matrix (<span class="math inline">\(n \times m\)</span>), assumed to satisfy:
<ul>
<li>Instrument Exogeneity: <span class="math inline">\(\mathbb{E}[Z'u] = 0\)</span>
</li>
<li>Instrument Relevance: <span class="math inline">\(\mathrm{rank}(\mathbb{E}[Z'X]) = k\)</span>
</li>
</ul>
</li>
</ul>
<p>The relevance assumption ensures that <span class="math inline">\(Z\)</span> contains valid information for predicting <span class="math inline">\(X\)</span>.</p>
<p>Relevance is typically assessed in the first-stage regression:</p>
<p><span class="math display">\[
X = Z \Pi + V
\]</span></p>
<p>If <span class="math inline">\(Z\)</span> is weakly correlated with <span class="math inline">\(X\)</span>, <span class="math inline">\(\Pi\)</span> is close to zero, violating the relevance assumption.</p>
<p>The AR test is a Wald-type test for the null hypothesis:</p>
<p><span class="math display">\[
H_0: \beta = \beta_0
\]</span></p>
<p>It is constructed by examining whether the residuals from imposing <span class="math inline">\(\beta_0\)</span> are orthogonal to the instruments <span class="math inline">\(Z\)</span>. Specifically:</p>
<ol style="list-style-type: decimal">
<li>Compute the reduced-form residuals under <span class="math inline">\(H_0\)</span>:</li>
</ol>
<p><span class="math display">\[
r(\beta_0) = Y - X \beta_0
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The AR test statistic is:</li>
</ol>
<p><span class="math display">\[
AR(\beta_0) = \frac{r(\beta_0)' P_Z r(\beta_0)}{\hat{\sigma}^2}
\]</span></p>
<ul>
<li>
<span class="math inline">\(P_Z = Z (Z'Z)^{-1} Z'\)</span> is the projection matrix onto the column space of <span class="math inline">\(Z\)</span>.</li>
<li>
<span class="math inline">\(\hat{\sigma}^2 = \frac{r(\beta_0)' M_Z r(\beta_0)}{n - m}\)</span> is the residual variance estimator, with <span class="math inline">\(M_Z = I - P_Z\)</span>.</li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>, and assuming homoskedasticity, the AR statistic follows an F-distribution:</p>
<p><span class="math display">\[
AR(\beta_0) \sim F(m, n - m)
\]</span></p>
<p>Alternatively, for large <span class="math inline">\(n\)</span>, the AR statistic can be approximated by a chi-squared distribution:</p>
<p><span class="math display">\[
AR(\beta_0) \sim \chi^2_m
\]</span></p>
<p>Interpretation</p>
<ul>
<li>If <span class="math inline">\(AR(\beta_0)\)</span> exceeds the critical value from the <span class="math inline">\(F\)</span> (or <span class="math inline">\(\chi^2\)</span>) distribution, we reject <span class="math inline">\(H_0\)</span>.</li>
<li>The test assesses whether <span class="math inline">\(r(\beta_0)\)</span> is orthogonal to <span class="math inline">\(Z\)</span>. If not, <span class="math inline">\(H_0\)</span> is inconsistent with the moment conditions.</li>
</ul>
<p>The key advantage of the AR test is that its size is correct even when instruments are weak. The AR statistic does not depend on the strength of the instruments (i.e., the magnitude of <span class="math inline">\(\Pi\)</span>), making it valid under weak identification.</p>
<p>This contrasts with standard 2SLS-based Wald tests, whose distribution depends on the first-stage relevance and can be severely distorted if <span class="math inline">\(Z\)</span> is weakly correlated with <span class="math inline">\(X\)</span>.</p>
<ul>
<li>The relevance assumption is necessary for point identification and consistent estimation in IV.</li>
<li>If instruments are weak, point estimates of <span class="math inline">\(\beta\)</span> from 2SLS can be biased.</li>
<li>The AR test allows for valid hypothesis testing, even if instrument relevance is weak.</li>
</ul>
<p>However, if instruments are completely irrelevant (i.e., <span class="math inline">\(Z'X = 0\)</span>), the IV model is unidentified, and the AR test lacks power (i.e., it does not reject <span class="math inline">\(H_0\)</span> for any <span class="math inline">\(\beta_0\)</span>).</p>
<p>The AR test can be inverted to form confidence sets for <span class="math inline">\(\beta\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Compute <span class="math inline">\(AR(\beta)\)</span> for a grid of <span class="math inline">\(\beta\)</span> values.</li>
<li>Include all <span class="math inline">\(\beta\)</span> values for which <span class="math inline">\(AR(\beta)\)</span> does not reject <span class="math inline">\(H_0\)</span> at the chosen significance level.</li>
</ol>
<p>These confidence sets are robust to weak instruments and can be disconnected or unbounded if identification is weak.</p>
<div class="sourceCode" id="cb913"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate data</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="co"># Instrument (include constant)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="co"># Weak first-stage relationship</span></span>
<span><span class="va">beta_true</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op">*</span> <span class="va">beta_true</span> <span class="op">+</span> <span class="va">u</span></span>
<span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">ivmodel</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/ivmodel/man/ivmodel.html">ivmodel</a></span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Y</span>, D <span class="op">=</span> <span class="va">X</span>, Z <span class="op">=</span> <span class="va">Z</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivmodel(Y = Y, D = X, Z = Z)</span></span>
<span><span class="co">#&gt; sample size: 500</span></span>
<span><span class="co">#&gt; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; First Stage Regression Result:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; F=0.684842, df1=2, df2=497, p-value is 0.50464</span></span>
<span><span class="co">#&gt; R-squared=0.002748329,   Adjusted R-squared=-0.001264756</span></span>
<span><span class="co">#&gt; Residual standard error: 1.01117 on 499 degrees of freedom</span></span>
<span><span class="co">#&gt; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sargan Test Result:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sargan Test Statistics=0.007046677, df=1, p-value is 0.9331</span></span>
<span><span class="co">#&gt; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients of k-Class Estimators:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;              k Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; OLS    0.00000  1.05559    0.04396  24.015   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Fuller 0.99800  1.81913    0.80900   2.249   0.0250 *  </span></span>
<span><span class="co">#&gt; TSLS   1.00000  2.37533    1.40555   1.690   0.0917 .  </span></span>
<span><span class="co">#&gt; LIML   1.00001  2.38211    1.41382   1.685   0.0926 .  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Alternative tests for the treatment effect under H_0: beta=0.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Anderson-Rubin test (under F distribution):</span></span>
<span><span class="co">#&gt; F=1.874305, df1=2, df2=497, p-value is 0.15454</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  Whole Real Line</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Conditional Likelihood Ratio test (under Normal approximation):</span></span>
<span><span class="co">#&gt; Test Stat=3.741629, p-value is 0.14881</span></span>
<span><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span><span class="co">#&gt;  Whole Real Line</span></span></code></pre></div>
</div>
<div id="sec-stock-wright-test" class="section level4" number="34.6.1.6">
<h4>
<span class="header-section-number">34.6.1.6</span> Stock-Wright Test<a class="anchor" aria-label="anchor" href="#sec-stock-wright-test"><i class="fas fa-link"></i></a>
</h4>
<p>While the <a href="sec-instrumental-variables.html#sec-anderson-rubin-test">Anderson-Rubin test</a> offers one solution by constructing test statistics that are valid regardless of instrument strength, another complementary approach is the Stock-Wright test, sometimes referred to as the S-test or Score test. This test belongs to a broader class of conditional likelihood ratio tests proposed by <span class="citation">M. J. Moreira (<a href="references.html#ref-moreira2003conditional">2003</a>)</span> and <span class="citation">Stock and Wright (<a href="references.html#ref-stock2000gmm">2000</a>)</span>, and it plays an important role in constructing weak-instrument robust confidence regions.</p>
<p>The Stock-Wright test exploits the conditional score function of the IV model to test hypotheses about structural parameters, offering robustness under weak identification.</p>
<p>Consider the linear IV model:</p>
<p><span class="math display">\[
Y = X \beta + u
\]</span></p>
<ul>
<li>
<span class="math inline">\(Y\)</span>: Outcome variable (<span class="math inline">\(n \times 1\)</span>)</li>
<li>
<span class="math inline">\(X\)</span>: Endogenous regressor (<span class="math inline">\(n \times k\)</span>)</li>
<li>
<span class="math inline">\(Z\)</span>: Instrument matrix (<span class="math inline">\(n \times m\)</span>), where <span class="math inline">\(m \ge k\)</span>
</li>
</ul>
<p>The exogeneity and relevance assumptions for <span class="math inline">\(Z\)</span> are:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\mathbb{E}[Z'u] = 0\)</span> (Exogeneity)</li>
<li>
<span class="math inline">\(\mathrm{rank}(\mathbb{E}[Z'X]) = k\)</span> (Relevance)</li>
</ol>
<p>Weak instruments imply that the matrix <span class="math inline">\(\mathbb{E}[Z'X]\)</span> is close to rank-deficient or nearly zero, which can invalidate standard inference.</p>
<p>The goal is to test the null hypothesis:</p>
<p><span class="math display">\[
H_0: \beta = \beta_0
\]</span></p>
<p>The Stock-Wright test provides a robust way to perform this hypothesis test by constructing a score statistic that is robust to weak instruments.</p>
<p>The score test (or Lagrange Multiplier test) evaluates whether the score (the gradient of the log-likelihood function with respect to <span class="math inline">\(\beta\)</span>) is close to zero under the null hypothesis.</p>
<p>In the IV context, the conditional score is evaluated from the reduced-form equations. The SW test uses the fact that under <span class="math inline">\(H_0\)</span>, the moment condition:</p>
<p><span class="math display">\[
\mathbb{E}[Z'(Y - X \beta_0)] = 0
\]</span></p>
<p>should hold. Deviations from this condition can be tested using a score statistic.</p>
<p>The reduced-form equations are:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;= Z \pi_Y + \epsilon_Y \\
X &amp;= Z \pi_X + \epsilon_X
\end{aligned}
\]</span></p>
<p>Under this system:</p>
<ul>
<li><p><span class="math inline">\(\epsilon_Y\)</span> and <span class="math inline">\(\epsilon_X\)</span> are jointly normally distributed with covariance matrix <span class="math inline">\(\Sigma\)</span>.</p></li>
<li><p>The structural model implies a restriction on <span class="math inline">\(\pi_Y\)</span>: <span class="math inline">\(\pi_Y = \pi_X \beta\)</span>.</p></li>
</ul>
<p>The Stock-Wright test statistic is given by:</p>
<p><span class="math display">\[
S(\beta_0) = (Z'(Y - X \beta_0))' \left[ \hat{\Omega}^{-1} \right] (Z'(Y - X \beta_0))
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\hat{\Omega}\)</span> is an estimator of the covariance matrix of the moment condition <span class="math inline">\(Z'u\)</span>, often estimated by <span class="math inline">\(Z'Z\)</span> times an estimator of <span class="math inline">\(\mathrm{Var}(u)\)</span>.</p></li>
<li><p>In homoskedastic settings, <span class="math inline">\(\hat{\Omega} = \hat{\sigma}^2 Z'Z\)</span>, with <span class="math inline">\(\hat{\sigma}^2\)</span> estimated from the residuals under <span class="math inline">\(H_0\)</span>:</p></li>
</ul>
<p><span class="math display">\[
\hat{\sigma}^2 = \frac{(Y - X \beta_0)' (Y - X \beta_0)}{n}
\]</span></p>
<p>Under the null hypothesis <span class="math inline">\(H_0\)</span>, the test statistic <span class="math inline">\(S(\beta_0)\)</span> follows a chi-squared distribution with <span class="math inline">\(m\)</span> degrees of freedom:</p>
<p><span class="math display">\[
S(\beta_0) \sim \chi^2_m
\]</span></p>
<p>The Stock-Wright test is closely related to the <a href="sec-instrumental-variables.html#sec-anderson-rubin-test">Anderson-Rubin test</a>. However:</p>
<ul>
<li>The AR test focuses on the orthogonality of the reduced-form residuals with <span class="math inline">\(Z\)</span>.</li>
<li>The SW test focuses on the conditional score, derived from the likelihood framework.</li>
<li>Both are robust to weak instruments, but they have different power properties depending on the data-generating process.</li>
</ul>
<p>Geometric Intuition</p>
<ul>
<li>The SW test can be seen as testing whether the orthogonality condition between <span class="math inline">\(Z\)</span> and <span class="math inline">\(u\)</span> holds, using a score function.</li>
<li>It effectively checks whether the directional derivative of the likelihood (evaluated at <span class="math inline">\(\beta_0\)</span>) is zero, offering a generalized method of moments interpretation.</li>
</ul>
<p>The Stock-Wright S-test can be inverted to form confidence regions:</p>
<ol style="list-style-type: decimal">
<li>For a grid of <span class="math inline">\(\beta\)</span> values, compute <span class="math inline">\(S(\beta)\)</span>.</li>
<li>The confidence region consists of all <span class="math inline">\(\beta\)</span> values where <span class="math inline">\(S(\beta)\)</span> does not reject <span class="math inline">\(H_0\)</span> at the chosen significance level.</li>
</ol>
<p>These regions are robust to weak instruments, and can be disconnected or unbounded if the instruments are too weak to deliver informative inference.</p>
<div class="sourceCode" id="cb914"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate data</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="co"># Instrument (include constant)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="co"># Weak first-stage relationship</span></span>
<span><span class="va">beta_true</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op">*</span> <span class="va">beta_true</span> <span class="op">+</span> <span class="va">u</span></span>
<span></span>
<span><span class="co"># Null hypothesis to test</span></span>
<span><span class="va">beta_0</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span></span>
<span><span class="co"># Residuals under H0</span></span>
<span><span class="va">r_beta0</span> <span class="op">&lt;-</span> <span class="va">Y</span> <span class="op">-</span> <span class="va">X</span> <span class="op">*</span> <span class="va">beta_0</span></span>
<span></span>
<span><span class="co"># Estimate variance of residuals under H0</span></span>
<span><span class="va">sigma2_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/mean.html">mean</a></span><span class="op">(</span><span class="va">r_beta0</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Omega matrix</span></span>
<span><span class="va">Omega_hat</span> <span class="op">&lt;-</span> <span class="va">sigma2_hat</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Z</span></span>
<span></span>
<span><span class="co"># Compute the Stock-Wright S-statistic</span></span>
<span><span class="va">S_stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">r_beta0</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">Omega_hat</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/t.html">t</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">r_beta0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># p-value from chi-squared distribution</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/nrow.html">ncol</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span> <span class="co"># degrees of freedom</span></span>
<span><span class="va">p_val</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="va">S_stat</span>, df <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Output</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Stock-Wright S-Statistic:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">S_stat</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Stock-Wright S-Statistic: 5.0957</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p-value:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_val</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p-value: 0.0783</span></span></code></pre></div>
</div>
<div id="sec-kleibergen-paap-rk-statistic" class="section level4" number="34.6.1.7">
<h4>
<span class="header-section-number">34.6.1.7</span> Kleibergen-Paap rk Statistic<a class="anchor" aria-label="anchor" href="#sec-kleibergen-paap-rk-statistic"><i class="fas fa-link"></i></a>
</h4>
<p>Traditional diagnostics for instrument relevance, such as:</p>
<ul>
<li><p>The <a href="sec-instrumental-variables.html#sec-first-stage-f-statistic">first-stage</a> <span class="math inline">\(F\)</span>-statistic (for single endogenous variables with homoskedastic errors)</p></li>
<li><p>The <a href="sec-instrumental-variables.html#sec-cragg-donald-test">Cragg-Donald statistic</a> (for multiple endogenous regressors under homoskedasticity)</p></li>
</ul>
<p>are not valid when errors exhibit heteroskedasticity or non-i.i.d. behavior.</p>
<p>The Kleibergen-Paap (KP) rk statistic addresses these limitations by providing a robust test for underidentification and weak identification in IV models, even in the presence of heteroskedasticity.</p>
<p>Consider the linear IV model:</p>
<p><span class="math display">\[
Y = X \beta + u
\]</span></p>
<ul>
<li>
<span class="math inline">\(Y\)</span>: Dependent variable (<span class="math inline">\(n \times 1\)</span>)</li>
<li>
<span class="math inline">\(X\)</span>: Matrix of endogenous regressors (<span class="math inline">\(n \times k\)</span>)</li>
<li>
<span class="math inline">\(Z\)</span>: Instrument matrix (<span class="math inline">\(n \times m\)</span>), with <span class="math inline">\(m \ge k\)</span>
</li>
<li>
<span class="math inline">\(u\)</span>: Structural error term</li>
</ul>
<p>The moment conditions under exogeneity are:</p>
<p><span class="math display">\[
\mathbb{E}[Z'u] = 0
\]</span></p>
<p>The relevance assumption requires:</p>
<p><span class="math display">\[
\mathrm{rank}(\mathbb{E}[Z'X]) = k
\]</span></p>
<p>If this condition fails, the model is underidentified, and consistent estimation of <span class="math inline">\(\beta\)</span> is impossible.</p>
<p>The Kleibergen-Paap rk statistic performs two key functions:</p>
<ol style="list-style-type: decimal">
<li>Test for underidentification (whether the instruments identify the equation)</li>
<li>Weak identification diagnostics, analogous to the <a href="sec-instrumental-variables.html#sec-cragg-donald-test">Cragg-Donald statistic</a>, but robust to heteroskedasticity.</li>
</ol>
<p>Why “rk”?</p>
<ul>
<li>“rk” stands for rank—the statistic tests whether the matrix of reduced-form coefficients has full rank, necessary for identification.</li>
</ul>
<p>The KP rk statistic builds on the generalized method of moments framework and the canonical correlations between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>.</p>
<p>The reduced-form for <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
X = Z \Pi + V
\]</span></p>
<ul>
<li>
<span class="math inline">\(\Pi\)</span>: Matrix of reduced-form coefficients (<span class="math inline">\(m \times k\)</span>)</li>
<li>
<span class="math inline">\(V\)</span>: First-stage residuals (<span class="math inline">\(n \times k\)</span>)</li>
</ul>
<p>Under the null hypothesis of underidentification, the matrix <span class="math inline">\(\Pi\)</span> is rank deficient, meaning <span class="math inline">\(\Pi\)</span> does not have full rank <span class="math inline">\(k\)</span>.</p>
<p>The Kleibergen-Paap rk statistic tests the null hypothesis:</p>
<p><span class="math display">\[
H_0: \mathrm{rank}(\mathbb{E}[Z'X]) &lt; k
\]</span></p>
<p>Against the alternative:</p>
<p><span class="math display">\[
H_A: \mathrm{rank}(\mathbb{E}[Z'X]) = k
\]</span></p>
<p>The Kleibergen-Paap rk statistic is a Lagrange Multiplier test statistic derived from the first-stage canonical correlations between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, adjusted for heteroskedasticity.</p>
<p>Computation Outline</p>
<ol style="list-style-type: decimal">
<li>Compute first-stage residuals for each endogenous regressor.</li>
<li>Estimate the covariance matrix of the residuals, allowing for heteroskedasticity.</li>
<li>Calculate the rank test statistic, which has an asymptotic chi-squared distribution with <span class="math inline">\(k(m - k)\)</span> degrees of freedom.</li>
</ol>
<p>Under <span class="math inline">\(H_0\)</span>, the KP rk statistic follows:</p>
<p><span class="math display">\[
KP_{rk} \sim \chi^2_{k(m - k)}
\]</span></p>
<p>Intuition Behind the KP rk Statistic</p>
<ul>
<li>The statistic examines whether the moment conditions based on <span class="math inline">\(Z\)</span> provide enough information to identify <span class="math inline">\(\beta\)</span>.</li>
<li>If <span class="math inline">\(Z\)</span> fails to explain sufficient variation in <span class="math inline">\(X\)</span>, the instruments are not relevant, and the model is underidentified.</li>
<li>The KP rk test is a necessary condition for relevance, though not a sufficient measure of instrument strength.</li>
</ul>
<p>Practical Usage</p>
<ul>
<li>A rejection of <span class="math inline">\(H_0\)</span> suggests that the instruments are relevant enough for identification.</li>
<li>A failure to reject <span class="math inline">\(H_0\)</span> implies underidentification, and the IV estimates are not valid.</li>
</ul>
<p>While the KP rk statistic tests for underidentification, it does not directly assess weak instruments. However, it is often reported alongside Kleibergen-Paap LM and Wald statistics, which address weak instrument diagnostics in heteroskedastic settings.</p>
<p>Comparison: Kleibergen-Paap rk vs Cragg-Donald Statistic</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="45%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th><a href="sec-instrumental-variables.html#sec-kleibergen-paap-rk-statistic">Kleibergen-Paap rk Statistic</a></th>
<th><a href="sec-instrumental-variables.html#sec-cragg-donald-test">Cragg-Donald Statistic</a></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Robust to Heteroskedasticity</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="even">
<td>Valid Under Non-i.i.d. Errors</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Underidentification Test</td>
<td>Yes</td>
<td>No (tests weak instruments)</td>
</tr>
<tr class="even">
<td>Degrees of Freedom</td>
<td><span class="math inline">\(k(m - k)\)</span></td>
<td>Varies (depends on <span class="math inline">\(k\)</span> and <span class="math inline">\(m\)</span>)</td>
</tr>
<tr class="odd">
<td>Applicability</td>
<td>Always preferred in practice</td>
<td>Homoskedastic errors only</td>
</tr>
</tbody>
</table></div>
<div class="sourceCode" id="cb915"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/">sandwich</a></span><span class="op">)</span>  <span class="co"># For robust covariance estimators</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span>    <span class="co"># For testing</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span>       <span class="co"># For IV estimation (optional)</span></span>
<span></span>
<span><span class="co"># Simulate data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">Z1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Z2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">Z1</span>, <span class="va">Z2</span><span class="op">)</span> <span class="co"># Instruments (include intercept)</span></span>
<span></span>
<span><span class="co"># Weak instrument case</span></span>
<span><span class="va">X1</span> <span class="op">&lt;-</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z1</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">X1</span><span class="op">)</span></span>
<span><span class="va">beta_true</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">X</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">beta_true</span> <span class="op">+</span> <span class="va">u</span></span>
<span></span>
<span><span class="co"># First-stage regression: X ~ Z</span></span>
<span><span class="va">first_stage</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">X1</span> <span class="op">~</span> <span class="va">Z1</span> <span class="op">+</span> <span class="va">Z2</span><span class="op">)</span></span>
<span><span class="va">V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span> <span class="co"># First-stage residuals</span></span>
<span></span>
<span><span class="co"># Calculate robust covariance matrix of residuals</span></span>
<span><span class="co"># Note: sandwich package already computes heteroskedasticity-consistent covariances</span></span>
<span></span>
<span><span class="co"># Kleibergen-Paap rk test via sandwich estimators (conceptual)</span></span>
<span><span class="co"># In practice, Kleibergen-Paap rk statistics are provided by ivreg2 (Stata) or via custom functions</span></span>
<span></span>
<span><span class="co"># For illustration, using ivreg and summary statistics</span></span>
<span><span class="va">iv_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X1</span> <span class="op">|</span> <span class="va">Z1</span> <span class="op">+</span> <span class="va">Z2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Kleibergen-Paap rk statistic (reported by summary under diagnostics)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">iv_model</span>, diagnostics <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivreg(formula = Y ~ X1 | Z1 + Z2)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;        Min         1Q     Median         3Q        Max </span></span>
<span><span class="co">#&gt; -3.2259666 -0.6433047  0.0004169  0.8384112  3.4831350 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.05389    0.04810   1.120    0.263    </span></span>
<span><span class="co">#&gt; X1           1.16437    0.22129   5.262 2.12e-07 ***</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Diagnostic tests:</span></span>
<span><span class="co">#&gt;                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">#&gt; Weak instruments   2 497    11.793 9.91e-06 ***</span></span>
<span><span class="co">#&gt; Wu-Hausman         1 497     2.356  0.12541    </span></span>
<span><span class="co">#&gt; Sargan             1  NA     6.934  0.00846 ** </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.066 on 498 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-Squared: 0.3593,  Adjusted R-squared: 0.358 </span></span>
<span><span class="co">#&gt; Wald test: 27.69 on 1 and 498 DF,  p-value: 2.123e-07</span></span>
<span></span>
<span><span class="co"># Interpretation:</span></span>
<span><span class="co"># Weak instruments are flagged if the KP rk statistic does not reject underidentification.</span></span></code></pre></div>
<p>Interpretation</p>
<ul>
<li><p>The Kleibergen-Paap rk statistic is reported alongside LM and Wald weak identification tests.</p></li>
<li><p>The p-value of the rk statistic tells us whether the equation is identified.</p></li>
<li><p>If the test rejects, we proceed to evaluate weak instrument strength using Wald or LM statistics.</p></li>
</ul>
</div>
<div id="comparison-of-weak-instrument-tests" class="section level4" number="34.6.1.8">
<h4>
<span class="header-section-number">34.6.1.8</span> Comparison of Weak Instrument Tests<a class="anchor" aria-label="anchor" href="#comparison-of-weak-instrument-tests"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="12%">
<col width="30%">
<col width="43%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Test</th>
<th>Description</th>
<th>Use Case</th>
<th>Assumptions</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>First-Stage F-Statistic</td>
<td>Joint significance of instruments on <span class="math inline">\(X_2\)</span>
</td>
<td>Simple IV models (1 endogenous regressor, 1+ instruments)</td>
<td>i.i.d. errors</td>
</tr>
<tr class="even">
<td>Cragg-Donald Wald</td>
<td>Wald test for multiple instruments and endogenous variables</td>
<td>Multi-equation IV models</td>
<td>i.i.d. errors</td>
</tr>
<tr class="odd">
<td>Stock-Yogo</td>
<td>Critical values for bias/size distortion</td>
<td>Assess bias and size distortions in 2SLS estimator</td>
<td>i.i.d. errors</td>
</tr>
<tr class="even">
<td>Anderson-Rubin</td>
<td>Joint significance test in structural equation</td>
<td>Robust to weak instruments; tests hypotheses on <span class="math inline">\(\beta_2\)</span>
</td>
<td>None (valid under weak IV)</td>
</tr>
<tr class="odd">
<td>Kleinbergen-Paap rk</td>
<td>Generalized Cragg-Donald for heteroskedastic/clustered errors</td>
<td>Robust inference when classical assumptions fail; heteroskedastic-consistent inference</td>
<td>Allows heteroskedasticity</td>
</tr>
</tbody>
</table></div>
<p>All the mentioned tests (Stock Yogo, Cragg-Donald, Anderson canonical correlation test) assume errors are independently and identically distributed. If this assumption is violated, the Kleinbergen-Paap test is robust against violations of the iid assumption and can be applied even with a single endogenous variable and instrument, provided the model is properly identified <span class="citation">(<a href="references.html#ref-baum2019advice">Baum and Lewbel 2019</a>)</span>.</p>
<hr>
</div>
</div>
<div id="independence-unconfoundedness" class="section level3" number="34.6.2">
<h3>
<span class="header-section-number">34.6.2</span> Independence (Unconfoundedness)<a class="anchor" aria-label="anchor" href="#independence-unconfoundedness"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span class="math inline">\(Z\)</span> is independent of any factors affecting <span class="math inline">\(Y\)</span>, apart from through <span class="math inline">\(X_2\)</span>.</li>
<li>Formally: <span class="math inline">\(Z \perp \epsilon\)</span>.</li>
</ul>
<p>This is stronger than exclusion restriction and typically requires randomized assignment of <span class="math inline">\(Z\)</span> or strong theoretical justification.</p>
<hr>
</div>
<div id="sec-monotonicity-assumption" class="section level3" number="34.6.3">
<h3>
<span class="header-section-number">34.6.3</span> Monotonicity Assumption<a class="anchor" aria-label="anchor" href="#sec-monotonicity-assumption"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Relevant for identifying <a href="sec-causal-inference.html#sec-local-average-treatment-effects">Local Average Treatment Effects</a> <span class="citation">(<a href="references.html#ref-imbens1994identification">G. W. Imbens and Angrist 1994</a>)</span>
</li>
<li>Assumes there are <strong>no defiers</strong>: the instrument <span class="math inline">\(Z\)</span> does not cause the treatment <span class="math inline">\(X_2\)</span> to increase for some units while decreasing it for others.</li>
</ul>
<p><span class="math display">\[
X_2(Z = 1) \ge X_2(Z = 0) \quad \text{for all individuals}
\]</span></p>
<ul>
<li>Ensures we are identifying a [Local Average Treatment Effect] (LATE) for the group of <strong>compliers</strong>—individuals whose treatment status responds to the instrument.</li>
</ul>
<p>This assumption is particularly important in empirical applications involving binary instruments and heterogeneous treatment effects.</p>
<p>In business settings, instruments often arise from policy changes, eligibility cutoffs, or randomized marketing campaigns. For instance:</p>
<ul>
<li>A firm rolls out a new loyalty program (<span class="math inline">\(Z = 1\)</span>) in selected regions to encourage purchases (<span class="math inline">\(X_2\)</span>). The monotonicity assumption implies that no customer <em>reduces</em> their purchases because of the program—it only increases or leaves them unchanged.</li>
</ul>
<p>This assumption rules out defiers—individuals who respond to the instrument in the opposite direction—which would otherwise bias the IV estimate by introducing effects not attributable to compliers. Violations of monotonicity make the IV estimate difficult to interpret, as it may average over both compliers and defiers, yielding a non-causal or ambiguous LATE.</p>
<p>While monotonicity is an assumption about unobserved counterfactuals and thus not directly testable, several empirical strategies can provide suggestive evidence:</p>
<ol style="list-style-type: decimal">
<li>First-Stage Regression</li>
</ol>
<p>Estimate the impact of the instrument on the treatment. A strong, consistent sign across subgroups supports monotonicity.</p>
<div class="sourceCode" id="cb916"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sample size</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span></span>
<span><span class="co"># Generate instrument (Z), treatment (D), and outcome (Y)</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, <span class="fl">0.5</span><span class="op">)</span>  <span class="co"># Binary instrument (e.g., policy change)</span></span>
<span><span class="va">U</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>           <span class="co"># Unobserved confounder</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="fl">0.8</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">U</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>  <span class="co"># Treatment variable</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">D</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">U</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>    <span class="co"># Outcome variable</span></span>
<span></span>
<span><span class="co"># Create a data frame</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">Z</span>, <span class="va">D</span>, <span class="va">Y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># First-stage regression</span></span>
<span><span class="va">first_stage</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">D</span> <span class="op">~</span> <span class="va">Z</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = D ~ Z, data = df)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -3.2277 -0.7054  0.0105  0.7047  3.3846 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02910    0.04651   0.626    0.532    </span></span>
<span><span class="co">#&gt; Z            0.74286    0.06623  11.216   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.047 on 998 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.1119, Adjusted R-squared:  0.111 </span></span>
<span><span class="co">#&gt; F-statistic: 125.8 on 1 and 998 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="co"># Check F-statistic</span></span>
<span><span class="va">fs_f_stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span><span class="op">$</span><span class="va">fstatistic</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">fs_f_stat</span></span>
<span><span class="co">#&gt;    value </span></span>
<span><span class="co">#&gt; 125.7911</span></span></code></pre></div>
<ul>
<li><p>A positive and significant coefficient on <span class="math inline">\(Z\)</span> supports a monotonic relationship.</p></li>
<li><p>If the coefficient is near zero or flips sign in subgroups, this may signal violations.</p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Density Plot of First-Stage Residuals</li>
</ol>
<div class="sourceCode" id="cb917"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract residuals</span></span>
<span><span class="va">residuals_first_stage</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot density</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>residuals <span class="op">=</span> <span class="va">residuals_first_stage</span><span class="op">)</span>,</span>
<span>       <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">residuals</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"blue"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"Density of First-Stage Residuals"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"Residuals from First-Stage Regression"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Density"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">causalverse</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/causalverse/man/ama_theme.html">ama_theme</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="34-instrumental_var_files/figure-html/unnamed-chunk-22-1.png" width="90%" style="display: block; margin: auto;"></div>
<ul>
<li><p>A unimodal residual distribution supports monotonicity.</p></li>
<li><p>A bimodal or heavily skewed pattern could suggest a mixture of compliers and defiers.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Subgroup Analysis</li>
</ol>
<p>Split the sample into subgroups (e.g., by market segment or region) and compare the first-stage coefficients.</p>
<div class="sourceCode" id="cb918"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create two random subgroups</span></span>
<span><span class="va">df</span><span class="op">$</span><span class="va">subgroup</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Group A"</span>, <span class="st">"Group B"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># First-stage regression in subgroups</span></span>
<span><span class="va">first_stage_A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">D</span> <span class="op">~</span> <span class="va">Z</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">[</span><span class="va">df</span><span class="op">$</span><span class="va">subgroup</span> <span class="op">==</span> <span class="st">"Group A"</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="va">first_stage_B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">D</span> <span class="op">~</span> <span class="va">Z</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">[</span><span class="va">df</span><span class="op">$</span><span class="va">subgroup</span> <span class="op">==</span> <span class="st">"Group B"</span>, <span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare coefficients</span></span>
<span><span class="va">coef_A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">first_stage_A</span><span class="op">)</span><span class="op">[</span><span class="st">"Z"</span><span class="op">]</span></span>
<span><span class="va">coef_B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">first_stage_B</span><span class="op">)</span><span class="op">[</span><span class="st">"Z"</span><span class="op">]</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"First-stage coefficient for Group A:"</span>, <span class="va">coef_A</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; First-stage coefficient for Group A: 0.6645617</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"First-stage coefficient for Group B:"</span>, <span class="va">coef_B</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; First-stage coefficient for Group B: 0.8256711</span></span></code></pre></div>
<ul>
<li><p>If both groups show a coefficient with the same sign, this supports monotonicity.</p></li>
<li><p>Opposing signs raise concerns that some individuals may respond <em>against</em> the instrument.</p></li>
</ul>
<hr>
</div>
<div id="homogeneous-treatment-effects-optional" class="section level3" number="34.6.4">
<h3>
<span class="header-section-number">34.6.4</span> Homogeneous Treatment Effects (Optional)<a class="anchor" aria-label="anchor" href="#homogeneous-treatment-effects-optional"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Assumes that the causal effect of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(Y\)</span> is constant across individuals.</li>
<li>Without this, IV estimates a local rather than global average treatment effect (LATE vs ATE).</li>
</ul>
<hr>
</div>
<div id="sec-linearity-and-additivity" class="section level3" number="34.6.5">
<h3>
<span class="header-section-number">34.6.5</span> Linearity and Additivity<a class="anchor" aria-label="anchor" href="#sec-linearity-and-additivity"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>The functional form is linear in parameters:</li>
</ul>
<p><span class="math display">\[
Y = X \beta + \epsilon
\]</span></p>
<ul>
<li>No interactions or non-linearities unless explicitly modeled.</li>
<li>Additivity of the error term <span class="math inline">\(\epsilon\)</span> implies no heteroskedasticity in classic IV models (though robust standard errors can relax this).</li>
</ul>
<hr>
</div>
<div id="instrument-exogeneity-exclusion-restriction" class="section level3" number="34.6.6">
<h3>
<span class="header-section-number">34.6.6</span> Instrument Exogeneity (Exclusion Restriction)<a class="anchor" aria-label="anchor" href="#instrument-exogeneity-exclusion-restriction"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span class="math inline">\(E[Z \epsilon] = 0\)</span>: Instruments must not be correlated with the error term.</li>
<li>
<span class="math inline">\(Z\)</span> influences <span class="math inline">\(Y\)</span> only through <span class="math inline">\(X_2\)</span>.</li>
<li>No omitted variable bias from unobserved confounders correlated with <span class="math inline">\(Z\)</span>.</li>
</ul>
<p>Key Interpretation:</p>
<ul>
<li>
<span class="math inline">\(Z\)</span> has no direct effect on <span class="math inline">\(Y\)</span>.</li>
<li>Any pathway from <span class="math inline">\(Z\)</span> to <span class="math inline">\(Y\)</span> must operate exclusively through <span class="math inline">\(X_2\)</span>.</li>
</ul>
</div>
<div id="sec-exogeneity-assumption" class="section level3" number="34.6.7">
<h3>
<span class="header-section-number">34.6.7</span> Exogeneity Assumption<a class="anchor" aria-label="anchor" href="#sec-exogeneity-assumption"><i class="fas fa-link"></i></a>
</h3>
<p>The local average treatment effect (LATE) is defined as:</p>
<p><span class="math display">\[
\text{LATE} = \frac{\text{reduced form}}{\text{first stage}} = \frac{\rho}{\phi}
\]</span></p>
<p>This implies that the reduced form (<span class="math inline">\(\rho\)</span>) is the product of the first stage (<span class="math inline">\(\phi\)</span>) and LATE:</p>
<p><span class="math display">\[
\rho = \phi \times \text{LATE}
\]</span></p>
<p>Thus, if the first stage (<span class="math inline">\(\phi\)</span>) is 0, the reduced form (<span class="math inline">\(\rho\)</span>) should also be 0.</p>
<div class="sourceCode" id="cb919"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://shiny.posit.co/">shiny</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span>  <span class="co"># for ivreg</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>  <span class="co"># for visualization</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span>  <span class="co"># for data manipulation</span></span>
<span></span>
<span><span class="co"># Function to simulate the dataset</span></span>
<span><span class="va">simulate_iv_data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">beta</span>, <span class="va">phi</span>, <span class="va">direct_effect</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span>  <span class="va">epsilon_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span>  <span class="va">epsilon_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="va">phi</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="va">epsilon_x</span></span>
<span>  <span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">direct_effect</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="va">epsilon_y</span></span>
<span>  <span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Y <span class="op">=</span> <span class="va">Y</span>, X <span class="op">=</span> <span class="va">X</span>, Z <span class="op">=</span> <span class="va">Z</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Function to run the simulations and calculate the effects</span></span>
<span><span class="va">run_simulation</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">beta</span>, <span class="va">phi</span>, <span class="va">direct_effect</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Simulate the data</span></span>
<span>  <span class="va">simulated_data</span> <span class="op">&lt;-</span> <span class="fu">simulate_iv_data</span><span class="op">(</span><span class="va">n</span>, <span class="va">beta</span>, <span class="va">phi</span>, <span class="va">direct_effect</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Estimate first-stage effect (phi)</span></span>
<span>  <span class="va">first_stage</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">X</span> <span class="op">~</span> <span class="va">Z</span>, data <span class="op">=</span> <span class="va">simulated_data</span><span class="op">)</span></span>
<span>  <span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span><span class="op">[</span><span class="st">"Z"</span><span class="op">]</span></span>
<span>  <span class="va">phi_ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">first_stage</span><span class="op">)</span><span class="op">[</span><span class="st">"Z"</span>, <span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Estimate reduced-form effect (rho)</span></span>
<span>  <span class="va">reduced_form</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span>, data <span class="op">=</span> <span class="va">simulated_data</span><span class="op">)</span></span>
<span>  <span class="va">rho</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">reduced_form</span><span class="op">)</span><span class="op">[</span><span class="st">"Z"</span><span class="op">]</span></span>
<span>  <span class="va">rho_ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">reduced_form</span><span class="op">)</span><span class="op">[</span><span class="st">"Z"</span>, <span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Estimate LATE using IV regression</span></span>
<span>  <span class="va">iv_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span> <span class="op">|</span> <span class="va">Z</span>, data <span class="op">=</span> <span class="va">simulated_data</span><span class="op">)</span></span>
<span>  <span class="va">iv_late</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">iv_model</span><span class="op">)</span><span class="op">[</span><span class="st">"X"</span><span class="op">]</span></span>
<span>  <span class="va">iv_late_ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">iv_model</span><span class="op">)</span><span class="op">[</span><span class="st">"X"</span>, <span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Calculate LATE as the ratio of reduced-form and first-stage coefficients</span></span>
<span>  <span class="va">calculated_late</span> <span class="op">&lt;-</span> <span class="va">rho</span> <span class="op">/</span> <span class="va">phi</span></span>
<span>  <span class="va">calculated_late_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span></span>
<span>    <span class="op">(</span><span class="va">rho_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">rho</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="va">phi</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="va">rho</span> <span class="op">*</span> <span class="op">(</span><span class="va">phi_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">phi</span><span class="op">)</span> <span class="op">/</span> <span class="va">phi</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="va">calculated_late_ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">calculated_late</span> <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">calculated_late_se</span>, </span>
<span>                          <span class="va">calculated_late</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">calculated_late_se</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Return a list of results</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span>phi <span class="op">=</span> <span class="va">phi</span>, </span>
<span>       phi_ci <span class="op">=</span> <span class="va">phi_ci</span>,</span>
<span>       rho <span class="op">=</span> <span class="va">rho</span>, </span>
<span>       rho_ci <span class="op">=</span> <span class="va">rho_ci</span>,</span>
<span>       direct_effect <span class="op">=</span> <span class="va">direct_effect</span>,</span>
<span>       direct_effect_ci <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">direct_effect</span>, <span class="va">direct_effect</span><span class="op">)</span>,  <span class="co"># Placeholder for direct effect CI</span></span>
<span>       iv_late <span class="op">=</span> <span class="va">iv_late</span>, </span>
<span>       iv_late_ci <span class="op">=</span> <span class="va">iv_late_ci</span>,</span>
<span>       calculated_late <span class="op">=</span> <span class="va">calculated_late</span>, </span>
<span>       calculated_late_ci <span class="op">=</span> <span class="va">calculated_late_ci</span>,</span>
<span>       true_effect <span class="op">=</span> <span class="va">beta</span>,</span>
<span>       true_effect_ci <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">beta</span>, <span class="va">beta</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Placeholder for true effect CI</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Define UI for the sliders</span></span>
<span><span class="va">ui</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/fluidPage.html">fluidPage</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/titlePanel.html">titlePanel</a></span><span class="op">(</span><span class="st">"IV Model Simulation"</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/sidebarLayout.html">sidebarLayout</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/sidebarLayout.html">sidebarPanel</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/sliderInput.html">sliderInput</a></span><span class="op">(</span><span class="st">"beta"</span>, <span class="st">"True Effect of X on Y (beta):"</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">1.0</span>, value <span class="op">=</span> <span class="fl">0.5</span>, step <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/sliderInput.html">sliderInput</a></span><span class="op">(</span><span class="st">"phi"</span>, <span class="st">"First Stage Effect (phi):"</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">1.0</span>, value <span class="op">=</span> <span class="fl">0.7</span>, step <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/sliderInput.html">sliderInput</a></span><span class="op">(</span><span class="st">"direct_effect"</span>, <span class="st">"Direct Effect of Z on Y:"</span>, min <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>, max <span class="op">=</span> <span class="fl">0.5</span>, value <span class="op">=</span> <span class="fl">0</span>, step <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span>    <span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/sidebarLayout.html">mainPanel</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/plotOutput.html">plotOutput</a></span><span class="op">(</span><span class="st">"dotPlot"</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define server logic to run the simulation and generate the plot</span></span>
<span><span class="va">server</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">input</span>, <span class="va">output</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">output</span><span class="op">$</span><span class="va">dotPlot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/shiny/man/renderPlot.html">renderPlot</a></span><span class="op">(</span><span class="op">{</span></span>
<span>    <span class="co"># Run simulation</span></span>
<span>    <span class="va">results</span> <span class="op">&lt;-</span> <span class="fu">run_simulation</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>, beta <span class="op">=</span> <span class="va">input</span><span class="op">$</span><span class="va">beta</span>, phi <span class="op">=</span> <span class="va">input</span><span class="op">$</span><span class="va">phi</span>, direct_effect <span class="op">=</span> <span class="va">input</span><span class="op">$</span><span class="va">direct_effect</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Prepare data for plotting</span></span>
<span>    <span class="va">plot_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>      Effect <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"First Stage (phi)"</span>, <span class="st">"Reduced Form (rho)"</span>, <span class="st">"Direct Effect"</span>, <span class="st">"LATE (Ratio)"</span>, <span class="st">"LATE (IV)"</span>, <span class="st">"True Effect"</span><span class="op">)</span>,</span>
<span>      Value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">results</span><span class="op">$</span><span class="va">phi</span>, <span class="va">results</span><span class="op">$</span><span class="va">rho</span>, <span class="va">results</span><span class="op">$</span><span class="va">direct_effect</span>, <span class="va">results</span><span class="op">$</span><span class="va">calculated_late</span>, <span class="va">results</span><span class="op">$</span><span class="va">iv_late</span>, <span class="va">results</span><span class="op">$</span><span class="va">true_effect</span><span class="op">)</span>,</span>
<span>      CI_Lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">results</span><span class="op">$</span><span class="va">phi_ci</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">rho_ci</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">direct_effect_ci</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">calculated_late_ci</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">iv_late_ci</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">true_effect_ci</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>      CI_Upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">results</span><span class="op">$</span><span class="va">phi_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">rho_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">direct_effect_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">calculated_late_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">iv_late_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="va">results</span><span class="op">$</span><span class="va">true_effect_ci</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Create dot plot with confidence intervals</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">plot_data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Effect</span>, y <span class="op">=</span> <span class="va">Value</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">CI_Lower</span>, ymax <span class="op">=</span> <span class="va">CI_Upper</span><span class="op">)</span>, width <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"IV Model Effects"</span>,</span>
<span>           y <span class="op">=</span> <span class="st">"Coefficient Value"</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_cartesian.html">coord_cartesian</a></span><span class="op">(</span>ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>  <span class="co"># Limits the y-axis to -1 to 1 but allows CI beyond</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">45</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Run the application </span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/shiny/man/shinyApp.html">shinyApp</a></span><span class="op">(</span>ui <span class="op">=</span> <span class="va">ui</span>, server <span class="op">=</span> <span class="va">server</span><span class="op">)</span></span></code></pre></div>
<p>A statistically significant reduced form estimate without a corresponding first stage indicates an issue, suggesting an alternative channel linking instruments to outcomes or a direct effect of the IV on the outcome.</p>
<ul>
<li>No Direct Effect: When the direct effect is 0 and the first stage is 0, the reduced form is 0.
<ul>
<li>Note: Extremely rare cases with multiple additional paths that perfectly cancel each other out can also produce this result, but testing for all possible paths is impractical.</li>
</ul>
</li>
<li>With Direct Effect: When there is a direct effect of the IV on the outcome, the reduced form can be significantly different from 0, even if the first stage is 0.
<ul>
<li>This violates the exogeneity assumption, as the IV should only affect the outcome through the treatment variable.</li>
</ul>
</li>
</ul>
<p>To test the validity of the exogeneity assumption, we can use a sanity test:</p>
<ul>
<li>Identify groups for which the effects of instruments on the treatment variable are small and not significantly different from 0. The reduced form estimate for these groups should also be 0. These “no-first-stage samples” provide evidence of whether the exogeneity assumption is violated.</li>
</ul>
<div id="overid-tests" class="section level4" number="34.6.7.1">
<h4>
<span class="header-section-number">34.6.7.1</span> Overid Tests<a class="anchor" aria-label="anchor" href="#overid-tests"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<p>Wald test and Hausman test for exogeneity of <span class="math inline">\(X\)</span> assuming <span class="math inline">\(Z\)</span> is exogenous</p>
<ul>
<li>People might prefer Wald test over Hausman test.</li>
</ul>
</li>
<li><p>Sargan (for 2SLS) is a simpler version of Hansen’s J test (for IV-GMM)</p></li>
<li><p>Modified J test (i.e., Regularized jacknife IV): can handle weak instruments and small sample size <span class="citation">(<a href="references.html#ref-carrasco2022testing">Carrasco and Doukali 2022</a>)</span> (also proposed a regularized F-test to test relevance assumption that is robust to heteroskedasticity).</p></li>
<li><p>New advances: endogeneity robust inference in finite sample and sensitivity analysis of inference <span class="citation">(<a href="references.html#ref-kiviet2020testing">Kiviet 2020</a>)</span></p></li>
</ul>
<p>These tests that can provide evidence fo the validity of the over-identifying restrictions is not sufficient or necessary for the validity of the moment conditions (i.e., this assumption cannot be tested). <span class="citation">(<a href="references.html#ref-deaton2010instruments">Deaton 2010</a>; <a href="references.html#ref-parente2012cautionary">Parente and Silva 2012</a>)</span></p>
<ul>
<li><p>The over-identifying restriction can still be valid even when the instruments are correlated with the error terms, but then in this case, what you’re estimating is no longer your parameters of interest.</p></li>
<li><p>Rejection of the over-identifying restrictions can also be the result of parameter heterogeneity <span class="citation">(<a href="references.html#ref-angrist2000interpretation">J. D. Angrist, Graddy, and Imbens 2000</a>)</span></p></li>
</ul>
<p>Why overid tests hold no value/info?</p>
<ul>
<li>
<p>Overidentifying restrictions are valid irrespective of the instruments’ validity</p>
<ul>
<li>Whenever instruments have the same motivation and are on the same scale, the estimated parameter of interests will be very close <span class="citation">(<a href="references.html#ref-parente2012cautionary">Parente and Silva 2012, 316</a>)</span>
</li>
</ul>
</li>
<li>
<p>Overidentifying restriction are invalid when each instrument is valid</p>
<ul>
<li>When the effect of your parameter of interest is heterogeneous (e.g., you have two groups with two different true effects), your first instrument can be correlated with your variable of interest only for the first group and your second interments can be correlated with your variable of interest only for the second group (i.e., each instrument is valid), and if you use each instrument, you can still identify the parameter of interest. However, if you use both of them, what you estimate is a mixture of the two groups. Hence, the overidentifying restriction will be invalid (because no single parameters can make the errors of the model orthogonal to both instruments). The result may seem confusing at first because if each subset of overidentifying restrictions is valid, the full set should also be valid. However, this interpretation is flawed because the residual’s orthogonality to the instruments depends on the chosen set of instruments, and therefore the set of restrictions tested when using two sets of instruments together is not the same as the union of the sets of restrictions tested when using each set of instruments separately <span class="citation">(<a href="references.html#ref-parente2012cautionary">Parente and Silva 2012, 316</a>)</span>
</li>
</ul>
</li>
</ul>
<p>These tests (of overidentifying restrictions) should be used to check whether different instruments identify the same parameters of interest, not to check their validity <span class="citation">(<a href="references.html#ref-hausman1983specification">J. A. Hausman 1983</a>; <a href="references.html#ref-parente2012cautionary">Parente and Silva 2012</a>)</span></p>
<div id="wald-test" class="section level5" number="34.6.7.1.1">
<h5>
<span class="header-section-number">34.6.7.1.1</span> Wald Test<a class="anchor" aria-label="anchor" href="#wald-test"><i class="fas fa-link"></i></a>
</h5>
<p>Assuming that <span class="math inline">\(Z\)</span> is exogenous (a valid instrument), we want to know whether <span class="math inline">\(X_2\)</span> is exogenous</p>
<p>1st stage:</p>
<p><span class="math display">\[
X_2 = \hat{\alpha} Z + \hat{\epsilon}
\]</span></p>
<p>2nd stage:</p>
<p><span class="math display">\[
Y = \delta_0 X_1 + \delta_1 X_2 + \delta_2 \hat{\epsilon} + u
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\hat{\epsilon}\)</span> is the residuals from the 1st stage</li>
</ul>
<p>The Wald test of exogeneity assumes</p>
<p><span class="math display">\[
H_0: \delta_2 = 0 \\
H_1: \delta_2 \neq 0
\]</span></p>
<p>If you have more than one endogenous variable with more than one instrument, <span class="math inline">\(\delta_2\)</span> is a vector of all residuals from all the first-stage equations. And the null hypothesis is that they are jointly equal 0.</p>
<p>If you reject this hypothesis, it means that <span class="math inline">\(X_2\)</span> is not endogenous. Hence, for this test, we do not want to reject the null hypothesis.</p>
<p>If the test is not sacrificially significant, we might just don’t have enough information to reject the null.</p>
<p>When you have a valid instrument <span class="math inline">\(Z\)</span>, whether <span class="math inline">\(X_2\)</span> is endogenous or exogenous, your coefficient estimates of <span class="math inline">\(X_2\)</span> should still be consistent. But if <span class="math inline">\(X_2\)</span> is exogenous, then 2SLS will be inefficient (i.e., larger standard errors).</p>
<p>Intuition:</p>
<p><span class="math inline">\(\hat{\epsilon}\)</span> is the supposed endogenous part of <span class="math inline">\(X_2\)</span>, When we regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(\hat{\epsilon}\)</span> and observe that its coefficient is not different from 0. It means that the exogenous part of <span class="math inline">\(X_2\)</span> can explain well the impact on <span class="math inline">\(Y\)</span>, and there is no endogenous part.</p>
</div>
<div id="hausmans-test" class="section level5" number="34.6.7.1.2">
<h5>
<span class="header-section-number">34.6.7.1.2</span> Hausman’s Test<a class="anchor" aria-label="anchor" href="#hausmans-test"><i class="fas fa-link"></i></a>
</h5>
<p>Similar to <a href="generalized-linear-models.html#sec-wald-test-logistic">Wald Test</a> and identical to <a href="generalized-linear-models.html#sec-wald-test-logistic">Wald Test</a> when we have homoskedasticity (i.e., homogeneity of variances). Because of this assumption, it’s used less often than <a href="generalized-linear-models.html#sec-wald-test-logistic">Wald Test</a></p>
</div>
<div id="hansens-j" class="section level5" number="34.6.7.1.3">
<h5>
<span class="header-section-number">34.6.7.1.3</span> Hansen’s J<a class="anchor" aria-label="anchor" href="#hansens-j"><i class="fas fa-link"></i></a>
</h5>
<ul>
<li><p><span class="citation">(<a href="references.html#ref-hansen1982large">L. P. Hansen 1982</a>)</span></p></li>
<li>
<p>J-test (over-identifying restrictions test): test whether additional instruments are exogenous</p>
<ul>
<li>Can only be applied in cases where you have more instruments than endogenous variables
<ul>
<li><span class="math inline">\(dim(Z) &gt; dim(X_2)\)</span></li>
</ul>
</li>
<li>Assume at least one instrument within <span class="math inline">\(Z\)</span> is exogenous</li>
</ul>
</li>
</ul>
<p>Procedure IV-GMM:</p>
<ol style="list-style-type: decimal">
<li>Obtain the residuals of the 2SLS estimation</li>
<li>Regress the residuals on all instruments and exogenous variables.</li>
<li>Test the joint hypothesis that all coefficients of the residuals across instruments are 0 (i.e., this is true when instruments are exogenous).
<ol style="list-style-type: decimal">
<li><p>Compute <span class="math inline">\(J = mF\)</span> where <span class="math inline">\(m\)</span> is the number of instruments, and <span class="math inline">\(F\)</span> is your equation <span class="math inline">\(F\)</span> statistic (can you use <code><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis()</a></code> again).</p></li>
<li><p>If your exogeneity assumption is true, then <span class="math inline">\(J \sim \chi^2_{m-k}\)</span> where <span class="math inline">\(k\)</span> is the number of endogenous variables.</p></li>
</ol>
</li>
<li>If you reject this hypothesis, it can be that
<ol style="list-style-type: decimal">
<li><p>The first sets of instruments are invalid</p></li>
<li><p>The second sets of instruments are invalid</p></li>
<li><p>Both sets of instruments are invalid</p></li>
</ol>
</li>
</ol>
<p>Note: This test is only true when your residuals are homoskedastic.</p>
<p>For a heteroskedasticity-robust <span class="math inline">\(J\)</span>-statistic, see <span class="citation">(<a href="references.html#ref-carrasco2022testing">Carrasco and Doukali 2022</a>; <a href="references.html#ref-li2022testing">H. Li et al. 2022</a>)</span></p>
</div>
<div id="sargan-test" class="section level5" number="34.6.7.1.4">
<h5>
<span class="header-section-number">34.6.7.1.4</span> Sargan Test<a class="anchor" aria-label="anchor" href="#sargan-test"><i class="fas fa-link"></i></a>
</h5>
<p><span class="citation">(<a href="references.html#ref-sargan1958estimation">Sargan 1958</a>)</span></p>
<p>Similar to <a href="sec-instrumental-variables.html#hansens-j">Hansen’s J</a>, but it assumes homoskedasticity</p>
<ul>
<li><p>Have to be careful when sample is not collected exogenously. As such, when you have choice-based sampling design, the sampling weights have to be considered to have consistent estimates. However, even if we apply sampling weights, the tests are not suitable because the iid assumption off errors are already violated. Hence, the test is invalid in this case <span class="citation">(<a href="references.html#ref-pitt2011overidentification">Pitt 2011</a>)</span>.</p></li>
<li><p>If one has heteroskedasticity in its design, the Sargan test is invalid <span class="citation">(<a href="references.html#ref-pitt2011overidentification">Pitt 2011</a>})</span></p></li>
</ul>
<hr>
</div>
</div>
<div id="standard-interpretation-of-the-j-test-for-overidentifying-restrictions-is-misleading" class="section level4" number="34.6.7.2">
<h4>
<span class="header-section-number">34.6.7.2</span> Standard Interpretation of the J-Test for Overidentifying Restrictions Is Misleading<a class="anchor" aria-label="anchor" href="#standard-interpretation-of-the-j-test-for-overidentifying-restrictions-is-misleading"><i class="fas fa-link"></i></a>
</h4>
<p>In IV estimation—particularly in overidentified models where the number of instruments <span class="math inline">\(m\)</span> exceeds the number of endogenous regressors <span class="math inline">\(k\)</span>—it is standard practice to conduct the J-test <span class="citation">(<a href="references.html#ref-sargan1958estimation">Sargan 1958</a>; <a href="references.html#ref-hansen1982large">L. P. Hansen 1982</a>)</span>. Commonly, the J-test (or Sargan-Hansen test) is described as a method to test whether the instruments are valid. But this is misleading. The J-test <em>cannot</em> establish instrument validity merely because it “fails to reject” the null; at best, it can uncover evidence <em>against</em> validity.</p>
<p><strong>What the J-Test Actually Does</strong></p>
<p>Let <span class="math inline">\(Z\)</span> denote the <span class="math inline">\(n \times m\)</span> matrix of instruments, and let <span class="math inline">\(u\)</span> be the structural error term from the IV model. The J-test evaluates the following moment conditions implied by instrument exogeneity:</p>
<p><span class="math display">\[
\begin{aligned}
H_0 &amp;: \mathbb{E}[Z'u] = 0 \quad \text{(All moment conditions hold simultaneously)},\\
H_A &amp;: \mathbb{E}[Z'u] \neq 0 \quad \text{(At least one moment condition fails)}.
\end{aligned}
\]</span></p>
<ul>
<li>
<strong>Reject</strong> <span class="math inline">\(H_0\)</span>: At least one instrument is invalid, or the model is otherwise misspecified.</li>
<li>
<strong>Fail to reject</strong> <span class="math inline">\(H_0\)</span>: There is <em>no sample evidence</em> that the instruments are invalid—but this <em>does not</em> mean they are necessarily valid.</li>
</ul>
<p>The J-statistic can be written (in a Generalized Method of Moments context) as:</p>
<p><span class="math display">\[
J = n \,\hat{g}'\, W \,\hat{g},
\]</span></p>
<p>where <span class="math inline">\(\hat{g} = \frac{1}{n} \sum_{i=1}^n z_i \hat{u}_i\)</span> is the sample average of instrument–residual covariances (for residuals <span class="math inline">\(\hat{u}_i\)</span>), and <span class="math inline">\(W\)</span> is an appropriate weighting matrix (often the inverse of the variance matrix of <span class="math inline">\(\hat{g}\)</span>).</p>
<p>Under the null, <span class="math inline">\(J\)</span> is asymptotically <span class="math inline">\(\chi^2_{m - k}\)</span>. A large <span class="math inline">\(J\)</span> (relative to a <span class="math inline">\(\chi^2\)</span> critical value) indicates rejection.</p>
<blockquote>
<p><strong>Key Insight</strong>: <strong>Failing to reject</strong> the J-test null <strong>does not confirm validity</strong>. It just means the test <em>did not detect</em> evidence of invalid instruments. If the test has <em>low power</em> (e.g., in small samples or with weak instruments), you may see “no rejection” even when instruments are truly invalid.</p>
</blockquote>
<hr>
<p>Why the “J-Test as a Validity Test” Is the Wrong Way to Think About It</p>
<ol style="list-style-type: decimal">
<li>
<strong>The Null Hypothesis Is Almost Always Too Strong</strong>
<ul>
<li>Economic models are approximations; strict exogeneity rarely holds perfectly.</li>
<li>Even when instruments are “plausibly” exogenous, the population moment <span class="math inline">\(\mathbb{E}[Z'u]\)</span> may only approximately hold.</li>
<li>The J-test requires <em>all</em> instruments to be perfectly valid. Failing to reject <span class="math inline">\(H_0\)</span> does not prove that they are.</li>
</ul>
</li>
<li>
<strong>Weak Instruments Lead to Weak Power</strong>
<ul>
<li>The J-test can have low power when instruments are weak.</li>
<li>You may fail to reject even <em>invalid</em> instruments if the test cannot detect violations.</li>
</ul>
</li>
<li>
<strong>Rejection Does Not Pinpoint Which Instrument Is Invalid</strong>
<ul>
<li>You only learn that <em>one or more</em> instruments (or the entire model) is problematic; the J-test doesn’t tell you which ones.</li>
</ul>
</li>
<li>
<strong>Model Specification Error Confounds Interpretation</strong>
<ul>
<li>A J-test rejection can stem from instrument invalidity <em>or</em> from broader model mis-specification (e.g., incorrect functional form).</li>
<li>The test does not distinguish these sources.</li>
</ul>
</li>
<li>
<strong>Overidentification Itself Does Not Guarantee a Validity Check</strong>
<ul>
<li>The J-test is only available if <span class="math inline">\(m &gt; k\)</span>. If <span class="math inline">\(m = k\)</span> (exact identification), no J-test is possible.</li>
<li>Ironically, exactly-identified models often go “unquestioned” because we <em>cannot</em> run a J-test—yet that does not mean they are more valid.</li>
</ul>
</li>
</ol>
<hr>
<p>How to Think About the J-Test Instead</p>
<p>A Diagnostic, Not a Proof</p>
<ul>
<li>
<strong>Rejection</strong>: Suggests a problem—invalid instruments or mis-specification.</li>
<li>
<strong>No rejection</strong>: Implies no detected evidence of invalidity—but not a proof of validity.</li>
</ul>
<blockquote>
<p><strong>Analogy</strong>: Not rejecting the J-test null is like a blood test that does not detect a virus. It does <em>not</em> guarantee the patient is healthy; the test may have been insensitive or the sample might have been too small.</p>
</blockquote>
<p>Contextual Evaluation Is Key</p>
<ul>
<li>
<strong>Substantive/Theoretical Knowledge</strong>: Instrument validity ultimately hinges on whether you can justify <span class="math inline">\(Z\)</span> being uncorrelated with the error term <em>in theory</em>.</li>
<li>The J-test is merely <em>complementary</em>, not a substitute for compelling arguments about why instruments are exogenous.</li>
</ul>
<hr>
<p><strong>Practical Implications and Recommendations</strong></p>
<ol style="list-style-type: decimal">
<li>
<strong>Don’t Rely Solely on the J-Test</strong>
<ul>
<li>Use it as a screening tool, but always provide theoretical or institutional justification for instrument exogeneity.</li>
</ul>
</li>
<li>
<strong>Assess Instrument Strength Separately</strong>
<ul>
<li>The J-test says nothing about relevance.</li>
<li>Weak instruments reduce the power of the J-test.</li>
<li>Check first-stage <span class="math inline">\(F\)</span>-statistics or Kleibergen-Paap rk statistics.</li>
</ul>
</li>
<li>
<strong>Sensitivity and Robustness Analysis</strong>
<ul>
<li>Test different subsets of instruments or alternative specifications.</li>
<li>Perform leave-one-out analyses to see whether dropping a particular instrument changes conclusions.</li>
</ul>
</li>
<li>
<strong>Use Weak-Instrument-Robust Tests</strong>
<ul>
<li>Consider Anderson-Rubin, Stock-Wright, or Conditional Likelihood Ratio tests.</li>
<li>These can remain valid or more robust in the presence of weak instruments or model misspecification.</li>
</ul>
</li>
</ol>
<hr>
<p>Summary Table: Common Misinterpretations vs. Reality</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="31%">
<col width="68%">
</colgroup>
<thead><tr class="header">
<th><strong>Common Misinterpretation</strong></th>
<th><strong>Correct Understanding</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>“The J-test proves my instruments are valid.”</td>
<td>Failing to reject does <em>not</em> prove validity; it only means no evidence <em>against</em> validity was found.</td>
</tr>
<tr class="even">
<td>“A high p-value shows strong evidence of validity.”</td>
<td>A high p-value shows no evidence <em>against</em> validity, possibly due to low power or other limitations.</td>
</tr>
<tr class="odd">
<td>“Rejecting the J-test means I know which instrument is bad.”</td>
<td>Rejection only indicates a problem. It doesn’t pinpoint which instrument or whether the issue is broader model misspecification.</td>
</tr>
<tr class="even">
<td>“The J-test replaces theory in validating instruments.”</td>
<td>The J-test is <em>complementary</em> to theory or institutional knowledge; instrument exogeneity still requires substantive justification.</td>
</tr>
</tbody>
</table></div>
<hr>
<div class="sourceCode" id="cb920"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span>  <span class="co"># Provides ivreg function</span></span>
<span></span>
<span><span class="co"># Simulate data for a small demonstration</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">Z1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Z2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Z</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BiocGenerics/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">Z1</span>, <span class="va">Z2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Construct a (potentially) weak first stage</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fl">0.2</span> <span class="op">*</span> <span class="va">Z1</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fl">1.5</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">u</span></span>
<span></span>
<span><span class="co"># Fit IV (overidentified) using both Z1 and Z2 as instruments</span></span>
<span><span class="va">iv_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span> <span class="op">|</span> <span class="va">Z1</span> <span class="op">+</span> <span class="va">Z2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Summary with diagnostics, including Sargan-Hansen J-test</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">iv_model</span>, diagnostics <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivreg(formula = Y ~ X | Z1 + Z2)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -2.62393 -0.68911 -0.01314  0.69803  3.53553 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02941    0.04579   0.642    0.521    </span></span>
<span><span class="co">#&gt; X            1.47136    0.17654   8.335 7.63e-16 ***</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Diagnostic tests:</span></span>
<span><span class="co">#&gt;                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">#&gt; Weak instruments   2 497    17.289 5.51e-08 ***</span></span>
<span><span class="co">#&gt; Wu-Hausman         1 497     0.003    0.959    </span></span>
<span><span class="co">#&gt; Sargan             1  NA     0.131    0.717    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.005 on 498 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-Squared: 0.6794,  Adjusted R-squared: 0.6787 </span></span>
<span><span class="co">#&gt; Wald test: 69.47 on 1 and 498 DF,  p-value: 7.628e-16</span></span>
<span></span>
<span><span class="co"># Interpretation:</span></span>
<span><span class="co"># - If the J-test p-value is large, do NOT conclude "valid instruments."</span></span>
<span><span class="co"># - Check the first-stage F-stat or other measures of strength.</span></span></code></pre></div>
<p><strong>Interpretation of Output</strong>:</p>
<ul>
<li><p>A large (non-significant) J-test statistic (with a large p-value) means you <em>do not</em> reject the hypothesis that <span class="math inline">\(\hat{u} = 0\)</span>. It does <em>not</em> prove that all instruments are valid—it only suggests the sample does not provide evidence against validity.</p></li>
<li><p>Always pair this with theory-based justifications for <span class="math inline">\(Z\)</span>.</p></li>
</ul>
</div>
<div id="j-test-rejects-even-with-valid-instruments-heterogeneous-treatment-effects" class="section level4" number="34.6.7.3">
<h4>
<span class="header-section-number">34.6.7.3</span> J-Test Rejects Even with Valid Instruments (Heterogeneous Treatment Effects)<a class="anchor" aria-label="anchor" href="#j-test-rejects-even-with-valid-instruments-heterogeneous-treatment-effects"><i class="fas fa-link"></i></a>
</h4>
<p>A more subtle point: The J-test can reject <em>even if all instruments truly are exogenous</em> when the <strong>treatment effect is heterogeneous</strong>—i.e., each instrument identifies a <em>different</em> local average treatment effect.</p>
<p><strong>The Core Issue: Inconsistent LATEs</strong></p>
<ul>
<li><p>The J-test implicitly assumes a single (homogeneous) treatment effect <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>If different instruments identify different segments of the population, each instrument can yield a <em>distinct</em> causal effect.</p></li>
<li><p>This discrepancy can trigger a J-test rejection, not because the instruments are invalid, but because they don’t agree on a <em>single</em> parameter value.</p></li>
</ul>
<div class="sourceCode" id="cb921"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n</span>  <span class="op">&lt;-</span> <span class="fl">5000</span></span>
<span><span class="va">Z1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="va">Z2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Assign "complier type" for illustration</span></span>
<span><span class="co"># (This is just one way to simulate different subpopulations responding differently.)</span></span>
<span><span class="va">complier_type</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">Z1</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">Z2</span> <span class="op">==</span> <span class="fl">0</span>, <span class="st">"Z1_only"</span>,</span>
<span>                 <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">Z2</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">&amp;</span> <span class="va">Z1</span> <span class="op">==</span> <span class="fl">0</span>, <span class="st">"Z2_only"</span>, <span class="st">"Both"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># True LATEs differ by instrument-induced compliance</span></span>
<span><span class="va">beta_Z1</span> <span class="op">&lt;-</span> <span class="fl">1.0</span></span>
<span><span class="va">beta_Z2</span> <span class="op">&lt;-</span> <span class="fl">2.0</span></span>
<span></span>
<span><span class="co"># Generate endogenous X with partial influence from Z1 and Z2</span></span>
<span><span class="va">propensity</span> <span class="op">&lt;-</span> <span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">Z1</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">Z2</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span>, <span class="va">propensity</span><span class="op">)</span></span>
<span><span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Outcome with heterogeneous effects</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">complier_type</span> <span class="op">==</span> <span class="st">"Z1_only"</span>, <span class="va">beta_Z1</span> <span class="op">*</span> <span class="va">X</span>,</span>
<span>     <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">complier_type</span> <span class="op">==</span> <span class="st">"Z2_only"</span>, <span class="va">beta_Z2</span> <span class="op">*</span> <span class="va">X</span>, <span class="fl">1.5</span> <span class="op">*</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="va">u</span></span>
<span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">Y</span>, <span class="va">X</span>, <span class="va">Z1</span>, <span class="va">Z2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># IV using Z1 only</span></span>
<span><span class="va">iv_Z1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span> <span class="op">|</span> <span class="va">Z1</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">iv_Z1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivreg(formula = Y ~ X | Z1, data = df)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -4.50464 -1.08224 -0.01943  1.08215  4.27937 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   1.1026     0.1101   10.01  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; X            -0.5259     0.1977   -2.66  0.00785 ** </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.466 on 3802 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-Squared: -0.2743, Adjusted R-squared: -0.2746 </span></span>
<span><span class="co">#&gt; Wald test: 7.076 on 1 and 3802 DF,  p-value: 0.007847</span></span>
<span></span>
<span><span class="co"># IV using Z2 only</span></span>
<span><span class="va">iv_Z2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span> <span class="op">|</span> <span class="va">Z2</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">iv_Z2</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivreg(formula = Y ~ X | Z2, data = df)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;        Min         1Q     Median         3Q        Max </span></span>
<span><span class="co">#&gt; -4.5025384 -1.1458187  0.0002382  1.1747011  5.0622955 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  -1.2145     0.1279  -9.499   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; X             3.7344     0.2306  16.195   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.533 on 3802 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-Squared: -0.3948, Adjusted R-squared: -0.3952 </span></span>
<span><span class="co">#&gt; Wald test: 262.3 on 1 and 3802 DF,  p-value: &lt; 2.2e-16</span></span>
<span></span>
<span><span class="co"># Overidentified model (Z1 + Z2)</span></span>
<span><span class="va">iv_both</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/AER/man/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">X</span> <span class="op">|</span> <span class="va">Z1</span> <span class="op">+</span> <span class="va">Z2</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">iv_both</span>, diagnostics <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ivreg(formula = Y ~ X | Z1 + Z2, data = df)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">#&gt; -3.46090 -0.71661 -0.01045  0.71687  3.82014 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  0.02763    0.04423   0.625    0.532    </span></span>
<span><span class="co">#&gt; X            1.45057    0.07494  19.356   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Diagnostic tests:</span></span>
<span><span class="co">#&gt;                   df1  df2 statistic p-value    </span></span>
<span><span class="co">#&gt; Weak instruments    2 3801   510.265  &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Wu-Hausman          1 3801     0.751   0.386    </span></span>
<span><span class="co">#&gt; Sargan              1   NA   264.175  &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.059 on 3802 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-Squared: 0.3345,  Adjusted R-squared: 0.3343 </span></span>
<span><span class="co">#&gt; Wald test: 374.7 on 1 and 3802 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p><strong>Expected Results</strong></p>
<ul>
<li><p>IV via Z1 only should yield an estimate <span class="math inline">\(\approx 1.0\)</span>.</p></li>
<li><p>IV via Z2 only should yield <span class="math inline">\(\approx 2.0\)</span>.</p></li>
<li><p>IV via Z1 and Z2 together yields a 2SLS estimate that is some <em>weighted average</em> of 1.0 and 2.0.</p></li>
<li><p>The J-test often <em>rejects</em> because the data do not support a <em>single</em> <span class="math inline">\(\beta\)</span> across both instruments.</p></li>
</ul>
<p><strong>Key Takeaways</strong></p>
<ol style="list-style-type: decimal">
<li><strong>The J-Test Assumes Homogeneity</strong></li>
</ol>
<ul>
<li>If treatment effects vary with the subpopulation that each instrument induces into treatment, the J-test can reject even when all instruments are <em>exogenous</em>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><strong>A Rejection May Signal Heterogeneity, Not Invalidity</strong></li>
</ol>
<ul>
<li>The test cannot distinguish invalid exogeneity from different underlying causal parameters.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>Practical Implications</strong></li>
</ol>
<ul>
<li><p>Be aware of the [Local Average Treatment Effect] interpretation of your instruments.</p></li>
<li><p>If multiple instruments target different “complier” groups, the standard J-test lumps them into one homogenous <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>Consider reporting separate IV estimates or using methods that explicitly account for treatment-effect heterogeneity (e.g., Marginal Treatment Effects or other advanced frameworks).</p></li>
</ul>
<hr>
</div>
</div>
</div>
<div id="cautions-in-iv" class="section level2" number="34.7">
<h2>
<span class="header-section-number">34.7</span> Cautions in IV<a class="anchor" aria-label="anchor" href="#cautions-in-iv"><i class="fas fa-link"></i></a>
</h2>
<div id="negative-r2-in-iv" class="section level3" number="34.7.1">
<h3>
<span class="header-section-number">34.7.1</span> Negative <span class="math inline">\(R^2\)</span> in IV<a class="anchor" aria-label="anchor" href="#negative-r2-in-iv"><i class="fas fa-link"></i></a>
</h3>
<p>In IV estimation, particularly 2SLS and 3SLS, it is common and not problematic to encounter negative <span class="math inline">\(R^2\)</span> values in the second stage regression. Unlike <a href="linear-regression.html#ordinary-least-squares">Ordinary Least Squares</a>, where <span class="math inline">\(R^2\)</span> is often used to assess the fit of the model, in IV regression the primary concern is consistency and unbiased estimation of the coefficients of interest, not the goodness-of-fit.</p>
<p>What Should You Look At Instead of <span class="math inline">\(R^2\)</span> in IV?</p>
<ol style="list-style-type: decimal">
<li>
<strong>Instrument Relevance</strong> (First-stage <span class="math inline">\(F\)</span>-statistics, Partial <span class="math inline">\(R^2\)</span>)</li>
<li>
<strong>Weak Instrument Tests</strong> (Kleibergen-Paap, Anderson-Rubin tests)</li>
<li>
<strong>Validity of Instruments</strong> (Overidentification tests like Sargan/Hansen J-test)</li>
<li>
<strong>Endogeneity Tests</strong> (Durbin-Wu-Hausman test for endogeneity)</li>
<li>
<a href="sec-instrumental-variables.html#sec-inference-iv">Confidence Intervals and Standard Errors</a>, focusing on inference for <span class="math inline">\(\hat{\beta}\)</span>.</li>
</ol>
<p><strong>Geometric Intuition</strong></p>
<ul>
<li>In OLS, the fitted values <span class="math inline">\(\hat{y}\)</span> are the orthogonal projection of <span class="math inline">\(y\)</span> onto the column space of <span class="math inline">\(X\)</span>.</li>
<li>In 2SLS, <span class="math inline">\(\hat{y}\)</span> is the projection onto the space spanned by <span class="math inline">\(Z\)</span>, not <span class="math inline">\(X\)</span>.</li>
<li>As a result, the angle between <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span> may not minimize the residual variance, and RSS can be larger than in OLS.</li>
</ul>
<hr>
<p>Recall the formula for the coefficient of determination (<span class="math inline">\(R^2\)</span>) in a regression model:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{RSS}{TSS} = \frac{MSS}{TSS}
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(TSS\)</span> is the Total Sum of Squares: <span class="math display">\[
TSS = \sum_{i=1}^n (y_i - \bar{y})^2
\]</span>
</li>
<li>
<span class="math inline">\(MSS\)</span> is the Model Sum of Squares: <span class="math display">\[
MSS = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2
\]</span>
</li>
<li>
<span class="math inline">\(RSS\)</span> is the Residual Sum of Squares: <span class="math display">\[
RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]</span>
</li>
</ul>
<p>In OLS, the <span class="math inline">\(R^2\)</span> measures the proportion of variance in <span class="math inline">\(Y\)</span> that is explained by the regressors <span class="math inline">\(X\)</span>.</p>
<p>Key Properties in OLS:</p>
<ul>
<li><span class="math inline">\(R^2 \in [0, 1]\)</span></li>
<li>Adding more regressors (even irrelevant ones) never decreases <span class="math inline">\(R^2\)</span>.</li>
<li>
<span class="math inline">\(R^2\)</span> measures in-sample goodness-of-fit, not causal interpretation.</li>
</ul>
<hr>
<div id="why-does-r2-lose-its-meaning-in-iv-regression" class="section level4" number="34.7.1.1">
<h4>
<span class="header-section-number">34.7.1.1</span> Why Does <span class="math inline">\(R^2\)</span> Lose Its Meaning in IV Regression?<a class="anchor" aria-label="anchor" href="#why-does-r2-lose-its-meaning-in-iv-regression"><i class="fas fa-link"></i></a>
</h4>
<p>In IV regression, the second stage regression replaces the endogenous variable <span class="math inline">\(X_2\)</span> with its predicted values from the first stage:</p>
<p>Stage 1:</p>
<p><span class="math display">\[
X_2 = Z \pi + v
\]</span></p>
<p>Stage 2:</p>
<p><span class="math display">\[
Y = X_1 \beta_1 + \hat{X}_2 \beta_2 + \epsilon
\]</span></p>
<ul>
<li>
<span class="math inline">\(\hat{X}_2\)</span> is not the observed <span class="math inline">\(X_2\)</span>, but a proxy constructed from <span class="math inline">\(Z\)</span>.</li>
<li>
<span class="math inline">\(\hat{X}_2\)</span> isolates the exogenous variation in <span class="math inline">\(X_2\)</span> that is independent of <span class="math inline">\(\epsilon\)</span>.</li>
<li>This reduces bias, but comes at a cost:
<ul>
<li>The variation in <span class="math inline">\(\hat{X}_2\)</span> is typically less than that in <span class="math inline">\(X_2\)</span>.</li>
<li>The predicted values <span class="math inline">\(\hat{y}_i\)</span> from the second stage are not necessarily close to <span class="math inline">\(y_i\)</span>.</li>
</ul>
</li>
</ul>
</div>
<div id="why-r2-can-be-negative" class="section level4" number="34.7.1.2">
<h4>
<span class="header-section-number">34.7.1.2</span> Why <span class="math inline">\(R^2\)</span> Can Be Negative:<a class="anchor" aria-label="anchor" href="#why-r2-can-be-negative"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(R^2\)</span> is calculated using: <span class="math display">\[
R^2 = 1 - \frac{RSS}{TSS}
\]</span> But in IV:</li>
</ol>
<ul>
<li>The predicted values of <span class="math inline">\(Y\)</span> are not chosen to minimize RSS, because IV is not minimizing the residuals in the second stage.</li>
<li>Unlike OLS, 2SLS chooses <span class="math inline">\(\hat{\beta}\)</span> to satisfy moment conditions rather than minimizing the sum of squared errors.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><p>It is possible (and common in IV) for the residual sum of squares to be greater than the total sum of squares: <span class="math display">\[
RSS &gt; TSS
\]</span> Which makes: <span class="math display">\[
R^2 = 1 - \frac{RSS}{TSS} &lt; 0
\]</span></p></li>
<li>
<p>This happens because:</p>
<ul>
<li>The predicted values <span class="math inline">\(\hat{y}_i\)</span> in IV are not optimized to fit the observed <span class="math inline">\(y_i\)</span>.</li>
<li>The residuals can be larger, because IV focuses on identifying causal effects, not prediction.</li>
</ul>
</li>
</ol>
<p>For example, assume we have:</p>
<ul>
<li><p><span class="math inline">\(TSS = 100\)</span></p></li>
<li><p><span class="math inline">\(RSS = 120\)</span></p></li>
</ul>
<p>Then: <span class="math display">\[ R^2 = 1 - \frac{120}{100} = -0.20 \]</span></p>
<p>This happens because the IV procedure does not minimize RSS. It prioritizes solving the endogeneity problem over explaining the variance in <span class="math inline">\(Y\)</span>.</p>
<hr>
</div>
<div id="why-we-dont-care-about-r2-in-iv" class="section level4" number="34.7.1.3">
<h4>
<span class="header-section-number">34.7.1.3</span> Why We Don’t Care About <span class="math inline">\(R^2\)</span> in IV<a class="anchor" aria-label="anchor" href="#why-we-dont-care-about-r2-in-iv"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>IV Estimates Focus on <strong>Consistency</strong>, Not <strong>Prediction</strong>
</li>
</ol>
<ul>
<li>The goal of IV is to obtain a consistent estimate of <span class="math inline">\(\beta_2\)</span>.</li>
<li>IV sacrifices fit (higher variance in <span class="math inline">\(\hat{y}_i\)</span>) to remove endogeneity bias.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>
<span class="math inline">\(R^2\)</span> Does Not Reflect the Quality of an IV Estimator</li>
</ol>
<ul>
<li>A high <span class="math inline">\(R^2\)</span> in IV may be misleading (for instance, when instruments are weak or invalid).</li>
<li>A negative <span class="math inline">\(R^2\)</span> does not imply a bad IV estimator if the assumptions of instrument validity are met.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>IV Regression Is About Identification, Not In-Sample Fit</li>
</ol>
<ul>
<li>IV relies on relevance and exogeneity of instruments, not residual minimization.</li>
</ul>
<hr>
</div>
<div id="technical-details-on-r2" class="section level4" number="34.7.1.4">
<h4>
<span class="header-section-number">34.7.1.4</span> Technical Details on <span class="math inline">\(R^2\)</span><a class="anchor" aria-label="anchor" href="#technical-details-on-r2"><i class="fas fa-link"></i></a>
</h4>
<p>In OLS: <span class="math display">\[
\hat{\beta}^{OLS} = (X'X)^{-1} X'Y
\]</span> Minimizes: <span class="math display">\[
RSS = (Y - X \hat{\beta}^{OLS})'(Y - X \hat{\beta}^{OLS})
\]</span></p>
<p>In IV: <span class="math display">\[
\hat{\beta}^{IV} = (X'P_Z X)^{-1} X'P_Z Y
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(P_Z = Z (Z'Z)^{-1} Z'\)</span> is the projection matrix onto <span class="math inline">\(Z\)</span>.</p></li>
<li><p>The IV estimator solves: <span class="math display">\[
Z'(Y - X\hat{\beta}) = 0
\]</span></p></li>
<li><p>No guarantee that this minimizes RSS.</p></li>
</ul>
<p>Residuals:</p>
<p><span class="math display">\[
e^{IV} = Y - X \hat{\beta}^{IV}
\]</span></p>
<p>The norm of <span class="math inline">\(e^{IV}\)</span> is typically larger than in OLS because IV uses fewer effective degrees of freedom (constrained variation via <span class="math inline">\(Z\)</span>).</p>
<p>A Note on <span class="math inline">\(R^2\)</span> in 3SLS and GMM</p>
<ul>
<li>In 3SLS or GMM IV, <span class="math inline">\(R^2\)</span> can be similarly misleading.</li>
<li>These methods often operate under moment conditions or system estimation, not residual minimization.</li>
</ul>
<hr>
</div>
</div>
<div id="sec-many-instruments-bias" class="section level3" number="34.7.2">
<h3>
<span class="header-section-number">34.7.2</span> Many-Instruments Bias<a class="anchor" aria-label="anchor" href="#sec-many-instruments-bias"><i class="fas fa-link"></i></a>
</h3>
<p>While IV is powerful, it is also delicate. One critical issue that arises is <strong>the many-instruments problem</strong>, also known as <strong>many-IV bias</strong>.</p>
<p>Consider the structural model:</p>
<p><span class="math inline">\(y_i = \beta x_i + u_i\)</span></p>
<p>where <span class="math inline">\(x_i\)</span> is endogenous: <span class="math inline">\(\mathbb{E}[x_i u_i] \neq 0\)</span>. To address this, we introduce instruments <span class="math inline">\(z_i\)</span> such that:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Relevance</strong>: <span class="math inline">\(\mathbb{E}[z_i x_i] \neq 0\)</span>
</li>
<li>
<strong>Exogeneity</strong>: <span class="math inline">\(\mathbb{E}[z_i u_i] = 0\)</span>
</li>
</ol>
<p>The standard 2SLS estimator is given by:</p>
<p><span class="math inline">\(\hat{\beta}_{2SLS} = (X'P_ZX)^{-1} X'P_Zy\)</span></p>
<p>where <span class="math inline">\(P_Z = Z(Z'Z)^{-1}Z'\)</span> is the projection matrix onto the column space of <span class="math inline">\(Z\)</span>.</p>
<p>The many-IV problem arises when the number of instruments <span class="math inline">\(L\)</span> is large relative to the number of observations <span class="math inline">\(n\)</span>. In particular, the issue becomes severe as <span class="math inline">\(L/n \to \alpha &gt; 0\)</span>, leading to several problems:</p>
<ul>
<li>
<strong>Bias Toward OLS</strong>: The 2SLS estimator becomes increasingly biased toward the OLS estimator.</li>
<li>
<strong>Overfitting</strong>: The first-stage regression overfits the endogenous variable, capturing noise rather than true variation.</li>
<li>
<strong>Inflated Variance</strong>: The second-stage estimates become imprecise, leading to misleading inference.</li>
</ul>
<p>Traditional IV asymptotics assume <span class="math inline">\(L\)</span> is fixed as <span class="math inline">\(n \to \infty\)</span>. <span class="citation">Bekker (<a href="references.html#ref-bekker1994alternative">1994</a>)</span> proposed an alternative framework where:</p>
<p><span class="math inline">\(L/n \to \alpha \in (0, \infty) \quad \text{as } n \to \infty\)</span></p>
<p>Under Bekker asymptotics:</p>
<ul>
<li>2SLS is <strong>biased and inconsistent</strong> unless the instruments are very strong.</li>
<li>The bias grows with <span class="math inline">\(\alpha\)</span>, approaching that of OLS.</li>
</ul>
<p>This formalized the intuition that adding more instruments—especially weak ones—does not help, and can actually harm estimation.</p>
<hr>
<div id="sources-of-many-iv-bias" class="section level4" number="34.7.2.1">
<h4>
<span class="header-section-number">34.7.2.1</span> Sources of Many-IV Bias<a class="anchor" aria-label="anchor" href="#sources-of-many-iv-bias"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li><a href="sec-instrumental-variables.html#sec-weak-instruments-problem"><strong>Weak Instruments</strong></a></li>
</ol>
<p>Many instruments are individually weak (i.e., contribute little to explaining <span class="math inline">\(x\)</span>), and collectively, they inflate the projection without improving identification.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Overfitting the First Stage</strong></li>
</ol>
<p>With too many instruments, the first-stage regression captures random noise, leading to poor out-of-sample performance and contamination of the second stage.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Endogeneity Leakage</strong></li>
</ol>
<p>Overfit first-stage predictions may reintroduce endogeneity due to incidental correlation with the structural error term.</p>
<hr>
</div>
<div id="diagnostic-tools" class="section level4" number="34.7.2.2">
<h4>
<span class="header-section-number">34.7.2.2</span> Diagnostic Tools<a class="anchor" aria-label="anchor" href="#diagnostic-tools"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>First-Stage F-Statistic</li>
</ol>
<p>A weak instrument test: rule of thumb is <span class="math inline">\(F &gt; 10\)</span> for a single instrument; more stringent thresholds apply for many instruments.</p>
<ol start="2" style="list-style-type: decimal">
<li>Overidentification Tests</li>
</ol>
<ul>
<li>
<strong>Sargan Test</strong>: Assumes homoskedastic errors</li>
<li>
<strong>Hansen’s J-Test</strong>: Robust to heteroskedasticity</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Eigenvalue Diagnostics</li>
</ol>
<ul>
<li>Kleibergen-Paap rk statistic (generalized for clustered or heteroskedastic settings)</li>
</ul>
<hr>
</div>
<div id="remedies-and-alternatives" class="section level4" number="34.7.2.3">
<h4>
<span class="header-section-number">34.7.2.3</span> Remedies and Alternatives<a class="anchor" aria-label="anchor" href="#remedies-and-alternatives"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>Instrument Selection</li>
</ol>
<ul>
<li>
<strong>Lasso IV / Post-Double Selection</strong>: Use regularization to select valid instruments.</li>
<li>
<strong>Factor-Based Methods</strong>: Project instruments onto principal components.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Shrinkage Estimators</li>
</ol>
<ul>
<li>
<strong>Limited Information Maximum Likelihood (LIML)</strong>: More robust to many-IV bias.</li>
<li>
<strong>Jackknife IV Estimator (JIVE)</strong>: Adjusts for the overfitting bias in the first stage.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Grouped or Aggregated Instruments</li>
</ol>
<ul>
<li>Collapse multiple instruments into a smaller number of aggregated measures.</li>
</ul>
<hr>
</div>
<div id="practical-guidelines" class="section level4" number="34.7.2.4">
<h4>
<span class="header-section-number">34.7.2.4</span> Practical Guidelines<a class="anchor" aria-label="anchor" href="#practical-guidelines"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Avoid Including All Possible Instruments</strong>: Parsimony matters more than volume.</li>
<li>
<strong>Always Check First-Stage Strength</strong>: Even if <span class="math inline">\(R^2\)</span> is high, individual instrument strength matters.</li>
<li>
<strong>Report Robustness with Alternative Estimators</strong>: LIML or JIVE can serve as robustness checks.</li>
<li>
<strong>Test for Overidentification</strong>: But interpret results cautiously when <span class="math inline">\(L\)</span> is large.</li>
</ol>
<hr>
<!-- ## Application in Marketing --><!-- ### Peer-based IV -->
</div>
</div>
<div id="heterogeneous-effects-in-iv-estimation" class="section level3" number="34.7.3">
<h3>
<span class="header-section-number">34.7.3</span> Heterogeneous Effects in IV Estimation<a class="anchor" aria-label="anchor" href="#heterogeneous-effects-in-iv-estimation"><i class="fas fa-link"></i></a>
</h3>
<div id="constant-vs.-heterogeneous-treatment-effects" class="section level4" number="34.7.3.1">
<h4>
<span class="header-section-number">34.7.3.1</span> Constant vs. Heterogeneous Treatment Effects<a class="anchor" aria-label="anchor" href="#constant-vs.-heterogeneous-treatment-effects"><i class="fas fa-link"></i></a>
</h4>
<p>The standard instrumental variables framework assumes that the causal effect of an endogenous regressor <span class="math inline">\(D_i\)</span> on an outcome <span class="math inline">\(Y_i\)</span> is constant across individuals, i.e.:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 D_i + u_i
\]</span></p>
<p>This implies homogeneous treatment effects, where <span class="math inline">\(\beta_1\)</span> is a structural parameter that applies uniformly to all individuals <span class="math inline">\(i\)</span> in the population. We refer to this as the homogeneous treatment effects model, and it underlies the traditional IV assumptions:</p>
<ul>
<li><p>Linearity with a constant effect <span class="math inline">\(\beta_1\)</span>.</p></li>
<li><p>Instrument relevance: <span class="math inline">\(\mathrm{Cov}(Z_i, D_i) \ne 0\)</span>.</p></li>
<li><p>Instrument exogeneity: <span class="math inline">\(\mathrm{Cov}(Z_i, u_i) = 0\)</span>.</p></li>
</ul>
<p>Under these assumptions, the IV estimator <span class="math inline">\(\hat{\beta}_1^{IV}\)</span> consistently estimates the causal effect <span class="math inline">\(\beta_1\)</span>.</p>
</div>
<div id="heterogeneous-treatment-effects-and-the-problem-for-iv" class="section level4" number="34.7.3.2">
<h4>
<span class="header-section-number">34.7.3.2</span> Heterogeneous Treatment Effects and the Problem for IV<a class="anchor" aria-label="anchor" href="#heterogeneous-treatment-effects-and-the-problem-for-iv"><i class="fas fa-link"></i></a>
</h4>
<p>In practice, treatment effects often vary across individuals. That is, the effect of <span class="math inline">\(D_i\)</span> on <span class="math inline">\(Y_i\)</span> depends on the individual’s characteristics or other unobserved factors:</p>
<p><span class="math display">\[
Y_i = \beta_{1i} D_i + u_i
\]</span></p>
<p>Here, <span class="math inline">\(\beta_{1i}\)</span> represents the individual-specific causal effect, and the population <a href="sec-causal-inference.html#sec-average-treatment-effect">Average Treatment Effect</a> is:</p>
<p><span class="math display">\[
ATE = \mathbb{E}[\beta_{1i}]
\]</span></p>
<p>In the presence of treatment effect heterogeneity, the IV estimator <span class="math inline">\(\hat{\beta}_1^{IV}\)</span> does not, in general, estimate the ATE. Instead, it estimates a weighted average of the heterogeneous treatment effects, with weights determined by the instrumental variation in the data.</p>
<p>This distinction is critical:</p>
<ul>
<li><p>OLS estimates a weighted average treatment effect, with weights depending on the variance of <span class="math inline">\(D_i\)</span>.</p></li>
<li><p>IV estimates a [Local Average Treatment Effect] (LATE), depending on the instrument <span class="math inline">\(Z_i\)</span>.</p></li>
</ul>
<p>When there is one endogenous regressor <span class="math inline">\(D_i\)</span> and one instrument <span class="math inline">\(Z_i\)</span>, both binary variables, we can interpret the IV estimator as the [Local Average Treatment Effect] under specific assumptions. The setup is:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_{1i} D_i + u_i
\]</span></p>
<ul>
<li>
<span class="math inline">\(D_i \in \{0, 1\}\)</span>: The treatment indicator.</li>
<li>
<span class="math inline">\(Z_i \in \{0, 1\}\)</span>: The binary instrument.</li>
</ul>
<p>Assumptions for the LATE Interpretation</p>
<ol style="list-style-type: decimal">
<li><a href="sec-instrumental-variables.html#sec-exogeneity-assumption">Instrument Exogeneity</a></li>
</ol>
<p><span class="math display">\[
Z_i \perp (u_i, v_i)
\]</span> - The instrument is as good as randomly assigned, and is independent of both the structural error term <span class="math inline">\(u_i\)</span> and the unobserved determinants <span class="math inline">\(v_i\)</span> that affect treatment selection.</p>
<ol start="2" style="list-style-type: decimal">
<li><a href="sec-instrumental-variables.html#sec-relevance-assumption">Relevance</a></li>
</ol>
<p><span class="math display">\[
\mathbb{P}(D_i = 1 | Z_i = 1) \ne \mathbb{P}(D_i = 1 | Z_i = 0)
\]</span> - The instrument must affect the likelihood of receiving treatment <span class="math inline">\(D_i\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>
<a href="sec-instrumental-variables.html#sec-monotonicity-assumption">Monotonicity</a> <span class="citation">(<a href="references.html#ref-imbens1994identification">G. W. Imbens and Angrist 1994</a>)</span>
</li>
</ol>
<p><span class="math display">\[
D_i(1) \ge D_i(0) \quad \forall i
\]</span></p>
<ul>
<li><p>There are no defiers: no individual who takes the treatment when <span class="math inline">\(Z_i = 0\)</span> but does not take it when <span class="math inline">\(Z_i = 1\)</span>.</p></li>
<li><p>Monotonicity is not testable, but must be defended on theoretical grounds.</p></li>
</ul>
<p>Under these assumptions, <span class="math inline">\(\hat{\beta}_1^{IV}\)</span> estimates the [Local Average Treatment Effect]:</p>
<p><span class="math display">\[
LATE = \mathbb{E}[\beta_{1i} | \text{Compliers}]
\]</span></p>
<ul>
<li>Compliers are individuals who receive the treatment when <span class="math inline">\(Z_i = 1\)</span>, but not when <span class="math inline">\(Z_i = 0\)</span>.</li>
<li>Local refers to the fact that the estimate pertains to this specific subpopulation of compliers.</li>
</ul>
<p><strong>Implications</strong>:</p>
<ul>
<li>The LATE is not the ATE, unless treatment effects are homogeneous, or the complier subpopulation is representative of the entire population.</li>
<li>Different instruments define different complier groups, leading to different LATEs.</li>
</ul>
</div>
<div id="multiple-instruments-and-multiple-lates" class="section level4" number="34.7.3.3">
<h4>
<span class="header-section-number">34.7.3.3</span> Multiple Instruments and Multiple LATEs<a class="anchor" aria-label="anchor" href="#multiple-instruments-and-multiple-lates"><i class="fas fa-link"></i></a>
</h4>
<p>When we have multiple instruments <span class="math inline">\(Z_i^{(1)}, Z_i^{(2)}, \dots, Z_i^{(m)}\)</span>, each can induce different complier groups:</p>
<ul>
<li><p>Each instrument has its own LATE, corresponding to its own group of compliers.</p></li>
<li><p>If heterogeneous treatment effects exist, these LATEs may differ.</p></li>
</ul>
<p>In an overidentified model, where <span class="math inline">\(m &gt; k\)</span>, the 2SLS estimator imposes the assumption that all instruments identify the same causal effect <span class="math inline">\(\beta_1\)</span>. This leads to the moment conditions:</p>
<p><span class="math display">\[
\mathbb{E}[Z_i^{(j)}(Y_i - D_i \beta_1)] = 0 \quad \forall j = 1, \dots, m
\]</span></p>
<p>If instruments identify different LATEs:</p>
<ul>
<li><p>These moment conditions can be inconsistent with one another.</p></li>
<li><p>The Sargan-Hansen J-test may reject, even though each instrument is valid (i.e., exogenous and relevant).</p></li>
</ul>
<blockquote>
<p><strong>Key Insight</strong>: The J-test rejects because the homogeneity assumption is violated—not because instruments are invalid in the exogeneity sense.</p>
</blockquote>
</div>
<div id="illustration-multiple-instruments-different-lates" class="section level4" number="34.7.3.4">
<h4>
<span class="header-section-number">34.7.3.4</span> Illustration: Multiple Instruments, Different LATEs<a class="anchor" aria-label="anchor" href="#illustration-multiple-instruments-different-lates"><i class="fas fa-link"></i></a>
</h4>
<p>Consider the following example:</p>
<ul>
<li><p><span class="math inline">\(Z_i^{(1)}\)</span> identifies a LATE of 1.0.</p></li>
<li><p><span class="math inline">\(Z_i^{(2)}\)</span> identifies a LATE of 2.0.</p></li>
<li>
<p>If both instruments are included in an overidentified IV model, the 2SLS estimator tries to reconcile these LATEs as if they were identifying the same <span class="math inline">\(\beta_1\)</span>, leading to:</p>
<ul>
<li><p>An average of these LATEs.</p></li>
<li><p>A possible rejection of the overidentification restrictions via the J-test.</p></li>
</ul>
</li>
</ul>
<p>This scenario is common in:</p>
<ul>
<li><p><strong>Labor economics</strong> (e.g., different instruments for education identify different populations).</p></li>
<li><p><strong>Marketing and pricing experiments</strong> (e.g., different price instruments impact different customer segments).</p></li>
</ul>
</div>
<div id="practical-implications-for-empirical-research" class="section level4" number="34.7.3.5">
<h4>
<span class="header-section-number">34.7.3.5</span> Practical Implications for Empirical Research<a class="anchor" aria-label="anchor" href="#practical-implications-for-empirical-research"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>Be Clear About Whose Effect You’re Estimating</li>
</ol>
<ul>
<li>Different instruments often imply different complier groups.</li>
<li>Understanding who the compliers are is essential for policy implications.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Interpret the J-Test Carefully</li>
</ol>
<ul>
<li>A rejection may indicate treatment effect heterogeneity, not necessarily instrument invalidity.</li>
<li>Supplement the J-test with:
<ul>
<li>Subgroup analysis.</li>
<li>Sensitivity analysis.</li>
<li>Local Instrumental Variable or Marginal Treatment Effects frameworks.</li>
</ul>
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Use Structural Models When Needed</li>
</ol>
<ul>
<li>If you need an ATE, consider parametric or semi-parametric structural models that explicitly model heterogeneity.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Don’t Assume LATE = ATE</li>
</ol>
<ul>
<li>Be cautious in generalizing LATE estimates beyond the complier subpopulation.</li>
</ul>
</div>
<div id="beyond-late" class="section level4" number="34.7.3.6">
<h4>
<span class="header-section-number">34.7.3.6</span> Beyond LATE<a class="anchor" aria-label="anchor" href="#beyond-late"><i class="fas fa-link"></i></a>
</h4>
<p>The presence of heterogeneous treatment effects (<span class="math inline">\(\beta_{1i}\)</span> varying across individuals) raises a fundamental challenge for causal inference using IV methods. As we have seen, the traditional IV estimator identifies the [Local Average Treatment Effect] (LATE) under certain assumptions. However, this approach implicitly adopts a <strong>reverse engineering</strong> strategy: it uses classical linear IV estimators designed under homogeneity, acknowledges their likely misspecification in the presence of unobserved heterogeneity, and interprets the resulting estimate in terms of a LATE.</p>
<p>This strategy has been highly influential and remains central to empirical work. Nevertheless, it comes with limitations:</p>
<ul>
<li>The interpretation depends critically on the specific instrument used (i.e., the definition of the complier group).</li>
<li>It cannot recover the <a href="sec-causal-inference.html#sec-average-treatment-effect">Average Treatment Effect</a> (ATE) or other policy-relevant parameters unless strong additional assumptions hold.</li>
</ul>
</div>
<div id="forward-engineering-the-marginal-treatment-effect" class="section level4" number="34.7.3.7">
<h4>
<span class="header-section-number">34.7.3.7</span> Forward Engineering: The Marginal Treatment Effect<a class="anchor" aria-label="anchor" href="#forward-engineering-the-marginal-treatment-effect"><i class="fas fa-link"></i></a>
</h4>
<p>In contrast, recent work—including that of <span class="citation">Mogstad and Torgovitsky (<a href="references.html#ref-mogstad2024instrumental">2024</a>)</span> —emphasizes a <strong>forward engineering</strong> approach. Rather than adapting estimators designed under homogeneity, this strategy builds models and estimators that explicitly allow for unobserved heterogeneity in treatment effects from the outset.</p>
<p>A key framework in this approach is the <strong>Marginal Treatment Effect (MTE)</strong>, originally developed in the context of selection models <span class="citation">(<a href="references.html#ref-gronau1974wage">Gronau 1974</a>; <a href="references.html#ref-heckman1979sample">J. J. Heckman 1979</a>)</span>. The idea is to model the treatment decision as the result of a latent index:</p>
<p><span class="math display">\[
D_i = \mathbb{1}[v_i \leq Z_i'\pi + \eta_i]
\]</span></p>
<p>and to let the treatment effect vary with unobserved selection variables. The MTE is then defined as:</p>
<p><span class="math display">\[
\text{MTE}(u) = \mathbb{E}[Y_i(1) - Y_i(0) | U_i = u]
\]</span></p>
<p>where <span class="math inline">\(U_i\)</span> is the latent variable governing treatment selection. This function traces out how the treatment effect varies across individuals with different propensities to receive treatment, and it underlies other average effects such as:</p>
<ul>
<li>ATE: <span class="math inline">\(\int_0^1 \text{MTE}(u) \, du\)</span>
</li>
<li>LATE: average of MTE over complier margin</li>
<li>TT and TUT: other weighted averages of MTE</li>
</ul>
<p>Comparison of IV, LATE, and MTE Approaches</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="26%">
<col width="23%">
<col width="51%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Traditional IV (LATE)</th>
<th>MTE / Selection Models</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Assumes constant effects</td>
<td>Implicitly violated</td>
<td>Explicitly allows heterogeneity</td>
</tr>
<tr class="even">
<td>Interpretation</td>
<td>LATE for compliers</td>
<td>MTE curve + all average treatment effects</td>
</tr>
<tr class="odd">
<td>Data requirements</td>
<td>Instrument + outcome</td>
<td>Richer variation in instrument (e.g., continuous)</td>
</tr>
<tr class="even">
<td>Estimation complexity</td>
<td>Low (2SLS)</td>
<td>Higher (requires modeling selection)</td>
</tr>
</tbody>
</table></div>
<p>The MTE framework also connects to:</p>
<ul>
<li>
<a href="sec-instrumental-variables.html#sec-control-function-approach"><strong>Control function methods</strong></a>: which account for selection via inclusion of latent variables (e.g., residuals) in outcome equations.</li>
<li>
<strong>Partial identification / bounding methods</strong>: which avoid strong parametric assumptions and seek informative bounds on treatment effects.</li>
</ul>
<p>These newer strategies reflect a shift in modern econometrics: away from treating unobserved heterogeneity as a nuisance, and toward modeling it directly for richer causal inference.</p>
<p>Understanding these two strategies helps practitioners choose appropriate methods based on:</p>
<ul>
<li>Their identifying assumptions.</li>
<li>The richness of their instruments.</li>
<li>Their target estimand (e.g., ATE, LATE, MTE).</li>
<li>Their willingness to model the selection process.</li>
</ul>
<p>Researchers should be cautious in interpreting IV estimates as general causal effects, especially when heterogeneous treatment effects are likely and the choice of instrument strongly influences the complier population.</p>
<hr>
</div>
</div>
<div id="zero-valued-outcomes" class="section level3" number="34.7.4">
<h3>
<span class="header-section-number">34.7.4</span> Zero-Valued Outcomes<a class="anchor" aria-label="anchor" href="#zero-valued-outcomes"><i class="fas fa-link"></i></a>
</h3>
<p>For outcomes that take zero values, log transformations can introduce interpretation issues. Specifically, the coefficient on a log-transformed outcome does not directly represent a percentage change <span class="citation">(<a href="references.html#ref-chen2024logs">J. Chen and Roth 2024</a>)</span>. We have to distinguish the treatment effect on the intensive (outcome: 10 to 11) vs. extensive margins (outcome: 0 to 1), and we can’t readily interpret the treatment coefficient of log-transformed outcome regression as percentage change. In such cases, researchers use alternative methods:</p>
<div id="proportional-late-estimation" class="section level4" number="34.7.4.1">
<h4>
<span class="header-section-number">34.7.4.1</span> Proportional LATE Estimation<a class="anchor" aria-label="anchor" href="#proportional-late-estimation"><i class="fas fa-link"></i></a>
</h4>
<p>When dealing with zero-valued outcomes, direct log transformations can lead to interpretation issues. To obtain an interpretable percentage change in the outcome due to treatment among <em>compliers</em>, we estimate the <strong>proportional Local Average Treatment Effect (LATE)</strong>, denoted as <span class="math inline">\(\theta_{ATE\%}\)</span>.</p>
<p>Steps to Estimate Proportional LATE:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Estimate LATE using 2SLS:</strong></p>
<p>We first estimate the treatment effect using a standard Two-Stage Least Squares regression: <span class="math display">\[ Y_i = \beta D_i + X_i + \epsilon_i, \]</span> where:</p>
<ul>
<li>
<span class="math inline">\(D_i\)</span> is the endogenous treatment variable.</li>
<li>
<span class="math inline">\(X_i\)</span> includes any exogenous controls.</li>
<li>
<span class="math inline">\(\beta\)</span> represents the LATE in <em>levels</em> for the mean of the control group’s compliers.</li>
</ul>
</li>
<li>
<p><strong>Estimate the control complier mean</strong> (<span class="math inline">\(\beta_{cc}\)</span>):</p>
<p>Using the same 2SLS setup, we estimate the control mean for compliers by transforming the outcome variable <span class="citation">(<a href="references.html#ref-abadie2002instrumental">Abadie, Angrist, and Imbens 2002</a>)</span>: <span class="math display">\[ Y_i^{CC} = -(D_i - 1) Y_i. \]</span> The estimated coefficient from this regression, <span class="math inline">\(\beta_{cc}\)</span>, captures the mean outcome for compliers in the control group.</p>
</li>
<li>
<p><strong>Compute the proportional LATE:</strong></p>
<p>The estimated proportional LATE is given by: <span class="math display">\[ \theta_{ATE\%} = \frac{\hat{\beta}}{\hat{\beta}_{cc}}, \]</span> which provides a direct <em>percentage change</em> interpretation for the outcome among compliers induced by the instrument.</p>
</li>
<li>
<p><strong>Obtain standard errors via non-parametric bootstrap:</strong></p>
<p>Since <span class="math inline">\(\theta_{ATE\%}\)</span> is a ratio of estimated coefficients, standard errors are best obtained using non-parametric bootstrap methods.</p>
</li>
<li>
<p><strong>Special case: Binary instrument</strong></p>
<p>If the instrument is binary, <span class="math inline">\(\theta_{ATE\%}\)</span> for the intensive margin of compliers can be directly estimated using <strong>Poisson IV regression</strong> (<code>ivpoisson</code> in Stata).</p>
</li>
</ol>
</div>
<div id="bounds-on-intensive-margin-effects" class="section level4" number="34.7.4.2">
<h4>
<span class="header-section-number">34.7.4.2</span> Bounds on Intensive-Margin Effects<a class="anchor" aria-label="anchor" href="#bounds-on-intensive-margin-effects"><i class="fas fa-link"></i></a>
</h4>
<p><span class="citation">Lee (<a href="references.html#ref-lee2009training">2009</a>)</span> proposed a bounding approach for intensive-margin effects, assuming that compliers always have positive outcomes regardless of treatment (i.e., intensive-margin effect). These bounds help estimate treatment effects without relying on log transformations. However, this requires a monotonicity assumption for compliers where they should still have positive outcome regardless of treatment status.</p>
</div>
</div>
</div>
<div id="types-of-iv" class="section level2" number="34.8">
<h2>
<span class="header-section-number">34.8</span> Types of IV<a class="anchor" aria-label="anchor" href="#types-of-iv"><i class="fas fa-link"></i></a>
</h2>
<div id="treatment-intensity" class="section level3" number="34.8.1">
<h3>
<span class="header-section-number">34.8.1</span> Treatment Intensity<a class="anchor" aria-label="anchor" href="#treatment-intensity"><i class="fas fa-link"></i></a>
</h3>
<p>Two-Stage Least Squares is a powerful method for estimating the average causal effect when treatment intensity varies across units. Rather than simple binary treatment (treated vs. untreated), many empirical applications involve treatments that can take on a range of values. TSLS can identify causal effects in these settings, capturing “a weighted average of per-unit treatment effects along the length of a causal response function” <span class="citation">(<a href="references.html#ref-angrist1995two">J. D. Angrist and Imbens 1995, 431</a>)</span>.</p>
<p>Common examples of treatment intensity include:</p>
<ul>
<li>
<strong>Drug dosage</strong> (e.g., milligrams administered)</li>
<li>
<strong>Hours of exam preparation</strong> on test performance <span class="citation">(<a href="references.html#ref-powers1984effects">Powers and Swinton 1984</a>)</span>
</li>
<li>
<strong>Cigarette consumption</strong> (e.g., packs per day) on infant birth weights <span class="citation">(<a href="references.html#ref-permutt1989simultaneous">Permutt and Hebel 1989</a>)</span>
</li>
<li>
<strong>Years of education</strong> and their effect on earnings</li>
<li>
<strong>Class size</strong> and its impact on student test scores <span class="citation">(<a href="references.html#ref-angrist1999using">J. D. Angrist and Lavy 1999</a>)</span>
</li>
<li>
<strong>Sibship size</strong> and later-life earnings <span class="citation">(<a href="references.html#ref-angrist2010multiple">J. Angrist, Lavy, and Schlosser 2010</a>)</span>
</li>
<li>
<strong>Social media adoption intensity</strong> (e.g., time spent, number of platforms)</li>
</ul>
<p>The average causal effect here refers to the conditional expectation of the difference in outcomes between the treated unit (at a given treatment intensity) and what would have happened in the counterfactual scenario (at a different treatment intensity). Importantly:</p>
<ul>
<li>Linearity is not required in the relationships between the dependent variable, treatment intensities, and instruments. TSLS can accommodate nonlinear causal response functions, provided the assumptions of the method hold.</li>
</ul>
<hr>
<div id="example-schooling-and-earnings" class="section level4" number="34.8.1.1">
<h4>
<span class="header-section-number">34.8.1.1</span> Example: Schooling and Earnings<a class="anchor" aria-label="anchor" href="#example-schooling-and-earnings"><i class="fas fa-link"></i></a>
</h4>
<p>In their seminal paper, <span class="citation">J. D. Angrist and Imbens (<a href="references.html#ref-angrist1995two">1995</a>)</span> estimate the causal effect of <strong>years of schooling</strong> on <strong>earnings</strong>, using <strong>quarter of birth</strong> as an instrumental variable. The intuition is that individuals born in different quarters are subject to different compulsory schooling laws, which affect educational attainment but are plausibly unrelated to unobserved ability or motivation (the typical omitted variables in this context).</p>
<p>The structural outcome equation is:</p>
<p><span class="math display">\[
Y = \gamma_0 + \gamma_1 X_1 + \rho S + \varepsilon
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(Y\)</span> is the log of earnings (the dependent variable)</li>
<li>
<span class="math inline">\(S\)</span> is years of schooling (the endogenous regressor)</li>
<li>
<span class="math inline">\(X_1\)</span> is a vector (or matrix) of exogenous covariates (e.g., demographic characteristics)</li>
<li>
<span class="math inline">\(\rho\)</span> is the causal return to schooling we wish to estimate</li>
<li>
<span class="math inline">\(\varepsilon\)</span> is the error term, capturing unobserved factors</li>
</ul>
<p>Because schooling <span class="math inline">\(S\)</span> may be endogenous (e.g., correlated with <span class="math inline">\(\varepsilon\)</span>), we model its first-stage relationship with the exogenous variables and instruments:</p>
<p><span class="math display">\[
S = \delta_0 + X_1 \delta_1 + X_2 \delta_2 + \eta
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(X_2\)</span> represents the instrumental variables (e.g., quarter of birth)</li>
<li>
<span class="math inline">\(\delta_2\)</span> is the coefficient on the instrument</li>
<li>
<span class="math inline">\(\eta\)</span> is the first-stage error term</li>
</ul>
<p>The Two-Stage Procedure</p>
<ol style="list-style-type: decimal">
<li>
<strong>First Stage Regression</strong><br>
Regress <span class="math inline">\(S\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> to obtain the predicted (fitted) values <span class="math inline">\(\hat{S}\)</span>.</li>
</ol>
<p><span class="math display">\[
\hat{S} = \widehat{\delta_0} + X_1 \widehat{\delta_1} + X_2 \widehat{\delta_2}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>
<strong>Second Stage Regression</strong><br>
Replace <span class="math inline">\(S\)</span> with <span class="math inline">\(\hat{S}\)</span> in the structural equation and estimate:</li>
</ol>
<p><span class="math display">\[
Y = \gamma_0 + \gamma_1 X_1 + \rho \hat{S} + \nu
\]</span></p>
<p>where <span class="math inline">\(\nu\)</span> is the new error term (different from <span class="math inline">\(\varepsilon\)</span> because <span class="math inline">\(\hat{S}\)</span> is constructed to be exogenous).</p>
<p>Under the standard IV assumptions, <span class="math inline">\(\rho\)</span> is a consistent estimator of the causal effect of schooling on earnings.</p>
<hr>
</div>
<div id="causal-interpretation-of-rho" class="section level4" number="34.8.1.2">
<h4>
<span class="header-section-number">34.8.1.2</span> Causal Interpretation of <span class="math inline">\(\rho\)</span><a class="anchor" aria-label="anchor" href="#causal-interpretation-of-rho"><i class="fas fa-link"></i></a>
</h4>
<p>For <span class="math inline">\(\rho\)</span> to have a valid causal interpretation, two key assumptions are essential:</p>
<ol style="list-style-type: decimal">
<li>SUTVA (<a href="sec-quasi-experimental.html#sec-sutva">Stable Unit Treatment Value Assumption</a>)
<ul>
<li>The potential outcomes of each individual are not affected by the treatment assignments of other units.</li>
<li>There are no hidden variations of the treatment; “one year of schooling” means the same treatment type across individuals.</li>
<li>While important, SUTVA is often assumed without extensive defense in empirical work, though violations (e.g., spillovers in education settings) should be acknowledged when plausible.</li>
</ul>
</li>
<li>[Local Average Treatment Effect] (LATE)
<ul>
<li>TSLS identifies a weighted average of marginal effects at the points where the instrument induces variation in treatment intensity.</li>
<li>Formally, <span class="math inline">\(\rho\)</span> converges in probability to a weighted average of causal increments:</li>
</ul>
</li>
</ol>
<p><span class="math display">\[
\text{plim } \hat{\rho} = \sum_j w_j \cdot E[Y_j - Y_{j-1} \mid \text{Compliers at level } j]
\]</span></p>
<p>where <span class="math inline">\(w_j\)</span> are weights determined by the distribution of the instrument and treatment intensity.</p>
<ul>
<li>This LATE interpretation means that TSLS estimates apply to compliers whose treatment intensity changes in response to the instrument. If there is heterogeneity in treatment effects across units, the interpretation of <span class="math inline">\(\rho\)</span> becomes instrument-dependent and may not generalize to the entire population.</li>
</ul>
<hr>
</div>
</div>
<div id="decision-maker-iv" class="section level3" number="34.8.2">
<h3>
<span class="header-section-number">34.8.2</span> Decision-Maker IV<a class="anchor" aria-label="anchor" href="#decision-maker-iv"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Examiner designs</strong>, <strong>judge IV designs</strong>, and <strong>leniency IV</strong> refer to a family of instrumental variable strategies that exploit quasi-random assignment of decision-makers (such as judges or examiners) to observational units. These designs are used to identify causal effects in settings where controlled experiments are not feasible.</p>
<p><strong>Examiner/judge IV design</strong> is an approach where the instrument is the identity or behavior of an assigned decision-maker (an “examiner” or a judge). The classic setup arises in courts: cases are typically assigned to judges in a manner that is as good as random (often conditional on timing or location), and different judges have systematically different propensities to rule harshly or leniently. This means that, purely by the luck of the draw, otherwise-similar individuals may receive different treatments (e.g. a longer vs. shorter sentence) depending on which judge they happen to get. In such a design, the judge assignment (or a function of it) serves as an instrumental variable for the treatment of interest (like sentence length). The key insight is that <em>who</em> the examiner/judge is can be treated as an exogenous shock that influences the treatment but is (ideally) unrelated to the person’s own characteristics.</p>
<p>The term <strong>judge IV design</strong> specifically refers to using judges in legal settings as instruments. This approach rose to prominence through studies of the criminal justice system; a well-known early example is <span class="citation">Kling (<a href="references.html#ref-kling2006incarceration">2006</a>)</span>, who used randomly assigned judges to instrument for incarceration length when studying its effect on later earnings. More generally, the literature often calls this the <strong>“judge leniency” design</strong>, because it leverages differences in judges’ leniency/harshness. Importantly, the same idea extends beyond literal judges. <em>Examiners</em> in various administrative or medical contexts can play an analogous role. For instance, bureaucrats evaluating benefit claims, patent examiners reviewing applications, or physicians making discretionary treatment decisions can all act like “judges” whose assignment is as-good-as-random and whose leniency varies. In these non-court contexts, researchers sometimes use the term <strong>examiner design</strong> as a more general label, but it is essentially the same IV strategy. In summary, whether we say <em>examiner design</em>, <em>judge IV</em>, or <em>judge leniency IV</em>, we are usually referring to the same identification strategy – using the quasi-random assignment of a decision-maker with varying tendencies as an instrument.</p>
<p><strong>How the design is structured:</strong> In practice, one can implement this IV in a couple of ways. One method is to include dummy variables for each judge/examiner as instruments (since each judge is a distinct source of variation). Another common approach is to construct a <strong>leniency measure</strong> for each decision-maker – for example, the judge’s historical rate of granting the treatment – and use that as a single continuous instrument. The latter approach (using a summary measure of leniency) is popular because it reduces dimensionality and mitigates weak-instrument concerns when there are many judges. For instance, instead of having 50 separate judge dummies, one can calculate each judge’s leave-one-out approval or sentencing rate and use that number as the instrument. This “leave-one-out” or jackknife approach ensures the measure for each judge is calculated excluding the case in question (avoiding mechanical endogeneity). Overall, the examiner/judge IV design turns the naturally occurring randomness in examiner assignment into a source of exogenous variation: who you were randomly assigned to becomes the instrument for the treatment you received.</p>
<div id="achieving-identification-with-a-leniency-iv" class="section level4" number="34.8.2.1">
<h4>
<span class="header-section-number">34.8.2.1</span> Achieving Identification with a Leniency IV<a class="anchor" aria-label="anchor" href="#achieving-identification-with-a-leniency-iv"><i class="fas fa-link"></i></a>
</h4>
<p>The examiner/judge design is a powerful way to achieve identification in observational data. It rests on the core requirements for a valid instrumental variable:</p>
<ul>
<li>
<p><strong>Quasi-Random Assignment (Exogeneity):</strong> Because examiners or judges are assigned to cases essentially at random (often by rotation, scheduling, or lottery), the particular decision-maker an individual gets is independent of that individual’s characteristics. This approximates the randomness of an experiment. As long as assignment is truly random (or as-good-as-random after conditioning on any known factors like time or location), the examiner identity is uncorrelated with unobserved confounders. In other words, which judge you draw should have no direct bearing on your outcome except through the judge’s decision. This satisfies the <strong>exogeneity</strong> condition for an IV.</p>
<ul>
<li><p><strong>Discretion Over a Binary Treatment</strong>: Each decision maker has discretionary authority over a treatment variable <span class="math inline">\(D_i\)</span>, typically binary (e.g., pretrial release vs. detention).</p></li>
<li><p><strong>Heterogeneity in Behavior</strong>: Decision makers differ systematically in their propensity to assign treatment, allowing us to use these differences as instrumental variation.</p></li>
</ul>
</li>
<li><p><strong>Instrument Relevance:</strong> Different examiners have different propensities to deliver the treatment. Some judges are more severe (more likely to incarcerate or give long sentences), while others are more lenient; some doctors are more likely to prescribe an intensive treatment, etc. This translates into substantial variation in the probability of treatment based solely on who the case was assigned to. For example, in the patent context, being assigned a lenient patent examiner vs. a strict one can significantly change the probability of a patent grant <span class="citation">(<a href="references.html#ref-farre2020patent">Farre-Mensa, Hegde, and Ljungqvist 2020</a>)</span>.</p></li>
<li><p><strong>Exclusion Restriction:</strong> The IV assumption is that the assigned examiner affects the outcome <em>only</em> through the treatment itself. In a judge design, this means the “type of judge you are assigned” should impact the defendant’s future outcomes solely via the judge’s decision (e.g. incarceration or release), not through any other channel. For instance, a harsh judge might send you to prison; a lenient judge might not – that difference can affect your future, but we assume that it’s only the incarceration that matters for your future outcome, not any direct effect of interacting with a harsh vs. nice judge per se. This exclusion restriction is more plausible when the decision-maker has no direct interaction with the individual beyond making the decision. Researchers take care to argue that conditional on the controls and the treatment itself, the identity of the examiner has no independent effect on outcomes. If these conditions (relevance and exogeneity/exclusion) hold, then the variation in treatment induced by examiner assignment can be used to consistently estimate the causal effect of the treatment.</p></li>
</ul>
<p>By meeting these conditions, examiner/judge IV designs create a natural experiment. Essentially, they compare outcomes between individuals who, by random luck, received different treatment assignments (e.g. one was incarcerated, another not) due to differing examiner leniency, despite those individuals being comparable in expectation. This helps isolate the causal impact of the treatment from confounding factors. Notably, the estimates from such designs often correspond to a <strong>local average treatment effect (LATE)</strong> for those cases whose treatment status is swayed by the examiner’s leniency – for example, the “marginal” defendants who would be incarcerated by a strict judge but released by a lenient judge. In sum, these designs allow researchers to <strong>mimic a randomized experiment</strong> within observational data by leveraging institutional randomness (who gets assigned to whom) as an instrument.</p>
</div>
<div id="leniency-iv-clarifying-the-terminology" class="section level4" number="34.8.2.2">
<h4>
<span class="header-section-number">34.8.2.2</span> Leniency IV: Clarifying the Terminology<a class="anchor" aria-label="anchor" href="#leniency-iv-clarifying-the-terminology"><i class="fas fa-link"></i></a>
</h4>
<p>The term <strong>leniency IV</strong> refers to this same instrumental variable strategy, emphasizing the role of the examiner’s <em>leniency</em> (or strictness). In many studies, the instrument is literally a measure of how lenient the assigned judge or examiner tends to be. For example, in a Social Security Disability study, researchers <em>“exploit variation in examiners’ allowance rates as an instrument for benefit receipt.”</em> <span class="citation">(<a href="references.html#ref-maestas2013does">Maestas, Mullen, and Strand 2013</a>)</span>. Here, an examiner’s <em>allowance rate</em> (the fraction of cases they approve) is a direct quantification of their leniency, and this serves as the instrumental variable. Similarly, one can define a judge’s leniency as the percentage of past defendants that judge jailed or the average sentence length they give, and use that as the IV. The phrase “leniency design” or <strong>leniency instrument</strong> simply underscores that it’s the lenient vs. strict tendencies of the decision-maker that provide the exogenous variation.</p>
<p>A leniency IV design typically involves constructing an instrument like <em>“the leave-out mean decision rate of the assigned examiner.”</em> This could be, for instance, the fraction of previous similar cases that the examiner approved (excluding the current case). That number captures how lenient or strict they generally are. Because assignment is random, some individuals get a high-leniency examiner and others a low-leniency examiner, creating exogenous variation in treatment. By comparing outcomes across these, one can identify the causal effect of the treatment. The term “leniency” highlights that it’s the discretionary toughness of the examiner that we’re leveraging.</p>
</div>
<div id="examples-in-economics" class="section level4" number="34.8.2.3">
<h4>
<span class="header-section-number">34.8.2.3</span> Examples in Economics<a class="anchor" aria-label="anchor" href="#examples-in-economics"><i class="fas fa-link"></i></a>
</h4>
<p>Many influential studies across economics and related fields have employed examiner or judge IV designs to answer causal questions. Below are several prominent examples illustrating the range of applications and findings:</p>
<ul>
<li><p><strong>Criminal Sentencing and Recidivism:</strong> In his seminal study, <span class="citation">Kling (<a href="references.html#ref-kling2006incarceration">2006</a>)</span> examined the effect of incarceration length on ex-prisoners’ labor market outcomes. He used the random assignment of judges as an instrument, capitalizing on the fact that some judges are harsher (give longer sentences) and others more lenient. This judge IV strategy has since been used extensively to study how prison time impacts future criminal behavior and employment.</p></li>
<li><p><strong>Pre-Trial Detention Decisions:</strong> The leniency design is also applied to bail and pre-trial release. <span class="citation">Dobbie, Goldin, and Yang (<a href="references.html#ref-dobbie2018effects">2018</a>)</span> use the fact that arraignment judges vary in their tendency to set bail (versus release defendants) as an instrument to study the impact of pre-trial detention on defendants’ case outcomes and future behavior. Because defendants are quasi-randomly assigned to bail judges, this approach isolates how being jailed before trial causally affects outcomes like conviction or re-offense. These authors and others find, for example, that having a more lenient bail judge (who releases you pre-trial) leads to better long-run outcomes compared to a strict judge, indicating that pre-trial detention can have harmful causal effects.</p></li>
<li><p><strong>Juvenile Incarceration and Life Outcomes:</strong> In a related vein, <span class="citation">Aizer and Doyle Jr (<a href="references.html#ref-aizer2015juvenile">2015</a>)</span> studied the effect of juvenile detention on high school completion and adult crime. They leveraged the random assignment of juvenile court judges, where some judges were more likely to incarcerate young offenders than others. This judge IV design revealed large negative causal impacts of juvenile incarceration on educational attainment and an increase in adult crime, evidence that sentencing leniency in youth can dramatically alter life trajectories (results consistent with the general pattern found in other judge IV studies of incarceration). This application illustrates how <em>judicial decisions</em> in youth have been treated as natural experiments.</p></li>
<li><p><strong>Disability Insurance and Labor Supply:</strong> In the realm of social insurance, <span class="citation">Maestas, Mullen, and Strand (<a href="references.html#ref-maestas2013does">2013</a>)</span> used an examiner design to determine whether receiving disability benefits discourages work. Each disability claim is assigned to a disability examiner, and some examiners approve benefits at higher rates than others. By using the quasi-random examiner assignment as an instrument, they found that for applicants on the margin of eligibility, receiving Disability Insurance caused a significant reduction in employment compared to if they had been denied. They report that about 23% of applicants are affected by which examiner they get, and those who were allowed benefits due to a lenient examiner would have had substantially higher employment rates had they instead been assigned a stricter examiner (and thus been denied). This study is a prime example of using <strong>medical or administrative examiner assignments</strong> to identify a policy’s effect.</p></li>
<li><p><strong>Patent Grants and Innovation:</strong> Examiner designs are not limited to courts or social programs; they have been applied in innovation economics as well. <span class="citation">Farre-Mensa, Hegde, and Ljungqvist (<a href="references.html#ref-farre2020patent">2020</a>)</span> analyze the value of obtaining a patent for startups by exploiting the U.S. Patent Office’s quasi-random assignment of applications to patent examiners. Some patent examiners are much more lenient (more likely to grant a patent) than others, effectively creating a “patent lottery”. The authors use examiner leniency as an instrument for whether a startup’s patent is approved. They find striking results: startups that “won” the lottery by drawing a lenient examiner had <strong>55% higher employment growth and 80% higher sales</strong> five years later on average, compared to similar startups that ended up with a strict examiner and thus didn’t get the patent. This suggests that patent grants have a large causal impact on firm growth. This study showcases an <strong>examiner design</strong> in a <em>regulatory/innovation</em> setting – the term <em>leniency IV</em> in this case refers to the examiner’s propensity to allow patents.</p></li>
<li><p><strong>Business Accelerators and Firm Growth:</strong> In an entrepreneurial finance context, <span class="citation">González-Uribe and Reyes (<a href="references.html#ref-gonzalez2021identifying">2021</a>)</span> evaluate the impact of getting accepted into a business accelerator. Admission to the accelerator was determined by panels of judges scoring startup applicants, and the judges’ scoring leniency varied randomly across groups. The researchers exploit this by constructing an instrument based on the <em>generosity of the judges’ scores</em> for each applicant. They find that participating in the accelerator had a dramatic effect: startups that just made it in (thanks to generous-scoring judges) grew about <strong>166% more</strong> in revenue than those that just missed the cutoff. This is an example of a “judge leniency” design outside of a courtroom – here the “judges” were competition evaluators, and their leniency in scoring provided the exogenous variation in program entry. It demonstrates that the examiner/judge IV approach can be applied to settings like business program evaluations or any scenario with selection committees.</p></li>
</ul>
<p>These examples illustrate how examiner/judge (leniency) IV designs have been used in a wide array of empirical settings: from judicial decisions about bail, sentencing, and juvenile detention, to administrative adjudications on disability and bankruptcy, to regulatory approvals like patents, to evaluation panels in business or education contexts. In each case, the randomness of assignment and the differing “strictness” of the decision-makers create a natural experiment that researchers harness to estimate causal effects.</p>
<p><strong>Why are these designs so valuable?</strong> They allow analysts to address the problem of unobserved heterogeneity or selection bias in observational data. Normally, people who receive a treatment (go to prison, get a benefit, win an award) may differ systematically from those who don’t, confounding simple comparisons. But if an outside examiner’s quasi-random decision determines who gets the treatment, we have a credible instrument to break that link. As one article notes, this approach has become quite popular as a reliable way to recover causal effects, even as many other attempted instruments face skepticism. The trade-off is that one must have a context where such random examiner assignment occurs and must carefully check the assumptions (e.g. truly random assignment, no direct effect of the examiner on outcomes aside from via treatment). When those conditions are met, examiner and judge IV designs provide compelling evidence on causal relationships that would be hard to identify otherwise.</p>
</div>
<div id="examples-in-marketing" class="section level4" number="34.8.2.4">
<h4>
<span class="header-section-number">34.8.2.4</span> Examples in Marketing<a class="anchor" aria-label="anchor" href="#examples-in-marketing"><i class="fas fa-link"></i></a>
</h4>
<p>In marketing research, analogous setups can be constructed by identifying quasi-random sources of variation in decision-makers’ behaviors—such as sales representatives, regional managers, or customer service agents—who differ systematically in their tendency to approve discounts, upgrade customers, or resolve complaints favorably. These agents’ “leniency” can serve as an instrument for treatment assignment, enabling researchers to isolate causal effects in observational data where randomization is infeasible.</p>
<p>This analogical use of judge leniency introduces a powerful framework for addressing endogeneity in business contexts, allowing us to disentangle the effect of marketing actions (e.g., discounts, loyalty offers) from the confounding influence of customer selection or targeting bias.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th><strong>Judge Analog</strong></th>
<th><strong>Case Analog</strong></th>
<th><strong>Instrument / Causal Variation</strong></th>
<th><strong>Use Case / Research Question</strong></th>
<th><strong>Potential Outcomes</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Ad reviewer</td>
<td>Submitted ad</td>
<td>Reviewer identity, shift rotation</td>
<td>Effect of ad rejection or delay on sales</td>
<td>CTR, sales, acquisition</td>
</tr>
<tr class="even">
<td>Search ranker</td>
<td>Product view/visit</td>
<td>Random tie-breaking in rank</td>
<td>Impact of product ranking on behavior</td>
<td>Purchases, engagement</td>
</tr>
<tr class="odd">
<td>Sales rep</td>
<td>Customer inquiry</td>
<td>Agent assignment variation</td>
<td>Salesperson influence on conversion</td>
<td>Conversion, satisfaction</td>
</tr>
<tr class="even">
<td>CSR rep</td>
<td>Complaint or service issue</td>
<td>Shift schedule, escalation rules</td>
<td>Does service response tone affect churn?</td>
<td>Retention, NPS</td>
</tr>
<tr class="odd">
<td>Matching algorithm</td>
<td>Influencer-brand pairing</td>
<td>Batch assignment randomness</td>
<td>Does match quality affect campaign ROI?</td>
<td>ROI, awareness</td>
</tr>
<tr class="even">
<td>Moderator</td>
<td>User post / ad</td>
<td>Moderator stringency variation</td>
<td>Enforcement effect on trust and activity</td>
<td>Engagement, advertiser trust</td>
</tr>
<tr class="odd">
<td>Grant reviewer</td>
<td>Startup or proposal</td>
<td>Panel assignment, reviewer fixed effects</td>
<td>Causal effect of grant approval on growth</td>
<td>Marketing scaling, performance</td>
</tr>
</tbody>
</table></div>
<hr>
<!-- #### Illustrative Examples --><!-- **Example 1: Bail Setting in Philadelphia** --><!-- The pretrial release decision in Philadelphia illustrates a decision maker IV design [@stevenson2018distortion]: --><!-- -   After arrest, a defendant may: --><!--     -   Be held in jail, --><!--     -   Released on bail (money required), --><!--     -   Or released on recognizance (no money required). --><!-- -   This decision is made by one of six magistrates during preliminary hearings. --><!-- -   Magistrate assignment depends on a **rotating schedule**, and the **time of arrest** introduces random variation in which magistrate a defendant faces. --><!-- **Example 2: Bankruptcy Judges in Chapter 13** --><!-- Another example arises in the Chapter 13 bankruptcy process [@dobbie2017consumer]: --><!-- -   When filing for Chapter 13, a debtor submits a repayment plan that must be approved by a judge. --><!-- -   Judge assignment is random within location and filing day, producing quasi-random variation in outcomes. --><!-- -   Judge leniency toward discharging debts creates the necessary heterogeneity. -->
</div>
<div id="formal-setup-and-notation" class="section level4" number="34.8.2.5">
<h4>
<span class="header-section-number">34.8.2.5</span> Formal Setup and Notation<a class="anchor" aria-label="anchor" href="#formal-setup-and-notation"><i class="fas fa-link"></i></a>
</h4>
<p>We define the setup formally as follows:</p>
<ul>
<li>Let <span class="math inline">\(i = 1, \dots, n\)</span> index individuals.</li>
<li>Each individual has two outcomes of interest:
<ul>
<li>
<span class="math inline">\(D_i\)</span>: the <strong>treatment decision</strong>, made by the decision maker (e.g., bail granted or not).</li>
<li>
<span class="math inline">\(Y_i\)</span>: the <strong>final outcome</strong>, potentially affected by <span class="math inline">\(D_i\)</span> (e.g., rearrest, discharge success).</li>
</ul>
</li>
<li>Each individual is randomly assigned to one of <span class="math inline">\(K\)</span> decision makers: <span class="math inline">\(Q_i \in \{0, 1, \dots, K - 1\}\)</span>.</li>
<li>We define potential treatment outcomes as:
<ul>
<li>
<span class="math inline">\(D_i(q)\)</span>: the treatment decision if individual <span class="math inline">\(i\)</span> were assigned to decision maker <span class="math inline">\(q\)</span>.</li>
</ul>
</li>
<li>We observe only the realized <span class="math inline">\(D_i = D_i(Q_i)\)</span>.</li>
</ul>
<p><strong>Note</strong>: There is no meaningful ordering of <span class="math inline">\(Q_i\)</span>. The variation is purely categorical, not ordinal.</p>
<p>We also define potential final outcomes:</p>
<ul>
<li>
<span class="math inline">\(Y_i(D_i(q), q)\)</span>: the final outcome if individual <span class="math inline">\(i\)</span> is assigned to decision maker <span class="math inline">\(q\)</span>, receives treatment <span class="math inline">\(D_i(q)\)</span>.</li>
</ul>
<p>When conducting IV, we focus on the reduced form and first-stage variation induced by <span class="math inline">\(Q_i\)</span>, and seek to shut down the <strong>direct effect</strong> of <span class="math inline">\(Q_i\)</span> on <span class="math inline">\(Y_i\)</span>, conditioning only on <span class="math inline">\(D_i\)</span>.</p>
<hr>
<p>We first consider the variation in <span class="math inline">\(D_i\)</span> across <span class="math inline">\(Q_i\)</span>, potentially conditioning on observed covariates <span class="math inline">\(W_i\)</span>.</p>
<ul>
<li>Assume <em>conditional ignorability</em>: assignment to <span class="math inline">\(Q_i\)</span> is as good as random given <span class="math inline">\(W_i\)</span>.</li>
<li>In the simplest case, <span class="math inline">\(W_i\)</span> contains only a constant (i.e., unadjusted analysis).</li>
</ul>
<p>We can define:</p>
<p><span class="math display">\[
\tau_{q, q'} = \mathbb{E}[D_i \mid Q_i = q] - \mathbb{E}[D_i \mid Q_i = q']
\]</span></p>
<p>This is the <strong>relative effect of decision maker</strong> <span class="math inline">\(q\)</span> vs. <span class="math inline">\(q'\)</span> on treatment assignment.</p>
<p>Define:</p>
<p><span class="math display">\[
\mu_D(q) = \mathbb{E}[D_i \mid Q_i = q]
\]</span></p>
<ul>
<li>This is <strong>judge</strong> <span class="math inline">\(q\)</span>’s average leniency (i.e., how often they assign the treatment).</li>
<li>Estimated via simple regression:</li>
</ul>
<p><span class="math display">\[
D_i = Q_i \mu_D + u_i
\]</span></p>
<p>Where <span class="math inline">\(Q_i\)</span> is a vector of <span class="math inline">\(K\)</span> dummies. Then <span class="math inline">\(\hat{\mu}_D(q)\)</span> are the fitted means.</p>
<hr>
<p>To isolate judge-level variation while adjusting for baseline differences (e.g., location), we control for <span class="math inline">\(W_i\)</span>:</p>
<p><span class="math display">\[
D_i = Q_i \mu_D + W_i \gamma + u_i
\]</span></p>
<ul>
<li>Here, <span class="math inline">\(\mu_D(q)\)</span> reflects <strong>conditional leniency</strong>, net of <span class="math inline">\(W_i\)</span>.</li>
<li>Define the predicted value:</li>
</ul>
<p><span class="math display">\[
Z_i = \hat{D}_i^\perp
\]</span></p>
<p>This is the <strong>residualized leniency score</strong>, representing how lenient decision maker <span class="math inline">\(Q_i\)</span> is, beyond what is expected from covariates <span class="math inline">\(W_i\)</span>.</p>
<hr>
<p>Interpreting the Instrument <span class="math inline">\(Z_i\)</span></p>
<ul>
<li>
<span class="math inline">\(Z_i\)</span> reflects <strong>within-location variation</strong> in decision maker behavior.</li>
<li>It isolates judge-specific variation that is not confounded by observable case-level or court-level factors.</li>
<li>Mechanically, we define the <strong>residualized predicted treatment</strong> as:</li>
</ul>
<p><span class="math display">\[
\hat{D}_i^\perp = \frac{1}{n} \left( \sum_i \underbrace{1(Q_i = q) D_i}_{\text{judge mean}} - \sum_i \underbrace{1(W_i = w) D_i}_{\text{location mean}} \right)
\]</span></p>
<ul>
<li>The <strong>judge mean</strong> captures the average treatment assigned by judge <span class="math inline">\(q\)</span>.</li>
<li>The <strong>location mean</strong> captures the average treatment assigned across individuals with the same observable characteristics <span class="math inline">\(W_i = w\)</span> (e.g., court, time window).</li>
<li>Subtracting the location mean <strong>residualizes</strong> the judge effect by removing location-specific variation.</li>
</ul>
<p>The goal is to extract only the variation across judges that is orthogonal to systematic location-level treatment patterns.</p>
<hr>
<p>Why Is This Re-centering Necessary?</p>
<p>The raw leniency score <span class="math inline">\(\mu_D(q) = \mathbb{E}[D_i \mid Q_i = q]\)</span> may reflect:</p>
<ul>
<li>Actual judge discretion, but also</li>
<li>Systematic differences in case mix, depending on location or time of day</li>
</ul>
<p>This can bias the instrument if judges are not assigned uniformly across these contexts.</p>
<p>By centering judge-level means relative to the mean outcome in their location, we obtain a more meaningful instrument:</p>
<ul>
<li>It now reflects how lenient this judge is relative to their local peer group, controlling for observable confounding.</li>
</ul>
<p>Once we apply this residualization:</p>
<ul>
<li>We obtain a <strong>“recentered” leniency measure</strong>, which reflects <strong>within-location differences</strong> in decision-maker behavior.</li>
<li>This measure is commonly used in empirical IV applications (e.g., in criminal justice, bankruptcy courts, asylum decisions).</li>
</ul>
<hr>
<p>This aligns with real-world practice in empirical work, which often uses <strong>leave-one-out</strong> versions of <span class="math inline">\(\mu_D(Q_i)\)</span> to avoid mechanical endogeneity.</p>
<p><strong>Why use leave-one-out?</strong></p>
<ul>
<li>Including the individual’s own outcome in the estimation of their instrument can induce endogeneity (mechanical correlation).</li>
<li>The <strong>leave-one-out leniency score</strong> for judge <span class="math inline">\(q\)</span> is:</li>
</ul>
<p><span class="math display">\[
\tilde{\mu}_D^{(-i)}(q) = \frac{1}{n_q - 1} \sum_{j \ne i, Q_j = q} D_j
\]</span></p>
<p>Where <span class="math inline">\(n_q\)</span> is the number of individuals assigned to judge <span class="math inline">\(q\)</span>.</p>
<ul>
<li>This ensures that the instrument is <strong>uncorrelated with individual-level shocks</strong>, a key IV requirement.</li>
</ul>
<hr>
<p>We can compute <strong>residualized outcomes</strong> by subtracting out location-level variation, just like we did with leniency:</p>
<p><span class="math display">\[
\hat{Y}_i^\perp = \hat{Y}_i - \mathbb{E}[Y_i \mid W_i]
\]</span></p>
<p>This helps us distinguish how much of the variation in outcomes is due to <strong>judge effects</strong>, rather than <strong>location-level factors</strong>.</p>
<ul>
<li>Even after controlling for <span class="math inline">\(W_i\)</span>, we often still observe meaningful variation in <span class="math inline">\(Y_i\)</span> across judges.</li>
<li>This implies that <strong>judge assignment induces variation in outcomes</strong>, even when holding location and case mix fixed.</li>
</ul>
<hr>
</div>
<div id="assumptions-5" class="section level4" number="34.8.2.6">
<h4>
<span class="header-section-number">34.8.2.6</span> Assumptions<a class="anchor" aria-label="anchor" href="#assumptions-5"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>
<strong>Relevance</strong>:
<ul>
<li>The instrument <span class="math inline">\(Z_i\)</span> (e.g., judge leniency) must be <strong>strongly correlated</strong> with <span class="math inline">\(D_i\)</span>.</li>
<li>That is, <span class="math inline">\(\mathbb{E}[D_i \mid Z_i]\)</span> must vary meaningfully across values of <span class="math inline">\(Z_i\)</span>.</li>
</ul>
</li>
<li>
<strong>Exclusion</strong>:
<ul>
<li>The instrument must affect the outcome <strong>only through</strong> <span class="math inline">\(D_i\)</span>.</li>
<li>That is, <span class="math inline">\(Q_i\)</span> should not have a <strong>direct effect</strong> on <span class="math inline">\(Y_i\)</span> once we control for <span class="math inline">\(D_i\)</span>.</li>
</ul>
</li>
<li>
<strong>Monotonicity</strong> <span class="citation">(<a href="references.html#ref-imbens1994identification">G. W. Imbens and Angrist 1994</a>)</span>:
<ul>
<li>The effect of the instrument should not “flip signs” across units.</li>
<li>If judge <span class="math inline">\(q\)</span> is more lenient than <span class="math inline">\(q'\)</span>, then they are more lenient for <strong>everyone</strong>.</li>
</ul>
</li>
</ol>
<p><strong>Note</strong>: The <strong>exclusion</strong> and <strong>monotonicity</strong> assumptions are not directly testable and can be controversial. We’ll return to this in more depth later.</p>
<hr>
</div>
<div id="one-vs.-many-instruments" class="section level4" number="34.8.2.7">
<h4>
<span class="header-section-number">34.8.2.7</span> One vs. Many Instruments<a class="anchor" aria-label="anchor" href="#one-vs.-many-instruments"><i class="fas fa-link"></i></a>
</h4>
<p>In many applied papers, researchers use the <strong>leniency score</strong> <span class="math inline">\(Z_i = \hat{D}_i^\perp\)</span> rather than judge dummies directly.</p>
<p>Why?</p>
<ol style="list-style-type: decimal">
<li>
<strong>Computational Simplicity</strong>:
<ul>
<li>Using <span class="math inline">\(Z_i\)</span> avoids estimating an over-identified system (i.e., one instrument per judge).</li>
<li>This speeds up computation, especially in large datasets.</li>
</ul>
</li>
<li>
<strong>Ease of Visualization</strong>:
<ul>
<li>Researchers often plot reduced-form and first-stage coefficients against leniency.</li>
<li>This offers intuitive visual evidence of instrument strength and outcome response.</li>
</ul>
</li>
<li>
<strong>First Stage Power</strong>:
<ul>
<li>In a just-identified model, checking the strength of the instrument (e.g., F-statistic) is straightforward.</li>
<li>In contrast, many-instrument settings can obscure weak instrument issues.</li>
</ul>
</li>
</ol>
<hr>
<p>Recall the 2SLS estimator in matrix form:</p>
<p><span class="math display">\[
\hat{\beta}_{2SLS} = \frac{D'Q(Q'Q)^{-1}Q'Y}{D'Q(Q'Q)^{-1}Q'D} = \frac{\hat{D}'Y}{\hat{D}'D}
\]</span></p>
<p>This highlights a key point:</p>
<ul>
<li>Using many instruments (judge dummies) and projecting onto them,</li>
<li>or using the predicted value <span class="math inline">\(\hat{D}_i\)</span> as a single instrument,</li>
</ul>
<p>are algebraically equivalent in the absence of controls.</p>
<ul>
<li>Adding controls just residualizes <span class="math inline">\(D_i\)</span>, <span class="math inline">\(Y_i\)</span>, and <span class="math inline">\(Z_i\)</span> with respect to <span class="math inline">\(W_i\)</span>.</li>
</ul>
<p>So which approach is better?</p>
<ul>
<li>The core identifying variation is still the random assignment to <span class="math inline">\(K\)</span> judges.</li>
<li>Collapsing that variation into a predicted value <span class="math inline">\(Z_i\)</span> does not change the source of randomness.</li>
<li>However, using the predicted value may obsure the experimental structure.</li>
</ul>
<p>While point estimates may be equivalent, the inference can differ:</p>
<ul>
<li>If we use <span class="math inline">\(\hat{\mu}_D(q)\)</span> as the first-stage effect, we ignore the sampling uncertainty in estimating these means.</li>
<li>The variance of <span class="math inline">\(\hat{D}_i^\perp\)</span> does not account for variability in the projection, leading to potentially understated standard errors.</li>
</ul>
<blockquote>
<p>This issue is closely related to the <strong>many instrument problem</strong>, and arises especially when <span class="math inline">\(K\)</span> is large or judge assignment is sparse.</p>
</blockquote>
<p>In many empirical cases:</p>
<ul>
<li>The just-identified leniency instrument and over-identified judge-dummy approach yield similar standard errors.</li>
<li>This occurs when <span class="math inline">\(\hat{\mu}_D(q)\)</span> is estimated with high precision (i.e., many observations per judge).</li>
</ul>
<hr>
<p>The most important advantage of leniency instruments is that they naturally handle the “own-observation” bias.</p>
<ul>
<li>In the naive case, <span class="math inline">\(\hat{\mu}_D(q)\)</span> includes individual <span class="math inline">\(i\)</span>’s own treatment <span class="math inline">\(D_i\)</span>:
<ul>
<li>This introduces mechanical endogeneity, as the instrument is correlated with the error in <span class="math inline">\(Y_i\)</span>.</li>
</ul>
</li>
<li>The leave-one-out leniency score corrects this:</li>
</ul>
<p><span class="math display">\[
\tilde{\mu}_D^{(-i)}(q) = \frac{1}{n_q - 1} \sum_{\substack{j \ne i \\ Q_j = q}} D_j
\]</span></p>
<ul>
<li>Researchers construct <span class="math inline">\(Z_i\)</span> using <span class="math inline">\(\tilde{\mu}_D^{(-i)}(Q_i)\)</span>, which excludes <span class="math inline">\(i\)</span>’s data from their own instrument.</li>
</ul>
<p>This is essentially a finite-sample approximation to jackknife IV (JIVE).</p>
<ul>
<li>The bias corrected by LOO is exactly the one that arises in <strong>many-instrument IV</strong> in small samples.</li>
<li>The correction is especially important when the number of judges per location is moderate or small.</li>
</ul>
<p>Given that the leniency approach can be understood as a form of JIVE <span class="citation">(<a href="references.html#ref-angrist1999jackknife">J. D. Angrist, Imbens, and Krueger 1999</a>)</span>, why not go further ? U-JIVE (Unbiased JIVE) is a modern refinement developed to directly address these issues in finite samples <span class="citation">(<a href="references.html#ref-kolesa2013estimation">KolesÃ et al. 2013</a>)</span>.</p>
<ul>
<li>With many instruments or many fixed effects (e.g., location and time dummies), we can face the <strong>same inference problems</strong>.</li>
<li>U-JIVE provides a <strong>consistent and unbiased estimator</strong> even in these challenging setups.</li>
</ul>
<hr>
</div>
<div id="testing-the-exclusion-restriction" class="section level4" number="34.8.2.8">
<h4>
<span class="header-section-number">34.8.2.8</span> Testing the Exclusion Restriction<a class="anchor" aria-label="anchor" href="#testing-the-exclusion-restriction"><i class="fas fa-link"></i></a>
</h4>
<p>The exclusion restriction requires that judge assignment affects the outcome only through treatment—i.e., <span class="math inline">\(Q_i\)</span> affects <span class="math inline">\(Y_i\)</span> only through <span class="math inline">\(D_i\)</span>.</p>
<p>This is challenging to test directly, but we can probe its plausibility using tools familiar from RCTs and observational causal inference: <strong>covariate balance checks</strong>.</p>
<p><strong>Step 1: Predict Propensity Scores</strong></p>
<ul>
<li>Compute the first-stage predicted treatment: <span class="math inline">\(\hat{\mu}_D(Q_i)\)</span>
</li>
<li>This is judge <span class="math inline">\(Q_i\)</span>’s estimated leniency (possibly residualized)</li>
<li>Use this as a “propensity score” for treatment <span class="math inline">\(D_i\)</span>
</li>
</ul>
<p><strong>Step 2: Test Covariate Balance Across Leniency</strong></p>
<ul>
<li>Regress observable covariates <span class="math inline">\(X_i\)</span> on <span class="math inline">\(\hat{\mu}_D(Q_i)\)</span>
</li>
<li>If the assignment of judges is random (conditional on covariates), we should see no systematic relationship between covariates and predicted treatment</li>
</ul>
<p>This is analogous to testing for pre-treatment balance in randomized experiments.</p>
<p>See Table 1 of <span class="citation">Dobbie, Goldsmith-Pinkham, and Yang (<a href="references.html#ref-dobbie2017consumer">2017</a>)</span>, which displays descriptive statistics and tests for covariate balance across predicted leniency scores. This is now a standard empirical diagnostic in papers using judge IV designs.</p>
<blockquote>
<p>If covariates vary significantly with <span class="math inline">\(\hat{\mu}_D(Q_i)\)</span>, the exclusion assumption is questionable—it may indicate that judge assignment is confounded with case characteristics.</p>
</blockquote>
<hr>
</div>
<div id="testing-the-monotonicity-assumption" class="section level4" number="34.8.2.9">
<h4>
<span class="header-section-number">34.8.2.9</span> Testing the Monotonicity Assumption<a class="anchor" aria-label="anchor" href="#testing-the-monotonicity-assumption"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>monotonicity assumption</strong> requires that if one judge is more lenient than another, they are more lenient <strong>for everyone</strong>—i.e., no individual who is treated by a strict judge would have been untreated by a lenient one.</p>
<p>This is subtle and particularly hard to test in multi-judge (non-binary instrument) designs.</p>
<blockquote>
<p><strong>Monotonicity violations imply violations of the LATE interpretation</strong>—our IV may no longer estimate a meaningful average causal effect.</p>
</blockquote>
<p>Recent work attempts to develop <em>testable implications</em> of monotonicity <span class="citation">(<a href="references.html#ref-kitagawa2015test">Kitagawa 2015</a>; <a href="references.html#ref-frandsen2023judging">Frandsen, Lefgren, and Leslie 2023</a>)</span>.</p>
<p>Let:</p>
<ul>
<li><p><span class="math inline">\(D\)</span> be a binary treatment</p></li>
<li><p><span class="math inline">\(Z\)</span> a binary instrument (e.g., judge assignment)</p></li>
<li><p><span class="math inline">\(Y\)</span> the outcome of interest</p></li>
</ul>
<p>Define the joint distributions:</p>
<ul>
<li><span class="math inline">\(P(y, d) = \mathbb{P}(Y = y, D = d \mid Z = 1)\)</span></li>
<li><span class="math inline">\(Q(y, d) = \mathbb{P}(Y = y, D = d \mid Z = 0)\)</span></li>
</ul>
<p><span class="citation">Kitagawa (<a href="references.html#ref-kitagawa2015test">2015</a>)</span> shows that in the binary IV case if the IV is <strong>valid and monotonic</strong>, then:</p>
<p><span class="math display">\[
P(B, 1) - Q(B, 1) \ge 0 \\
P(B, 0) - Q(B, 0) \ge 0
\]</span></p>
<p>Where <span class="math inline">\(B\)</span> is any set of outcomes (e.g., <span class="math inline">\(Y \in \{1\}\)</span>). These inequalities follow from the <strong>complier potential outcome structure</strong> under monotonicity.</p>
<blockquote>
<p>These are testable implications. Violations suggest either invalid instruments or monotonicity failures.</p>
</blockquote>
<hr>
<p>In judge IV designs, the instrument is not binary. <span class="citation">Frandsen, Lefgren, and Leslie (<a href="references.html#ref-frandsen2023judging">2023</a>)</span> extended <span class="citation">Kitagawa (<a href="references.html#ref-kitagawa2015test">2015</a>)</span> idea to this case.</p>
<p>Key idea:</p>
<ul>
<li><p>Map multiple judges to a scalar leniency index, <span class="math inline">\(Z_i = \hat{\mu}_D(Q_i)\)</span></p></li>
<li><p>Discretize <span class="math inline">\(Z_i\)</span> if needed, then apply Kitagawa-style tests</p></li>
</ul>
<p>Challenges:</p>
<ul>
<li><p>The mapping introduces estimation error and noise</p></li>
<li><p>Tests rely on finite-sample approximations and may be underpowered</p></li>
</ul>
<p>Still, these tests provide useful diagnostic tools to evaluate monotonicity plausibility.</p>
<p>A practical question: what can we do if the exclusion or monotonicity assumptions fail?</p>
<ul>
<li>Estimating bounds on the treatment effect under partial violations</li>
<li>Exploring subgroup monotonicity (e.g., within strata where judges are plausibly rankable)</li>
<li>Using alternative identification strategies (e.g., weaker forms of LATE)</li>
</ul>
<p>In other words, all is not lost—but interpretation becomes more cautious and subtle.</p>
<p>Even when assumptions are reasonable, inference remains challenging in judge-IV setups.</p>
<ul>
<li>Many instruments <span class="math inline">\(\to\)</span> weak instrument concerns</li>
<li>Many controls (fixed effects) <span class="math inline">\(\to\)</span> finite-sample bias and overfitting</li>
<li>Heteroskedasticity <span class="math inline">\(\to\)</span> robust inference is tricky</li>
</ul>
<hr>
</div>
</div>
<div id="sec-proxy-variables" class="section level3" number="34.8.3">
<h3>
<span class="header-section-number">34.8.3</span> Proxy Variables<a class="anchor" aria-label="anchor" href="#sec-proxy-variables"><i class="fas fa-link"></i></a>
</h3>
<p>In applied business and economic analysis, we often confront a frustrating reality: the variables we truly care about—like <em>brand loyalty</em>, <em>employee ability</em>, or <em>investor sentiment</em>—are not directly observable. Instead, we rely on <strong>proxy variables</strong>, which are observable measures that stand in for these latent or omitted variables. Though useful, proxy variables must be used with care, as they introduce their own risks, most notably <strong>measurement error</strong> and <strong>incomplete control of endogeneity</strong>.</p>
<p>A <strong>proxy variable</strong> is an observed variable used in place of a variable that is either unobservable or omitted from a model. It is typically used under the assumption that it is correlated with the latent variable and explains some of its variation.</p>
<p>Let:</p>
<ul>
<li><p><span class="math inline">\(X^*\)</span> be the latent (unobserved) variable,</p></li>
<li><p><span class="math inline">\(X\)</span> be the observed proxy,</p></li>
<li><p><span class="math inline">\(Y\)</span> be the outcome.</p></li>
</ul>
<p>We may desire to estimate: <span class="math display">\[
Y = \beta_0 + \beta_1 X^* + \varepsilon,
\]</span> but since <span class="math inline">\(X^*\)</span> is unavailable, we instead estimate:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X + u.
\]</span></p>
<p>The effectiveness of this approach hinges on whether <span class="math inline">\(X\)</span> can validly stand in for <span class="math inline">\(X^*\)</span>.</p>
<div id="proxy-use-and-omitted-variable-bias" class="section level4" number="34.8.3.1">
<h4>
<span class="header-section-number">34.8.3.1</span> Proxy Use and Omitted Variable Bias<a class="anchor" aria-label="anchor" href="#proxy-use-and-omitted-variable-bias"><i class="fas fa-link"></i></a>
</h4>
<p>Proxy variables are sometimes used as <strong>substitutes</strong> for omitted variables that cause <a href="sec-endogeneity.html#sec-endogeneity">endogeneity</a>. Including a proxy can <strong>reduce</strong> endogeneity, but it will <strong>not</strong> generally eliminate bias, unless strict conditions are met.</p>
<blockquote>
<p><strong>Key Insight</strong>: Including a proxy does not allow us to estimate the effect of the omitted variable; rather, it helps mitigate the bias introduced by its omission.</p>
</blockquote>
<p>To be more precise, let’s consider a classic omitted variable setup:</p>
<p>Suppose the true model is: <span class="math display">\[
Y = \beta_0 + \beta_1 X + \beta_2 Z + \varepsilon,
\]</span> but <span class="math inline">\(Z\)</span> is omitted from the estimation. If <span class="math inline">\(Z\)</span> is correlated with <span class="math inline">\(X\)</span>, the OLS estimate of <span class="math inline">\(\beta_1\)</span> will be biased.</p>
<p>Now, suppose we have a proxy <span class="math inline">\(Z_p\)</span> for <span class="math inline">\(Z\)</span>. Including <span class="math inline">\(Z_p\)</span> in the regression: <span class="math display">\[
Y = \beta_0 + \beta_1 X + \beta_2 Z_p + u
\]</span> can help reduce the bias <strong>if</strong> <span class="math inline">\(Z_p\)</span> meets the following criteria.</p>
<hr>
<p>Let <span class="math inline">\(Z\)</span> be the unobserved variable and <span class="math inline">\(Z_p\)</span> be the proxy. Then, <span class="math inline">\(Z_p\)</span> is a valid proxy if:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Correlation</strong>: <span class="math inline">\(Z_p\)</span> is correlated with <span class="math inline">\(Z\)</span> (i.e., <span class="math inline">\(\text{Cov}(Z_p, Z) \ne 0\)</span>).</li>
<li>
<strong>Residual Independence</strong>: The residual variation in <span class="math inline">\(Z\)</span> unexplained by <span class="math inline">\(Z_p\)</span> is uncorrelated with all regressors (including <span class="math inline">\(Z_p\)</span> and <span class="math inline">\(X\)</span>): <span class="math display">\[
Z = \gamma_0 + \gamma_1 Z_p + \nu, \quad \text{where } \text{Cov}(\nu, X) = \text{Cov}(\nu, Z_p) = 0.
\]</span>
</li>
<li>
<strong>No direct effect</strong>: <span class="math inline">\(Z_p\)</span> affects <span class="math inline">\(Y\)</span> only through <span class="math inline">\(Z\)</span> (or at least not directly).</li>
</ol>
<p>Violation of these conditions can lead to <strong>biased</strong> or <strong>inconsistent</strong> estimates.</p>
<hr>
</div>
<div id="example-iq-as-a-proxy-for-ability-in-wage-regressions" class="section level4" number="34.8.3.2">
<h4>
<span class="header-section-number">34.8.3.2</span> Example: IQ as a Proxy for Ability in Wage Regressions<a class="anchor" aria-label="anchor" href="#example-iq-as-a-proxy-for-ability-in-wage-regressions"><i class="fas fa-link"></i></a>
</h4>
<p>In labor economics, researchers often study the effect of education on wages. But ability—an unobservable factor—also affects both education and wages, leading to omitted variable bias.</p>
<p>Let:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> = wage,</p></li>
<li><p><span class="math inline">\(X\)</span> = education,</p></li>
<li><p><span class="math inline">\(Z^*\)</span> = ability (unobserved),</p></li>
<li><p><span class="math inline">\(Z\)</span> = IQ test score (proxy for ability).</p></li>
</ul>
<p>Suppose the true model is: <span class="math display">\[
\text{wage} = \beta_0 + \beta_1 \text{education} + \beta_2 \text{ability} + \varepsilon.
\]</span></p>
<p>Since ability is unobserved, we estimate: <span class="math display">\[
\text{wage} = \beta_0 + \beta_1 \text{education} + \beta_2 \text{IQ} + u,
\]</span> under the assumption: <span class="math display">\[
\text{ability} = \gamma_0 + \gamma_1 \text{IQ} + \nu,
\]</span> with <span class="math inline">\(\text{Cov}(\nu, \text{education}) = \text{Cov}(\nu, \text{IQ}) = 0\)</span>.</p>
<p>This inclusion of IQ helps reduce <a href="sec-endogeneity.html#sec-endogeneity">endogeneity</a> but does <strong>not</strong> identify the pure effect of ability unless all variation in ability is captured by IQ.</p>
<hr>
</div>
<div id="pros-and-cons-of-proxy-variables" class="section level4" number="34.8.3.3">
<h4>
<span class="header-section-number">34.8.3.3</span> Pros and Cons of Proxy Variables<a class="anchor" aria-label="anchor" href="#pros-and-cons-of-proxy-variables"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Advantages</strong></p>
<ul>
<li>
<strong>Make latent variables measurable</strong>: Allows analysis of constructs that cannot be directly observed.</li>
<li>
<strong>Practicality</strong>: Makes use of available data to address <a href="sec-endogeneity.html#sec-endogeneity">endogeneity</a>.</li>
<li>
<strong>Improved specification</strong>: Can reduce omitted variable bias if proxies are well chosen.</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>
<p><a href="sec-endogeneity.html#sec-measurement-error">Measurement error</a>: Proxies usually include noise, causing <strong>attenuation bias</strong> (i.e., coefficients biased toward zero).</p>
<p>If <span class="math inline">\(X = X^* + \nu\)</span>, with <span class="math inline">\(\nu\)</span> classical measurement error (zero mean, uncorrelated with <span class="math inline">\(X^*\)</span> and <span class="math inline">\(\varepsilon\)</span>), then: <span class="math display">\[
\text{plim}(\hat{\beta}_1) = \lambda \beta_1, \quad \text{where } \lambda = \frac{\sigma^2_{X^*}}{\sigma^2_{X^*} + \sigma^2_\nu} &lt; 1.
\]</span></p>
</li>
<li><p><strong>Interpretation issues</strong>: Coefficients on proxies conflate the causal effect with proxy quality.</p></li>
<li><p><strong>Insufficient control</strong>: Proxies only partially reduce omitted variable bias unless they meet strict independence conditions.</p></li>
</ul>
</div>
<div id="empirical-illustration-simulating-attenuation-bias" class="section level4" number="34.8.3.4">
<h4>
<span class="header-section-number">34.8.3.4</span> Empirical Illustration: Simulating Attenuation Bias<a class="anchor" aria-label="anchor" href="#empirical-illustration-simulating-attenuation-bias"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb922"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2025</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">ability</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>                   <span class="co"># latent variable</span></span>
<span><span class="va">IQ</span> <span class="op">&lt;-</span> <span class="va">ability</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>    <span class="co"># proxy variable</span></span>
<span><span class="va">education</span> <span class="op">&lt;-</span> <span class="fl">12</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">ability</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>  <span class="co"># correlated regressor</span></span>
<span><span class="va">wage</span> <span class="op">&lt;-</span> <span class="fl">20</span> <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> <span class="va">education</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">ability</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>  <span class="co"># true model</span></span>
<span></span>
<span><span class="co"># Model using education only (omitted variable bias)</span></span>
<span><span class="va">mod1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">wage</span> <span class="op">~</span> <span class="va">education</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Model using education and proxy</span></span>
<span><span class="va">mod2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">wage</span> <span class="op">~</span> <span class="va">education</span> <span class="op">+</span> <span class="va">IQ</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mod1</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = wage ~ education)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -7.4949 -1.3590 -0.0082  1.3766  6.6601 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 10.51325    0.71353   14.73   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; education    2.28903    0.05918   38.68   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 2.061 on 998 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.5999, Adjusted R-squared:  0.5995 </span></span>
<span><span class="co">#&gt; F-statistic:  1496 on 1 and 998 DF,  p-value: &lt; 2.2e-16</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mod2</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = wage ~ education + IQ)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -5.3224 -0.9052  0.0523  0.9370  4.5822 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 17.96426    0.49599   36.22   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; education    1.67098    0.04114   40.62   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; IQ           1.55953    0.04096   38.07   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.317 on 997 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.8369, Adjusted R-squared:  0.8366 </span></span>
<span><span class="co">#&gt; F-statistic:  2558 on 2 and 997 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Observe how including the proxy reduces the bias in the coefficient on education, even if it doesn’t eliminate it entirely.</p>
</div>
<div id="example-marketing-brand-loyalty" class="section level4" number="34.8.3.5">
<h4>
<span class="header-section-number">34.8.3.5</span> Example: Marketing — Brand Loyalty<a class="anchor" aria-label="anchor" href="#example-marketing-brand-loyalty"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose you’re modeling the effect of brand loyalty (<span class="math inline">\(X^*\)</span>) on repeat purchase (<span class="math inline">\(Y\)</span>). Since loyalty is latent, we might use:</p>
<ul>
<li>Number of prior purchases,</li>
<li>Duration of current brand use,</li>
<li>Membership in loyalty programs.</li>
</ul>
<p>These proxies are likely to be correlated with true loyalty, but none is a perfect substitute.</p>
<div class="sourceCode" id="cb923"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulating attenuation bias with a proxy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">X_star</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>  <span class="co"># true unobserved brand loyalty</span></span>
<span><span class="va">proxy</span> <span class="op">&lt;-</span> <span class="va">X_star</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, sd <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span>  <span class="co"># proxy with measurement error</span></span>
<span><span class="va">error</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fl">3</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">X_star</span> <span class="op">+</span> <span class="va">error</span>  <span class="co"># true model</span></span>
<span></span>
<span><span class="co"># Model using the proxy variable</span></span>
<span><span class="va">model_proxy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">proxy</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model_proxy</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = Y ~ proxy)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -4.3060 -1.0130 -0.0018  0.9131  4.5493 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  2.98737    0.04584   65.17   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; proxy        1.45513    0.03921   37.11   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 1.449 on 998 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.5798, Adjusted R-squared:  0.5794 </span></span>
<span><span class="co">#&gt; F-statistic:  1377 on 1 and 998 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Observe that the estimated coefficient on <code>proxy</code> is less than the true coefficient (2), due to <a href="sec-endogeneity.html#sec-measurement-error">measurement error</a>.</p>
<hr>
</div>
<div id="example-finance-investor-sentiment" class="section level4" number="34.8.3.6">
<h4>
<span class="header-section-number">34.8.3.6</span> Example: Finance — Investor Sentiment<a class="anchor" aria-label="anchor" href="#example-finance-investor-sentiment"><i class="fas fa-link"></i></a>
</h4>
<p>Investor sentiment affects market movements but cannot be directly measured. Proxies include:</p>
<ul>
<li><p><strong>Put-call ratios</strong></p></li>
<li><p><strong>Bullish/bearish sentiment surveys</strong>,</p></li>
<li><p><strong>Volume of IPO activity</strong>,</p></li>
<li><p><strong>Retail investor trading flows</strong>.</p></li>
</ul>
<p>These capture different dimensions of sentiment, and their effectiveness varies by context.</p>
<hr>
</div>
<div id="strategies-to-improve-proxy-use" class="section level4" number="34.8.3.7">
<h4>
<span class="header-section-number">34.8.3.7</span> Strategies to Improve Proxy Use<a class="anchor" aria-label="anchor" href="#strategies-to-improve-proxy-use"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Multiple proxies</strong>: Use several proxies and combine them via factor analysis or PCA</p></li>
<li><p><a href="sec-instrumental-variables.html#sec-instrumental-variables">Instrumental variables</a>: If a valid instrument exists for the proxy, use two-stage least squares to correct for <a href="sec-endogeneity.html#sec-measurement-error">measurement error</a>.</p></li>
<li><p><strong>Latent variable models</strong>: Structural Equation Modeling (SEM) allows estimation of models with latent variables explicitly.</p></li>
</ul>
<p>Proxy variables are valuable tools in empirical research when used with caution. They offer a bridge between theory and data when important variables are unobservable. However, this bridge is built on assumptions—especially regarding correlation, measurement error, and residual independence—that must be carefully justified.</p>
<blockquote>
<p><strong>Key Takeaway</strong>: A proxy can reduce bias from omitted variables but introduces its own risks—especially measurement error and interpretive ambiguity. The best practice is to use proxies transparently, test assumptions when possible, and consider alternative solutions such as instruments or structural models.</p>
</blockquote>

</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></div>
<div class="next"><a href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-instrumental-variables"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="nav-link" href="#challenges-with-instrumental-variables"><span class="header-section-number">34.1</span> Challenges with Instrumental Variables</a></li>
<li>
<a class="nav-link" href="#framework-for-instrumental-variables"><span class="header-section-number">34.2</span> Framework for Instrumental Variables</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#constant-treatment-effect-model"><span class="header-section-number">34.2.1</span> Constant-Treatment-Effect Model</a></li>
<li><a class="nav-link" href="#instrumental-variable-solution"><span class="header-section-number">34.2.2</span> Instrumental Variable Solution</a></li>
<li><a class="nav-link" href="#heterogeneous-treatment-effects-and-the-late-framework"><span class="header-section-number">34.2.3</span> Heterogeneous Treatment Effects and the LATE Framework</a></li>
<li><a class="nav-link" href="#assumptions-for-late-identification"><span class="header-section-number">34.2.4</span> Assumptions for LATE Identification</a></li>
<li><a class="nav-link" href="#local-average-treatment-effect-theorem"><span class="header-section-number">34.2.5</span> Local Average Treatment Effect Theorem</a></li>
<li><a class="nav-link" href="#iv-in-randomized-trials-noncompliance"><span class="header-section-number">34.2.6</span> IV in Randomized Trials (Noncompliance)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-estimation"><span class="header-section-number">34.3</span> Estimation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-two-stage-least-squares-estimation"><span class="header-section-number">34.3.1</span> Two-Stage Least Squares Estimation</a></li>
<li><a class="nav-link" href="#iv-gmm"><span class="header-section-number">34.3.2</span> IV-GMM</a></li>
<li><a class="nav-link" href="#limited-information-maximum-likelihood"><span class="header-section-number">34.3.3</span> Limited Information Maximum Likelihood</a></li>
<li><a class="nav-link" href="#jackknife-iv"><span class="header-section-number">34.3.4</span> Jackknife IV</a></li>
<li><a class="nav-link" href="#sec-control-function-approach"><span class="header-section-number">34.3.5</span> Control Function Approach</a></li>
<li><a class="nav-link" href="#fuller-and-bias-reduced-iv"><span class="header-section-number">34.3.6</span> Fuller and Bias-Reduced IV</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#asymptotic-properties-of-the-iv-estimator"><span class="header-section-number">34.4</span> Asymptotic Properties of the IV Estimator</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-consistency-iv"><span class="header-section-number">34.4.1</span> Consistency</a></li>
<li><a class="nav-link" href="#asymptotic-normality-1"><span class="header-section-number">34.4.2</span> Asymptotic Normality</a></li>
<li><a class="nav-link" href="#asymptotic-efficiency-1"><span class="header-section-number">34.4.3</span> Asymptotic Efficiency</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-inference-iv"><span class="header-section-number">34.5</span> Inference</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-weak-instruments-problem"><span class="header-section-number">34.5.1</span> Weak Instruments Problem</a></li>
<li><a class="nav-link" href="#solutions-and-approaches-for-valid-inference"><span class="header-section-number">34.5.2</span> Solutions and Approaches for Valid Inference</a></li>
<li><a class="nav-link" href="#sec-anderson-rubin-approach"><span class="header-section-number">34.5.3</span> Anderson-Rubin Approach</a></li>
<li><a class="nav-link" href="#sec-tf-procedure"><span class="header-section-number">34.5.4</span> tF Procedure</a></li>
<li><a class="nav-link" href="#sec-ak-approach"><span class="header-section-number">34.5.5</span> AK Approach</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#testing-assumptions"><span class="header-section-number">34.6</span> Testing Assumptions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-relevance-assumption"><span class="header-section-number">34.6.1</span> Relevance Assumption</a></li>
<li><a class="nav-link" href="#independence-unconfoundedness"><span class="header-section-number">34.6.2</span> Independence (Unconfoundedness)</a></li>
<li><a class="nav-link" href="#sec-monotonicity-assumption"><span class="header-section-number">34.6.3</span> Monotonicity Assumption</a></li>
<li><a class="nav-link" href="#homogeneous-treatment-effects-optional"><span class="header-section-number">34.6.4</span> Homogeneous Treatment Effects (Optional)</a></li>
<li><a class="nav-link" href="#sec-linearity-and-additivity"><span class="header-section-number">34.6.5</span> Linearity and Additivity</a></li>
<li><a class="nav-link" href="#instrument-exogeneity-exclusion-restriction"><span class="header-section-number">34.6.6</span> Instrument Exogeneity (Exclusion Restriction)</a></li>
<li><a class="nav-link" href="#sec-exogeneity-assumption"><span class="header-section-number">34.6.7</span> Exogeneity Assumption</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#cautions-in-iv"><span class="header-section-number">34.7</span> Cautions in IV</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#negative-r2-in-iv"><span class="header-section-number">34.7.1</span> Negative \(R^2\) in IV</a></li>
<li><a class="nav-link" href="#sec-many-instruments-bias"><span class="header-section-number">34.7.2</span> Many-Instruments Bias</a></li>
<li><a class="nav-link" href="#heterogeneous-effects-in-iv-estimation"><span class="header-section-number">34.7.3</span> Heterogeneous Effects in IV Estimation</a></li>
<li><a class="nav-link" href="#zero-valued-outcomes"><span class="header-section-number">34.7.4</span> Zero-Valued Outcomes</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#types-of-iv"><span class="header-section-number">34.8</span> Types of IV</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#treatment-intensity"><span class="header-section-number">34.8.1</span> Treatment Intensity</a></li>
<li><a class="nav-link" href="#decision-maker-iv"><span class="header-section-number">34.8.2</span> Decision-Maker IV</a></li>
<li><a class="nav-link" href="#sec-proxy-variables"><span class="header-section-number">34.8.3</span> Proxy Variables</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/34-instrumental_var.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/34-instrumental_var.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-05-24.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
