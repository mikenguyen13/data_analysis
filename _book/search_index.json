[["index.html", "A Guide on Data Analysis Preface", " A Guide on Data Analysis Mike Nguyen 2024-12-09 Preface This book is an effort to simplify and demystify the complex process of data analysis, making it accessible to a wide audience. While I do not claim to be a professional statistician, econometrician, or data scientist, I firmly believe in the value of learning through teaching and practical application. Writing this book has been as much a learning journey for me as I hope it will be for you. The intended audience includes those with little to no experience in statistics, econometrics, or data science, as well as individuals with a budding interest in these fields who are eager to deepen their knowledge. While my primary domain of interest is marketing, the principles and methods discussed in this book are universally applicable to any discipline that employs scientific methods or data analysis. I hope this book provides a valuable starting point for aspiring statisticians, econometricians, and data scientists, empowering you to navigate the fascinating world of causal inference and data analysis with confidence. "],["how-to-cite-this-book.html", "How to cite this book", " How to cite this book 1. APA (7th edition): Nguyen, M. (2020). A Guide on Data Analysis. Bookdown. https://bookdown.org/mike/data_analysis/ 2. MLA (8th edition): Nguyen, Mike. A Guide on Data Analysis. Bookdown, 2020. https://bookdown.org/mike/data_analysis/ 3. Chicago (17th edition): Nguyen, Mike. 2020. A Guide on Data Analysis. Bookdown. https://bookdown.org/mike/data_analysis/ 4. Harvard: Nguyen, M. (2020) A Guide on Data Analysis. Bookdown. Available at: https://bookdown.org/mike/data_analysis/ @book{nguyen2020guide, title={A Guide on Data Analysis}, author={Nguyen, Mike}, year={2020}, publisher={Bookdown}, url={https://bookdown.org/mike/data_analysis/} } "],["more-books.html", "More books", " More books More books by the author can be found here: Advanced Data Analysis: the second book in the data analysis series, which covers machine learning models (with a focus on prediction) Marketing Research Communication Theory "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Since the turn of the century, we have witnessed remarkable advancements and innovations, particularly in statistics, information technology, computer science, and the rapidly emerging field of data science. However, one challenge of these developments is the overuse of buzzwords like big data, machine learning, and deep learning. While these terms are powerful in context, they can sometimes obscure the foundational principles underlying their application. Every substantive field often has its own specialized metric subfield, such as: Econometrics in economics Psychometrics in psychology Chemometrics in chemistry Sabermetrics in sports analytics Biostatistics in public health and medicine To the layperson, these disciplines are often grouped under broader terms like: Data Science Applied Statistics Computational Social Science As exciting as it is to explore these new tools and techniques, I must admit that retaining these concepts can be challenging. For me, the most effective way to internalize and apply these ideas has been to document the data analysis process from start to finish. With that in mind, let’s dive in and explore the fascinating world of data analysis together. "],["general-recommendations.html", "1.1 General Recommendations", " 1.1 General Recommendations The journey of mastering data analysis is fueled by practice and repetition. The more lines of code you write, the more functions you familiarize yourself with, and the more you experiment, the more enjoyable and rewarding this process becomes. Readers can approach this book in several ways: Focused Learning: If you are interested in specific methods or tools, you can jump directly to the relevant section by navigating through the table of contents. Sequential Learning: To follow a traditional path of data analysis, start with the Linear Regression section. Experimental Approach: If you are interested in designing experiments and testing hypotheses, explore the Analysis of Variance (ANOVA) section. For those primarily interested in applications and less concerned with theoretical foundations, focus on the summary and application sections of each chapter. If a concept is unclear, consider researching the topic online. This book serves as a guide, and external resources like tutorials or articles can provide additional insights. To customize the code examples provided in this book, use R’s built-in help functions. For instance: To learn more about a specific function, type help(function_name) or ?function_name in the R console. For example, to find details about the hist function, type ?hist or help(hist) in the console. Additionally, searching online is a powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages to achieve similar results. For instance, if you need to create a histogram in R, a simple search like “histogram in R” will provide multiple approaches and examples. By adopting these strategies, you can tailor your learning experience and maximize the value of this book. Tools of statistics Probability Theory Mathematical Analysis Computer Science Numerical Analysis Database Management Code Replication This book was built with R version 4.2.3 (2023-03-15 ucrt) and the following packages: package version source abind 1.4-5 CRAN (R 4.2.0) agridat 1.21 CRAN (R 4.2.3) ape 5.7-1 CRAN (R 4.2.3) assertthat 0.2.1 CRAN (R 4.2.3) backports 1.4.1 CRAN (R 4.2.0) bookdown 0.35 CRAN (R 4.2.3) boot 1.3-28.1 CRAN (R 4.2.3) broom 1.0.5 CRAN (R 4.2.3) bslib 0.6.1 CRAN (R 4.2.3) cachem 1.0.8 CRAN (R 4.2.3) callr 3.7.3 CRAN (R 4.2.3) car 3.1-2 CRAN (R 4.2.3) carData 3.0-5 CRAN (R 4.2.3) cellranger 1.1.0 CRAN (R 4.2.3) cli 3.6.1 CRAN (R 4.2.3) coda 0.19-4 CRAN (R 4.2.3) colorspace 2.1-0 CRAN (R 4.2.3) corpcor 1.6.10 CRAN (R 4.2.0) crayon 1.5.2 CRAN (R 4.2.3) cubature 2.1.0 CRAN (R 4.2.3) curl 5.1.0 CRAN (R 4.2.3) data.table 1.14.8 CRAN (R 4.2.3) DBI 1.2.0 CRAN (R 4.2.3) dbplyr 2.4.0 CRAN (R 4.2.3) desc 1.4.3 CRAN (R 4.2.3) devtools 2.4.5 CRAN (R 4.2.3) digest 0.6.31 CRAN (R 4.2.3) dplyr 1.1.2 CRAN (R 4.2.3) ellipsis 0.3.2 CRAN (R 4.2.3) evaluate 0.23 CRAN (R 4.2.3) extrafont 0.19 CRAN (R 4.2.2) extrafontdb 1.0 CRAN (R 4.2.0) fansi 1.0.4 CRAN (R 4.2.3) faraway 1.0.8 CRAN (R 4.2.3) fastmap 1.1.1 CRAN (R 4.2.3) forcats 1.0.0 CRAN (R 4.2.3) foreign 0.8-84 CRAN (R 4.2.3) fs 1.6.3 CRAN (R 4.2.3) generics 0.1.3 CRAN (R 4.2.3) ggplot2 3.4.4 CRAN (R 4.2.3) glue 1.6.2 CRAN (R 4.2.3) gtable 0.3.4 CRAN (R 4.2.3) haven 2.5.3 CRAN (R 4.2.3) Hmisc 5.1-0 CRAN (R 4.2.3) hms 1.1.3 CRAN (R 4.2.3) htmltools 0.5.7 CRAN (R 4.2.3) htmlwidgets 1.6.2 CRAN (R 4.2.3) httr 1.4.7 CRAN (R 4.2.3) investr 1.4.2 CRAN (R 4.2.3) jpeg 0.1-10 CRAN (R 4.2.2) jquerylib 0.1.4 CRAN (R 4.2.3) jsonlite 1.8.8 CRAN (R 4.2.3) kableExtra 1.3.4 CRAN (R 4.2.3) knitr 1.45 CRAN (R 4.2.3) lattice 0.21-8 CRAN (R 4.2.3) latticeExtra 0.6-30 CRAN (R 4.2.3) lifecycle 1.0.4 CRAN (R 4.2.3) lme4 1.1-35.1 CRAN (R 4.2.3) lmerTest 3.1-3 CRAN (R 4.2.3) lsr 0.5.2 CRAN (R 4.2.3) ltm 1.2-0 CRAN (R 4.2.3) lubridate 1.9.2 CRAN (R 4.2.3) magrittr 2.0.3 CRAN (R 4.2.3) MASS 7.3-60 CRAN (R 4.2.3) matlib 0.9.6 CRAN (R 4.2.3) Matrix 1.6-1 CRAN (R 4.2.3) MCMCglmm 2.35 CRAN (R 4.2.3) memoise 2.0.1 CRAN (R 4.2.3) mgcv 1.9-0 CRAN (R 4.2.3) minqa 1.2.6 CRAN (R 4.2.3) modelr 0.1.11 CRAN (R 4.2.3) munsell 0.5.0 CRAN (R 4.2.3) nlme 3.1-163 CRAN (R 4.2.3) nloptr 2.0.3 CRAN (R 4.2.3) nlstools 2.0-0 CRAN (R 4.2.3) nnet 7.3-19 CRAN (R 4.2.3) numDeriv 2016.8-1.1 CRAN (R 4.2.0) openxlsx 4.2.5.2 CRAN (R 4.2.3) pbkrtest 0.5.2 CRAN (R 4.2.3) pillar 1.9.0 CRAN (R 4.2.3) pkgbuild 1.4.3 CRAN (R 4.2.3) pkgconfig 2.0.3 CRAN (R 4.2.3) pkgload 1.3.3 CRAN (R 4.2.3) png 0.1-8 CRAN (R 4.2.2) ppsr 0.0.2 CRAN (R 4.2.3) prettyunits 1.2.0 CRAN (R 4.2.3) processx 3.8.2 CRAN (R 4.2.3) ps 1.7.5 CRAN (R 4.2.3) pscl 1.5.5.1 CRAN (R 4.2.3) purrr 1.0.2 CRAN (R 4.2.3) R6 2.5.1 CRAN (R 4.2.3) RColorBrewer 1.1-3 CRAN (R 4.2.0) Rcpp 1.0.11 CRAN (R 4.2.3) readr 2.1.4 CRAN (R 4.2.3) readxl 1.4.3 CRAN (R 4.2.3) remotes 2.4.2.1 CRAN (R 4.2.3) reprex 2.0.2 CRAN (R 4.2.3) rgl 1.2.1 CRAN (R 4.2.3) rio 1.0.1 CRAN (R 4.2.3) rlang 1.1.1 CRAN (R 4.2.3) RLRsim 3.1-8 CRAN (R 4.2.3) rmarkdown 2.25 CRAN (R 4.2.3) rprojroot 2.0.4 CRAN (R 4.2.3) rstudioapi 0.15.0 CRAN (R 4.2.3) Rttf2pt1 1.3.12 CRAN (R 4.2.2) rvest 1.0.3 CRAN (R 4.2.3) sass 0.4.8 CRAN (R 4.2.3) scales 1.3.0 CRAN (R 4.2.3) sessioninfo 1.2.2 CRAN (R 4.2.3) stringi 1.7.12 CRAN (R 4.2.2) stringr 1.5.1 CRAN (R 4.2.3) svglite 2.1.1 CRAN (R 4.2.3) systemfonts 1.0.5 CRAN (R 4.2.3) tensorA 0.36.2 CRAN (R 4.2.0) testthat 3.1.10 CRAN (R 4.2.3) tibble 3.2.1 CRAN (R 4.2.3) tidyr 1.3.0 CRAN (R 4.2.3) tidyselect 1.2.0 CRAN (R 4.2.3) tidyverse 2.0.0 CRAN (R 4.2.3) tzdb 0.4.0 CRAN (R 4.2.3) usethis 2.2.2 CRAN (R 4.2.3) utf8 1.2.3 CRAN (R 4.2.3) vctrs 0.6.3 CRAN (R 4.2.3) viridisLite 0.4.2 CRAN (R 4.2.3) webshot 0.5.5 CRAN (R 4.2.3) withr 2.5.2 CRAN (R 4.2.3) xfun 0.39 CRAN (R 4.2.3) xml2 1.3.6 CRAN (R 4.2.3) xtable 1.8-4 CRAN (R 4.2.3) yaml 2.3.7 CRAN (R 4.2.3) zip 2.3.0 CRAN (R 4.2.3) #&gt; ─ Session info ─────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 4.2.3 (2023-03-15 ucrt) #&gt; os Windows 10 x64 (build 22631) #&gt; system x86_64, mingw32 #&gt; ui RTerm #&gt; language (EN) #&gt; collate English_United States.utf8 #&gt; ctype English_United States.utf8 #&gt; tz America/Los_Angeles #&gt; date 2024-02-08 #&gt; pandoc 3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown) #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────── #&gt; package * version date (UTC) lib source #&gt; bookdown 0.35 2023-08-09 [1] CRAN (R 4.2.3) #&gt; bslib 0.6.1 2023-11-28 [1] CRAN (R 4.2.3) #&gt; cachem 1.0.8 2023-05-01 [1] CRAN (R 4.2.3) #&gt; cli 3.6.1 2023-03-23 [1] CRAN (R 4.2.3) #&gt; codetools 0.2-19 2023-02-01 [1] CRAN (R 4.2.3) #&gt; colorspace 2.1-0 2023-01-23 [1] CRAN (R 4.2.3) #&gt; desc 1.4.3 2023-12-10 [1] CRAN (R 4.2.3) #&gt; devtools 2.4.5 2022-10-11 [1] CRAN (R 4.2.3) #&gt; digest 0.6.31 2022-12-11 [1] CRAN (R 4.2.3) #&gt; dplyr * 1.1.2 2023-04-20 [1] CRAN (R 4.2.3) #&gt; ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.2.3) #&gt; evaluate 0.23 2023-11-01 [1] CRAN (R 4.2.3) #&gt; fansi 1.0.4 2023-01-22 [1] CRAN (R 4.2.3) #&gt; fastmap 1.1.1 2023-02-24 [1] CRAN (R 4.2.3) #&gt; forcats * 1.0.0 2023-01-29 [1] CRAN (R 4.2.3) #&gt; fs 1.6.3 2023-07-20 [1] CRAN (R 4.2.3) #&gt; generics 0.1.3 2022-07-05 [1] CRAN (R 4.2.3) #&gt; ggplot2 * 3.4.4 2023-10-12 [1] CRAN (R 4.2.3) #&gt; glue 1.6.2 2022-02-24 [1] CRAN (R 4.2.3) #&gt; gtable 0.3.4 2023-08-21 [1] CRAN (R 4.2.3) #&gt; highr 0.10 2022-12-22 [1] CRAN (R 4.2.3) #&gt; hms 1.1.3 2023-03-21 [1] CRAN (R 4.2.3) #&gt; htmltools 0.5.7 2023-11-03 [1] CRAN (R 4.2.3) #&gt; htmlwidgets 1.6.2 2023-03-17 [1] CRAN (R 4.2.3) #&gt; httpuv 1.6.11 2023-05-11 [1] CRAN (R 4.2.3) #&gt; jpeg * 0.1-10 2022-11-29 [1] CRAN (R 4.2.2) #&gt; jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.2.3) #&gt; jsonlite 1.8.8 2023-12-04 [1] CRAN (R 4.2.3) #&gt; knitr 1.45 2023-10-30 [1] CRAN (R 4.2.3) #&gt; later 1.3.1 2023-05-02 [1] CRAN (R 4.2.3) #&gt; lifecycle 1.0.4 2023-11-07 [1] CRAN (R 4.2.3) #&gt; lubridate * 1.9.2 2023-02-10 [1] CRAN (R 4.2.3) #&gt; magrittr 2.0.3 2022-03-30 [1] CRAN (R 4.2.3) #&gt; memoise 2.0.1 2021-11-26 [1] CRAN (R 4.2.3) #&gt; mime 0.12 2021-09-28 [1] CRAN (R 4.2.0) #&gt; miniUI 0.1.1.1 2018-05-18 [1] CRAN (R 4.2.3) #&gt; munsell 0.5.0 2018-06-12 [1] CRAN (R 4.2.3) #&gt; pillar 1.9.0 2023-03-22 [1] CRAN (R 4.2.3) #&gt; pkgbuild 1.4.3 2023-12-10 [1] CRAN (R 4.2.3) #&gt; pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.2.3) #&gt; pkgload 1.3.3 2023-09-22 [1] CRAN (R 4.2.3) #&gt; profvis 0.3.8 2023-05-02 [1] CRAN (R 4.2.3) #&gt; promises 1.2.1 2023-08-10 [1] CRAN (R 4.2.3) #&gt; purrr * 1.0.2 2023-08-10 [1] CRAN (R 4.2.3) #&gt; R6 2.5.1 2021-08-19 [1] CRAN (R 4.2.3) #&gt; Rcpp 1.0.11 2023-07-06 [1] CRAN (R 4.2.3) #&gt; readr * 2.1.4 2023-02-10 [1] CRAN (R 4.2.3) #&gt; remotes 2.4.2.1 2023-07-18 [1] CRAN (R 4.2.3) #&gt; rlang 1.1.1 2023-04-28 [1] CRAN (R 4.2.3) #&gt; rmarkdown 2.25 2023-09-18 [1] CRAN (R 4.2.3) #&gt; rstudioapi 0.15.0 2023-07-07 [1] CRAN (R 4.2.3) #&gt; sass 0.4.8 2023-12-06 [1] CRAN (R 4.2.3) #&gt; scales * 1.3.0 2023-11-28 [1] CRAN (R 4.2.3) #&gt; sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.2.3) #&gt; shiny 1.7.5 2023-08-12 [1] CRAN (R 4.2.3) #&gt; stringi 1.7.12 2023-01-11 [1] CRAN (R 4.2.2) #&gt; stringr * 1.5.1 2023-11-14 [1] CRAN (R 4.2.3) #&gt; tibble * 3.2.1 2023-03-20 [1] CRAN (R 4.2.3) #&gt; tidyr * 1.3.0 2023-01-24 [1] CRAN (R 4.2.3) #&gt; tidyselect 1.2.0 2022-10-10 [1] CRAN (R 4.2.3) #&gt; tidyverse * 2.0.0 2023-02-22 [1] CRAN (R 4.2.3) #&gt; timechange 0.2.0 2023-01-11 [1] CRAN (R 4.2.3) #&gt; tzdb 0.4.0 2023-05-12 [1] CRAN (R 4.2.3) #&gt; urlchecker 1.0.1 2021-11-30 [1] CRAN (R 4.2.3) #&gt; usethis 2.2.2 2023-07-06 [1] CRAN (R 4.2.3) #&gt; utf8 1.2.3 2023-01-31 [1] CRAN (R 4.2.3) #&gt; vctrs 0.6.3 2023-06-14 [1] CRAN (R 4.2.3) #&gt; withr 2.5.2 2023-10-30 [1] CRAN (R 4.2.3) #&gt; xfun 0.39 2023-04-20 [1] CRAN (R 4.2.3) #&gt; xtable 1.8-4 2019-04-21 [1] CRAN (R 4.2.3) #&gt; yaml 2.3.7 2023-01-23 [1] CRAN (R 4.2.3) #&gt; #&gt; [1] C:/Program Files/R/R-4.2.3/library #&gt; #&gt; ────────────────────────────────────────────────────────────────────────────── "],["prerequisites.html", "Chapter 2 Prerequisites", " Chapter 2 Prerequisites This chapter serves as a concise review of fundamental concepts in Matrix Theory and Probability Theory. If you are confident in your understanding of these topics, you can proceed directly to the Descriptive Statistics section to begin exploring applied data analysis. "],["matrix-theory.html", "2.1 Matrix Theory", " 2.1 Matrix Theory Matrix \\(A\\) represents the original matrix. It’s a 2x2 matrix with elements \\(a_{ij}\\), where \\(i\\) represents the row and \\(j\\) represents the column. \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix} \\] \\(A&#39;\\) is the transpose of \\(A\\). The transpose of a matrix flips its rows and columns. \\[ A&#39; = \\begin{bmatrix} a_{11} &amp; a_{21} \\\\ a_{12} &amp; a_{22} \\end{bmatrix} \\] Fundamental properties and rules of matrices, essential for understanding operations in linear algebra: \\[ \\begin{aligned} \\mathbf{(ABC)&#39;} &amp; = \\mathbf{C&#39;B&#39;A&#39;} \\quad &amp;\\text{(Transpose reverses order in a product)} \\\\ \\mathbf{A(B+C)} &amp; = \\mathbf{AB + AC} \\quad &amp;\\text{(Distributive property)} \\\\ \\mathbf{AB} &amp; \\neq \\mathbf{BA} \\quad &amp;\\text{(Multiplication is not commutative)} \\\\ \\mathbf{(A&#39;)&#39;} &amp; = \\mathbf{A} \\quad &amp;\\text{(Double transpose is the original matrix)} \\\\ \\mathbf{(A+B)&#39;} &amp; = \\mathbf{A&#39; + B&#39;} \\quad &amp;\\text{(Transpose of a sum is the sum of transposes)} \\\\ \\mathbf{(AB)&#39;} &amp; = \\mathbf{B&#39;A&#39;} \\quad &amp;\\text{(Transpose reverses order in a product)} \\\\ \\mathbf{(AB)^{-1}} &amp; = \\mathbf{B^{-1}A^{-1}} \\quad &amp;\\text{(Inverse reverses order in a product)} \\\\ \\mathbf{A+B} &amp; = \\mathbf{B + A} \\quad &amp;\\text{(Addition is commutative)} \\\\ \\mathbf{AA^{-1}} &amp; = \\mathbf{I} \\quad &amp;\\text{(Matrix times its inverse is identity)} \\end{aligned} \\] These properties are critical in solving systems of equations, optimizing models, and performing data transformations. If a matrix \\(\\mathbf{A}\\) has an inverse, it is called invertible. If \\(\\mathbf{A}\\) does not have an inverse, it is referred to as singular. The product of two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is computed as: \\[ \\begin{aligned} \\mathbf{A} &amp;= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\end{bmatrix} \\begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} \\\\ b_{21} &amp; b_{22} &amp; b_{23} \\\\ b_{31} &amp; b_{32} &amp; b_{33} \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} &amp; \\sum_{i=1}^{3}a_{1i}b_{i2} &amp; \\sum_{i=1}^{3}a_{1i}b_{i3} \\\\ \\sum_{i=1}^{3}a_{2i}b_{i1} &amp; \\sum_{i=1}^{3}a_{2i}b_{i2} &amp; \\sum_{i=1}^{3}a_{2i}b_{i3} \\end{bmatrix} \\end{aligned} \\] Quadratic Form Let \\(\\mathbf{a}\\) be a \\(3 \\times 1\\) vector. The quadratic form involving a matrix \\(\\mathbf{B}\\) is given by: \\[ \\mathbf{a&#39;Ba} = \\sum_{i=1}^{3}\\sum_{j=1}^{3}a_i b_{ij} a_{j} \\] Length of a Vector The length (or 2-norm) of a vector \\(\\mathbf{a}\\), denoted as \\(||\\mathbf{a}||\\), is defined as the square root of the inner product of the vector with itself: \\[ ||\\mathbf{a}|| = \\sqrt{\\mathbf{a&#39;a}} \\] 2.1.1 Rank of a Matrix The rank of a matrix refers to: The dimension of the space spanned by its columns (or rows). The number of linearly independent columns or rows. For an \\(n \\times k\\) matrix \\(\\mathbf{A}\\) and a \\(k \\times k\\) matrix \\(\\mathbf{B}\\), the following properties hold: \\(\\text{rank}(\\mathbf{A}) \\leq \\min(n, k)\\) \\(\\text{rank}(\\mathbf{A}) = \\text{rank}(\\mathbf{A&#39;}) = \\text{rank}(\\mathbf{A&#39;A}) = \\text{rank}(\\mathbf{AA&#39;})\\) \\(\\text{rank}(\\mathbf{AB}) = \\min(\\text{rank}(\\mathbf{A}), \\text{rank}(\\mathbf{B}))\\) \\(\\mathbf{B}\\) is invertible (non-singular) if and only if \\(\\text{rank}(\\mathbf{B}) = k\\). 2.1.2 Inverse of a Matrix In scalar algebra, if \\(a = 0\\), then \\(1/a\\) does not exist. In matrix algebra, a matrix is invertible if it is non-singular, meaning it has a non-zero determinant and its inverse exists. A square matrix \\(\\mathbf{A}\\) is invertible if there exists another square matrix \\(\\mathbf{B}\\) such that: \\[ \\mathbf{AB} = \\mathbf{I} \\quad \\text{(Identity Matrix)}. \\] In this case, \\(\\mathbf{A}^{-1} = \\mathbf{B}\\). For a \\(2 \\times 2\\) matrix: \\[ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The inverse is: \\[ \\mathbf{A}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} \\] This inverse exists only if \\(ad - bc \\neq 0\\), where \\(ad - bc\\) is the determinant of \\(\\mathbf{A}\\). For a partitioned block matrix: \\[ \\begin{bmatrix} A &amp; B \\\\ C &amp; D \\end{bmatrix}^{-1} = \\begin{bmatrix} \\mathbf{(A-BD^{-1}C)^{-1}} &amp; \\mathbf{-(A-BD^{-1}C)^{-1}BD^{-1}} \\\\ \\mathbf{-D^{-1}C(A-BD^{-1}C)^{-1}} &amp; \\mathbf{D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}} \\end{bmatrix} \\] This formula assumes that \\(\\mathbf{D}\\) and \\(\\mathbf{A - BD^{-1}C}\\) are invertible. Properties of the Inverse for Non-Singular Matrices \\(\\mathbf{(A^{-1})^{-1}} = \\mathbf{A}\\) For a non-zero scalar \\(b\\), \\(\\mathbf{(bA)^{-1} = b^{-1}A^{-1}}\\) For a matrix \\(\\mathbf{B}\\), \\(\\mathbf{(BA)^{-1} = B^{-1}A^{-1}}\\) (only if \\(\\mathbf{B}\\) is non-singular). \\(\\mathbf{(A^{-1})&#39; = (A&#39;)^{-1}}\\) (the transpose of the inverse equals the inverse of the transpose). Never notate \\(\\mathbf{1/A}\\); use \\(\\mathbf{A^{-1}}\\) instead. Notes: - The determinant of a matrix determines whether it is invertible. For square matrices, a determinant of \\(0\\) means the matrix is singular and has no inverse. - Always verify the conditions for invertibility, particularly when dealing with partitioned or block matrices. 2.1.3 Definiteness of a Matrix A symmetric square \\(k \\times k\\) matrix \\(\\mathbf{A}\\) is classified based on the following conditions: Positive Semi-Definite (PSD): \\(\\mathbf{A}\\) is PSD if, for any non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[ \\mathbf{x&#39;Ax \\geq 0}. \\] Negative Semi-Definite (NSD): \\(\\mathbf{A}\\) is NSD if, for any non-zero \\(k \\times 1\\) vector \\(\\mathbf{x}\\): \\[ \\mathbf{x&#39;Ax \\leq 0}. \\] Indefinite: \\(\\mathbf{A}\\) is indefinite if it is neither PSD nor NSD. The identity matrix is always positive definite (PD). Example Let \\(\\mathbf{x} = (x_1, x_2)&#39;\\), and consider a \\(2 \\times 2\\) identity matrix \\(\\mathbf{I}\\): \\[ \\begin{aligned} \\mathbf{x&#39;Ix} &amp;= (x_1, x_2) \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\\\ &amp;= (x_1, x_2) \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\\\ &amp;= x_1^2 + x_2^2 \\geq 0. \\end{aligned} \\] Thus, \\(\\mathbf{I}\\) is PD because \\(\\mathbf{x&#39;Ix} &gt; 0\\) for all non-zero \\(\\mathbf{x}\\). Properties of Definiteness Any variance-covariance matrix is PSD. A matrix \\(\\mathbf{A}\\) is PSD if and only if there exists a matrix \\(\\mathbf{B}\\) such that: \\[ \\mathbf{A = B&#39;B}. \\] If \\(\\mathbf{A}\\) is PSD, then \\(\\mathbf{B&#39;AB}\\) is also PSD for any conformable matrix \\(\\mathbf{B}\\). If \\(\\mathbf{A}\\) and \\(\\mathbf{C}\\) are non-singular, then \\(\\mathbf{A - C}\\) is PSD if and only if \\(\\mathbf{C^{-1} - A^{-1}}\\) is PSD. If \\(\\mathbf{A}\\) is PD (or ND), then \\(\\mathbf{A^{-1}}\\) is also PD (or ND). Notes An indefinite matrix \\(\\mathbf{A}\\) is neither PSD nor NSD. This concept does not have a direct counterpart in scalar algebra. If a square matrix is PSD and invertible, then it is PD. Examples of Definiteness Invertible / Indefinite: \\[ \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 10 \\end{bmatrix} \\] Non-Invertible / Indefinite: \\[ \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\] Invertible / PSD: \\[ \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] Non-Invertible / PSD: \\[ \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] 2.1.4 Matrix Calculus Consider a scalar function \\(y = f(x_1, x_2, \\dots, x_k) = f(x)\\), where \\(x\\) is a \\(1 \\times k\\) row vector. 2.1.4.1 Gradient (First-Order Derivative) The gradient, or the first-order derivative of \\(f(x)\\) with respect to the vector \\(x\\), is given by: \\[ \\frac{\\partial f(x)}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_k} \\end{bmatrix} \\] 2.1.4.2 Hessian (Second-Order Derivative) The Hessian, or the second-order derivative of \\(f(x)\\) with respect to \\(x\\), is a symmetric matrix defined as: \\[ \\frac{\\partial^2 f(x)}{\\partial x \\partial x&#39;} = \\begin{bmatrix} \\frac{\\partial^2 f(x)}{\\partial x_1^2} &amp; \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_k} \\\\ \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f(x)}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_1} &amp; \\frac{\\partial^2 f(x)}{\\partial x_k \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f(x)}{\\partial x_k^2} \\end{bmatrix} \\] 2.1.4.3 Derivative of a Scalar Function with Respect to a Matrix Let \\(f(\\mathbf{X})\\) be a scalar function, where \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix. The derivative is: \\[ \\frac{\\partial f(\\mathbf{X})}{\\partial \\mathbf{X}} = \\begin{bmatrix} \\frac{\\partial f(\\mathbf{X})}{\\partial x_{11}} &amp; \\cdots &amp; \\frac{\\partial f(\\mathbf{X})}{\\partial x_{1p}} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f(\\mathbf{X})}{\\partial x_{n1}} &amp; \\cdots &amp; \\frac{\\partial f(\\mathbf{X})}{\\partial x_{np}} \\end{bmatrix} \\] 2.1.4.4 Common Matrix Derivatives If \\(\\mathbf{a}\\) is a vector and \\(\\mathbf{A}\\) is a matrix independent of \\(\\mathbf{y}\\): \\(\\frac{\\partial \\mathbf{a&#39;y}}{\\partial \\mathbf{y}} = \\mathbf{a}\\) \\(\\frac{\\partial \\mathbf{y&#39;y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\\) \\(\\frac{\\partial \\mathbf{y&#39;Ay}}{\\partial \\mathbf{y}} = (\\mathbf{A} + \\mathbf{A&#39;})\\mathbf{y}\\) If \\(\\mathbf{X}\\) is symmetric: \\(\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \\begin{cases} X_{ii}, &amp; i = j \\\\ X_{ij}, &amp; i \\neq j \\end{cases}\\) where \\(X_{ij}\\) is the \\((i,j)\\)-th cofactor of \\(\\mathbf{X}\\). If \\(\\mathbf{X}\\) is symmetric and \\(\\mathbf{A}\\) is a matrix independent of \\(\\mathbf{X}\\): \\(\\frac{\\partial \\text{tr}(\\mathbf{XA})}{\\partial \\mathbf{X}} = \\mathbf{A} + \\mathbf{A&#39;} - \\text{diag}(\\mathbf{A})\\). If \\(\\mathbf{X}\\) is symmetric, let \\(\\mathbf{J}_{ij}\\) be a matrix with 1 at the \\((i,j)\\)-th position and 0 elsewhere: \\(\\frac{\\partial \\mathbf{X}^{-1}}{\\partial x_{ij}} = \\begin{cases} -\\mathbf{X}^{-1}\\mathbf{J}_{ii}\\mathbf{X}^{-1}, &amp; i = j \\\\ -\\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji})\\mathbf{X}^{-1}, &amp; i \\neq j \\end{cases}.\\) 2.1.5 Optimization in Scalar and Vector Spaces Optimization is the process of finding the minimum or maximum of a function. The conditions for optimization differ depending on whether the function involves a scalar or a vector. Below is a comparison of scalar and vector optimization: Condition Scalar Optimization Vector Optimization First-Order Condition \\[\\frac{\\partial f(x_0)}{\\partial x} = 0\\] \\[\\frac{\\partial f(x_0)}{\\partial x} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\\] Second-Order Condition For convex functions, this implies a minimum. \\[\\frac{\\partial^2 f(x_0)}{\\partial x^2} &gt; 0\\] \\[\\frac{\\partial^2 f(x_0)}{\\partial x \\partial x&#39;} &gt; 0\\] For concave functions, this implies a maximum. \\[\\frac{\\partial^2 f(x_0)}{\\partial x^2} &lt; 0\\] \\[\\frac{\\partial^2 f(x_0)}{\\partial x \\partial x&#39;} &lt; 0\\] Key Concepts First-Order Condition: The first-order derivative of the function must equal zero at a critical point. This holds for both scalar and vector functions: In the scalar case, \\(\\frac{\\partial f(x)}{\\partial x} = 0\\) identifies critical points. In the vector case, \\(\\frac{\\partial f(x)}{\\partial x}\\) is a gradient vector, and the condition is satisfied when all elements of the gradient are zero. Second-Order Condition: The second-order derivative determines whether the critical point is a minimum, maximum, or saddle point: For scalar functions, \\(\\frac{\\partial^2 f(x)}{\\partial x^2} &gt; 0\\) implies a local minimum, while \\(\\frac{\\partial^2 f(x)}{\\partial x^2} &lt; 0\\) implies a local maximum. For vector functions, the Hessian matrix \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x&#39;}\\) must be: Positive Definite: For a minimum (convex function). Negative Definite: For a maximum (concave function). Indefinite: For a saddle point (neither minimum nor maximum). Convex and Concave Functions: A function \\(f(x)\\) is: Convex if \\(\\frac{\\partial^2 f(x)}{\\partial x^2} &gt; 0\\) or the Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x&#39;}\\) is positive definite. Concave if \\(\\frac{\\partial^2 f(x)}{\\partial x^2} &lt; 0\\) or the Hessian is negative definite. Convexity ensures global optimization for minimization problems, while concavity ensures global optimization for maximization problems. Hessian Matrix: In vector optimization, the Hessian \\(\\frac{\\partial^2 f(x)}{\\partial x \\partial x&#39;}\\) plays a crucial role in determining the nature of critical points: Positive definite Hessian: All eigenvalues are positive. Negative definite Hessian: All eigenvalues are negative. Indefinite Hessian: Eigenvalues have mixed signs. "],["probability-theory.html", "2.2 Probability Theory", " 2.2 Probability Theory 2.2.1 Axioms and Theorems of Probability Let \\(S\\) denote the sample space of an experiment. Then: \\[ P[S] = 1 \\] (The probability of the sample space is always 1.) For any event \\(A\\): \\[ P[A] \\geq 0 \\] (Probabilities are always non-negative.) Let \\(A_1, A_2, A_3, \\dots\\) be a finite or infinite collection of mutually exclusive events. Then: \\[ P[A_1 \\cup A_2 \\cup A_3 \\dots] = P[A_1] + P[A_2] + P[A_3] + \\dots \\] (The probability of the union of mutually exclusive events is the sum of their probabilities.) The probability of the empty set is: \\[ P[\\emptyset] = 0 \\] The complement rule: \\[ P[A&#39;] = 1 - P[A] \\] The probability of the union of two events: \\[ P[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2] \\] 2.2.1.1 Conditional Probability The conditional probability of \\(A\\) given \\(B\\) is defined as: \\[ P[A|B] = \\frac{P[A \\cap B]}{P[B]}, \\quad \\text{provided } P[B] \\neq 0. \\] 2.2.1.2 Independent Events Two events \\(A\\) and \\(B\\) are independent if and only if: \\(P[A \\cap B] = P[A]P[B]\\) \\(P[A|B] = P[A]\\) \\(P[B|A] = P[B]\\) A collection of events \\(A_1, A_2, \\dots, A_n\\) is independent if and only if every subcollection is independent. 2.2.1.3 Multiplication Rule The probability of the intersection of two events can be calculated as: \\[ P[A \\cap B] = P[A|B]P[B] = P[B|A]P[A]. \\] 2.2.1.4 Bayes’ Theorem Let \\(A_1, A_2, \\dots, A_n\\) be a collection of mutually exclusive events whose union is \\(S\\), and let \\(B\\) be an event with \\(P[B] \\neq 0\\). Then, for any event \\(A_j\\) (\\(j = 1, 2, \\dots, n\\)): \\[ P[A_j|B] = \\frac{P[B|A_j]P[A_j]}{\\sum_{i=1}^n P[B|A_i]P[A_i]}. \\] 2.2.1.5 Jensen’s Inequality If \\(g(x)\\) is convex, then: \\[ E[g(X)] \\geq g(E[X]) \\] If \\(g(x)\\) is concave, then: \\[ E[g(X)] \\leq g(E[X]). \\] 2.2.1.6 Law of Iterated Expectations The law of iterated expectations states: \\[ E[Y] = E[E[Y|X]]. \\] 2.2.1.7 Correlation and Independence The strength of the relationship between random variables can be ranked from strongest to weakest as: Independence: \\(f(x, y) = f_X(x)f_Y(y)\\) \\(f_{Y|X}(y|x) = f_Y(y)\\) and \\(f_{X|Y}(x|y) = f_X(x)\\) \\(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\\) Mean Independence (implied by independence): \\(Y\\) is mean independent of \\(X\\) if: \\[ E[Y|X] = E[Y]. \\] \\(E[Xg(Y)] = E[X]E[g(Y)]\\) Uncorrelatedness (implied by independence and mean independence): \\(\\text{Cov}(X, Y) = 0\\) \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) \\(E[XY] = E[X]E[Y]\\) 2.2.2 Central Limit Theorem The Central Limit Theorem states that for a sufficiently large sample size (\\(n \\geq 25\\)), the sampling distribution of the sample mean or proportion approaches a normal distribution, regardless of the population’s original distribution. Let \\(X_1, X_2, \\dots, X_n\\) be a random sample of size \\(n\\) from a distribution \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then, for large \\(n\\): The sample mean \\(\\bar{X}\\) is approximately normal: \\[ \\mu_{\\bar{X}} = \\mu, \\quad \\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}. \\] The sample proportion \\(\\hat{p}\\) is approximately normal: \\[ \\mu_{\\hat{p}} = p, \\quad \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}. \\] The difference in sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) is approximately normal: \\[ \\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, \\quad \\sigma^2_{\\hat{p}_1 - \\hat{p}_2} = \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}. \\] The difference in sample means \\(\\bar{X}_1 - \\bar{X}_2\\) is approximately normal: \\[ \\mu_{\\bar{X}_1 - \\bar{X}_2} = \\mu_1 - \\mu_2, \\quad \\sigma^2_{\\bar{X}_1 - \\bar{X}_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}. \\] The following random variables are approximately standard normal: \\(\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\) \\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) \\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\\) \\(\\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\) 2.2.2.1 Limiting Distribution of the Sample Mean If \\(\\{X_i\\}_{i=1}^{n}\\) is an iid random sample from a distribution with finite mean \\(\\mu\\) and finite variance \\(\\sigma^2\\), the sample mean \\(\\bar{X}\\) scaled by \\(\\sqrt{n}\\) has the following limiting distribution: \\[ \\sqrt{n}(\\bar{X} - \\mu) \\xrightarrow{d} N(0, \\sigma^2). \\] Standardizing the sample mean gives: \\[ \\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} \\xrightarrow{d} N(0, 1). \\] Notes: The CLT holds for most random samples from any distribution (continuous, discrete, or unknown). It extends to the multivariate case: A random sample of a random vector converges to a multivariate normal distribution. 2.2.2.2 Asymptotic Variance and Limiting Variance Asymptotic Variance (Avar): \\[ Avar(\\sqrt{n}(\\bar{X} - \\mu)) = \\sigma^2. \\] Refers to the variance of the limiting distribution of an estimator as the sample size (\\(n\\)) approaches infinity. It characterizes the variability of the scaled estimator \\(\\sqrt{n}(\\bar{x} - \\mu)\\) in its asymptotic distribution (e.g., normal distribution). Limiting Variance (\\(\\lim_{n \\to \\infty} Var\\)) \\[ \\lim_{n \\to \\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2 \\] Represents the value that the actual variance of \\(\\sqrt{n}(\\bar{x} - \\mu)\\) converges to as \\(n \\to \\infty\\). For a well-behaved estimator, \\[ Avar(\\sqrt{n}(\\bar{X} - \\mu)) = \\lim_{n \\to \\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2. \\] However, asymptotic variance is not necessarily equal to the limiting value of the variance because asymptotic variance is derived from the limiting distribution, while limiting variance is a convergence result of the sequence of variances. \\[ Avar(.) \\neq lim_{n \\to \\infty} Var(.) \\] Both the asymptotic variance \\(Avar\\) and the limiting variance \\(\\lim_{n \\to \\infty} Var\\) are numerically equal to \\(\\sigma^2\\), but their conceptual definitions differ. \\(Avar(\\cdot) \\neq \\lim_{n \\to \\infty} Var(\\cdot)\\). This emphasizes that while the numerical result may match, their derivation and meaning differ: \\(Avar\\) depends on the asymptotic (large-sample) distribution of the estimator. \\(\\lim_{n \\to \\infty} Var(\\cdot)\\) involves the sequence of variances as \\(n\\) grows. Cases where the two do not match: Sample Quantiles: Consider the sample quantile of order \\(p\\), for some \\(0 &lt; p &lt; 1\\). Under regularity conditions, the asymptotic distribution of the sample quantile is normal, with a variance that depends on \\(p\\) and the density of the distribution at the \\(p\\)-th quantile. However, the variance of the sample quantile itself does not necessarily converge to this limit as the sample size grows. Bootstrap Methods: When using bootstrapping techniques to estimate the distribution of a statistic, the bootstrap distribution might converge to a different limiting distribution than the original statistic. In these cases, the variance of the bootstrap distribution (or the bootstrap variance) might differ from the limiting variance of the original statistic. Statistics with Randomly Varying Asymptotic Behavior: In some cases, the asymptotic behavior of a statistic can vary randomly depending on the sample path. For such statistics, the asymptotic variance might not provide a consistent estimate of the limiting variance. M-estimators with Varying Asymptotic Behavior: M-estimators can sometimes have different asymptotic behaviors depending on the tail behavior of the underlying distribution. For heavy-tailed distributions, the variance of the estimator might not stabilize even as the sample size grows large, making the asymptotic variance different from the variance of any limiting distribution. 2.2.3 Random Variable Random variables can be categorized as either discrete or continuous, with distinct properties and functions defining each type. Discrete Variable Continuous Variable Definition A random variable is discrete if it can assume at most a finite or countably infinite number of values. A random variable is continuous if it can assume any value in some interval or intervals of real numbers, with \\(P(X=x) = 0\\). Density Function A function \\(f\\) is called a density for \\(X\\) if: A function \\(f\\) is called a density for \\(X\\) if: 1. \\(f(x) \\geq 0\\) 1. \\(f(x) \\geq 0\\) for \\(x\\) real 2. \\(\\sum_{x} f(x) = 1\\) 2. \\(\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\) 3. \\(f(x) = P(X = x)\\) for \\(x\\) real 3. \\(P[a \\leq X \\leq b] = \\int_{a}^{b} f(x) \\, dx\\) for \\(a, b\\) real Cumulative Distribution Function \\(F(x) = P(X \\leq x)\\) \\(F(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f(t) \\, dt\\) \\(E[H(X)]\\) \\(\\sum_{x} H(x) f(x)\\) \\(\\int_{-\\infty}^{\\infty} H(x) f(x) \\, dx\\) \\(\\mu = E[X]\\) \\(\\sum_{x} x f(x)\\) \\(\\int_{-\\infty}^{\\infty} x f(x) \\, dx\\) Ordinary Moments \\(\\sum_{x} x^k f(x)\\) \\(\\int_{-\\infty}^{\\infty} x^k f(x) \\, dx\\) Moment Generating Function \\(m_X(t) = E[e^{tX}] = \\sum_{x} e^{tx} f(x)\\) \\(m_X(t) = E[e^{tX}] = \\int_{-\\infty}^{\\infty} e^{tx} f(x) \\, dx\\) Expected Value Properties \\(E[c] = c\\) for any constant \\(c\\). \\(E[cX] = cE[X]\\) for any constant \\(c\\). \\(E[X + Y] = E[X] + E[Y]\\). \\(E[XY] = E[X]E[Y]\\) (if \\(X\\) and \\(Y\\) are independent). Variance Properties \\(\\text{Var}(c) = 0\\) for any constant \\(c\\). \\(\\text{Var}(cX) = c^2 \\text{Var}(X)\\) for any constant \\(c\\). \\(\\text{Var}(X) \\geq 0\\). \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\). \\(\\text{Var}(X + c) = \\text{Var}(X)\\). \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (if \\(X\\) and \\(Y\\) are independent). The standard deviation \\(\\sigma\\) is given by: \\[ \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\text{Var}(X)}. \\] 2.2.3.1 Multivariate Random Variables Suppose \\(y_1, \\dots, y_p\\) are random variables with means \\(\\mu_1, \\dots, \\mu_p\\). Then: \\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_p \\end{bmatrix}, \\quad E[\\mathbf{y}] = \\begin{bmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_p \\end{bmatrix} = \\boldsymbol{\\mu}. \\] The covariance between \\(y_i\\) and \\(y_j\\) is \\(\\sigma_{ij} = \\text{Cov}(y_i, y_j)\\). The variance-covariance (or dispersion) matrix is: \\[ \\mathbf{\\Sigma} = (\\sigma_{ij})= \\begin{bmatrix} \\sigma_{11} &amp; \\sigma_{12} &amp; \\dots &amp; \\sigma_{1p} \\\\ \\sigma_{21} &amp; \\sigma_{22} &amp; \\dots &amp; \\sigma_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{p1} &amp; \\sigma_{p2} &amp; \\dots &amp; \\sigma_{pp} \\end{bmatrix}. \\] And \\(\\mathbf{\\Sigma}\\) is symmetric with \\((p+1)p/2\\) unique parameters. Alternatively, let \\(u_{p \\times 1}\\) and \\(v_{v \\times 1}\\) be random vectors with means \\(\\mathbf{\\mu_u}\\) and \\(\\mathbf{\\mu_v}\\). then \\[ \\mathbf{\\Sigma_{uv}} = cov(\\mathbf{u,v}) = E[\\mathbf{(u-\\mu_u)(v-\\mu_v)&#39;}] \\] \\(\\Sigma_{uv} \\neq \\Sigma_{vu}\\) (but \\(\\Sigma_{uv} = \\Sigma_{vu}&#39;\\)) Properties of Covariance Matrices Symmetry: \\(\\mathbf{\\Sigma}&#39; = \\mathbf{\\Sigma}\\). Eigen-Decomposition (spectral decomposition,symmetric decomposition): \\(\\mathbf{\\Sigma = \\Phi \\Lambda \\Phi}\\), where \\(\\mathbf{\\Phi}\\) is a matrix of eigenvectors such that \\(\\mathbf{\\Phi \\Phi&#39; = I}\\) (orthonormal), and \\(\\mathbf{\\Lambda}\\) is a diagonal matrix with eigenvalues \\((\\lambda_1,...,\\lambda_p)\\) on the diagonal. Non-Negative Definiteness: \\(\\mathbf{a \\Sigma a} \\ge 0\\) for any \\(\\mathbf{a} \\in R^p\\). Equivalently, the eigenvalues of \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge ... \\ge \\lambda_p \\ge 0\\) Generalized Variance: \\(|\\mathbf{\\Sigma}| = \\lambda_1 \\dots \\lambda_p \\geq 0\\). Trace: \\(\\text{tr}(\\mathbf{\\Sigma}) = \\lambda_1 + \\dots + \\lambda_p = \\sigma_{11} + \\dots+ \\sigma_{pp} = \\sum \\sigma_{ii}\\) = sum of variances (total variance). Note: \\(\\mathbf{\\Sigma}\\) is required to be positive definite. This implies that all eigenvalues are positive, and \\(\\mathbf{\\Sigma}\\) has an inverse \\(\\mathbf{\\Sigma}^{-1}\\), such that \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma}= \\mathbf{I}_{p \\times p} = \\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\) 2.2.3.2 Correlation Matrices The correlation coefficient \\(\\rho_{ij}\\) and correlation matrix \\(\\mathbf{R}\\) are defined as: \\[ \\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}}, \\quad \\mathbf{R} = \\begin{bmatrix} 1 &amp; \\rho_{12} &amp; \\dots &amp; \\rho_{1p} \\\\ \\rho_{21} &amp; 1 &amp; \\dots &amp; \\rho_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho_{p1} &amp; \\rho_{p2} &amp; \\dots &amp; 1 \\end{bmatrix}. \\] where \\(\\rho_{ii} = 1 \\forall i\\) 2.2.3.3 Linear Transformations Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices of constants, and \\(\\mathbf{c}\\) and \\(\\mathbf{d}\\) be vectors of constants. Then: \\(E[\\mathbf{Ay + c}] = \\mathbf{A \\mu_y + c}\\). \\(\\text{Var}(\\mathbf{Ay + c}) = \\mathbf{A \\Sigma_y A&#39;}\\). \\(\\text{Cov}(\\mathbf{Ay + c, By + d}) = \\mathbf{A \\Sigma_y B&#39;}\\). 2.2.4 Moment Generating Function 2.2.4.1 Properties of the Moment Generating Function \\(\\frac{d^k(m_X(t))}{dt^k} \\bigg|_{t=0} = E[X^k]\\) (The \\(k\\)-th derivative at \\(t=0\\) gives the \\(k\\)-th moment of \\(X\\)). \\(\\mu = E[X] = m_X&#39;(0)\\) (The first derivative at \\(t=0\\) gives the mean). \\(E[X^2] = m_X&#39;&#39;(0)\\) (The second derivative at \\(t=0\\) gives the second moment). 2.2.4.2 Theorems Involving MGFs Let \\(X_1, X_2, \\dots, X_n, Y\\) be random variables with MGFs \\(m_{X_1}(t), m_{X_2}(t), \\dots, m_{X_n}(t), m_Y(t)\\): If \\(m_{X_1}(t) = m_{X_2}(t)\\) for all \\(t\\) in some open interval about 0, then \\(X_1\\) and \\(X_2\\) have the same distribution. If \\(Y = \\alpha + \\beta X_1\\), then: \\[ m_Y(t) = e^{\\alpha t}m_{X_1}(\\beta t). \\] If \\(X_1, X_2, \\dots, X_n\\) are independent and \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), where \\(\\alpha_0, \\alpha_1, \\dots, \\alpha_n\\) are constants, then: \\[ m_Y(t) = e^{\\alpha_0 t} m_{X_1}(\\alpha_1 t) m_{X_2}(\\alpha_2 t) \\dots m_{X_n}(\\alpha_n t). \\] Suppose \\(X_1, X_2, \\dots, X_n\\) are independent normal random variables with means \\(\\mu_1, \\mu_2, \\dots, \\mu_n\\) and variances \\(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2\\). If \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + \\dots + \\alpha_n X_n\\), then: \\(Y\\) is normally distributed. Mean: \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 + \\alpha_2 \\mu_2 + \\dots + \\alpha_n \\mu_n\\). Variance: \\(\\sigma_Y^2 = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + \\dots + \\alpha_n^2 \\sigma_n^2\\). 2.2.5 Moments Moment Uncentered Centered 1st \\(E[X] = \\mu = \\text{Mean}(X)\\) 2nd \\(E[X^2]\\) \\(E[(X-\\mu)^2] = \\text{Var}(X) = \\sigma^2\\) 3rd \\(E[X^3]\\) \\(E[(X-\\mu)^3]\\) 4th \\(E[X^4]\\) \\(E[(X-\\mu)^4]\\) Skewness: \\(\\text{Skewness}(X) = \\frac{E[(X-\\mu)^3]}{\\sigma^3}\\) Kurtosis: \\(\\text{Kurtosis}(X) = \\frac{E[(X-\\mu)^4]}{\\sigma^4}\\) 2.2.5.1 Conditional Moments For a random variable \\(Y\\) given \\(X=x\\): Expected Value: \\[ E[Y|X=x] = \\begin{cases} \\sum_y y f_Y(y|x) &amp; \\text{for discrete RV}, \\\\ \\int_y y f_Y(y|x) \\, dy &amp; \\text{for continuous RV}. \\end{cases} \\] Variance: \\[ \\text{Var}(Y|X=x) = \\begin{cases} \\sum_y (y - E[Y|X=x])^2 f_Y(y|x) &amp; \\text{for discrete RV}, \\\\ \\int_y (y - E[Y|X=x])^2 f_Y(y|x) \\, dy &amp; \\text{for continuous RV}. \\end{cases} \\] 2.2.5.2 Multivariate Moments Expected Value: \\[ E \\begin{bmatrix} X \\\\ Y \\end{bmatrix} = \\begin{bmatrix} E[X] \\\\ E[Y] \\end{bmatrix} = \\begin{bmatrix} \\mu_X \\\\ \\mu_Y \\end{bmatrix} \\] Variance-Covariance Matrix: \\[ \\begin{aligned} \\text{Var} \\begin{bmatrix} X \\\\ Y \\end{bmatrix} &amp;= \\begin{bmatrix} \\text{Var}(X) &amp; \\text{Cov}(X, Y) \\\\ \\text{Cov}(X, Y) &amp; \\text{Var}(Y) \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} E[(X-\\mu_X)^2] &amp; E[(X-\\mu_X)(Y-\\mu_Y)] \\\\ E[(X-\\mu_X)(Y-\\mu_Y)] &amp; E[(Y-\\mu_Y)^2] \\end{bmatrix} \\end{aligned} \\] 2.2.5.3 Properties of Moments \\(E[aX + bY + c] = aE[X] + bE[Y] + c\\) \\(\\text{Var}(aX + bY + c) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\\) \\(\\text{Cov}(aX + bY, cX + dY) = ac \\text{Var}(X) + bd \\text{Var}(Y) + (ad + bc) \\text{Cov}(X, Y)\\) Correlation: \\(\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\) 2.2.6 Distributions 2.2.6.1 Conditional Distributions \\[ f_{X|Y}(x|y) = \\frac{f(x, y)}{f_Y(y)} \\] If \\(X\\) and \\(Y\\) are independent: \\[ f_{X|Y}(x|y) = f_X(x). \\] 2.2.6.2 Discrete Distributions 2.2.6.2.1 Bernoulli Distribution A random variable \\(X\\) follows a Bernoulli distribution, denoted as \\(X \\sim \\text{Bernoulli}(p)\\), if it represents a single trial with: Success probability \\(p\\) Failure probability \\(q = 1-p\\). Density Function\\[ f(x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0, 1\\} \\] CDF: Use table or manual computation. PDF hist( mc2d::rbern(1000, prob = 0.5), main = &quot;Histogram of Bernoulli Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) Mean \\[ \\mu = E[X] = p \\] Variance \\[ \\sigma^2 = \\text{Var}(X) = p(1-p) \\] 2.2.6.2.2 Binomial Distribution \\(X \\sim B(n, p)\\) is the number of successes in \\(n\\) independent Bernoulli trials, where: \\(n\\) is the number of trials \\(p\\) is the success probability. The trials are identical and independent, and probability of success (\\(p\\)) and probability of failure (\\(q = 1 - p\\)) remains the same for all trials. Density Function \\[ f(x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\dots, n \\] PDF hist( rbinom(1000, size = 100, prob = 0.5), main = &quot;Histogram of Binomial Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF \\[ m_X(t) = (1 - p + p e^t)^n \\] Mean \\[ \\mu = np \\] Variance \\[ \\sigma^2 = np(1-p) \\] 2.2.6.2.3 Poisson Distribution \\(X \\sim \\text{Poisson}(\\lambda)\\) models the number of occurrences of an event in a fixed interval, with average rate \\(\\lambda\\). Arises with Poisson process, which involves observing discrete events in a continuous “interval” of time, length, or space. The random variable \\(X\\) is the number of occurrences of the event within an interval of \\(s\\) units. The parameter \\(\\lambda\\) is the average number of occurrences of the event in question per measurement unit. For the distribution, we use the parameter \\(k = \\lambda s\\). Density Function \\[ f(x) = \\frac{e^{-k} k^x}{x!}, \\quad x = 0, 1, 2, \\dots \\] CDF PDF hist(rpois(1000, lambda = 5), main = &quot;Histogram of Poisson Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot;) MGF \\[ m_X(t) = e^{k (e^t - 1)} \\] Mean \\[ \\mu = E(X) = k \\] Variance \\[ \\sigma^2 = Var(X) = k \\] 2.2.6.2.4 Geometric Distribution \\(X \\sim \\text{G}(p)\\) models the number of trials needed to obtain the first success, with: \\(p\\): probability of success \\(q = 1-p\\): probability of failure. The experiment consists of a series of trails. The outcome of each trial can be classed as being either a “success” (s) or “failure” (f). (i.e., Bernoulli trial). The trials are identical and independent in the sense that the outcome of one trial has no effect on the outcome of any other (i..e, lack of memory - momerylessness). The probability of success (\\(p\\)) and probability of failure (\\(q = 1- p\\)) remains the same from trial to trial. Density Function \\[ f(x) = p(1-p)^{x-1}, \\quad x = 1, 2, \\dots \\] CDF\\[ F(x) = 1 - (1-p)^x \\] PDF hist(rgeom(1000, prob = 0.5), main = &quot;Histogram of Geometric Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot;) MGF \\[ m_X(t) = \\frac{p e^t}{1 - (1-p)e^t}, \\quad t &lt; -\\ln(1-p) \\] Mean \\[ \\mu = \\frac{1}{p} \\] Variance \\[ \\sigma^2 = \\frac{1-p}{p^2} \\] 2.2.6.2.5 Hypergeometric Distribution \\(X \\sim \\text{H}(N, r, n)\\) models the number of successes in a sample of size \\(n\\) drawn without replacement from a population of size \\(N\\), where: \\(r\\) objects have the trait of interest \\(N-r\\) do not have the trait. Density Function \\[ f(x) = \\frac{\\binom{r}{x} \\binom{N-r}{n-x}}{\\binom{N}{n}}, \\quad \\max(0, n-(N-r)) \\leq x \\leq \\min(n, r) \\] PDF hist( rhyper(1000, m = 50, n = 20, k = 30), main = &quot;Histogram of Hypergeometric Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) Mean\\[ \\mu = E[X] = \\frac{n r}{N} \\] Variance\\[ \\sigma^2 = \\text{Var}(X) = n \\frac{r}{N} \\frac{N-r}{N} \\frac{N-n}{N-1} \\] Note: For large \\(N\\) (when \\(\\frac{n}{N} \\leq 0.05\\)), the hypergeometric distribution can be approximated by a binomial distribution with \\(p = \\frac{r}{N}\\). 2.2.6.3 Continuous Distributions 2.2.6.3.1 Uniform Distribution Defined over an interval \\((a, b)\\), where the probabilities are “equally likely” for subintervals of equal length. Density Function: \\[ f(x) = \\frac{1}{b-a}, \\quad a &lt; x &lt; b \\] CDF\\[ F(x) = \\begin{cases} 0 &amp; \\text{if } x &lt; a \\\\ \\frac{x-a}{b-a} &amp; a \\le x \\le b \\\\ 1 &amp; \\text{if } x &gt; b \\end{cases} \\] PDF hist( runif(1000, min = 0, max = 1), main = &quot;Histogram of Uniform Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF\\[ m_X(t) = \\begin{cases} \\frac{e^{tb} - e^{ta}}{t(b-a)} &amp; \\text{if } t \\neq 0 \\\\ 1 &amp; \\text{if } t = 0 \\end{cases} \\] Mean\\[ \\mu = E[X] = \\frac{a + b}{2} \\] Variance \\[ \\sigma^2 = \\text{Var}(X) = \\frac{(b-a)^2}{12} \\] 2.2.6.3.2 Gamma Distribution The gamma distribution is used to define the exponential and \\(\\chi^2\\) distributions. The gamma function is defined as: \\[ \\Gamma(\\alpha) = \\int_0^{\\infty} z^{\\alpha-1}e^{-z}dz, \\quad \\alpha &gt; 0 \\] Properties of the Gamma Function: \\(\\Gamma(1) = 1\\) For \\(\\alpha &gt; 1\\), \\(\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)\\) If \\(n\\) is an integer and \\(n &gt; 1\\), then \\(\\Gamma(n) = (n-1)!\\) Density Function: \\[ f(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}} x^{\\alpha-1} e^{-x/\\beta}, \\quad x &gt; 0 \\] CDF (for \\(\\alpha = n\\), and \\(x&gt;0\\) a positive integer): \\[ F(x, n, \\beta) = 1 - \\sum_{k=0}^{n-1} \\frac{(\\frac{x}{\\beta})^k e^{-x/\\beta}}{k!} \\] PDF: hist( rgamma(n = 1000, shape = 5, rate = 1), main = &quot;Histogram of Gamma Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF \\[ m_X(t) = (1 - \\beta t)^{-\\alpha}, \\quad t &lt; \\frac{1}{\\beta} \\] Mean \\[ \\mu = E[X] = \\alpha \\beta \\] Variance \\[ \\sigma^2 = \\text{Var}(X) = \\alpha \\beta^2 \\] 2.2.6.3.3 Normal Distribution The normal distribution, denoted as \\(N(\\mu, \\sigma^2)\\), is symmetric and bell-shaped with parameters \\(\\mu\\) (mean) and \\(\\sigma^2\\) (variance). It is also known as the Gaussian distribution. Density Function: \\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}, \\quad -\\infty &lt; x &lt; \\infty, \\; \\sigma &gt; 0 \\] CDF: Use table or numerical methods. PDF hist( rnorm(1000, mean = 0, sd = 1), main = &quot;Histogram of Normal Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF \\[ m_X(t) = e^{\\mu t + \\frac{\\sigma^2 t^2}{2}} \\] Mean \\[ \\mu = E[X] \\] Variance \\[ \\sigma^2 = \\text{Var}(X) \\] Standard Normal Random Variable: The normal random variable \\(Z\\) with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\) is called a standard normal random variable. Any normal random variable \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) can be converted to the standard normal random variable \\(Z\\): \\[ Z = \\frac{X - \\mu}{\\sigma} \\] Normal Approximation to the Binomial Distribution: Let \\(X\\) be binomial with parameters \\(n\\) and \\(p\\). For large \\(n\\): If \\(p \\le 0.5\\) and \\(np &gt; 5\\), or If \\(p &gt; 0.5\\) and \\(n(1-p) &gt; 5\\), \\(X\\) is approximately normally distributed with mean \\(\\mu = np\\) and standard deviation \\(\\sigma = \\sqrt{np(1-p)}\\). When using the normal approximation, add or subtract 0.5 as needed for the continuity correction. Discrete Approximate Normal (Corrected): Normal Probability Rule Discrete Approximate Normal (corrected) \\(P(X = c)\\) \\(P(c -0.5 &lt; Y &lt; c + 0.5)\\) \\(P(X &lt; c)\\) \\(P(Y &lt; c - 0.5)\\) \\(P(X \\le c)\\) \\(P(Y &lt; c + 0.5)\\) \\(P(X &gt; c)\\) \\(P(Y &gt; c + 0.5)\\) \\(P(X \\ge c)\\) \\(P(Y &gt; c - 0.5)\\) If X is normally distributed with parameters \\(\\mu\\) and \\(\\sigma\\), then \\(P(-\\sigma &lt; X - \\mu &lt; \\sigma) \\approx .68\\) \\(P(-2\\sigma &lt; X - \\mu &lt; 2\\sigma) \\approx .95\\) \\(P(-3\\sigma &lt; X - \\mu &lt; 3\\sigma) \\approx .997\\) 2.2.6.3.4 Logistic Distribution The logistic distribution is a continuous probability distribution commonly used in logistic regression and other types of statistical modeling. It resembles the normal distribution but has heavier tails, allowing for more extreme values. - The logistic distribution is symmetric around \\(\\mu\\). - Its heavier tails make it useful for modeling outcomes with occasional extreme values. Density Function \\[ f(x; \\mu, s) = \\frac{e^{-(x-\\mu)/s}}{s \\left(1 + e^{-(x-\\mu)/s}\\right)^2}, \\quad -\\infty &lt; x &lt; \\infty \\] where \\(\\mu\\) is the location parameter (mean) and \\(s &gt; 0\\) is the scale parameter. CDF \\[ F(x; \\mu, s) = \\frac{1}{1 + e^{-(x-\\mu)/s}}, \\quad -\\infty &lt; x &lt; \\infty \\] PDF hist( rlogis(1000, location = 0, scale = 1), main = &quot;Histogram of Logistic Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF The MGF of the logistic distribution does not exist because its expected value diverges for most \\(t\\). Mean \\[ \\mu = E[X] = \\mu \\] Variance \\[ \\sigma^2 = \\text{Var}(X) = \\frac{\\pi^2 s^2}{3} \\] 2.2.6.3.5 Laplace Distribution The Laplace distribution, also known as the double exponential distribution, is a continuous probability distribution often used in economics, finance, and engineering. It is characterized by a peak at its mean and heavier tails compared to the normal distribution. The Laplace distribution is symmetric around \\(\\mu\\). It has heavier tails than the normal distribution, making it suitable for modeling data with more extreme outliers. Density Function \\[ f(x; \\mu, b) = \\frac{1}{2b} e^{-|x-\\mu|/b}, \\quad -\\infty &lt; x &lt; \\infty \\] where \\(\\mu\\) is the location parameter (mean) and \\(b &gt; 0\\) is the scale parameter. CDF \\[ F(x; \\mu, b) = \\begin{cases} \\frac{1}{2} e^{(x-\\mu)/b} &amp; \\text{if } x &lt; \\mu \\\\ 1 - \\frac{1}{2} e^{-(x-\\mu)/b} &amp; \\text{if } x \\ge \\mu \\end{cases} \\] PDF hist( VGAM::rlaplace(1000, location = 0, scale = 1), main = &quot;Histogram of Laplace Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF \\[ m_X(t) = \\frac{e^{\\mu t}}{1 - b^2 t^2}, \\quad |t| &lt; \\frac{1}{b} \\] Mean \\[ \\mu = E[X] = \\mu \\] Variance \\[ \\sigma^2 = \\text{Var}(X) = 2b^2 \\] 2.2.6.3.6 Log-normal Distribution The log-normal distribution is denoted as \\(\\text{Lognormal}(\\mu, \\sigma^2)\\). PDF hist(rlnorm(n = 1000, meanlog = 0, sdlog = 1), main=&quot;Histogram of Log-normal Distribution&quot;, xlab=&quot;Value&quot;, ylab=&quot;Frequency&quot;) 2.2.6.3.7 Lognormal Distribution The lognormal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. It is often used to model variables that are positively skewed, such as income or biological measurements. The lognormal distribution is positively skewed. It is useful for modeling data that cannot take negative values and is often used in finance and environmental studies. Density Function \\[ f(x; \\mu, \\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} e^{-(\\ln(x) - \\mu)^2 / (2\\sigma^2)}, \\quad x &gt; 0 \\] where \\(\\mu\\) is the mean of the underlying normal distribution and \\(\\sigma &gt; 0\\) is the standard deviation. CDF The cumulative distribution function of the lognormal distribution is given by: \\[ F(x; \\mu, \\sigma) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{\\ln(x) - \\mu}{\\sigma \\sqrt{2}} \\right) \\right], \\quad x &gt; 0 \\] PDF hist( rlnorm(1000, meanlog = 0, sdlog = 1), main = &quot;Histogram of Lognormal Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF The moment generating function (MGF) of the lognormal distribution does not exist in a simple closed form. Mean \\[ E[X] = e^{\\mu + \\sigma^2 / 2} \\] Variance \\[ \\sigma^2 = \\text{Var}(X) = \\left( e^{\\sigma^2} - 1 \\right) e^{2\\mu + \\sigma^2} \\] 2.2.6.3.8 Exponential Distribution The exponential distribution, denoted as \\(\\text{Exp}(\\lambda)\\), is a special case of the gamma distribution with \\(\\alpha = 1\\). It is commonly used to model the time between independent events that occur at a constant rate. It is often applied in reliability analysis and queuing theory. The exponential distribution is memoryless, meaning the probability of an event occurring in the future is independent of the past. It is commonly used to model waiting times, such as the time until the next customer arrives or the time until a radioactive particle decays. Density Function \\[ f(x) = \\frac{1}{\\beta} e^{-x/\\beta}, \\quad x, \\beta &gt; 0 \\] CDF\\[ F(x) = \\begin{cases} 0 &amp; \\text{if } x \\le 0 \\\\ 1 - e^{-x/\\beta} &amp; \\text{if } x &gt; 0 \\end{cases} \\] PDF hist(rexp(n = 1000, rate = 1), main = &quot;Histogram of Exponential Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot;) MGF\\[ m_X(t) = (1-\\beta t)^{-1}, \\quad t &lt; 1/\\beta \\] Mean\\[ \\mu = E[X] = \\beta \\] Variance\\[ \\sigma^2 = \\text{Var}(X) = \\beta^2 \\] 2.2.6.3.9 Chi-Squared Distribution The chi-squared distribution is a continuous probability distribution commonly used in statistical inference, particularly in hypothesis testing and construction of confidence intervals for variance. It is also used in goodness-of-fit tests. The chi-squared distribution is defined only for positive values. It is often used to model the distribution of the sum of the squares of \\(k\\) independent standard normal random variables. Density Function \\[ f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \\quad x \\ge 0 \\] where \\(k\\) is the degrees of freedom and \\(\\Gamma\\) is the gamma function. CDF The cumulative distribution function of the chi-squared distribution is given by: \\[ F(x; k) = \\frac{\\gamma(k/2, x/2)}{\\Gamma(k/2)}, \\quad x \\ge 0 \\] where \\(\\gamma\\) is the lower incomplete gamma function. PDF hist( rchisq(1000, df = 5), main = &quot;Histogram of Chi-Squared Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF \\[ m_X(t) = (1 - 2t)^{-k/2}, \\quad t &lt; \\frac{1}{2} \\] Mean \\[ E[X] = k \\] Variance \\[ \\sigma^2 = \\text{Var}(X) = 2k \\] 2.2.6.3.10 Student’s T Distribution The Student’s t-distribution, denoted as \\(T(v)\\), is defined by: \\[ T = \\frac{Z}{\\sqrt{\\chi^2_v / v}}, \\] where \\(Z\\) is a standard normal random variable and \\(\\chi^2_v\\) follows a chi-squared distribution with \\(v\\) degrees of freedom. The Student’s T distribution is a continuous probability distribution used in statistical inference, particularly for estimating population parameters when the sample size is small and/or the population variance is unknown. It is similar to the normal distribution but has heavier tails, which makes it more robust for small sample sizes. The Student’s T distribution is symmetric around 0. It has heavier tails than the normal distribution, making it useful for dealing with outliers or small sample sizes. Density Function \\[ f(x;u) = \\frac{\\Gamma((u + 1)/2)}{\\sqrt{u \\pi} \\Gamma(u/2)} \\left( 1 + \\frac{x^2}{u} \\right)^{-(u + 1)/2} \\] where \\(u\\) is the degrees of freedom and \\(\\Gamma(x)\\) is the Gamma function. CDF The cumulative distribution function of the Student’s T distribution is more complex and typically evaluated using numerical methods. PDF hist( rt(1000, df = 5), main = &quot;Histogram of Student&#39;s T Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF The moment generating function (MGF) of the Student’s T distribution does not exist in a simple closed form. Mean For \\(u &gt; 1\\): \\[ E[X] = 0 \\] Variance For \\(u &gt; 2\\): \\[ \\sigma^2 = \\text{Var}(X) = \\frac{ u}{u - 2} \\] 2.2.6.3.11 F Distribution The F-distribution, denoted as \\(F(d_1, d_2)\\), is strictly positive and used to compare variances. Definition: \\[ F = \\frac{\\chi^2_{d_1} / d_1}{\\chi^2_{d_2} / d_2}, \\] where \\(\\chi^2_{d_1}\\) and \\(\\chi^2_{d_2}\\) are independent chi-squared random variables with degrees of freedom \\(d_1\\) and \\(d_2\\), respectively. The distribution is asymmetric and never negative. The F distribution arises frequently as the null distribution of a test statistic, especially in the context of comparing variances, such as in analysis of variance (ANOVA). Density Function \\[ f(x; d_1, d_2) = \\frac{\\sqrt{\\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}}}{x B\\left( \\frac{d_1}{2}, \\frac{d_2}{2} \\right)}, \\quad x &gt; 0 \\] where \\(d_1\\) and \\(d_2\\) are the degrees of freedom and \\(B\\) is the beta function. CDF The cumulative distribution function of the F distribution is typically evaluated using numerical methods. PDF hist( rf(1000, df1 = 5, df2 = 2), main = &quot;Histogram of F Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF The moment generating function (MGF) of the F distribution does not exist in a simple closed form. Mean For \\(d_2 &gt; 2\\): \\[ E[X] = \\frac{d_2}{d_2 - 2} \\] Variance For \\(d_2 &gt; 4\\): \\[ \\sigma^2 = \\text{Var}(X) = \\frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)} \\] 2.2.6.3.12 Cauchy Distribution The Cauchy distribution is a continuous probability distribution that is often used in physics and has heavier tails than the normal distribution. It is notable because it does not have a finite mean or variance. The Cauchy distribution does not have a finite mean or variance. The Central Limit Theorem and Weak Law of Large Numbers do not apply to the Cauchy distribution. Density Function \\[ f(x; x_0, \\gamma) = \\frac{1}{\\pi \\gamma \\left[ 1 + \\left( \\frac{x - x_0}{\\gamma} \\right)^2 \\right]} \\] where \\(x_0\\) is the location parameter and \\(\\gamma &gt; 0\\) is the scale parameter. CDF The cumulative distribution function of the Cauchy distribution is given by: \\[ F(x; x_0, \\gamma) = \\frac{1}{\\pi} \\arctan \\left( \\frac{x - x_0}{\\gamma} \\right) + \\frac{1}{2} \\] PDF hist( rcauchy(1000, location = 0, scale = 1), main = &quot;Histogram of Cauchy Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF The MGF of the Cauchy distribution does not exist. Mean The mean of the Cauchy distribution is undefined. Variance The variance of the Cauchy distribution is undefined. 2.2.6.3.13 Multivariate Normal Distribution Let \\(y\\) be a \\(p\\)-dimensional multivariate normal (MVN) random variable with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\). The density function of \\(y\\) is given by: \\[ f(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\mu)&#39; \\Sigma^{-1} (\\mathbf{y}-\\mu)\\right) \\] where \\(|\\mathbf{\\Sigma}|\\) represents the determinant of the variance-covariance matrix \\(\\Sigma\\), and \\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). Properties: Let \\(\\mathbf{A}_{r \\times p}\\) be a fixed matrix. Then \\(\\mathbf{A y} \\sim N_r(\\mathbf{A \\mu}, \\mathbf{A \\Sigma A&#39;})\\). Note that \\(r \\le p\\), and all rows of \\(\\mathbf{A}\\) must be linearly independent to guarantee that \\(\\mathbf{A \\Sigma A&#39;}\\) is non-singular. Let \\(\\mathbf{G}\\) be a matrix such that \\(\\mathbf{\\Sigma^{-1} = G G&#39;}\\). Then \\(\\mathbf{G&#39;y} \\sim N_p(\\mathbf{G&#39;\\mu}, \\mathbf{I})\\) and \\(\\mathbf{G&#39;(y - \\mu)} \\sim N_p(\\mathbf{0}, \\mathbf{I})\\). Any fixed linear combination of \\(y_1, \\dots, y_p\\), say \\(\\mathbf{c&#39;y}\\), follows \\(\\mathbf{c&#39;y} \\sim N_1(\\mathbf{c&#39;\\mu}, \\mathbf{c&#39;\\Sigma c})\\). Large Sample Properties Suppose that \\(y_1, \\dots, y_n\\) are a random sample from some population with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\): \\[ \\mathbf{Y} \\sim MVN(\\mathbf{\\mu}, \\mathbf{\\Sigma}) \\] Then: \\(\\bar{\\mathbf{y}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{y}_i\\) is a consistent estimator for \\(\\mathbf{\\mu}\\). \\(\\mathbf{S} = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})&#39;\\) is a consistent estimator for \\(\\mathbf{\\Sigma}\\). Multivariate Central Limit Theorem: Similar to the univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\sim N_p(\\mathbf{0}, \\mathbf{\\Sigma})\\) when \\(n\\) is large relative to \\(p\\) (e.g., \\(n \\ge 25p\\)), which is equivalent to \\(\\bar{\\mathbf{y}} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma/n})\\). Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)&#39; \\mathbf{S^{-1}} (\\bar{\\mathbf{y}} - \\mu) \\sim \\chi^2_{(p)}\\) when \\(n\\) is large relative to \\(p\\). Density Function \\[ f(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{k/2} | \\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right) \\] where \\(\\boldsymbol{\\mu}\\) is the mean vector, \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix, \\(\\mathbf{x} \\in \\mathbb{R}^k\\) and \\(k\\) is the number of variables. CDF The cumulative distribution function of the multivariate normal distribution does not have a simple closed form and is typically evaluated using numerical methods. PDF k &lt;- 2 n &lt;- 1000 mu &lt;- c(0, 0) sigma &lt;- matrix(c(1, 0.5, 0.5, 1), nrow = k) library(MASS) hist( mvrnorm(n, mu = mu, Sigma = sigma)[,1], main = &quot;Histogram of MVN Distribution (1st Var)&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) MGF \\[ m_{\\mathbf{X}}(\\mathbf{t}) = \\exp\\left(\\boldsymbol{\\mu}^T \\mathbf{t} + \\frac{1}{2} \\mathbf{t}^T \\boldsymbol{\\Sigma} \\mathbf{t} \\right) \\] Mean \\[ E[\\mathbf{X}] = \\boldsymbol{\\mu} \\] Variance \\[ \\text{Var}(\\mathbf{X}) = \\boldsymbol{\\Sigma} \\] "],["general-math.html", "2.3 General Math", " 2.3 General Math 2.3.1 Number Sets Notation Denotes Examples \\(\\emptyset\\) Empty set No members \\(\\mathbb{N}\\) Natural numbers \\(\\{1, 2, \\ldots\\}\\) \\(\\mathbb{Z}\\) Integers \\(\\{\\ldots, -1, 0, 1, \\ldots\\}\\) \\(\\mathbb{Q}\\) Rational numbers Including fractions \\(\\mathbb{R}\\) Real numbers Including all finite decimals, irrational numbers \\(\\mathbb{C}\\) Complex numbers Including numbers of the form \\(a + bi\\) where \\(i^2 = -1\\) 2.3.2 Summation Notation and Series 2.3.2.1 Chebyshev’s Inequality Let \\(X\\) be a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). For any positive number \\(k\\), Chebyshev’s Inequality states: \\[ P(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} \\] This provides a probabilistic bound on the deviation of \\(X\\) from its mean and does not require \\(X\\) to follow a normal distribution. 2.3.2.2 Geometric Sum For a geometric series of the form \\(\\sum_{k=0}^{n-1} ar^k\\), the sum is given by: \\[ \\sum_{k=0}^{n-1} ar^k = a\\frac{1-r^n}{1-r} \\quad \\text{where } r \\neq 1 \\] 2.3.2.3 Infinite Geometric Series When \\(|r| &lt; 1\\), the geometric series converges to: \\[ \\sum_{k=0}^\\infty ar^k = \\frac{a}{1-r} \\] 2.3.2.4 Binomial Theorem The binomial expansion for \\((x + y)^n\\) is: \\[ (x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^{n-k} y^k \\quad \\text{where } n \\geq 0 \\] 2.3.2.5 Binomial Series For non-integer exponents \\(\\alpha\\): \\[ \\sum_{k=0}^\\infty \\binom{\\alpha}{k} x^k = (1 + x)^\\alpha \\quad \\text{where } |x| &lt; 1 \\] 2.3.2.6 Telescoping Sum A telescoping sum simplifies as intermediate terms cancel, leaving: \\[ \\sum_{a \\leq k &lt; b} \\Delta F(k) = F(b) - F(a) \\quad \\text{where } a, b \\in \\mathbb{Z}, a \\leq b \\] 2.3.2.7 Vandermonde Convolution The Vandermonde convolution identity is: \\[ \\sum_{k=0}^n \\binom{r}{k} \\binom{s}{n-k} = \\binom{r+s}{n} \\quad \\text{where } n \\in \\mathbb{Z} \\] 2.3.2.8 Exponential Series The exponential function \\(e^x\\) can be represented as: \\[ \\sum_{k=0}^\\infty \\frac{x^k}{k!} = e^x \\quad \\text{where } x \\in \\mathbb{C} \\] 2.3.2.9 Taylor Series The Taylor series expansion for a function \\(f(x)\\) about \\(x=a\\) is: \\[ \\sum_{k=0}^\\infty \\frac{f^{(k)}(a)}{k!} (x-a)^k = f(x) \\] For \\(a = 0\\), this becomes the Maclaurin series. 2.3.2.10 Maclaurin Series for \\(e^z\\) A special case of the Taylor series, the Maclaurin expansion for \\(e^z\\) is: \\[ e^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots \\] 2.3.2.11 Euler’s Summation Formula Euler’s summation formula connects sums and integrals: \\[ \\sum_{a \\leq k &lt; b} f(k) = \\int_a^b f(x) \\, dx + \\sum_{k=1}^m \\frac{B_k}{k!} \\left[f^{(k-1)}(x)\\right]_a^b + (-1)^{m+1} \\int_a^b \\frac{B_m(x-\\lfloor x \\rfloor)}{m!} f^{(m)}(x) \\, dx \\] Here, \\(B_k\\) are Bernoulli numbers. For \\(m=1\\) (Trapezoidal Rule): \\[ \\sum_{a \\leq k &lt; b} f(k) \\approx \\int_a^b f(x) \\, dx - \\frac{1}{2}(f(b) - f(a)) \\] 2.3.3 Taylor Expansion A differentiable function, \\(G(x)\\), can be written as an infinite sum of its derivatives. More specifically, if \\(G(x)\\) is infinitely differentiable and evaluated at \\(a\\), its Taylor expansion is: \\[ G(x) = G(a) + \\frac{G&#39;(a)}{1!} (x-a) + \\frac{G&#39;&#39;(a)}{2!}(x-a)^2 + \\frac{G&#39;&#39;&#39;(a)}{3!}(x-a)^3 + \\dots \\] This expansion is valid within the radius of convergence. 2.3.4 Law of Large Numbers Let \\(X_1, X_2, \\ldots\\) be an infinite sequence of independent and identically distributed (i.i.d.) random variables with finite mean \\(\\mu\\) and variance \\(\\sigma^2\\). The Law of Large Numbers (LLN) states that the sample average: \\[ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\] converges to the expected value \\(\\mu\\) as \\(n \\rightarrow \\infty\\). This can be expressed as: \\[ \\bar{X}_n \\rightarrow \\mu \\quad \\text{(as $n \\rightarrow \\infty$)}. \\] 2.3.4.1 Variance of the Sample Mean The variance of the sample mean decreases as the sample size increases: \\[ Var(\\bar{X}_n) = Var\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) = \\frac{\\sigma^2}{n}. \\] \\[ \\begin{aligned} Var(\\bar{X}_n) &amp;= Var(\\frac{1}{n}(X_1 + ... + X_n)) =Var\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) \\\\ &amp;= \\frac{1}{n^2}Var(X_1 + ... + X_n) \\\\ &amp;=\\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n} \\end{aligned} \\] Note: The connection between the Law of Large Numbers and the Normal Distribution lies in the Central Limit Theorem. The CLT states that, regardless of the original distribution of a dataset, the distribution of the sample means will tend to follow a normal distribution as the sample size becomes larger. The difference between [Weak Law] and [Strong Law] regards the mode of convergence. 2.3.4.2 Weak Law of Large Numbers The Weak Law of Large Numbers states that the sample average converges in probability to the expected value: \\[ \\bar{X}_n \\xrightarrow{p} \\mu \\quad \\text{as } n \\rightarrow \\infty. \\] Formally, for any \\(\\epsilon &gt; 0\\): \\[ \\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| &gt; \\epsilon) = 0. \\] Additionally, the sample mean of an i.i.d. random sample (\\(\\{ X_i \\}_{i=1}^n\\)) from any population with a finite mean and variance is a consistent estimator of the population mean \\(\\mu\\): \\[ plim(\\bar{X}_n) = plim\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\mu. \\] 2.3.4.3 Strong Law of Large Numbers The Strong Law of Large Numbers states that the sample average converges almost surely to the expected value: \\[ \\bar{X}_n \\xrightarrow{a.s.} \\mu \\quad \\text{as } n \\rightarrow \\infty. \\] Equivalently, this can be expressed as: \\[ P\\left(\\lim_{n \\to \\infty} \\bar{X}_n = \\mu\\right) = 1. \\] 2.3.5 Law of Iterated Expectation The Law of Iterated Expectation states that for random variables \\(X\\) and \\(Y\\): \\[ E(X) = E(E(X|Y)). \\] This means the expected value of \\(X\\) can be obtained by first calculating the conditional expectation \\(E(X|Y)\\) and then taking the expectation of this quantity over the distribution of \\(Y\\). 2.3.6 Convergence 2.3.6.1 Convergence in Probability As \\(n \\rightarrow \\infty\\), an estimator (random variable) \\(\\theta_n\\) is said to converge in probability to a constant \\(c\\) if: \\[ \\lim_{n \\to \\infty} P(|\\theta_n - c| \\geq \\epsilon) = 0 \\quad \\text{for any } \\epsilon &gt; 0. \\] This is denoted as: \\[ plim(\\theta_n) = c \\quad \\text{or equivalently, } \\theta_n \\xrightarrow{p} c. \\] Properties of Convergence in Probability: Slutsky’s Theorem: For a continuous function \\(g(\\cdot)\\), if \\(plim(\\theta_n) = \\theta\\), then: \\[ plim(g(\\theta_n)) = g(\\theta) \\] If \\(\\gamma_n \\xrightarrow{p} \\gamma\\), then: \\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\), \\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\), \\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (if \\(\\gamma \\neq 0\\)). These properties extend to random vectors and matrices. 2.3.6.2 Convergence in Distribution As \\(n \\rightarrow \\infty\\), the distribution of a random variable \\(X_n\\) may converge to another (“fixed”) distribution. Formally, \\(X_n\\) with CDF \\(F_n(x)\\) converges in distribution to \\(X\\) with CDF \\(F(x)\\) if: \\[ \\lim_{n \\to \\infty} |F_n(x) - F(x)| = 0 \\] at all points of continuity of \\(F(x)\\). This is denoted as: \\[ X_n \\xrightarrow{d} X \\quad \\text{or equivalently, } F(x) \\text{ is the limiting distribution of } X_n. \\] Asymptotic Properties: \\(E(X)\\): Limiting mean (asymptotic mean). \\(Var(X)\\): Limiting variance (asymptotic variance). Note: Limiting expectations and variances do not necessarily match the expectations and variances of \\(X_n\\): \\[ \\begin{aligned} E(X) &amp;\\neq \\lim_{n \\to \\infty} E(X_n), \\\\ Avar(X_n) &amp;\\neq \\lim_{n \\to \\infty} Var(X_n). \\end{aligned} \\] Properties of Convergence in Distribution: Continuous Mapping Theorem: For a continuous function \\(g(\\cdot)\\), if \\(X_n \\xrightarrow{d} X\\), then: \\[ g(X_n) \\xrightarrow{d} g(X). \\] If \\(Y_n \\xrightarrow{d} c\\) (a constant), then: \\(X_n + Y_n \\xrightarrow{d} X + c\\), \\(Y_n X_n \\xrightarrow{d} c X\\), \\(X_n / Y_n \\xrightarrow{d} X / c\\) (if \\(c \\neq 0\\)). These properties also extend to random vectors and matrices. 2.3.6.3 Summary: Properties of Convergence Convergence in Probability Convergence in Distribution Slutsky’s Theorem: For a continuous \\(g(\\cdot)\\), if \\(plim(\\theta_n) = \\theta\\), then \\(plim(g(\\theta_n)) = g(\\theta)\\) Continuous Mapping Theorem: For a continuous \\(g(\\cdot)\\), if \\(X_n \\xrightarrow{d} X\\), then \\(g(X_n) \\xrightarrow{d} g(X)\\) If \\(\\gamma_n \\xrightarrow{p} \\gamma\\), then: If \\(Y_n \\xrightarrow{d} c\\), then: \\(plim(\\theta_n + \\gamma_n) = \\theta + \\gamma\\) \\(X_n + Y_n \\xrightarrow{d} X + c\\) \\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\) \\(Y_n X_n \\xrightarrow{d} c X\\) \\(plim(\\theta_n / \\gamma_n) = \\theta / \\gamma\\) (if \\(\\gamma \\neq 0\\)) \\(X_n / Y_n \\xrightarrow{d} X / c\\) (if \\(c \\neq 0\\)) Relationship between Convergence Types: Convergence in Probability is stronger than Convergence in Distribution. Therefore: Convergence in Distribution does not guarantee Convergence in Probability. 2.3.7 Sufficient Statistics and Likelihood 2.3.7.1 Likelihood The likelihood describes the degree to which the observed data supports a particular value of a parameter \\(\\theta\\). The exact value of the likelihood is not meaningful; only relative comparisons matter. Likelihood is informative when comparing parameter values, helping identify which values of \\(\\theta\\) are more plausible given the data. For a single observation \\(Y=y\\), the likelihood function is: \\[ L(\\theta_0; y) = P(Y = y | \\theta = \\theta_0) = f_Y(y; \\theta_0) \\] 2.3.7.2 Likelihood Ratio The likelihood ratio compares the relative likelihood of two parameter values \\(\\theta_0\\) and \\(\\theta_1\\) given the data: \\[ \\frac{L(\\theta_0; y)}{L(\\theta_1; y)} \\] A likelihood ratio greater than 1 implies that \\(\\theta_0\\) is more likely than \\(\\theta_1\\), given the observed data. 2.3.7.3 Likelihood Function For a given sample, the likelihood for all possible values of \\(\\theta\\) forms the likelihood function: \\[ L(\\theta) = L(\\theta; y) = f_Y(y; \\theta). \\] For a sample of size \\(n\\), assuming independence among observations: \\[ L(\\theta) = \\prod_{i=1}^{n} f_i(y_i; \\theta). \\] Taking the natural logarithm of the likelihood gives the log-likelihood function: \\[ l(\\theta) = \\sum_{i=1}^{n} \\log f_i(y_i; \\theta). \\] The log-likelihood function is particularly useful in optimization problems, as logarithms convert products into sums, simplifying computation. 2.3.7.4 Sufficient Statistics A statistic \\(T(y)\\) is sufficient for a parameter \\(\\theta\\) if it summarizes all the information in the data about \\(\\theta\\). Formally, by the Factorization Theorem, \\(T(y)\\) is sufficient for \\(\\theta\\) if: \\[ L(\\theta; y) = c(y) L^*(\\theta; T(y)), \\] where: \\(c(y)\\) is a function of the data independent of \\(\\theta\\). \\(L^*(\\theta; T(y))\\) is a function that depends on \\(\\theta\\) and \\(T(y)\\). In other words, the likelihood function can be rewritten in terms of \\(T(y)\\) alone, without loss of information about \\(\\theta\\). Example: For a sample of i.i.d. observations \\(Y_1, Y_2, \\dots, Y_n\\) from a normal distribution \\(N(\\mu, \\sigma^2)\\): The sample mean \\(\\bar{Y}\\) is sufficient for \\(\\mu\\). The sufficient statistic conveys all the information about \\(\\mu\\) contained in the data. 2.3.7.5 Nuisance Parameters Parameters that are not of direct interest in the analysis but are necessary to model the data are called nuisance parameters. Profile Likelihood: To handle nuisance parameters, replace them with their maximum likelihood estimates (MLEs) in the likelihood function, creating a profile likelihood for the parameter of interest. 2.3.8 Parameter Transformations Transformations of parameters are often used to improve interpretability or statistical properties of models. 2.3.8.1 Log-Odds Transformation The log-odds transformation is commonly used in logistic regression and binary classification problems. It transforms probabilities (which are bounded between 0 and 1) to the real line: \\[ \\text{Log odds} = g(\\theta) = \\ln\\left(\\frac{\\theta}{1-\\theta}\\right), \\] where \\(\\theta\\) represents a probability (e.g., the success probability in a Bernoulli trial). 2.3.8.2 General Parameter Transformations For a parameter \\(\\theta\\) and a transformation \\(g(\\cdot)\\): If \\(\\theta \\in (a, b)\\), \\(g(\\theta)\\) may map \\(\\theta\\) to a different range (e.g., \\(\\mathbb{R}\\)). Useful transformations include: Logarithmic: \\(g(\\theta) = \\ln(\\theta)\\) for \\(\\theta &gt; 0\\). Exponential: \\(g(\\theta) = e^{\\theta}\\) for unconstrained \\(\\theta\\). Square root: \\(g(\\theta) = \\sqrt{\\theta}\\) for \\(\\theta \\geq 0\\). Jacobian Adjustment for Transformations: If transforming a parameter in Bayesian inference, the Jacobian of the transformation must be included to ensure proper posterior scaling. 2.3.8.3 Applications of Parameter Transformations Improving Interpretability: Probabilities can be transformed to odds or log-odds for logistic models. Rates can be transformed logarithmically for multiplicative effects. Statistical Modeling: Variance-stabilizing transformations (e.g., log for Poisson data or arcsine for proportions). Regularization or simplification of complex relationships. Optimization: Transforming constrained parameters (e.g., probabilities or positive scales) to unconstrained scales simplifies optimization algorithms. "],["data-importexport.html", "2.4 Data Import/Export", " 2.4 Data Import/Export Extended Manual by R Table by Rio Vignette Format Typical Extension Import Package Export Package Installed by Default Comma-separated data .csv data.table data.table Yes Pipe-separated data .psv data.table data.table Yes Tab-separated data .tsv data.table data.table Yes CSVY (CSV + YAML metadata header) .csvy data.table data.table Yes SAS .sas7bdat haven haven Yes SPSS .sav haven haven Yes SPSS (compressed) .zsav haven haven Yes Stata .dta haven haven Yes SAS XPORT .xpt haven haven Yes SPSS Portable .por haven Yes Excel .xls readxl Yes Excel .xlsx readxl openxlsx Yes R syntax .R base base Yes Saved R objects .RData, .rda base base Yes Serialized R objects .rds base base Yes Epiinfo .rec foreign Yes Minitab .mtp foreign Yes Systat .syd foreign Yes “XBASE” database files .dbf foreign foreign Yes Weka Attribute-Relation File Format .arff foreign foreign Yes Data Interchange Format .dif utils Yes Fortran data no recognized extension utils Yes Fixed-width format data .fwf utils utils Yes gzip comma-separated data .csv.gz utils utils Yes Apache Arrow (Parquet) .parquet arrow arrow No EViews .wf1 hexView No Feather R/Python interchange format .feather feather feather No Fast Storage .fst fst fst No JSON .json jsonlite jsonlite No Matlab .mat rmatio rmatio No OpenDocument Spreadsheet .ods readODS readODS No HTML Tables .html xml2 xml2 No Shallow XML documents .xml xml2 xml2 No YAML .yml yaml yaml No Clipboard default is tsv clipr clipr No Google Sheets as Comma-separated data R limitations: By default, R use 1 core in CPU R puts data into memory (limit around 2-4 GB), while SAS uses data from files on demand Categorization Medium-size file: within RAM limit, around 1-2 GB Large file: 2-10 GB, there might be some workaround solution Very large file &gt; 10 GB, you have to use distributed or parallel computing Solutions: buy more RAM HPC packages Explicit Parallelism Implicit Parallelism Large Memory Map/Reduce specify number of rows and columns, typically including command nrow = Use packages that store data differently bigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ to store matrices, but also support one class type For multiple class types, use ff package Very Large datasets use RHaddop package HadoopStreaming Rhipe 2.4.1 Medium size library(&quot;rio&quot;) To import multiple files in a directory str(import_list(dir()), which = 1) To export a single data file export(data, &quot;data.csv&quot;) export(data,&quot;data.dta&quot;) export(data,&quot;data.txt&quot;) export(data,&quot;data_cyl.rds&quot;) export(data,&quot;data.rdata&quot;) export(data,&quot;data.R&quot;) export(data,&quot;data.csv.zip&quot;) export(data,&quot;list.json&quot;) To export multiple data files export(list(mtcars = mtcars, iris = iris), &quot;data_file_type&quot;) # where data_file_type should substituted with the extension listed above To convert between data file types # convert Stata to SPSS convert(&quot;data.dta&quot;, &quot;data.sav&quot;) 2.4.2 Large size 2.4.2.1 Cloud Computing: Using AWS for Big Data Amazon Web Service (AWS): Compute resources can be rented at approximately $1/hr. Use AWS to process large datasets without overwhelming your local machine. 2.4.2.2 Importing Large Files as Chunks 2.4.2.2.1 Using Base R file_in &lt;- file(&quot;in.csv&quot;, &quot;r&quot;) # Open a connection to the file chunk_size &lt;- 100000 # Define chunk size x &lt;- readLines(file_in, n = chunk_size) # Read data in chunks close(file_in) # Close the file connection 2.4.2.2.2 Using the data.table Package library(data.table) mydata &lt;- fread(&quot;in.csv&quot;, header = TRUE) # Fast and memory-efficient 2.4.2.2.3 Using the ff Package library(ff) x &lt;- read.csv.ffdf( file = &quot;file.csv&quot;, nrow = 10, # Total rows header = TRUE, # Include headers VERBOSE = TRUE, # Display progress first.rows = 10000, # Initial chunk next.rows = 50000, # Subsequent chunks colClasses = NA ) 2.4.2.2.4 Using the bigmemory Package library(bigmemory) my_data &lt;- read.big.matrix(&#39;in.csv&#39;, header = TRUE) 2.4.2.2.5 Using the sqldf Package library(sqldf) my_data &lt;- read.csv.sql(&#39;in.csv&#39;) # Example: Filtering during import iris2 &lt;- read.csv.sql(&quot;iris.csv&quot;, sql = &quot;SELECT * FROM file WHERE Species = &#39;setosa&#39;&quot;) 2.4.2.2.6 Using the RMySQL Package library(RMySQL) RQLite package Download SQLite, pick “A bundle of command-line tools for managing SQLite database files” for Window 10 Unzip file, and open sqlite3.exe. Type in the prompt sqlite&gt; .cd 'C:\\Users\\data' specify path to your desired directory sqlite&gt; .open database_name.db to open a database To import the CSV file into the database sqlite&gt; .mode csv specify to SQLite that the next file is .csv file sqlite&gt; .import file_name.csv datbase_name to import the csv file to the database sqlite&gt; .exit After you’re done, exit the sqlite program library(DBI) library(dplyr) library(&quot;RSQLite&quot;) setwd(&quot;&quot;) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data_base.db&quot;) tbl &lt;- tbl(con, &quot;data_table&quot;) tbl %&gt;% filter() %&gt;% select() %&gt;% collect() # to actually pull the data into the workspace dbDisconnect(con) 2.4.2.2.7 Using the arrow Package library(arrow) data &lt;- read_csv_arrow(&quot;file.csv&quot;) 2.4.2.2.8 Using the vroom Package library(vroom) # Import a compressed CSV file compressed &lt;- vroom_example(&quot;mtcars.csv.zip&quot;) data &lt;- vroom(compressed) 2.4.2.2.9 Using the data.table Package s = fread(&quot;sample.csv&quot;) 2.4.2.2.10 Comparisons Regarding Storage Space test = ff::read.csv.ffdf(file = &quot;&quot;) object.size(test) # Highest memory usage test1 = data.table::fread(file = &quot;&quot;) object.size(test1) # Lowest memory usage test2 = readr::read_csv(file = &quot;&quot;) object.size(test2) # Second lowest memory usage test3 = vroom::vroom(file = &quot;&quot;) object.size(test3) # Similar to read_csv To work with large datasets, you can compress them into csv.gz format. However, typically, R requires loading the entire dataset before exporting it, which can be impractical for data over 10 GB. In such cases, processing the data sequentially becomes necessary. Although read.csv is slower compared to readr::read_csv, it can handle connections and allows for sequential looping, making it useful for large files. Currently, readr::read_csv does not support the skip argument efficiently for large data. Even if you specify skip, the function reads all preceding lines again. For instance, if you run read_csv(file, n_max = 100, skip = 0) followed by read_csv(file, n_max = 200, skip = 100), the first 100 rows are re-read. In contrast, read.csv can continue from where it left off without re-reading previous rows. If you encounter an error such as: “Error in (function (con, what, n = 1L, size = NA_integer_, signed = TRUE): can only read from a binary connection”, you can modify the connection mode from \"r\" to \"rb\" (read binary). Although the file function is designed to detect the appropriate format automatically, this workaround can help resolve the issue when it does not behave as expected. 2.4.2.3 Sequential Processing for Large Data # Open file for sequential reading file_conn &lt;- file(&quot;file.csv&quot;, open = &quot;r&quot;) while (TRUE) { # Read a chunk of data data_chunk &lt;- read.csv(file_conn, nrows = 1000) if (nrow(data_chunk) == 0) break # Stop if no more rows # Process the chunk here } close(file_conn) # Close connection "],["data-manipulation.html", "2.5 Data Manipulation", " 2.5 Data Manipulation # Load required packages library(tidyverse) library(lubridate) # ----------------------------- # Data Structures in R # ----------------------------- # Create vectors x &lt;- c(1, 4, 23, 4, 45) n &lt;- c(1, 3, 5) g &lt;- c(&quot;M&quot;, &quot;M&quot;, &quot;F&quot;) # Create a data frame df &lt;- data.frame(n, g) df # View the data frame #&gt; n g #&gt; 1 1 M #&gt; 2 3 M #&gt; 3 5 F str(df) # Check its structure #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ n: num 1 3 5 #&gt; $ g: chr &quot;M&quot; &quot;M&quot; &quot;F&quot; # Using tibble for cleaner outputs df &lt;- tibble(n, g) df # View the tibble #&gt; # A tibble: 3 × 2 #&gt; n g #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 M #&gt; 2 3 M #&gt; 3 5 F str(df) #&gt; tibble [3 × 2] (S3: tbl_df/tbl/data.frame) #&gt; $ n: num [1:3] 1 3 5 #&gt; $ g: chr [1:3] &quot;M&quot; &quot;M&quot; &quot;F&quot; # Create a list lst &lt;- list(x, n, g, df) lst # Display the list #&gt; [[1]] #&gt; [1] 1 4 23 4 45 #&gt; #&gt; [[2]] #&gt; [1] 1 3 5 #&gt; #&gt; [[3]] #&gt; [1] &quot;M&quot; &quot;M&quot; &quot;F&quot; #&gt; #&gt; [[4]] #&gt; # A tibble: 3 × 2 #&gt; n g #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 M #&gt; 2 3 M #&gt; 3 5 F # Name list elements lst2 &lt;- list(num = x, size = n, sex = g, data = df) lst2 # Named list elements are easier to reference #&gt; $num #&gt; [1] 1 4 23 4 45 #&gt; #&gt; $size #&gt; [1] 1 3 5 #&gt; #&gt; $sex #&gt; [1] &quot;M&quot; &quot;M&quot; &quot;F&quot; #&gt; #&gt; $data #&gt; # A tibble: 3 × 2 #&gt; n g #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 M #&gt; 2 3 M #&gt; 3 5 F # Another list example with numeric vectors lst3 &lt;- list( x = c(1, 3, 5, 7), y = c(2, 2, 2, 4, 5, 5, 5, 6), z = c(22, 3, 3, 3, 5, 10) ) lst3 #&gt; $x #&gt; [1] 1 3 5 7 #&gt; #&gt; $y #&gt; [1] 2 2 2 4 5 5 5 6 #&gt; #&gt; $z #&gt; [1] 22 3 3 3 5 10 # Find means of list elements # One at a time mean(lst3$x) #&gt; [1] 4 mean(lst3$y) #&gt; [1] 3.875 mean(lst3$z) #&gt; [1] 7.666667 # Using lapply to calculate means lapply(lst3, mean) #&gt; $x #&gt; [1] 4 #&gt; #&gt; $y #&gt; [1] 3.875 #&gt; #&gt; $z #&gt; [1] 7.666667 # Simplified output with sapply sapply(lst3, mean) #&gt; x y z #&gt; 4.000000 3.875000 7.666667 # Tidyverse alternative: map() function map(lst3, mean) #&gt; $x #&gt; [1] 4 #&gt; #&gt; $y #&gt; [1] 3.875 #&gt; #&gt; $z #&gt; [1] 7.666667 # Tidyverse with numeric output: map_dbl() map_dbl(lst3, mean) #&gt; x y z #&gt; 4.000000 3.875000 7.666667 # ----------------------------- # Binding Data Frames # ----------------------------- # Create tibbles for demonstration dat01 &lt;- tibble(x = 1:5, y = 5:1) dat02 &lt;- tibble(x = 10:16, y = x / 2) dat03 &lt;- tibble(z = runif(5)) # 5 random numbers from (0, 1) # Row binding bind_rows(dat01, dat02, dat01) #&gt; # A tibble: 17 × 2 #&gt; x y #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 5 #&gt; 2 2 4 #&gt; 3 3 3 #&gt; 4 4 2 #&gt; 5 5 1 #&gt; 6 10 5 #&gt; 7 11 5.5 #&gt; 8 12 6 #&gt; 9 13 6.5 #&gt; 10 14 7 #&gt; 11 15 7.5 #&gt; 12 16 8 #&gt; 13 1 5 #&gt; 14 2 4 #&gt; 15 3 3 #&gt; 16 4 2 #&gt; 17 5 1 # Add a new identifier column with .id bind_rows(dat01, dat02, .id = &quot;id&quot;) #&gt; # A tibble: 12 × 3 #&gt; id x y #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 1 5 #&gt; 2 1 2 4 #&gt; 3 1 3 3 #&gt; 4 1 4 2 #&gt; 5 1 5 1 #&gt; 6 2 10 5 #&gt; 7 2 11 5.5 #&gt; 8 2 12 6 #&gt; 9 2 13 6.5 #&gt; 10 2 14 7 #&gt; 11 2 15 7.5 #&gt; 12 2 16 8 # Use named inputs for better identification bind_rows(&quot;dat01&quot; = dat01, &quot;dat02&quot; = dat02, .id = &quot;id&quot;) #&gt; # A tibble: 12 × 3 #&gt; id x y #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 dat01 1 5 #&gt; 2 dat01 2 4 #&gt; 3 dat01 3 3 #&gt; 4 dat01 4 2 #&gt; 5 dat01 5 1 #&gt; 6 dat02 10 5 #&gt; 7 dat02 11 5.5 #&gt; 8 dat02 12 6 #&gt; 9 dat02 13 6.5 #&gt; 10 dat02 14 7 #&gt; 11 dat02 15 7.5 #&gt; 12 dat02 16 8 # Bind a list of data frames list01 &lt;- list(&quot;dat01&quot; = dat01, &quot;dat02&quot; = dat02) bind_rows(list01, .id = &quot;source&quot;) #&gt; # A tibble: 12 × 3 #&gt; source x y #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 dat01 1 5 #&gt; 2 dat01 2 4 #&gt; 3 dat01 3 3 #&gt; 4 dat01 4 2 #&gt; 5 dat01 5 1 #&gt; 6 dat02 10 5 #&gt; 7 dat02 11 5.5 #&gt; 8 dat02 12 6 #&gt; 9 dat02 13 6.5 #&gt; 10 dat02 14 7 #&gt; 11 dat02 15 7.5 #&gt; 12 dat02 16 8 # Column binding bind_cols(dat01, dat03) #&gt; # A tibble: 5 × 3 #&gt; x y z #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 5 0.265 #&gt; 2 2 4 0.410 #&gt; 3 3 3 0.780 #&gt; 4 4 2 0.926 #&gt; 5 5 1 0.501 # ----------------------------- # String Manipulation # ----------------------------- names &lt;- c(&quot;Ford, MS&quot;, &quot;Jones, PhD&quot;, &quot;Martin, Phd&quot;, &quot;Huck, MA, MLS&quot;) # Remove everything after the first comma str_remove(names, pattern = &quot;, [[:print:]]+&quot;) #&gt; [1] &quot;Ford&quot; &quot;Jones&quot; &quot;Martin&quot; &quot;Huck&quot; # Explanation: [[:print:]]+ matches one or more printable characters # ----------------------------- # Reshaping Data # ----------------------------- # Wide format data wide &lt;- data.frame( name = c(&quot;Clay&quot;, &quot;Garrett&quot;, &quot;Addison&quot;), test1 = c(78, 93, 90), test2 = c(87, 91, 97), test3 = c(88, 99, 91) ) # Long format data long &lt;- data.frame( name = rep(c(&quot;Clay&quot;, &quot;Garrett&quot;, &quot;Addison&quot;), each = 3), test = rep(1:3, 3), score = c(78, 87, 88, 93, 91, 99, 90, 97, 91) ) # Summary statistics aggregate(score ~ name, data = long, mean) # Mean score per student #&gt; name score #&gt; 1 Addison 92.66667 #&gt; 2 Clay 84.33333 #&gt; 3 Garrett 94.33333 aggregate(score ~ test, data = long, mean) # Mean score per test #&gt; test score #&gt; 1 1 87.00000 #&gt; 2 2 91.66667 #&gt; 3 3 92.66667 # Line plot of scores over tests ggplot(long, aes( x = factor(test), y = score, color = name, group = name )) + geom_point() + geom_line() + xlab(&quot;Test&quot;) + ggtitle(&quot;Test Scores by Student&quot;) # Reshape wide to long pivot_longer(wide, test1:test3, names_to = &quot;test&quot;, values_to = &quot;score&quot;) #&gt; # A tibble: 9 × 3 #&gt; name test score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Clay test1 78 #&gt; 2 Clay test2 87 #&gt; 3 Clay test3 88 #&gt; 4 Garrett test1 93 #&gt; 5 Garrett test2 91 #&gt; 6 Garrett test3 99 #&gt; 7 Addison test1 90 #&gt; 8 Addison test2 97 #&gt; 9 Addison test3 91 # Use names_prefix to clean column names pivot_longer( wide, -name, names_to = &quot;test&quot;, values_to = &quot;score&quot;, names_prefix = &quot;test&quot; ) #&gt; # A tibble: 9 × 3 #&gt; name test score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Clay 1 78 #&gt; 2 Clay 2 87 #&gt; 3 Clay 3 88 #&gt; 4 Garrett 1 93 #&gt; 5 Garrett 2 91 #&gt; 6 Garrett 3 99 #&gt; 7 Addison 1 90 #&gt; 8 Addison 2 97 #&gt; 9 Addison 3 91 # Reshape long to wide with explicit id_cols argument pivot_wider( long, id_cols = name, names_from = test, values_from = score ) #&gt; # A tibble: 3 × 4 #&gt; name `1` `2` `3` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Clay 78 87 88 #&gt; 2 Garrett 93 91 99 #&gt; 3 Addison 90 97 91 # Add a prefix to the resulting columns pivot_wider( long, id_cols = name, names_from = test, values_from = score, names_prefix = &quot;test&quot; ) #&gt; # A tibble: 3 × 4 #&gt; name test1 test2 test3 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Clay 78 87 88 #&gt; 2 Garrett 93 91 99 #&gt; 3 Addison 90 97 91 The verbs of data manipulation select: selecting (or not selecting) columns based on their names (eg: select columns Q1 through Q25) slice: selecting (or not selecting) rows based on their position (eg: select rows 1:10) mutate: add or derive new columns (or variables) based on existing columns (eg: create a new column that expresses measurement in cm based on existing measure in inches) rename: rename variables or change column names (eg: change “GraduationRate100” to “grad100”) filter: selecting rows based on a condition (eg: all rows where gender = Male) arrange: ordering rows based on variable(s) numeric or alphabetical order (eg: sort in descending order of Income) sample: take random samples of data (eg: sample 80% of data to create a “training” set) summarize: condense or aggregate multiple values into single summary values (eg: calculate median income by age group) group_by: convert a tbl into a grouped tbl so that operations are performed “by group”; allows us to summarize data or apply verbs to data by groups (eg, by gender or treatment) the pipe: %&gt;% Use Ctrl + Shift + M (Win) or Cmd + Shift + M (Mac) to enter in RStudio The pipe takes the output of a function and “pipes” into the first argument of the next function. new pipe is |&gt; It should be identical to the old one, except for certain special cases. := (Walrus operator): similar to = , but for cases where you want to use the glue package (i.e., dynamic changes in the variable name in the left-hand side) Writing function in R Tunneling {{ (called curly-curly) allows you to tunnel data-variables through arg-variables (i.e., function arguments) library(tidyverse) # ----------------------------- # Writing Functions with {{ }} # ----------------------------- # Define a custom function using {{ }} get_mean &lt;- function(data, group_var, var_to_mean) { data %&gt;% group_by({{group_var}}) %&gt;% summarize(mean = mean({{var_to_mean}}, na.rm = TRUE)) } # Apply the function data(&quot;mtcars&quot;) mtcars %&gt;% get_mean(group_var = cyl, var_to_mean = mpg) #&gt; # A tibble: 3 × 2 #&gt; cyl mean #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 26.7 #&gt; 2 6 19.7 #&gt; 3 8 15.1 # Dynamically name the resulting variable get_mean &lt;- function(data, group_var, var_to_mean, prefix = &quot;mean_of&quot;) { data %&gt;% group_by({{group_var}}) %&gt;% summarize(&quot;{prefix}_{{var_to_mean}}&quot; := mean({{var_to_mean}}, na.rm = TRUE)) } # Apply the modified function mtcars %&gt;% get_mean(group_var = cyl, var_to_mean = mpg) #&gt; # A tibble: 3 × 2 #&gt; cyl mean_of_mpg #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 26.7 #&gt; 2 6 19.7 #&gt; 3 8 15.1 "],["descriptive-statistics.html", "Chapter 3 Descriptive Statistics", " Chapter 3 Descriptive Statistics When you have an area of interest to research, a problem to solve, or a relationship to investigate, theoretical and empirical processes will help you. Estimand: Defined as “a quantity of scientific interest that can be calculated in the population and does not change its value depending on the data collection design used to measure it (i.e., it does not vary with sample size, survey design, the number of non-respondents, or follow-up efforts).” (Rubin 1996) Examples of estimands include: Population means Population variances Correlations Factor loadings Regression coefficients References "],["numerical-measures.html", "3.1 Numerical Measures", " 3.1 Numerical Measures There are differences between a population and a sample: Measures of Category Population Sample What is it? Reality A small fraction of reality (inference) Characteristics described by Parameters Statistics Central Tendency Mean \\(\\mu = E(Y)\\) \\(\\hat{\\mu} = \\overline{y}\\) Central Tendency Median 50th percentile \\(y_{(\\frac{n+1}{2})}\\) Dispersion Variance \\[\\sigma^2 = var(Y) = E[(Y-\\mu)^2]\\] \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\overline{y})^2\\) Dispersion Coefficient of Variation \\(\\frac{\\sigma}{\\mu}\\) \\(\\frac{s}{\\overline{y}}\\) Dispersion Interquartile Range Difference between 25th and 75th percentiles; robust to outliers Shape Skewness Standardized 3rd central moment (unitless) \\(g_1 = \\frac{\\mu_3}{\\sigma^3}\\) \\(\\hat{g_1} = \\frac{m_3}{m_2^{3/2}}\\) Shape Central moments \\(\\mu=E(Y)\\), \\(\\mu_2 = \\sigma^2 = E[(Y-\\mu)^2]\\), \\(\\mu_3 = E[(Y-\\mu)^3]\\), \\(\\mu_4 = E[(Y-\\mu)^4]\\) \\(m_2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\overline{y})^2\\) \\(m_3 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\overline{y})^3\\) Shape Kurtosis (peakedness and tail thickness) Standardized 4th central moment \\(g_2^* = \\frac{E[(Y-\\mu)^4]}{\\sigma^4}\\) \\(\\hat{g_2} = \\frac{m_4}{m_2^2} - 3\\) Notes: Order Statistics: \\(y_{(1)}, y_{(2)}, \\ldots, y_{(n)}\\), where \\(y_{(1)} &lt; y_{(2)} &lt; \\ldots &lt; y_{(n)}\\). Coefficient of Variation: Defined as the standard deviation divided by the mean. A stable, unitless statistic useful for comparison. Symmetry: Symmetric distributions: Mean = Median; Skewness = 0. Skewed Right: Mean &gt; Median; Skewness &gt; 0. Skewed Left: Mean &lt; Median; Skewness &lt; 0. Central Moments: \\(\\mu = E(Y)\\) \\(\\mu_2 = \\sigma^2 = E[(Y-\\mu)^2]\\) \\(\\mu_3 = E[(Y-\\mu)^3]\\) \\(\\mu_4 = E[(Y-\\mu)^4]\\) Skewness (\\(\\hat{g_1}\\)) Sampling Distribution: For samples drawn from a normal population: \\(\\hat{g_1}\\) is approximately distributed as \\(N(0, \\frac{6}{n})\\) when \\(n &gt; 150\\). Inference: Large Samples: Inference on skewness can be based on the standard normal distribution. The 95% confidence interval for \\(g_1\\) is given by: \\[ \\hat{g_1} \\pm 1.96 \\sqrt{\\frac{6}{n}} \\] Small Samples: For small samples, consult special tables such as: Snedecor and Cochran (1989), Table A 19(i) Monte Carlo test results Kurtosis (\\(\\hat{g_2}\\)) Definitions and Relationships: A normal distribution has kurtosis \\(g_2^* = 3\\). Kurtosis is often redefined as: \\[ g_2 = \\frac{E[(Y - \\mu)^4]}{\\sigma^4} - 3 \\] where the 4th central moment is estimated by: \\[ m_4 = \\frac{\\sum_{i=1}^n (y_i - \\overline{y})^4}{n} \\] Sampling Distribution: For large samples (\\(n &gt; 1000\\)): \\(\\hat{g_2}\\) is approximately distributed as \\(N(0, \\frac{24}{n})\\). Inference: Large Samples: Inference for kurtosis can use standard normal tables. Small Samples: Refer to specialized tables such as: Snedecor and Cochran (1989), Table A 19(ii) Geary (1936) Kurtosis Value Tail Behavior Comparison to Normal Distribution \\(g_2 &gt; 0\\) (Leptokurtic) Heavier Tails Examples: \\(t\\)-distributions \\(g_2 &lt; 0\\) (Platykurtic) Lighter Tails Examples: Uniform or certain bounded distributions \\(g_2 = 0\\) (Mesokurtic) Normal Tails Exactly matches the normal distribution # Generate random data from a normal distribution data &lt;- rnorm(100) # Load the e1071 package for skewness and kurtosis functions library(e1071) # Calculate skewness skewness_value &lt;- skewness(data) cat(&quot;Skewness:&quot;, skewness_value, &quot;\\n&quot;) #&gt; Skewness: 0.362615 # Calculate kurtosis kurtosis_value &lt;- kurtosis(data) cat(&quot;Kurtosis:&quot;, kurtosis_value, &quot;\\n&quot;) #&gt; Kurtosis: -0.3066409 References "],["graphical-measures.html", "3.2 Graphical Measures", " 3.2 Graphical Measures 3.2.1 Shape Properly labeling your graphs is essential to ensure that viewers can easily understand the data presented. Below are several examples of graphical measures used to assess the shape of a dataset. # Generate random data for demonstration purposes data &lt;- rnorm(100) # Histogram: A graphical representation of the distribution of a dataset. hist( data, labels = TRUE, col = &quot;grey&quot;, breaks = 12, main = &quot;Histogram of Random Data&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot; ) # Interactive Histogram: Using &#39;highcharter&#39; for a more interactive visualization. # pacman::p_load(&quot;highcharter&quot;) # hchart(data, type = &quot;column&quot;, name = &quot;Random Data Distribution&quot;) # Box-and-Whisker Plot: Useful for visualizing the distribution and identifying outliers. boxplot( count ~ spray, data = InsectSprays, col = &quot;lightgray&quot;, main = &quot;Boxplot of Insect Sprays&quot;, xlab = &quot;Spray Type&quot;, ylab = &quot;Count&quot; ) # Notched Boxplot: The notches indicate a confidence interval around the median. boxplot( len ~ supp * dose, data = ToothGrowth, notch = TRUE, col = c(&quot;gold&quot;, &quot;darkgreen&quot;), main = &quot;Tooth Growth by Supplement and Dose&quot;, xlab = &quot;Supplement and Dose&quot;, ylab = &quot;Length&quot; ) # If the notches of two boxes do not overlap, this suggests that the medians differ significantly. # Stem-and-Leaf Plot: Provides a quick way to visualize the distribution of data. stem(data) #&gt; #&gt; The decimal point is at the | #&gt; #&gt; -2 | 4321000 #&gt; -1 | 87665 #&gt; -1 | 44433222111000 #&gt; -0 | 998888886666665555 #&gt; -0 | 433322221100 #&gt; 0 | 0112233333344 #&gt; 0 | 5666677888999999 #&gt; 1 | 0111122344 #&gt; 1 | 699 #&gt; 2 | 34 # Bagplot - A 2D Boxplot Extension: Visualizes the spread and identifies outliers in two-dimensional data. pacman::p_load(aplpack) attach(mtcars) bagplot(wt, mpg, xlab = &quot;Car Weight&quot;, ylab = &quot;Miles Per Gallon&quot;, main = &quot;Bagplot of Car Weight vs. Miles Per Gallon&quot;) detach(mtcars) Below are some advanced plot types that can provide deeper insights into data: # boxplot.matrix(): Creates boxplots for each column in a matrix. Useful for comparing multiple variables. graphics::boxplot.matrix( cbind( Uni05 = (1:100) / 21, Norm = rnorm(100), T5 = rt(100, df = 5), Gam2 = rgamma(100, shape = 2) ), main = &quot;Boxplot Marix&quot;, notch = TRUE, col = 1:4 ) # Violin Plot (vioplot()): Combines a boxplot with a density plot, providing more information about the distribution. library(&quot;vioplot&quot;) vioplot(data, col = &quot;lightblue&quot;, main = &quot;Violin Plot Example&quot;) 3.2.2 Scatterplot Scatterplots are useful for visualizing relationships between two continuous variables. They help identify patterns, correlations, and outliers. Pairwise Scatterplots: Visualizes relationships between all pairs of variables in a dataset. This is especially useful for exploring potential correlations. pairs(mtcars, main = &quot;Pairwise Scatterplots&quot;, pch = 19, col = &quot;blue&quot;) "],["normality-assessment.html", "3.3 Normality Assessment", " 3.3 Normality Assessment The Normal (Gaussian) distribution plays a critical role in statistical analyses due to its theoretical and practical applications. Many statistical methods assume normality in the data, making it essential to assess whether our variable of interest follows a normal distribution. To achieve this, we utilize both Numerical Measures and Graphical Assessment. 3.3.1 Graphical Assessment Graphical methods provide an intuitive way to visually inspect the normality of a dataset. One of the most common methods is the Q-Q plot (quantile-quantile plot). The Q-Q plot compares the quantiles of the sample data to the quantiles of a theoretical normal distribution. Deviations from the line indicate departures from normality. Below is an example of using the qqnorm and qqline functions in R to assess the normality of the precip dataset, which contains precipitation data (in inches per year) for 70 U.S. cities: # Load the required package pacman::p_load(&quot;car&quot;) # Generate a Q-Q plot qqnorm(precip, ylab = &quot;Precipitation [in/yr] for 70 US cities&quot;, main = &quot;Q-Q Plot of Precipitation Data&quot;) qqline(precip, col = &quot;red&quot;) Interpretation Theoretical Line: The red line represents the expected relationship if the data were perfectly normally distributed. Data Points: The dots represent the actual empirical data. If the points closely align with the theoretical line, we can conclude that the data likely follow a normal distribution. However, noticeable deviations from the line, particularly systematic patterns (e.g., curves or s-shaped patterns), indicate potential departures from normality. Tips Small Deviations: Minor deviations from the line in small datasets are not uncommon and may not significantly impact analyses that assume normality. Systematic Patterns: Look for clear trends, such as clusters or s-shaped curves, which suggest skewness or heavy tails. Complementary Tests: Always pair graphical methods with numerical measures (e.g., Shapiro-Wilk test) to make a robust conclusion. When interpreting a Q-Q plot, it is helpful to see both ideal and non-ideal scenarios. Below is an illustrative example: Normal Data: Points fall closely along the line. Skewed Data: Points systematically deviate from the line, curving upward or downward. Heavy Tails: Points deviate at the extremes (ends) of the distribution. By combining visual inspection and numerical measures, we can better understand the nature of our data and its alignment with the assumption of normality. 3.3.2 Summary Statistics While graphical assessments, such as Q-Q plots, provide a visual indication of normality, they may not always offer a definitive conclusion. To supplement graphical methods, statistical tests are often employed. These tests provide quantitative evidence to support or refute the assumption of normality. The most common methods can be classified into two categories: Methods Based on Normal Probability Plot Correlation Coefficient with Normal Probability Plots Shapiro-Wilk Test Methods based on empirical cumulative distribution function Anderson-Darling Test Kolmogorov-Smirnov Test Cramer-von Mises Test Jarque–Bera Test 3.3.2.1 Methods Based on Normal Probability Plot 3.3.2.1.1 Correlation Coefficient with Normal Probability Plots As described by Looney and Gulledge Jr (1985) and Samuel S. Shapiro and Francia (1972), this method evaluates the linearity of a normal probability plot by calculating the correlation coefficient between the ordered sample values \\(y_{(i)}\\) and their theoretical normal quantiles \\(m_i^*\\). A perfectly linear relationship suggests that the data follow a normal distribution. The correlation coefficient, denoted \\(W^*\\), is given by: \\[ W^* = \\frac{\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})(m_i^* - 0)}{\\sqrt{\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})^2 \\cdot \\sum_{i=1}^{n}(m_i^* - 0)^2}} \\] where: \\(\\bar{y}\\) is the sample mean, \\(\\bar{m^*} = 0\\) under the null hypothesis of normality. The Pearson product-moment correlation formula can also be used to evaluate this relationship: \\[ \\hat{\\rho} = \\frac{\\sum_{i=1}^{n}(y_i - \\bar{y})(x_i - \\bar{x})}{\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2 \\cdot \\sum_{i=1}^{n}(x_i - \\bar{x})^2}} \\] Interpretation: When the correlation is 1, the plot is exactly linear, and normality is assumed. The closer the correlation is to 0, the stronger the evidence to reject normality. Inference on \\(W^*\\) requires reference to special tables (Looney and Gulledge Jr 1985). library(&quot;EnvStats&quot;) # Perform Probability Plot Correlation Coefficient (PPCC) Test gofTest(data, test = &quot;ppcc&quot;)$p.value # Probability Plot Correlation Coefficient #&gt; [1] 0.3701575 3.3.2.1.2 Shapiro-Wilk Test The Shapiro-Wilk test (Samuel Sanford Shapiro and Wilk 1965) is one of the most widely used tests for assessing normality, especially for sample sizes \\(n &lt; 2000\\). This test evaluates how well the data’s order statistics match a theoretical normal distribution. The test statistic, \\(W\\), is computed as: \\[ W=\\frac{\\sum_{i=1}^{n}a_i x_{(i)}}{\\sum_{i=1}^{n}(x_{(i)}-\\bar{x})^2} \\] where \\(n\\): The sample size. \\(x_{(i)}\\): The \\(i\\)-th smallest value in the sample (the ordered data). \\(\\bar{x}\\): The sample mean. \\(a_i\\): Weights derived from the expected values and variances of the order statistics of a normal distribution, precomputed based on the sample size \\(n\\). Sensitive to: Symmetry The Shapiro-Wilk test assesses whether a sample is drawn from a normal distribution, which assumes symmetry around the mean. If the data exhibit skewness (a lack of symmetry), the test is likely to reject the null hypothesis of normality. Heavy Tails Heavy tails refer to distributions where extreme values (outliers) are more likely compared to a normal distribution. The Shapiro-Wilk test is also sensitive to such departures from normality because heavy tails affect the spread and variance, which are central to the calculation of the test statistic \\(W\\). Hence, the Shapiro-Wilk test’s sensitivity to these deviations makes it a powerful tool for detecting non-normality only in small to moderate-sized samples. However: It is generally more sensitive to symmetry (skewness) than to tail behavior (kurtosis). In very large samples, even small deviations in symmetry or tail behavior may cause the test to reject the null hypothesis, even if the data is practically “normal” for the intended analysis. Small sample sizes may lack power to detect deviations from normality. Large sample sizes may detect minor deviations that are not practically significant. Key Steps: Sort the Data: Arrange the sample data in ascending order, yielding \\(x_{(1)}, x_{(2)}, \\dots, x_{(n)}\\). Compute Weights: The weights \\(a_i\\) are determined using a covariance matrix of the normal order statistics. These are optimized to maximize the power of the test. Calculate \\(W\\): Use the formula to determine \\(W\\), which ranges from 0 to 1. Decision Rule: Null Hypothesis (\\(H_0\\)): The data follows a normal distribution. Alternative Hypothesis (\\(H_1\\)): The data does not follow a normal distribution. A small \\(W\\) value, along with a \\(p\\)-value below a chosen significance level (e.g., 0.05), leads to rejection of \\(H_0\\). Under normality, \\(W\\) approaches 1. Smaller values of \\(W\\) indicate deviations from normality. # Perform Shapiro-Wilk Test (Default for gofTest) EnvStats::gofTest(mtcars$mpg, test = &quot;sw&quot;) #&gt; #&gt; Results of Goodness-of-Fit Test #&gt; ------------------------------- #&gt; #&gt; Test Method: Shapiro-Wilk GOF #&gt; #&gt; Hypothesized Distribution: Normal #&gt; #&gt; Estimated Parameter(s): mean = 20.090625 #&gt; sd = 6.026948 #&gt; #&gt; Estimation Method: mvue #&gt; #&gt; Data: mtcars$mpg #&gt; #&gt; Sample Size: 32 #&gt; #&gt; Test Statistic: W = 0.9475647 #&gt; #&gt; Test Statistic Parameter: n = 32 #&gt; #&gt; P-value: 0.1228814 #&gt; #&gt; Alternative Hypothesis: True cdf does not equal the #&gt; Normal Distribution. 3.3.2.2 Methods Based on Empirical Cumulative Distribution Function The Empirical Cumulative Distribution Function (ECDF) is a way to represent the distribution of a sample dataset in cumulative terms. It answers the question: “What fraction of the observations in my dataset are less than or equal to a given value \\(x\\)?” The ECDF is defined as: \\[ F_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}(X_i \\leq x) \\] where: \\(F_(x)\\): ECDF at value \\(x\\). \\(n\\): Total number of data points. \\(\\mathbb{I}(X_i \\leq x)\\): Indicator function, equal to 1 if \\(X_i \\leq x\\), otherwise 0. This method is especially useful for large sample sizes and can be applied to distributions beyond the normal (Gaussian) distribution. Properties of the ECDF Step Function: The ECDF is a step function that increases by \\(1/n\\) at each data point. Non-decreasing: As \\(x\\) increases, \\(F_n(x)\\) never decreases. Range: The ECDF starts at 0 and ends at 1: \\(F_n(x) = 0\\) for \\(x &lt; \\min(X)\\). \\(F_n(x) = 1\\) for \\(x \\geq \\max(X)\\). Convergence: As \\(n \\to \\infty\\), the ECDF approaches the true cumulative distribution function (CDF) of the population. Let’s consider a sample dataset \\(\\{3, 7, 7, 10, 15\\}\\). The ECDF at different values of \\(x\\) is calculated as: \\(x\\) \\(\\mathbb{I}(X_i \\leq x)\\) for each \\(X_i\\) Count \\(\\leq x\\) ECDF \\(F_n(x)\\) \\(x = 5\\) \\(\\{1, 0, 0, 0, 0\\}\\) 1 \\(1/5 = 0.2\\) \\(x = 7\\) \\(\\{1, 1, 1, 0, 0\\}\\) 3 \\(3/5 = 0.6\\) \\(x = 12\\) \\(\\{1, 1, 1, 1, 0\\}\\) 4 \\(4/5 = 0.8\\) \\(x = 15\\) \\(\\{1, 1, 1, 1, 1\\}\\) 5 \\(5/5 = 1.0\\) Applications of the ECDF Goodness-of-fit Tests: Compare the ECDF to a theoretical CDF (e.g., using the Kolmogorov-Smirnov test). Outlier Detection: Analyze cumulative trends to spot unusual data points. Visual Data Exploration: Use the ECDF to understand the spread, skewness, and distribution of the data. Comparing Distributions: Compare the ECDFs of two datasets to assess differences in their distributions. # Load required libraries library(ggplot2) # Sample dataset data &lt;- c(3, 7, 7, 10, 15) # ECDF calculation ecdf_function &lt;- ecdf(data) # Generate a data frame for plotting ecdf_data &lt;- data.frame(x = sort(unique(data)), ecdf = sapply(sort(unique(data)), function(x) mean(data &lt;= x))) # Display ECDF values print(ecdf_data) #&gt; x ecdf #&gt; 1 3 0.2 #&gt; 2 7 0.6 #&gt; 3 10 0.8 #&gt; 4 15 1.0 # Plot the ECDF ggplot(ecdf_data, aes(x = x, y = ecdf)) + geom_step() + labs( title = &quot;Empirical Cumulative Distribution Function&quot;, x = &quot;Data Values&quot;, y = &quot;Cumulative Proportion&quot; ) + theme_minimal() # Alternatively plot.ecdf(as.numeric(mtcars[1, ]), verticals = TRUE, do.points = FALSE) 3.3.2.2.1 Anderson-Darling Test The Anderson-Darling test statistic (T. W. Anderson and Darling 1952) is given by: \\[ A^2 = \\int_{-\\infty}^{\\infty} \\frac{\\left(F_n(t) - F(t)\\right)^2}{F(t)(1 - F(t))} dF(t) \\] This test calculates a weighted average of squared deviations between the empirical cumulative distribution function (CDF), \\(F_n(t)\\), and the theoretical CDF, \\(F(t)\\). More weight is given to deviations in the tails of the distribution, which makes the test particularly sensitive to these regions. For a sample of size \\(n\\), with ordered observations \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\), the Anderson-Darling test statistic can also be written as: \\[ A^2 = -n - \\frac{1}{n} \\sum_{i=1}^n \\left[ (2i - 1) \\ln(F(y_{(i)})) + (2n + 1 - 2i) \\ln(1 - F(y_{(i)})) \\right] \\] For the normal distribution, the test statistic is further simplified. Using the transformation: \\[ p_i = \\Phi\\left(\\frac{y_{(i)} - \\bar{y}}{s}\\right), \\] where: \\(p_i\\) is the cumulative probability under the standard normal distribution, \\(y_{(i)}\\) are the ordered sample values, \\(\\bar{y}\\) is the sample mean, \\(s\\) is the sample standard deviation, the formula becomes: \\[ A^2 = -n - \\frac{1}{n} \\sum_{i=1}^n \\left[ (2i - 1) \\ln(p_i) + (2n + 1 - 2i) \\ln(1 - p_i) \\right]. \\] Key Features of the Test CDF-Based Weighting: The Anderson-Darling test gives more weight to deviations in the tails, which makes it particularly sensitive to detecting non-normality in these regions. Sensitivity: Compared to other goodness-of-fit tests, such as the Kolmogorov-Smirnov Test, the Anderson-Darling test is better at identifying differences in the tails of the distribution. Integral Form: The test statistic can also be expressed as an integral over the theoretical CDF: \\[ A^2 = n \\int_{-\\infty}^\\infty \\frac{\\left[F_n(t) - F(t)\\right]^2}{F(t)(1 - F(t))} dF(t), \\] where \\(F_n(t)\\) is the empirical CDF, and \\(F(t)\\) is the specified theoretical CDF. Applications: Testing for normality or other distributions (e.g., exponential, Weibull). Validating assumptions in statistical models. Comparing data to theoretical distributions. Hypothesis Testing Null Hypothesis (\\(H_0\\)): The data follows the specified distribution (e.g., normal distribution). Alternative Hypothesis (\\(H_1\\)): The data does not follow the specified distribution. The null hypothesis is rejected if \\(A^2\\) is too large, indicating a poor fit to the specified distribution. Critical values for the test statistic are provided by (Marsaglia and Marsaglia 2004) and (Stephens 1974). Applications to Other Distributions The Anderson-Darling test can be applied to various distributions by using specific transformation methods. Examples include: Exponential Logistic Gumbel Extreme-value Weibull (after logarithmic transformation: \\(\\log(\\text{Weibull}) = \\text{Gumbel}\\)) Gamma Cauchy von Mises Log-normal (two-parameter) For more details on transformations and critical values, consult (Stephens 1974). # Perform Anderson-Darling Test library(nortest) ad_test_result &lt;- ad.test(mtcars$mpg) # Output the test statistic and p-value ad_test_result #&gt; #&gt; Anderson-Darling normality test #&gt; #&gt; data: mtcars$mpg #&gt; A = 0.57968, p-value = 0.1207 Alternatively, for a broader range of distributions, use the gofTest function from the gof package: # General goodness-of-fit test with Anderson-Darling library(EnvStats) gof_test_result &lt;- EnvStats::gofTest(mtcars$mpg, test = &quot;ad&quot;) # Extract the p-value gof_test_result$p.value #&gt; [1] 0.1207371 3.3.2.2.2 Kolmogorov-Smirnov Test The Kolmogorov-Smirnov (K-S) test is a nonparametric test that compares the empirical cumulative distribution function (ECDF) of a sample to a theoretical cumulative distribution function (CDF), or compares the ECDFs of two samples. It is used to assess whether a sample comes from a specific distribution (one-sample test) or to compare two samples (two-sample test). The test statistic \\(D_n\\) for the one-sample test is defined as: \\[ D_n = \\sup_x \\left| F_n(x) - F(x) \\right|, \\] where: \\(F_n(x)\\) is the empirical CDF of the sample, \\(F(x)\\) is the theoretical CDF under the null hypothesis, \\(\\sup_x\\) denotes the supremum (largest value) over all possible values of \\(x\\). For the two-sample K-S test, the statistic is: \\[ D_{n,m} = \\sup_x \\left| F_{n,1}(x) - F_{m,2}(x) \\right|, \\] where \\(F_{n,1}(x)\\) and \\(F_{m,2}(x)\\) are the empirical CDFs of the two samples, with sizes \\(n\\) and \\(m\\), respectively. Hypotheses Null hypothesis (\\(H_0\\)): The sample comes from the specified distribution (one-sample) or the two samples are drawn from the same distribution (two-sample). Alternative hypothesis (\\(H_1\\)): The sample does not come from the specified distribution (one-sample) or the two samples are drawn from different distributions (two-sample). Properties Based on the Largest Deviation: The K-S test is sensitive to the largest absolute difference between the empirical and expected CDFs, making it effective for detecting shifts in location or scale. Distribution-Free: The test does not assume a specific distribution for the data under the null hypothesis. Its significance level is determined from the distribution of the test statistic under the null hypothesis. Limitations: The test is more sensitive near the center of the distribution than in the tails. It may not perform well with discrete data or small sample sizes. Related Tests: Kuiper’s Test: A variation of the K-S test that is sensitive to deviations in both the center and tails of the distribution. The Kuiper test statistic is: \\[ V_n = D^+ + D^-, \\] where \\(D^+\\) and \\(D^-\\) are the maximum positive and negative deviations of the empirical CDF from the theoretical CDF. Applications Testing for normality or other specified distributions. Comparing two datasets to determine if they are drawn from the same distribution. To perform a one-sample K-S test in R, use the ks.test() function. To check the goodness of fit for a specific distribution, the gofTest() function from a package like DescTools can also be used. # One-sample Kolmogorov-Smirnov test for normality data &lt;- rnorm(50) # Generate random normal data ks.test(data, &quot;pnorm&quot;, mean(data), sd(data)) #&gt; #&gt; Exact one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: data #&gt; D = 0.098643, p-value = 0.6785 #&gt; alternative hypothesis: two-sided # Goodness-of-fit test using gofTest library(DescTools) gofTest(data, test = &quot;ks&quot;)$p.value # Kolmogorov-Smirnov test p-value #&gt; [1] 0.6785444 Advantages: Simple and widely applicable. Distribution-free under the null hypothesis. Limitations: Sensitive to sample size: small deviations may lead to significance in large samples. Reduced sensitivity to differences in the tails compared to the Anderson-Darling test. The Kolmogorov-Smirnov test provides a general-purpose method for goodness-of-fit testing and sample comparison, with particular utility in detecting central deviations. 3.3.2.2.3 Cramer-von Mises Test The Cramer-von Mises (CVM) test is a nonparametric goodness-of-fit test that evaluates the agreement between the empirical cumulative distribution function (ECDF) of a sample and a specified theoretical cumulative distribution function (CDF). Unlike the Kolmogorov-Smirnov test, which focuses on the largest discrepancy, the Cramer-von Mises test considers the average squared discrepancy across the entire distribution. Unlike the Anderson-Darling test, it weights all parts of the distribution equally. The test statistic \\(W^2\\) for the one-sample Cramer-von Mises test is defined as: \\[ W^2 = n \\int_{-\\infty}^\\infty \\left[ F_n(t) - F(t) \\right]^2 dF(t), \\] where: \\(F_n(t)\\) is the empirical CDF, \\(F(t)\\) is the specified theoretical CDF under the null hypothesis, \\(n\\) is the sample size. In practice, \\(W^2\\) is computed using the ordered sample values \\(y_{(1)}, y_{(2)}, \\dots, y_{(n)}\\) as: \\[ W^2 = \\sum_{i=1}^n \\left( F(y_{(i)}) - \\frac{2i - 1}{2n} \\right)^2 + \\frac{1}{12n}, \\] where: \\(F(y_{(i)})\\) is the theoretical CDF evaluated at the ordered sample values \\(y_{(i)}\\). Hypotheses Null hypothesis (H0): The sample data follow the specified distribution. Alternative hypothesis (H1): The sample data do not follow the specified distribution. Properties Focus on Average Discrepancy: The Cramer-von Mises test measures the overall goodness-of-fit by considering the squared deviations across all points in the distribution, ensuring equal weighting of discrepancies. Comparison to Anderson-Darling: Unlike the Anderson-Darling test, which gives more weight to deviations in the tails, the CVM test weights all parts of the distribution equally. Integral Representation: The statistic is expressed as an integral over the squared differences between the empirical and theoretical CDFs. Two-Sample Test: The Cramer-von Mises framework can also be extended to compare two empirical CDFs. The two-sample statistic is based on the pooled empirical CDF. Applications Assessing goodness-of-fit for a theoretical distribution (e.g., normal, exponential, Weibull). Comparing two datasets to determine if they are drawn from similar distributions. Validating model assumptions. To perform a Cramer-von Mises test in R, the gofTest() function from the DescTools package can be used. Below is an example: # Generate random normal data data &lt;- rnorm(50) # Perform the Cramer-von Mises test library(DescTools) gofTest(data, test = &quot;cvm&quot;)$p.value # Cramer-von Mises test p-value #&gt; [1] 0.04846959 Advantages: Considers discrepancies across the entire distribution. Robust to outliers due to equal weighting. Simple to compute and interpret. Limitations: Less sensitive to deviations in the tails compared to the Anderson-Darling test. May be less powerful than the Kolmogorov-Smirnov test in detecting central shifts. 3.3.2.2.4 Jarque-Bera Test The Jarque-Bera (JB) test (Bera and Jarque 1981) is a goodness-of-fit test used to check whether a dataset follows a normal distribution. It is based on the skewness and kurtosis of the data, which measure the asymmetry and the “tailedness” of the distribution, respectively. The Jarque-Bera test statistic is defined as: \\[ JB = \\frac{n}{6}\\left(S^2 + \\frac{(K - 3)^2}{4}\\right), \\] where: \\(n\\) is the sample size, \\(S\\) is the sample skewness, \\(K\\) is the sample kurtosis. Skewness (\\(S\\)) is calculated as: \\[ S = \\frac{\\hat{\\mu}_3}{\\hat{\\sigma}^3} = \\frac{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^3}{\\left(\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\\right)^{3/2}}, \\] where: \\(\\hat{\\mu}_3\\) is the third central moment, \\(\\hat{\\sigma}\\) is the standard deviation, \\(\\bar{x}\\) is the sample mean. Kurtosis (\\(K\\)) is calculated as: \\[ K = \\frac{\\hat{\\mu}_4}{\\hat{\\sigma}^4} = \\frac{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^4}{\\left(\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\\right)^2}, \\] where: \\(\\hat{\\mu}_4\\) is the fourth central moment. Hypothesis Null hypothesis (\\(H_0\\)): The data follow a normal distribution, implying: Skewness \\(S = 0\\), Excess kurtosis \\(K - 3 = 0\\). Alternative hypothesis (\\(H_1\\)): The data do not follow a normal distribution. Distribution of the JB Statistic Under the null hypothesis, the Jarque-Bera statistic asymptotically follows a chi-squared distribution with 2 degrees of freedom: \\[ JB \\sim \\chi^2_2. \\] Properties Sensitivity: Skewness (\\(S\\)) captures asymmetry in the data. Kurtosis (\\(K\\)) measures how heavy-tailed or light-tailed the distribution is compared to a normal distribution. Limitations: The test is sensitive to large sample sizes; even small deviations from normality may result in rejection of \\(H_0\\). Assumes that the data are independently and identically distributed. Applications Testing normality in regression residuals. Validating distributional assumptions in econometrics and time series analysis. The Jarque-Bera test can be performed in R using the tseries package: library(tseries) # Generate a sample dataset data &lt;- rnorm(100) # Normally distributed data # Perform the Jarque-Bera test jarque.bera.test(data) #&gt; #&gt; Jarque Bera Test #&gt; #&gt; data: data #&gt; X-squared = 0.89476, df = 2, p-value = 0.6393 References "],["bivariate-statistics.html", "3.4 Bivariate Statistics", " 3.4 Bivariate Statistics Bivariate statistics involve the analysis of relationships between two variables. Understanding these relationships can provide insights into patterns, associations, or (suggestive of) causal connections. Below, we explore the correlation between different types of variables: Two Continuous Variables Two Discrete Variables Categorical and Continuous Variables Before delving into the analysis, it is critical to consider the following: Is the relationship linear or non-linear? Linear relationships can be modeled with simpler statistical methods such as Pearson’s correlation, while non-linear relationships may require alternative approaches, such as Spearman’s rank correlation or regression with transformations. If the variable is continuous, is it normal and homoskedastic? For parametric methods like Pearson’s correlation, assumptions such as normality and homoskedasticity (equal variance) must be met. When these assumptions fail, non-parametric methods like Spearman’s correlation or robust alternatives are preferred. How big is your dataset? Large datasets can reveal subtle patterns but may lead to statistically significant results that are not practically meaningful. For smaller datasets, careful selection of statistical methods is essential to ensure reliability and validity. Categorical Continuous Categorical Chi-squared Test Phi Coefficient Cramer’s V Tschuprow’s T Spearman’s Rank Correlation Kendall’s Tau Gamma Statistic Freeman’s Theta Epsilon-squared Goodman Kruskal’s Gamma Somers’ D Kendall’s Tau-b Yule’s Q and Y Tetrachoric Correlation Polychoric Correlation Continuous Point-Biserial Correlation Logistic Regression Pearson Correlation Spearman Correlation 3.4.1 Two Continuous set.seed(1) n = 100 # (sample size) data = data.frame(A = sample(1:20, replace = TRUE, size = n), B = sample(1:30, replace = TRUE, size = n)) 3.4.1.1 Pearson Correlation Pearson correlation quantifies the strength and direction of a linear relationship between two continuous variables. Formula: \\[ r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\cdot \\sum (y_i - \\bar{y})^2}} \\] where \\(x_i, y_i\\): Individual data points of variables \\(X\\) and \\(Y\\). \\(\\bar{x}, \\bar{y}\\): Means of \\(X\\) and \\(Y\\). Assumptions: The relationship between variables is linear. Variables are normally distributed. Data exhibits homoscedasticity (equal variance of \\(Y\\) for all values of \\(X\\)). Use Case: Use when the relationship is expected to be linear, and assumptions of normality and homoscedasticity are met. Interpretation: \\(r = +1\\): Perfect positive linear relationship. \\(r = -1\\): Perfect negative linear relationship. \\(r = 0\\): No linear relationship. # Pearson correlation pearson_corr &lt;- stats::cor(data$A, data$B, method = &quot;pearson&quot;) cat(&quot;Pearson Correlation (r):&quot;, pearson_corr, &quot;\\n&quot;) #&gt; Pearson Correlation (r): 0.02394939 3.4.1.2 Spearman Correlation Spearman correlation measures the strength of a monotonic relationship between two variables. It ranks the data and calculates correlation based on ranks. Formula: \\[ \\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 -1)} \\] where \\(d_i\\): Difference between the ranks of \\(x_i\\) and \\(y_i\\). \\(n\\): Number of paired observations. Assumptions: Relationship must be monotonic, not necessarily linear. No assumptions about the distribution of variables. Use Case: Use when data is ordinal or when normality and linearity assumptions are violated. Interpretation: \\(\\rho = +1\\): Perfect positive monotonic relationship. \\(\\rho = -1\\): Perfect negative monotonic relationship. \\(\\rho = 0\\): No monotonic relationship. # Spearman correlation spearman_corr &lt;- stats::cor(data$A, data$B, method = &quot;spearman&quot;) cat(&quot;Spearman Correlation (rho):&quot;, spearman_corr, &quot;\\n&quot;) #&gt; Spearman Correlation (rho): 0.02304636 3.4.1.3 Kendall’s Tau Correlation Kendall’s Tau measures the strength of a monotonic relationship by comparing concordant and discordant pairs. Formula: \\[ \\tau = \\frac{(C- D)}{\\binom{n}{2}} \\] where​ \\(C\\): Number of concordant pairs (where ranks of \\(X\\) and \\(Y\\) increase or decrease together). \\(D\\): Number of discordant pairs (where one rank increases while the other decreases). \\(\\binom{n}{2}\\): Total number of possible pairs. Assumptions: No specific assumptions about the data distribution. Measures monotonic relationships. Use Case: Preferred for small datasets or when data contains outliers. Interpretation: \\(\\tau = +1\\): Perfect positive monotonic relationship. \\(\\tau = -1\\): Perfect negative monotonic relationship. \\(\\tau = 0\\): No monotonic relationship. # Kendall&#39;s Tau correlation kendall_corr &lt;- stats::cor(data$A, data$B, method = &quot;kendall&quot;) cat(&quot;Kendall&#39;s Tau Correlation (tau):&quot;, kendall_corr, &quot;\\n&quot;) #&gt; Kendall&#39;s Tau Correlation (tau): 0.02171284 3.4.1.4 Distance Correlation Distance Correlation measures both linear and non-linear relationships between variables. It does not require monotonicity or linearity. Formula: \\[ d Cor = \\frac{d Cov(X,Y)}{\\sqrt{d Var (X) \\cdot d Var (Y)}} \\] where​ \\(dCov\\): Distance covariance between \\(X\\) and \\(Y\\). \\(dVar\\): Distance variances of \\(X\\) and \\(Y\\). Assumptions: No specific assumptions about the relationship (linear, monotonic, or otherwise). Use Case: Use for complex relationships, including non-linear patterns. Interpretation: \\(dCor = 0\\): No association. \\(dCor = 1\\): Perfect association. # Distance correlation distance_corr &lt;- energy::dcor(data$A, data$B) cat(&quot;Distance Correlation (dCor):&quot;, distance_corr, &quot;\\n&quot;) #&gt; Distance Correlation (dCor): 0.1008934 3.4.1.5 Summary Table of Correlation Methods Method Formula/Approach Detects Relationship Type Assumptions Sensitivity to Outliers Use Case Pearson Linear covariance Linear Normality, homoscedasticity High Linear relationships. Spearman Ranks and monotonicity formula Monotonic None Moderate Monotonic, non-linear data. Kendall’s Tau Concordance/discordance ratio Monotonic None Low Small datasets, robust to outliers. Distance Correlation Distance-based variance Linear and non-linear None Low Complex, non-linear relationships. 3.4.2 Categorical and Continuous Analyzing the relationship between a categorical variable (binary or multi-class) and a continuous variable requires specialized techniques. These methods assess whether the categorical variable significantly influences the continuous variable or vice versa. We focus on the following methods: Point-Biserial Correlation Logistic Regression Analysis of Variance (ANOVA) T-test 3.4.2.1 Point-Biserial Correlation The Point-Biserial Correlation is a special case of the Pearson correlation used to assess the relationship between a binary categorical variable (coded as 0 and 1) and a continuous variable. It measures the strength and direction of the linear relationship. Formula: \\[ r_{pb} = \\frac{\\bar{Y_1} - \\bar{Y_0}}{s_Y} \\sqrt{\\frac{n_1 n_0}{n^2}} \\] where \\(\\bar{Y_1}\\), \\(\\bar{Y_0}\\): Mean of the continuous variable for the groups coded as 1 and 0, respectively. \\(s_Y\\): Standard deviation of the continuous variable. \\(n_1, n_0\\): Number of observations in each group (1 and 0). \\(n\\): Total number of observations. Key Properties: Range: \\(-1\\) to \\(1\\). \\(r_{pb} = +1\\): Perfect positive correlation. \\(r_{pb} = -1\\): Perfect negative correlation. \\(r_{pb} = 0\\): No linear relationship. A positive \\(r_{pb}\\) indicates higher values of the continuous variable are associated with the 1 group, while a negative \\(r_{pb}\\) indicates the opposite. Assumptions: The binary variable is truly dichotomous (e.g., male/female, success/failure). The continuous variable is approximately normally distributed. Homogeneity of variance across the two groups (not strictly required but recommended). Use Case: To evaluate the linear relationship between a binary categorical variable and a continuous variable. library(ltm) # Point-Biserial Correlation biserial_corr &lt;- ltm::biserial.cor( c(12.5, 15.3, 10.7, 18.1, 11.2, 16.8, 13.4, 14.9), c(0, 1, 0, 1, 0, 1, 0, 1), use = &quot;all.obs&quot;, level = 2 ) cat(&quot;Point-Biserial Correlation:&quot;, biserial_corr, &quot;\\n&quot;) #&gt; Point-Biserial Correlation: 0.8792835 3.4.2.2 Logistic Regression Logistic Regression models the relationship between a binary categorical variable (dependent variable) and one or more independent variables (which may include continuous variables). It predicts the probability of the binary outcome (e.g., success/failure, yes/no). Refer to 3.4.2.2 for more detail. Formula: The logistic regression model is represented as: \\[ \\text{logit}(p) = \\ln \\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 X \\] where \\(p\\): Probability of the outcome being 1. \\(\\beta_0\\): Intercept. \\(\\beta_1\\): Coefficient for the continuous variable \\(X\\). \\(\\text{logit}(p)\\): Log-odds of the probability. Key Features: Output: Odds ratio or probability of the binary outcome. Can include multiple predictors (continuous and categorical). Non-linear transformation ensures predictions are probabilities between 0 and 1. Assumptions: The dependent variable is binary. Observations are independent. There is a linear relationship between the logit of the dependent variable and the independent variable. No multicollinearity between predictors. Use Case: To predict the likelihood of a binary outcome based on a continuous predictor (e.g., probability of success given test scores). # Simulated data set.seed(123) x &lt;- rnorm(100, mean = 50, sd = 10) # Continuous predictor y &lt;- ifelse(x &gt; 55, 1, 0) # Binary outcome based on threshold # Logistic Regression logistic_model &lt;- glm(y ~ x, family = binomial) summary(logistic_model) #&gt; #&gt; Call: #&gt; glm(formula = y ~ x, family = binomial) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.770e-04 -2.100e-08 -2.100e-08 2.100e-08 2.548e-04 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -3749.9 495083.0 -0.008 0.994 #&gt; x 67.9 8966.6 0.008 0.994 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 1.2217e+02 on 99 degrees of freedom #&gt; Residual deviance: 1.4317e-07 on 98 degrees of freedom #&gt; AIC: 4 #&gt; #&gt; Number of Fisher Scoring iterations: 25 # Predicted probabilities predicted_probs &lt;- predict(logistic_model, type = &quot;response&quot;) print(head(predicted_probs)) #&gt; 1 2 3 4 5 6 #&gt; -735.6466 -511.3844 703.2134 -307.2281 -267.3187 809.3747 # Visualize logistic regression curve library(ggplot2) data &lt;- data.frame(x = x, y = y, predicted = predicted_probs) ggplot(data, aes(x = x, y = predicted)) + geom_point(aes(y = y), color = &quot;red&quot;, alpha = 0.5) + geom_line(color = &quot;blue&quot;) + labs(title = &quot;Logistic Regression: Continuous vs Binary&quot;, x = &quot;Continuous Predictor&quot;, y = &quot;Predicted Probability&quot;) 3.4.2.3 Summary Table of Methods (Between Categorical and Continuous) Method Type of Variable Relationship Key Assumptions Use Case Point-Biserial Correlation Binary Categorical vs Continuous Linear, normality (continuous) Assess linear association. Logistic Regression Continuous → Binary Categorical Logit-linear relationship Predict probability of binary outcome. ANOVA Multi-level Categorical vs Continuous Normality, homogeneity of variance Compare means across groups. T-Test Binary Categorical vs Continuous Normality, equal variance Compare means between two groups. 3.4.3 Two Discrete When analyzing the relationship between two discrete variables (categorical or ordinal), various methods are available to quantify the degree of association or similarity. These methods can broadly be classified into: Distance Metrics Statistical Metrics 3.4.3.1 Distance Metrics Distance metrics measure the dissimilarity between two discrete variables and are often used as a proxy for correlation in specific applications like clustering or machine learning. 3.4.3.1.1 Euclidean Distance \\[ d(x, y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} \\] Measures the straight-line distance between two variables in Euclidean space. Sensitive to scaling; variables should be normalized for meaningful comparisons. 3.4.3.1.2 Manhattan Distance \\[ d(x, y) = \\sum_{i=1}^n |x_i - y_i| \\] Measures distance by summing the absolute differences along each dimension. Also called L1 norm; often used in grid-based problems. 3.4.3.1.3 Chebyshev Distance \\[ d(x, y) = \\max_{i=1}^n |x_i - y_i| \\] Measures the maximum single-step distance along any dimension. Useful in discrete, grid-based problems (e.g., chess moves). 3.4.3.1.4 Minkowski Distance \\[ d(x, y) = \\left( \\sum_{i=1}^n |x_i - y_i|^p \\right)^{1/p} \\] Generalized distance metric. Special cases include: \\(p = 1\\): Manhattan Distance. \\(p = 2\\): Euclidean Distance. \\(p \\to \\infty\\): Chebyshev Distance. 3.4.3.1.5 Canberra Distance \\[ d(x, y) = \\sum_{i=1}^n \\frac{|x_i - y_i|}{|x_i| + |y_i|} \\] Emphasizes proportional differences, making it sensitive to smaller values. 3.4.3.1.6 Hamming Distance \\[ d(x, y) = \\sum_{i=1}^n I(x_i \\neq y_i) \\] Counts the number of differing positions between two sequences. Widely used in text similarity and binary data. 3.4.3.1.7 Cosine Similarity and Distance \\[ \\text{Cosine Similarity} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\cdot \\sqrt{\\sum_{i=1}^n y_i^2}} \\] \\[ \\text{Cosine Distance} = 1 - \\text{Cosine Similarity} \\] Measures the angle between two vectors in a high-dimensional space. Often used in text and document similarity. 3.4.3.1.8 Sum of Absolute Differences \\[ d(x, y) = \\sum_{i=1}^n |x_i - y_i| \\] Equivalent to Manhattan Distance but without coordinate context. 3.4.3.1.9 Sum of Squared Differences \\[ d(x, y) = \\sum_{i=1}^n (x_i - y_i)^2 \\] Equivalent to squared Euclidean Distance. 3.4.3.1.10 Mean Absolute Error \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |x_i - y_i| \\] Measures average absolute differences. # Example data x &lt;- c(1, 2, 3, 4, 5) y &lt;- c(2, 3, 4, 5, 6) # Compute distances euclidean &lt;- sqrt(sum((x - y)^2)) manhattan &lt;- sum(abs(x - y)) chebyshev &lt;- max(abs(x - y)) hamming &lt;- sum(x != y) cosine_similarity &lt;- sum(x * y) / (sqrt(sum(x^2)) * sqrt(sum(y^2))) cosine_distance &lt;- 1 - cosine_similarity # Display results cat(&quot;Euclidean Distance:&quot;, euclidean, &quot;\\n&quot;) #&gt; Euclidean Distance: 2.236068 cat(&quot;Manhattan Distance:&quot;, manhattan, &quot;\\n&quot;) #&gt; Manhattan Distance: 5 cat(&quot;Chebyshev Distance:&quot;, chebyshev, &quot;\\n&quot;) #&gt; Chebyshev Distance: 1 cat(&quot;Hamming Distance:&quot;, hamming, &quot;\\n&quot;) #&gt; Hamming Distance: 5 cat(&quot;Cosine Distance:&quot;, cosine_distance, &quot;\\n&quot;) #&gt; Cosine Distance: 0.005063324 3.4.3.2 Statistical Metrics 3.4.3.2.1 Chi-squared Test The Chi-Squared Test evaluates whether two categorical variables are independent by comparing observed and expected frequencies in a contingency table. Formula: \\[ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i} \\] where \\(O_i\\): Observed frequency in each cell of the table. \\(E_i\\): Expected frequency under the assumption of independence. Steps: Construct a contingency table with observed counts. Compute expected frequencies: \\(E_{ij} = \\frac{\\text{Row Total}_i \\cdot \\text{Column Total}_j}{\\text{Grand Total}}\\) Apply the Chi-squared formula. Compare \\(\\chi^2\\) with a critical value from the Chi-squared distribution. Assumptions: Observations are independent. Expected frequencies should be \\(\\geq 5\\) in at least 80% of the cells. Use Case: Tests for independence between two nominal variables. # Example data dt &lt;- matrix(c(15, 25, 20, 40), nrow = 2) rownames(dt) &lt;- c(&quot;Group A&quot;, &quot;Group B&quot;) colnames(dt) &lt;- c(&quot;Category 1&quot;, &quot;Category 2&quot;) # Perform Chi-Squared Test chi_sq_test &lt;- chisq.test(dt) print(chi_sq_test) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: dt #&gt; X-squared = 0.045788, df = 1, p-value = 0.8306 3.4.3.2.2 Phi Coefficient The Phi Coefficient is a measure of association between two binary variables, derived from the Chi-squared statistic. Formula: \\[ \\phi = \\frac{\\chi^2}{n} \\] where \\(n\\): Total sample size. Interpretation: \\(\\phi = 0\\): No association. \\(\\phi = +1\\): Perfect positive association. \\(\\phi = -1\\): Perfect negative association. Use Case: Suitable for 2x2 contingency tables. 2 binary library(psych) # Compute Phi Coefficient phi_coeff &lt;- phi(dt) cat(&quot;Phi Coefficient:&quot;, phi_coeff, &quot;\\n&quot;) #&gt; Phi Coefficient: 0.04 3.4.3.2.3 Cramer’s V Cramer’s V generalizes the Phi coefficient to handle contingency tables with more than two rows or columns. Formula: \\[ V = \\sqrt{\\frac{\\chi^2 / n}{\\min(r-1, c-1)}} \\] where​​ \\(r\\): Number of rows. \\(c\\): Number of columns. Assumptions: Variables are nominal. Suitable for larger contingency tables. Use Case: Measures the strength of association between nominal variables with no natural order. library(lsr) # Simulate data set.seed(1) data &lt;- data.frame( A = sample(1:5, replace = TRUE, size = 100), # Nominal variable B = sample(1:6, replace = TRUE, size = 100) # Nominal variable ) # Compute Cramer&#39;s V cramers_v &lt;- cramersV(data$A, data$B) cat(&quot;Cramer&#39;s V:&quot;, cramers_v, &quot;\\n&quot;) #&gt; Cramer&#39;s V: 0.1944616 Alternatively, ncchisq noncentral Chi-square nchisqadj Adjusted noncentral Chi-square fisher Fisher Z transformation fisheradj bias correction Fisher z transformation DescTools::CramerV(data, conf.level = 0.95,method = &quot;ncchisqadj&quot;) #&gt; Cramer V lwr.ci upr.ci #&gt; 0.3472325 0.3929964 0.4033053 3.4.3.2.4 Adjusted Cramer’s V Adjusted versions of Cramer’s V correct for bias, especially in small samples. Adjusted formulas account for non-central Chi-squared or bias correction. Examples include: Non-central Chi-squared: \\(V_{adj} = \\sqrt{\\frac{\\chi^2_{nc} / n}{\\min(r-1, c-1)}}\\)​ Bias Correction: \\(V_{adj} = V - \\text{Bias Term}\\) library(DescTools) # Compute Adjusted Cramer&#39;s V cramers_v_adj &lt;- CramerV(data, conf.level = 0.95, method = &quot;ncchisqadj&quot;) cramers_v_adj #&gt; Cramer V lwr.ci upr.ci #&gt; 0.3472325 0.3929964 0.4033053 3.4.3.2.5 Tschuprow’s T Tschuprow’s T is a symmetric measure of association for nominal variables. It differs from Cramer’s V by considering the product of rows and columns, making it less sensitive to asymmetrical tables. Formula: \\[ T = \\sqrt{\\frac{\\chi^2/n}{\\sqrt{(r-1)(c-1)}}} \\] Assumptions: Applicable to nominal variables. Suitable for contingency tables with unequal dimensions. Use Case: Preferred when table dimensions are highly unequal. # Compute Tschuprow&#39;s T tschuprow_t &lt;- DescTools::TschuprowT(data$A, data$B) cat(&quot;Tschuprow&#39;s T:&quot;, tschuprow_t, &quot;\\n&quot;) #&gt; Tschuprow&#39;s T: 0.1839104 3.4.3.2.6 Ordinal Association (Rank correlation) When at least one variable is ordinal, rank-based methods are the most appropriate as they respect the order of the categories. These methods are often used when relationships are monotonic (increasing or decreasing consistently) but not necessarily linear. 3.4.3.2.6.1 Spearman’s Rank Correlation Spearman’s Rank Correlation (\\(\\rho\\)) measures the strength and direction of a monotonic relationship between two variables. It transforms the data into ranks and calculates Pearson correlation on the ranks. Formula: \\[ \\rho = 1 - \\frac{6 \\sum d_i^2}{n (n^2 -1)} \\] where​​ \\(d_i\\): Difference between the ranks of the paired observations. \\(n\\): Number of paired observations. Assumptions: Data must be ordinal or continuous but convertible to ranks. Relationship is monotonic. Use Case: Suitable for ordinal-ordinal or ordinal-continuous associations. # Simulating ordinal data set.seed(123) ordinal_x &lt;- sample(1:5, 100, replace = TRUE) ordinal_y &lt;- sample(1:5, 100, replace = TRUE) # Spearman&#39;s Correlation spearman_corr &lt;- cor(ordinal_x, ordinal_y, method = &quot;spearman&quot;) cat(&quot;Spearman&#39;s Correlation (rho):&quot;, spearman_corr, &quot;\\n&quot;) #&gt; Spearman&#39;s Correlation (rho): 0.08731195 3.4.3.2.6.2 Kendall’s Tau Kendall’s Tau (\\(\\tau\\)) measures the strength of a monotonic relationship by comparing concordant and discordant pairs. Formula: \\[ \\tau = \\frac{C - D}{C + D} \\]​where \\(C\\): Number of concordant pairs (ranks increase together). \\(D\\): Number of discordant pairs (one rank increases while the other decreases). Variants: Kendall’s Tau-a: For data with no ties. Kendall’s Tau-b: Adjusted for ties in ranks. Kendall’s Tau-c: Adjusted for ties in large tables. Use Case: Ideal for small datasets or when ties are present. # Kendall&#39;s Tau kendall_corr &lt;- cor(ordinal_x, ordinal_y, method = &quot;kendall&quot;) cat(&quot;Kendall&#39;s Tau (tau):&quot;, kendall_corr, &quot;\\n&quot;) #&gt; Kendall&#39;s Tau (tau): 0.06795076 3.4.3.2.6.3 Gamma Statistic The Gamma Statistic measures the strength of association between two ordinal variables by focusing on concordant and discordant pairs, ignoring ties. Formula: \\[ \\gamma = \\frac{C- D}{C + D} \\] Use Case: Works well when there are many ties in the data. library(vcd) # Simulating ordinal data cont_table &lt;- table(ordinal_x, ordinal_y) # Gamma Statistic gamma_stat &lt;- assocstats(cont_table)$gamma cat(&quot;Gamma Statistic:&quot;, gamma_stat, &quot;\\n&quot;) #&gt; Gamma Statistic: 3.4.3.2.6.4 Freeman’s Theta Freeman’s Theta measures the association between an ordinal variable and a nominal variable. It quantifies how well the grouping in the nominal variable explains the ordering in the ordinal variable. Use Case: Useful when analyzing relationships between ordinal predictors and nominal responses (or vice versa). rcompanion::freemanTheta(ordinal_x, ordinal_y) #&gt; Freeman.theta #&gt; 0.094 3.4.3.2.6.5 Epsilon-squared Epsilon-Squared (\\(\\epsilon^2\\)) measures the proportion of variance in the ordinal variable explained by a nominal variable. It is conceptually similar to the coefficient of determination (\\(R^2\\)) in linear regression but adapted for ordinal-nominal relationships. Formula: \\[ \\epsilon^2 = \\frac{\\text{variance explained by group differences}}{\\text{total variance}} \\] where The numerator represents the variance between ordinal categories due to differences in nominal groups. The denominator is the total variance in the ordinal variable. Use Case: Quantifies the effect size when analyzing how well a nominal variable explains an ordinal variable. set.seed(123) ordinal_x &lt;- sample(1:5, 100, replace = TRUE) # Ordinal variable nominal_y &lt;- sample(1:3, 100, replace = TRUE) # Nominal variable # Compute Epsilon-Squared library(rcompanion) epsilon_squared &lt;- rcompanion::epsilonSquared(ordinal_x, nominal_y) print(epsilon_squared) #&gt; epsilon.squared #&gt; 0.00446 3.4.3.2.6.6 Goodman-Kruskal’s Gamma Goodman-Kruskal’s Gamma measures the strength of association between two ordinal variables. It is a rank-based measure, focusing only on concordant and discordant pairs while ignoring ties. Formula: \\[ \\gamma = \\frac{C - D}{C + D} \\] where \\(C\\): Number of concordant pairs (where ranks move in the same direction). \\(D\\): Number of discordant pairs (where ranks move in opposite directions). Use Case: Suitable for ordinal variables with many ties. n = 100 # (sample size) set.seed(1) dt = table(data.frame( A = sample(1:4, replace = TRUE, size = n), # ordinal B = sample(1:3, replace = TRUE, size = n) # ordinal )) dt #&gt; B #&gt; A 1 2 3 #&gt; 1 7 11 9 #&gt; 2 11 6 14 #&gt; 3 7 11 4 #&gt; 4 6 4 10 # Compute Goodman-Kruskal&#39;s Gamma library(DescTools) goodman_kruskal_gamma &lt;- GoodmanKruskalGamma(dt, conf.level = 0.95) cat(&quot;Goodman-Kruskal&#39;s Gamma:&quot;, goodman_kruskal_gamma, &quot;\\n&quot;) #&gt; Goodman-Kruskal&#39;s Gamma: 0.006781013 -0.2290321 0.2425941 3.4.3.2.6.7 Somers’ D Somers’ D (also called Somers’ Delta) extends Kendall’s Tau by focusing on asymmetric relationships, where one variable is a predictor and the other is a response. Formula: \\[ D_{XY} = \\frac{C - D}{C + D + T_Y} \\] where \\(T_Y\\): Tied pairs in the dependent variable. Use Case: Appropriate when there is a clear predictor-response relationship between two ordinal variables. # Compute Somers&#39; D somers_d &lt;- SomersDelta(dt, conf.level = 0.95) somers_d #&gt; somers lwr.ci upr.ci #&gt; 0.005115859 -0.172800185 0.183031903 3.4.3.2.6.8 Kendall’s Tau-b Kendall’s Tau-b is an extension of Kendall’s Tau that accounts for ties in the data. Formula: \\[ \\tau_b = \\frac{C - D}{\\sqrt{(C + D+ T_X) (C + D + T_Y)}} \\] where \\(T_X, T_Y\\): Tied pairs in each variable. Use Case: Use when ordinal data contains ties. # Compute Kendall&#39;s Tau-b kendalls_tau_b &lt;- KendallTauB(dt, conf.level = 0.95) kendalls_tau_b #&gt; tau_b lwr.ci upr.ci #&gt; 0.004839732 -0.163472443 0.173151906 3.4.3.2.6.9 Yule’s Q and Y Yule’s Q and Yule’s Y are specialized measures for 2x2 contingency tables. They are simplified versions of Goodman-Kruskal’s Gamma, designed for binary ordinal variables.​​ Use Case: Ideal for binary ordinal variables in a 2x2 table. Special version \\((2 \\times 2)\\) of the Goodman Kruskal’s Gamma coefficient. Variable 1 Variable 2 a b c d \\[ \\text{Yule&#39;s Q} = \\frac{ad - bc}{ad + bc} \\] We typically use Yule’s \\(Q\\) in practice while Yule’s Y has the following relationship with \\(Q\\). \\[ \\text{Yule&#39;s Y} = \\frac{\\sqrt{ad} - \\sqrt{bc}}{\\sqrt{ad} + \\sqrt{bc}} \\] \\[ Q = \\frac{2Y}{1 + Y^2} \\] \\[ Y = \\frac{1 = \\sqrt{1-Q^2}}{Q} \\] # Create 2x2 table dt_binary &lt;- table(data.frame( A = sample(c(0, 1), replace = TRUE, size = n), B = sample(c(0, 1), replace = TRUE, size = n) )) # Compute Yule&#39;s Q yules_q &lt;- YuleQ(dt_binary) yules_q #&gt; [1] -0.07667474 3.4.3.2.6.10 Tetrachoric Correlation Tetrachoric Correlation measures the association between two binary variables by assuming they represent thresholds of underlying continuous normal distributions. It is a special case of Polychoric Correlation when both variables are binary # Simulate binary data library(psych) data_binary &lt;- data.frame( A = sample(c(0, 1), replace = TRUE, size = n), B = sample(c(0, 1), replace = TRUE, size = n) ) # Compute Tetrachoric Correlation tetrachoric_corr &lt;- tetrachoric(data_binary) print(tetrachoric_corr) #&gt; Call: tetrachoric(x = data_binary) #&gt; tetrachoric correlation #&gt; A B #&gt; A 1.00 #&gt; B 0.31 1.00 #&gt; #&gt; with tau of #&gt; A B #&gt; 0.126 -0.025 3.4.3.2.6.11 Polychoric Correlation Polychoric Correlation measures the association between ordinal variables by assuming they are discretized versions of latent, normally distributed continuous variables. Assumptions: The ordinal variables represent categories of an underlying normal distribution. Use Case: Suitable for ordinal variables with a natural order. # Simulate ordinal data library(polycor) data_ordinal &lt;- data.frame( A = sample(1:4, replace = TRUE, size = n), B = sample(1:6, replace = TRUE, size = n) ) # Compute Polychoric Correlation polychoric_corr &lt;- polychor(data_ordinal$A, data_ordinal$B) cat(&quot;Polychoric Correlation:&quot;, polychoric_corr, &quot;\\n&quot;) #&gt; Polychoric Correlation: 0.1908334 Metric Variable Types Use Case Spearman’s Correlation Ordinal vs. Ordinal Non-linear, monotonic relationships. Kendall’s Tau Ordinal vs. Ordinal Non-linear, monotonic relationships with ties. Gamma Statistic Ordinal vs. Ordinal Handles data with many ties effectively. Freeman’s Theta Ordinal vs. Nominal Mixed data types (ordinal and nominal). Epsilon-Squared Ordinal vs. Nominal Variance explained by nominal groups. Goodman-Kruskal’s Gamma Ordinal vs. Ordinal Strong association; ignores ties. Somers’ D Ordinal Predictor and Response Asymmetric association. Kendall’s Tau-b Ordinal vs. Ordinal Adjusts for ties in data. Yule’s Q Binary Ordinal vs. Binary Ordinal Special case for 2x2 tables. Tetrachoric Correlation Binary vs. Binary Binary ordinal variables. Polychoric Correlation Ordinal vs. Ordinal Continuous latent structure. 3.4.4 General Approach to Bivariate Statistics library(tidyverse) data(&quot;mtcars&quot;) df = mtcars %&gt;% dplyr::select(cyl, vs, carb) df_factor = df %&gt;% dplyr::mutate( cyl = factor(cyl), vs = factor(vs), carb = factor(carb) ) # summary(df) str(df) #&gt; &#39;data.frame&#39;: 32 obs. of 3 variables: #&gt; $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... #&gt; $ vs : num 0 0 1 1 0 1 0 1 1 1 ... #&gt; $ carb: num 4 4 1 1 2 1 4 2 2 4 ... str(df_factor) #&gt; &#39;data.frame&#39;: 32 obs. of 3 variables: #&gt; $ cyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 2 2 1 2 3 2 3 1 1 2 ... #&gt; $ vs : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 2 2 1 2 1 2 2 2 ... #&gt; $ carb: Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 4 4 1 1 2 1 4 2 2 4 ... Get the correlation table for continuous variables only cor(df) #&gt; cyl vs carb #&gt; cyl 1.0000000 -0.8108118 0.5269883 #&gt; vs -0.8108118 1.0000000 -0.5696071 #&gt; carb 0.5269883 -0.5696071 1.0000000 # only complete obs # cor(df, use = &quot;complete.obs&quot;) Alternatively, you can also have the Hmisc::rcorr(as.matrix(df), type = &quot;pearson&quot;) #&gt; cyl vs carb #&gt; cyl 1.00 -0.81 0.53 #&gt; vs -0.81 1.00 -0.57 #&gt; carb 0.53 -0.57 1.00 #&gt; #&gt; n= 32 #&gt; #&gt; #&gt; P #&gt; cyl vs carb #&gt; cyl 0.0000 0.0019 #&gt; vs 0.0000 0.0007 #&gt; carb 0.0019 0.0007 modelsummary::datasummary_correlation(df) cyl vs carb cyl 1 . . vs −.81 1 . carb .53 −.57 1 ggcorrplot::ggcorrplot(cor(df)) Comparing correlations between different types of variables (e.g., continuous vs. categorical) poses unique challenges. One key issue is ensuring that methods are appropriate for the nature of the variables being analyzed. Another challenge lies in detecting non-linear relationships, as traditional correlation measures, such as Pearson’s correlation coefficient, are designed to assess linear associations. To address these challenges, a potential solution is to utilize mutual information from information theory. Mutual information quantifies how much knowing one variable reduces the uncertainty of another, providing a more general measure of association that accommodates both linear and non-linear relationships. 3.4.4.1 Approximating Mutual Information We can approximate mutual information using the following relationship: \\[ \\downarrow \\text{Prediction Error} \\approx \\downarrow \\text{Uncertainty} \\approx \\uparrow \\text{Association Strength} \\] This principle underpins the X2Y metric, which is implemented through the following steps: Predict \\(y\\) without \\(x\\) (baseline model): If \\(y\\) is continuous, predict the mean of \\(y\\). If \\(y\\) is categorical, predict the mode of \\(y\\). Predict \\(y\\) with \\(x\\) using a model (e.g., linear regression, random forest, etc.). Calculate the difference in prediction error between steps 1 and 2. This difference reflects the reduction in uncertainty about \\(y\\) when \\(x\\) is included, serving as a measure of association strength. 3.4.4.2 Generalizing Across Variable Types To construct a comprehensive framework that handles different variable combinations, such as: Continuous vs. continuous Categorical vs. continuous Continuous vs. categorical Categorical vs. categorical a flexible modeling approach is required. Classification and Regression Trees (CART) are particularly well-suited for this purpose, as they can accommodate both continuous and categorical predictors and outcomes. However, other models, such as random forests or generalized additive models (GAMs), may also be employed. 3.4.4.3 Limitations of the Approach Despite its strengths, this approach has some limitations: Asymmetry: The measure is not symmetric, meaning \\((x, y) \\neq (y, x)\\). Comparability: Different variable pairs may yield metrics that are not directly comparable. For instance, continuous outcomes often use metrics like Mean Absolute Error (MAE), while categorical outcomes use measures like misclassification error. These limitations should be considered when interpreting results, especially in multi-variable or mixed-data contexts. library(ppsr) library(tidyverse) iris &lt;- iris %&gt;% dplyr::select(1:3) # ppsr::score_df(iris) # if you want a dataframe ppsr::score_matrix(iris, do_parallel = TRUE, n_cores = parallel::detectCores() / 2) #&gt; Sepal.Length Sepal.Width Petal.Length #&gt; Sepal.Length 1.00000000 0.04632352 0.5491398 #&gt; Sepal.Width 0.06790301 1.00000000 0.2376991 #&gt; Petal.Length 0.61608360 0.24263851 1.0000000 # if you want a similar correlation matrix ppsr::score_matrix(df, do_parallel = TRUE, n_cores = parallel::detectCores() / 2) #&gt; cyl vs carb #&gt; cyl 1.00000000 0.3982789 0.2092533 #&gt; vs 0.02514286 1.0000000 0.2000000 #&gt; carb 0.30798148 0.2537309 1.0000000 corrplot::corrplot(cor(df)) Alternatively, PerformanceAnalytics::chart.Correlation(df, histogram = T, pch = 19) heatmap(as.matrix(df)) More general form, ppsr::visualize_pps( df = iris, do_parallel = TRUE, n_cores = parallel::detectCores() / 2 ) ppsr::visualize_correlations( df = iris ) Both heat map and correlation at the same time ppsr::visualize_both( df = iris, do_parallel = TRUE, n_cores = parallel::detectCores() / 2 ) More elaboration with ggplot2 ppsr::visualize_pps( df = iris, color_value_high = &#39;red&#39;, color_value_low = &#39;yellow&#39;, color_text = &#39;black&#39; ) + ggplot2::theme_classic() + ggplot2::theme(plot.background = ggplot2::element_rect(fill = &quot;lightgrey&quot;)) + ggplot2::theme(title = ggplot2::element_text(size = 15)) + ggplot2::labs( title = &#39;Correlation aand Heatmap&#39;, subtitle = &#39;Subtitle&#39;, caption = &#39;Caption&#39;, x = &#39;More info&#39; ) "],["basic-statistical-inference.html", "Chapter 4 Basic Statistical Inference", " Chapter 4 Basic Statistical Inference One Sample Inference Two Sample Inference Categorical Data Analysis Divergence Metrics and Test for Comparing Distributions Make inferences (an interpretation) about the true parameter value \\(\\beta\\) based on our estimator/estimate Test whether our underlying assumptions (about the true population parameters, random variables, or model specification) hold true. Testing does not Confirm with 100% a hypothesis is true Confirm with 100% a hypothesis is false Tell you how to interpret the estimate value (Economic vs. Practical vs. Statistical Significance) Hypothesis: Translate an objective in better understanding the results in terms of specifying a value (or sets of values) in which our population parameters should/should not lie. Null hypothesis (\\(H_0\\)): A statement about the population parameter that we take to be true in which we would need the data to provide substantial evidence that against it. Can be either a single value (ex: \\(H_0: \\beta=0\\)) or a set of values (ex: \\(H_0: \\beta_1 \\ge 0\\)) Will generally be the value you would not like the population parameter to be (subjective) \\(H_0: \\beta_1=0\\) means you would like to see a non-zero coefficient \\(H_0: \\beta_1 \\ge 0\\) means you would like to see a negative effect “Test of Significance” refers to the two-sided test: \\(H_0: \\beta_j=0\\) Alternative hypothesis (\\(H_a\\) or \\(H_1\\)) (Research Hypothesis): All other possible values that the population parameter may be if the null hypothesis does not hold. Type I Error Error made when \\(H_0\\) is rejected when, in fact, \\(H_0\\) is true. The probability of committing a Type I error is \\(\\alpha\\) (known as level of significance of the test) Type I error (\\(\\alpha\\)): probability of rejecting \\(H_0\\) when it is true. Legal analogy: In U.S. law, a defendant is presumed to be “innocent until proven guilty”. If the null hypothesis is that a person is innocent, the Type I error is the probability that you conclude the person is guilty when he is innocent. Type II Error Type II error level (\\(\\beta\\)): probability that you fail to reject the null hypothesis when it is false. In the legal analogy, this is the probability that you fail to find the person guilty when he or she is guilty. Error made when \\(H_0\\) is not rejected when, in fact, \\(H_1\\) is true The probability of committing a Type II error is \\(\\beta\\) (known as the power of the test) Random sample of size n: A collection of n independent random variables taken from the distribution X, each with the same distribution as X. Sample mean \\[ \\bar{X}= (\\sum_{i=1}^{n}X_i)/n \\] Sample Median \\(\\tilde{x}\\) = the middle observation in a sample of observation order from smallest to largest (or vice versa). If n is odd, \\(\\tilde{x}\\) is the middle observation, If n is even, \\(\\tilde{x}\\) is the average of the two middle observations. Sample variance \\[ S^2 = \\frac{\\sum_{i=1}^{n}(X_i = \\bar{X})^2}{n-1}= \\frac{n\\sum_{i=1}^{n}X_i^2 -(\\sum_{i=1}^{n}X_i)^2}{n(n-1)} \\] Sample standard deviation \\[ S = \\sqrt{S^2} \\] Sample proportions \\[ \\hat{p} = \\frac{X}{n} = \\frac{\\text{number in the sample with trait}}{\\text{sample size}} \\] \\[ \\widehat{p_1-p_2} = \\hat{p_1}-\\hat{p_2} = \\frac{X_1}{n_1} - \\frac{X_2}{n_2} = \\frac{n_2X_1 = n_1X_2}{n_1n_2} \\] Estimators Point Estimator \\(\\hat{\\theta}\\) is a statistic used to approximate a population parameter \\(\\theta\\) Point estimate The numerical value assumed by \\(\\hat{\\theta}\\) when evaluated for a given sample Unbiased estimator If \\(E(\\hat{\\theta}) = \\theta\\), then \\(\\hat{\\theta}\\) is an unbiased estimator for \\(\\theta\\) \\(\\bar{X}\\) is an unbiased estimator for \\(\\mu\\) \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\) \\(\\hat{p}\\) is an unbiased estimator for p \\(\\widehat{p_1-p_2}\\) is an unbiased estimator for \\(p_1- p_2\\) \\(\\bar{X_1} - \\bar{X_2}\\) is an unbiased estimator for \\(\\mu_1 - \\mu_2\\) Note: \\(S\\) is a biased estimator for \\(\\sigma\\) Distribution of the sample mean If \\(\\bar{X}\\) is the sample mean based on a random sample of size \\(n\\) drawn from a normal distribution \\(X\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the \\(\\bar{X}\\) is normally distributed, with mean \\(\\mu_{\\bar{X}} = \\mu\\) and variance \\(\\sigma_{\\bar{X}}^2 = Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\). Then the standard error of the mean is: \\(\\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\) "],["one-sample-inference.html", "4.1 One Sample Inference", " 4.1 One Sample Inference \\(Y_i \\sim i.i.d. N(\\mu, \\sigma^2)\\) i.i.d. standards for “independent and identically distributed” Hence, we have the following model: \\(Y_i=\\mu +\\epsilon_i\\) where \\(\\epsilon_i \\sim^{iid} N(0,\\sigma^2)\\) \\(E(Y_i)=\\mu\\) \\(Var(Y_i)=\\sigma^2\\) \\(\\bar{y} \\sim N(\\mu,\\sigma^2/n)\\) 4.1.1 The Mean When \\(\\sigma^2\\) is estimated by \\(s^2\\), then \\[ \\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\sim t_{n-1} \\] Then, a \\(100(1-\\alpha) \\%\\) confidence interval for \\(\\mu\\) is obtained from: \\[ 1 - \\alpha = P(-t_{\\alpha/2;n-1} \\le \\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\le t_{\\alpha/2;n-1}) \\\\ = P(\\bar{y} - (t_{\\alpha/2;n-1})s/\\sqrt{n} \\le \\mu \\le \\bar{y} + (t_{\\alpha/2;n-1})s/\\sqrt{n}) \\] And the interval is \\[ \\bar{y} \\pm (t_{\\alpha/2;n-1})s/\\sqrt{n} \\] and \\(s/\\sqrt{n}\\) is the standard error of \\(\\bar{y}\\) If the experiment were repeated many times, \\(100(1-\\alpha) \\%\\) of these intervals would contain \\(\\mu\\) Confidence Interval \\(100(1-\\alpha)%\\) Sample Sizes Confidence \\(\\alpha\\), Error \\(d\\) Hypothesis Testing Test Statistic When \\(\\sigma^2\\) is known, X is normal (or \\(n \\ge 25\\)) \\(\\bar{X} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2 \\sigma^2}{d^2}\\) \\(z = \\frac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}}\\) When \\(\\sigma^2\\) is unknown, X is normal (or \\(n \\ge 25\\)) \\(\\bar{X} \\pm t_{\\alpha/2}\\frac{s}{\\sqrt{n}}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2 s^2}{d^2}\\) \\(t = \\frac{\\bar{X}-\\mu_0}{s/\\sqrt{n}}\\) 4.1.1.1 For Difference of Means (\\(\\mu_1-\\mu_2\\)), Independent Samples \\(100(1-\\alpha)%\\) Confidence Interval Hypothesis Testing Test Statistic When \\(\\sigma^2\\) is known \\(\\bar{X}_1 - \\bar{X}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\) \\(z= \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)_0}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}}\\) When \\(\\sigma^2\\) is unknown, Variances Assumed EQUAL \\(\\bar{X}_1 - \\bar{X}_2 \\pm t_{\\alpha/2}\\sqrt{s^2_p(\\frac{1}{n_1}+\\frac{1}{n_2})}\\) \\(t = \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)_0}{\\sqrt{s^2_p(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\) Pooled Variance: \\(s_p^2 = \\frac{(n_1 -1)s^2_1 - (n_2-1)s^2_2}{n_1 + n_2 -2}\\) Degrees of Freedom: \\(\\gamma = n_1 + n_2 -2\\) When \\(\\sigma^2\\) is unknown, Variances Assumed UNEQUAL \\(\\bar{X}_1 - \\bar{X}_2 \\pm t_{\\alpha/2}\\sqrt{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})}\\) \\(t = \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)_0}{\\sqrt{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})}}\\) Degrees of Freedom: \\(\\gamma = \\frac{(\\frac{s_1^2}{n_1}+\\frac{s^2_2}{n_2})^2}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1-1}+\\frac{(\\frac{s_2^2}{n_2})^2}{n_2-1}}\\) 4.1.1.2 For Difference of Means (\\(\\mu_1 - \\mu_2\\)), Paired Samples (D = X-Y) \\(100(1-\\alpha)%\\) Confidence Interval \\[ \\bar{D} \\pm t_{\\alpha/2}\\frac{s_d}{\\sqrt{n}} \\] Hypothesis Testing Test Statistic \\[ t = \\frac{\\bar{D}-D_0}{s_d / \\sqrt{n}} \\] 4.1.1.3 Difference of Two Proportions Mean \\[ \\hat{p_1}-\\hat{p_2} \\] Variance \\[ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2} \\] \\(100(1-\\alpha)%\\) Confidence Interval \\[ \\hat{p_1}-\\hat{p_2} + z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\] Sample Sizes, Confidence \\(\\alpha\\), Error d (Prior Estimate fo \\(\\hat{p_1},\\hat{p_2}\\)) \\[ n \\approx \\frac{z_{\\alpha/2}^2[p_1(1-p_1)+p_2(1-p_2)]}{d^2} \\] (No Prior Estimates for \\(\\hat{p}\\)) \\[ n \\approx \\frac{z_{\\alpha/2}^2}{2d^2} \\] Hypothesis Testing - Test Statistics Null Value \\((p_1 - p_2) \\neq 0\\) \\[ z = \\frac{(\\hat{p_1} - \\hat{p_2})-(p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}} \\] Null Value \\((p_1 - p_2)_0 = 0\\) \\[ z = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}} \\] where \\[ \\hat{p}= \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{n_1 \\hat{p_1} + n_2 \\hat{p_2}}{n_1 + n_2} \\] 4.1.2 Single Variance \\[ 1 - \\alpha = P( \\chi_{1-\\alpha/2;n-1}^2) \\le (n-1)s^2/\\sigma^2 \\le \\chi_{\\alpha/2;n-1}^2) \\\\ = P(\\frac{(n-1)s^2}{\\chi_{\\alpha/2}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2}^2}) \\] and a \\(100(1-\\alpha) \\%\\) confidence interval for \\(\\sigma^2\\) is: \\[ (\\frac{(n-1)s^2}{\\chi_{\\alpha/2;n-1}^2},\\frac{(n-1)s^2}{\\chi_{1-\\alpha/2;n-1}^2}) \\] Confidence limits for \\(\\sigma^2\\) are obtained by computing the positive square roots of these limits Equivalently, \\(100(1-\\alpha)%\\) Confidence Interval \\[ L_1 = \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} \\\\ L_1 = \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}} \\] Hypothesis Testing Test Statistic \\[ \\chi^2 = \\frac{(n-1)s^2}{\\sigma^2_0} \\] 4.1.3 Single Proportion (p) Confidence Interval \\(100(1-\\alpha)%\\) Sample Sizes Confidence \\(\\alpha\\), Error d (prior estimate for \\(\\hat{p}\\)) (No prior estimate for \\(\\hat{p}\\)) Hypothesis Testing Test Statistic \\(\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2 \\hat{p}(1-\\hat{p})}{d^2}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2}{4d^2}\\) \\(z = \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\\) 4.1.4 Power Formally, power (for the test of the mean) is given by: \\[ \\pi(\\mu) = 1 - \\beta = P(\\text{test rejects } H_0|\\mu) \\] To evaluate the power, one needs to know the distribution of the test statistic if the null hypothesis is false. For 1-sided z-test where \\(H_0: \\mu \\le \\mu_0 \\\\ H_A: \\mu &gt;0\\) The power is: \\[ \\begin{aligned} \\pi(\\mu) &amp;= P(\\bar{y} &gt; \\mu_0 + z_{\\alpha} \\sigma/\\sqrt{n}|\\mu) \\\\ &amp;= P(Z = \\frac{\\bar{y} - \\mu}{\\sigma / \\sqrt{n}} &gt; z_{\\alpha} + \\frac{\\mu_0 - \\mu}{\\sigma/ \\sqrt{n}}|\\mu) \\\\ &amp;= 1 - \\Phi(z_{\\alpha} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) \\\\ &amp;= \\Phi(-z_{\\alpha}+\\frac{(\\mu -\\mu_0)\\sqrt{n}}{\\sigma}) \\end{aligned} \\] where \\(1-\\Phi(x) = \\Phi(-x)\\) since the normal pdf is symmetric Power is correlated to the difference in \\(\\mu - \\mu_0\\), sample size n, variance \\(\\sigma^2\\), and the \\(\\alpha\\)-level of the test (through \\(z_{\\alpha}\\)) Equivalently, power can be increased by making \\(\\alpha\\) large, \\(\\sigma^2\\) smaller, or n larger. For 2-sided z-test is: \\[ \\pi(\\mu) = \\Phi(-z_{\\alpha/2} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) + \\Phi(-z_{\\alpha/2}+\\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}) \\] 4.1.5 Sample Size 4.1.5.1 1-sided Z-test Example: to show that the mean response \\(\\mu\\) under the treatment is higher than the mean response \\(\\mu_0\\) without treatment (show that the treatment effect \\(\\delta = \\mu -\\mu_0\\) is large) Because power is an increasing function of \\(\\mu - \\mu_0\\), it is only necessary to find n that makes the power equal to \\(1- \\beta\\) at \\(\\mu = \\mu_0 + \\delta\\) Hence, we have \\[ \\pi(\\mu_0 + \\delta) = \\Phi(-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma}) = 1 - \\beta \\] Since \\(\\Phi (z_{\\beta})= 1-\\beta\\), we have \\[ -z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma} = z_{\\beta} \\] Then n is \\[ n = (\\frac{(z_{\\alpha}+z_{\\beta})\\sigma}{\\delta})^2 \\] Then, we need larger samples, when the sample variability is large (\\(\\sigma\\) is large) \\(\\alpha\\) is small (\\(z_{\\alpha}\\) is large) power \\(1-\\beta\\) is large (\\(z_{\\beta}\\) is large) The magnitude of the effect is smaller (\\(\\delta\\) is small) Since we don’t know \\(\\delta\\) and \\(\\sigma\\). We can base \\(\\sigma\\) on previous studies, pilot studies. Or, obtain an estimate of \\(\\sigma\\) by anticipating the range of the observation (without outliers). divide this range by 4 and use the resulting number as an approximate estimate of \\(\\sigma\\). For normal (distribution) data, this is reasonable. 4.1.5.2 2-sided Z-test We want to know the min n, required to guarantee \\(1-\\beta\\) power when the treatment effect \\(\\delta = |\\mu - \\mu_0|\\) is at least greater than 0. Since the power function for the 2-sided is increasing and symmetric in \\(|\\mu - \\mu_0|\\), we only need to find n that makes the power equal to \\(1-\\beta\\) when \\(\\mu = \\mu_0 + \\delta\\) \\[ n = (\\frac{(z_{\\alpha/2} + z_{\\beta}) \\sigma}{\\delta})^2 \\] We could also use the confidence interval approach. If we require that an \\(\\alpha\\)-level two-sided CI for \\(\\mu\\) be \\[ \\bar{y} \\pm D \\] where \\(D = z_{\\alpha/2}\\sigma/\\sqrt{n}\\) gives \\[ n = (\\frac{z_{\\alpha/2}\\sigma}{D})^2 \\] (round up to the nearest integer) data = rnorm(100) t.test(data, conf.level=0.95) #&gt; #&gt; One Sample t-test #&gt; #&gt; data: data #&gt; t = 0.42865, df = 99, p-value = 0.6691 #&gt; alternative hypothesis: true mean is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.1728394 0.2680940 #&gt; sample estimates: #&gt; mean of x #&gt; 0.04762729 \\[ H_0: \\mu \\ge 30 \\\\ H_a: \\mu &lt; 30 \\] t.test(data, mu=30,alternative=&quot;less&quot;) #&gt; #&gt; One Sample t-test #&gt; #&gt; data: data #&gt; t = -269.57, df = 99, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true mean is less than 30 #&gt; 95 percent confidence interval: #&gt; -Inf 0.2321136 #&gt; sample estimates: #&gt; mean of x #&gt; 0.04762729 4.1.6 Note For t-tests, the sample and power are not as easy as z-test. \\[ \\pi(\\mu) = P(\\frac{\\bar{y}-\\mu_0}{s/\\sqrt{n}}&gt; t_{n-1;\\alpha}|\\mu) \\] when \\(\\mu &gt; \\mu_0\\) (i.e., \\(\\mu - \\mu_0 = \\delta\\)), the random variable \\((\\bar{y} - \\mu_0)/(s/\\sqrt{n})\\) does not have a [Student’s t distribution][Student T], but rather is distributed as a non-central t-distribution with non-centrality parameter \\(\\delta \\sqrt{n}/\\sigma\\) and d.f. of \\(n-1\\) The power is an increasing function of this non-centrality parameter (note, when \\(\\delta = 0\\) the distribution is usual Student’s t-distribution). To evaluate power, one must consider numerical procedure or use special charts Approximate Sample Size Adjustment for t-test. We use an adjustment to the z-test determination for sample size. Let \\(v = n-1\\), where n is sample size derived based on the z-test power. Then the 2-sided t-test sample size (approximate) is given: \\[ n^* = \\frac{(t_{v;\\alpha/2}+t_{v;\\beta})^2 \\sigma^2}{\\delta^2} \\] 4.1.7 One-sample Non-parametric Methods lecture.data = c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83,-0.43,-0.34, 3.34, 2.33) 4.1.7.1 Sign Test If we want to test \\(H_0: \\mu_{(0.5)} = 0; H_a: \\mu_{(0.5)} &gt;0\\) where \\(\\mu_{(0.5)}\\) is the population median. We can Count the number of observation (\\(y_i\\)’s) that exceed 0. Denote this number by \\(s_+\\), called the number of plus signs. Let \\(s_- = n - s_+\\), which is the number of minus signs. Reject \\(H_0\\) if \\(s_+\\) is large or equivalently, if \\(s_-\\) is small. To determine how large \\(s_+\\) must be to reject \\(H_0\\) at a given significance level, we need to know the distribution of the corresponding random variable \\(S_+\\) under the null hypothesis, which is a binomial with p = 1/2,w hen the null is true. To work out the null distribution using the binomial formula, we have \\(\\alpha\\)-level test rejects \\(H_0\\) if \\(s_+ \\ge b_{n,\\alpha}\\), where \\(b_{n,\\alpha}\\) is the upper \\(\\alpha\\) critical point of the \\(Bin(n,1/2)\\) distribution. Both \\(S_+\\) and \\(S_-\\) have this same distribution (\\(S = S_+ = S_-\\)). \\[ \\text{p-value} = P(S \\ge s_+) = \\sum_{i = s_+}^{n} {{n}\\choose{i}} (\\frac{1}{2})^n \\] equivalently, \\[ P(S \\le s_-) = \\sum_{i=0}^{s_-}{{n}\\choose{i}} (\\frac{1}{2})^2 \\] For large sample sizes, we could use the normal approximation for the binomial, in which case reject \\(H_0\\) if \\[ s_+ \\ge n/2 + 1/2 + z_{\\alpha}\\sqrt{n/4} \\] For the 2-sided test, we use the tests statistic \\(s_{max} = max(s_+,s_-)\\) or \\(s_{min} = min(s_+, s_-)\\). An \\(\\alpha\\)-level test rejects \\(H_0\\) if the p-value is \\(\\le \\alpha\\), where the p-value is computed from: \\[ p-value = 2 \\sum_{i=s_{max}}^{n} {{n}\\choose{i}} (\\frac{1}{2})^n = s \\sum_{i=0}^{s_{min}} {{n}\\choose{i}} (\\frac{1}{2})^n \\] Equivalently, rejecting \\(H_0\\) if \\(s_{max} \\ge b_{n,\\alpha/2}\\) A large sample normal approximation can be used, where \\[ z = \\frac{s_{max}- n/2 -1/2}{\\sqrt{n/4}} \\] and reject \\(H_0\\) at \\(\\alpha\\) if \\(z \\ge z_{\\alpha/2}\\) However, treatment of 0 is problematic for this test. Solution 1: randomly assign 0 to the positive or negative (2 researchers might get different results). Solution 2: count each 0 as a contribution 1/2 toward \\(s_+\\) and \\(s_-\\) (but then could not apply the binomial distribution) Solution 3: ignore 0 (reduces the power of test due to decreased sample size). binom.test(sum(lecture.data &gt; 0), length(lecture.data)) #&gt; #&gt; Exact binomial test #&gt; #&gt; data: sum(lecture.data &gt; 0) and length(lecture.data) #&gt; number of successes = 8, number of trials = 10, p-value = 0.1094 #&gt; alternative hypothesis: true probability of success is not equal to 0.5 #&gt; 95 percent confidence interval: #&gt; 0.4439045 0.9747893 #&gt; sample estimates: #&gt; probability of success #&gt; 0.8 # alternative = &quot;greater&quot; or alternative = &quot;less&quot; 4.1.7.2 Wilcoxon Signed Rank Test Since the Sign Test could not consider the magnitude of each observation from 0, the Wilcoxon Signed Rank Test improves by taking account the ordered magnitudes of the observation, but it will impose the requirement of symmetric to this test (while Sign Test does not) \\[ H_0: \\mu_{0.5} = 0 \\\\ H_a: \\mu_{0.5} &gt; 0 \\] (assume no ties or same observations) The signed rank test procedure: rank order the observation \\(y_i\\) in terms of their absolute values. Let \\(r_i\\) be the rank of \\(y_i\\) in this ordering. Since we assume no ties, the ranks \\(r_i\\) are uniquely determined and are a permutation of the integers \\(1,2,…,n\\). Calculate \\(w_+\\), which is the sum of the ranks of the positive values, and \\(w_-\\), which is the sum of the ranks of the negative values. Note that \\(w_+ + w_- = r_1 + r_2 + ... = 1 + 2 + ... + n = n(n+1)/2\\) Reject \\(H_0\\) if \\(w_+\\) is large (or if \\(w_-\\) is small) To know what is large or small with regard to \\(w_+\\) and \\(w_-\\), we need the distribution of \\(W_+\\) and \\(W_-\\) when the null is true. Since these null distributions are identical and symmetric, the p-value is \\(P(W \\ge w_+) = P(W \\le w_-)\\) An \\(\\alpha\\)-level test rejects the null if the p-value is \\(\\le \\alpha\\), or if \\(w_+ \\ge w_{n,\\alpha}\\), where \\(w_{n,\\alpha}\\) is the upper \\(\\alpha\\) critical point of the null distribution of W. This distribution of W has a special table. For large n, the distribution of W is approximately normal. \\[ z = \\frac{w_+ - n(n+1) /4 -1/2}{\\sqrt{n(n+1)(2n+1)/24}} \\] The test rejects \\(H_0\\) at level \\(\\alpha\\) if \\[ w_+ \\ge n(n+1)/4 +1/2 + z_{\\alpha}\\sqrt{n(n+1)(2n+1)/24} \\approx w_{n,\\alpha} \\] For the 2-sided test, we use \\(w_{max}=max(w_+,w_-)\\) or \\(w_{min}=min(w_+,w_-)\\), with p-value given by: \\[ p-value = 2P(W \\ge w_{max}) = 2P(W \\le w_{min}) \\] Same as Sign Test,we ignore 0. In some cases where some of the \\(|y_i|\\)’s may be tied for the same rank, we simply assign each of the tied ranks the average rank (or “midrank”). Example, if \\(y_1 = -1\\), \\(y_3 = 3\\) and \\(y_3 = -3\\), and \\(y_4 =5\\), then \\(r_1 = 1\\), \\(r_2 = r_3=(2+3)/2 = 2.5\\), \\(r_4 = 4\\) wilcox.test(lecture.data) #&gt; #&gt; Wilcoxon signed rank exact test #&gt; #&gt; data: lecture.data #&gt; V = 52, p-value = 0.009766 #&gt; alternative hypothesis: true location is not equal to 0 # does not use normal approximation # (using the underlying W distribution) wilcox.test(lecture.data,exact=F) #&gt; #&gt; Wilcoxon signed rank test with continuity correction #&gt; #&gt; data: lecture.data #&gt; V = 52, p-value = 0.01443 #&gt; alternative hypothesis: true location is not equal to 0 # uses normal approximation "],["two-sample-inference.html", "4.2 Two Sample Inference", " 4.2 Two Sample Inference 4.2.1 Means Suppose we have 2 sets of observations, \\(y_1,..., y_{n_y}\\) \\(x_1,...,x_{n_x}\\) that are random samples from two independent populations with means \\(\\mu_y\\) and \\(\\mu_x\\) and variances \\(\\sigma^2_y\\),\\(\\sigma^2_x\\). Our goal is to compare \\(\\mu_x\\) and \\(\\mu_y\\) or \\(\\sigma^2_y = \\sigma^2_x\\) 4.2.1.1 Large Sample Tests Assume that \\(n_y\\) and \\(n_x\\) are large (\\(\\ge 30\\)). Then, \\[ E(\\bar{y} - \\bar{x}) = \\mu_y - \\mu_x \\\\ Var(\\bar{y} - \\bar{x}) = \\sigma^2_y /n_y + \\sigma^2_x/n_x \\] Then, \\[ Z = \\frac{\\bar{y}-\\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\sigma^2_y /n_y + \\sigma^2_x/n_x}} \\sim N(0,1) \\] (according to Central Limit Theorem). For large samples, we can replace variances by their unbiased estimators (\\(s^2_y,s^2_x\\)), and get the same large sample distribution. An approximate \\(100(1-\\alpha) \\%\\) CI for \\(\\mu_y - \\mu_x\\) is given by: \\[ \\bar{y} - \\bar{x} \\pm z_{\\alpha/2}\\sqrt{s^2_y/n_y + s^2_x/n_x} \\] \\[ H_0: \\mu_y - \\mu_x = \\delta_0 \\\\ H_A: \\mu_y - \\mu_x \\neq \\delta_0 \\] at the \\(\\alpha\\)-level with the statistic: \\[ z = \\frac{\\bar{y}-\\bar{x} - \\delta_0}{\\sqrt{s^2_y /n_y + s^2_x/n_x}} \\] and reject \\(H_0\\) if \\(|z| &gt; z_{\\alpha/2}\\) If \\(\\delta = )\\), it means that we are testing whether two means are equal. 4.2.1.2 Small Sample Tests If the two samples are from normal distribution, iid \\(N(\\mu_y,\\sigma^2_y)\\) and iid \\(N(\\mu_x,\\sigma^2_x)\\) and the two samples are independent, we can do inference based on the [t-distribution][Student T] Then we have 2 cases Equal Variance Unequal Variance 4.2.1.2.1 Equal variance Assumptions iid: so that \\(var(\\bar{y}) = \\sigma^2_y / n_y ; var(\\bar{x}) = \\sigma^2_x / n_x\\) Independence between samples: No observation from one sample can influence any observation from the other sample, to have \\[ \\begin{aligned} var(\\bar{y} - \\bar{x}) &amp;= var(\\bar{y}) + var{\\bar{x}} - 2cov(\\bar{y},\\bar{x}) \\\\ &amp;= var(\\bar{y}) + var{\\bar{x}} \\\\ &amp;= \\sigma^2_y / n_y + \\sigma^2_x / n_x \\end{aligned} \\] Normality: Justifies the use of the [t-distribution][Student T] Let \\(\\sigma^2 = \\sigma^2_y = \\sigma^2_x\\). Then, \\(s^2_y\\) and \\(s^2_x\\) are both unbiased estimators of \\(\\sigma^2\\). We then can pool them. Then the pooled variance estimate is \\[ s^2 = \\frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y-1)+(n_x-1)} \\] has \\(n_y + n_x -2\\) df. Then the test statistic \\[ T = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{s\\sqrt{1/n_y + 1/n_x}} \\sim t_{n_y + n_x -2} \\] \\(100(1 - \\alpha) \\%\\) CI for \\(\\mu_y - \\mu_x\\) is \\[ \\bar{y} - \\bar{x} \\pm (t_{n_y + n_x -2})s\\sqrt{1/n_y + 1/n_x} \\] Hypothesis testing: \\[ H_0: \\mu_y - \\mu_x = \\delta_0 \\\\ H_1: \\mu_y - \\mu_x \\neq \\delta_0 \\] we reject \\(H_0\\) if \\(|t| &gt; t_{n_y + n_x -2;\\alpha/2}\\) 4.2.1.2.2 Unequal Variance Assumptions Two samples are independent Scatter plots Correlation coefficient (if normal) Independence of observation in each sample Test for serial correlation For each sample, homogeneity of variance Scatter plots Formal tests Normality Equality of variances (homogeneity of variance between samples) F-test Barlett test [Modified Levene Test] To compare 2 normal \\(\\sigma^2_y \\neq \\sigma^2_x\\), we use the test statistic: \\[ T = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{\\sqrt{s^2_y/n_y + s^2_x/n_x}} \\] In this case, T does not follow the [t-distribution][Student T] (its distribution depends on the ratio of the unknown variances \\(\\sigma^2_y,\\sigma^2_x\\)). In the case of small sizes, we can can approximate tests by using the Welch-Satterthwaite method (Satterthwaite 1946). We assume T can be approximated by a [t-distribution][Student T], and adjust the degrees of freedom. Let \\(w_y = s^2_y /n_y\\) and \\(w_x = s^2_x /n_x\\) (the w’s are the square of the respective standard errors) Then, the degrees of freedom are \\[ v = \\frac{(w_y + w_x)^2}{w^2_y / (n_y-1) + w^2_x / (n_x-1)} \\] Since v is usually fractional, we truncate down to the nearest integer. \\(100 (1-\\alpha) \\%\\) CI for \\(\\mu_y - \\mu_x\\) is \\[ \\bar{y} - \\bar{x} \\pm t_{v,\\alpha/2} \\sqrt{s^2_y/n_y + s^2_x /n_x} \\] Reject \\(H_0\\) if \\(|t| &gt; t_{v,\\alpha/2}\\), where \\[ t = \\frac{\\bar{y} - \\bar{x}-\\delta_0}{\\sqrt{s^2_y/n_y + s^2_x /n_x}} \\] 4.2.2 Variances \\[ F_{ndf,ddf}= \\frac{s^2_1}{s^2_2} \\] where \\(s^2_1&gt;s^2_2, ndf = n_1-1,ddf = n_2-1\\) 4.2.2.1 F-test Test \\[ H_0: \\sigma^2_y = \\sigma^2_x \\\\ H_a: \\sigma^2_y \\neq \\sigma^2_x \\] Consider the test statistic, \\[ F= \\frac{s^2_y}{s^2_x} \\] Reject \\(H_0\\) if \\(F&gt;f_{n_y -1,n_x -1,\\alpha/2}\\) or \\(F&lt;f_{n_y -1,n_x -1,1-\\alpha/2}\\) Where \\(F&gt;f_{n_y -1,n_x -1,\\alpha/2}\\) and \\(F&lt;f_{n_y -1,n_x -1,1-\\alpha/2}\\) are the upper and lower \\(\\alpha/2\\) critical points of an [F-distribution][F-Distribution], with a \\(n_y-1\\) and \\(n_x-1\\) degrees of freedom. Note This test depends heavily on the assumption Normality. In particular, it could give to many significant results when observations come from long-tailed distributions (i.e., positive kurtosis). If we cannot find support for normality, then we can use nonparametric tests such as the Modified Levene Test (Brown-Forsythe Test) data(iris) irisVe=iris$Petal.Width[iris$Species==&quot;versicolor&quot;] irisVi=iris$Petal.Width[iris$Species==&quot;virginica&quot;] var.test(irisVe,irisVi) #&gt; #&gt; F test to compare two variances #&gt; #&gt; data: irisVe and irisVi #&gt; F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335 #&gt; alternative hypothesis: true ratio of variances is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.2941935 0.9135614 #&gt; sample estimates: #&gt; ratio of variances #&gt; 0.5184243 4.2.2.2 Modified Levene Test (Brown-Forsythe Test) considers averages of absolute deviations rather than squared deviations. Hence, less sensitive to long-tailed distributions. This test is still good for normal data For each sample, we consider the absolute deviation of each observation form the median: \\[ d_{y,i} = |y_i - y_{.5}| \\\\ d_{x,i} = |x_i - x_{.5}| \\] Then, \\[ t_L^* = \\frac{\\bar{d}_y-\\bar{d}_x}{s \\sqrt{1/n_y + 1/n_x}} \\] The pooled variance \\(s^2\\) is given by: \\[ s^2 = \\frac{\\sum_i^{n_y}(d_{y,i}-\\bar{d}_y)^2 + \\sum_j^{n_x}(d_{x,i}-\\bar{d}_x)^2}{n_y + n_x -2} \\] If the error terms have constant variance and \\(n_y\\) and \\(n_x\\) are not extremely small, then \\(t_L^* \\sim t_{n_x + n_y -2}\\) We reject the null hypothesis when \\(|t_L^*| &gt; t_{n_y + n_x -2;\\alpha/2}\\) This is just the two-sample t-test applied to the absolute deviations. dVe=abs(irisVe-median(irisVe)) dVi=abs(irisVi-median(irisVi)) t.test(dVe,dVi,var.equal=T) #&gt; #&gt; Two Sample t-test #&gt; #&gt; data: dVe and dVi #&gt; t = -2.5584, df = 98, p-value = 0.01205 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.12784786 -0.01615214 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 0.154 0.226 # small samples t-test t.test(irisVe,irisVi,var.equal=F) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: irisVe and irisVi #&gt; t = -14.625, df = 89.043, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.7951002 -0.6048998 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 1.326 2.026 4.2.3 Power Consider \\(\\sigma^2_y = \\sigma^2_x = \\sigma^2\\) Under the assumption of equal variances, we take size samples from both groups (\\(n_y = n_x = n\\)) For 1-sided testing, \\[ H_0: \\mu_y - \\mu_x \\le 0 \\\\ H_a: \\mu_y - \\mu_x &gt; 0 \\] \\(\\alpha\\)-level z-test rejects \\(H_0\\) if \\[ z = \\frac{\\bar{y} - \\bar{x}}{\\sigma \\sqrt{2/n}} &gt; z_{\\alpha} \\] \\[ \\pi(\\mu_y - \\mu_x) = \\Phi(-z_{\\alpha} + \\frac{\\mu_y -\\mu_x}{\\sigma}\\sqrt{n/2}) \\] We need sample size n that give at least \\(1-\\beta\\) power when \\(\\mu_y - \\mu_x = \\delta\\), where \\(\\delta\\) is the smallest difference that we want to see. Power is given by: \\[ \\Phi(-z_{\\alpha} + \\frac{\\delta}{\\sigma}\\sqrt{n/2}) = 1 - \\beta \\] 4.2.4 Sample Size Then, the sample size is \\[ n = 2(\\frac{\\sigma (z_{\\alpha} + z_{\\beta}}{\\delta})^2 \\] For 2-sided test, replace \\(z_{\\alpha}\\) with \\(z_{\\alpha/2}\\). As with the one-sample case, to perform an exact 2-sample t-test sample size calculation, we must use a non-central [t-distribution][Student T]. A correction that gives the approximate t-test sample size can be obtained by using the z-test n value in the formula: \\[ n^* = 2(\\frac{\\sigma (t_{2n-2;\\alpha} + t_{2n-2;\\beta})}{\\delta})^2 \\] where we use \\(\\alpha/2\\) for the two-sided test 4.2.5 Matched Pair Designs We have two treatments Subject Treatment A Treatment B Difference 1 \\(y_1\\) \\(x_1\\) \\(d_1 = y_1 - x_1\\) 2 \\(y_2\\) \\(x_2\\) \\(d_2 = y_2 - x_2\\) . . . . n \\(y_n\\) \\(x_n\\) \\(d_n = y_n - x_n\\) we assume \\(y_i \\sim^{iid} N(\\mu_y, \\sigma^2_y)\\) and \\(x_i \\sim^{iid} N(\\mu_x,\\sigma^2_x)\\), but since \\(y_i\\) and \\(x_i\\) are measured on the same subject, they are correlated. Let \\[ \\mu_D = E(y_i - x_i) = \\mu_y -\\mu_x \\\\ \\sigma^2_D = var(y_i - x_i) = Var(y_i) + Var(x_i) -2cov(y_i,x_i) \\] If the matching induces positive correlation, then the variance of the difference of the measurements is reduced as compared to the independent case. This is the point of Matched Pair Designs. Although covariance can be negative, giving a larger variance of the difference than the independent sample case, usually the covariance is positive. This means both \\(y_i\\) and \\(x_i\\) are large for many of the same subjects, and for others, both measurement are small. (we still assume that various subjects respond independently of each other, which is necessary for the iid assumption within groups). Let \\(d_i = y_i - x_i\\), then \\(\\bar{d} = \\bar{y}-\\bar{x}\\) is the sample mean of the \\(d_i\\) \\(s_d^2=\\frac{1}{n-1}\\sum_{i=1}^n (d_i - \\bar{d})^2\\) is the sample variance of the difference Once the data are converted to differences, we are back to One Sample Inference and can use its tests and CIs. 4.2.6 Nonparametric Tests for Two Samples For Matched Pair Designs, we can use the One-sample Non-parametric Methods. Assume that Y and X are random variables with CDF \\(F_y\\) and \\(F_x\\). then, Y is stochastically larger than X for all real number u, \\(P(Y &gt; u) \\ge P(X &gt; u)\\). Equivalently, \\(P(Y \\le u) \\le P(X \\le u)\\), which is \\(F_Y(u) \\le F_X(u)\\), same thing as \\(F_Y &lt; F_X\\) If two distributions are identical, except that one is shifted relative to the other, then each of distribution can be indexed by a location parameter, say \\(\\theta_y\\) and \\(\\theta_x\\). In this case, \\(Y&gt;X\\) if \\(\\theta_y &gt; \\theta_x\\) Consider the hypotheses, \\[ H_0: F_Y = F_X \\\\ H_a: F_Y &lt; F_X \\] where the alternative is an upper one-sided alternative. We can also consider the lower one-sided alternative \\[ H_a: F_Y &gt; F_X \\text{ or} \\\\ H_a: F_Y &lt; F_X \\text{ or } F_Y &gt; F_X \\] In this case, we don’t use \\(H_a: F_Y \\neq F_X\\) as that allows arbitrary differences between the distributions, without requiring one be stochastically larger than the other. If the distributions only differ in terms of their location parameters, we can focus hypothesis tests on the parameters (e.g., \\(H_0: \\theta_y = \\theta_x\\) vs. \\(\\theta_y &gt; \\theta_x\\)) We have 2 equivalent nonparametric tests that consider the hypothesis mentioned above Wilcoxon rank test Mann-Whitney U test 4.2.6.1 Wilcoxon rank test Combine all \\(n= n_y + n_x\\) observations and rank them in ascending order. Sum the ranks of the \\(y\\)’s and \\(x\\)’s separately. Let \\(w_y\\) and \\(w_x\\) be these sums. (\\(w_y + w_x = 1 + 2 + ... + n = n(n+1)/2\\)) Reject \\(H_0\\) if \\(w_y\\) is large (equivalently, \\(w_x\\) is small) Under \\(H_0\\), any arrangement of the \\(y\\)’s and \\(x\\)’s is equally likely to occur, and there are \\((n_y + n_x)!/(n_y! n_x!)\\) possible arrangements. Technically, for each arrangement we can compute the values of \\(w_y\\) and \\(w_x\\), and thus generate the distribution of the statistic under the null hypothesis. This could lead to computationally intensive. wilcox.test( irisVe, irisVi, alternative = &quot;two.sided&quot;, conf.level = 0.95, exact = F, correct = T ) #&gt; #&gt; Wilcoxon rank sum test with continuity correction #&gt; #&gt; data: irisVe and irisVi #&gt; W = 49, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true location shift is not equal to 0 4.2.6.2 Mann-Whitney U test The Mann-Whitney test is computed as follows: Compare each \\(y_i\\) with each \\(x_i\\). Let \\(u_y\\) be the number of pairs in which \\(y_i &gt; x_i\\) Let \\(u_x\\) be the number of pairs in which \\(y_i &lt; x_i\\). (assume there are no ties). There are \\(n_y n_x\\) such comparisons and \\(u_y + u_x = n_y n_x\\). Reject \\(H_0\\) if \\(u_y\\) is large (or \\(u_x\\) is small) Mann-Whitney U test and Wilcoxon rank test are related: \\[ u_y = w_y - n_y(n_y+1) /2 \\\\ u_x = w_x - n_x(n_x +1)/2 \\] An \\(\\alpha\\)-level test rejects \\(H_0\\) if \\(u_y \\ge u_{n_y,n_x,\\alpha}\\), where \\(u_{n_y,n_x,\\alpha}\\) is the upper \\(\\alpha\\) critical point of the null distribution of the random variable, U. The p-value is defined to be \\(P(Y \\ge u_y) = P(U \\le u_x)\\). One advantage of Mann-Whitney U test is that we can use either \\(u_y\\) or \\(u_x\\) to carry out the test. For large \\(n_y\\) and \\(n_x\\), the null distribution of U can be well approximated by a normal distribution with mean \\(E(U) = n_y n_x /2\\) and variance \\(var(U) = n_y n_x (n+1)/12\\). A large sample z-test can be based on the statistic: \\[ z = \\frac{u_y - n_y n_x /2 -1/2}{\\sqrt{n_y n_x (n+1)/12}} \\] The test rejects \\(H_0\\) at level \\(\\alpha\\) if \\(z \\ge z_{\\alpha}\\) or if \\(u_y \\ge u_{n_y,n_x,\\alpha}\\) where \\[ u_{n_y, n_x, \\alpha} \\approx n_y n_x /2 + 1/2 + z_{\\alpha}\\sqrt{n_y n_x (n+1)/12} \\] For the 2-sided test, we use the test statistic \\(u_{max} = max(u_y,u_x)\\) and \\(u_{min} = min(u_y, u_x)\\) and p-value is given by \\[ p-value = 2P(U \\ge u_{max}) = 2P(U \\le u_{min}) \\] Since we assume there are no ties (when \\(y_i = x_j\\)), we count 1/2 towards both \\(u_y\\) and \\(u_x\\). Even though the sampling distribution is not the same, but large sample approximation is still reasonable, References "],["categorical-data-analysis.html", "4.3 Categorical Data Analysis", " 4.3 Categorical Data Analysis Categorical Data Analysis when we have categorical outcomes Nominal variables: no logical ordering (e.g., sex) Ordinal variables: logical order, but relative distances between values are not clear (e.g., small, medium, large) The distribution of one variable changes when the level (or values) of the other variable change. The row percentages are different in each column. 4.3.1 Inferences for Small Samples The approximate tests based on the asymptotic normality of \\(\\hat{p}_1 - \\hat{p}_2\\) do not apply for small samples. Using Fisher’s Exact Test to evaluate \\(H_0: p_1 = p_2\\) Assume \\(X_1\\) and \\(X_2\\) are independent Binomial Let \\(x_1\\) and \\(x_2\\) be the corresponding observed values. Let \\(n= n_1 + n_2\\) be the total sample size \\(m = x_1 + x_2\\) be the observed number of successes. By assuming that m (total successes) is fixed, and conditioning on this value, one can show that the conditional distribution of the number of successes from sample 1 is [Hypergeometric] If we want to test \\(H_0: p_1 = p_2\\) and \\(H_a: p_1 \\neq p_2\\), we have \\[ Z^2 = (\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}})^2 \\sim \\chi_{1,\\alpha}^2 \\] where \\(\\chi_{1,\\alpha}^2\\) is the upper \\(\\alpha\\) percentage point for the central [Chi-squared] with one d.f. This extends to the contingency table setting: whether the observed frequencies are equal to those expected under a null hypothesis of no association. 4.3.2 Test of Association Pearson Chi-square test statistic is \\[ \\chi^2 = \\sum_{\\text{all categories}} \\frac{(observed - epxected)^2}{expected} \\] Comparison of proportions for several independent surveys or experiments Experiment 1 Experiment 2 … Experiment k Number of successes \\(x_1\\) \\(x_2\\) … \\(x_k\\) Number of failures \\(n_1 - x_1\\) \\(n_2 - x_2\\) … \\(n_k - x_k\\) \\(n_1\\) \\(n_2\\) … \\(n_k\\) \\(H_0: p_1 = p_2 = \\dots = p_k\\) vs. the alternative that the null is not true (at least one pair are not equal). We estimate the common value of the probability of success on a single trial assuming \\(H_0\\) is true: \\[ \\hat{p} = \\frac{x_1 + x_2 + ... + x_k}{n_1 + n_2 + ...+ n_k} \\] we use table of expected counts when \\(H_0\\) is true: success \\(n_1 \\hat{p}\\) \\(n_2 \\hat{p}\\) … \\(n_k \\hat{p}\\) failure \\(n_1(1-\\hat{p})\\) \\(n_2(1-\\hat{p})\\) … \\(n_k (1-\\hat{p})\\) \\(n_1\\) \\(n_2\\) … \\(n_k\\) \\[ \\chi^2 = \\sum_{\\text{all cells in table}} \\frac{(observed - expected)^2}{expected} \\] with \\(k-1\\) degrees of freedom 4.3.2.1 Two-way Count Data 1 2 … j … c Row Total 1 \\(n_{11}\\) \\(n_{12}\\) … \\(n_{1j}\\) … \\(n_{1c}\\) \\(n_{1.}\\) 2 \\(n_{21}\\) \\(n_{22}\\) … \\(n_{2j}\\) … \\(n_{2c}\\) \\(n_{2.}\\) . . . . . . . . r \\(n_{r1}\\) \\(n_{r2}\\) … \\(n_{rj}\\) … \\(n_{rc}\\) \\(n_{r.}\\) Column Total \\(n_{.1}\\) \\(n_{.2}\\) … \\(n_{.j}\\) … \\(n_{.c}\\) \\(n_{}\\) Design 1 total sample size fixed \\(n\\) = constant (e.g., survey on job satisfaction and income); both row and column totals are random variables Design 2 Fix the sample size in each group (in each row) (e.g., Drug treatments success or failure); fixed number of participants for each treatment; independent random samples from the two row populations. These different sampling designs imply two different probability models. 4.3.2.2 Total Sample Size Fixed Design 1 random sample of size n drawn from a single population, and sample units are cross-classified into \\(r\\) row categories and \\(c\\) column This results in an \\(r \\times c\\) table of observed counts \\(n_{ij} = 1,...,r;j=1,...,c\\) Let \\(p_{ij}\\) be the probability of classification into cell \\((i,j)\\) and \\(\\sum_{i=1}^r \\sum_{j=1}^c p_{ij} = 1\\). Let \\(N_{ij}\\) be the random variable corresponding to \\(n_{ij}\\) The joint distribution of the \\(N_{ij}\\) is multinomial with unknown parameters \\(p_{ij}\\) Denote the row variable by \\(X\\) and column variable by Y, then \\(p_{ij} = P(X=i,Y = j)\\) and \\(p_{i.} = P(X = i)\\) and \\(p_{.j} = P(Y = j)\\) are the marginal probabilities. The null hypothesis that X and Y are statistically independent (i.e., no association) is just: \\[ H_0: p_{ij} = P(X =i,Y=j) = P(X =i) P(Y =j) = p_{i.}p_{.j} \\\\ H_a: p_{ij} \\neq p_{i.}p_{.j} \\] for all \\(i,j\\). 4.3.2.3 Row Total Fixed Design 2 Random samples of sizes \\(n_1,...,n_r\\) are drawn independently from \\(r \\ge 2\\) row populations. In this case, the 2-way table row totals are \\(n_{i.} = n_i\\) for \\(i = 1,...,r\\). The counts from each row are modeled by independent multinomial distributions. \\(X\\) is fixed, \\(Y\\) is observed. Then, \\(p_{ij}\\) represent conditional probabilities \\(p_{ij} = P(Y=j|X=i)\\) The null hypothesis is the probability of response j is the same, regardless of the row population (i.e., no association): \\[ \\begin{cases} H_0: p_{ij} = P(Y = j | X = i) = p_j &amp; \\text{for all } i,j =1,2,...,c \\\\ \\text{or } H_0: (p_{i1},p_{i2},...,p_{ic}) = (p_1,p_2,...,p_c) &amp; \\text{ for all } i\\\\ H_a: (p_{i1},p_{i2},...,p_{ic}) &amp; \\text{ are not the same for all } i \\end{cases} \\] Although the hypotheses to be tested are different for two sampling designs, The \\(\\chi^2\\) test is identical We have estimated expected frequencies: \\[ \\hat{e}_{ij} = \\frac{n_{i.}n_{.j}}{n} \\] The Chi-square statistic is \\[ \\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(n_{ij}-\\hat{e}_{ij})^2}{\\hat{e}_{ij}} \\sim \\chi_{(r-1)(c-1)} \\] \\(\\alpha\\)-level test rejects \\(H_0\\) if \\(\\chi^2 &gt; \\chi^2_{(r-1)(c-1),\\alpha}\\) 4.3.2.4 Pearson Chi-square Test Determine whether an association exists Sometimes, \\(H_0\\) represents the model whose validity is to be tested. Contrast this with the conventional formulation of \\(H_0\\) as the hypothesis that is to be disproved. The goal in this case is not to disprove the model, but to see whether data are consistent with the model and if deviation can be attributed to chance. These tests do not measure the strength of an association. These tests depend on and reflect the sample size - double the sample size by copying each observation, double the \\(\\chi^2\\) statistic even thought the strength of the association does not change. The Pearson Chi-square Test is not appropriate when more than about 20% of the cells have an expected cell frequency of less than 5 (large-sample p-values not appropriate). When the sample size is small the exact p-values can be calculated (this is prohibitive for large samples); calculation of the exact p-values assumes that the column totals and row totals are fixed. july.x=480 july.n=1000 sept.x=704 sept.n=1600 \\[ H_0: p_J = 0.5 \\\\ H_a: p_J &lt; 0.5 \\] prop.test( x = july.x, n = july.n, p = 0.5, alternative = &quot;less&quot;, correct = F ) #&gt; #&gt; 1-sample proportions test without continuity correction #&gt; #&gt; data: july.x out of july.n, null probability 0.5 #&gt; X-squared = 1.6, df = 1, p-value = 0.103 #&gt; alternative hypothesis: true p is less than 0.5 #&gt; 95 percent confidence interval: #&gt; 0.0000000 0.5060055 #&gt; sample estimates: #&gt; p #&gt; 0.48 \\[ H_0: p_J = p_S \\\\ H_a: p_j \\neq p_S \\] prop.test( x = c(july.x, sept.x), n = c(july.n, sept.n), correct = F ) #&gt; #&gt; 2-sample test for equality of proportions without continuity correction #&gt; #&gt; data: c(july.x, sept.x) out of c(july.n, sept.n) #&gt; X-squared = 3.9701, df = 1, p-value = 0.04632 #&gt; alternative hypothesis: two.sided #&gt; 95 percent confidence interval: #&gt; 0.0006247187 0.0793752813 #&gt; sample estimates: #&gt; prop 1 prop 2 #&gt; 0.48 0.44 4.3.3 Ordinal Association An ordinal association implies that as one variable increases, the other tends to increase or decrease (depending on the nature of the association). For tests for variables with two or more levels, the levels must be in a logical ordering. 4.3.3.1 Mantel-Haenszel Chi-square Test The Mantel-Haenszel Chi-square Test is more powerful for testing ordinal associations, but does not test for the strength of the association. This test is presented in the case where one has a series of \\(2 \\times 2\\) tables that examine the same effects under different conditions (If there are \\(K\\) such tables, we have \\(2 \\times 2 \\times K\\) table) In stratum \\(k\\), given the marginal totals \\((n_{.1k},n_{.2k},n_{1.k},n_{2.k})\\), the sampling model for cell counts is the [Hypergeometric] (knowing \\(n_{11k}\\) determines \\((n_{12k},n_{21k},n_{22k})\\), given the marginal totals) Assuming conditional independence, the [Hypergeometric] mean and variance of \\(n_{11k}\\) are \\[ m_{11k} = E(n_{11k}) = \\frac{n_{1.k} n_{.1k}}{n_{..k}} \\\\ var(n_{11k}) = \\frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2(n_{..k}-1)} \\] To test conditional independence, Mantel and Haenszel proposed \\[ M^2 = \\frac{(|\\sum_{k} n_{11k} - \\sum_k m_{11k}| -.5)^2}{\\sum_{k}var(n_{11k})} \\sim \\chi^2_{1} \\] This method can be extended to general \\(I \\times J \\times K\\) tables. \\((2 \\times 2 \\times 3)\\) table Bron = array( c(20, 9, 382, 214, 10, 7, 172, 120, 12, 6, 327, 183), dim = c(2, 2, 3), dimnames = list( Particulate = c(&quot;High&quot;, &quot;Low&quot;), Bronchitis = c(&quot;Yes&quot;, &quot;No&quot;), Age = c(&quot;15-24&quot;, &quot;25-39&quot;, &quot;40+&quot;) ) ) margin.table(Bron, c(1, 2)) #&gt; Bronchitis #&gt; Particulate Yes No #&gt; High 42 881 #&gt; Low 22 517 # assess whether the relationship between # Bronchitis by Particulate Level varies by Age library(samplesizeCMH) marginal_table = margin.table(Bron, c(1, 2)) odds.ratio(marginal_table) #&gt; [1] 1.120318 # whether these odds vary by age. # The conditional odds can be calculated using the original table. apply(Bron, 3, odds.ratio) #&gt; 15-24 25-39 40+ #&gt; 1.2449098 0.9966777 1.1192661 # Mantel-Haenszel Test mantelhaen.test(Bron, correct = T) #&gt; #&gt; Mantel-Haenszel chi-squared test with continuity correction #&gt; #&gt; data: Bron #&gt; Mantel-Haenszel X-squared = 0.11442, df = 1, p-value = 0.7352 #&gt; alternative hypothesis: true common odds ratio is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 0.6693022 1.9265813 #&gt; sample estimates: #&gt; common odds ratio #&gt; 1.135546 4.3.3.1.1 McNemar’s Test special case of Mantel-Haenszel Chi-square Test vote = cbind(c(682, 22), c(86, 810)) mcnemar.test(vote, correct = T) #&gt; #&gt; McNemar&#39;s Chi-squared test with continuity correction #&gt; #&gt; data: vote #&gt; McNemar&#39;s chi-squared = 36.75, df = 1, p-value = 1.343e-09 4.3.3.2 Spearman Rank Correlation To test for the strength of association between two ordinally scaled variables, we can use Spearman Rank Correlation statistic Let \\(X\\) and \\(Y\\) be two random variables measured on an ordinal scale. Consider \\(n\\) pairs of observations (\\(x_i,y_i\\)), \\(i = 1,\\dots,n\\) The Spearman Rank Correlation coefficient (denoted by \\(r_S\\) is calculated using the Pearson correlation formula, but based on the ranks of \\(x_i\\) and \\(y_i\\)). Spearman Rank Correlation be calculated Assign ranks to \\(x_i\\)’s and \\(y_i\\)’s separately. Let \\(u_i = rank(x_i)\\) and \\(v_i = rank(y_i)\\) Calculate \\(r_S\\) using the formula for the Pearson correlation coefficient, but applied to the ranks: \\[ r_S = \\frac{\\sum_{i=1}^{n}(u_i - \\bar{u})(v_i - \\bar{v})}{\\sqrt{(\\sum_{i = 1}^{n}(u_i - \\bar{u})^2)(\\sum_{i=1}^{n}(v_i - \\bar{v})^2)}} \\] \\(r_S\\) ranges between -1 and +1 , with \\(r_S = -1\\) if there is a perfect negative monotone association \\(r_S = +1\\) if there is a perfect positive monotone association between X and Y. To test \\(H_0:\\) \\(X\\) and \\(Y\\) independent \\(H_a\\): \\(X\\) and \\(Y\\) positively associated For large \\(n\\) (e.g., \\(n \\ge 10\\)), \\[ r_S \\sim N(0,1/(n-1)) \\] Then, \\[ Z = r_s \\sqrt{n-1} \\sim N(0,1) \\] "],["divergence-metrics-and-test-for-comparing-distributions.html", "4.4 Divergence Metrics and Test for Comparing Distributions", " 4.4 Divergence Metrics and Test for Comparing Distributions Similarity among distributions using divergence statistics, which is different from Deviation statistics: difference between the realization of a variable and some value (e.g., mean). Statistics of the deviation distributions consist of standard deviation, average absolute deviation, median absolute deviation , maximum absolute deviation. Deviance statistics: goodness-of-fit statistic for statistical models (comparable to the sum of squares of residuals in OLS to cases that use ML estimation). Usually used in generalized linear models. Divergence statistics is a statistical distance (different from metrics) Divergences do not require symmetry Divergences generalize squared distance (instead of linear distance). Hence, fail the triangle inequity Can be used for Detecting data drift in machine learning Feature selections Variational Auto Encoder Detect similarity between policies (i.e., distributions) in reinforcement learning To see consistency in two measured variables of two constructs. Techniques Kullback-Leibler Divergence Jensen-Shannon Divergence Kolmogorov-Smirnov Test Packages entropy philentropy 4.4.1 Kullback-Leibler Divergence Also known as relative entropy Not a metric (does not satisfy the triangle inequality) Can be generalized to the multivariate case Measure the similarity between two discrete probability distributions \\(P\\) = true data distribution \\(Q\\) = predicted data distribution It quantifies info loss when moving from \\(P\\) to \\(Q\\) (i.e., information loss when \\(P\\) is approximated by \\(Q\\)) Discrete \\[ D_{KL}(P ||Q) = \\sum_i P_i \\log(\\frac{P_i}{Q_i}) \\] Continuous \\[ D_{KL}(P||Q) = \\int P(x) \\log(\\frac{P(x)}{Q(x)}) dx \\] where \\(K \\in [0, \\infty)\\) from similar to diverge Non-symmetric between two distributions: \\(D_{KL}(P|Q) \\neq D_{KL}(Q|P)\\) library(philentropy) # philentropy::dist.diversity(rbind(X = 1:10 / sum(1:10), # Y = 1:20 / sum(1:20)), # p = 2, # unit = &quot;log2&quot;) # continuous KL(rbind(X = 1:10 / sum(1:10), Y = 1:10 / sum(1:10)), unit = &quot;log2&quot;) #&gt; kullback-leibler #&gt; 0 # discrete KL(rbind(X = 1:10, Y = 1:10), est.prob = &quot;empirical&quot;) #&gt; kullback-leibler #&gt; 0 4.4.2 Jensen-Shannon Divergence Also known as info radius or total divergence to the average \\[ D_{JS} (P ||Q) = \\frac{1}{2}( D_{KL}(P||M)+ D_{KL}(Q||M)) \\] where \\(M = \\frac{1}{2} (P + Q)\\) is a mixed distribution \\(D_{JS} \\in [0,1]\\) for \\(\\log_2\\) and \\(D_{JS} \\in [0,\\ln(2)]\\) for \\(\\log_e\\) library(philentropy) # continous JSD(rbind(X = 1:10, Y = 1:20), unit = &quot;log2&quot;) #&gt; jensen-shannon #&gt; 20.03201 # discrete JSD(rbind(X = 1:10, Y = 1:20), est.prob = &quot;empirical&quot;) #&gt; jensen-shannon #&gt; 0.06004756 4.4.3 Wasserstein Distance measure the distance between two empirical CDFs \\[ W = \\int_{x \\in R}|E(x) - F(X)|^p \\] This is also a test statistics set.seed(1) transport::wasserstein1d(rnorm(100), rnorm(100, mean = 1)) #&gt; [1] 0.8533046 set.seed(1) # Wasserstein metric twosamples::wass_stat(rnorm(100), rnorm(100, mean = 1)) #&gt; [1] 0.8533046 set.seed(1) # permutation-based tw sample test using Wasserstein metric twosamples::wass_test(rnorm(100), rnorm(100, mean = 1)) #&gt; Test Stat P-Value #&gt; 0.8533046 0.0002500 4.4.4 Kolmogorov-Smirnov Test Can be used for continuous distribution \\(H_0\\): Empirical distribution follows a specified distribution \\(H_1\\): Empirical distribution does not follow a specified distribution Using non-parametric \\[ D= \\max|P(X) - Q(X)| \\] \\(D \\in [0,1]\\) from the densities are evenly distributed to not evenly distributed library(entropy) library(tidyverse) lst = list(sample_1 = c(1:20), sample_2 = c(2:30), sample_3 = c(3:30)) expand.grid(1:length(lst), 1:length(lst)) %&gt;% rowwise() %&gt;% mutate(KL = KL.empirical(lst[[Var1]], lst[[Var2]])) #&gt; # A tibble: 9 × 3 #&gt; # Rowwise: #&gt; Var1 Var2 KL #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 1 0 #&gt; 2 2 1 0.150 #&gt; 3 3 1 0.183 #&gt; 4 1 2 0.704 #&gt; 5 2 2 0 #&gt; 6 3 2 0.0679 #&gt; 7 1 3 0.622 #&gt; 8 2 3 0.0870 #&gt; 9 3 3 0 To use the test for discrete date, use bootstrap version of the KS test (bypass the continuity requirement) Matching::ks.boot(Tr = c(0:10), Co = c(0:10)) #&gt; $ks.boot.pvalue #&gt; [1] 1 #&gt; #&gt; $ks #&gt; #&gt; Exact two-sample Kolmogorov-Smirnov test #&gt; #&gt; data: Tr and Co #&gt; D = 0, p-value = 1 #&gt; alternative hypothesis: two-sided #&gt; #&gt; #&gt; $nboots #&gt; [1] 1000 #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;ks.boot&quot; "],["linear-regression.html", "Chapter 5 Linear Regression", " Chapter 5 Linear Regression Estimating parameters -&gt; parametric (finite parameters) Estimating functions -&gt; non-parametric Estimator Desirable Properties Unbiased Consistency \\(plim\\hat{\\beta_n}=\\beta\\) based on the law of large numbers, we can derive consistency More observations means more precise, closer to the true value. Efficiency Minimum variance in comparison to another estimator. OLS is BLUE (best linear unbiased estimator) means that OLS is the most efficient among the class of linear unbiased estimator Gauss-Markov Theorem If we have correct distributional assumptions, then the Maximum Likelihood is asymptotically efficient among consistent estimators. "],["ordinary-least-squares.html", "5.1 Ordinary Least Squares", " 5.1 Ordinary Least Squares The most fundamental model in statistics or econometric is a OLS linear regression. OLS = Maximum likelihood when the error term is assumed to be normally distributed. Regression is still great if the underlying CEF (conditional expectation function) is not linear. Because regression has the following properties: For \\(E[Y_i | X_{1i}, \\dots, X_{Ki}] = a + \\sum_{k=1}^K b_k X_{ki}\\) (i.e., the CEF of \\(Y_i\\) on \\(X_{1i}, \\dots, X_{Ki}\\) is linear, then the regression of \\(Y_i\\) on \\(X_{1i}, \\dots, X_{Ki}\\) is the CEF For \\(E[Y_i | X_{1i} , \\dots, X_{Ki}]\\) is a nonlinear function of the conditioning variables, the regression of \\(Y_i\\) on \\(X_{1i}, \\dots, X_{Ki}\\) will give you the best linear approximation to the nonlinear CEF (i.e., minimize the expected squared deviation between the fitted values from the linear model and the CEF). 5.1.1 Simple Regression (Basic Model) \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] \\(Y_i\\): response (dependent) variable at i-th observation \\(\\beta_0,\\beta_1\\): regression parameters for intercept and slope. \\(X_i\\): known constant (independent or predictor variable) for i-th observation \\(\\epsilon_i\\): random error term \\[ \\begin{aligned} E(\\epsilon_i) &amp;= 0 \\\\ var(\\epsilon_i) &amp;= \\sigma^2 \\\\ cov(\\epsilon_i,\\epsilon_j) &amp;= 0 \\text{ for all } i \\neq j \\end{aligned} \\] \\(Y_i\\) is random since \\(\\epsilon_i\\) is: \\[ \\begin{aligned} E(Y_i) &amp;= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\ &amp;= E(\\beta_0) + E(\\beta_1 X_i) + E(\\epsilon) \\\\ &amp;= \\beta_0 + \\beta_1 X_i \\end{aligned} \\] \\[ \\begin{aligned} var(Y_i) &amp;= var(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\ &amp;= var(\\epsilon_i) \\\\ &amp;= \\sigma^2 \\end{aligned} \\] Since \\(cov(\\epsilon_i, \\epsilon_j) = 0\\) (uncorrelated), the outcome in any one trail has no effect on the outcome of any other. Hence, \\(Y_i, Y_j\\) are uncorrelated as well (conditioned on the \\(X\\)’s) Note Least Squares does not require a distributional assumption Relationship between bivariate regression and covariance Covariance between 2 variables: \\[ C(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])] \\] Which has the following properties \\(C(X_i, X_i) = \\sigma^2_X\\) If either \\(E(X_i) = 0 | E(Y_i) = 0\\), then \\(Cov(X_i, Y_i) = E[X_i Y_i]\\) Given \\(W_i = a + b X_i\\) and \\(Z_i = c + d Y_i\\), then \\(Cov(W_i, Z_i) = bdC(X_i, Y_i)\\) For the bivariate regression, the slope is \\[ \\beta = \\frac{Cov(Y_i, X_i)}{Var(X_i)} \\] To extend this to a multivariate case \\[ \\beta_k = \\frac{C(Y_i, \\tilde{X}_{ki})}{Var(\\tilde{X}_{ki})} \\] Where \\(\\tilde{X}_{ki}\\) is the residual from a regression of \\(X_{ki}\\) on the \\(K-1\\) other covariates included in the model And intercept \\[ \\alpha = E[Y_i] - \\beta E(X_i) \\] 5.1.1.1 Estimation Deviation of \\(Y_i\\) from its expected value: \\[ Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i) \\] Consider the sum of the square of such deviations: \\[ Q = \\sum_{i=1}^{n} (Y_i - \\beta_0 -\\beta_1 X_i)^2 \\] \\[ \\begin{aligned} b_1 &amp;= \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\\\ b_0 &amp;= \\frac{1}{n}(\\sum_{i=1}^{n}Y_i - b_1\\sum_{i=1}^{n}X_i) = \\bar{Y} - b_1 \\bar{X} \\end{aligned} \\] 5.1.1.2 Properties of Least Least Estimators \\[ \\begin{aligned} E(b_1) &amp;= \\beta_1 \\\\ E(b_0) &amp;= E(\\bar{Y}) - \\bar{X}\\beta_1 \\\\ E(\\bar{Y}) &amp;= \\beta_0 + \\beta_1 \\bar{X} \\\\ E(b_0) &amp;= \\beta_0 \\\\ var(b_1) &amp;= \\frac{\\sigma^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\\\ var(b_0) &amp;= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum (X_i - \\bar{X})^2}) \\end{aligned} \\] \\(var(b_1) \\to 0\\) as more measurements are taken at more \\(X_i\\) values (unless \\(X_i\\) is at its mean value) \\(var(b_0) \\to 0\\) as \\(n\\) increases when the \\(X_i\\) values are judiciously selected. Mean Square Error \\[ MSE = \\frac{SSE}{n-2} = \\frac{\\sum_{i=1}^{n}e_i^2}{n-2} = \\frac{\\sum(Y_i - \\hat{Y_i})^2}{n-2} \\] Unbiased estimator of MSE: \\[ E(MSE) = \\sigma^2 \\] \\[ \\begin{aligned} s^2(b_1) &amp;= \\widehat{var(b_1)} = \\frac{MSE}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\\\ s^2(b_0) &amp;= \\widehat{var(b_0)} = MSE(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\end{aligned} \\] \\[ \\begin{aligned} E(s^2(b_1)) &amp;= var(b_1) \\\\ E(s^2(b_0)) &amp;= var(b_0) \\end{aligned} \\] 5.1.1.3 Residuals \\[ e_i = Y_i - \\hat{Y} = Y_i - (b_0 + b_1 X_i) \\] \\(e_i\\) is an estimate of \\(\\epsilon_i = Y_i - E(Y_i)\\) \\(\\epsilon_i\\) is always unknown since we don’t know the true \\(\\beta_0, \\beta_1\\) \\[ \\begin{aligned} \\sum_{i=1}^{n} e_i &amp;= 0 \\\\ \\sum_{i=1}^{n} X_i e_i &amp;= 0 \\end{aligned} \\] Residual properties \\(E[e_i] =0\\) \\(E[X_i e_i] = 0\\) and \\(E[\\hat{Y}_i e_i ] = 0\\) 5.1.1.4 Inference Normality Assumption Least Squares estimation does not require assumptions of normality. However, to do inference on the parameters, we need distributional assumptions. Inference on \\(\\beta_0,\\beta_1\\) and \\(Y_h\\) are not extremely sensitive to moderate departures from normality, especially if the sample size is large. Inference on \\(Y_{pred}\\) is very sensitive to the normality assumptions. Normal Error Regression Model \\[ Y_i \\sim N(\\beta_0+\\beta_1X_i, \\sigma^2) \\] 5.1.1.4.1 \\(\\beta_1\\) Under the normal error model, \\[ b_1 \\sim N(\\beta_1,\\frac{\\sigma^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\] A linear combination of independent normal random variable is normally distributed Hence, \\[ \\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2} \\] A \\((1-\\alpha) 100 \\%\\) confidence interval for \\(\\beta_1\\) is \\[ b_1 \\pm t_{t-\\alpha/2 ; n-2}s(b_1) \\] 5.1.1.4.2 \\(\\beta_0\\) Under the normal error model, the sampling distribution for \\(b_0\\) is \\[ b_0 \\sim N(\\beta_0,\\sigma^2(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2})) \\] Hence, \\[ \\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2} \\] A \\((1-\\alpha)100 \\%\\) confidence interval for \\(\\beta_0\\) is \\[ b_0 \\pm t_{1-\\alpha/2;n-2}s(b_0) \\] 5.1.1.4.3 Mean Response Let \\(X_h\\) denote the level of X for which we wish to estimate the mean response We denote the mean response when \\(X = X_h\\) by \\(E(Y_h)\\) A point estimator of \\(E(Y_h)\\) is \\(\\hat{Y}_h\\): \\[ \\hat{Y}_h = b_0 + b_1 X_h \\] Note \\[ \\begin{aligned} E(\\bar{Y}_h) &amp;= E(b_0 + b_1X_h) \\\\ &amp;= \\beta_0 + \\beta_1 X_h \\\\ &amp;= E(Y_h) \\end{aligned} \\] (unbiased estimator) \\[ \\begin{aligned} var(\\hat{Y}_h) &amp;= var(b_0 + b_1 X_h) \\\\ &amp;= var(\\hat{Y} + b_1 (X_h - \\bar{X})) \\\\ &amp;= var(\\bar{Y}) + (X_h - \\bar{X})^2var(b_1) + 2(X_h - \\bar{X})cov(\\bar{Y},b_1) \\\\ &amp;= \\frac{\\sigma^2}{n} + (X_h - \\bar{X})^2 \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2} \\\\ &amp;= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}) \\end{aligned} \\] Since \\(cov(\\bar{Y},b_1) = 0\\) due to the iid assumption on \\(\\epsilon_i\\) An estimate of this variance is \\[ s^2(\\hat{Y}_h) = MSE (\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\] the sampling distribution for the mean response is \\[ \\begin{aligned} \\hat{Y}_h &amp;\\sim N(E(Y_h),var(\\hat{Y_h})) \\\\ \\frac{\\hat{Y}_h - E(Y_h)}{s(\\hat{Y}_h)} &amp;\\sim t_{n-2} \\end{aligned} \\] A \\(100(1-\\alpha) \\%\\) CI for \\(E(Y_h)\\) is \\[ \\hat{Y}_h \\pm t_{1-\\alpha/2;n-2}s(\\hat{Y}_h) \\] 5.1.1.4.4 Prediction of a new observation Regarding the Mean Response, we are interested in estimating mean of the distribution of Y given a certain X. Now, we want to predict an individual outcome for the distribution of Y at a given X. We call \\(Y_{pred}\\) Estimation of mean response versus prediction of a new observation: the point estimates are the same in both cases: \\(\\hat{Y}_{pred} = \\hat{Y}_h\\) It is the variance of the prediction that is different; hence, prediction intervals are different than confidence intervals. The prediction variance must consider: Variation in the mean of the distribution of \\(Y\\) Variation within the distribution of \\(Y\\) We want to predict: mean response + error \\[ \\beta_0 + \\beta_1 X_h + \\epsilon \\] Since \\(E(\\epsilon) = 0\\), use the least squares predictor: \\[ \\hat{Y}_h = b_0 + b_1 X_h \\] The variance of the predictor is \\[ \\begin{aligned} var(b_0 + b_1 X_h + \\epsilon) &amp;= var(b_0 + b_1 X_h) + var(\\epsilon) \\\\ &amp;= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) + \\sigma^2 \\\\ &amp;= \\sigma^2(1+\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\end{aligned} \\] An estimate of the variance is given by \\[ s^2(pred)= MSE (1+ \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\] and \\[ \\frac{Y_{pred}-\\hat{Y}_h}{s(pred)} \\sim t_{n-2} \\] \\(100(1-\\alpha) \\%\\) prediction interval is \\[ \\bar{Y}_h \\pm t_{1-\\alpha/2; n-2}s(pred) \\] The prediction interval is very sensitive to the distributional assumption on the errors, \\(\\epsilon\\) 5.1.1.4.5 Confidence Band We want to know the confidence interval for the entire regression line, so we can draw conclusions about any and all mean response fo the entire regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) rather than for a given response \\(Y\\) Working-Hotelling Confidence Band For a given \\(X_h\\), this band is \\[ \\hat{Y}_h \\pm W s(\\hat{Y}_h) \\] where \\(W^2 = 2F_{1-\\alpha;2,n-2}\\), which is just 2 times the F-stat with 2 and \\(n-2\\) degrees of freedom the interval width will change with each \\(X_h\\) (since \\(s(\\hat{Y}_h)\\) changes) the boundary values for this confidence band will always define a hyperbole containing the regression line will be smallest at \\(X = \\bar{X}\\) 5.1.1.5 ANOVA Partitioning the Total Sum of Squares: Consider the corrected Total sum of squares: \\[ SSTO = \\sum_{i=1}^{n} (Y_i -\\bar{Y})^2 \\] Measures the overall dispersion in the response variable We use the term corrected because we correct for mean, the uncorrected total sum of squares is given by \\(\\sum Y_i^2\\) use \\(\\hat{Y}_i = b_0 + b_1 X_i\\) to estimate the conditional mean for Y at \\(X_i\\) \\[ \\begin{aligned} \\sum_{i=1}^n (Y_i - \\bar{Y})^2 &amp;= \\sum_{i=1}^n (Y_i - \\hat{Y}_i + \\hat{Y}_i - \\bar{Y})^2 \\\\ &amp;= \\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{i=1}^n(\\hat{Y}_i - \\bar{Y})^2 + 2\\sum_{i=1}^n(Y_i - \\hat{Y}_i)(\\hat{Y}_i-\\bar{Y}) \\\\ &amp;= \\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{i=1}^n(\\bar{Y}_i -\\bar{Y})^2 \\\\ STTO &amp;= SSE + SSR \\\\ \\end{aligned} \\] where SSR is the regression sum of squares, which measures how the conditional mean varies about a central value. The cross-product term in the decomposition is 0: \\[ \\begin{aligned} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y}) &amp;= \\sum_{i=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))(\\bar{Y} + b_1 (X_i - \\bar{X})-\\bar{Y}) \\\\ &amp;= b_1 \\sum_{i=1}^{n} (Y_i - \\bar{Y})(X_i - \\bar{X}) - b_1^2\\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\\\ &amp;= b_1 \\frac{\\sum_{i=1}^{n}(Y_i -\\bar{Y})(X_i - \\bar{X})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\sum_{i=1}^{n}(X_i - \\bar{X})^2 - b_1^2\\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\\\ &amp;= b_1^2 \\sum_{i=1}^{n}(X_i - \\bar{X})^2 - b_1^2 \\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\\\ &amp;= 0 \\end{aligned} \\] and \\[ \\begin{aligned} SSTO &amp;= SSR + SSE \\\\ (n-1 d.f) &amp;= (1 d.f.) + (n-2 d.f.) \\end{aligned} \\] Source of Variation Sum of Squares df Mean Square F Regression (model) SSR \\(1\\) MSR = SSR/df MSR/MSE Error SSE \\(n-2\\) MSE = SSE/df Total (Corrected) SSTO \\(n-1\\) \\[ \\begin{aligned} E(MSE) &amp;= \\sigma^2 \\\\ E(MSR) &amp;= \\sigma^2 + \\beta_1^2 \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\end{aligned} \\] If \\(\\beta_1 = 0\\), then these two expected values are the same if \\(\\beta_1 \\neq 0\\) then E(MSR) will be larger than E(MSE) which means the ratio of these two quantities, we can infer something about \\(\\beta_1\\) Distribution theory tells us that if \\(\\epsilon_i \\sim iid N(0,\\sigma^2)\\) and assuming \\(H_0: \\beta_1 = 0\\) is true, \\[ \\begin{aligned} \\frac{MSE}{\\sigma^2} &amp;\\sim \\chi_{n-2}^2 \\\\ \\frac{MSR}{\\sigma^2} &amp;\\sim \\chi_{1}^2 \\text{ if } \\beta_1=0 \\end{aligned} \\] where these two chi-square random variables are independent. Since the ratio of 2 independent chi-square random variable follows an F distribution, we consider: \\[ F = \\frac{MSR}{MSE} \\sim F_{1,n-2} \\] when \\(\\beta_1 =0\\). Thus, we reject \\(H_0: \\beta_1 = 0\\) (or \\(E(Y_i)\\) = constant) at \\(\\alpha\\) if \\[ F &gt; F_{1 - \\alpha;1,n-2} \\] this is the only null hypothesis that can be tested with this approach. Coefficient of Determination \\[ R^2 = \\frac{SSR}{SSTO} = 1- \\frac{SSE}{SSTO} \\] where \\(0 \\le R^2 \\le 1\\) Interpretation: The proportionate reduction of the total variation in \\(Y\\) after fitting a linear model in \\(X\\). It is not really correct to say that \\(R^2\\) is the “variation in \\(Y\\) explained by \\(X\\)”. \\(R^2\\) is related to the correlation coefficient between \\(Y\\) and \\(X\\): \\[ R^2 = (r)^2 \\] where \\(r= corr(x,y)\\) is an estimate of the Pearson correlation coefficient. Also, note \\[ \\begin{aligned} b_1 &amp;= (\\frac{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2})^{1/2} \\\\ r &amp;= \\frac{s_y}{s_x} r \\end{aligned} \\] Lack of Fit \\(Y_{11},Y_{21}, \\dots ,Y_{n_1,1}\\): \\(n_1\\) repeat obs at \\(X_1\\) \\(Y_{1c},Y_{2c}, \\dots ,Y_{n_c,c}\\): \\(n_c\\) repeat obs at \\(X_c\\) So, there are \\(c\\) distinct \\(X\\) values. Let \\(\\bar{Y}_j\\) be the mean over replicates for \\(X_j\\) Partition the Error Sum of Squares: \\[ \\begin{aligned} \\sum_{i} \\sum_{j} (Y_{ij} - \\hat{Y}_{ij})^2 &amp;= \\sum_{i} \\sum_{j} (Y_{ij} - \\bar{Y}_j + \\bar{Y}_j + \\hat{Y}_{ij})^2 \\\\ &amp;= \\sum_{i} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{i} \\sum_{j} (\\bar{Y}_j - \\hat{Y}_{ij})^2 + \\text{cross product term} \\\\ &amp;= \\sum_{i} \\sum_{j}(Y_{ij} - \\bar{Y}_j)^2 + \\sum_j n_j (\\bar{Y}_j- \\hat{Y}_{ij})^2 \\\\ SSE &amp;= SSPE + SSLF \\\\ \\end{aligned} \\] SSPE: “pure error sum of squares” has \\(n-c\\) degrees of freedom since we need to estimate \\(c\\) means SSLF: “lack of fit sum of squares” has \\(c - 2\\) degrees of freedom (the number of unique \\(X\\) values - number of parameters used to specify the conditional mean regression model) \\[ \\begin{aligned} MSPE &amp;= \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c} \\\\ MSLF &amp;= \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2} \\end{aligned} \\] The F-test for Lack-of-Fit tests \\[ \\begin{aligned} H_0: Y_{ij} &amp;= \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\epsilon_{ij} \\sim iid N(0,\\sigma^2) \\\\ H_a: Y_{ij} &amp;= \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1,...) + \\epsilon_{ij}^*,\\epsilon_{ij}^* \\sim iid N(0, \\sigma^2) \\end{aligned} \\] \\(E(MSPE) = \\sigma^2\\) under either \\(H_0\\), \\(H_a\\) \\(E(MSLF) = \\sigma^2 + \\frac{\\sum n_j(f(X_i,...))^2}{n-2}\\) in general and \\(E(MSLF) = \\sigma^2\\) when \\(H_0\\) is true We reject \\(H_0\\) (i.e., the model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) is not adequate) if \\[ F = \\frac{MSLF}{MSPE} &gt; F_{1-\\alpha;c-2,n-c} \\] Failing to reject \\(H_0\\) does not imply that \\(H_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}\\) is exactly true, but it suggests that this model may provide a reasonable approximation to the true model. Source of Variation Sum of Squares df Mean Square F Regression SSR \\(1\\) MSR MSR / MSE Error SSE \\(n-2\\) MSE Lack of fit SSLF \\(c-2\\) MSLF MSLF / MSPE Pure Error SSPE \\(n-c\\) MSPE Total (Corrected) SSTO \\(n-1\\) Repeat observations have an effect on \\(R^2\\): It is impossible for \\(R^2\\) to attain 1 when repeat obs. exist (SSE can’t be 0) The maximum \\(R^2\\) attainable in this situation: \\[ R^2_{max} = \\frac{SSTo - SSPE}{SSTO} \\] Not all levels of X need have repeat observations. Typically, when \\(H_0\\) is appropriate, one still uses MSE as the estimate for \\(\\sigma^2\\) rather than MSPE, Since MSE has more degrees of freedom, sometimes people will pool these estimates. Joint Inference The confidence coefficient for both \\(\\beta_0\\) and \\(\\beta_1\\) considered simultaneously is \\(\\le \\alpha\\) Let \\(\\bar{A}_1\\) be the event that the first interval covers \\(\\beta_0\\) \\(\\bar{A}_2\\) be the event that the second interval covers \\(\\beta_1\\) \\[ \\begin{aligned} P(\\bar{A}_1) &amp;= 1 - \\alpha \\\\ P(\\bar{A}_2) &amp;= 1 - \\alpha \\end{aligned} \\] The probability that both \\(\\bar{A}_1\\) and \\(\\bar{A}_2\\) \\[ \\begin{aligned} P(\\bar{A}_1 \\cap \\bar{A}_2) &amp;= 1 - P(\\bar{A}_1 \\cup \\bar{A}_2) \\\\ &amp;= 1 - P(A_1) - P(A_2) + P(A_1 \\cap A_2) \\\\ &amp;\\ge 1 - P(A_1) - P(A_2) \\\\ &amp;= 1 - 2\\alpha \\end{aligned} \\] If \\(\\beta_0\\) and \\(\\beta_1\\) have separate 95% confidence intervals, the joint (family) confidence coefficient is at least \\(1 - 2(0.05) = 0.9\\). This is called a Bonferroni Inequality We could use a procedure in which we obtained \\(1-\\alpha/2\\) confidence intervals for the two regression parameters separately, then the joint (Bonferroni) family confidence coefficient would be at least \\(1- \\alpha\\) The \\(1-\\alpha\\) joint Bonferroni confidence interval for \\(\\beta_0\\) and \\(\\beta_1\\) is given by calculating: \\[ \\begin{aligned} b_0 &amp;\\pm B s(b_0) \\\\ b_1 &amp;\\pm B s(b_1) \\end{aligned} \\] where \\(B= t_{1-\\alpha/4;n-2}\\) Interpretation: If repeated samples were taken and the joint \\((1-\\alpha)\\) intervals for \\(\\beta_0\\) and \\(\\beta_1\\) were obtained, \\((1-\\alpha)100\\)% of the joint intervals would contain the true pair \\((\\beta_0, \\beta_1)\\). That is, in \\(\\alpha \\times 100\\)% of the samples, one or both intervals would not contain the true value. The Bonferroni interval is conservative. It is a lower bound and the joint intervals will tend to be correct more than \\((1-\\alpha)100\\)% of the time (lower power). People usually consider a larger \\(\\alpha\\) for the Bonferroni joint tests (e.g, \\(\\alpha=0.1\\)) The Bonferroni procedure extends to testing more than 2 parameters. Say we are interested in testing \\(\\beta_0,\\beta_1,..., \\beta_{g-1}\\) (g parameters to test). Then, the joint Bonferroni interval is obtained by calculating the \\((1-\\alpha/g)\\) 100% level interval for each separately. For example, if \\(\\alpha = 0.05\\) and \\(g=10\\), each individual test is done at the \\(1- \\frac{.05}{10}\\) level. For 2-sided intervals, this corresponds to using \\(t_{1-\\frac{0.05}{2(10)};n-p}\\) in the CI formula. This procedure works best if g is relatively small, otherwise the intervals for each individual parameter are very wide and the test is way too conservative. \\(b_0,b_1\\) are usually correlated (negatively if \\(\\bar{X} &gt;0\\) and positively if \\(\\bar{X}&lt;0\\)) Other multiple comparison procedures are available. 5.1.1.6 Assumptions Linearity of the regression function Error terms have constant variance Error terms are independent No outliers Error terms are normally distributed No Omitted variables 5.1.1.7 Diagnostics Constant Variance Plot residuals vs. X Outliers plot residuals vs. X box plots stem-leaf plots scatter plots We could use standardize the residuals to have unit variance. These standardized residuals are called studentized residuals: \\[ r_i = \\frac{e_i -\\bar{e}}{s(e_i)} = \\frac{e_i}{s(e_i)} \\] A simplified standardization procedure gives semi-studentized residuals: \\[ e_i^* = \\frac{e_i - \\bar{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}} \\] Non-independent of Error Terms plot residuals vs. time Residuals \\(e_i\\) are not independent random variables because they involve the fitted values \\(\\hat{Y}_i\\), which are based on the same fitted regression function. If the sample size is large, the dependency among \\(e_i\\) is relatively unimportant. To detect non-independence, it helps to plot the residual for the \\(i\\)-th response vs. the \\((i-1)\\)-th Non-normality of Error Terms to detect non-normality (distribution plots of residuals, box plots of residuals, stem-leaf plots of residuals, normal probability plots of residuals) Need relatively large sample sizes. Other types of departure affect the distribution of the residuals (wrong regression function, non-constant error variance,…) 5.1.1.7.1 Objective Tests of Model Assumptions Normality Use Methods based on empirical cumulative distribution function to test on residuals. Constancy of error variance Brown-Forsythe Test (Modified Levene Test) Breusch-Pagan Test (Cook-Weisberg Test) 5.1.1.8 Remedial Measures If the simple linear regression is not appropriate, one can: more complicated models transformations on \\(X\\) and/or \\(Y\\) (may not be “optimal” results) Remedial measures based on deviations: Non-linearity: Transformations more complicated models Non-constant error variance: Weighted Least Squares Transformations Correlated errors: serially correlated error models (times series) Non-normality Additional variables: multiple regression. Outliers: Robust estimation. 5.1.1.8.1 Transformations use transformations of one or both variables before performing the regression analysis. The properties of least-squares estimates apply to the transformed regression, not the original variable. If we transform the Y variable and perform regression to get: \\[ g(Y_i) = b_0 + b_1 X_i \\] Transform back: \\[ \\hat{Y}_i = g^{-1}(b_0 + b_1 X_i) \\] \\(\\hat{Y}_i\\) will be biased. we can correct this bias. Box-Cox Family Transformations \\[ Y&#39;= Y^{\\lambda} \\] where \\(\\lambda\\) is a parameter to be determined from the data. \\(\\lambda\\) \\(Y&#39;\\) 2 \\(Y^2\\) 0.5 \\(\\sqrt{Y}\\) 0 \\(ln(Y)\\) -0.5 \\(1/\\sqrt{Y}\\) -1 \\(1/Y\\) To pick \\(\\lambda\\), we can do estimation by: trial and error maximum likelihood numerical search Variance Stabilizing Transformations A general method for finding a variance stabilizing transformation, when the standard deviation is a function of the mean, is the delta method - an application of a Taylor series expansion. \\[ \\sigma = \\sqrt{var(Y)} = f(\\mu) \\] where \\(\\mu = E(Y)\\) and \\(f(\\mu)\\) is some smooth function of the mean. Consider the transformation \\(h(Y)\\). Expand this function in a Taylor series about \\(\\mu\\). Then, \\[ h(Y) = h(\\mu) + h&#39;(\\mu)(Y-\\mu) + \\text{small terms} \\] we want to select the function h(.) so that the variance of h(Y) is nearly constant for all values of \\(\\mu= E(Y)\\): \\[ \\begin{aligned} const &amp;= var(h(Y)) \\\\ &amp;= var(h(\\mu) + h&#39;(\\mu)(Y-\\mu)) \\\\ &amp;= (h&#39;(\\mu))^2 var(Y-\\mu) \\\\ &amp;= (h&#39;(\\mu))^2 var(Y) \\\\ &amp;= (h&#39;(\\mu))^2(f(\\mu))^2 \\\\ \\end{aligned} \\] we must have, \\[ h&#39;(\\mu) \\propto \\frac{1}{f(\\mu)} \\] then, \\[ h(\\mu) = \\int\\frac{1}{f(\\mu)}d\\mu \\] Example: For the Poisson distribution: \\(\\sigma^2 = var(Y) = E(Y) = \\mu\\) Then, \\[ \\begin{aligned} \\sigma = f(\\mu) &amp;= \\sqrt{mu} \\\\ h&#39;(\\mu) &amp;\\propto \\frac{1}{\\mu} = \\mu^{-.5} \\end{aligned} \\] Then, the variance stabilizing transformation is: \\[ h(\\mu) = \\int \\mu^{-.5} d\\mu = \\frac{1}{2} \\sqrt{\\mu} \\] hence, \\(\\sqrt{Y}\\) is used as the variance stabilizing transformation. If we don’t know \\(f(\\mu)\\) Trial and error. Look at residuals plots Ask researchers about previous studies or find published results on similar experiments and determine what transformation was used. If you have multiple observations \\(Y_{ij}\\) at the same X values, compute \\(\\bar{Y}_i\\) and \\(s_i\\) and plot them If \\(s_i \\propto \\bar{Y}_i^{\\lambda}\\) then consider \\(s_i = a \\bar{Y}_i^{\\lambda}\\) or \\(ln(s_i) = ln(a) + \\lambda ln(\\bar{Y}_i)\\). So regression the natural log of \\(s_i\\) on the natural log of \\(\\bar{Y}_i\\) gives \\(\\hat{a}\\) and \\(\\hat{\\lambda}\\) and suggests the form of \\(f(\\mu)\\) If we don’t have multiple obs, might still be able to “group” the observations to get \\(\\bar{Y}_i\\) and \\(s_i\\). Transformation Situation Comments \\(\\sqrt{Y}\\) \\(var(\\epsilon_i) = k E(Y_i)\\) counts from Poisson dist \\(\\ sqrt{Y} + \\sqrt{Y+1}\\) \\(var(\\epsilon_i) = k E(Y_i)\\) small counts or zeroes \\(log(Y)\\) \\(var(\\epsilon_i) = k (E(Y_i))^2\\) positive integers with wide range \\(log(Y+1)\\) \\(var(\\epsilon_i) = k(E(Y_i))^2\\) some counts zero 1/Y \\(var(\\epsilon_i) = k(E(Y_i))^4\\) most responses near zero, others large \\(arcsin(\\sqrt{Y})\\) \\(var(\\epsilon_i) = k E(Y_i)(1-E(Y_i))\\) data are binomial proportions or % 5.1.2 Multiple Linear Regression Geometry of Least Squares \\[ \\begin{aligned} \\mathbf{\\hat{y}} &amp;= \\mathbf{Xb} \\\\ &amp;= \\mathbf{X(X&#39;X)^{-1}X&#39;y} \\\\ &amp;= \\mathbf{Hy} \\end{aligned} \\] sometimes \\(\\mathbf{H}\\) is denoted as \\(\\mathbf{P}\\). \\(\\mathbf{H}\\) is the projection operator. \\[ \\mathbf{\\hat{y}= Hy} \\] is the projection of y onto the linear space spanned by the columns of \\(\\mathbf{X}\\) (model space). The dimension of the model space is the rank of \\(\\mathbf{X}\\). Facts: \\(\\mathbf{H}\\) is symmetric (i.e., \\(\\mathbf{H} = \\mathbf{H}&#39;\\)) \\(\\mathbf{HH} = \\mathbf{H}\\) \\[ \\begin{aligned} \\mathbf{HH} &amp;= \\mathbf{X(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}X&#39;} \\\\ &amp;= \\mathbf{X(X&#39;X)^{-1}IX&#39;} \\\\ &amp;= \\mathbf{X(X&#39;X)^{-1}X&#39;} \\end{aligned} \\] \\(\\mathbf{H}\\) is an \\(n \\times n\\) matrix with \\(rank(\\mathbf{H}) = rank(\\mathbf{X})\\) \\(\\mathbf{(I-H) = I - X(X&#39;X)^{-1}X&#39;}\\) is also a projection operator. It projects onto the \\(n - k\\) dimensional space that is orthogonal to the \\(k\\) dimensional space spanned by the columns of \\(\\mathbf{X}\\) \\(\\mathbf{H(I-H)=(I-H)H = 0}\\) Partition of uncorrected total sum of squares: \\[ \\begin{aligned} \\mathbf{y&#39;y} &amp;= \\mathbf{\\hat{y}&#39;\\hat{y} + e&#39;e} \\\\ &amp;= \\mathbf{(Hy)&#39;(Hy) + ((I-H)y)&#39;((I-H)y)} \\\\ &amp;= \\mathbf{y&#39;H&#39;Hy + y&#39;(I-H)&#39;(I-H)y} \\\\ &amp;= \\mathbf{y&#39;Hy + y&#39;(I-H)y} \\\\ \\end{aligned} \\] or partition for the corrected total sum of squares: \\[ \\mathbf{y&#39;(I-H_1)y = y&#39;(H-H_1)y + y&#39;(I-H)y} \\] where \\(H_1 = \\frac{1}{n} J = 1&#39;(1&#39;1)1\\) Source SS df MS F Regression \\(SSR = \\mathbf{y&#39; (H-\\frac{1}{n}J)y}\\) \\(p - 1\\) \\(SSR/(p-1)\\) \\(MSR /MSE\\) Error \\(SSE = \\mathbf{y&#39;(I - H)y}\\) \\(n - p\\) \\(SSE /(n-p)\\) Total \\(\\mathbf{y&#39;y - y&#39;Jy/n}\\) \\(n -1\\) Equivalently, we can express \\[ \\mathbf{Y = X\\hat{\\beta} + (Y - X\\hat{\\beta})} \\] where \\(\\mathbf{\\hat{Y} = X \\hat{\\beta}}\\) = sum of a vector of fitted values \\(\\mathbf{e = ( Y - X \\hat{\\beta})}\\) = residual \\(\\mathbf{Y}\\) is the \\(n \\times 1\\) vector in a n-dimensional space \\(R^n\\) \\(\\mathbf{X}\\) is an \\(n \\times p\\) full rank matrix. and its columns generate a \\(p\\)-dimensional subspace of \\(R^n\\). Hence, any estimator \\(\\mathbf{X \\hat{\\beta}}\\) is also in this subspace. We choose least squares estimator that minimize the distance between \\(\\mathbf{Y}\\) and \\(\\mathbf{X \\hat{\\beta}}\\), which is the orthogonal projection of \\(\\mathbf{Y}\\) onto \\(\\mathbf{X\\beta}\\). \\[ \\begin{aligned} ||\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}||^2 &amp;= \\mathbf{||Y - X\\hat{\\beta}||}^2 + \\mathbf{||X \\hat{\\beta}||}^2 \\\\ &amp;= \\mathbf{(Y - X\\hat{\\beta})&#39;(Y - X\\hat{\\beta}) +(X \\hat{\\beta})&#39;(X \\hat{\\beta})} \\\\ &amp;= \\mathbf{(Y - X\\hat{\\beta})&#39;Y - (Y - X\\hat{\\beta})&#39;X\\hat{\\beta} + \\hat{\\beta}&#39; X&#39;X\\hat{\\beta}} \\\\ &amp;= \\mathbf{(Y-X\\hat{\\beta})&#39;Y + \\hat{\\beta}&#39;X&#39;X(XX&#39;)^{-1}X&#39;Y} \\\\ &amp;= \\mathbf{Y&#39;Y - \\hat{\\beta}&#39;X&#39;Y + \\hat{\\beta}&#39;X&#39;Y} \\end{aligned} \\] where the norm of a \\((p \\times 1)\\) vector \\(\\mathbf{a}\\) is defined by: \\[ \\mathbf{||a|| = \\sqrt{a&#39;a}} = \\sqrt{\\sum_{i=1}^p a^2_i} \\] Coefficient of Multiple Determination \\[ R^2 = \\frac{SSR}{SSTO}= 1- \\frac{SSE}{SSTO} \\] Adjusted Coefficient of Multiple Determination \\[ R^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{(n-1)SSE}{(n-p)SSTO} \\] Sequential and Partial Sums of Squares: In a regression model with coefficients \\(\\beta = (\\beta_0, \\beta_1,...,\\beta_{p-1})&#39;\\), we denote the uncorrected and corrected SS by \\[ \\begin{aligned} SSM &amp;= SS(\\beta_0, \\beta_1,...,\\beta_{p-1}) \\\\ SSM_m &amp;= SS(\\beta_0, \\beta_1,...,\\beta_{p-1}|\\beta_0) \\end{aligned} \\] There are 2 decompositions of \\(SSM_m\\): Sequential SS: (not unique -depends on order, also referred to as Type I SS, and is the default of anova() in R) \\[ SSM_m = SS(\\beta_1 | \\beta_0) + SS(\\beta_2 | \\beta_0, \\beta_1) + ...+ SS(\\beta_{p-1}| \\beta_0,...,\\beta_{p-2}) \\] Partial SS: (use more in practice - contribution of each given all of the others) \\[ SSM_m = SS(\\beta_1 | \\beta_0,\\beta_2,...,\\beta_{p-1}) + ... + SS(\\beta_{p-1}| \\beta_0, \\beta_1,...,\\beta_{p-2}) \\] 5.1.3 OLS Assumptions A1 Linearity A2 Full rank A3 Exogeneity of Independent Variables A4 Homoskedasticity A5 Data Generation (random Sampling) A6 Normal Distribution 5.1.3.1 A1 Linearity \\[\\begin{equation} A1: y=\\mathbf{x}\\beta + \\epsilon \\tag{5.1} \\end{equation}\\] Not restrictive \\(x\\) can be nonlinear transformation including interactions, natural log, quadratic With A3 (Exogeneity of Independent), linearity can be restrictive 5.1.3.1.1 Log Model Model Form Interpretation of \\(\\beta\\) In words Level-Level \\(y =\\beta_0+\\beta_1x+\\epsilon\\) \\(\\Delta y = \\beta_1 \\Delta x\\) A unit change in \\(x\\) will result in \\(\\beta_1\\) unit change in \\(y\\) Log-Level \\(ln(y) = \\beta_0 + \\beta_1x + \\epsilon\\) \\(\\% \\Delta y=100 \\beta_1 \\Delta x\\) A unit change in \\(x\\) result in 100 \\(\\beta_1\\) % change in \\(y\\) Level-Log \\(y = \\beta _0 + \\beta_1 ln (x) + \\epsilon\\) \\(\\Delta y = (\\beta_1/ 100)\\%\\Delta x\\) One percent change in \\(x\\) result in \\(\\beta_1/100\\) units change in \\(y\\) Log-Log \\(ln(y) = \\beta_0 + \\beta_1 l n(x) +\\epsilon\\) \\(\\% \\Delta y= \\beta _1 \\% \\Delta x\\) One percent change in \\(x\\) result in \\(\\beta_1\\) percent change in \\(y\\) 5.1.3.1.2 Higher Orders \\(y=\\beta_0 + x_1\\beta_1 + x_1^2\\beta_2 + \\epsilon\\) \\[ \\frac{\\partial y}{\\partial x_1}=\\beta_1 + 2x_1\\beta_2 \\] The effect of \\(x_1\\) on y depends on the level of \\(x_1\\) The partial effect at the average = \\(\\beta_1+2E(x_1)\\beta_2\\) Average Partial Effect = \\(E(\\beta_1 + 2x_1\\beta_2)\\) 5.1.3.1.3 Interactions \\(y=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_3 + \\epsilon\\) \\(\\beta_1\\) is the average effect on y for a unit change in \\(x_1\\) when \\(x_2=0\\) \\(\\beta_1 + x_2\\beta_3\\) is the partial effect of \\(x_1\\) on y which depends on the level of \\(x_2\\) 5.1.3.2 A2 Full rank \\[\\begin{equation} A2: rank(E(x&#39;x))=k \\tag{5.2} \\end{equation}\\] also known as identification condition columns of \\(\\mathbf{x}\\) cannot be written as a linear function of the other columns which ensures that each parameter is unique and exists in the population regression equation 5.1.3.3 A3 Exogeneity of Independent Variables \\[\\begin{equation} A3: E[\\epsilon|x_1,x_2,...,x_k]=E[\\epsilon|\\mathbf{x}]=0 \\tag{5.3} \\end{equation}\\] strict exogeneity also known as mean independence check back on Correlation and Independence by the Law of Iterated Expectations \\(E(\\epsilon)=0\\), which can be satisfied by always including an intercept. independent variables do not carry information for prediction of \\(\\epsilon\\) A3 implies \\(E(y|x)=x\\beta\\), which means the conditional mean function must be a linear function of \\(x\\) A1 Linearity 5.1.3.3.1 A3a Weaker Exogeneity Assumption Exogeneity of Independent variables A3a: \\(E(\\mathbf{x_i&#39;}\\epsilon_i)=0\\) \\(x_i\\) is uncorrelated with \\(\\epsilon_i\\) Correlation and Independence Weaker than mean independence A3 A3 implies A3a, not the reverse No causality interpretations Cannot test the difference 5.1.3.4 A4 Homoskedasticity \\[\\begin{equation} A4: Var(\\epsilon|x)=Var(\\epsilon)=\\sigma^2 \\tag{5.4} \\end{equation}\\] Variation in the disturbance to be the same over the independent variables 5.1.3.5 A5 Data Generation (random Sampling) \\[\\begin{equation} A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n} \\tag{5.5} \\end{equation}\\] is a random sample random sample mean samples are independent and identically distributed (iid) from a joint distribution of \\((y,\\mathbf{x})\\) with A3 and A4, we have Strict Exogeneity: \\(E(\\epsilon_i|x_1,...,x_n)=0\\). independent variables do not carry information for prediction of \\(\\epsilon\\) Non-autocorrelation: \\(E(\\epsilon_i\\epsilon_j|x_1,...,x_n)=0\\) The error term is uncorrelated across the draws conditional on the independent variables \\(\\rightarrow\\) \\(A4: Var(\\epsilon|\\mathbf{X})=Var(\\epsilon)=\\sigma^2I_n\\) In times series and spatial settings, A5 is less likely to hold. 5.1.3.5.1 A5a A stochastic process \\(\\{x_t\\}_{t=1}^T\\) is stationary if for every collection fo time indices \\(\\{t_1,t_2,...,t_m\\}\\), the joint distribution of \\[ x_{t_1},x_{t_2},...,x_{t_m} \\] is the same as the joint distribution of \\[ x_{t_1+h},x_{t_2+h},...,x_{t_m+h} \\] for any \\(h \\ge 1\\) The joint distribution for the first ten observation is the same for the next ten, etc. Independent draws automatically satisfies this A stochastic process \\(\\{x_t\\}_{t=1}^T\\) is weakly stationary if \\(x_t\\) and \\(x_{t+h}\\) are “almost independent” as h increases without bounds. two observation that are very far apart should be “almost independent” Common Weakly Dependent Processes Moving Average process of order 1 (MA(1)) MA(1) means that there is only one period lag. \\[ \\begin{aligned} y_t &amp;= u_t + \\alpha_1 u_{t-1} \\\\ E(y_t) &amp;= E(u_t) + \\alpha_1E(u_{t-1}) = 0 \\\\ Var(y_t) &amp;= var(u_t) + \\alpha_1 var(u_{t-1}) \\\\ &amp;= \\sigma^2 + \\alpha_1^2 \\sigma^2 \\\\ &amp;= \\sigma^2(1+\\alpha_1^2) \\end{aligned} \\] where \\(u_t\\) is drawn iid over t with variance \\(\\sigma^2\\) An increase in the absolute value of \\(\\alpha_1\\) increases the variance When the MA(1) process can be inverted (\\(|\\alpha|&lt;1\\) then \\[ u_t = y_t - \\alpha_1u_{t-1} \\] called the autoregressive representation (express current observation in term of past observation). We can expand it to more than 1 lag, then we have MA(q) process \\[ y_t = u_t + \\alpha_1 u_{t-1} + ... + \\alpha_q u_{t-q} \\] where \\(u_t \\sim WN(0,\\sigma^2)\\) Covariance stationary: irrespective of the value of the parameters. Invertibility when \\(\\alpha &lt; 1\\) The conditional mean of MA(q) depends on the q lags (long-term memory). In MA(q), all autocorrelations beyond q are 0. \\[ \\begin{aligned} Cov(y_t,y_{t-1}) &amp;= Cov(u_t + \\alpha_1 u_{t-1},u_{t-1}+\\alpha_1u_{t-2}) \\\\ &amp;= \\alpha_1var(u_{t-1}) \\\\ &amp;= \\alpha_1\\sigma^2 \\end{aligned} \\] \\[ \\begin{aligned} Cov(y_t,y_{t-2}) &amp;= Cov(u_t + \\alpha_1 u_{t-1},u_{t-2}+\\alpha_{1}u_{t-3}) \\\\ &amp;= 0 \\end{aligned} \\] An MA models a linear relationship between the dependent variable and the current and past values of a stochastic term. Auto regressive process of order 1 (AR(1)) \\[ y_t = \\rho y_{t-1}+ u_t, |\\rho|&lt;1 \\] where \\(u_t\\) is drawn iid over t with variance \\(\\sigma^2\\) \\[ \\begin{aligned} Cov(y_t,y_{t-1}) &amp;= Cov(\\rho y_{t-1} + u-t,y_{t-1}) \\\\ &amp;= \\rho Var(y_{t-1}) \\\\ &amp;= \\rho \\frac{\\sigma^2}{1-\\rho^2} \\end{aligned} \\] \\[ \\begin{aligned} Cov(y_t,y_{t-h}) &amp;= \\rho^h \\frac{\\sigma^2}{1-\\rho^2} \\end{aligned} \\] Stationarity: in the continuum of t, the distribution of each t is the same \\[ \\begin{aligned} E(y_t) &amp;= E(y_{t-1}) = ...= E(y_0) \\\\ y_1 &amp;= \\rho y_0 + u_1 \\end{aligned} \\] where the initial observation \\(y_0=0\\) Assume \\(E(y_t)=0\\) \\[ \\begin{aligned} y_t &amp;= \\rho^t y_{t-t} + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t \\\\ &amp;= \\rho^t y_0 + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t \\end{aligned} \\] Hence, \\(y_t\\) is the weighted of all of the \\(u_t\\) time observations before. y will be correlated with all the previous observations as well as future observations. \\[ \\begin{aligned} Var(y_t) &amp;= Var(\\rho y_{t-1} + u_t) \\\\ &amp;= \\rho^2 Var(y_{t-1}) + Var(u_t) + 2\\rho Cov(y_{t-1}u_t) \\\\ &amp;= \\rho^2 Var(y_{t-1}) + \\sigma^2 \\end{aligned} \\] Hence, \\[ Var(y_t) = \\frac{\\sigma^2}{1-\\rho^2} \\] to have Variance constantly over time, then \\(\\rho \\neq 1\\) or \\(-1\\). Then stationarity requires \\(\\rho \\neq 1\\) or -1. weakly dependent process \\(|\\rho|&lt;1\\) To estimate the AR(1) process, we use Yule-Walker Equation \\[ \\begin{aligned} y_t &amp;= \\epsilon_t + \\phi y_{t-1} \\\\ y_t y_{t-\\tau} &amp;= \\epsilon_t y_{t-\\tau} + \\phi y_{t-1}y_{t-\\tau} \\end{aligned} \\] For \\(\\tau \\ge 1\\), we have \\[ \\gamma \\tau = \\phi \\gamma (\\tau -1) \\] \\[ \\rho_t = \\phi^t \\] when you generalize to \\(p\\)-th order autoregressive process, AR(p): \\[ y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t \\] AR(p) process is covariance stationary, and decay in autocorrelations. When we combine MA(q) and AR(p), we have ARMA(p,q) process, where you can see seasonality. For example, ARMA(1,1) \\[ y_t = \\phi y_{t-1} + \\epsilon_t + \\alpha \\epsilon_{t-1} \\] Random Walk process \\[ y_t = y_0 + \\sum_{s=1}^{t}u_t \\] not stationary : when \\(y_0 = 0\\) then \\(E(y_t)= 0\\), but \\(Var(y_t)=t\\sigma^2\\). Further along in the spectrum, the variance will be larger not weakly dependent: \\(Cov(\\sum_{s=1}^{t}u_s,\\sum_{s=1}^{t-h}u_s) = (t-h)\\sigma^2\\). So the covariance (fixed) is not diminishing as h increases Assumption A5a: \\(\\{y_t,x_{t1},..,x_{tk-1} \\}\\) where \\(t=1,...,T\\) are stationary and weakly dependent processes. Alternative [Weak Law], Central Limit Theorem If \\(z_t\\) is a weakly dependent stationary process with a finite first absolute moment and \\(E(z_t) = \\mu\\), then \\[ T^{-1}\\sum_{t=1}^{T}z_t \\to^p \\mu \\] If additional regulatory conditions hold (Greene 1990), then \\[ \\sqrt{T}(\\bar{z}-\\mu) \\to^d N(0,B) \\] where \\(B= Var(z_t) + 2\\sum_{h=1}^{\\infty}Cov(z_t,z_{t-h})\\) 5.1.3.6 A6 Normal Distribution \\[\\begin{equation} A6: \\epsilon|\\mathbf{x}\\sim N(0,\\sigma^2I_n) \\tag{5.6} \\end{equation}\\] The error term is normally distributed From A1-A3, we have identification (also known as Orthogonality Condition) of the population parameter \\(\\beta\\) \\[ \\begin{aligned} y &amp;= {x}\\beta + \\epsilon &amp;&amp; \\text{A1} \\\\ x&#39;y &amp;= x&#39;x\\beta + x&#39;\\epsilon &amp;&amp; \\text{} \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\beta + E(x&#39;\\epsilon) &amp;&amp; \\text{} \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\beta &amp;&amp; \\text{A3} \\\\ [E(x&#39;x)]^{-1}E(x&#39;y) &amp;= [E(x&#39;x)]^{-1}E(x&#39;x)\\beta &amp;&amp; \\text{A2} \\\\ [E(x&#39;x)]^{-1}E(x&#39;y) &amp;= \\beta \\end{aligned} \\] \\(\\beta\\) is the row vector of parameters that produces the best predictor of y we choose the min of \\(\\gamma\\) : \\[ \\underset{\\gamma}{\\operatorname{argmin}}E((y-x\\gamma)^2) \\] First Order Condition \\[ \\begin{aligned} \\frac{\\partial((y-x\\gamma)^2)}{\\partial\\gamma}&amp;=0 \\\\ -2E(x&#39;(y-x\\gamma))&amp;=0 \\\\ E(x&#39;y)-E(x&#39;x\\gamma) &amp;=0 \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\gamma \\\\ (E(x&#39;x))^{-1}E(x&#39;y) &amp;= \\gamma \\end{aligned} \\] Second Order Condition \\[ \\begin{aligned} \\frac{\\partial^2E((y-x\\gamma)^2)}{\\partial \\gamma&#39;^2}&amp;=0 \\\\ E(\\frac{\\partial(y-x\\partial)^2)}{\\partial\\gamma\\partial\\gamma&#39;}) &amp;= 2E(x&#39;x) \\end{aligned} \\] If A3 holds, then \\(2E(x&#39;x)\\) is PSD \\(\\rightarrow\\) minimum 5.1.4 Theorems 5.1.4.1 Frisch-Waugh-Lovell Theorem \\[ \\mathbf{y=X\\beta + \\epsilon=X_1\\beta_1+X_2\\beta_2 +\\epsilon} \\] Equivalently, \\[ \\left( \\begin{array} {cc} X_1&#39;X_1 &amp; X_1&#39;X_2 \\\\ X_2&#39;X_1 &amp; X_2&#39;X_2 \\end{array} \\right) \\left( \\begin{array} {c} \\hat{\\beta_1} \\\\ \\hat{\\beta_2} \\end{array} \\right) = \\left( \\begin{array}{c} X_1&#39;y \\\\ X_2&#39;y \\end{array} \\right) \\] Hence, \\[ \\mathbf{\\hat{\\beta_1}=(X_1&#39;X_1)^{-1}X_1&#39;y - (X_1&#39;X_1)^{-1}X_1&#39;X_2\\hat{\\beta_2}} \\] Betas from the multiple regression are not the same as the betas from each of the individual simple regression Different set of X will affect all the coefficient estimates. If \\(X_1&#39;X_2 = 0\\) or \\(\\hat{\\beta_2}=0\\), then 1 and 2 do not hold. 5.1.4.2 Gauss-Markov Theorem For a linear regression model \\[ \\mathbf{y=X\\beta + \\epsilon} \\] Under A1, A2, A3, A4, OLS estimator defined as \\[ \\hat{\\beta} = \\mathbf{(X&#39;X)^{-1}X&#39;y} \\] is the minimum variance linear (in \\(y\\)) unbiased estimator of \\(\\beta\\) Let \\(\\tilde{\\beta}=\\mathbf{Cy}\\), be another linear estimator where \\(\\mathbf{C}\\) is \\(k \\times n\\) and only function of \\(\\mathbf{X}\\)), then for it be unbiased, \\[ \\begin{aligned} E(\\tilde{\\beta}|\\mathbf{X}) &amp;= E(\\mathbf{Cy|X}) \\\\ &amp;= E(\\mathbf{CX\\beta + C\\epsilon|X}) \\\\ &amp;= \\mathbf{CX\\beta} \\end{aligned} \\] which equals the true parameter \\(\\beta\\) only if \\(\\mathbf{CX=I}\\) Equivalently, \\(\\tilde{\\beta} = \\beta + \\mathbf{C}\\epsilon\\) and the variance of the estimator is \\(Var(\\tilde{\\beta}|\\mathbf{X}) = \\sigma^2\\mathbf{CC&#39;}\\) To show minimum variance, \\[ \\begin{aligned} &amp;=\\sigma^2\\mathbf{(C-(X&#39;X)^{-1}X&#39;)(C-(X&#39;X)^{-1}X&#39;)&#39;} \\\\ &amp;= \\sigma^2\\mathbf{(CC&#39; - CX(X&#39;X)^{-1})-(X&#39;X)^{-1}X&#39;C + (X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1})} \\\\ &amp;= \\sigma^2 (\\mathbf{CC&#39; - (X&#39;X)^{-1}-(X&#39;X)^{-1} + (X&#39;X)^{-1}}) \\\\ &amp;= \\sigma^2\\mathbf{CC&#39;} - \\sigma^2(\\mathbf{X&#39;X})^{-1} \\\\ &amp;= Var(\\tilde{\\beta}|\\mathbf{X}) - Var(\\hat{\\beta}|\\mathbf{X}) \\end{aligned} \\] Hierarchy of OLS Assumptions Identification Data Description Unbiasedness Consistency Gauss- Markov (BLUE) Asymptotic Inference (z and Chi-squared) Classical LM (BUE) Small-sample Inference (t and F) Variation in \\(\\mathbf{X}\\) Variation in \\(\\mathbf{X}\\) Variation in \\(\\mathbf{X}\\) Variation in \\(\\mathbf{X}\\) Random Sampling Random Sampling Random Sampling Linearity in Parameters Linearity in Parameters Linearity in Parameters Zero Conditional Mean Zero Conditional Mean Zero Conditional Mean \\(\\mathbf{H}\\) homoskedasticity \\(\\mathbf{H}\\) homoskedasticity Normality of Errors 5.1.5 Variable Selection depends on Objectives or goals Previously acquired expertise Availability of data Availability of computer software Let \\(P - 1\\) be the number of possible \\(X\\) variables 5.1.5.1 Mallows’s \\(C_p\\) Statistic (Mallows, 1973, Technometrics, 15, 661-675) A measure of the predictive ability of a fitted model Let \\(\\hat{Y}_{ip}\\) be the predicted value of \\(Y_i\\) using the model with \\(p\\) parameters. The total standardized mean square error of prediction is: \\[ \\begin{aligned} \\Gamma_p &amp;= \\frac{\\sum_{i=1}^n E(\\hat{Y}_{ip}-E(Y_i))^2}{\\sigma^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n [E(\\hat{Y}_{ip})-E(Y_i)]^2+\\sum_{i=1}^n var(\\hat{Y}_{ip})}{\\sigma^2} \\end{aligned} \\] the first term in the numerator is the (bias)^2 term and the 2nd term is the prediction variance term. bias term decreases as more variables are added to the model. if we assume the full model \\((p=P)\\) is the true model, then \\(E(\\hat{Y}_{ip}) - E(Y_i) = 0\\) and the bias is 0. Prediction variance increase as more variables are added to the model \\(\\sum var(\\hat{Y}_{ip}) = p \\sigma^2\\) thus, a tradeoff between bias and variance terns is achieved by minimizing \\(\\Gamma_p\\). Since \\(\\Gamma_p\\) is unknown (due to \\(\\beta\\)). we use an estimate: \\(C_p = \\frac{SSE_p}{\\hat{\\sigma^2}}- (n-2p)\\) which is an unbiased estimate of \\(\\Gamma_p\\) As more variables are added to the model, the \\(SSE_p\\) decreases but 2p increases. where \\(\\hat{\\sigma^2}=MSE(X_1,..,X_{P-1})\\) the MSE with all possible X variables in the model. when there is no bias then \\(E(C_p) \\approx p\\). Thus, good models have \\(C_p\\) close to p. Prediction: consider models with \\(C_p \\le p\\) Parameter estimation: consider models with \\(C_p \\le 2p -(P-1)\\). Fewer variables should be eliminated from the model to avoid excess bias in the estimates. 5.1.5.2 Akaike Information Criterion (AIC) \\[ AUC = n ln(\\frac{SSE_p}{n}) + 2p \\] increasing \\(p\\) (number of parameters) leads first-term decreases, and second-term increases. We want model with small values of AIC. If the AIC increases when a parameter is added to the model, that parameter is not needed. AIC represents a tradeoff between precision of fit against the number of parameters used. 5.1.5.3 Bayes (or Schwarz) Information Criterion \\[ BIC = n \\ln(\\frac{SSE_p}{n})+ (\\ln n)p \\] The coefficient in front of p tends to penalize more heavily models with a larger number of parameters (as compared to AIC). 5.1.5.4 Prediction Error Sum of Squares (PRESS) \\[ PRESS_p = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_{i(i)})^2 \\] where \\(\\hat{Y}_{i(i)}\\) is the prediction of the i-th response when the i-th observation is not used, obtained for the model with p parameters. evaluates the predictive ability of a postulated model by omitting one observation at a time. We want small \\(PRESS_p\\) values It can be computationally intensive when you have large p. 5.1.5.5 Best Subsets Algorithm “leap and bounds” algorithm of (Furnival and Wilson 2000) combines comparison of SSE for different subset models with control over the sequence in which the subset regression are computed. Guarantees finding the best m subset regressions within each subset size with less computational burden than all possible subsets. library(&quot;leaps&quot;) regsubsets() 5.1.5.6 Stepwise Selection Procedures The forward stepwise procedure: finds a plausible subset sequentially. at each step, a variable is added or deleted. criterion for adding or deleting is based on SSE, \\(R^2\\), t, or F-statistic. Note: Instead of using exact F-values, computer packages usually specify the equivalent “significance” level. For example, SLE is the “significance” level to enter, and SLS is the “significance” level to stay. The SLE and SLS are guides rather than true tests of significance. The choice of SLE and SLS represents a balancing of opposing tendencies. Use of large SLE values tends to result in too many predictor variables; models with small SLE tend to be under-specified resulting in \\(\\sigma^2\\) being badly overestimated. As for choice of SLE, can choose between 0.05 and 0.5. If SLE &gt; SLS then a cycling pattern may occur. Although most computer packages can detect can stop when it happens. A quick fix: SLS = SLE /2 (Bendel and Afifi 1977). If SLE &lt; SLS then the procedure is conservative and may lead variables with low contribution to be retained. Order of variable entry does not matter. Automated Selection Procedures: Forward selection: Same idea as forward stepwise except it doesn’t test if variables should be dropped once enter. (not as good as forward stepwise). Backward Elimination: begin with all variables and identifies the one with the smallest F-value to be dropped. 5.1.6 Diagnostics 5.1.6.1 Normality of errors could use Methods based on normal probability plot or Methods based on empirical cumulative distribution function or plots such as y = 1:100 x = rnorm(100) qqplot(x,y) 5.1.6.2 Influential observations/outliers 5.1.6.2.1 Hat matrix \\[ \\mathbf{H = X(X&#39;X)^{-1}} \\] where \\(\\mathbf{\\hat{Y}= HY, e = (I-H)Y}\\) and \\(var(\\mathbf{e}) = \\sigma^2 (\\mathbf{I-H})\\) \\(\\sigma^2(e_i) = \\sigma^2 (1-h_{ii})\\), where \\(h_{ii}\\) is the \\(i\\)-th element of the main diagonal of \\(\\mathbf{H}\\) (must be between 0 and 1). \\(\\sum_{i=1}^{n} h_{ii} = p\\) \\(cov(e_i,e_j) = -h_{ii}\\sigma^2\\) where \\(i \\neq j\\) Estimate: \\(s^2(e_i) = MSE (1-h_{ii})\\) Estimate: \\(\\hat{cov}(e_i,e_j) = -h_{ij}(MSE)\\); if model assumption are correct, this covariance is very small for large data sets. If \\(\\mathbf{x}_i = [1 X_{i,1} ... X_{i,p-1}]&#39;\\) (the vector of X-values for a given response), then \\(h_{ii} = \\mathbf{x_i&#39;(X&#39;X)^{-1}x_i}\\) (depends on relative positions of the design points \\(X_{i,1},...,X_{i,p-1}\\)) 5.1.6.2.2 Studentized Residuals \\[ \\begin{aligned} r_i &amp;= \\frac{e_i}{s(e_i)} \\\\ r_i &amp;\\sim N(0,1) \\end{aligned} \\] where \\(s(e_i) = \\sqrt{MSE(1-h_{ii})}\\). \\(r_i\\) is called the studentized residual or standardized residual. you can use the semi-studentized residual before, \\(e_i^*= e_i \\sqrt{MSE}\\). This doesn’t take into account the different variances for each \\(e_i\\). We would want to see the model without a particular value. You delete the \\(i\\)-th case, fit the regression to the remaining \\(n-1\\) cases, get estimated responses for the \\(i\\)-th case, \\(\\hat{Y}_{i(i)}\\), and find the difference, called the deleted residual: \\[ \\begin{aligned} d_i &amp;= Y_i - \\hat{Y}_{i(i)} \\\\ &amp;= \\frac{e_i}{1-h_{ii}} \\end{aligned} \\] we don’t need to recompute the regression model for each case As \\(h_{ii}\\) increases, \\(d_i\\) increases. \\[ s^2(d_i)= \\frac{MSE_{(i)}}{1-h_{ii}} \\] where \\(MSE_{(i)}\\) is the mean square error when the i-th case is omitted. Let \\[ t_i = \\frac{d_i}{s(d_i)} = \\frac{e_i}{\\sqrt{MSE_{(i)}(1-h_{ii})}} \\] be the studentized deleted residual, which follows a t-distribution with \\(n-p-1\\) df. \\[ (n-p)MSE = (n-p-1)MSE_{(i)}+ \\frac{e^2_{i}}{1-h_{ii}} \\] hence, we do not need to fit regressions for each case and \\[ t_i = e_i (\\frac{n-p-1}{SSE(1-h_{ii})-e^2_i})^{1/2} \\] The outlying \\(Y\\)-observations are those cases whose studentized deleted residuals are large in absolute value. If there are many residuals to consider, a Bonferroni critical value can be can (\\(t_{1-\\alpha/2n;n-p-1}\\)) Outlying X Observations Recall, \\(0 \\le h_{ii} \\le 1\\) and \\(\\sum_{i=1}^{n}h_{ii}=p\\) (the total number of parameters) A large \\(h_{ii}\\) indicates that the \\(i\\)-th case is distant from the center of all \\(X\\) observations (the leverage of the \\(i\\)-th case). That is, a large value suggests that the observation exercises substantial leverage in determining the fitted value \\(\\hat{Y}_i\\) We have \\(\\mathbf{\\hat{Y}=HY}\\), a linear combination of Y-values; \\(h_{ii}\\) is the weight of the observation \\(Y_i\\); so \\(h_{ii}\\) measures the role of the X values in determining how important \\(Y_i\\) is in affecting the \\(\\hat{Y}_i\\). Large \\(h_{ii}\\) implies \\(var(e_i)\\) is small, so larger \\(h_{ii}\\) implies that \\(\\hat{Y}_i\\) is close to \\(Y_i\\) small data sets: \\(h_{ii} &gt; .5\\) suggests “large”. large data sets: \\(h_{ii} &gt; \\frac{2p}{n}\\) is “large. Using the hat matrix to identify extrapolation: Let \\(\\mathbf{x_{new}}\\) be a vector containing the X values for which an inference about a mean response or a new observation is to be made. Let \\(\\mathbf{X}\\) be the data design matrix used to fit the data. Then, if \\(h_{new,new} = \\mathbf{x_{new}(X&#39;X)^{-1}x_{new}}\\) is within the range of leverage values (\\(h_{ii}\\)) for cases in the data set, no extrapolation is involved; otherwise; extrapolation is indicated. Identifying Influential Cases: by influential we mean that exclusion of an observation causes major changes int he fitted regression. (not all outliers are influential) Influence on Single Fitted Values: DFFITS Influence on All Fitted Values: Cook’s D Influence on the Regression Coefficients: DFBETAS 5.1.6.2.3 DFFITS Influence on Single Fitted Values: DFFITS \\[ \\begin{aligned} (DFFITS)_i &amp;= \\frac{\\hat{Y}_i - \\hat{Y}_{i(i)}}{\\sqrt{MSE_{(i)}h_{ii}}} \\\\ &amp;= t_i (\\frac{h_{ii}}{1-h_{ii}})^{1/2} \\end{aligned} \\] the standardized difference between the i-th fitted value with all observations and with the i-th case removed. studentized deleted residual multiplied by a factor that is a function fo the i-th leverage value. influence if: small to medium data sets: \\(|DFFITS|&gt;1\\) large data sets: \\(|DFFITS|&gt; 2 \\sqrt{p/n}\\) 5.1.6.2.4 Cook’s D Influence on All Fitted Values: Cook’s D \\[ \\begin{aligned} D_i &amp;= \\frac{\\sum_{j=1}^{n}(\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{p(MSE)} \\\\ &amp;= \\frac{e^2_i}{p(MSE)}(\\frac{h_{ii}}{(1-h_{ii})^2}) \\end{aligned} \\] gives the influence of i-th case on all fitted values. If \\(e_i\\) increases or \\(h_{ii}\\) increases, then \\(D_i\\) increases. \\(D_i\\) is a percentile of an \\(F_{(p,n-p)}\\) distribution. If the percentile is greater than \\(.5(50\\%)\\) then the \\(i\\)-th case has major influence. In practice, if \\(D_i &gt;4/n\\), then the \\(i\\)-th case has major influence. 5.1.6.2.5 DFBETAS Influence on the Regression Coefficients: DFBETAS \\[ (DFBETAS)_{k(i)} = \\frac{b_k - b_{k(i)}}{\\sqrt{MSE_{(i)}c_{kk}}} \\] for \\(k = 0,...,p-1\\) and \\(c_{kk}\\) is the k-th diagonal element of \\(\\mathbf{X&#39;X}^{-1}\\) Influence of the \\(i\\)-th case on each regression coefficient \\(b_k\\) \\((k=0,\\dots,p-1)\\) is the difference between the estimated regression coefficients based on all \\(n\\) cases and the regression coefficients obtained when the \\(i\\)-th case is omitted (\\(b_{k(i)}\\)) small data sets: \\(|DFBETA|&gt;1\\) large data sets: \\(|DFBETA| &gt; 2\\sqrt{n}\\) Sign of DFBETA inculcates whether inclusion of a case leads to an increase or a decrease in estimates of the regression coefficient. 5.1.6.3 Collinearity Multicollinearity refers to correlation among explanatory variables. large changes in the estimated regression coefficient when a predictor variable is added or deleted, or when an observation is altered or deleted. non insignificant results in individual tests on regression coefficients for important predictor variables. estimated regression coefficients with an algebraic sign that is the opposite of that expected from theoretical consideration or prior experience. large coefficients of simple correlation between pairs of predictor variables in the correlation matrix. wide confidence intervals for the regression coefficients representing important predictor variables. When some of \\(X\\) variables are so highly correlated that the inverse \\((X&#39;X)^{-1}\\) does not exist or is very computationally unstable. Correlated Predictor Variables: if some X variables are “perfectly” correlated, the system is undetermined and there are an infinite number of models that fit the data. That is, if \\(X&#39;X\\) is singular, then \\((X&#39;X)^{-1}\\) doesn’t exist. Then, parameters cannot be interpreted (\\(\\mathbf{b = (X&#39;X)^{-1}X&#39;y}\\)) sampling variability is infinite (\\(\\mathbf{s^2(b) = MSE (X&#39;X)^{-1}}\\)) 5.1.6.3.1 VIFs Let \\(R^2_k\\) be the coefficient of multiple determination when \\(X_k\\) is regressed on the \\(p - 2\\) other \\(X\\) variables in the model. Then, \\[ VIF_k = \\frac{1}{1-R^2_k} \\] large values indicate that a near collinearity is causing the variance of \\(b_k\\) to be inflated, \\(var(b_k) \\propto \\sigma^2 (VIF_k)\\) Typically, the rule of thumb is that \\(VIF &gt; 4\\) mean you should see why this is the case, and \\(VIF_k &gt; 10\\) indicates a serious problem collinearity problem that could result in poor parameters estimates. the mean of all VIF’s provide an estimate of the ratio of the true multicollinearity to a model where the \\(X\\) variables are uncorrelated serious multicollinearity if \\(avg(VIF) &gt;&gt;1\\) 5.1.6.3.2 Condition Number Condition Number spectral decomposition \\[ \\mathbf{X&#39;X}= \\sum_{i=1}^{p} \\lambda_i \\mathbf{u_i u_i&#39;} \\] where \\(\\lambda_i\\) is the eigenvalue and \\(\\mathbf{u}_i\\) is the eigenvector. \\(\\lambda_1 &gt; ...&gt;\\lambda_p\\) and the eigenvecotrs are orthogonal: \\[ \\begin{cases} \\mathbf{u_i&#39;u_j} = 0&amp;\\text{for $i \\neq j$}\\\\ 1&amp;\\text{for $i =j$}\\\\ \\end{cases} \\] The condition number is then \\[ k = \\sqrt{\\frac{\\lambda_{max}}{\\lambda_{min}}} \\] values \\(k&gt;30\\) are cause for concern values \\(30&lt;k&lt;100\\) imply moderate dependencies. values \\(k&gt;100\\) imply strong collinearity Condition index \\[ \\delta_i = \\sqrt{\\frac{\\lambda_{max}}{\\lambda_i}} \\] where \\(i = 1,...,p\\) we can find the proportion of the total variance associated with the k-th regression coefficient and the i-th eigen mode: \\[ \\frac{u_{ik}^2/\\lambda_i}{\\sum_j (u^2_{jk}/\\lambda_j)} \\] These variance proportions can be helpful for identifying serious collinearity the condition index must be large the variance proportions must be large (&gt;,5) for at least two regression coefficients. 5.1.6.4 Constancy of Error Variance 5.1.6.4.1 Brown-Forsythe Test (Modified Levene Test) Does not depend on normality Applicable when error variance increases or decreases with \\(X\\) relatively large sample size needed (so we can ignore dependency between residuals) Split residuals into 2 groups (\\(e_{i1}, i = 1, ..., n_1; e_{i2}, j=1,...,n_2\\)) Let \\(d_{i1}= |e_{i1}-\\tilde{e}_{1}|\\) where \\(\\tilde{e}_{1}\\) is the median of group 1. Let \\(d_{j2}=|e_{j2}-\\tilde{e}_{2}|\\). Then, a 2-sample t-test: \\[ t_L = \\frac{\\bar{d}_1 - \\bar{d}_2}{s\\sqrt{1/n_1+1/n_2}} \\] where \\[ s^2 = \\frac{\\sum_i(d_{i1}-\\bar{d}_1)^2+\\sum_j(d_{j2}-\\bar{d}_2)^2}{n-2} \\] If \\(|t_L|&gt;t_{1-\\alpha/2;n-2}\\) conclude the error variance is not constant. 5.1.6.4.2 Breusch-Pagan Test (Cook-Weisberg Test) Assume the error terms are independent and normally distributed, and \\[ \\sigma^2_i = \\gamma_0 + \\gamma_1 X_i \\] Constant error variance corresponds to \\(\\gamma_1 = 0\\), i.e., test \\(H_0: \\gamma_1 =0\\) \\(H_1: \\gamma_1 \\neq 0\\) by regressing the squared residuals on X in the usual manner. Obtain the regression sum of squares from this: \\(SSR^*\\) (the SSR from the regression of \\(e^2_i\\) on \\(X_i\\)). Then, define \\[ X^2_{BP} = \\frac{SSR^*/2}{(SSE/n)^2} \\] where SSE is the error sum of squares from the regression of Y on X. If \\(H_0: \\gamma_1 = 0\\) holds and n is reasonably large, \\(X^2_{BP}\\) follows approximately the \\(\\chi^2\\) distribution with 1 d.f. We reject \\(H_0\\) (Homogeneous variance) if \\(X^2_{BP} &gt; \\chi^2_{1-\\alpha;1}\\) 5.1.6.5 Independence 5.1.6.5.1 Plots 5.1.6.5.2 Durbin-Watson 5.1.6.5.3 Time-series 5.1.6.5.4 Spatial Statistics 5.1.7 Model Validation split data into 2 groups: training (model building) sample and validation (prediction) sample. the model MSE will tend to underestimate the inherent variability in making future predictions. to consider actual predictive ability, consider mean squared prediction error (MSPE): \\[ MSPE = \\frac{\\sum_{i=1}^{n} (Y_i- \\hat{Y}_i)^2}{n^*} \\] where \\(Y_i\\) is the known value of the response variable in the \\(i\\)-th validation case. \\(\\hat{Y}_i\\) is the predicted value based on a model fit with the training data set. \\(n^*\\) is the number of cases in the validation set. we want MSPE to be close to MSE (in which MSE is not biased); so look at the the ratio MSPE / MSE (closer to 1, the better). 5.1.8 Finite Sample Properties \\(n\\) is fixed Bias On average, how close is our estimate to the true value \\(Bias = E(\\hat{\\beta}) -\\beta\\) where \\(\\beta\\) is the true parameter value and \\(\\hat{\\beta}\\) is the estimator for \\(\\beta\\) An estimator is unbiased when \\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) or \\(E(\\hat{\\beta})=\\beta\\) means that the estimator will produce estimates that are, on average, equal to the value it it trying to estimate Distribution of an estimator: An estimator is a function of random variables (data) Standard Deviation: the spread of the estimator. OLS Under A1 A2 A3, OLS is unbiased \\[ \\begin{aligned} E(\\hat{\\beta}) &amp;= E(\\mathbf{(X&#39;X)^{-1}X&#39;y}) &amp;&amp; \\text{A2}\\\\ &amp;= E(\\mathbf{(X&#39;X)^{-1}X&#39;(X\\beta + \\epsilon)}) &amp;&amp; \\text{A1}\\\\ &amp;= E(\\mathbf{(X&#39;X)^{-1}X&#39;X\\beta + (X&#39;X)^{-1}X&#39;\\epsilon}) &amp;&amp; \\text{} \\\\ &amp;= E(\\beta + \\mathbf{(X&#39;X)^{-1}X&#39;\\epsilon}) \\\\ &amp;= \\beta + E(\\mathbf{(X&#39;X^{-1}\\epsilon)}) \\\\ &amp;= \\beta + E(E((\\mathbf{X&#39;X)^{-1}X&#39;\\epsilon|X})) &amp;&amp;\\text{LIE} \\\\ &amp;= \\beta + E((\\mathbf{X&#39;X)^{-1}X&#39;}E\\mathbf{(\\epsilon|X})) \\\\ &amp;= \\beta + E((\\mathbf{X&#39;X)^{-1}X&#39;}0)) &amp;&amp; \\text{A3} \\\\ &amp;= \\beta \\end{aligned} \\] where LIE stands for Law of Iterated Expectation If A3 does not hold, then OLS will be biased From Frisch-Waugh-Lovell Theorem, if we have the omitted variable \\(\\hat{\\beta}_2 \\neq 0\\) and \\(\\mathbf{X_1&#39;X_2} \\neq 0\\), then the omitted variable will cause OLS estimator to be biased. Under A1 A2 A3 A4, we have the conditional variance of the OLS estimator as follows] \\[ \\begin{aligned} Var(\\hat{\\beta}|\\mathbf{X}) &amp;= Var(\\beta + \\mathbf{(X&#39;X)^{-1}X&#39;\\epsilon|X}) &amp;&amp; \\text{A1-A2}\\\\ &amp;= Var((\\mathbf{X&#39;X)^{-1}X&#39;\\epsilon|X)} \\\\ &amp;= \\mathbf{X&#39;X^{-1}X&#39;} Var(\\epsilon|\\mathbf{X})\\mathbf{X(X&#39;X)^{-1}} \\\\ &amp;= \\mathbf{X&#39;X^{-1}X&#39;} \\sigma^2I \\mathbf{X(X&#39;X)^{-1}} &amp;&amp; \\text{A4} \\\\ &amp;= \\sigma^2\\mathbf{X&#39;X^{-1}X&#39;} I \\mathbf{X(X&#39;X)^{-1}} \\\\ &amp;= \\sigma^2\\mathbf{(X&#39;X)^{-1}} \\end{aligned} \\] Sources of variation \\(\\sigma^2=Var(\\epsilon_i|\\mathbf{X})\\) The amount of unexplained variation \\(\\epsilon_i\\) is large relative to the explained \\(\\mathbf{x_i \\beta}\\) variation “Small” \\(Var(x_{i1}), Var(x_{i1}),..\\) Not a lot of variation in \\(\\mathbf{X}\\) (no information) small sample size “Strong” correlation between the explanatory variables \\(x_{i1}\\) is highly correlated with a linear combination of 1, \\(x_{i2}\\), \\(x_{i3}\\), … include many irrelevant variables will contribute to this. If \\(x_1\\) is perfectly determined in the regression \\(\\rightarrow\\) Perfect Collinearity \\(\\rightarrow\\) A2 is violated. If \\(x_1\\) is highly correlated with a linear combination of other variables, then we have Multicollinearity 5.1.8.1 Check for Multicollinearity Variance Inflation Factor (VIF) Rule of thumb \\(VIF \\ge 10\\) is large \\[ VIF = \\frac{1}{1-R_1^2} \\] Multicollinearity and VIF: High VIFs with indicator variables are normal and not problematic. VIF is generally not useful for detecting multicollinearity concerns in models with fixed effects. Overemphasis on Multicollinearity: Multicollinearity inflates standard errors and widens confidence intervals but does not bias results. If key variables have narrow confidence intervals, multicollinearity is not an issue. Goldberger’s Insight (Goldberger 1991): Multicollinearity is akin to small sample size (“micronumerosity”). Large standard errors are expected with highly correlated independent variables. Practical Implications: Evaluate whether confidence intervals for key variables are sufficiently narrow. If not, the study is inconclusive, and a larger dataset or redesigned study is needed. 5.1.8.2 Standard Errors \\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X&#39;X)^{-1}}\\) is the variance of the estimate \\(\\hat{\\beta}\\) Standard Errors are estimators/estimates of the standard deviation (square root of the variance) of the estimator \\(\\hat{\\beta}\\) Under A1-A5, then we can estimate \\(\\sigma^2=Var(\\epsilon^2|\\mathbf{X})\\) the standard errors as \\[ \\begin{aligned} s^2 &amp;= \\frac{1}{n-k}\\sum_{i=1}^{n}e_i^2 \\\\ &amp;= \\frac{1}{n-k}SSR \\end{aligned} \\] degrees of freedom adjustment: because \\(e_i \\neq \\epsilon_i\\) and are estimated using k estimates for \\(\\beta\\), we lose degrees of freedom in our variance estimate. \\(s=\\sqrt{s^2}\\) is a biased estimator for the standard deviation (Jensen’s Inequality) Standard Errors for \\(\\hat{\\beta}\\) \\[ \\begin{aligned} SE(\\hat{\\beta}_{j-1})&amp;=s\\sqrt{[(\\mathbf{X&#39;X})^{-1}]_{jj}} \\\\ &amp;= \\frac{s}{\\sqrt{SST_{j-1}(1-R_{j-1}^2)}} \\end{aligned} \\] where \\(SST_{j-1}\\) and \\(R_{j-1}^2\\) from the following regression \\(x_{j-1}\\) on 1, \\(x_1\\),… \\(x_{j-2}\\),\\(x_j\\),\\(x_{j+1}\\), …, \\(x_{k-1}\\) Summary of Finite Sample Properties Under A1-A3: OLS is unbiased Under A1-A4: The variance of the OLS estimator is \\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X&#39;X)^{-1}}\\) Under A1-A4, A6: OLS estimator \\(\\hat{\\beta} \\sim N(\\beta,\\sigma^2\\mathbf{(X&#39;X)^{-1}})\\) Under A1-A4, Gauss-Markov Theorem holds \\(\\rightarrow\\) OLS is BLUE Under A1-A5, the above standard errors are unbiased estimator of standard deviation for \\(\\hat{\\beta}\\) 5.1.9 Large Sample Properties Let \\(n \\rightarrow \\infty\\) A perspective that allows us to evaluate the “quality” of estimators when finite sample properties are not informative, or impossible to compute Consistency, asymptotic distribution, asymptotic variance Motivation Finite Sample Properties need strong assumption A1 A3 A4 A6 Other estimation such as GLS, MLE need to be analyzed using Large Sample Properties Let \\(\\mu(\\mathbf{X})=E(y|\\mathbf{X})\\) be the Conditional Expectation Function \\(\\mu(\\mathbf{X})\\) is the minimum mean squared predictor (over all possible functions) \\[ minE((y-f(\\mathbf{X}))^2) \\] under A1 and A3, \\[ \\mu(\\mathbf{X})=\\mathbf{X}\\beta \\] Then the linear projection \\[ L(y|1,\\mathbf{X})=\\gamma_0 + \\mathbf{X}Var(X)^{-1}Cov(X,Y) \\] where \\(\\mathbf{X}Var(X)^{-1}Cov(X,Y)=\\gamma\\) is the minimum mean squared linear approximation to be conditional mean function \\[ (\\gamma_0,\\gamma) = arg min E((E(y|\\mathbf{X})-(a+\\mathbf{Xb})^2) \\] OLS is always consistent for the linear projection, but not necessarily unbiased. Linear projection has no causal interpretation Linear projection does not depend on assumption A1 and A3 Evaluating an estimator using large sample properties: Consistency: measure of centrality Limiting Distribution: the shape of the scaled estimator as the sample size increases Asymptotic variance: spread of the estimator with regards to its limiting distribution. An estimator \\(\\hat{\\theta}\\) is consistent for \\(\\theta\\) if \\(\\hat{\\theta}_n \\to^p \\theta\\) As n increases, the estimator converges to the population parameter value. Unbiased does not imply consistency and consistency does not imply unbiased. Based on [Weak Law] of Large Numbers \\[ \\begin{aligned} \\hat{\\beta} &amp;= \\mathbf{(X&#39;X)^{-1}X&#39;y} \\\\ &amp;= \\mathbf{(\\sum_{i=1}^{n}x_i&#39;x_i)^{-1} \\sum_{i=1}^{n}x_i&#39;y_i} \\\\ &amp;= (n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}} n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i} \\end{aligned} \\] \\[ \\begin{aligned} plim(\\hat{\\beta}) &amp;= plim((n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}} n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i}) \\\\ &amp;= plim((n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i}) \\\\ &amp;= (plim(n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i}) \\text{ due to A2, A5} \\\\ &amp;= E(\\mathbf{x_i&#39;x_i})^{-1}E(\\mathbf{x_i&#39;y_i}) \\end{aligned} \\] \\[ E(\\mathbf{x_i&#39;x_i})^{-1}E(\\mathbf{x_i&#39;y_i}) = \\beta + E(\\mathbf{x_i&#39;x_i})^{-1}E(\\mathbf{x_i&#39;\\epsilon_i}) \\] Under A1, A2, A3a, A5 OLS is consistent, but not guarantee unbiased. Under A1, A2, A3a, A5, and \\(\\mathbf{x_i&#39;x_i}\\) has finite first and second moments (CLT), \\(Var(\\mathbf{x_i&#39;}\\epsilon_i)=\\mathbf{B}\\) \\((n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;x_i})^{-1} \\to^p (E(\\mathbf{x&#39;_ix_i}))^{-1}\\) \\(\\sqrt{n}(n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;}\\epsilon_i) \\to^d N(0,\\mathbf{B})\\) \\[ \\sqrt{n}(\\hat{\\beta}-\\beta) = (n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;x_i})^{-1}\\sqrt{n}(n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;x_i}) \\to^{d} N(0,\\Sigma) \\] where \\(\\Sigma=(E(\\mathbf{x_i&#39;x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i&#39;x_i}))^{-1}\\) holds under A3a Do not need A4 and A6 to apply CLT If A4 does not hold, then \\(\\mathbf{B}=Var(\\mathbf{x_i&#39;}\\epsilon_i)=\\sigma^2E(x_i&#39;x_i)\\) which means \\(\\Sigma=\\sigma^2(E(\\mathbf{x_i&#39;x_i}))^{-1}\\), use standard errors Heteroskedasticity can be from Limited dependent variable Dependent variables with large/skewed ranges Solving Asymptotic Variance \\[ \\begin{aligned} \\Sigma &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i&#39;x_i}))^{-1} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}Var(\\mathbf{x_i&#39;}\\epsilon_i)(E(\\mathbf{x_i&#39;x_i}))^{-1} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}E[(\\mathbf{x_i&#39;}\\epsilon_i-0)(\\mathbf{x_i&#39;}\\epsilon_i-0)](E(\\mathbf{x_i&#39;x_i}))^{-1} &amp; \\text{A3a} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}E[E(\\mathbf{\\epsilon_i^2|x_i)x_i&#39;x_i]}(E(\\mathbf{x_i&#39;x_i}))^{-1} &amp; \\text{LIE} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}\\sigma^2E(\\mathbf{x_i&#39;x_i})(E(\\mathbf{x_i&#39;x_i}))^{-1} &amp; \\text{A4} \\\\ &amp;= \\sigma^2(E(\\mathbf{x_i&#39;x_i})) \\end{aligned} \\] Under A1, A2, A3a, A4, A5: \\[ \\sqrt{n}(\\hat{\\beta}-\\beta) \\to^d N(0,\\sigma^2(E(\\mathbf{x_i&#39;x_i}))^{-1}) \\] The Asymptotic variance is approximation for the variance in the scaled random variable for \\(\\sqrt{n}(\\hat{\\beta}-\\beta)\\) when n is large. use \\(Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n\\) as an approximation for finite sample variance for large n: \\[ \\begin{aligned} Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)) &amp;\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\\\ Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &amp;\\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta))/n = Var(\\hat{\\beta}) \\end{aligned} \\] \\(Avar(.)\\) does not behave the same way as \\(Var(.)\\) \\[ \\begin{aligned} Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n &amp;\\neq Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)/\\sqrt{n}) \\\\ &amp;\\neq Avar(\\hat{\\beta}) \\end{aligned} \\] In Finite Sample Properties, we calculate standard errors as an estimate for the conditional standard deviation: \\[ SE_{fs}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Var}}(\\hat{\\beta}_{j-1}|\\mathbf{X}) = \\sqrt{s^2[\\mathbf{(X&#39;X)}^{-1}]_{jj}} \\] In Large Sample Properties, we calculate standard errors as an estimate for the square root of asymptotic variance \\[ SE_{ls}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Avar}(\\sqrt{n}\\hat{\\beta}_{j-1})/n} = \\sqrt{s^2[\\mathbf{(X&#39;X)}^{-1}]_{jj}} \\] Hence, the standard error estimator is the same for finite sample and large sample. Same estimator, but conceptually estimating two different things. Valid under weaker assumptions: the assumptions needed to produce a consistent estimator for the finite sample conditional variance (A1-A5) are stronger than those needed to produce a consistent estimator for the asymptotic variance (A1,A2,A3a,A4,A5) Suppose that \\(y_1,...,y_n\\) are a random sample from some population with mean \\(\\mu\\) and variance-covariance matrix \\(\\Sigma\\) \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\) is a consistent estimator for \\(\\mu\\) \\(S = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i -\\bar{y})(y_i-\\bar{y})&#39;\\) is a consistent estimator for \\(\\Sigma\\). Multivariate Central limit Theorem: Similar to the univariate case, \\(\\sqrt{n}(\\bar{y}-\\mu) \\sim N_p(0,\\Sigma)\\), when \\(n\\) is large relative to p (e.g., \\(n \\ge 25p\\)). Equivalently, \\(\\bar{y} \\sim N_p(\\mu,\\Sigma/n)\\). Wald’s Theorem: \\(n(\\bar{y} - \\mu)&#39;S^{-1}(\\bar{y}-\\mu) \\sim \\chi^2_{(p)}\\) when \\(n\\) is large relative to \\(p\\). References "],["feasible-generalized-least-squares.html", "5.2 Feasible Generalized Least Squares", " 5.2 Feasible Generalized Least Squares Motivation for a more efficient estimator Gauss-Markov Theorem holds under A1-A4 A4: \\(Var(\\epsilon| \\mathbf{X} )=\\sigma^2I_n\\) Heteroskedasticity: \\(Var(\\epsilon_i|\\mathbf{X}) \\neq \\sigma^2I_n\\) Serial Correlation: \\(Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) \\neq 0\\) Without A4, how can we know which unbiased estimator is the most efficient? Original (unweighted) model: \\[ \\mathbf{y=X\\beta+ \\epsilon} \\] Suppose A1-A3 hold, but A4 does not hold, \\[ \\mathbf{Var(\\epsilon|X)=\\Omega \\neq \\sigma^2 I_n} \\] We will try to use OLS to estimate the transformed (weighted) model \\[ \\mathbf{wy=wX\\beta + w\\epsilon} \\] We need to choose \\(\\mathbf{w}\\) so that \\[ \\mathbf{w&#39;w = \\Omega^{-1}} \\] then \\(\\mathbf{w}\\) (full-rank matrix) is the Cholesky decomposition of \\(\\mathbf{\\Omega^{-1}}\\) (full-rank matrix) In other words, \\(\\mathbf{w}\\) is the squared root of \\(\\Omega\\) (squared root version in matrix) \\[ \\begin{aligned} \\Omega &amp;= var(\\epsilon | X) \\\\ \\Omega^{-1} &amp;= var(\\epsilon | X)^{-1} \\end{aligned} \\] Then, the transformed equation (IGLS) will have the following properties. \\[ \\begin{aligned} \\mathbf{\\hat{\\beta}_{IGLS}} &amp;= \\mathbf{(X&#39;w&#39;wX)^{-1}X&#39;w&#39;wy} \\\\ &amp; = \\mathbf{(X&#39;\\Omega^{-1}X)^{-1}X&#39;\\Omega^{-1}y} \\\\ &amp; = \\mathbf{\\beta + X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon} \\end{aligned} \\] Since A1-A3 hold for the unweighted model \\[ \\begin{aligned} \\mathbf{E(\\hat{\\beta}_{IGLS}|X)} &amp; = E(\\mathbf{\\beta + (X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon)}|X)\\\\ &amp; = \\mathbf{\\beta + E(X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon)|X)} \\\\ &amp; = \\mathbf{\\beta + X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}E(\\epsilon|X)} &amp;&amp; \\text{since A3}: E(\\epsilon|X)=0 \\\\ &amp; = \\mathbf{\\beta} \\end{aligned} \\] \\(\\rightarrow\\) IGLS estimator is unbiased \\[ \\begin{aligned} \\mathbf{Var(w\\epsilon|X)} &amp;= \\mathbf{wVar(\\epsilon|X)w&#39;} \\\\ &amp; = \\mathbf{w\\Omega w&#39;} \\\\ &amp; = \\mathbf{w(w&#39;w)^{-1}w&#39;} &amp;&amp; \\text{since w is a full-rank matrix}\\\\ &amp; = \\mathbf{ww^{-1}(w&#39;)^{-1}w&#39;} \\\\ &amp; = \\mathbf{I_n} \\end{aligned} \\] \\(\\rightarrow\\) A4 holds for the transformed (weighted) equation Then, the variance for the estimator is \\[ \\begin{aligned} Var(\\hat{\\beta}_{IGLS}|\\mathbf{X}) &amp; = \\mathbf{Var(\\beta + (X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1}\\epsilon|X)} \\\\ &amp;= \\mathbf{Var((X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1}\\epsilon|X)} \\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1} Var(\\epsilon|X) \\Omega^{-1}X(X&#39;\\Omega ^{-1}X)^{-1}} &amp;&amp; \\text{because A4 holds}\\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1} \\Omega \\Omega^{-1} \\Omega^{-1}X(X&#39;\\Omega ^{-1}X)^{-1}} \\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}} \\end{aligned} \\] Let \\(A = \\mathbf{(X&#39;X)^{-1}X&#39;-(X&#39;\\Omega ^{-1} X)X&#39; \\Omega^{-1}}\\) then \\[ Var(\\hat{\\beta}_{OLS}|X)- Var(\\hat{\\beta}_{IGLS}|X) = A\\Omega A&#39; \\] And \\(\\Omega\\) is Positive Semi Definite, then \\(A\\Omega A&#39;\\) also PSD, then IGLS is more efficient The name Infeasible comes from the fact that it is impossible to compute this estimator. \\[ \\mathbf{w} = \\left( \\begin{array}{ccccc} w_{11} &amp; 0 &amp; 0 &amp; ... &amp; 0 \\\\ w_{21} &amp; w_{22} &amp; 0 &amp; ... &amp; 0 \\\\ w_{31} &amp; w_{32} &amp; w_{33} &amp; ... &amp; ... \\\\ w_{n1} &amp; w_{n2} &amp; w_{n3} &amp; ... &amp; w_{nn} \\\\ \\end{array} \\right) \\] With \\(n(n+1)/2\\) number of elements and n observations \\(\\rightarrow\\) infeasible to estimate. (number of equation &gt; data) Hence, we need to make assumption on \\(\\Omega\\) to make it feasible to estimate \\(\\mathbf{w}\\): Heteroskedasticity : multiplicative exponential model AR(1) Cluster 5.2.1 Heteroskedasticity \\[\\begin{equation} \\begin{aligned} Var(\\epsilon_i |x_i) &amp; = E(\\epsilon^2|x_i) \\neq \\sigma^2 \\\\ &amp; = h(x_i) = \\sigma_i^2 \\text{(variance of the error term is a function of x)} \\end{aligned} \\tag{5.7} \\end{equation}\\] For our model, \\[ \\begin{aligned} y_i &amp;= x_i\\beta + \\epsilon_i \\\\ (1/\\sigma_i)y_i &amp;= (1/\\sigma_i)x_i\\beta + (1/\\sigma_i)\\epsilon_i \\end{aligned} \\] then, from (5.7) \\[ \\begin{aligned} Var((1/\\sigma_i)\\epsilon_i|X) &amp;= (1/\\sigma_i^2) Var(\\epsilon_i|X) \\\\ &amp;= (1/\\sigma_i^2)\\sigma_i^2 \\\\ &amp;= 1 \\end{aligned} \\] then the weight matrix \\(\\mathbf{w}\\) in the matrix equation \\[ \\mathbf{wy=wX\\beta + w\\epsilon} \\] \\[ \\mathbf{w}= \\left( \\begin{array}{ccccc} 1/\\sigma_1 &amp; 0 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; 1/\\sigma_2 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; 0 &amp; 1/\\sigma_3 &amp; ... &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; 0 \\\\ 0 &amp; 0 &amp; . &amp; . &amp; 1/\\sigma_n \\end{array} \\right) \\] Infeasible Weighted Least Squares Assume we know \\(\\sigma_i^2\\) (Infeasible) The IWLS estimator is obtained as the least squared estimated for the following weighted equation \\[ (1/\\sigma_i)y_i = (1/\\sigma_i)\\mathbf{x}_i\\beta + (1/\\sigma_i)\\epsilon_i \\] Usual standard errors for the weighted equation are valid if \\(Var(\\epsilon | \\mathbf{X}) = \\sigma_i^2\\) If \\(Var(\\epsilon | \\mathbf{X}) \\neq \\sigma_i^2\\) then heteroskedastic robust standard errors are valid. Problem: We do not know \\(\\sigma_i^2=Var(\\epsilon_i|\\mathbf{x_i})=E(\\epsilon_i^2|\\mathbf{x}_i)\\) One observation \\(\\epsilon_i\\) cannot estimate a sample variance estimate \\(\\sigma_i^2\\) Model \\(\\epsilon_i^2\\) as reasonable (strictly positive) function of \\(x_i\\) and independent error \\(v_i\\) (strictly positive) \\[ \\epsilon_i^2=v_i exp(\\mathbf{x_i\\gamma}) \\] Then we can apply a log transformation to recover a linear in parameters model, \\[ ln(\\epsilon_i^2) = \\mathbf{x_i\\gamma} + ln(v_i) \\] where \\(ln(v_i)\\) is independent \\(\\mathbf{x}_i\\) We do not observe \\(\\epsilon_i\\) * OLS residual (\\(e_i\\)) as an approximate 5.2.2 Serial Correlation \\[ Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0 \\] Under covariance stationary, \\[ Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) = Cov(\\epsilon_i, \\epsilon_{i+h}|\\mathbf{x_i,x_{i+h}})=\\gamma_h \\] And the variance covariance matrix is \\[ Var(\\epsilon|\\mathbf{X}) = \\Omega = \\left( \\begin{array}{ccccc} \\sigma^2 &amp; \\gamma_1 &amp; \\gamma_2 &amp; ... &amp; \\gamma_{n-1} \\\\ \\gamma_1 &amp; \\sigma^2 &amp; \\gamma_1 &amp; ... &amp; \\gamma_{n-2} \\\\ \\gamma_2 &amp; \\gamma_1 &amp; \\sigma^2 &amp; ... &amp; ... \\\\ . &amp; . &amp; . &amp; . &amp; \\gamma_1 \\\\ \\gamma_{n-1} &amp; \\gamma_{n-2} &amp; . &amp; \\gamma_1 &amp; \\sigma^2 \\end{array} \\right) \\] There n parameters to estimate - need some sort fo structure to reduce number of parameters to estimate. Time Series Effect of inflation and deficit on Treasury Bill interest rates Cross-sectional Clustering 5.2.2.1 AR(1) \\[ \\begin{aligned} y_t &amp;= \\beta_0 + x_t\\beta_1 + \\epsilon_t \\\\ \\epsilon_t &amp;= \\rho \\epsilon_{t-1} + u_t \\end{aligned} \\] and the variance covariance matrix is \\[ Var(\\epsilon | \\mathbf{X})= \\frac{\\sigma^2_u}{1-\\rho} \\left( \\begin{array}{ccccc} 1 &amp; \\rho &amp; \\rho^2 &amp; ... &amp; \\rho^{n-1} \\\\ \\rho &amp; 1 &amp; \\rho &amp; ... &amp; \\rho^{n-2} \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; . &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; \\rho \\\\ \\rho^{n-1} &amp; \\rho^{n-2} &amp; . &amp; \\rho &amp; 1 \\\\ \\end{array} \\right) \\] Hence, there is only 1 parameter to estimate: \\(\\rho\\) Under A1, A2, A3a, A5a, OLS is consistent and asymptotically normal Use Newey West Standard Errors for valid inference. Apply Infeasible Cochrane Orcutt (as if we knew \\(\\rho\\)) Because \\[ u_t = \\epsilon_t - \\rho \\epsilon_{t-1} \\] satisfies A3, A4, A5 we’d like to to transform the above equation to one that has \\(u_t\\) as the error. \\[ \\begin{aligned} y_t - \\rho y_{t-1} &amp;= (\\beta_0 + x\\beta_1 + \\epsilon_t) - \\rho (\\beta_0 + x_{t-1}\\beta_1 + \\epsilon_{t-1}) \\\\ &amp; = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t \\end{aligned} \\] 5.2.2.1.1 Infeasible Cochrane Orcutt Assume that we know \\(\\rho\\) (Infeasible) The ICO estimator is obtained as the least squared estimated for the following weighted first difference equation \\[ y_t -\\rho y_{t-1} = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t \\] Usual standard errors for the weighted first difference equation are valid if the errors truly follow an AR(1) process If the serial correlation is generated from a more complex dynamic process then Newey-West HAC standard errors are valid Problem We do not know \\(\\rho\\) \\(\\rho\\) is the correlation between \\(\\epsilon_t\\) and \\(\\epsilon_{t-1}\\): estimate using OLS residuals (\\(e_i\\)) as proxy \\[ \\hat{\\rho} = \\frac{\\sum_{t=1}^{T}e_te_{t-1}}{\\sum_{t=1}^{T}e_t^2} \\] which can be obtained from the OLS regression of \\[ e_t = \\rho e_{t-1} + u_t \\] where we suppress the intercept. We are losing an observation By taking the first difference we are dropping the first observation \\[ y_1 = \\beta_0 + x_1 \\beta_1 + \\epsilon_1 \\] Feasiable Prais Winsten Transformation applies the Infeasible Cochrane Orcutt but includes a weighted version of the first observation \\[ (\\sqrt{1-\\rho^2})y_1 = \\beta_0 + (\\sqrt{1-\\rho^2})x_1 \\beta_1 + (\\sqrt{1-\\rho^2}) \\epsilon_1 \\] 5.2.2.2 Cluster \\[ y_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi} \\] \\[ Cov(\\epsilon_{gi}, \\epsilon_{hj}) \\begin{cases} = 0 &amp; \\text{for $g \\neq h$ and any pair (i,j)} \\\\ \\neq 0 &amp; \\text{for any (i,j) pair}\\\\ \\end{cases} \\] Intra-group Correlation Each individual in a single group may be correlated but independent across groups. A4 is violated. usual standard errors for OLS are valid. Use cluster robust standard errors for OLS. Suppose there are 3 groups with different n \\[ Var(\\epsilon| \\mathbf{X})= \\Omega = \\left( \\begin{array}{cccccc} \\sigma^2 &amp; \\delta_{12}^1 &amp; \\delta_{13}^1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\delta_{12}^1 &amp; \\sigma^2 &amp; \\delta_{23}^1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\delta_{13}^1 &amp; \\delta_{23}^1 &amp; \\sigma^2 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2 &amp; \\delta_{12}^2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\delta_{12}^2 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2 \\end{array} \\right) \\] where \\(Cov(\\epsilon_{gi}, \\epsilon_{gj}) = \\delta_{ij}^g\\) and \\(Cov(\\epsilon_{gi}, \\epsilon_{hj}) = 0\\) for any i and j Infeasible Generalized Least Squares (Cluster) Assume that \\(\\sigma^2\\) and \\(\\delta_{ij}^g\\) are known, plug into \\(\\Omega\\) and solve for the inverse \\(\\Omega^{-1}\\) (infeasible) The Infeasible Generalized Least Squares Estimator is \\[ \\hat{\\beta}_{IGLS} = \\mathbf{(X&#39;\\Omega^{-1}X)^{-1}X&#39;\\Omega^{-1}y} \\] Problem * We do not know \\(\\sigma^2\\) and \\(\\delta_{ij}^g\\) + Can make assumptions about data generating process that is causing the clustering behavior. - Will give structure to \\(Cov(\\epsilon_{gi},\\epsilon_{gj})= \\delta_{ij}^g\\) which makes it feasible to estimate - if the assumptions are wrong then we should use cluster robust standard errors. Solution Assume group level random effects specification in the error \\[ \\begin{aligned} y_{gi} &amp;= \\mathbf{g}_i \\beta + c_g + u_{gi} \\\\ Var(c_g|\\mathbf{x}_i) &amp;= \\sigma^2_c \\\\ Var(u_{gi}|\\mathbf{x}_i) &amp;= \\sigma^2_u \\end{aligned} \\] where \\(c_g\\) and \\(u_{gi}\\) are independent of each other, and mean independent of \\(\\mathbf{x}_i\\) \\(c_g\\) captures the common group shocks (independent across groups) \\(u_{gi}\\) captures the individual shocks (independent across individuals and groups) Then the error variance is \\[ Var(\\epsilon| \\mathbf{X})= \\Omega = \\left( \\begin{array}{cccccc} \\sigma^2_c + \\sigma^2_u &amp; \\sigma^2_c &amp; \\sigma^2_c &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma^2_c &amp; \\sigma^2 + \\sigma^2_u &amp; \\sigma^2_c &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma^2_c &amp; \\sigma^2_c &amp; \\sigma^2+ \\sigma^2_u &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2+ \\sigma^2_u &amp; \\sigma^2_c &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2_c &amp; \\sigma^2+ \\sigma^2_u &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2+ \\sigma^2_u \\end{array} \\right) \\] Use Feasible group level Random Effects "],["weighted-least-squares.html", "5.3 Weighted Least Squares", " 5.3 Weighted Least Squares Estimate the following equation using OLS \\[ y_i = \\mathbf{x}_i \\beta + \\epsilon_i \\] and obtain the residuals \\(e_i=y_i -\\mathbf{x}_i \\hat{\\beta}\\) Transform the residual and estimate the following by OLS, \\[ ln(e_i^2)= \\mathbf{x}_i\\gamma + ln(v_i) \\] and obtain the predicted values \\(g_i=\\mathbf{x}_i \\hat{\\gamma}\\) The weights will be the untransformed predicted outcome, \\[ \\hat{\\sigma}_i =\\sqrt{exp(g_i)} \\] The FWLS (Feasible WLS) estimator is obtained as the least squared estimated for the following weighted equation \\[ (1/\\hat{\\sigma}_i)y_i = (1/\\hat{\\sigma}_i) \\mathbf{x}_i\\beta + (1/\\hat{\\sigma}_i)\\epsilon_i \\] Properties of the FWLS The infeasible WLS estimator is unbiased under A1-A3 for the unweighted equation. The FWLS estimator is NOT an unbiased estimator. The FWLS estimator is consistent under A1, A2, (for the unweighted equation), A5, and \\(E(\\mathbf{x}_i&#39;\\epsilon_i/\\sigma^2_i)=0\\) A3a is not sufficient for the above equation A3 is sufficient for the above equation. The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity. If the errors are truly multiplicative exponential heteroskedasticity, then usual standard errors are valid If we believe that there may be some mis-specification with the multiplicative exponential model, then we should report heteroskedastic robust standard errors. "],["generalized-least-squares.html", "5.4 Generalized Least Squares", " 5.4 Generalized Least Squares Consider \\[ \\mathbf{y = X\\beta + \\epsilon} \\] where, \\[ var(\\epsilon) = \\mathbf{G} = \\left( \\begin{array} {cccc} g_{11} &amp; g_{12} &amp; ... &amp; g_{1n} \\\\ g_{21} &amp; g_{22} &amp; ... &amp; g_{2n} \\\\ . &amp; . &amp; . &amp; . \\\\ g_{n1} &amp; . &amp; . &amp; g_{nn}\\\\ \\end{array} \\right) \\] The variances are heterogeneous, and the errors are correlated. \\[ \\mathbf{\\hat{b}_G = (X&#39;G^{-1}X)^{-1}X&#39;G^{-1}Y} \\] if we know G, we can estimate \\(\\mathbf{b}\\) just like OLS. However, we do not know G. Hence, we model the structure of G. "],["feasiable-prais-winsten.html", "5.5 Feasible Prais Winsten", " 5.5 Feasible Prais Winsten Weighting Matrix \\[ \\mathbf{w} = \\left( \\begin{array}{ccccc} \\sqrt{1- \\hat{\\rho}^2} &amp; 0 &amp; 0 &amp;... &amp; 0 \\\\ -\\hat{\\rho} &amp; 1 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; -\\hat{\\rho} &amp; 1 &amp; &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; 0 \\\\ 0 &amp; . &amp; 0 &amp; -\\hat{\\rho} &amp; 1 \\end{array} \\right) \\] Estimate the following equation using OLS \\[ y_t = \\mathbf{x}_t \\beta + \\epsilon_t \\] and obtain the residuals \\(e_t = y_t - \\mathbf{x}_t \\hat{\\beta}\\) Estimate the correlation coefficient for the AR(1) process by estimating the following by OLS (without no intercept) \\[ e_t = \\rho e_{t-1} + u_t \\] Transform the outcome and independent variables \\(\\mathbf{wy}\\) and \\(\\mathbf{wX}\\) respectively (weight matrix as stated). The FPW estimator is obtained as the least squared estimated for the following weighted equation \\[ \\mathbf{wy = wX\\beta + w\\epsilon} \\] Properties of FeasiblePrais Winsten Estimator The Infeasible PW estimator is under A1-A3 for the unweighted equation The FPW estimator is biased The FPW is consistent under A1 A2 A5 and \\[ E((\\mathbf{x_t - \\rho x_{t-1}})&#39;)(\\epsilon_t - \\rho \\epsilon_{t-1})=0 \\] A3a is not sufficient for the above equation A3 is sufficient for the above equation The FPW estimator is asymptotically more efficient than OLS if the errors are truly generated as AR(1) process If the errors are truly generated as AR(1) process then usual standard errors are valid If we are concerned that there may be a more complex dependence structure of heteroskedasticity, then we use Newey West Standard Errors "],["feasible-group-level-random-effects.html", "5.6 Feasible group level Random Effects", " 5.6 Feasible group level Random Effects Estimate the following equation using OLS \\[ y_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi} \\] and obtain the residuals \\(e_{gi} = y_{gi} - \\mathbf{x}_{gi}\\hat{\\beta}\\) 2. Estimate the variance using the usual $s^2 estimator \\[ s^2 = \\frac{1}{n-k}\\sum_{i=1}^{n}e_i^2 \\] as an estimator for \\(\\sigma^2_c + \\sigma^2_u\\) and estimate the within group correlation, \\[ \\hat{\\sigma}^2_c = \\frac{1}{G} \\sum_{g=1}^{G} (\\frac{1}{\\sum_{i=1}^{n_g-1}i}\\sum_{i\\neq j}\\sum_{j}^{n_g}e_{gi}e_{gj}) \\] and plug in the estimates to obtain \\(\\hat{\\Omega}\\) The feasible group level RE estimator is obtained as \\[ \\hat{\\beta}= \\mathbf{(X&#39;\\hat{\\Omega}^{-1}X)^{-1}X&#39;\\hat{\\Omega}^{-1}y} \\] Properties of the Feasible group level Random Effects Estimator The infeasible group RE estimator is a linear estimator and is unbiased under A1-A3 for the unweighted equation A3 requires \\(E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\) so we generally assume \\(E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\). The assumption \\(E(c_{g}|\\mathbf{x}_i)=0\\) is generally called random effects assumption The Feasible group level Random Effects is biased The Feasible group level Random Effects is consistent under A1-A3a, and A5a for the unweighted equation. A3a requires \\(E(\\mathbf{x}_i&#39;\\epsilon_{gi}) = E(\\mathbf{x}_i&#39;c_{g})+ (\\mathbf{x}_i&#39;u_{gi})=0\\) The Feasible group level Random Effects estimator is asymptotically more efficient than OLS if the errors follow the random effects specification If the errors do follow the random effects specification than the usual standard errors are consistent If there might be a more complex dependence structure or heteroskedasticity, then we need cluster robust standard errors. "],["ridge-regression.html", "5.7 Ridge Regression", " 5.7 Ridge Regression When we have the Collinearity problem, we could use the Ridge regression. The main problem with multicollinearity is that \\(\\mathbf{X&#39;X}\\) is “ill-conditioned”. The idea for ridge regression: adding a constant to the diagonal of \\(\\mathbf{X&#39;X}\\) improves the conditioning \\[ \\mathbf{X&#39;X} + c\\mathbf{I} (c&gt;0) \\] The choice of c is hard. The estimator \\[ \\mathbf{b}^R = (\\mathbf{X&#39;X}+c\\mathbf{I})^{-1}\\mathbf{X&#39;y} \\] is biased. It has smaller variance than the OLS estimator; as c increases, the bias increases but the variance decreases. Always exists some value of c for which the ridge regression estimator has a smaller total MSE than the OLS The optimal c varies with application and data set. To find the “optimal” \\(c\\) we could use “ridge trace”. We plot the values of the \\(p - 1\\) parameter estimates for different values of c, simultaneously. Typically, as c increases toward 1 the coefficients decreases to 0. The values of the VIF tend to decrease rapidly as c gets bigger than 0. The VIF values begin to change slowly as \\(c \\to 1\\). Then we can examine the ridge trace and VIF values and chooses the smallest value of c where the regression coefficients first become stable in the ridge trace and the VIF values have become sufficiently small (which is very subjective). Typically, this procedure is applied to the standardized regression model. "],["principal-component-regression.html", "5.8 Principal Component Regression", " 5.8 Principal Component Regression This also addresses the problem of multicollinearity "],["robust-regression.html", "5.9 Robust Regression", " 5.9 Robust Regression To address the problem of influential cases. Can be used when a known functional form is to be fitted, and when the errors are not normal due to a few outlying cases. 5.9.1 Least Absolute Residuals (LAR) Regression also known as minimum \\(L_1\\)-norm regression. \\[ L_1 = \\sum_{i=1}^{n}|Y_i - (\\beta_0 + \\beta_1 X_{i1} + .. + \\beta_{p-1}X_{i,p-1}) \\] which is not sensitive to outliers and inadequacies of the model specification. 5.9.2 Least Median of Squares (LMS) Regression \\[ median\\{[Y_i - (\\beta_0 - \\beta_1X_{i1} + ... + \\beta_{p-1}X_{i,p-1})]^2 \\} \\] 5.9.3 Iteratively Reweighted Least Squares (IRLS) Robust Regression uses Weighted Least Squares to lessen the influence of outliers. the weights \\(w_i\\) are inversely proportional to how far an outlying case is (e.g., based on the residual) the weights are revised iteratively until a robust fit Process: Step 1: Choose a weight function for weighting the cases. Step 2: obtain starting weights for all cases. Step 3: Use the starting weights in WLS and obtain the residuals from the fitted regression function. Step 4: use the residuals in Step 3 to obtain revised weights. Step 5: continue until convergence. Note: If you don’t know the form of the regression function, consider using nonparametric regression (e.g., locally weighted regression, regression trees, projection pursuit, neural networks, smoothing splines, loess, wavelets). could use to detect outliers or confirm OLS. "],["maximum-likelihood-regression.html", "5.10 Maximum Likelihood", " 5.10 Maximum Likelihood Premise: find values of the parameters that maximize the probability of observing the data In other words, we try to maximize the value of theta in the likelihood function \\[ L(\\theta)=\\prod_{i=1}^{n}f(y_i|\\theta) \\] \\(f(y|\\theta)\\) is the probability density of observing a single value of \\(Y\\) given some value of \\(\\theta\\) \\(f(y|\\theta)\\) can be specify as various type of distributions. You can review back section Distributions. For example, if \\(y\\) is a dichotomous variable, then \\[ L(\\theta)=\\prod_{i=1}^{n}\\theta^{y_i}(1-\\theta)^{1-y_i} \\] \\(\\hat{\\theta}\\) is the Maximum Likelihood estimate if \\(L(\\hat{\\theta}) &gt; L(\\theta_0)\\) for all values of \\(\\theta_0\\) in the parameter space. 5.10.1 Motivation for MLE Suppose we know the conditional distribution of y given x: \\[ f_{Y|X}(y,x;\\theta) \\] where \\(\\theta\\) is the unknown parameter of distribution. Sometimes we are only concerned with the unconditional distribution \\(f_{Y}(y;\\theta)\\) Then given a sample of iid data, we can calculate the joint distribution of the entire sample, \\[ f_{Y_1,...,Y_n|X_1,...,X_n(y_1,...y_n,x_1,...,x_n;\\theta)}= \\prod_{i=1}^{n}f_{Y|X}(y_i,x_i;\\theta) \\] The joint distribution evaluated at the sample is the likelihood (probability) that we observed this particular sample (depends on \\(\\theta\\)) Idea for MLE: Given a sample, we choose our estimates of the parameters that gives the highest likelihood (probability) of observing our particular sample \\[ max_{\\theta} \\prod_{i=1}^{n}f_{Y|X}(y_i,x_i; \\theta) \\] Equivalently, \\[ max_{\\theta} \\prod_{i=1}^{n} ln(f_{Y|X}(y_i,x_i; \\theta)) \\] Solving for the Maximum Likelihood Estimator Solve First Order Condition \\[ \\frac{\\partial}{\\partial \\theta}\\sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) = 0 \\] where \\(\\hat{\\theta}_{MLE}\\) is defined. Evaluate Second Order Condition \\[ \\frac{\\partial^2}{\\partial \\theta^2} \\sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) &lt; 0 \\] where the above condition ensures we can solve for a maximum Examples: Unconditional Poisson Distribution: Number of products ordered on Amazon within an hour, number of website visits a day for a political campaign. Exponential Distribution: Length of time until an earthquake occurs, length of time a car battery lasts. \\[ \\begin{aligned} f_{Y|X}(y,x;\\theta) &amp;= exp(-y/x\\theta)/x\\theta \\\\ f_{Y_1,..Y_n|X_1,...,X_n(y_1,...,y_n,x_1,...,x_n;\\theta)} &amp;= \\prod_{i=1}^{n}exp(-y_i/x_i \\theta)/x_i \\theta \\end{aligned} \\] 5.10.2 Assumption High Level Regulatory Assumptions is the sufficient condition used to show large sample properties Hence, for each MLE, we will need to either assume or verify if the regulatory assumption holds. observations are independent and have the same density function. Under multivariate normal assumption, maximum likelihood yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments (Little 1988) To find the MLE, we usually differentiate the log-likelihood function and set it equal to 0. \\[ \\frac{d}{d\\theta}l(\\theta) = 0 \\] This is the score equation Our confidence in the MLE is quantified by the “pointedness” of the log-likelihood \\[ I_O(\\theta)= \\frac{d^2}{d\\theta^2}l(\\theta) = 0 \\] called the observed information while \\[ I(\\theta)=E[I_O(\\theta;Y)] \\] is the expected information. (also known as Fisher Information). which we base our variance of the estimator. \\[ V(\\hat{\\Theta}) \\approx I(\\theta)^{-1} \\] Consistency of MLE Suppose that \\(y_i\\) and \\(x_i\\) are iid drawn from the true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta_0)\\). If the following regulatory assumptions hold, R1: If \\(\\theta \\neq \\theta_0\\) then \\(f_{Y|X}(y_i,x_i;\\theta) \\neq f_{Y|X}(y_i,x_i;\\theta_0)\\) R2: The set \\(\\Theta\\) that contains the true parameters \\(\\theta_0\\) is compact R3: The log-likelihood \\(ln(f_{Y|X}(y_i,x_i;\\theta_0))\\) is continuous at each \\(\\theta\\) with probability 1 R4: \\(E(sup_{\\theta \\in \\Theta}|ln(f_{Y|X}(y_i,x_i;\\theta_0))|)\\) then the MLE estimator is consistent, \\[ \\hat{\\theta}_{MLE} \\to^p \\theta_0 \\] Asymptotic Normality of MLE Suppose that \\(y_1\\) and \\(x_i\\) are iid drawn from the true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta)\\). If R1-R4 and the following hold R5: \\(\\theta_0\\) is in the interior of the set \\(\\Theta\\) R6: \\(f_{Y|X}(y_i,x_i;\\theta)\\) is twice continuously differentiable in \\(\\theta\\) and \\(f_{Y|X}(y_i,x_i;\\theta) &gt;0\\) for a neighborhood \\(N \\in \\Theta\\) around \\(\\theta_0\\) R7: \\(\\int sup_{\\theta \\in N}||\\partial f_{Y|X}(y_i,x_i;\\theta)\\partial\\theta||d(y,x) &lt;\\infty\\), \\(\\int sup_{\\theta \\in N} || \\partial^2 f_{Y|X}(y_i,x_i;\\theta)/\\partial \\theta \\partial \\theta&#39; || d(y,x) &lt; \\infty\\) and \\(E(sup_{\\theta \\in N} || \\partial^2ln(f_{Y|X}(y_i,x_i;\\theta)) / \\partial \\theta \\partial \\theta&#39; ||) &lt; \\infty\\) R8: The information matrix \\(I(\\theta_0) = Var(\\partial f_{Y|X}(y,x_i; \\theta_0)/\\partial \\theta)\\) exists and is non-singular then the MLE estimator is asymptotically normal, \\[ \\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta_0) \\to^d N(0,I(\\theta_0)^{-1}) \\] 5.10.3 Properties Consistent: estimates are approximately unbiased in large samples Asymptotically efficient: approximately smaller standard errors compared to other estimator Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution. Suppose that \\(\\hat{\\theta}_n\\) is the MLE for \\(\\theta\\) based on n independent observations. then \\(\\hat{\\theta}_n \\sim N(\\theta,H^{-1})\\). where H is called the Fisher information matrix. It contains the expected values of the second partial derivatives of the log-likelihood function. The (i.j)th element of H is \\(-E(\\frac{\\partial^2l(\\theta)}{\\partial \\theta_i \\partial \\theta_j})\\) We can estimate H by finding the form determined above, and evaluating it at \\(\\theta = \\hat{\\theta}_n\\) Invariance: MLE for \\(g(\\theta) = g(\\hat{\\theta})\\) for any function g(.) \\[ \\hat{\\Theta} \\approx^d (\\theta,I(\\hat{\\theta)^{-1}})) \\] Explicit vs Implicit MLE If we solve the score equation to get an expression of MLE, then it’s called explicit If there is no closed form for MLE, and we need some algorithms to derive its expression, it’s called implicit Large Sample Property of MLE Implicit in these theorems is the assumption that we know what the conditional distribution, \\[ f_{Y|X}(y_i,x_i;\\theta_0) \\] but just do now know the exact parameter value. Any Distributional mis-specification will result in inconsistent parameter estimates. Quasi-MLE: Particular settings/ assumption that allow for certain types of distributional mis-specification (Ex: as long as the distribution is part of particular class or satisfies a particular assumption, then estimating with a wrong distribution will not lead to inconsistent parameter estimates). non-parametric/ Semi-parametric estimation: no or very little distributional assumption are made. (hard to implement, derive properties, and interpret) The asymptotic variance of the MLE achieves the Cramer-Rao Lower Bound C. R. Rao (1992) The Cramer-Rao Lower Bound is a lower brand for the asymptotic variance of a consistent and asymptotically normally distributed estimator. If an estimator achieves the lower bound then it is the most efficient estimator. The maximum Likelihood estimator (assuming the distribution is correctly specified and R1-R8 hold) is the most efficient consistent and asymptotically normal estimator. * most efficient among ALL consistent estimators (not limited to unbiased or linear estimators). Note ML is better choice for binary, strictly positive, count, or inherent heteroskedasticity than linear model. ML will assume that we know the conditional distribution of the outcome, and derive an estimator using that information. Adds an assumption that we know the distribution (which is similar to A6 Normal Distribution in linear model) will produce a more efficient estimator. 5.10.4 Compare to OLS MLE is not a cure for most of OLS problems: To do joint inference in MLE, we typically use log-likelihood calculation, instead of F-score Functional form affects estimation of MLE and OLS. Perfect Collinearity/Multicollinearity: highly correlated are likely to yield large standard errors. Endogeneity (Omitted variables bias, Simultaneous equations): Like OLS, MLE is also biased against this problem 5.10.5 Application Other applications of MLE Corner Solution Ex: hours worked, donations to charity Estimate with Tobit Non-negative count Ex: Numbers of arrest, Number of cigarettes smoked a day Estimate with Poisson regression Multinomial Choice Ex: Demand for cars, votes for primary election Estimate with mutinomial probit or logit Ordinal Choice Ex: Levels of Happiness, Levels of Income Ordered Probit Model for binary Response A binary variable will have a [Bernoulli] distribution: \\[ f_Y(y_i;p) = p^{y_i}(1-p)^{(1-y_i)} \\] where \\(p\\) is the probability of success. The conditional distribution is: \\[ f_{Y|X}(y_i,x_i;p(.)) = p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)} \\] So choose \\(p(x_i)\\) to be a reasonable function of \\(x_i\\) and unknown parameters \\(\\theta\\) We can use latent variable model as probability functions \\[ \\begin{aligned} y_i &amp;= 1\\{y_i^* &gt; 0 \\} \\\\ y_i^* &amp;= x_i \\beta-\\epsilon_i \\end{aligned} \\] \\(y_i^*\\) is a latent variable (unobserved) that is not well-defined in terms of units/magnitudes \\(\\epsilon_i\\) is a mean 0 unobserved random variable. We can rewrite the model without the latent variable, \\[ y_i = 1\\{x_i beta &gt; \\epsilon_i \\} \\] Then the probability function, \\[ \\begin{aligned} p(x_i) &amp;= P(y_i = 1|x_i) \\\\ &amp;= P(x_i \\beta &gt; \\epsilon_i | x_i) \\\\ &amp;= F_{\\epsilon|X}(x_i \\beta | x_i) \\end{aligned} \\] then we need to choose a conditional distribution for \\(\\epsilon_i\\). Hence, we can make additional strong independence assumption \\(\\epsilon_i\\) is independent of \\(x_i\\) Then the probability function is simply, \\[ p(x_i) = F_\\epsilon(x_i \\beta) \\] The probability function is also the conditional expectation function, \\[ E(y_i | x_i) = P(y_i = 1|x_i) = F_\\epsilon (x_i \\beta) \\] so we allow the conditional expectation function to be non-linear. Common distributional assumption Probit: Assume \\(\\epsilon_i\\) is standard normally distributed, then \\(F_\\epsilon(.) = \\Phi(.)\\) is the standard normal CDF. Logit: Assume \\(\\epsilon_i\\) is standard logistically distributed, then \\(F_\\epsilon(.) = \\Lambda(.)\\) is the standard normal CDF. Step to derive Choose a distribution (normal or logistic) and plug into the following log likelihood, \\[ ln(f_{Y|X} (y_i , x_i; \\beta)) = y_i ln(F_\\epsilon(x_i \\beta)) + (1-y_i)ln(1-F_\\epsilon(x_i \\beta)) \\] Solve the MLE by finding the Maximum of \\[ \\hat{\\beta}_{MLE} = argmax \\sum_{i=1}^{n}ln(f_{Y|X}(y_i,x_i; \\beta)) \\] Properties of the Probit and Logit Estimators Probit or Logit is consistent and asymptotically normal if A2 Full rank holds: \\(E(x_i&#39; x_i)\\) exists and is non-singular A5 Data Generation (random Sampling) (or A5a) holds: {y_i,x_i} are iid (or stationary and weakly dependent). Distributional assumptions on \\(\\epsilon_i\\) hold: Normal/Logistic and independent of \\(x_i\\) Under the same assumptions, Probit or Logit is also asymptotically efficient with asymptotic variance, \\[ I(\\beta_0)^{-1} = [E(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i\\beta_0)(1-F_\\epsilon(x_i\\beta_0))}x_i&#39; x_i)]^{-1} \\] where \\(F_\\epsilon(x_i\\beta_0)\\) is the probability density function (derivative of the CDF) 5.10.5.1 Interpretation \\(\\beta\\) is the average response in the latent variable associated with a change in \\(x_i\\) Magnitudes do not have meaning Direction does have meaning The partial effect for a Non-linear binary response model \\[ \\begin{aligned} E(y_i |x_i) &amp;= F_\\epsilon (x_i \\beta) \\\\ PE(x_{ij}) &amp;= \\frac{\\partial E(y_i |x_i)}{\\partial x_{ij}} = f_\\epsilon (x_i \\beta)\\beta_j \\end{aligned} \\] The partial effect is the coefficient parameter \\(\\beta_j\\) multiplied by a scaling factor \\(f_\\epsilon (x_i \\beta)\\) The scaling factor depends on \\(x_i\\) so the partial effect changes depending on what \\(x_i\\) is Single value for the partial effect Partial Effect at the Average (PEA) is the partial effect for an average individual \\[ f_{\\epsilon}(\\bar{x}\\hat{\\beta})\\hat{\\beta}_j \\] Average Partial Effect (APE) is the average of all partial effect for each individual. \\[ \\frac{1}{n}\\sum_{i=1}^{n}f_\\epsilon(x_i \\hat{\\beta})\\hat{\\beta}_j \\] In the linear model, \\(APE = PEA\\). In a non-linear model (e.g., binary response), \\(APE \\neq PEA\\) References "],["non-linear-regression.html", "Chapter 6 Non-linear Regression", " Chapter 6 Non-linear Regression Definition: models in which the derivatives of the mean function with respect to the parameters depend on one or more of the parameters. To approximate data, we can approximate the function by a high-order polynomial by a linear model (e.g., a Taylor expansion around \\(X\\)’s) a collection of locally linear models or basis function but it would not easy to interpret, or not enough data, or can’t interpret them globally. intrinsically nonlinear models: \\[ Y_i = f(\\mathbf{x_i;\\theta}) + \\epsilon_i \\] where \\(f(\\mathbf{x_i;\\theta})\\) is a nonlinear function relating \\(E(Y_i)\\) to the independent variables \\(x_i\\) \\(\\mathbf{x}_i\\) is a \\(k \\times 1\\) vector of independent variables (fixed). \\(\\mathbf{\\theta}\\) is a \\(p \\times 1\\) vector of parameters. \\(\\epsilon_i\\)s are iid variables mean 0 and variance \\(\\sigma^2\\). (sometimes it’s normal). "],["inference-1.html", "6.1 Inference", " 6.1 Inference Since \\(Y_i = f(\\mathbf{x}_i,\\theta) + \\epsilon_i\\), where \\(\\epsilon_i \\sim iid(0,\\sigma^2)\\), we can obtain \\(\\hat{\\theta}\\) by minimizing \\(\\sum_{i=1}^{n}(Y_i - f(x_i,\\theta))^2\\) and estimate \\(s^2 = \\hat{\\sigma}^2_{\\epsilon}=\\frac{\\sum_{i=1}^{n}(Y_i - f(x_i,\\theta))^2}{n-p}\\) 6.1.1 Linear Function of the Parameters If we assume \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), then \\[ \\hat{\\theta} \\sim AN(\\mathbf{\\theta},\\sigma^2[\\mathbf{F}(\\theta)&#39;\\mathbf{F}(\\theta)]^{-1}) \\] where AN = asymptotic normality Asymptotic means we have enough data to make inference (As your sample size increases, this becomes more and more accurate (to the true value)). Since we want to do inference on linear combinations of parameters or contrasts. If we have \\(\\mathbf{\\theta} = (\\theta_0,\\theta_1,\\theta_2)&#39;\\) and we want to look at \\(\\theta_1 - \\theta_2\\); we can define vector \\(\\mathbf{a} = (0,1,-1)&#39;\\), consider inference for \\(\\mathbf{a&#39;\\theta}\\) Rules for expectation and variance of a fixed vector \\(\\mathbf{a}\\) and random vector \\(\\mathbf{Z}\\); \\[ \\begin{aligned} E(\\mathbf{a&#39;Z}) &amp;= \\mathbf{a&#39;}E(\\mathbf{Z}) \\\\ var(\\mathbf{a&#39;Z}) &amp;= \\mathbf{a&#39;}var(\\mathbf{Z}) \\mathbf{a} \\end{aligned} \\] Then, \\[ \\mathbf{a&#39;\\hat{\\theta}} \\sim AN(\\mathbf{a&#39;\\theta},\\sigma^2\\mathbf{a&#39;[F(\\theta)&#39;F(\\theta)]^{-1}a}) \\] and \\(\\mathbf{a&#39;\\hat{\\theta}}\\) is asymptotically independent of \\(s^2\\) (to order 1/n) then \\[ \\frac{\\mathbf{a&#39;\\hat{\\theta}-a&#39;\\theta}}{s(\\mathbf{a&#39;[F(\\theta)&#39;F(\\theta)]^{-1}a})^{1/2}} \\sim t_{n-p} \\] to construct \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\mathbf{a&#39;\\theta}\\) \\[ \\mathbf{a&#39;\\theta} \\pm t_{(1-\\alpha/2,n-p)}s(\\mathbf{a&#39;[F(\\theta)&#39;F(\\theta)]^{-1}a})^{1/2} \\] Suppose \\(\\mathbf{a&#39;} = (0,...,j,...,0)\\). Then, a confidence interval for the \\(j\\)-th element of \\(\\mathbf{\\theta}\\) is \\[ \\hat{\\theta}_j \\pm t_{(1-\\alpha/2,n-p)}s\\sqrt{\\hat{c}^{j}} \\] where \\(\\hat{c}^{j}\\) is the \\(j\\)-th diagonal element of \\([\\mathbf{F(\\hat{\\theta})&#39;F(\\hat{\\theta})}]^{-1}\\) #set a seed value set.seed(23) #Generate x as 100 integers using seq function x &lt;- seq(0, 100, 1) #Generate y as a*e^(bx)+c y &lt;- runif(1, 0, 20) * exp(runif(1, 0.005, 0.075) * x) + runif(101, 0, 5) # visualize plot(x, y) #define our data frame datf = data.frame(x, y) #define our model function mod = function(a, b, x) a * exp(b * x) In this example, we can get the starting values by using linearized version of the function \\(\\log y = \\log a + b x\\). Then, we can fit a linear regression to this and use our estimates as starting values #get starting values by linearizing lin_mod = lm(log(y) ~ x, data = datf) #convert the a parameter back from the log scale; b is ok astrt = exp(as.numeric(lin_mod$coef[1])) bstrt = as.numeric(lin_mod$coef[2]) print(c(astrt, bstrt)) #&gt; [1] 14.07964761 0.01855635 with nls, we can fit the nonlinear model via least squares nlin_mod = nls(y ~ mod(a, b, x), start = list(a = astrt, b = bstrt), data = datf) #look at model fit summary summary(nlin_mod) #&gt; #&gt; Formula: y ~ mod(a, b, x) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; a 13.603909 0.165390 82.25 &lt;2e-16 *** #&gt; b 0.019110 0.000153 124.90 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.542 on 99 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 3 #&gt; Achieved convergence tolerance: 7.006e-07 #add prediction to plot plot(x, y) lines(x, predict(nlin_mod), col = &quot;red&quot;) 6.1.2 Nonlinear Suppose that \\(h(\\theta)\\) is a nonlinear function of the parameters. We can use Taylor series about \\(\\theta\\) \\[ h(\\hat{\\theta}) \\approx h(\\theta) + \\mathbf{h}&#39;[\\hat{\\theta}-\\theta] \\] where \\(\\mathbf{h} = (\\frac{\\partial h}{\\partial \\theta_1},...,\\frac{\\partial h}{\\partial \\theta_p})&#39;\\) with \\[ \\begin{aligned} E( \\hat{\\theta}) &amp;\\approx \\theta \\\\ var(\\hat{\\theta}) &amp;\\approx \\sigma^2[\\mathbf{F(\\theta)&#39;F(\\theta)}]^{-1} \\\\ E(h(\\hat{\\theta})) &amp;\\approx h(\\theta) \\\\ var(h(\\hat{\\theta})) &amp;\\approx \\sigma^2 \\mathbf{h&#39;[F(\\theta)&#39;F(\\theta)]^{-1}h} \\end{aligned} \\] Thus, \\[ h(\\hat{\\theta}) \\sim AN(h(\\theta),\\sigma^2\\mathbf{h&#39;[F(\\theta)&#39;F(\\theta)]^{-1}h}) \\] and an approximate \\(100(1-\\alpha)\\%\\) confidence interval for \\(h(\\theta)\\) is \\[ h(\\hat{\\theta}) \\pm t_{(1-\\alpha/2;n-p)}s(\\mathbf{h&#39;[F(\\theta)&#39;F(\\theta)]^{-1}h})^{1/2} \\] where \\(\\mathbf{h}\\) and \\(\\mathbf{F}(\\theta)\\) are evaluated at \\(\\hat{\\theta}\\) Regarding prediction interval for Y at \\(x=x_0\\) \\[ \\begin{aligned} Y_0 &amp;= f(x_0;\\theta) + \\epsilon_0, \\epsilon_0 \\sim N(0,\\sigma^2) \\\\ \\hat{Y}_0 &amp;= f(x_0,\\hat{\\theta}) \\end{aligned} \\] As \\(n \\to \\infty\\), \\(\\hat{\\theta} \\to \\theta\\), so we \\[ f(x_0, \\hat{\\theta}) \\approx f(x_0,\\theta) + \\mathbf{f}_0(\\mathbf{\\theta})&#39;[\\hat{\\theta}-\\theta] \\] where \\[ f_0(\\theta)= (\\frac{\\partial f(x_0,\\theta)}{\\partial \\theta_1},..,\\frac{\\partial f(x_0,\\theta)}{\\partial \\theta_p})&#39; \\] (note: this \\(f_0(\\theta)\\) is different from \\(f(\\theta)\\)). \\[ \\begin{aligned} Y_0 - \\hat{Y}_0 &amp;\\approx Y_0 - f(x_0,\\theta) - f_0(\\theta)&#39;[\\hat{\\theta}-\\theta] \\\\ &amp;= \\epsilon_0 - f_0(\\theta)&#39;[\\hat{\\theta}-\\theta] \\end{aligned} \\] \\[ \\begin{aligned} E(Y_0 - \\hat{Y}_0) &amp;\\approx E(\\epsilon_0)E(\\hat{\\theta}-\\theta) = 0 \\\\ var(Y_0 - \\hat{Y}_0) &amp;\\approx var(\\epsilon_0 - \\mathbf{(f_0(\\theta)&#39;[\\hat{\\theta}-\\theta])}) \\\\ &amp;= \\sigma^2 + \\sigma^2 \\mathbf{f_0 (\\theta)&#39;[F(\\theta)&#39;F(\\theta)]^{-1}f_0(\\theta)} \\\\ &amp;= \\sigma^2 (1 + \\mathbf{f_0 (\\theta)&#39;[F(\\theta)&#39;F(\\theta)]^{-1}f_0(\\theta)}) \\end{aligned} \\] Hence, combining \\[ Y_0 - \\hat{Y}_0 \\sim AN (0,\\sigma^2 (1 + \\mathbf{f_0 (\\theta)&#39;[F(\\theta)&#39;F(\\theta)]^{-1}f_0(\\theta)})) \\] Note: Confidence intervals for the mean response \\(Y_i\\) (which is different from prediction intervals) can be obtained similarly. "],["non-linear-least-squares.html", "6.2 Non-linear Least Squares", " 6.2 Non-linear Least Squares The LS estimate of \\(\\theta\\), \\(\\hat{\\theta}\\) is the set of parameters that minimizes the residual sum of squares: \\[ S(\\hat{\\theta}) = SSE(\\hat{\\theta}) = \\sum_{i=1}^{n}\\{Y_i - f(\\mathbf{x_i};\\hat{\\theta})\\}^2 \\] to obtain the solution, we can consider the partial derivatives of \\(S(\\theta)\\) with respect to each \\(\\theta_j\\) and set them to 0, which gives a system of p equations. Each normal equation is \\[ \\frac{\\partial S(\\theta)}{\\partial \\theta_j} = -2\\sum_{i=1}^{n}\\{Y_i -f(\\mathbf{x}_i;\\theta)\\}[\\frac{\\partial(\\mathbf{x}_i;\\theta)}{\\partial \\theta_j}] = 0 \\] but we can’t obtain a solution directly/analytically for this equation. Numerical Solutions Grid search A “grid” of possible parameter values and see which one minimize the residual sum of squares. finer grid = greater accuracy could be inefficient, and hard when p is large. Gauss-Newton Algorithm we have an initial estimate of \\(\\theta\\) denoted as \\(\\hat{\\theta}^{(0)}\\) use a Taylor expansions of \\(f(\\mathbf{x}_i;\\theta)\\) as a function of \\(\\theta\\) about the point \\(\\hat{\\theta}^{(0)}\\) \\[ \\begin{aligned} Y_i &amp;= f(x_i;\\theta) + \\epsilon_i \\\\ &amp;= f(x_i;\\theta) + \\sum_{j=1}^{p}\\{\\frac{\\partial f(x_i;\\theta)}{\\partial \\theta_j}\\}_{\\theta = \\hat{\\theta}^{(0)}} (\\theta_j - \\hat{\\theta}^{(0)}) + \\text{remainder} + \\epsilon_i \\end{aligned} \\] Equivalently, In matrix notation, \\[ \\mathbf{Y} = \\left[ \\begin{array} {c} Y_1 \\\\ . \\\\ Y_n \\end{array} \\right] \\] \\[ \\mathbf{f}(\\hat{\\theta}^{(0)}) = \\left[ \\begin{array} {c} f(\\mathbf{x_1,\\hat{\\theta}}^{(0)}) \\\\ . \\\\ f(\\mathbf{x_n,\\hat{\\theta}}^{(0)}) \\end{array} \\right] \\] \\[ \\mathbf{\\epsilon} = \\left[ \\begin{array} {c} \\epsilon_1 \\\\ . \\\\ \\epsilon_n \\end{array} \\right] \\] \\[ \\mathbf{F}(\\hat{\\theta}^{(0)}) = \\left[ \\begin{array} {ccc} \\frac{\\partial f(x_1,\\mathbf{\\theta})}{\\partial \\theta_1} &amp; ... &amp; \\frac{\\partial f(x_1,\\mathbf{\\theta})}{\\partial \\theta_p}\\\\ . &amp; . &amp; . \\\\ \\frac{\\partial f(x_n,\\mathbf{\\theta})}{\\partial \\theta_1} &amp; ... &amp; \\frac{\\partial f(x_n,\\mathbf{\\theta})}{\\partial \\theta_p} \\end{array} \\right]_{\\theta = \\hat{\\theta}^{(0)}} \\] Hence, \\[ \\mathbf{Y} = \\mathbf{f}(\\hat{\\theta}^{(0)}) + \\mathbf{F}(\\hat{\\theta}^{(0)})(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon + \\text{remainder} \\] where we assume that the remainder is small and the error term is only assumed to be iid with mean 0 and variance \\(\\sigma^2\\). We can rewrite the above equation as \\[ \\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(0)}) \\approx \\mathbf{F}(\\hat{\\theta}^{(0)})(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon \\] where it is in the form of linear model. After we solve for \\((\\theta - \\hat{\\theta}^{(0)})\\) and let it equal to \\(\\hat{\\delta}^{(1)}\\) Then we new estimate is given by adding the Gauss increment adjustment to the initial estimate \\(\\hat{\\theta}^{(1)} = \\hat{\\theta}^{(0)} + \\hat{\\delta}^{(1)}\\) We can repeat this process. Gauss-Newton Algorithm Steps: initial estimate \\(\\hat{\\theta}^{(0)}\\), set j = 0 Taylor series expansion and calculate \\(\\mathbf{f}(\\hat{\\theta}^{(j)})\\) and \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) Use OLS to get \\(\\hat{\\delta}^{(j+1)}\\) get the new estimate \\(\\hat{\\theta}^{(j+1)}\\), return to step 2 continue until “convergence” With the final parameter estimate \\(\\hat{\\theta}\\), we can estimate \\(\\sigma^2\\) if \\(\\epsilon \\sim (\\mathbf{0}, \\sigma^2 \\mathbf{I})\\) by \\[ \\hat{\\sigma}^2= \\frac{1}{n-p}(\\mathbf{Y}-\\mathbf{f}(x;\\hat{\\theta}))&#39;(\\mathbf{Y}-\\mathbf{f}(x;\\hat{\\theta})) \\] Criteria for convergence Minor change in the objective function (SSE = residual sum of squares) \\[ \\frac{|SSE(\\hat{\\theta}^{(j+1)})-SSE(\\hat{\\theta}^{(j)})|}{SSE(\\hat{\\theta}^{(j)})} &lt; \\gamma_1 \\] Minor change in the parameter estimates \\[ |\\hat{\\theta}^{(j+1)}-\\hat{\\theta}^{(j)}| &lt; \\gamma_2 \\] “residual projection” criterion of (Bates and Watts 1981) 6.2.1 Alternative of Gauss-Newton Algorithm 6.2.1.1 Gauss-Newton Algorithm Normal equations: \\[ \\frac{\\partial SSE(\\theta)}{\\partial \\theta} = 2\\mathbf{F}(\\theta)&#39;[\\mathbf{Y}-\\mathbf{f}(\\theta)] \\] \\[ \\begin{aligned} \\hat{\\theta}^{(j+1)} &amp;= \\hat{\\theta}^{(j)} + \\hat{\\delta}^{(j+1)} \\\\ &amp;= \\hat{\\theta}^{(j)} + [\\mathbf{F}((\\hat{\\theta})^{(j)})&#39;\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\mathbf{F}(\\hat{\\theta})^{(j)} \\\\ &amp;= \\hat{\\theta}^{(j)} - \\frac{1}{2}[\\mathbf{F}(\\hat{\\theta}^{(j)})&#39;\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta} \\end{aligned} \\] where \\(\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) is a gradient vector (points in the direction in which the SSE increases most rapidly). This path is known as steepest ascent. \\([\\mathbf{F}(\\hat{\\theta}^{(j)})&#39;\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\) indicates how far to move \\(-1/2\\): indicator of the direction of steepest descent. 6.2.1.2 Modified Gauss-Newton Algorithm To avoid overstepping (the local min), we can use the modified Gauss-Newton Algorithm. We define a new proposal for \\(\\theta\\) \\[ \\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\alpha_j \\hat{\\delta}^{(j+1)}, 0 &lt; \\alpha_j &lt; 1 \\] where \\(\\alpha_j\\) (called the “learning rate”): is used to modify the step length. We could also have \\(\\alpha \\times 1/2\\), but typically it is assumed to be absorbed into the learning rate. A way to choose \\(\\alpha_j\\), we can use step halving \\[ \\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\frac{1}{2^k}\\hat{\\delta}^{(j+1)} \\] where \\(k\\) is the smallest non-negative integer such that \\[ SSE(\\hat{\\theta}^{(j)}+\\frac{1}{2^k}\\hat{\\delta}^{(j+1)}) &lt; SSE(\\hat{\\theta}^{(j)}) \\] which means we try \\(\\hat{\\delta}^{(j+1)}\\), then \\(\\hat{\\delta}^{(j+1)}/2\\), \\(\\hat{\\delta}^{(j+1)}/4\\), etc. The most general form of the convergence algorithm is \\[ \\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{A}_j \\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta} \\] where \\(\\mathbf{A}_j\\) is a positive definite matrix \\(\\alpha_j\\) is the learning rate \\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\)is the gradient based on some objective function Q (a function of \\(\\theta\\)), which is typically the SSE in nonlinear regression applications (e.g., cross-entropy for classification). Refer back to the Modified Gauss-Newton Algorithm, we can see it is in this form \\[ \\hat{\\theta}^{(j+1)} =\\hat{\\theta}^{(j)} - \\alpha_j[\\mathbf{F}(\\hat{\\theta}^{(j)})&#39;\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta} \\] where Q = SSE, \\([\\mathbf{F}(\\hat{\\theta}^{(j)})&#39;\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1} = \\mathbf{A}\\) 6.2.1.3 Steepest Descent (also known just “gradient descent”) \\[ \\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{I}_{p \\times p}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta} \\] slow to converge, moves rapidly initially. could be use for starting values 6.2.1.4 Levenberg -Marquardt \\[ \\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})&#39;\\mathbf{F}(\\hat{\\theta}^{(j)})+ \\tau \\mathbf{I}_{p \\times p}]\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta} \\] which is a compromise between the Gauss-Newton Algorithm and the Steepest Descent. best when \\(\\mathbf{F}(\\hat{\\theta}^{(j)})&#39;\\mathbf{F}(\\hat{\\theta}^{(j)})\\) is nearly singular (\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) isn’t of full rank) similar to ridge regression If \\(SSE(\\hat{\\theta}^{(j+1)}) &lt; SSE(\\hat{\\theta}^{(j)})\\), then \\(\\tau= \\tau/10\\) for the next iteration. Otherwise, \\(\\tau = 10 \\tau\\) 6.2.1.5 Newton-Raphson \\[ \\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\frac{\\partial^2Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta&#39;}]^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta} \\] The Hessian matrix can be rewritten as: \\[ \\frac{ \\partial^2Q(\\hat{ \\theta}^{(j)})}{ \\partial \\theta \\partial \\theta&#39;} = 2 \\mathbf{F}((\\hat{ \\theta})^{(j)})&#39; \\mathbf{F} ( \\hat{\\theta}^{(j)}) - 2\\sum_{i=1}^{n} [Y_i - f(x_i;\\theta)] \\frac{\\partial^2f(x_i;\\theta)}{\\partial \\theta \\partial \\theta&#39;} \\] which contains the same term that Gauss-Newton Algorithm, combined with one containing the second partial derivatives of f(). (methods that require the second derivatives of the objective function are known as “second-order methods”.) However, the last term \\(\\frac{\\partial^2f(x_i;\\theta)}{\\partial \\theta \\partial \\theta&#39;}\\) can sometimes be non-singular. 6.2.1.6 Quasi-Newton update \\(\\theta\\) according to \\[ \\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{H}_j^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta} \\] where \\(H_j\\) is a symmetric positive definite approximation to the Hessian, which gets closer as \\(j \\to \\infty\\). \\(\\mathbf{H}_j\\) is computed iteratively Among first-order methods (where only first derivatives are required), this method performs best. 6.2.1.7 Derivative Free Methods secant Method: like Gauss-Newton Algorithm, but calculates the derivatives numerically from past iterations. Simplex Methods Genetic Algorithm Differential Evolution Algorithms Particle Swarm Optimization Ant Colony Optimization 6.2.2 Practical Considerations To converge, algorithm need good initial estimates. Starting values: Prior or theoretical info A grid search or a graph of \\(SSE(\\theta)\\) could also use OLS to get starting values. Model interpretation: if you have some idea regarding the form of the objective function, then you can try to guess the initial value. Expected Value Parameterization Constrained Parameters: (constraints on parameters like \\(\\theta_i&gt;a,a&lt; \\theta_i &lt;b\\)) fit the model first to see if the converged parameter estimates satisfy the constraints. if they don’t satisfy, then try re-parameterizing 6.2.2.1 Failure to converge \\(SSE(\\theta)\\) may be “flat” in a neighborhood of the minimum. You can try different or “better” starting values. Might suggest the model is too complex for the data, might consider simpler model. 6.2.2.2 Convergence to a Local Minimum Linear least squares has the property that \\(SSE(\\theta) = \\mathbf{(Y-X\\beta)&#39;(Y-X\\beta)}\\), which is quadratic and has a unique minimum (or maximum). Nonlinear east squares need not have a unique minimum Using different starting values can help If the dimension of \\(\\theta\\) is low, graph \\(SSE(\\theta)\\) as a function of \\(\\theta_i\\) Different algorithm can help (e.g., genetic algorithm, particle swarm) To converge, algorithms need good initial estimates. Starting values: prior or theoretical info A grid search or a graph OLS estimates as starting values Model interpretation Expected Value Parameterization Constrained Parameters: try the model without the constraints first. If the resulted parameter estimates does not satisfy the constraint, try re-parameterizing # Grid search # choose grid of a and b values aseq = seq(10,18,.2) bseq = seq(.001,.075,.001) na = length(aseq) nb = length(bseq) SSout = matrix(0,na*nb,3) #matrix to save output cnt = 0 for (k in 1:na){ for (j in 1:nb){ cnt = cnt+1 ypred = mod(aseq[k],bseq[j],x) #evaluate model w/ these parms ss = sum((y-ypred)^2) #this is our SSE objective function #save values of a, b, and SSE SSout[cnt,1]=aseq[k] SSout[cnt,2]=bseq[j] SSout[cnt,3]=ss } } #find minimum SSE and associated a,b values mn_indx = which.min(SSout[,3]) astrt = SSout[mn_indx,1] bstrt = SSout[mn_indx,2] #now, run nls function with these starting values nlin_modG=nls(y~mod(a,b,x),start=list(a=astrt,b=bstrt)) nlin_modG #&gt; Nonlinear regression model #&gt; model: y ~ mod(a, b, x) #&gt; data: parent.frame() #&gt; a b #&gt; 13.60391 0.01911 #&gt; residual sum-of-squares: 235.5 #&gt; #&gt; Number of iterations to convergence: 3 #&gt; Achieved convergence tolerance: 2.293e-07 # Note, the package `nls_multstart` will allow you # to do a grid search without programming your own loop For prediction interval plotFit( nlin_modG, interval = &quot;both&quot;, pch = 19, shade = TRUE, col.conf = &quot;skyblue4&quot;, col.pred = &quot;lightskyblue2&quot;, data = datf ) Based on the forms of your function, you can also have programmed starting values from nls function (e.e.g, logistic growth, asymptotic regression, etc). apropos(&quot;^SS&quot;) #&gt; [1] &quot;ss&quot; &quot;SSasymp&quot; &quot;SSasympOff&quot; &quot;SSasympOrig&quot; &quot;SSbiexp&quot; #&gt; [6] &quot;SSD&quot; &quot;SSfol&quot; &quot;SSfpl&quot; &quot;SSgompertz&quot; &quot;SSlogis&quot; #&gt; [11] &quot;SSmicmen&quot; &quot;SSout&quot; &quot;SSweibull&quot; For example, a logistic growth model: \\[ P = \\frac{K}{1+ exp(P_0+ rt)} + \\epsilon \\] where P = population at time t K = carrying capacity r = population growth rate but in R you have slight different parameterization: \\[ P = \\frac{asym}{1 + exp(\\frac{xmid - t}{scal})} \\] where \\(asym\\) = carrying capacity \\(xmid\\) = the x value at the inflection point of the curve \\(scal\\) = scaling parameter. Hence, you have \\(K = asym\\) \\(r = -1/scal\\) \\(P_0 = -rxmid\\) # simulated data time &lt;- c(1, 2, 3, 5, 10, 15, 20, 25, 30, 35) population &lt;- c(2.8, 4.2, 3.5, 6.3, 15.7, 21.3, 23.7, 25.1, 25.8, 25.9) plot(time, population, las = 1, pch = 16) # model fitting logisticModelSS &lt;- nls(population ~ SSlogis(time, Asym, xmid, scal)) summary(logisticModelSS) #&gt; #&gt; Formula: population ~ SSlogis(time, Asym, xmid, scal) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; Asym 25.5029 0.3666 69.56 3.34e-11 *** #&gt; xmid 8.7347 0.3007 29.05 1.48e-08 *** #&gt; scal 3.6353 0.2186 16.63 6.96e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.6528 on 7 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 1 #&gt; Achieved convergence tolerance: 1.908e-06 coef(logisticModelSS) #&gt; Asym xmid scal #&gt; 25.502890 8.734698 3.635333 Other parameterization #convert to other parameterization Ks = as.numeric(coef(logisticModelSS)[1]) rs = -1 / as.numeric(coef(logisticModelSS)[3]) Pos = -rs * as.numeric(coef(logisticModelSS)[2]) #let&#39;s refit with these parameters logisticModel &lt;- nls(population ~ K / (1 + exp(Po + r * time)), start = list(Po = Pos, r = rs, K = Ks)) summary(logisticModel) #&gt; #&gt; Formula: population ~ K/(1 + exp(Po + r * time)) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; Po 2.40272 0.12702 18.92 2.87e-07 *** #&gt; r -0.27508 0.01654 -16.63 6.96e-07 *** #&gt; K 25.50289 0.36665 69.56 3.34e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.6528 on 7 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 0 #&gt; Achieved convergence tolerance: 1.924e-06 #note: initial values = solution (highly unusual, but ok) plot(time, population, las = 1, pch = 16) lines(time, predict(logisticModel), col = &quot;red&quot;) If can also define your own self-starting function if your models are uncommon (built in nls) Example is based on (Schabenberger and Pierce 2001) #Load data dat &lt;- read.table(&quot;images/dat.txt&quot;, header = T) # plot dat.plot &lt;- ggplot(dat) + geom_point(aes( x = no3, y = ryp, color = as.factor(depth) )) + labs(color = &#39;Depth (cm)&#39;) + xlab(&#39;Soil NO3&#39;) + ylab(&#39;relative yield percent&#39;) dat.plot The suggested model (known as plateau model) is \\[ E(Y_{ij}) = (\\beta_{0j} + \\beta_{1j}N_{ij})I_{N_{ij}\\le \\alpha_j} + (\\beta_{0j} + \\beta_{1j}\\alpha_j)I_{N_{ij} &gt; \\alpha_j} \\] where N is an observation i is a particular observation j = 1,2 corresponding to depths (30,60) #First define model as a function nonlinModel &lt;- function(predictor, b0, b1, alpha) { ifelse(predictor &lt;= alpha, #if observation less than cutoff simple linear model b0 + b1 * predictor, b0 + b1 * alpha) #otherwise flat line } define selfStart function. Because we defined our model to be linear in the first part and then plateau (remain constant) we can use the first half of our predictors (sorted by increasing value) to get an initial estimate for the slope and intercept of the model, and the last predictor value (alpha) can be the starting value for the plateau parameter. nonlinModelInit &lt;- function(mCall, LHS, data) { # sort data by increasing predictor value - # done so we can just use the low level # no3 conc to fit a simple model xy &lt;- sortedXyData(mCall[[&#39;predictor&#39;]], LHS, data) n &lt;- nrow(xy) #For the first half of the data a simple linear model is fit lmFit &lt;- lm(xy[1:(n / 2), &#39;y&#39;] ~ xy[1:(n / 2), &#39;x&#39;]) b0 &lt;- coef(lmFit)[1] b1 &lt;- coef(lmFit)[2] # for the cut off to the flat part select the last # x value used in creating linear model alpha &lt;- xy[(n / 2), &#39;x&#39;] value &lt;- c(b0, b1, alpha) names(value) &lt;- mCall[c(&#39;b0&#39;, &#39;b1&#39;, &#39;alpha&#39;)] value } combine model and custom function to calculate starting values. SS_nonlinModel &lt;- selfStart(nonlinModel,nonlinModelInit,c(&#39;b0&#39;,&#39;b1&#39;,&#39;alpha&#39;)) # Above code defined model and selfStart now just need to call it for each of the depths sep30_nls &lt;- nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), data = dat[dat$depth == 30,]) sep60_nls &lt;- nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), data = dat[dat$depth == 60,]) par(mfrow = c(1, 2)) plotFit( sep30_nls, interval = &quot;both&quot;, pch = 19, shade = TRUE, col.conf = &quot;skyblue4&quot;, col.pred = &quot;lightskyblue2&quot;, data = dat[dat$depth == 30,], main = &#39;Results 30 cm depth&#39;, ylab = &#39;relative yield percent&#39;, xlab = &#39;Soil NO3 concentration&#39;, xlim = c(0, 120) ) plotFit( sep60_nls, interval = &quot;both&quot;, pch = 19, shade = TRUE, col.conf = &quot;lightpink4&quot;, col.pred = &quot;lightpink2&quot;, data = dat[dat$depth == 60,], main = &#39;Results 60 cm depth&#39;, ylab = &#39;relative yield percent&#39;, xlab = &#39;Soil NO3 concentration&#39;, xlim = c(0, 120) ) summary(sep30_nls) #&gt; #&gt; Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; b0 15.1943 2.9781 5.102 6.89e-07 *** #&gt; b1 3.5760 0.1853 19.297 &lt; 2e-16 *** #&gt; alpha 23.1324 0.5098 45.373 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.258 on 237 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 6 #&gt; Achieved convergence tolerance: 3.608e-09 summary(sep60_nls) #&gt; #&gt; Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; b0 5.4519 2.9785 1.83 0.0684 . #&gt; b1 5.6820 0.2529 22.46 &lt;2e-16 *** #&gt; alpha 16.2863 0.2818 57.80 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 7.427 on 237 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 5 #&gt; Achieved convergence tolerance: 8.571e-09 Instead of modeling the depths model separately we model them together - so there is a common slope, intercept, and plateau. red_nls &lt;- nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), data = dat) summary(red_nls) #&gt; #&gt; Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; b0 8.7901 2.7688 3.175 0.0016 ** #&gt; b1 4.8995 0.2207 22.203 &lt;2e-16 *** #&gt; alpha 18.0333 0.3242 55.630 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 9.13 on 477 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 7 #&gt; Achieved convergence tolerance: 7.126e-09 par(mfrow = c(1, 1)) plotFit( red_nls, interval = &quot;both&quot;, pch = 19, shade = TRUE, col.conf = &quot;lightblue4&quot;, col.pred = &quot;lightblue2&quot;, data = dat, main = &#39;Results combined&#39;, ylab = &#39;relative yield percent&#39;, xlab = &#39;Soil NO3 concentration&#39; ) Examine residual values for the combined model. library(nlstools) # using nlstools nlsResiduals function to get some quick residual plots # can also use test.nlsResiduals(resid) # https://www.rdocumentation.org/packages/nlstools/versions/1.0-2 resid &lt;- nlsResiduals(red_nls) plot(resid) can we test whether the parameters for the two soil depth fits are significantly different? To know if the combined model is appropriate, we consider a parameterization where we let the parameters for the 60cm model be equal to the parameters from the 30cm model plus some increment: \\[ \\begin{aligned} \\beta_{02} &amp;= \\beta_{01} + d_0 \\\\ \\beta_{12} &amp;= \\beta_{11} + d_1 \\\\ \\alpha_{2} &amp;= \\alpha_{1} + d_a \\end{aligned} \\] We can implement this in the following function: nonlinModelF &lt;- function(predictor,soildep,b01,b11,a1,d0,d1,da){ b02 = b01 + d0 #make 60cm parms = 30cm parms + increment b12 = b11 + d1 a2 = a1 + da y1 = ifelse(predictor&lt;=a1, #if observation less than cutoff simple linear model b01+b11*predictor, b01+b11*a1) # otherwise flat line y2 = ifelse(predictor&lt;=a2, b02+b12*predictor, b02+b12*a2) y = y1*(soildep == 30) + y2*(soildep == 60) #combine models return(y) } Starting values are easy now because we fit each model individually. Soil_full = nls( ryp ~ nonlinModelF( predictor = no3, soildep = depth, b01, b11, a1, d0, d1, da ), data = dat, start = list( b01 = 15.2, b11 = 3.58, a1 = 23.13, d0 = -9.74, d1 = 2.11, da = -6.85 ) ) summary(Soil_full) #&gt; #&gt; Formula: ryp ~ nonlinModelF(predictor = no3, soildep = depth, b01, b11, #&gt; a1, d0, d1, da) #&gt; #&gt; Parameters: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; b01 15.1943 2.8322 5.365 1.27e-07 *** #&gt; b11 3.5760 0.1762 20.291 &lt; 2e-16 *** #&gt; a1 23.1324 0.4848 47.711 &lt; 2e-16 *** #&gt; d0 -9.7424 4.2357 -2.300 0.0219 * #&gt; d1 2.1060 0.3203 6.575 1.29e-10 *** #&gt; da -6.8461 0.5691 -12.030 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 7.854 on 474 degrees of freedom #&gt; #&gt; Number of iterations to convergence: 1 #&gt; Achieved convergence tolerance: 3.742e-06 So, the increment parameters, \\(d_1\\),\\(d_2\\),\\(d_a\\) are all significantly different from 0, suggesting that we should have two models here. 6.2.3 Model/Estimation Adequacy (Bates and Watts 1980) assess nonlinearity in terms of 2 components of curvature: Intrinsic nonlinearity: the degree of bending and twisting in \\(f(\\theta)\\); our estimation approach assumes that the true function is relatively flat (planar) in the neighborhood fo \\(\\hat{\\theta}\\), which would not be true if \\(f()\\) has a lot of “bending” in the neighborhood of \\(\\hat{\\theta}\\) (independent of parameterization) If bad, the distribution of residuals will be seriously distorted slow to converge difficult to identify ( could use this function rms.curve) Solution: could use higher order Taylor expansions estimation Bayesian method Parameter effects nonlinearity: degree to which curvature (nonlinearity) is affected by choice of \\(\\theta\\) (data dependent; dependent on parameterization) leads to problems with inference on \\(\\hat{\\theta}\\) rms.curve in MASS can identify bootstrap-based inference can also be used Solution: try to reparaemterize. #check parameter effects and intrinsic curvature modD = deriv3(~ a*exp(b*x), c(&quot;a&quot;,&quot;b&quot;),function(a,b,x) NULL) nlin_modD = nls(y ~ modD(a, b, x), start = list(a = astrt, b = bstrt), data = datf) rms.curv(nlin_modD) #&gt; Parameter effects: c^theta x sqrt(F) = 0.0626 #&gt; Intrinsic: c^iota x sqrt(F) = 0.0062 In linear model, we have Linear Regression, we have goodness of fit measure as \\(R^2\\): \\[ \\begin{aligned} R^2 &amp;= \\frac{SSR}{SSTO} = 1- \\frac{SSE}{SSTO} \\\\ &amp;= \\frac{\\sum_{i=1}^n (\\hat{Y}_i- \\bar{Y})^2}{\\sum_{i=1}^n (Y_i- \\bar{Y})^2} = 1- \\frac{\\sum_{i=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{i=1}^n (Y_i- \\bar{Y})^2} \\end{aligned} \\] but not valid in the nonlinear case because the error sum of squares and model sum of squares do not add to the total corrected sum of squares \\[ SSR + SSE \\neq SST \\] but we can use pseudo-\\(R^2\\): \\[ R^2_{pseudo} = 1 - \\frac{\\sum_{i=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{i=1}^n (Y_i- \\bar{Y})^2} \\] But we can’t interpret this as the proportion of variability explained by the model. We should use as a relative comparison of different models. Residual Plots: standardize, similar to OLS. useful when the intrinsic curvature is small: The studentized residuals \\[ r_i = \\frac{e_i}{s\\sqrt{1-\\hat{c}_i}} \\] where \\(\\hat{c}_i\\)is the i-th diagonal of \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})&#39;F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})&#39;}\\) We could have problems of Collinearity: the condition number of \\(\\mathbf{[F(\\hat{\\theta})&#39;F(\\hat{\\theta})]^{-1}}\\) should be less than 30. Follow (Magel and Hertsgaard 1987); reparameterize if possible Leverage: Like OLS, but consider \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})&#39;F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})&#39;}\\) (also known as “tangent plant hat matrix”) (St Laurent and Cook 1992) Heterogeneous Errors: weighted Non-linear Least Squares Correlated Errors: Generalized Nonlinear Least Squares Nonlinear Mixed Models Bayesian methods 6.2.4 Application \\[ y_i = \\frac{\\theta_0 + \\theta_1 x_i}{1 + \\theta_2 \\exp(0.4 x_i)} + \\epsilon_i \\] where \\(i = 1,..,n\\) Get the starting values plot(my_data) We notice that \\(Y_{max} = \\theta_0 + \\theta_1 x_i\\) in which we can find x_i from data max(my_data$y) #&gt; [1] 2.6722 my_data$x[which.max(my_data$y)] #&gt; [1] 0.0094 hence, \\(x = 0.0094\\) when \\(y = 2.6722\\) when we have the first equation as \\[ 2.6722 = \\theta_0 + 0.0094 \\theta_1 \\] \\[ \\theta_0 + 0.0094 \\theta_1 + 0 \\theta_2 = 2.6722 \\] Secondly, we notice that we can obtain the “average” of y when \\[ 1+ \\theta_2 exp(0.4 x) = 2 \\] then we can find this average numbers of x and y # find mean y mean(my_data$y) #&gt; [1] -0.0747864 # find y closest to its mean my_data$y[which.min(abs(my_data$y - (mean(my_data$y))))] #&gt; [1] -0.0773 # find x closest to the mean y my_data$x[which.min(abs(my_data$y - (mean(my_data$y))))] #&gt; [1] 11.0648 we have the second equation \\[ 1 + \\theta_2 exp(0.4*11.0648) = 2 \\] \\[ 0 \\theta_1 + 0 \\theta_1 + 83.58967 \\theta_2 = 1 \\] Thirdly, we can plug in the value of x closest to 1 to find the value of y # find value of x closet to 1 my_data$x[which.min(abs(my_data$x - 1))] #&gt; [1] 0.9895 # find index of x closest to 1 match(my_data$x[which.min(abs(my_data$x - 1))], my_data$x) #&gt; [1] 14 # find y value my_data$y[match(my_data$x[which.min(abs(my_data$x - 1))], my_data$x)] #&gt; [1] 1.4577 hence we have \\[ 1.457 = \\frac{\\theta_0 + \\theta_1*0.9895}{1 + \\theta_2 exp(0.4*0.9895)} \\] \\[ 1.457 + 2.164479 *\\theta_2 = \\theta_0 + \\theta_1*0.9895 \\] \\[ \\theta_0 + \\theta_1*0.9895 - 2.164479 *\\theta_2 = 1.457 \\] with 3 equations, we can solve them to get the starting value for \\(\\theta_0,\\theta_1, \\theta_2\\) \\[ \\theta_0 + 0.0094 \\theta_1 + 0 \\theta_2 = 2.6722 \\] \\[ 0 \\theta_1 + 0 \\theta_1 + 83.58967 \\theta_2 = 1 \\] \\[ \\theta_0 + \\theta_1*0.9895 - 2.164479 *\\theta_2 = 1.457 \\] library(matlib) A = matrix( c(0, 0.0094, 0, 0, 0, 83.58967, 1, 0.9895,-2.164479), nrow = 3, ncol = 3, byrow = T ) b = c(2.6722, 1, 1.457) showEqn(A, b) #&gt; 0*x1 + 0.0094*x2 + 0*x3 = 2.6722 #&gt; 0*x1 + 0*x2 + 83.58967*x3 = 1 #&gt; 1*x1 + 0.9895*x2 - 2.164479*x3 = 1.457 Solve(A, b, fractions = F) #&gt; x1 = -279.80879739 #&gt; x2 = 284.27659574 #&gt; x3 = 0.0119632 Construct manually Gauss-Newton Algorithm #starting value theta_0_strt = -279.80879739 theta_1_strt = 284.27659574 theta_2_strt = 0.0119632 #model mod_4 = function(theta_0, theta_1, theta_2, x) { (theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x)) } #define a function f_4 = expression((theta_0 + theta_1 * x) / (1 + theta_2 * exp(0.4 * x))) #take the first derivative df_4.d_theta_0 = D(f_4, &#39;theta_0&#39;) df_4.d_theta_1 = D(f_4, &#39;theta_1&#39;) df_4.d_theta_2 = D(f_4, &#39;theta_2&#39;) # save the result of all iterations theta_vec = matrix(c(theta_0_strt, theta_1_strt, theta_2_strt)) delta = matrix(NA, nrow = 3, ncol = 1) f_theta = as.matrix(eval( f_4, list( x = my_data$x, theta_0 = theta_vec[1, 1], theta_1 = theta_vec[2, 1], theta_2 = theta_vec[3, 1] ) )) i = 1 repeat { F_theta_0 = as.matrix(cbind( eval( df_4.d_theta_0, list( x = my_data$x, theta_0 = theta_vec[1, i], theta_1 = theta_vec[2, i], theta_2 = theta_vec[3, i] ) ), eval( df_4.d_theta_1, list( x = my_data$x, theta_0 = theta_vec[1, i], theta_1 = theta_vec[2, i], theta_2 = theta_vec[3, i] ) ), eval( df_4.d_theta_2, list( x = my_data$x, theta_0 = theta_vec[1, i], theta_1 = theta_vec[2, i], theta_2 = theta_vec[3, i] ) ) )) delta[, i] = (solve(t(F_theta_0) %*% F_theta_0)) %*% t(F_theta_0) %*% (my_data$y - f_theta[, i]) theta_vec = cbind(theta_vec, matrix(NA, nrow = 3, ncol = 1)) theta_vec[, i + 1] = theta_vec[, i] + delta[, i] i = i + 1 f_theta = cbind(f_theta, as.matrix(eval( f_4, list( x = my_data$x, theta_0 = theta_vec[1, i], theta_1 = theta_vec[2, i], theta_2 = theta_vec[3, i] ) ))) delta = cbind(delta, matrix(NA, nrow = 3, ncol = 1)) #convergence criteria based on SSE if (abs(sum((my_data$y - f_theta[, i]) ^ 2) - sum((my_data$y - f_theta[, i - 1]) ^ 2)) / (sum((my_data$y - f_theta[, i - 1]) ^ 2)) &lt; 0.001) { break } } delta #&gt; [,1] [,2] [,3] [,4] [,5] [,6] #&gt; [1,] 2.811840e+02 -0.03929013 0.43160654 0.6904856 0.6746748 0.4056460 #&gt; [2,] -2.846545e+02 0.03198446 -0.16403964 -0.2895487 -0.2933345 -0.1734087 #&gt; [3,] -1.804567e-05 0.01530258 0.05137285 0.1183271 0.1613129 0.1160404 #&gt; [,7] [,8] #&gt; [1,] 0.09517681 NA #&gt; [2,] -0.03928239 NA #&gt; [3,] 0.03004911 NA theta_vec #&gt; [,1] [,2] [,3] [,4] [,5] [,6] #&gt; [1,] -279.8087974 1.37521388 1.33592375 1.76753029 2.4580158 3.1326907 #&gt; [2,] 284.2765957 -0.37788712 -0.34590266 -0.50994230 -0.7994910 -1.0928255 #&gt; [3,] 0.0119632 0.01194515 0.02724773 0.07862059 0.1969477 0.3582607 #&gt; [,7] [,8] #&gt; [1,] 3.5383367 3.6335135 #&gt; [2,] -1.2662342 -1.3055166 #&gt; [3,] 0.4743011 0.5043502 head(f_theta) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] -273.8482 1.355410 1.297194 1.633802 2.046023 2.296554 2.389041 2.404144 #&gt; [2,] -209.0859 1.268192 1.216738 1.514575 1.863098 2.059505 2.126009 2.135969 #&gt; [3,] -190.3323 1.242916 1.193433 1.480136 1.810629 1.992095 2.051603 2.060202 #&gt; [4,] -177.1891 1.225196 1.177099 1.456024 1.774000 1.945197 1.999945 2.007625 #&gt; [5,] -148.5872 1.186618 1.141549 1.403631 1.694715 1.844154 1.888953 1.894730 #&gt; [6,] -119.9585 1.147980 1.105961 1.351301 1.615968 1.744450 1.779859 1.783866 # estimate sigma^2 sigma2 = 1 / (nrow(my_data) - 3) * (t(my_data$y - (f_theta[, ncol(f_theta)]))) %*% (my_data$y - (f_theta[, ncol(f_theta)])) # p = 3 sigma2 #&gt; [,1] #&gt; [1,] 0.0801686 After 8 iterations, my function has converged. And objective function value at convergence is sum((my_data$y - f_theta[,i])^2) #&gt; [1] 19.80165 and the parameters of \\(\\theta\\)s are theta_vec[,ncol(theta_vec)] #&gt; [1] 3.6335135 -1.3055166 0.5043502 and the asymptotic variance covariance matrix is as.numeric(sigma2)*as.matrix(solve(crossprod(F_theta_0))) #&gt; [,1] [,2] [,3] #&gt; [1,] 0.11552571 -0.04817428 0.02685848 #&gt; [2,] -0.04817428 0.02100861 -0.01158212 #&gt; [3,] 0.02685848 -0.01158212 0.00703916 Issue that I encounter in this problem was that it was very sensitive to starting values. when I tried the value of 1 for all \\(\\theta\\)s, I have vastly different parameter estimates. Then, I try to use the model interpretation to try to find reasonable starting values. Check with predefined function in nls nlin_4 = nls( y ~ mod_4(theta_0, theta_1, theta_2, x), start = list( theta_0 = -279.80879739 , theta_1 = 284.27659574 , theta_2 = 0.0119632 ), data = my_data ) nlin_4 #&gt; Nonlinear regression model #&gt; model: y ~ mod_4(theta_0, theta_1, theta_2, x) #&gt; data: my_data #&gt; theta_0 theta_1 theta_2 #&gt; 3.6359 -1.3064 0.5053 #&gt; residual sum-of-squares: 19.8 #&gt; #&gt; Number of iterations to convergence: 9 #&gt; Achieved convergence tolerance: 2.294e-07 References "],["generalized-linear-models.html", "Chapter 7 Generalized Linear Models", " Chapter 7 Generalized Linear Models Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model derived from the fact that we have \\(\\mathbf{x&#39;_i \\beta}\\) (which is linear form) in the model. "],["logistic-regression-1.html", "7.1 Logistic Regression", " 7.1 Logistic Regression \\[ p_i = f(\\mathbf{x}_i ; \\beta) = \\frac{exp(\\mathbf{x_i&#39;\\beta})}{1 + exp(\\mathbf{x_i&#39;\\beta})} \\] Equivalently, \\[ logit(p_i) = log(\\frac{p_i}{1+p_i}) = \\mathbf{x_i&#39;\\beta} \\] where \\(\\frac{p_i}{1+p_i}\\)is the odds. In this form, the model is specified such that a function of the mean response is linear. Hence, Generalized Linear Models The likelihood function \\[ L(p_i) = \\prod_{i=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i} \\] where \\(p_i = \\frac{\\mathbf{x&#39;_i \\beta}}{1+\\mathbf{x&#39;_i \\beta}}\\) and \\(1-p_i = (1+ exp(\\mathbf{x&#39;_i \\beta}))^{-1}\\) Hence, our objective function is \\[ Q(\\beta) = log(L(\\beta)) = \\sum_{i=1}^n Y_i \\mathbf{x&#39;_i \\beta} - \\sum_{i=1}^n log(1+ exp(\\mathbf{x&#39;_i \\beta})) \\] we could maximize this function numerically using the optimization method above, which allows us to find numerical MLE for \\(\\hat{\\beta}\\). Then we can use the standard asymptotic properties of MLEs to make inference. Property of MLEs is that parameters are asymptotically unbiased with sample variance-covariance matrix given by the inverse Fisher information matrix \\[ \\hat{\\beta} \\dot{\\sim} AN(\\beta,[\\mathbf{I}(\\beta)]^{-1}) \\] where the Fisher Information matrix, \\(\\mathbf{I}(\\beta)\\) is \\[ \\begin{aligned} \\mathbf{I}(\\beta) &amp;= E[\\frac{\\partial \\log(L(\\beta))}{\\partial (\\beta)}\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta&#39;}] \\\\ &amp;= E[(\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_i} \\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_j})_{ij}] \\end{aligned} \\] Under regularity conditions, this is equivalent to the negative of the expected value of the Hessian Matrix \\[ \\begin{aligned} \\mathbf{I}(\\beta) &amp;= -E[\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta \\partial \\beta&#39;}] \\\\ &amp;= -E[(\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta_i \\partial \\beta_j})_{ij}] \\end{aligned} \\] Example: \\[ x_i&#39; \\beta = \\beta_0 + \\beta_1 x_i \\] \\[ \\begin{aligned} - \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_0} &amp;= \\sum_{i=1}^n \\frac{\\exp(x&#39;_i \\beta)}{1 + \\exp(x&#39;_i \\beta)} - [\\frac{\\exp(x_i&#39; \\beta)}{1+ \\exp(x&#39;_i \\beta)}]^2 = \\sum_{i=1}^n p_i (1-p_i) \\\\ - \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_1} &amp;= \\sum_{i=1}^n \\frac{x_i^2\\exp(x&#39;_i \\beta)}{1 + \\exp(x&#39;_i \\beta)} - [\\frac{x_i\\exp(x_i&#39; \\beta)}{1+ \\exp(x&#39;_i \\beta)}]^2 = \\sum_{i=1}^n x_i^2p_i (1-p_i) \\\\ - \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta_0 \\partial \\beta_1} &amp;= \\sum_{i=1}^n \\frac{x_i\\exp(x&#39;_i \\beta)}{1 + \\exp(x&#39;_i \\beta)} - x_i[\\frac{\\exp(x_i&#39; \\beta)}{1+ \\exp(x&#39;_i \\beta)}]^2 = \\sum_{i=1}^n x_ip_i (1-p_i) \\end{aligned} \\] Hence, \\[ \\mathbf{I} (\\beta) = \\left[ \\begin{array} {cc} \\sum_i p_i(1-p_i) &amp; \\sum_i x_i p_i(1-p_i) \\\\ \\sum_i x_i p_i(1-p_i) &amp; \\sum_i x_i^2 p_i(1-p_i) \\end{array} \\right] \\] Inference Likelihood Ratio Tests To formulate the test, let \\(\\beta = [\\beta_1&#39;, \\beta_2&#39;]&#39;\\). If you are interested in testing a hypothesis about \\(\\beta_1\\), then we leave \\(\\beta_2\\) unspecified (called nuisance parameters). \\(\\beta_1\\) and \\(\\beta_2\\) can either a vector or scalar, or \\(\\beta_2\\) can be null. Example: \\(H_0: \\beta_1 = \\beta_{1,0}\\) (where \\(\\beta_{1,0}\\) is specified) and \\(\\hat{\\beta}_{2,0}\\) be the MLE of \\(\\beta_2\\) under the restriction that \\(\\beta_1 = \\beta_{1,0}\\). The likelihood ratio test statistic is \\[ -2\\log\\Lambda = -2[\\log(L(\\beta_{1,0},\\hat{\\beta}_{2,0})) - \\log(L(\\hat{\\beta}_1,\\hat{\\beta}_2))] \\] where the first term is the value fo the likelihood for the fitted restricted model the second term is the likelihood value of the fitted unrestricted model Under the null, \\[ -2 \\log \\Lambda \\sim \\chi^2_{\\upsilon} \\] where \\(\\upsilon\\) is the dimension of \\(\\beta_1\\) We reject the null when \\(-2\\log \\Lambda &gt; \\chi_{\\upsilon,1-\\alpha}^2\\) Wald Statistics Based on \\[ \\hat{\\beta} \\sim AN (\\beta, [\\mathbf{I}(\\beta)^{-1}]) \\] \\[ H_0: \\mathbf{L}\\hat{\\beta} = 0 \\] where \\(\\mathbf{L}\\) is a \\(q \\times p\\) matrix with \\(q\\) linearly independent rows. Then \\[ W = (\\mathbf{L\\hat{\\beta}})&#39;(\\mathbf{L[I(\\hat{\\beta})]^{-1}L&#39;})^{-1}(\\mathbf{L\\hat{\\beta}}) \\] under the null hypothesis Confidence interval \\[ \\hat{\\beta}_i \\pm 1.96 \\hat{s}_{ii}^2 \\] where \\(\\hat{s}_{ii}^2\\) is the i-th diagonal of \\(\\mathbf{[I(\\hat{\\beta})]}^{-1}\\) If you have large sample size, the likelihood ratio and Wald tests have similar results. small sample size, the likelihood ratio test is better. Logistic Regression: Interpretation of \\(\\beta\\) For single regressor, the model is \\[ logit\\{\\hat{p}_{x_i}\\} \\equiv logit (\\hat{p}_i) = \\log(\\frac{\\hat{p}_i}{1 - \\hat{p}_i}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] When \\(x= x_i + 1\\) \\[ logit\\{\\hat{p}_{x_i +1}\\} = \\hat{\\beta}_0 + \\hat{\\beta}(x_i + 1) = logit\\{\\hat{p}_{x_i}\\} + \\hat{\\beta}_1 \\] Then, \\[ \\begin{aligned} logit\\{\\hat{p}_{x_i +1}\\} - logit\\{\\hat{p}_{x_i}\\} &amp;= log\\{odds[\\hat{p}_{x_i +1}]\\} - log\\{odds[\\hat{p}_{x_i}]\\} \\\\ &amp;= log(\\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]}) = \\hat{\\beta}_1 \\end{aligned} \\] and \\[ exp(\\hat{\\beta}_1) = \\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]} \\] the estimated odds ratio the estimated odds ratio, when there is a difference of c units in the regressor x, is \\(exp(c\\hat{\\beta}_1)\\). When there are multiple covariates, \\(exp(\\hat{\\beta}_k)\\) is the estimated odds ratio for the variable \\(x_k\\), assuming that all of the other variables are held constant. Inference on the Mean Response Let \\(x_h = (1, x_{h1}, ...,x_{h,p-1})&#39;\\). Then \\[ \\hat{p}_h = \\frac{exp(\\mathbf{x&#39;_h \\hat{\\beta}})}{1 + exp(\\mathbf{x&#39;_h \\hat{\\beta}})} \\] and \\(s^2(\\hat{p}_h) = \\mathbf{x&#39;_h[I(\\hat{\\beta})]^{-1}x_h}\\) For new observation, we can have a cutoff point to decide whether y = 0 or 1. 7.1.1 Application library(kableExtra) library(dplyr) library(pscl) library(ggplot2) library(faraway) library(nnet) library(agridat) library(nlstools) Logistic Regression \\(x \\sim Unif(-0.5,2.5)\\). Then \\(\\eta = 0.5 + 0.75 x\\) set.seed(23) #set seed for reproducibility x &lt;- runif(1000, min = -0.5, max = 2.5) eta1 &lt;- 0.5 + 0.75 * x Passing \\(\\eta\\)’s into the inverse-logit function, we get \\[ p = \\frac{\\exp(\\eta)}{1+ \\exp(\\eta)} \\] where \\(p \\in [0,1]\\) Then, we generate \\(y \\sim Bernoulli(p)\\) p &lt;- exp(eta1) / (1 + exp(eta1)) y &lt;- rbinom(1000, 1, p) BinData &lt;- data.frame(X = x, Y = y) Model Fit Logistic_Model &lt;- glm(formula = Y ~ X, family = binomial, # family = specifies the response distribution data = BinData) summary(Logistic_Model) #&gt; #&gt; Call: #&gt; glm(formula = Y ~ X, family = binomial, data = BinData) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.2317 0.4153 0.5574 0.7922 1.1469 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.46205 0.10201 4.530 5.91e-06 *** #&gt; X 0.78527 0.09296 8.447 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 1106.7 on 999 degrees of freedom #&gt; Residual deviance: 1027.4 on 998 degrees of freedom #&gt; AIC: 1031.4 #&gt; #&gt; Number of Fisher Scoring iterations: 4 nlstools::confint2(Logistic_Model) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 0.2618709 0.6622204 #&gt; X 0.6028433 0.9676934 OddsRatio &lt;- coef(Logistic_Model) %&gt;% exp OddsRatio #&gt; (Intercept) X #&gt; 1.587318 2.192995 Based on the odds ratio, when \\(x = 0\\) , the odds of success of 1.59 \\(x = 1\\), the odds of success increase by a factor of 2.19 (i.e., 119.29% increase). Deviance Tests \\(H_0\\): No variables are related to the response (i.e., model with just the intercept) \\(H_1\\): At least one variable is related to the response Test_Dev &lt;- Logistic_Model$null.deviance - Logistic_Model$deviance p_val_dev &lt;- 1 - pchisq(q = Test_Dev, df = 1) Since we see the p-value of 0, we reject the null that no variables are related to the response Deviance residuals Logistic_Resids &lt;- residuals(Logistic_Model, type = &quot;deviance&quot;) plot( y = Logistic_Resids, x = BinData$X, xlab = &#39;X&#39;, ylab = &#39;Deviance Resids&#39; ) However, this plot is not informative. Hence, we can can see the residuals plots that are grouped into bins based on prediction values. plot_bin &lt;- function(Y, X, bins = 100, return.DF = FALSE) { Y_Name &lt;- deparse(substitute(Y)) X_Name &lt;- deparse(substitute(X)) Binned_Plot &lt;- data.frame(Plot_Y = Y, Plot_X = X) Binned_Plot$bin &lt;- cut(Binned_Plot$Plot_X, breaks = bins) %&gt;% as.numeric Binned_Plot_summary &lt;- Binned_Plot %&gt;% group_by(bin) %&gt;% summarise( Y_ave = mean(Plot_Y), X_ave = mean(Plot_X), Count = n() ) %&gt;% as.data.frame plot( y = Binned_Plot_summary$Y_ave, x = Binned_Plot_summary$X_ave, ylab = Y_Name, xlab = X_Name ) if (return.DF) return(Binned_Plot_summary) } plot_bin(Y = Logistic_Resids, X = BinData$X, bins = 100) We can also see the predicted value against the residuals. Logistic_Predictions &lt;- predict(Logistic_Model, type = &quot;response&quot;) plot_bin(Y = Logistic_Resids, X = Logistic_Predictions, bins = 100) We can also look at a binned plot of the logistic prediction versus the true category NumBins &lt;- 10 Binned_Data &lt;- plot_bin( Y = BinData$Y, X = Logistic_Predictions, bins = NumBins, return.DF = TRUE ) Binned_Data #&gt; bin Y_ave X_ave Count #&gt; 1 1 0.5833333 0.5382095 72 #&gt; 2 2 0.5200000 0.5795887 75 #&gt; 3 3 0.6567164 0.6156540 67 #&gt; 4 4 0.7014925 0.6579674 67 #&gt; 5 5 0.6373626 0.6984765 91 #&gt; 6 6 0.7500000 0.7373341 72 #&gt; 7 7 0.7096774 0.7786747 93 #&gt; 8 8 0.8503937 0.8203819 127 #&gt; 9 9 0.8947368 0.8601232 133 #&gt; 10 10 0.8916256 0.9004734 203 abline(0, 1, lty = 2, col = &#39;blue&#39;) Formal deviance test Hosmer-Lemeshow test Null hypothesis: the observed events match the expected evens \\[ X^2_{HL} = \\sum_{j=1}^{J} \\frac{(y_j - m_j \\hat{p}_j)^2}{m_j \\hat{p}_j(1-\\hat{p}_j)} \\] where within the j-th bin, \\(y_j\\) is the number of successes \\(m_j\\) = number of observations \\(\\hat{p}_j\\) = predicted probability Under the null hypothesis, \\(X^2_{HLL} \\sim \\chi^2_{J-1}\\) HL_BinVals &lt;- (Binned_Data$Count * Binned_Data$Y_ave - Binned_Data$Count * Binned_Data$X_ave) ^ 2 / Binned_Data$Count * Binned_Data$X_ave * (1 - Binned_Data$X_ave) HLpval &lt;- pchisq(q = sum(HL_BinVals), df = NumBins, lower.tail = FALSE) HLpval #&gt; [1] 0.9999989 Since \\(p\\)-value = 0.99, we do not reject the null hypothesis (i.e., the model is fitting well). "],["probit-regression.html", "7.2 Probit Regression", " 7.2 Probit Regression \\[ E(Y_i) = p_i = \\Phi(\\mathbf{x_i&#39;\\theta}) \\] where \\(\\Phi()\\) is the CDF of a \\(N(0,1)\\) random variable. Other models (e..g, t–distribution; log-log; I complimentary log-log) We let \\(Y_i = 1\\) success, \\(Y_i =0\\) no success. assume \\(Y \\sim Ber\\) and \\(p_i = P(Y_i =1)\\), the success probability. consider a logistic regression with the response function \\(logit(p_i) = x&#39;_i \\beta\\) Confusion matrix Predicted Truth 1 0 1 True Positive (TP) False Negative (FN) 0 False Positive (FP) True Negative (TN) Sensitivity: ability to identify positive results \\[ \\text{Sensitivity} = \\frac{TP}{TP + FN} \\] Specificity: ability to identify negative results \\[ \\text{Specificity} = \\frac{TN}{TN + FP} \\] False positive rate: Type I error (1- specificity) \\[ \\text{ False Positive Rate} = \\frac{FP}{TN+ FP} \\] False Negative Rate: Type II error (1-sensitivity) \\[ \\text{False Negative Rate} = \\frac{FN}{TP + FN} \\] Predicted Truth 1 0 1 Sensitivity False Negative Rate 0 False Positive Rate Specificity "],["binomial-regression.html", "7.3 Binomial Regression", " 7.3 Binomial Regression Binomial Here, cancer case = successes, and control case = failures. data(&quot;esoph&quot;) head(esoph, n = 3) #&gt; agegp alcgp tobgp ncases ncontrols #&gt; 1 25-34 0-39g/day 0-9g/day 0 40 #&gt; 2 25-34 0-39g/day 10-19 0 10 #&gt; 3 25-34 0-39g/day 20-29 0 6 plot( esoph$ncases / (esoph$ncases + esoph$ncontrols) ~ esoph$alcgp, ylab = &quot;Proportion&quot;, xlab = &#39;Alcohol consumption&#39;, main = &#39;Esophageal Cancer data&#39; ) class(esoph$agegp) &lt;- &quot;factor&quot; class(esoph$alcgp) &lt;- &quot;factor&quot; class(esoph$tobgp) &lt;- &quot;factor&quot; # only the alcohol consumption as a predictor model &lt;- glm(cbind(ncases, ncontrols) ~ alcgp, data = esoph, family = binomial) summary(model) #&gt; #&gt; Call: #&gt; glm(formula = cbind(ncases, ncontrols) ~ alcgp, family = binomial, #&gt; data = esoph) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.0759 -1.2037 -0.0183 1.0928 3.7336 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -2.5885 0.1925 -13.444 &lt; 2e-16 *** #&gt; alcgp40-79 1.2712 0.2323 5.472 4.46e-08 *** #&gt; alcgp80-119 2.0545 0.2611 7.868 3.59e-15 *** #&gt; alcgp120+ 3.3042 0.3237 10.209 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 367.95 on 87 degrees of freedom #&gt; Residual deviance: 221.46 on 84 degrees of freedom #&gt; AIC: 344.51 #&gt; #&gt; Number of Fisher Scoring iterations: 5 #Coefficient Odds coefficients(model) %&gt;% exp #&gt; (Intercept) alcgp40-79 alcgp80-119 alcgp120+ #&gt; 0.07512953 3.56527094 7.80261593 27.22570533 deviance(model)/df.residual(model) #&gt; [1] 2.63638 model$aic #&gt; [1] 344.5109 # alcohol consumption and age as predictors better_model &lt;- glm(cbind(ncases, ncontrols) ~ agegp + alcgp, data = esoph, family = binomial) summary(better_model) #&gt; #&gt; Call: #&gt; glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial, #&gt; data = esoph) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.2395 -0.7186 -0.2324 0.7930 3.3538 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -6.1472 1.0419 -5.900 3.63e-09 *** #&gt; agegp35-44 1.6311 1.0800 1.510 0.130973 #&gt; agegp45-54 3.4258 1.0389 3.297 0.000976 *** #&gt; agegp55-64 3.9435 1.0346 3.811 0.000138 *** #&gt; agegp65-74 4.3568 1.0413 4.184 2.87e-05 *** #&gt; agegp75+ 4.4242 1.0914 4.054 5.04e-05 *** #&gt; alcgp40-79 1.4343 0.2448 5.859 4.64e-09 *** #&gt; alcgp80-119 2.0071 0.2776 7.230 4.84e-13 *** #&gt; alcgp120+ 3.6800 0.3763 9.778 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 367.95 on 87 degrees of freedom #&gt; Residual deviance: 105.88 on 79 degrees of freedom #&gt; AIC: 238.94 #&gt; #&gt; Number of Fisher Scoring iterations: 6 better_model$aic #smaller AIC is better #&gt; [1] 238.9361 coefficients(better_model) %&gt;% exp #&gt; (Intercept) agegp35-44 agegp45-54 agegp55-64 agegp65-74 agegp75+ #&gt; 0.002139482 5.109601844 30.748594216 51.596634690 78.005283850 83.448437749 #&gt; alcgp40-79 alcgp80-119 alcgp120+ #&gt; 4.196747169 7.441782227 39.646885126 pchisq( q = model$deviance - better_model$deviance, df = model$df.residual - better_model$df.residual, lower = FALSE ) #&gt; [1] 2.713923e-23 # specify link function as probit Prob_better_model &lt;- glm( cbind(ncases, ncontrols) ~ agegp + alcgp, data = esoph, family = binomial(link = probit) ) summary(Prob_better_model) #&gt; #&gt; Call: #&gt; glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial(link = probit), #&gt; data = esoph) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.1325 -0.6877 -0.1661 0.7654 3.3258 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -3.3741 0.4922 -6.855 7.13e-12 *** #&gt; agegp35-44 0.8562 0.5081 1.685 0.092003 . #&gt; agegp45-54 1.7829 0.4904 3.636 0.000277 *** #&gt; agegp55-64 2.1034 0.4876 4.314 1.61e-05 *** #&gt; agegp65-74 2.3374 0.4930 4.741 2.13e-06 *** #&gt; agegp75+ 2.3694 0.5275 4.491 7.08e-06 *** #&gt; alcgp40-79 0.8080 0.1330 6.076 1.23e-09 *** #&gt; alcgp80-119 1.1399 0.1558 7.318 2.52e-13 *** #&gt; alcgp120+ 2.1204 0.2060 10.295 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 367.95 on 87 degrees of freedom #&gt; Residual deviance: 104.48 on 79 degrees of freedom #&gt; AIC: 237.53 #&gt; #&gt; Number of Fisher Scoring iterations: 6 "],["poisson-regression.html", "7.4 Poisson Regression", " 7.4 Poisson Regression From the Poisson distribution \\[ \\begin{aligned} f(Y_i) &amp;= \\frac{\\mu_i^{Y_i}exp(-\\mu_i)}{Y_i!}, Y_i = 0,1,.. \\\\ E(Y_i) &amp;= \\mu_i \\\\ var(Y_i) &amp;= \\mu_i \\end{aligned} \\] which is a natural distribution for counts. We can see that the variance is a function of the mean. If we let \\(\\mu_i = f(\\mathbf{x_i; \\theta})\\), it would be similar to Logistic Regression since we can choose \\(f()\\) as \\(\\mu_i = \\mathbf{x_i&#39;\\theta}, \\mu_i = \\exp(\\mathbf{x_i&#39;\\theta}), \\mu_i = \\log(\\mathbf{x_i&#39;\\theta})\\) 7.4.1 Application Count Data and Poisson regression data(bioChemists, package = &quot;pscl&quot;) bioChemists &lt;- bioChemists %&gt;% rename( Num_Article = art, #articles in last 3 years of PhD Sex = fem, #coded 1 if female Married = mar, #coded 1 if married Num_Kid5 = kid5, #number of childeren under age 6 PhD_Quality = phd, #prestige of PhD program Num_MentArticle = ment #articles by mentor in last 3 years ) hist(bioChemists$Num_Article, breaks = 25, main = &#39;Number of Articles&#39;) Poisson_Mod &lt;- glm(Num_Article ~ ., family=poisson, bioChemists) summary(Poisson_Mod) #&gt; #&gt; Call: #&gt; glm(formula = Num_Article ~ ., family = poisson, data = bioChemists) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.5672 -1.5398 -0.3660 0.5722 5.4467 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.304617 0.102981 2.958 0.0031 ** #&gt; SexWomen -0.224594 0.054613 -4.112 3.92e-05 *** #&gt; MarriedMarried 0.155243 0.061374 2.529 0.0114 * #&gt; Num_Kid5 -0.184883 0.040127 -4.607 4.08e-06 *** #&gt; PhD_Quality 0.012823 0.026397 0.486 0.6271 #&gt; Num_MentArticle 0.025543 0.002006 12.733 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1) #&gt; #&gt; Null deviance: 1817.4 on 914 degrees of freedom #&gt; Residual deviance: 1634.4 on 909 degrees of freedom #&gt; AIC: 3314.1 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Residual of 1634 with 909 df isn’t great. We see Pearson \\(\\chi^2\\) Predicted_Means &lt;- predict(Poisson_Mod,type = &quot;response&quot;) X2 &lt;- sum((bioChemists$Num_Article - Predicted_Means)^2/Predicted_Means) X2 #&gt; [1] 1662.547 pchisq(X2,Poisson_Mod$df.residual, lower.tail = FALSE) #&gt; [1] 7.849882e-47 With interaction terms, there are some improvements Poisson_Mod_All2way &lt;- glm(Num_Article ~ .^2, family=poisson, bioChemists) Poisson_Mod_All3way &lt;- glm(Num_Article ~ .^3, family=poisson, bioChemists) Consider the \\(\\hat{\\phi} = \\frac{\\text{deviance}}{df}\\) Poisson_Mod$deviance / Poisson_Mod$df.residual #&gt; [1] 1.797988 This is evidence for over-dispersion. Likely cause is missing variables. And remedies could either be to include more variables or consider random effects. A quick fix is to force the Poisson Regression to include this value of \\(\\phi\\), and this model is called “Quasi-Poisson”. phi_hat = Poisson_Mod$deviance/Poisson_Mod$df.residual summary(Poisson_Mod,dispersion = phi_hat) #&gt; #&gt; Call: #&gt; glm(formula = Num_Article ~ ., family = poisson, data = bioChemists) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.5672 -1.5398 -0.3660 0.5722 5.4467 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.30462 0.13809 2.206 0.02739 * #&gt; SexWomen -0.22459 0.07323 -3.067 0.00216 ** #&gt; MarriedMarried 0.15524 0.08230 1.886 0.05924 . #&gt; Num_Kid5 -0.18488 0.05381 -3.436 0.00059 *** #&gt; PhD_Quality 0.01282 0.03540 0.362 0.71715 #&gt; Num_MentArticle 0.02554 0.00269 9.496 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for poisson family taken to be 1.797988) #&gt; #&gt; Null deviance: 1817.4 on 914 degrees of freedom #&gt; Residual deviance: 1634.4 on 909 degrees of freedom #&gt; AIC: 3314.1 #&gt; #&gt; Number of Fisher Scoring iterations: 5 Or directly rerun the model as quasiPoisson_Mod &lt;- glm(Num_Article ~ ., family=quasipoisson, bioChemists) Quasi-Poisson is not recommended, but Negative Binomial Regression that has an extra parameter to account for over-dispersion is. "],["negative-binomial-regression.html", "7.5 Negative Binomial Regression", " 7.5 Negative Binomial Regression library(MASS) NegBinom_Mod &lt;- MASS::glm.nb(Num_Article ~ .,bioChemists) summary(NegBinom_Mod) #&gt; #&gt; Call: #&gt; MASS::glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, #&gt; link = log) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.1678 -1.3617 -0.2806 0.4476 3.4524 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.256144 0.137348 1.865 0.062191 . #&gt; SexWomen -0.216418 0.072636 -2.979 0.002887 ** #&gt; MarriedMarried 0.150489 0.082097 1.833 0.066791 . #&gt; Num_Kid5 -0.176415 0.052813 -3.340 0.000837 *** #&gt; PhD_Quality 0.015271 0.035873 0.426 0.670326 #&gt; Num_MentArticle 0.029082 0.003214 9.048 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1) #&gt; #&gt; Null deviance: 1109.0 on 914 degrees of freedom #&gt; Residual deviance: 1004.3 on 909 degrees of freedom #&gt; AIC: 3135.9 #&gt; #&gt; Number of Fisher Scoring iterations: 1 #&gt; #&gt; #&gt; Theta: 2.264 #&gt; Std. Err.: 0.271 #&gt; #&gt; 2 x log-likelihood: -3121.917 We can see the dispersion is 2.264 with SE = 0.271, which is significantly different from 1, indicating over-dispersion. Check Over-Dispersion for more detail "],["multinomial.html", "7.6 Multinomial", " 7.6 Multinomial If we have more than two categories or groups that we want to model relative to covariates (e.g., we have observations \\(i = 1,…,n\\) and groups/ covariates \\(j = 1,2,…,J\\)), multinomial is our candidate model Let \\(p_{ij}\\) be the probability that the i-th observation belongs to the j-th group \\(Y_{ij}\\) be the number of observations for individual i in group j; An individual will have observations \\(Y_{i1},Y_{i2},…Y_{iJ}\\) assume the probability of observing this response is given by a multinomial distribution in terms of probabilities \\(p_{ij}\\), where \\(\\sum_{j = 1}^J p_{ij} = 1\\) . For interpretation, we have a baseline category \\(p_{i1} = 1 - \\sum_{j = 2}^J p_{ij}\\) The link between the mean response (probability) \\(p_{ij}\\) and a linear function of the covariates \\[ \\eta_{ij} = \\mathbf{x&#39;_i \\beta_j} = \\log \\frac{p_{ij}}{p_{i1}}, j = 2,..,J \\] We compare \\(p_{ij}\\) to the baseline \\(p_{i1}\\), suggesting \\[ p_{ij} = \\frac{\\exp(\\eta_{ij})}{1 + \\sum_{i=2}^J \\exp(\\eta_{ij})} \\] which is known as multinomial logistic model. Note: Softmax coding for multinomial logistic regression: rather than selecting a baseline class, we treat all \\(K\\) class symmetrically - equally important (no baseline). \\[ P(Y = k | X = x) = \\frac{exp(\\beta_{k1} + \\dots + \\beta_{k_p x_p})}{\\sum_{l = 1}^K exp(\\beta_{l0} + \\dots + \\beta_{l_p x_p})} \\] then the log odds ratio between k-th and k’-th classes is \\[ \\log (\\frac{P(Y=k|X=x)}{P(Y = k&#39; | X=x)}) = (\\beta_{k0} - \\beta_{k&#39;0}) + \\dots + (\\beta_{kp} - \\beta_{k&#39;p}) x_p \\] library(faraway) library(dplyr) data(nes96, package=&quot;faraway&quot;) head(nes96,3) #&gt; popul TVnews selfLR ClinLR DoleLR PID age educ income vote #&gt; 1 0 7 extCon extLib Con strRep 36 HS $3Kminus Dole #&gt; 2 190 1 sliLib sliLib sliCon weakDem 20 Coll $3Kminus Clinton #&gt; 3 31 7 Lib Lib Con weakDem 24 BAdeg $3Kminus Clinton We try to understand their political strength table(nes96$PID) #&gt; #&gt; strDem weakDem indDem indind indRep weakRep strRep #&gt; 200 180 108 37 94 150 175 nes96$Political_Strength &lt;- NA nes96$Political_Strength[nes96$PID %in% c(&quot;strDem&quot;, &quot;strRep&quot;)] &lt;- &quot;Strong&quot; nes96$Political_Strength[nes96$PID %in% c(&quot;weakDem&quot;, &quot;weakRep&quot;)] &lt;- &quot;Weak&quot; nes96$Political_Strength[nes96$PID %in% c(&quot;indDem&quot;, &quot;indind&quot;, &quot;indRep&quot;)] &lt;- &quot;Neutral&quot; nes96 %&gt;% group_by(Political_Strength) %&gt;% summarise(Count = n()) #&gt; # A tibble: 3 × 2 #&gt; Political_Strength Count #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Neutral 239 #&gt; 2 Strong 375 #&gt; 3 Weak 330 visualize the political strength variable library(ggplot2) Plot_DF &lt;- nes96 %&gt;% mutate(Age_Grp = cut_number(age, 4)) %&gt;% group_by(Age_Grp, Political_Strength) %&gt;% summarise(count = n()) %&gt;% group_by(Age_Grp) %&gt;% mutate(etotal = sum(count), proportion = count / etotal) Age_Plot &lt;- ggplot( Plot_DF, aes( x = Age_Grp, y = proportion, group = Political_Strength, linetype = Political_Strength, color = Political_Strength ) ) + geom_line(size = 2) Age_Plot Fit the multinomial logistic model: model political strength as a function of age and education library(nnet) Multinomial_Model &lt;- multinom(Political_Strength ~ age + educ, nes96, trace = F) summary(Multinomial_Model) #&gt; Call: #&gt; multinom(formula = Political_Strength ~ age + educ, data = nes96, #&gt; trace = F) #&gt; #&gt; Coefficients: #&gt; (Intercept) age educ.L educ.Q educ.C educ^4 #&gt; Strong -0.08788729 0.010700364 -0.1098951 -0.2016197 -0.1757739 -0.02116307 #&gt; Weak 0.51976285 -0.004868771 -0.1431104 -0.2405395 -0.2411795 0.18353634 #&gt; educ^5 educ^6 #&gt; Strong -0.1664377 -0.1359449 #&gt; Weak -0.1489030 -0.2173144 #&gt; #&gt; Std. Errors: #&gt; (Intercept) age educ.L educ.Q educ.C educ^4 #&gt; Strong 0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776 #&gt; Weak 0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149 #&gt; educ^5 educ^6 #&gt; Strong 0.2515012 0.2166774 #&gt; Weak 0.2643747 0.2199186 #&gt; #&gt; Residual Deviance: 2024.596 #&gt; AIC: 2056.596 Alternatively, stepwise model selection based AIC Multinomial_Step &lt;- step(Multinomial_Model,trace = 0) #&gt; trying - age #&gt; trying - educ #&gt; trying - age Multinomial_Step #&gt; Call: #&gt; multinom(formula = Political_Strength ~ age, data = nes96, trace = F) #&gt; #&gt; Coefficients: #&gt; (Intercept) age #&gt; Strong -0.01988977 0.009832916 #&gt; Weak 0.59497046 -0.005954348 #&gt; #&gt; Residual Deviance: 2030.756 #&gt; AIC: 2038.756 compare the best model to the full model based on deviance pchisq(q = deviance(Multinomial_Step) - deviance(Multinomial_Model), df = Multinomial_Model$edf-Multinomial_Step$edf,lower=F) #&gt; [1] 0.9078172 We see no significant difference Plot of the fitted model PlotData &lt;- data.frame(age = seq(from = 19, to = 91)) Preds &lt;- PlotData %&gt;% bind_cols(data.frame(predict( object = Multinomial_Step, PlotData, type = &quot;probs&quot; ))) plot( x = Preds$age, y = Preds$Neutral, type = &quot;l&quot;, ylim = c(0.2, 0.6), col = &quot;black&quot;, ylab = &quot;Proportion&quot;, xlab = &quot;Age&quot; ) lines(x = Preds$age, y = Preds$Weak, col = &quot;blue&quot;) lines(x = Preds$age, y = Preds$Strong, col = &quot;red&quot;) legend( &#39;topleft&#39;, legend = c(&#39;Neutral&#39;, &#39;Weak&#39;, &#39;Strong&#39;), col = c(&#39;black&#39;, &#39;blue&#39;, &#39;red&#39;), lty = 1 ) predict(Multinomial_Step,data.frame(age = 34)) # predicted result (categoriy of political strength) of 34 year old #&gt; [1] Weak #&gt; Levels: Neutral Strong Weak predict(Multinomial_Step,data.frame(age = c(34,35)),type=&quot;probs&quot;) # predicted result of the probabilities of each level of political strength for a 34 and 35 #&gt; Neutral Strong Weak #&gt; 1 0.2597275 0.3556910 0.3845815 #&gt; 2 0.2594080 0.3587639 0.3818281 If categories are ordered (i.e., ordinal data), we must use another approach (still multinomial, but use cumulative probabilities). Another example library(agridat) dat &lt;- agridat::streibig.competition # See Schaberger and Pierce, pages 370+ # Consider only the mono-species barley data (no competition from Sinapis) gammaDat &lt;- subset(dat, sseeds &lt; 1) gammaDat &lt;- transform(gammaDat, x = bseeds, y = bdwt, block = factor(block)) # Inverse yield looks like it will be a good fit for Gamma&#39;s inverse link ggplot(gammaDat, aes(x = x, y = 1 / y)) + geom_point(aes(color = block, shape = block)) + xlab(&#39;Seeding Rate&#39;) + ylab(&#39;Inverse yield&#39;) + ggtitle(&#39;Streibig Competion - Barley only&#39;) \\[ Y \\sim Gamma \\] because Gamma is non-negative as opposed to Normal. The canonical Gamma link function is the inverse (or reciprocal) link \\[ \\begin{aligned} \\eta_{ij} &amp;= \\beta_{0j} + \\beta_{1j}x_{ij} + \\beta_2x_{ij}^2 \\\\ Y_{ij} &amp;= \\eta_{ij}^{-1} \\end{aligned} \\] The linear predictor is a quadratic model fit to each of the j-th blocks. A different model (not fitted) could be one with common slopes: glm(y ~ x + I(x^2),…) # linear predictor is quadratic, with separate intercept and slope per block m1 &lt;- glm(y ~ block + block * x + block * I(x ^ 2), data = gammaDat, family = Gamma(link = &quot;inverse&quot;)) summary(m1) #&gt; #&gt; Call: #&gt; glm(formula = y ~ block + block * x + block * I(x^2), family = Gamma(link = &quot;inverse&quot;), #&gt; data = gammaDat) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.21708 -0.44148 0.02479 0.17999 0.80745 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.115e-01 2.870e-02 3.886 0.000854 *** #&gt; blockB2 -1.208e-02 3.880e-02 -0.311 0.758630 #&gt; blockB3 -2.386e-02 3.683e-02 -0.648 0.524029 #&gt; x -2.075e-03 1.099e-03 -1.888 0.072884 . #&gt; I(x^2) 1.372e-05 9.109e-06 1.506 0.146849 #&gt; blockB2:x 5.198e-04 1.468e-03 0.354 0.726814 #&gt; blockB3:x 7.475e-04 1.393e-03 0.537 0.597103 #&gt; blockB2:I(x^2) -5.076e-06 1.184e-05 -0.429 0.672475 #&gt; blockB3:I(x^2) -6.651e-06 1.123e-05 -0.592 0.560012 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for Gamma family taken to be 0.3232083) #&gt; #&gt; Null deviance: 13.1677 on 29 degrees of freedom #&gt; Residual deviance: 7.8605 on 21 degrees of freedom #&gt; AIC: 225.32 #&gt; #&gt; Number of Fisher Scoring iterations: 5 For predict new value of \\(x\\) newdf &lt;- expand.grid(x = seq(0, 120, length = 50), block = factor(c(&#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;))) newdf$pred &lt;- predict(m1, new = newdf, type = &#39;response&#39;) ggplot(gammaDat, aes(x = x, y = y)) + geom_point(aes(color = block, shape = block)) + xlab(&#39;Seeding Rate&#39;) + ylab(&#39;Inverse yield&#39;) + ggtitle(&#39;Streibig Competion - Barley only Predictions&#39;) + geom_line(data = newdf, aes( x = x, y = pred, color = block, linetype = block )) "],["generalization.html", "7.7 Generalization", " 7.7 Generalization We can see that Poisson regression looks similar to logistic regression. Hence, we can generalize to a class of modeling. Thanks to Nelder and Wedderburn (1972), we have the generalized linear models (GLMs). Estimation is generalize in these models. Exponential Family The theory of GLMs is developed for data with distribution given y the exponential family. The form of the data distribution that is useful for GLMs is \\[ f(y;\\theta, \\phi) = \\exp(\\frac{\\theta y - b(\\theta)}{a(\\phi)} + c(y, \\phi)) \\] where \\(\\theta\\) is called the natural parameter \\(\\phi\\) is called the dispersion parameter Note: This family includes the [Gamma], [Normal], [Poisson], and other. For all parameterization of the exponential family, check this link Example if we have \\(Y \\sim N(\\mu, \\sigma^2)\\) \\[ \\begin{aligned} f(y; \\mu, \\sigma^2) &amp;= \\frac{1}{(2\\pi \\sigma^2)^{1/2}}\\exp(-\\frac{1}{2\\sigma^2}(y- \\mu)^2) \\\\ &amp;= \\exp(-\\frac{1}{2\\sigma^2}(y^2 - 2y \\mu +\\mu^2)- \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\ &amp;= \\exp(\\frac{y \\mu - \\mu^2/2}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\ &amp;= \\exp(\\frac{\\theta y - b(\\theta)}{a(\\phi)} + c(y , \\phi)) \\end{aligned} \\] where \\(\\theta = \\mu\\) \\(b(\\theta) = \\frac{\\mu^2}{2}\\) \\(a(\\phi) = \\sigma^2 = \\phi\\) \\(c(y , \\phi) = - \\frac{1}{2}(\\frac{y^2}{\\phi}+\\log(2\\pi \\sigma^2))\\) Properties of GLM exponential families \\(E(Y) = b&#39; (\\theta)\\) where \\(b&#39;(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\) (here ' is “prime”, not transpose) \\(var(Y) = a(\\phi)b&#39;&#39;(\\theta)= a(\\phi)V(\\mu)\\). \\(V(\\mu)\\) is the variance function; however, it is only the variance in the case that \\(a(\\phi) =1\\) If \\(a(), b(), c()\\) are identifiable, we will derive expected value and variance of Y. Example Normal distribution \\[ \\begin{aligned} b&#39;(\\theta) &amp;= \\frac{\\partial b(\\mu^2/2)}{\\partial \\mu} = \\mu \\\\ V(\\mu) &amp;= \\frac{\\partial^2 (\\mu^2/2)}{\\partial \\mu^2} = 1 \\\\ \\to var(Y) &amp;= a(\\phi) = \\sigma^2 \\end{aligned} \\] Poisson distribution \\[ \\begin{aligned} f(y, \\theta, \\phi) &amp;= \\frac{\\mu^y \\exp(-\\mu)}{y!} \\\\ &amp;= \\exp(y\\log(\\mu) - \\mu - \\log(y!)) \\\\ &amp;= \\exp(y\\theta - \\exp(\\theta) - \\log(y!)) \\end{aligned} \\] where \\(\\theta = \\log(\\mu)\\) \\(a(\\phi) = 1\\) \\(b(\\theta) = \\exp(\\theta)\\) \\(c(y, \\phi) = \\log(y!)\\) Hence, \\[ \\begin{aligned} E(Y) = \\frac{\\partial b(\\theta)}{\\partial \\theta} = \\exp(\\theta) &amp;= \\mu \\\\ var(Y) = \\frac{\\partial^2 b(\\theta)}{\\partial \\theta^2} &amp;= \\mu \\end{aligned} \\] Since \\(\\mu = E(Y) = b&#39;(\\theta)\\) In GLM, we take some monotone function (typically nonlinear) of \\(\\mu\\) to be linear in the set of covariates \\[ g(\\mu) = g(b&#39;(\\theta)) = \\mathbf{x&#39;\\beta} \\] Equivalently, \\[ \\mu = g^{-1}(\\mathbf{x&#39;\\beta}) \\] where \\(g(.)\\) is the link function since it links mean response (\\(\\mu = E(Y)\\)) and a linear expression of the covariates Some people use \\(\\eta = \\mathbf{x&#39;\\beta}\\) where \\(\\eta\\) = the “linear predictor” GLM is composed of 2 components The random component: is the distribution chosen to model the response variables \\(Y_1,...,Y_n\\) is specified by the choice fo \\(a(), b(), c()\\) in the exponential form Notation: Assume that there are n independent response variables \\(Y_1,...,Y_n\\) with densities \\[ f(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{a(\\phi)}+ c(y_i, \\phi)) \\] notice each observation might have different densities Assume that \\(\\phi\\) is constant for all \\(i = 1,...,n\\), but \\(\\theta_i\\) will vary. \\(\\mu_i = E(Y_i)\\) for all i. The systematic component is the portion of the model that gives the relation between \\(\\mu\\) and the covariates \\(\\mathbf{x}\\) consists of 2 parts: the link function, \\(g(.)\\) the linear predictor, \\(\\eta = \\mathbf{x&#39;\\beta}\\) Notation: assume \\(g(\\mu_i) = \\mathbf{x&#39;\\beta} = \\eta_i\\) where \\(\\mathbf{\\beta} = (\\beta_1,..., \\beta_p)&#39;\\) The parameters to be estimated are \\(\\beta_1,...\\beta_p , \\phi\\) The Canonical Link To choose \\(g(.)\\), we can use canonical link function (Remember: Canonical link is just a special case of the link function) If the link function \\(g(.)\\) is such \\(g(\\mu_i) = \\eta_i = \\theta_i\\), the natural parameter, then \\(g(.)\\) is the canonical link. \\(b(\\theta)\\) = cumulant moment generating function \\(g(\\mu)\\) is the link function, which relates the linear predictor to the mean and is required to be monotone increasing, continuously differentiable and invertible. Equivalently, we can think of canonical link function as \\[ \\gamma^{-1} \\circ g^{-1} = I \\] which is the identity. Hence, \\[ \\theta = \\eta \\] The inverse link \\(g^{-1}(.)\\) is also known as the mean function, take linear predictor output (ranging from \\(-\\infty\\) to \\(\\infty\\)) and transform it into a different scale. Exponential: converts \\(\\mathbf{\\beta X}\\) into a curve that is restricted between 0 and \\(\\infty\\) (which you can see that is useful in case you want to convert a linear predictor into a non-negative value). \\(\\lambda = \\exp(y) = \\mathbf{\\beta X}\\) Inverse Logit (also known as logistic): converts \\(\\mathbf{\\beta X}\\) into a curve that is restricted between 0 and 1, which is useful in case you want to convert a linear predictor to a probability. \\(\\theta = \\frac{1}{1 + \\exp(-y)} = \\frac{1}{1 + \\exp(- \\mathbf{\\beta X})}\\) \\(y\\) = linear predictor value \\(\\theta\\) = transformed value The identity link is that \\[ \\begin{aligned} \\eta_i &amp;= g(\\mu_i) = \\mu_i \\\\ \\mu_i &amp;= g^{-1}(\\eta_i) = \\eta_i \\end{aligned} \\] Table 15.1 Generalized Linear Models 15.1 the Structure of Generalized Linear Models More example on the link functions and their inverses can be found on page 380 Example Normal random component Mean Response: \\(\\mu_i = \\theta_i\\) Canonical Link: \\(g( \\mu_i) = \\mu_i\\) (the identity link) Binomial random component Mean Response: \\(\\mu_i = \\frac{n_i \\exp( \\theta)}{1+\\exp (\\theta_i)}\\) and \\(\\theta(\\mu_i) = \\log(\\frac{p_i }{1-p_i}) = \\log (\\frac{\\mu_i} {n_i - \\mu_i})\\) Canonical link: \\(g(\\mu_i) = \\log(\\frac{\\mu_i} {n_i - \\mu_i})\\) (logit link) Poisson random component Mean Response: \\(\\mu_i = \\exp(\\theta_i)\\) Canonical Link: \\(g(\\mu_i) = \\log(\\mu_i)\\) Gamma random component: Mean response: \\(\\mu_i = -\\frac{1}{\\theta_i}\\) and \\(\\theta(\\mu_i) = - \\mu_i^{-1}\\) Canonical Link: \\(g(\\mu\\_i) = - \\frac{1}{\\mu_i}\\) Inverse Gaussian random Canonical Link: \\(g(\\mu_i) = \\frac{1}{\\mu_i^2}\\) 7.7.1 Estimation MLE for parameters of the systematic component (\\(\\beta\\)) Unification of derivation and computation (thanks to the exponential forms) No unification for estimation of the dispersion parameter (\\(\\phi\\)) 7.7.1.1 Estimation of \\(\\beta\\) We have \\[ \\begin{aligned} f(y_i ; \\theta_i, \\phi) &amp;= \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{a(\\phi)}+ c(y_i, \\phi)) \\\\ E(Y_i) &amp;= \\mu_i = b&#39;(\\theta) \\\\ var(Y_i) &amp;= b&#39;&#39;(\\theta)a(\\phi) = V(\\mu_i)a(\\phi) \\\\ g(\\mu_i) &amp;= \\mathbf{x}_i&#39;\\beta = \\eta_i \\end{aligned} \\] If the log-likelihood for a single observation is \\(l_i (\\beta,\\phi)\\). The log-likelihood for all n observations is \\[ \\begin{aligned} l(\\beta,\\phi) &amp;= \\sum_{i=1}^n l_i (\\beta,\\phi) \\\\ &amp;= \\sum_{i=1}^n (\\frac{\\theta_i y_i - b(\\theta_i)}{a(\\phi)}+ c(y_i, \\phi)) \\end{aligned} \\] Using MLE to find \\(\\beta\\), we use the chain rule to get the derivatives \\[ \\begin{aligned} \\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} &amp;= \\frac{\\partial l_i (\\beta, \\phi)}{\\partial \\theta_i} \\times \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i}\\times \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\\\ &amp;= \\sum_{i=1}^{n}(\\frac{ y_i - \\mu_i}{a(\\phi)} \\times \\frac{1}{V(\\mu_i)} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij}) \\end{aligned} \\] If we let \\[ w_i \\equiv ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1} \\] Then, \\[ \\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} = \\sum_{i=1}^n (\\frac{y_i \\mu_i}{a(\\phi)} \\times w_i \\times \\frac{\\partial \\eta_i}{\\partial \\mu_i} \\times x_{ij}) \\] We can also get the second derivatives using the chain rule. Example: For the \\[Newton-Raphson\\] algorithm, we need \\[ - E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k}) \\] where \\((j,k)\\)-th element of the Fisher information matrix \\(\\mathbf{I}(\\beta)\\) Hence, \\[ - E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k}) = \\sum_{i=1}^n \\frac{w_i}{a(\\phi)}x_{ij}x_{ik} \\] for the (j,k)th element If Bernoulli model with logit link function (which is the canonical link) \\[ \\begin{aligned} b(\\theta) &amp;= \\log(1 + \\exp(\\theta)) = \\log(1 + \\exp(\\mathbf{x&#39;\\beta})) \\\\ a(\\phi) &amp;= 1 \\\\ c(y_i, \\phi) &amp;= 0 \\\\ E(Y) = b&#39;(\\theta) &amp;= \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} = \\mu = p \\\\ \\eta = g(\\mu) &amp;= \\log(\\frac{\\mu}{1-\\mu}) = \\theta = \\log(\\frac{p}{1-p}) = \\mathbf{x&#39;\\beta} \\end{aligned} \\] For \\(Y_i\\), i = 1,.., the log-likelihood is \\[ l_i (\\beta, \\phi) = \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i, \\phi) = y_i \\mathbf{x}&#39;_i \\beta - \\log(1+ \\exp(\\mathbf{x&#39;\\beta})) \\] Additionally, \\[ \\begin{aligned} V(\\mu_i) &amp;= \\mu_i(1-\\mu_i)= p_i (1-p_i) \\\\ \\frac{\\partial \\mu_i}{\\partial \\eta_i} &amp;= p_i(1-p_i) \\end{aligned} \\] Hence, \\[ \\begin{aligned} \\frac{\\partial l(\\beta, \\phi)}{\\partial \\beta_j} &amp;= \\sum_{i=1}^n[\\frac{y_i - \\mu_i}{a(\\phi)} \\times \\frac{1}{V(\\mu_i)}\\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij}] \\\\ &amp;= \\sum_{i=1}^n (y_i - p_i) \\times \\frac{1}{p_i(1-p_i)} \\times p_i(1-p_i) \\times x_{ij} \\\\ &amp;= \\sum_{i=1}^n (y_i - p_i) x_{ij} \\\\ &amp;= \\sum_{i=1}^n (y_i - \\frac{\\exp(\\mathbf{x&#39;_i\\beta})}{1+ \\exp(\\mathbf{x&#39;_i\\beta})})x_{ij} \\end{aligned} \\] then \\[ w_i = ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1} = p_i (1-p_i) \\] \\[ \\mathbf{I}_{jk}(\\mathbf{\\beta}) = \\sum_{i=1}^n \\frac{w_i}{a(\\phi)} x_{ij}x_{ik} = \\sum_{i=1}^n p_i (1-p_i)x_{ij}x_{ik} \\] The Fisher-scoring algorithm for the MLE of \\(\\mathbf{\\beta}\\) is \\[ \\left( \\begin{array} {c} \\beta_1 \\\\ \\beta_2 \\\\ . \\\\ . \\\\ . \\\\ \\beta_p \\\\ \\end{array} \\right)^{(m+1)} = \\left( \\begin{array} {c} \\beta_1 \\\\ \\beta_2 \\\\ . \\\\ . \\\\ . \\\\ \\beta_p \\\\ \\end{array} \\right)^{(m)} + \\mathbf{I}^{-1}(\\mathbf{\\beta}) \\left( \\begin{array} {c} \\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_1} \\\\ \\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_2} \\\\ . \\\\ . \\\\ . \\\\ \\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_p} \\\\ \\end{array} \\right)|_{\\beta = \\beta^{(m)}} \\] Similar to \\[Newton-Raphson\\] expect the matrix of second derivatives by the expected value of the second derivative matrix. In matrix notation, \\[ \\begin{aligned} \\frac{\\partial l }{\\partial \\beta} &amp;= \\frac{1}{a(\\phi)}\\mathbf{X&#39;W\\Delta(y - \\mu)} \\\\ &amp;= \\frac{1}{a(\\phi)}\\mathbf{F&#39;V^{-1}(y - \\mu)} \\\\ \\end{aligned} \\] \\[ \\mathbf{I}(\\beta) = \\frac{1}{a(\\phi)}\\mathbf{X&#39;WX} = \\frac{1}{a(\\phi)}\\mathbf{F&#39;V^{-1}F} \\] where \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix of covariates \\(\\mathbf{W}\\) is an \\(n \\times n\\) diagonal matrix with \\((i,i)\\)-th element given by \\(w_i\\) \\(\\mathbf{\\Delta}\\) an \\(n \\times n\\) diagonal matrix with \\((i,i)\\)-th element given by \\(\\frac{\\partial \\eta_i}{\\partial \\mu_i}\\) \\(\\mathbf{F} = \\mathbf{\\frac{\\partial \\mu}{\\partial \\beta}}\\) an \\(n \\times p\\) matrix with \\(i\\)-th row \\(\\frac{\\partial \\mu_i}{\\partial \\beta} = (\\frac{\\partial \\mu_i}{\\partial \\eta_i})\\mathbf{x}&#39;_i\\) \\(\\mathbf{V}\\) an \\(n \\times n\\) diagonal matrix with \\((i,i)\\)-th element given by \\(V(\\mu_i)\\) Setting the derivative of the log-likelihood equal to 0, ML estimating equations are \\[ \\mathbf{F&#39;V^{-1}y= F&#39;V^{-1}\\mu} \\] where all components of this equation expect y depends on the parameters \\(\\beta\\) Special Cases If one has a canonical link, the estimating equations reduce to \\[ \\mathbf{X&#39;y= X&#39;\\mu} \\] If one has an identity link, then \\[ \\mathbf{X&#39;V^{-1}y = X&#39;V^{-1}X\\hat{\\beta}} \\] which gives the generalized least squares estimator Generally, we can rewrite the Fisher-scoring algorithm as \\[ \\beta^{(m+1)} = \\beta^{(m)} + \\mathbf{(\\hat{F}&#39;\\hat{V}^{-1}\\hat{F})^{-1}\\hat{F}&#39;\\hat{V}^{-1}(y- \\hat{\\mu})} \\] Since \\(\\hat{F},\\hat{V}, \\hat{\\mu}\\) depend on \\(\\beta\\), we evaluate at \\(\\beta^{(m)}\\) From starting values \\(\\beta^{(0)}\\), we can iterate until convergence. Notes: if \\(a(\\phi)\\) is a constant or of the form \\(m_i \\phi\\) with known \\(m_i\\), then \\(\\phi\\) cancels. 7.7.1.2 Estimation of \\(\\phi\\) 2 approaches: MLE \\[ \\frac{\\partial l_i}{\\partial \\phi} = \\frac{(\\theta_i y_i - b(\\theta_i)a&#39;(\\phi))}{a^2(\\phi)} + \\frac{\\partial c(y_i,\\phi)}{\\partial \\phi} \\] the MLE of \\(\\phi\\) solves \\[ \\frac{a^2(\\phi)}{a&#39;(\\phi)}\\sum_{i=1}^n \\frac{\\partial c(y_i, \\phi)}{\\partial \\phi} = \\sum_{i=1}^n(\\theta_i y_i - b(\\theta_i)) \\] Situation others than normal error case, expression for \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) are not simple Even for the canonical link and \\(a(\\phi)\\) constant, there is no nice general expression for \\(-E(\\frac{\\partial^2 l}{\\partial \\phi^2})\\), so the unification GLMs provide for estimation of \\(\\beta\\) breaks down for \\(\\phi\\) Moment Estimation (“Bias Corrected \\(\\chi^2\\)”) The MLE is not conventional approach to estimation of \\(\\phi\\) in GLMS. For the exponential family \\(var(Y) =V(\\mu)a(\\phi)\\). This implies \\[ \\begin{aligned} a(\\phi) &amp;= \\frac{var(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\ a(\\hat{\\phi}) &amp;= \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu})} \\end{aligned} \\] where \\(p\\) is the dimension of \\(\\beta\\) GLM with canonical link function \\(g(.)= (b&#39;(.))^{-1}\\) \\[ \\begin{aligned} g(\\mu) &amp;= \\theta = \\eta = \\mathbf{x&#39;\\beta} \\\\ \\mu &amp;= g^{-1}(\\eta)= b&#39;(\\eta) \\end{aligned} \\] so the method estimator for \\(a(\\phi)=\\phi\\) is \\[ \\hat{\\phi} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - g^{-1}(\\hat{\\eta}_i))^2}{V(g^{-1}(\\hat{\\eta}_i))} \\] 7.7.2 Inference We have \\[ \\hat{var}(\\beta) = a(\\phi)(\\mathbf{\\hat{F}&#39;\\hat{V}\\hat{F}})^{-1} \\] where \\(\\mathbf{V}\\) is an \\(n \\times n\\) diagonal matrix with diagonal elements given by \\(V(\\mu_i)\\) \\(\\mathbf{F}\\) is an \\(n \\times p\\) matrix given by \\(\\mathbf{F} = \\frac{\\partial \\mu}{\\partial \\beta}\\) Both \\(\\mathbf{V,F}\\) are dependent on the mean \\(\\mu\\), and thus \\(\\beta\\). Hence, their estimates (\\(\\mathbf{\\hat{V},\\hat{F}}\\)) depend on \\(\\hat{\\beta}\\). \\[ H_0: \\mathbf{L\\beta = d} \\] where \\(\\mathbf{L}\\) is a q x p matrix with a Wald test \\[ W = \\mathbf{(L \\hat{\\beta}-d)&#39;(a(\\phi)L(\\hat{F}&#39;\\hat{V}^{-1}\\hat{F})L&#39;)^{-1}(L \\hat{\\beta}-d)} \\] which follows \\(\\chi_q^2\\) distribution (asymptotically), where \\(q\\) is the rank of \\(\\mathbf{L}\\) In the simple case \\(H_0: \\beta_j = 0\\) gives \\(W = \\frac{\\hat{\\beta}^2_j}{\\hat{var}(\\hat{\\beta}_j)} \\sim \\chi^2_1\\) asymptotically Likelihood ratio test \\[ \\Lambda = 2 (l(\\hat{\\beta}_f)-l(\\hat{\\beta}_r)) \\sim \\chi^2_q \\] where \\(q\\) is the number of constraints used to fit the reduced model \\(\\hat{\\beta}_r\\), and \\(\\hat{\\beta}_r\\) is the fit under the full model. Wald test is easier to implement, but likelihood ratio test is better (especially for small samples). 7.7.3 Deviance Deviance is necessary for goodness of fit, inference and for alternative estimation of the dispersion parameter. We define and consider Deviance from a likelihood ratio perspective. Assume that \\(\\phi\\) is known. Let \\(\\tilde{\\theta}\\) denote the full and \\(\\hat{\\theta}\\) denote the reduced model MLEs. Then, the likelihood ratio (2 times the difference in log-likelihoods) is \\[ 2\\sum_{i=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)} \\] For exponential families, \\(\\mu = E(y) = b&#39;(\\theta)\\), so the natural parameter is a function of \\(\\mu: \\theta = \\theta(\\mu) = b&#39;^{-1}(\\mu)\\), and the likelihood ratio turns into \\[ 2 \\sum_{i=1}^m \\frac{y_i\\{\\theta(\\tilde{\\mu}_i - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)} \\] Comparing a fitted model to “the fullest possible model”, which is the saturated model: \\(\\tilde{\\mu}_i = y_i\\), i = 1,..,n. If \\(\\tilde{\\theta}_i^* = \\theta(y_i), \\hat{\\theta}_i^* = \\theta (\\hat{\\mu})\\), the likelihood ratio is \\[ 2 \\sum_{i=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^* + b(\\hat{\\theta}_i^*))}{a_i(\\phi)} \\] (McCullagh 2019) specify \\(a(\\phi) = \\phi\\), then the likelihood ratio can be written as \\[ D^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi}\\sum_{i=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*) \\} \\] where \\(D^*(\\mathbf{y, \\hat{\\mu}})\\) = scaled deviance \\(D(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\\) = deviance Note: in some random component distributions, we can write \\(a_i(\\phi) = \\phi m_i\\), where \\(m_i\\) is some known scalar that may change with the observations. Then, the scaled deviance components are divided by \\(m_i\\): \\[ D^*(\\mathbf{y, \\hat{\\mu}}) \\equiv 2\\sum_{i=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)\\} / (\\phi m_i) \\] \\(D^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{i=1}^n d_i\\)m where \\(d_i\\) is the deviance contribution from the \\(i\\)-th observation. \\(D\\) is used in model selection \\(D^*\\) is used in goodness of fit tests (as it is a likelihood ratio statistic). \\[ D^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\} \\] \\(d_i\\) are used to form deviance residuals Normal We have \\[ \\begin{aligned} \\theta &amp;= \\mu \\\\ \\phi &amp;= \\sigma^2 \\\\ b(\\theta) &amp;= \\frac{1}{2} \\theta^2 \\\\ a(\\phi) &amp;= \\phi \\end{aligned} \\] Hence, \\[ \\begin{aligned} \\tilde{\\theta}_i &amp;= y_i \\\\ \\hat{\\theta}_i &amp;= \\hat{\\mu}_i = g^{-1}(\\hat{\\eta}_i) \\end{aligned} \\] And \\[ \\begin{aligned} D &amp;= 2 \\sum_{1=1}^n Y^2_i - y_i \\hat{\\mu}_i - \\frac{1}{2}y^2_i + \\frac{1}{2} \\hat{\\mu}_i^2 \\\\ &amp;= \\sum_{i=1}^n y_i^2 - 2y_i \\hat{\\mu}_i + \\hat{\\mu}_i^2 \\\\ &amp;= \\sum_{i=1}^n (y_i - \\hat{\\mu}_i)^2 \\end{aligned} \\] which is the residual sum of squares Poisson \\[ \\begin{aligned} f(y) &amp;= \\exp\\{y\\log(\\mu) - \\mu - \\log(y!)\\} \\\\ \\theta &amp;= \\log(\\mu) \\\\ b(\\theta) &amp;= \\exp(\\theta) \\\\ a(\\phi) &amp;= 1 \\\\ \\tilde{\\theta}_i &amp;= \\log(y_i) \\\\ \\hat{\\theta}_i &amp;= \\log(\\hat{\\mu}_i) \\\\ \\hat{\\mu}_i &amp;= g^{-1}(\\hat{\\eta}_i) \\end{aligned} \\] Then, \\[ \\begin{aligned} D &amp;= 2 \\sum_{i = 1}^n y_i \\log(y_i) - y_i \\log(\\hat{\\mu}_i) - y_i + \\hat{\\mu}_i \\\\ &amp;= 2 \\sum_{i = 1}^n y_i \\log(\\frac{y_i}{\\hat{\\mu}_i}) - (y_i - \\hat{\\mu}_i) \\end{aligned} \\] and \\[ d_i = 2\\{y_i \\log(\\frac{y_i}{\\hat{\\mu}})- (y_i - \\hat{\\mu}_i)\\} \\] 7.7.3.1 Analysis of Deviance The difference in deviance between a reduced and full model, where q is the difference in the number of free parameters, has an asymptotic \\(\\chi^2_q\\). The likelihood ratio test \\[ D^*(\\mathbf{y;\\hat{\\mu}_r}) - D^*(\\mathbf{y;\\hat{\\mu}_f}) = 2\\{l(\\mathbf{y;\\hat{\\mu}_f})-l(\\mathbf{y;\\hat{\\mu}_r})\\} \\] this comparison of models is Analysis of Deviance. GLM uses this analysis for model selection. An estimation of \\(\\phi\\) is \\[ \\hat{\\phi} = \\frac{D(\\mathbf{y, \\hat{\\mu}})}{n - p} \\] where \\(p\\) = number of parameters fit. Excessive use of \\(\\chi^2\\) test could be problematic since it is asymptotic (McCullagh 2019) 7.7.3.2 Deviance Residuals We have \\(D = \\sum_{i=1}^{n}d_i\\). Then, we define deviance residuals \\[ r_{D_i} = \\text{sign}(y_i -\\hat{\\mu}_i)\\sqrt{d_i} \\] Standardized version of deviance residuals is \\[ r_{s,i} = \\frac{y_i -\\hat{\\mu}}{\\hat{\\sigma}(1-h_{ii})^{1/2}} \\] Let \\(\\mathbf{H^{GLM} = W^{1/2}X(X&#39;WX)^{-1}X&#39;W^{-1/2}}\\), where \\(\\mathbf{W}\\) is an \\(n \\times n\\) diagonal matrix with \\((i,i)\\)-th element given by \\(w_i\\) (see Estimation of \\(\\beta\\)). Then Standardized deviance residuals is equivalently \\[ r_{s, D_i} = \\frac{r_{D_i}}{\\{\\hat{\\phi}(1-h_{ii}^{glm}\\}^{1/2}} \\] where \\(h_{ii}^{glm}\\) is the \\(i\\)-th diagonal of \\(\\mathbf{H}^{GLM}\\) 7.7.3.3 Pearson Chi-square Residuals Another \\(\\chi^2\\) statistic is Pearson \\(\\chi^2\\) statistics: (assume \\(m_i = 1\\)) \\[ X^2 = \\sum_{i=1}^{n} \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)} \\] where \\(\\hat{\\mu}_i\\) is the fitted mean response fo the model of interest. The Scaled Pearson \\(\\chi^2\\) statistic is given by \\(\\frac{X^2}{\\phi} \\sim \\chi^2_{n-p}\\) where p is the number of parameters estimated. Hence, the Pearson \\(\\chi^2\\) residuals are \\[ X^2_i = \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)} \\] If we have the following assumptions: Independent samples No over-dispersion: If \\(\\phi = 1\\), \\(\\frac{D(\\mathbf{y;\\hat{\\mu}})}{n-p}\\) and \\(\\frac{X^2}{n-p}\\) have a value substantially larger 1 indicates improperly specified model or overdispersion Multiple groups then \\(\\frac{X^2}{\\phi}\\) and \\(D^*(\\mathbf{y; \\hat{\\mu}})\\) both follow \\(\\chi^2_{n-p}\\) 7.7.4 Diagnostic Plots Standardized residual Plots: plot(\\(r_{s, D_i}\\), \\(\\hat{\\mu}_i\\)) or plot(\\(r_{s, D_i}\\), \\(T(\\hat{\\mu}_i)\\)) where \\(T(\\hat{\\mu}_i)\\) is transformation(\\(\\hat{\\mu}_i\\)) called constant information scale: plot(\\(r_{s, D_i}\\), \\(\\hat{\\eta}_i\\)) Random Component \\(T(\\hat{\\mu}_i)\\) Normal \\(\\hat{\\mu}\\) Poisson \\(2\\sqrt{\\mu}\\) Binomial \\(2 \\sin^{-1}(\\sqrt{\\hat{\\mu}})\\) Gamma \\(2 \\log(\\hat{\\mu})\\) Inverse Gaussian \\(-2\\hat{\\mu}^{-1/2}\\) If we see: Trend, it means we might have a wrong link function, or choice of scale Systematic change in range of residuals with a change in \\(T(\\hat{\\mu})\\) (incorrect random component) (systematic \\(\\neq\\) random) plot(\\(|r_{D_i}|,\\hat{\\mu}_i\\)) to check Variance Function. 7.7.5 Goodness of Fit To assess goodness of fit, we can use Deviance Pearson Chi-square Residuals In nested model, we could use likelihood-based information measures: \\[ \\begin{aligned} AIC &amp;= -2l(\\mathbf{\\hat{\\mu}}) + 2p \\\\ AICC &amp;= -2l(\\mathbf{\\hat{\\mu}}) + 2p(\\frac{n}{n-p-1}) \\\\ BIC &amp;= 2l(\\hat{\\mu}) + p \\log(n) \\end{aligned} \\] where \\(l(\\hat{\\mu})\\) is the log-likelihood evaluated at the parameter estimates \\(p\\) is the number of parameters \\(n\\) is the number of observations. Note: you have to use the same data with the same model (i.e., same link function, same random underlying random distribution). but you can have different number of parameters. Even though statisticians try to come up with measures that are similar to \\(R^2\\), in practice, it is not so appropriate. For example, they compare the log-likelihood of the fitted model against the that of a model with just the intercept: \\[ R^2_p = 1 - \\frac{l(\\hat{\\mu})}{l(\\hat{\\mu}_0)} \\] For certain specific random components such as binary response model, we have rescaled generalized \\(R^2\\) \\[ \\bar{R}^2 = \\frac{R^2_*}{\\max(R^2_*)} = \\frac{1-\\exp\\{-\\frac{2}{n}(l(\\hat{\\mu}) - l(\\hat{\\mu}_0) \\}}{1 - \\exp\\{\\frac{2}{n}l(\\hat{\\mu}_0)\\}} \\] 7.7.6 Over-Dispersion Random Components \\(var(Y)\\) \\(V(\\mu)\\) Binomial \\(var(Y) = n \\mu (1- \\mu)\\) \\(V(\\mu) = \\phi n \\mu(1- \\mu)\\) where \\(m_i =n\\) Poisson \\(var(Y) = \\mu\\) \\(V(\\mu) = \\phi \\mu\\) In both cases \\(\\phi = 1\\). Recall \\(b&#39;&#39;(\\theta)= V(\\mu)\\) check Estimation of \\(\\phi\\). If we find \\(\\phi &gt;1\\): over-dispersion (i.e., too much variation for an independent binomial or Poisson distribution). \\(\\phi&lt;1\\): under-dispersion (i.e., too little variation for an independent binomial or Poisson distribution). If we have either over or under-dispersion, it means we might have unspecified random component, we could Select a different random component distribution that can accommodate over or under-dispersion (e.g., negative binomial, Conway-Maxwell Poisson) use Nonlinear and Generalized Linear Mixed Models to handle random effects in generalized linear models. References "],["linear-mixed-models.html", "Chapter 8 Linear Mixed Models ", " Chapter 8 Linear Mixed Models "],["dependent-data.html", "8.1 Dependent Data", " 8.1 Dependent Data Forms of dependent data: Multivariate measurements on different individuals: (e.g., a person’s blood pressure, fat, etc are correlated) Clustered measurements: (e.g., blood pressure measurements of people in the same family can be correlated). Repeated measurements: (e.g., measurement of cholesterol over time can be correlated) “If data are collected repeatedly on experimental material to which treatments were applied initially, the data is a repeated measure.” (Schabenberger and Pierce 2001) Longitudinal data: (e.g., individual’s cholesterol tracked over time are correlated): “data collected repeatedly over time in an observational study are termed longitudinal.” (Schabenberger and Pierce 2001) Spatial data: (e.g., measurement of individuals living in the same neighborhood are correlated) Hence, we like to account for these correlations. Linear Mixed Model (LMM), also known as Mixed Linear Model has 2 components: Fixed effect (e.g, gender, age, diet, time) Random effects representing individual variation or auto correlation/spatial effects that imply dependent (correlated) errors Review Two-Way Mixed Effects ANOVA We choose to model the random subject-specific effect instead of including dummy subject covariates in our model because: reduction in the number of parameters to estimate when you do inference, it would make more sense that you can infer from a population (i.e., random effect). LLM Motivation In a repeated measurements analysis where \\(Y_{ij}\\) is the response for the \\(i\\)-th individual measured at the \\(j\\)-th time, \\(i =1,…,N\\) ; \\(j = 1,…,n_i\\) \\[ \\mathbf{Y}_i = \\left( \\begin{array} {c} Y_{i1} \\\\ . \\\\ .\\\\ .\\\\ Y_{in_i} \\end{array} \\right) \\] is all measurements for subject \\(i\\). Stage 1: (Regression Model) how the response changes over time for the \\(i\\)-th subject \\[ \\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i} \\] where \\(Z_i\\) is an \\(n_i \\times q\\) matrix of known covariates \\(\\beta_i\\) is an unknown \\(q \\times 1\\) vector of subjective -specific coefficients (regression coefficients different for each subject) \\(\\epsilon_i\\) are the random errors (typically \\(\\sim N(0, \\sigma^2 I)\\)) We notice that there are two many \\(\\beta\\) to estimate here. Hence, this is the motivation for the second stage Stage 2: (Parameter Model) \\[ \\mathbf{\\beta_i = K_i \\beta + b_i} \\] where \\(K_i\\) is a \\(q \\times p\\) matrix of known covariates \\(\\beta\\) is a \\(p \\times 1\\) vector of unknown parameter \\(\\mathbf{b}_i\\) are independent \\(N(0,D)\\) random variables This model explain the observed variability between subjects with respect to the subject-specific regression coefficients, \\(\\beta_i\\). We model our different coefficient (\\(\\beta_i\\)) with respect to \\(\\beta\\). Example: Stage 1: \\[ Y_{ij} = \\beta_{1i} + \\beta_{2i}t_{ij} + \\epsilon_{ij} \\] where \\(j = 1,..,n_i\\) In the matrix notation, \\[ \\mathbf{Y_i} = \\left( \\begin{array} {c} Y_{i1} \\\\ .\\\\ Y_{in_i} \\end{array} \\right); \\mathbf{Z}_i = \\left( \\begin{array} {cc} 1 &amp; t_{i1} \\\\ . &amp; . \\\\ 1 &amp; t_{in_i} \\end{array} \\right) \\] \\[ \\beta_i = \\left( \\begin{array} {c} \\beta_{1i} \\\\ \\beta_{2i} \\end{array} \\right); \\epsilon_i = \\left( \\begin{array} {c} \\epsilon_{i1} \\\\ . \\\\ \\epsilon_{in_i} \\end{array} \\right) \\] Thus, \\[ \\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i} \\] Stage 2: \\[ \\begin{aligned} \\beta_{1i} &amp;= \\beta_0 + b_{1i} \\\\ \\beta_{2i} &amp;= \\beta_1 L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i} \\end{aligned} \\] where \\(L_i, H_i, C_i\\) are indicator variables defined to 1 as the subject falls into different categories. Subject specific intercepts do not depend upon treatment, with \\(\\beta_0\\) (the average response at the start of treatment), and \\(\\beta_1 , \\beta_2, \\beta_3\\) (the average time effects for each of three treatment groups). \\[ \\begin{aligned} \\mathbf{K}_i &amp;= \\left( \\begin{array} {cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; L_i &amp; H_i &amp; C_i \\end{array} \\right) \\\\ \\beta &amp;= (\\beta_0 , \\beta_1, \\beta_2, \\beta_3)&#39; \\\\ \\mathbf{b}_i &amp;= \\left( \\begin{array} {c} b_{1i} \\\\ b_{2i} \\\\ \\end{array} \\right) \\\\ \\beta_i &amp;= \\mathbf{K_i \\beta + b_i} \\end{aligned} \\] To get \\(\\hat{\\beta}\\), we can fit the model sequentially: Estimate \\(\\hat{\\beta_i}\\) in the first stage Estimate \\(\\hat{\\beta}\\) in the second stage by replacing \\(\\beta_i\\) with \\(\\hat{\\beta}_i\\) However, problems arise from this method: information is lost by summarizing the vector \\(\\mathbf{Y}_i\\) solely by \\(\\hat{\\beta}_i\\) we need to account for variability when replacing \\(\\beta_i\\) with its estimate different subjects might have different number of observations. To address these problems, we can use Linear Mixed Model (Laird and Ware 1982) Substituting stage 2 into stage 1: \\[ \\mathbf{Y}_i = \\mathbf{Z}_i \\mathbf{K}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i \\] Let \\(\\mathbf{X}_i = \\mathbf{Z}_i \\mathbf{K}_i\\) be an \\(n_i \\times p\\) matrix . Then, the LMM is \\[ \\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i \\] where \\(i = 1,..,N\\) \\(\\beta\\) are the fixed effects, which are common to all subjects \\(\\mathbf{b}_i\\) are the subject specific random effects. \\(\\mathbf{b}_i \\sim N_q (\\mathbf{0,D})\\) \\(\\mathbf{\\epsilon}_i \\sim N_{n_i}(\\mathbf{0,\\Sigma_i})\\) \\(\\mathbf{b}_i\\) and \\(\\epsilon_i\\) are independent \\(\\mathbf{Z}_{i(n_i \\times q})\\) and \\(\\mathbf{X}_{i(n_i \\times p})\\) are matrices of known covariates. Equivalently, in the hierarchical form, we call conditional or hierarchical formulation of the linear mixed model \\[ \\begin{aligned} \\mathbf{Y}_i | \\mathbf{b}_i &amp;\\sim N(\\mathbf{X}_i \\beta+ \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i) \\\\ \\mathbf{b}_i &amp;\\sim N(\\mathbf{0,D}) \\end{aligned} \\] for \\(i = 1,..,N\\). denote the respective functions by \\(f(\\mathbf{Y}_i |\\mathbf{b}_i)\\) and \\(f(\\mathbf{b}_i)\\) In general, \\[ \\begin{aligned} f(A,B) &amp;= f(A|B)f(B) \\\\ f(A) &amp;= \\int f(A,B)dB = \\int f(A|B) f(B) dB \\end{aligned} \\] In the LMM, the marginal density of \\(\\mathbf{Y}_i\\) is \\[ f(\\mathbf{Y}_i) = \\int f(\\mathbf{Y}_i | \\mathbf{b}_i) f(\\mathbf{b}_i) d\\mathbf{b}_i \\] which can be shown \\[ \\mathbf{Y}_i \\sim N(\\mathbf{X_i \\beta, Z_i DZ&#39;_i + \\Sigma_i}) \\] This is the marginal formulation of the linear mixed model Notes: We no longer have \\(Z_i b_i\\) in the mean, but add error in the variance (marginal dependence in Y). kinda of averaging out the common effect. Technically, we shouldn’t call it averaging the error b (adding it to the variance covariance matrix), it should be called adding random effect Continue with our example \\[ Y_{ij} = (\\beta_0 + b_{1i}) + (\\beta_1L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i})t_{ij} + \\epsilon_{ij} \\] for each treatment group \\[ Y_{ik}= \\begin{cases} \\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} &amp; L \\\\ \\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} &amp; H\\\\ \\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} &amp; C \\end{cases} \\] Intercepts and slopes are all subject specific Different treatment groups have different slops, but the same intercept. In the hierarchical model form \\[ \\begin{aligned} \\mathbf{Y}_i | \\mathbf{b}_i &amp;\\sim N(\\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i)\\\\ \\mathbf{b}_i &amp;\\sim N(\\mathbf{0,D}) \\end{aligned} \\] X will be in the form of \\[ \\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)&#39; \\] \\[ \\begin{aligned} \\mathbf{X}_i &amp;= \\mathbf{Z}_i \\mathbf{K}_i \\\\ &amp;= \\left[ \\begin{array} {cc} 1 &amp; t_{i1} \\\\ 1 &amp; t_{i2} \\\\ . &amp; . \\\\ 1 &amp; t_{in_i} \\end{array} \\right] \\times \\left[ \\begin{array} {cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; L_i &amp; H_i &amp; C_i \\\\ \\end{array} \\right] \\\\ &amp;= \\left[ \\begin{array} {cccc} 1 &amp; t_{i1}L_i &amp; t_{i1}H_i &amp; T_{i1}C_i \\\\ 1 &amp; t_{i2}L_i &amp; t_{i2}H_i &amp; T_{i2}C_i \\\\ . &amp;. &amp;. &amp;. \\\\ 1 &amp; t_{in_i}L_i &amp; t_{in_i}H_i &amp; T_{in_i}C_i \\\\ \\end{array} \\right]\\end{aligned} \\] \\[ \\mathbf{b}_i = \\left( \\begin{array} {c} b_{1i} \\\\ b_{2i} \\end{array} \\right) \\] \\[ D = \\left( \\begin{array} {cc} d_{11} &amp; d_{12}\\\\ d_{12} &amp; d_{22} \\end{array} \\right) \\] Assuming \\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{I}_{n_i}\\), which is called conditional independence, meaning the response on subject i are independent conditional on \\(\\mathbf{b}_i\\) and \\(\\beta\\) In the marginal model form \\[ Y_{ij} = \\beta_0 + \\beta_1 L_i t_{ij} + \\beta_2 H_i t_{ij} + \\beta_3 C_i t_{ij} + \\eta_{ij} \\] where \\(\\eta_i \\sim N(\\mathbf{0},\\mathbf{Z}_i\\mathbf{DZ}_i&#39;+ \\mathbf{\\Sigma}_i)\\) Equivalently, \\[ \\mathbf{Y_i \\sim N(X_i \\beta, Z_i DZ_i&#39; + \\Sigma_i}) \\] In this case that \\(n_i = 2\\) \\[ \\begin{aligned} \\mathbf{Z_iDZ_i&#39;} &amp;= \\left( \\begin{array} {cc} 1 &amp; t_{i1} \\\\ 1 &amp; t_{i2} \\end{array} \\right) \\left( \\begin{array} {cc} d_{11} &amp; d_{12} \\\\ d_{12} &amp; d_{22} \\end{array} \\right) \\left( \\begin{array} {cc} 1 &amp; 1 \\\\ t_{i1} &amp; t_{i2} \\end{array} \\right) \\\\ &amp;= \\left( \\begin{array} {cc} d_{11} + 2d_{12}t_{i1} + d_{22}t_{i1}^2 &amp; d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\\\ d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22} t_{i1} t_{i2} &amp; d_{11} + 2d_{12}t_{i2} + d_{22}t_{i2}^2 \\end{array} \\right) \\end{aligned} \\] \\[ var(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \\sigma^2 \\] On top of correlation in the errors, the marginal implies that the variance function of the response is quadratic over time, with positive curvature \\(d_{22}\\) 8.1.1 Random-Intercepts Model If we remove the random slopes, the assumption is that all variability in subject-specific slopes can be attributed to treatment differences the model is random-intercepts model. This has subject specific intercepts, but the same slopes within each treatment group. \\[ \\begin{aligned} \\mathbf{Y}_i | b_i &amp;\\sim N(\\mathbf{X}_i \\beta + 1 b_i , \\Sigma_i) \\\\ b_i &amp;\\sim N(0,d_{11}) \\end{aligned} \\] The marginal model is then (\\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{I}\\)) \\[ \\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, 11&#39;d_{11} + \\sigma^2 \\mathbf{I}) \\] The marginal covariance matrix is \\[ \\begin{aligned} cov(\\mathbf{Y}_i) &amp;= 11&#39;d_{11} + \\sigma^2I \\\\ &amp;= \\left( \\begin{array} {cccc} d_{11}+ \\sigma^2 &amp; d_{11} &amp; ... &amp; d_{11} \\\\ d_{11} &amp; d_{11} + \\sigma^2 &amp; d_{11} &amp; ... \\\\ . &amp; . &amp; . &amp; . \\\\ d_{11} &amp; ... &amp; ... &amp; d_{11} + \\sigma^2 \\end{array} \\right) \\end{aligned} \\] the associated correlation matrix is \\[ corr(\\mathbf{Y}_i) = \\left( \\begin{array} {cccc} 1 &amp; \\rho &amp; ... &amp; \\rho \\\\ \\rho &amp; 1 &amp; \\rho &amp; ... \\\\ . &amp; . &amp; . &amp; . \\\\ \\rho &amp; ... &amp; ... &amp; 1 \\\\ \\end{array} \\right) \\] where \\(\\rho \\equiv \\frac{d_{11}}{d_{11} + \\sigma^2}\\) Thu, we have constant variance over time equal, positive correlation between any two measurements from the same subject a covariance structure that is called compound symmetry, and \\(\\rho\\) is called the intra-class correlation that when \\(\\rho\\) is large, the inter-subject variability (\\(d_{11}\\)) is large relative to the intra-subject variability (\\(\\sigma^2\\)) 8.1.2 Covariance Models If the conditional independence assumption, (\\(\\mathbf{\\Sigma_i= \\sigma^2 I_{n_i}}\\)). Consider, \\(\\epsilon_i = \\epsilon_{(1)i} + \\epsilon_{(2)i}\\), where \\(\\epsilon_{(1)i}\\) is a “serial correlation” component. That is, part of the individual’s profile is a response to time-varying stochastic processes. \\(\\epsilon_{(2)i}\\) is the measurement error component, and is independent of \\(\\epsilon_{(1)i}\\) Then \\[ \\mathbf{Y_i = X_i \\beta + Z_i b_i + \\epsilon_{(1)i} + \\epsilon_{(2)i}} \\] where \\(\\mathbf{b_i} \\sim N(\\mathbf{0,D})\\) \\(\\epsilon_{(2)i} \\sim N(\\mathbf{0,\\sigma^2 I_{n_i}})\\) \\(\\epsilon_{(1)i} \\sim N(\\mathbf{0,\\tau^2H_i})\\) \\(\\mathbf{b}_i\\) and \\(\\epsilon_i\\) are mutually independent To model the structure of the \\(n_i \\times n_i\\) correlation (or covariance ) matrix \\(\\mathbf{H}_i\\). Let the (j,k)th element of \\(\\mathbf{H}_i\\) be \\(h_{ijk}= g(t_{ij}t_{ik})\\). that is a function of the times \\(t_{ij}\\) and \\(t_{ik}\\) , which is assumed to be some function of the “distance’ between the times. \\[ h_{ijk} = g(|t_{ij}-t_{ik}|) \\] for some decreasing function \\(g(.)\\) with \\(g(0)=1\\) (for correlation matrices). Examples of this type of function: Exponential function: \\(g(|t_{ij}-t_{ik}|) = \\exp(-\\phi|t_{ij} - t_{ik}|)\\) Gaussian function: \\(g(|t_{ij} - t_{ik}|) = \\exp(-\\phi(t_{ij} - t_{ik})^2)\\) Similar structures could also be used for \\(\\mathbf{D}\\) matrix (of \\(\\mathbf{b}\\)) Example: Autoregressive Covariance Structure A first order Autoregressive Model (AR(1)) has the form \\[ \\alpha_t = \\phi \\alpha_{t-1} + \\eta_t \\] where \\(\\eta_t \\sim iid N (0,\\sigma^2_\\eta)\\) Then, the covariance between two observations is \\[ cov(\\alpha_t, \\alpha_{t+h}) = \\frac{\\sigma^2_\\eta \\phi^{|h|}}{1- \\phi^2} \\] for \\(h = 0, \\pm 1, \\pm 2, ...; |\\phi|&lt;1\\) Hence, \\[ corr(\\alpha_t, \\alpha_{t+h}) = \\phi^{|h|} \\] If we let \\(\\alpha_T = (\\alpha_1,...\\alpha_T)&#39;\\), then \\[ corr(\\alpha_T) = \\left[ \\begin{array} {ccccc} 1 &amp; \\phi^1 &amp; \\phi^2 &amp; ... &amp; \\phi^2 \\\\ \\phi^1 &amp; 1 &amp; \\phi^1 &amp; ... &amp; \\phi^{T-1} \\\\ \\phi^2 &amp; \\phi^1 &amp; 1 &amp; ... &amp; \\phi^{T-2} \\\\ . &amp; . &amp; . &amp; . &amp;. \\\\ \\phi^T &amp; \\phi^{T-1} &amp; \\phi^{T-2} &amp; ... &amp; 1 \\end{array} \\right] \\] Notes: The correlation decreases as time lag increases This matrix structure is known as a Toeplitz structure More complicated covariance structures are possible, which is critical component of spatial random effects models and time series models. Often, we don’t need both random effects \\(\\mathbf{b}\\) and \\(\\epsilon_{(1)i}\\) More in the Time Series section References "],["estimation-2.html", "8.2 Estimation", " 8.2 Estimation \\[ \\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i \\] where \\(\\beta, \\mathbf{b}_i, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) we must obtain estimation from the data \\(\\mathbf{\\beta}, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) are unknown, but fixed, parameters, and must be estimated from the data \\(\\mathbf{b}_i\\) is a random variable. Thus, we can’t estimate these values, but we can predict them. (i.e., you can’t estimate a random thing). If we have \\(\\hat{\\beta}\\) as an estimator of \\(\\beta\\) \\(\\hat{\\mathbf{b}}_i\\) as a predictor of \\(\\mathbf{b}_i\\) Then, The population average estimate of \\(\\mathbf{Y}_i\\) is \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta}\\) The subject-specific prediction is \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta} + \\mathbf{Z}_i \\hat{b}_i\\) According to (Henderson 1975), estimating equations known as the mixed model equations: \\[ \\left[ \\begin{array} {c} \\hat{\\beta} \\\\ \\hat{\\mathbf{b}} \\end{array} \\right] = \\left[ \\begin{array} {cc} \\mathbf{X&#39;\\Sigma^{-1}X} &amp; \\mathbf{X&#39;\\Sigma^{-1}Z} \\\\ \\mathbf{Z&#39;\\Sigma^{-1}X} &amp; \\mathbf{Z&#39;\\Sigma^{-1}Z +B^{-1}} \\end{array} \\right] \\left[ \\begin{array} {cc} \\mathbf{X&#39;\\Sigma^{-1}Y} \\\\ \\mathbf{Z&#39;\\Sigma^{-1}Y} \\end{array} \\right] \\] where \\[ \\begin{aligned} \\mathbf{Y} &amp;= \\left[ \\begin{array} {c} \\mathbf{y}_1 \\\\ . \\\\ \\mathbf{y}_N \\end{array} \\right] ; \\mathbf{X} = \\left[ \\begin{array} {c} \\mathbf{X}_1 \\\\ . \\\\ \\mathbf{X}_N \\end{array} \\right]; \\mathbf{b} = \\left[ \\begin{array} {c} \\mathbf{b}_1 \\\\ . \\\\ \\mathbf{b}_N \\end{array} \\right] ; \\epsilon = \\left[ \\begin{array} {c} \\epsilon_1 \\\\ . \\\\ \\epsilon_N \\end{array} \\right] \\\\ cov(\\epsilon) &amp;= \\mathbf{\\Sigma}, \\mathbf{Z} = \\left[ \\begin{array} {cccc} \\mathbf{Z}_1 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; \\mathbf{Z}_2 &amp; ... &amp; 0 \\\\ . &amp; . &amp; . &amp; . \\\\ 0 &amp; 0 &amp; ... &amp; \\mathbf{Z}_n \\end{array} \\right], \\mathbf{B} = \\left[ \\begin{array} {cccc} \\mathbf{D} &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; \\mathbf{D} &amp; ... &amp; 0 \\\\ . &amp; . &amp; . &amp; . \\\\ 0 &amp; 0 &amp; ... &amp; \\mathbf{D} \\end{array} \\right] \\end{aligned} \\] The model has the form \\[ \\begin{aligned} \\mathbf{Y} &amp;= \\mathbf{X \\beta + Z b + \\epsilon} \\\\ \\mathbf{Y} &amp;\\sim N(\\mathbf{X \\beta, ZBZ&#39; + \\Sigma}) \\end{aligned} \\] If \\(\\mathbf{V = ZBZ&#39; + \\Sigma}\\), then the solutions to the estimating equations can be \\[ \\begin{aligned} \\hat{\\beta} &amp;= \\mathbf{(X&#39;V^{-1}X)^{-1}X&#39;V^{-1}Y} \\\\ \\hat{\\mathbf{b}} &amp;= \\mathbf{BZ&#39;V^{-1}(Y-X\\hat{\\beta}}) \\end{aligned} \\] The estimate \\(\\hat{\\beta}\\) is a generalized least squares estimate. The predictor, \\(\\hat{\\mathbf{b}}\\) is the best linear unbiased predictor (BLUP), for \\(\\mathbf{b}\\) \\[ \\begin{aligned} E(\\hat{\\beta}) &amp;= \\beta \\\\ var(\\hat{\\beta}) &amp;= (\\mathbf{X&#39;V^{-1}X})^{-1} \\\\ E(\\hat{\\mathbf{b}}) &amp;= 0 \\end{aligned} \\] \\[ var(\\mathbf{\\hat{b}-b}) = \\mathbf{B-BZ&#39;V^{-1}ZB + BZ&#39;V^{-1}X(X&#39;V^{-1}X)^{-1}X&#39;V^{-1}B} \\] The variance here is the variance of the prediction error (mean squared prediction error, MSPE), which is more meaningful than \\(var(\\hat{\\mathbf{b}})\\), since MSPE accounts for both variance and bias in the prediction. To derive the mixed model equations, consider \\[ \\mathbf{\\epsilon = Y - X\\beta - Zb} \\] Let \\(T = \\sum_{i=1}^N n_i\\) be the total number of observations (i.e., the length of \\(\\mathbf{Y},\\epsilon\\)) and \\(Nq\\) the length of \\(\\mathbf{b}\\). The joint distribution of \\(\\mathbf{b, \\epsilon}\\) is \\[ f(\\mathbf{b,\\epsilon})= \\frac{1}{(2\\pi)^{(T+ Nq)/2}} \\left| \\begin{array} {cc} \\mathbf{B} &amp; 0 \\\\ 0 &amp; \\mathbf{\\Sigma} \\end{array} \\right| ^{-1/2} \\exp \\left( -\\frac{1}{2} \\left[ \\begin{array} {c} \\mathbf{b} \\\\ \\mathbf{Y - X \\beta - Zb} \\end{array} \\right]&#39; \\left[ \\begin{array} {cc} \\mathbf{B} &amp; 0 \\\\ 0 &amp; \\mathbf{\\Sigma} \\end{array} \\right]^{-1} \\left[ \\begin{array} {c} \\mathbf{b} \\\\ \\mathbf{Y - X \\beta - Zb} \\end{array} \\right] \\right) \\] Maximization of \\(f(\\mathbf{b},\\epsilon)\\) with respect to \\(\\mathbf{b}\\) and \\(\\beta\\) requires minimization of \\[ \\begin{aligned} Q &amp;= \\left[ \\begin{array} {c} \\mathbf{b} \\\\ \\mathbf{Y - X \\beta - Zb} \\end{array} \\right]&#39; \\left[ \\begin{array} {cc} \\mathbf{B} &amp; 0 \\\\ 0 &amp; \\mathbf{\\Sigma} \\end{array} \\right]^{-1} \\left[ \\begin{array} {c} \\mathbf{b} \\\\ \\mathbf{Y - X \\beta - Zb} \\end{array} \\right] \\\\ &amp;= \\mathbf{b&#39;B^{-1}b+(Y-X \\beta-Zb)&#39;\\Sigma^{-1}(Y-X \\beta-Zb)} \\end{aligned} \\] Setting the derivatives of Q with respect to \\(\\mathbf{b}\\) and \\(\\mathbf{\\beta}\\) to zero leads to the system of equations: \\[ \\begin{aligned} \\mathbf{X&#39;\\Sigma^{-1}X\\beta + X&#39;\\Sigma^{-1}Zb} &amp;= \\mathbf{X&#39;\\Sigma^{-1}Y}\\\\ \\mathbf{(Z&#39;\\Sigma^{-1}Z + B^{-1})b + Z&#39;\\Sigma^{-1}X\\beta} &amp;= \\mathbf{Z&#39;\\Sigma^{-1}Y} \\end{aligned} \\] Rearranging \\[ \\left[ \\begin{array} {cc} \\mathbf{X&#39;\\Sigma^{-1}X} &amp; \\mathbf{X&#39;\\Sigma^{-1}Z} \\\\ \\mathbf{Z&#39;\\Sigma^{-1}X} &amp; \\mathbf{Z&#39;\\Sigma^{-1}Z + B^{-1}} \\end{array} \\right] \\left[ \\begin{array} {c} \\beta \\\\ \\mathbf{b} \\end{array} \\right] = \\left[ \\begin{array} {c} \\mathbf{X&#39;\\Sigma^{-1}Y} \\\\ \\mathbf{Z&#39;\\Sigma^{-1}Y} \\end{array} \\right] \\] Thus, the solution to the mixed model equations give: \\[ \\left[ \\begin{array} {c} \\hat{\\beta} \\\\ \\hat{\\mathbf{b}} \\end{array} \\right] = \\left[ \\begin{array} {cc} \\mathbf{X&#39;\\Sigma^{-1}X} &amp; \\mathbf{X&#39;\\Sigma^{-1}Z} \\\\ \\mathbf{Z&#39;\\Sigma^{-1}X} &amp; \\mathbf{Z&#39;\\Sigma^{-1}Z + B^{-1}} \\end{array} \\right] ^{-1} \\left[ \\begin{array} {c} \\mathbf{X&#39;\\Sigma^{-1}Y} \\\\ \\mathbf{Z&#39;\\Sigma^{-1}Y} \\end{array} \\right] \\] Equivalently, Bayes’ theorem \\[ f(\\mathbf{b}| \\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b})}{\\int f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b}) d\\mathbf{b}} \\] where \\(f(\\mathbf{Y}|\\mathbf{b})\\) is the “likelihood” \\(f(\\mathbf{b})\\) is the prior the denominator is the “normalizing constant” \\(f(\\mathbf{b}|\\mathbf{Y})\\) is the posterior distribution In this case \\[ \\begin{aligned} \\mathbf{Y} | \\mathbf{b} &amp;\\sim N(\\mathbf{X\\beta+Zb,\\Sigma}) \\\\ \\mathbf{b} &amp;\\sim N(\\mathbf{0,B}) \\end{aligned} \\] The posterior distribution has the form \\[ \\mathbf{b}|\\mathbf{Y} \\sim N(\\mathbf{BZ&#39;V^{-1}(Y-X\\beta),(Z&#39;\\Sigma^{-1}Z + B^{-1})^{-1}}) \\] Hence, the best predictor (based on squared error loss) \\[ E(\\mathbf{b}|\\mathbf{Y}) = \\mathbf{BZ&#39;V^{-1}(Y-X\\beta)} \\] 8.2.1 Estimating \\(\\mathbf{V}\\) If we have \\(\\tilde{\\mathbf{V}}\\) (estimate of \\(\\mathbf{V}\\)), then we can estimate: \\[ \\begin{aligned} \\hat{\\beta} &amp;= \\mathbf{(X&#39;\\tilde{V}^{-1}X)^{-1}X&#39;\\tilde{V}^{-1}Y} \\\\ \\hat{\\mathbf{b}} &amp;= \\mathbf{BZ&#39;\\tilde{V}^{-1}(Y-X\\hat{\\beta})} \\end{aligned} \\] where \\({\\mathbf{b}}\\) is EBLUP (estimated BLUP) or empirical Bayes estimate Note: \\(\\hat{var}(\\hat{\\beta})\\) is a consistent estimator of \\(var(\\hat{\\beta})\\) if \\(\\tilde{\\mathbf{V}}\\) is a consistent estimator of \\(\\mathbf{V}\\) However, \\(\\hat{var}(\\hat{\\beta})\\) is biased since the variability arises from estimating \\(\\mathbf{V}\\) is not accounted for in the estimate. Hence, \\(\\hat{var}(\\hat{\\beta})\\) underestimates the true variability Ways to estimate \\(\\mathbf{V}\\) Maximum Likelihood Estimation (MLE) Restricted Maximum Likelihood (REML) Estimated Generalized Least Squares Bayesian Hierarchical Models (BHM) 8.2.1.1 Maximum Likelihood Estimation (MLE) Grouping unknown parameters in \\(\\Sigma\\) and \\(B\\) under a parameter vector \\(\\theta\\). Under MLE, \\(\\hat{\\theta}\\) and \\(\\hat{\\beta}\\) maximize the likelihood \\(\\mathbf{y} \\sim N(\\mathbf{X\\beta, V(\\theta))}\\). Synonymously, \\(-2\\log L(\\mathbf{y;\\theta,\\beta})\\): \\[ -2l(\\mathbf{\\beta,\\theta,y}) = \\log |\\mathbf{V(\\theta)}| + \\mathbf{(y-X\\beta)&#39;V(\\theta)^{-1}(y-X\\beta)} + N \\log(2\\pi) \\] Step 1: Replace \\(\\beta\\) with its maximum likelihood (where \\(\\theta\\) is known \\(\\hat{\\beta}= (\\mathbf{X&#39;V(\\theta)^{-1}X)^{-1}X&#39;V(\\theta)^{-1}y}\\) Step 2: Minimize the above equation with respect to \\(\\theta\\) to get the estimator \\(\\hat{\\theta}_{MLE}\\) Step 3: Substitute \\(\\hat{\\theta}_{MLE}\\) back to get \\(\\hat{\\beta}_{MLE} = (\\mathbf{X&#39;V(\\theta_{MLE})^{-1}X)^{-1}X&#39;V(\\theta_{MLE})^{-1}y}\\) Step 4: Get \\(\\hat{\\mathbf{b}}_{MLE} = \\mathbf{B(\\hat{\\theta}_{MLE})Z&#39;V(\\hat{\\theta}_{MLE})^{-1}(y-X\\hat{\\beta}_{MLE})}\\) Note: \\(\\hat{\\theta}\\) are typically negatively biased due to unaccounted fixed effects being estimated, which we could try to account for. 8.2.1.2 Restricted Maximum Likelihood (REML) REML accounts for the number of estimated mean parameters by adjusting the objective function. Specifically, the likelihood of linear combination of the elements of \\(\\mathbf{y}\\) is accounted for. We have \\(\\mathbf{K&#39;y}\\), where \\(\\mathbf{K}\\) is any \\(N \\times (N - p)\\) full-rank contrast matrix, which has columns orthogonal to the \\(\\mathbf{X}\\) matrix (that is \\(\\mathbf{K&#39;X} = 0\\)). Then, \\[ \\mathbf{K&#39;y} \\sim N(0,\\mathbf{K&#39;V(\\theta)K}) \\] where \\(\\beta\\) is no longer in the distribution We can proceed to maximize this likelihood for the contrasts to get \\(\\hat{\\theta}_{REML}\\), which does not depend on the choice of \\(\\mathbf{K}\\). And \\(\\hat{\\beta}\\) are based on \\(\\hat{\\theta}\\) Comparison REML and MLE Both methods are based upon the likelihood principle, and have desired properties for the estimates: consistency asymptotic normality efficiency ML estimation provides estimates for fixed effects, while REML can’t In balanced models, REML is identical to ANOVA REML accounts for df for the fixed effects int eh model, which is important when \\(\\mathbf{X}\\) is large relative to the sample size Changing \\(\\mathbf{\\beta}\\) has no effect on the REML estimates of \\(\\theta\\) REML is less sensitive to outliers than MLE MLE is better than REML regarding model comparisons (e.g., AIC or BIC) 8.2.1.3 Estimated Generalized Least Squares MLE and REML rely upon the Gaussian assumption. To overcome this issue, EGLS uses the first and second moments. \\[ \\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i \\] where \\(\\epsilon_i \\sim (\\mathbf{0,\\Sigma_i})\\) \\(\\mathbf{b}_i \\sim (\\mathbf{0,D})\\) \\(cov(\\epsilon_i, \\mathbf{b}_i) = 0\\) Then the EGLS estimator is \\[ \\begin{aligned} \\hat{\\beta}_{GLS} &amp;= \\{\\sum_{i=1}^n \\mathbf{X&#39;_iV_i(\\theta)^{-1}X_i} \\}^{-1} \\sum_{i=1}^n \\mathbf{X&#39;_iV_i(\\theta)^{-1}Y_i} \\\\ &amp;=\\{\\mathbf{X&#39;V(\\theta)^{-1}X} \\}^{-1} \\mathbf{X&#39;V(\\theta)^{-1}Y} \\end{aligned} \\] depends on the first two moments \\(E(\\mathbf{Y}_i) = \\mathbf{X}_i \\beta\\) \\(var(\\mathbf{Y}_i)= \\mathbf{V}_i\\) EGLS use \\(\\hat{\\mathbf{V}}\\) for \\(\\mathbf{V(\\theta)}\\) \\[ \\hat{\\beta}_{EGLS} = \\{ \\mathbf{X&#39;\\hat{V}^{-1}X} \\}^{-1} \\mathbf{X&#39;\\hat{V}^{-1}Y} \\] Hence, the fixed effects estimators for the MLE, REML, and EGLS are of the same form, except for the estimate of \\(\\mathbf{V}\\) In case of non-iterative approach, EGLS can be appealing when \\(\\mathbf{V}\\) can be estimated without much computational burden. 8.2.1.4 Bayesian Hierarchical Models (BHM) Joint distribution cane be decomposed hierarchically in terms of the product of conditional distributions and a marginal distribution \\[ f(A,B,C) = f(A|B,C) f(B|C)f(C) \\] Applying to estimate \\(\\mathbf{V}\\) \\[ \\begin{aligned} f(\\mathbf{Y, \\beta, b, \\theta}) &amp;= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta,\\beta})f(\\mathbf{\\beta|\\theta})f(\\mathbf{\\theta}) &amp; \\text{based on probability decomposition} \\\\ &amp;= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta}) &amp; \\text{based on simplifying modeling assumptions} \\end{aligned} \\] elaborate on the second equality, if we assume conditional independence (e.g., given \\(\\theta\\), no additional info about \\(\\mathbf{b}\\) is given by knowing \\(\\beta\\)), then we can simply from the first equality Using Bayes’ rule \\[ f(\\mathbf{\\beta, b, \\theta|Y}) \\propto f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta}) \\] where \\[ \\begin{aligned} \\mathbf{Y| \\beta, b, \\theta} &amp;\\sim \\mathbf{N(X\\beta+ Zb, \\Sigma(\\theta))} \\\\ \\mathbf{b | \\theta} &amp;\\sim \\mathbf{N(0, B(\\theta))} \\end{aligned} \\] and we also have to have prior distributions for \\(f(\\beta), f(\\theta)\\) With normalizing constant, we can obtain the posterior distribution. Typically, we can’t get analytical solution right away. Hence, we can use Markov Chain Monte Carlo (MCMC) to obtain samples from the posterior distribution. Bayesian Methods: account for the uncertainty in parameters estimates and accommodate the propagation of that uncertainty through the model can adjust prior information (i.e., priori) in parameters Can extend beyond Gaussian distributions but hard to implement algorithms and might have problem converging References "],["inference-3.html", "8.3 Inference", " 8.3 Inference 8.3.1 Parameters \\(\\beta\\) 8.3.1.1 Wald test We have \\[ \\begin{aligned} \\mathbf{\\hat{\\beta}(\\theta)} &amp;= \\mathbf{\\{X&#39;V^{-1}(\\theta) X\\}^{-1}X&#39;V^{-1}(\\theta) Y} \\\\ var(\\hat{\\beta}(\\theta)) &amp;= \\mathbf{\\{X&#39;V^{-1}(\\theta) X\\}^{-1}} \\end{aligned} \\] We can use \\(\\hat{\\theta}\\) in place of \\(\\theta\\) to approximate Wald test \\[ H_0: \\mathbf{A \\beta =d} \\] With \\[ W = \\mathbf{(A\\hat{\\beta} - d)&#39;[A(X&#39;\\hat{V}^{-1}X)^{-1}A&#39;]^{-1}(A\\hat{\\beta} - d)} \\] where \\(W \\sim \\chi^2_{rank(A)}\\) under \\(H_0\\) is true. However, it does not take into account variability from using \\(\\hat{\\theta}\\) in place of \\(\\theta\\), hence the standard errors are underestimated 8.3.1.2 F-test Alternatively, we can use the modified F-test, suppose we have \\(var(\\mathbf{Y}) = \\sigma^2 \\mathbf{V}(\\theta)\\), then \\[ F^* = \\frac{\\mathbf{(A\\hat{\\beta} - d)&#39;[A(X&#39;\\hat{V}^{-1}X)^{-1}A&#39;]^{-1}(A\\hat{\\beta} - d)}}{\\hat{\\sigma}^2 \\text{rank}(A)} \\] where \\(F^* \\sim f_{rank(A), den(df)}\\) under the null hypothesis. And den(df) needs to be approximated from the data by either: Satterthwaite method Kenward-Roger approximation Under balanced cases, the Wald and F tests are similar. But for small sample sizes, they can differ in p-values. And both can be reduced to t-test for a single \\(\\beta\\) 8.3.1.3 Likelihood Ratio Test \\[ H_0: \\beta \\in \\Theta_{\\beta,0} \\] where \\(\\Theta_{\\beta, 0}\\) is a subspace of the parameter space, \\(\\Theta_{\\beta}\\) of the fixed effects \\(\\beta\\) . Then \\[ -2\\log \\lambda_N = -2\\log\\{\\frac{\\hat{L}_{ML,0}}{\\hat{L}_{ML}}\\} \\] where \\(\\hat{L}_{ML,0}\\) , \\(\\hat{L}_{ML}\\) are the maximized likelihood obtained from maximizing over \\(\\Theta_{\\beta,0}\\) and \\(\\Theta_{\\beta}\\) \\(-2 \\log \\lambda_N \\dot{\\sim} \\chi^2_{df}\\) where df is the difference in the dimension (i.e., number of parameters) of \\(\\Theta_{\\beta,0}\\) and \\(\\Theta_{\\beta}\\) This method is not applicable for REML. But REML can still be used to test for covariance parameters between nested models. 8.3.2 Variance Components For ML and REML estimator, \\(\\hat{\\theta} \\sim N(\\theta, I(\\theta))\\) for large samples Wald test in variance components is analogous to the fixed effects case (see 8.3.1.1 ) However, the normal approximation depends largely on the true value of \\(\\theta\\). It will fail if the true value of \\(\\theta\\) is close to the boundary of the parameter space \\(\\Theta_{\\theta}\\) (i.e., \\(\\sigma^2 \\approx 0\\)) Typically works better for covariance parameter, than variance parameters. The likelihood ratio tests can also be used with ML or REML estimates. However, the same problem of parameters "],["information-criteria.html", "8.4 Information Criteria", " 8.4 Information Criteria account for the likelihood and the number of parameters to assess model comparison. 8.4.1 Akaike’s Information Criteria (AIC) Derived as an estimator of the expected Kullback discrepancy between the true model and a fitted candidate model \\[ AIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + 2q \\] where \\(l(\\hat{\\theta}, \\hat{\\beta})\\) is the log-likelihood q = the effective number of parameters; total of fixed and those associated with random effects (variance/covariance; those not estimated to be on a boundary constraint) Note: In comparing models that differ in their random effects, this method is not advised to due the inability to get the correct number of effective parameters). We prefer smaller AIC values. If your program uses \\(l-q\\) then we prefer larger AIC values (but rarely). can be used for mixed model section, (e.g., selection of the covariance structure), but the sample size must be very large to have adequate comparison based on the criterion Can have a large negative bias (e.g., when sample size is small but the number of parameters is large) due to the penalty term can’t approximate the bias adjustment adequately 8.4.2 Corrected AIC (AICC) developed by (Hurvich and Tsai 1989) correct small-sample adjustment depends on the candidate model class Only if you have fixed covariance structure, then AICC is justified, but not general covariance structure 8.4.3 Bayesian Information Criteria (BIC) \\[ BIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + q \\log n \\] where n = number of observations. we prefer smaller BIC value BIC and AIC are used for both REML and MLE if we have the same mean structure. Otherwise, in general, we should prefer MLE With our example presented at the beginning of Linear Mixed Models, \\[ Y_{ik}= \\begin{cases} \\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} &amp; L \\\\ \\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} &amp; H\\\\ \\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} &amp; C \\end{cases} \\] where \\(i = 1,..,N\\) \\(j = 1,..,n_i\\) (measures at time \\(t_{ij}\\)) Note: we have subject-specific intercepts, \\[ \\begin{aligned} \\mathbf{Y}_i |b_i &amp;\\sim N(\\mathbf{X}_i \\beta + 1 b_i, \\sigma^2 \\mathbf{I}) \\\\ b_i &amp;\\sim N(0,d_{11}) \\end{aligned} \\] here, we want to estimate \\(\\beta, \\sigma^2, d_{11}\\) and predict \\(b_i\\) References "],["split-plot-designs.html", "8.5 Split-Plot Designs", " 8.5 Split-Plot Designs Typically used in the case that you have two factors where one needs much larger units than the other. Example: A: 3 levels (large units) B: 2 levels (small units) A and B levels are randomized into 4 blocks. But it differs from Randomized Block Designs. In each block, both have one of the 6 (3x2) treatment combinations. But Randomized Block Designs assign in each block randomly, while split-plot does not randomize this step. Moreover, because A needs to be applied in large units, factor A is applied only once in each block while B can be applied multiple times. Hence, we have our model If A is our factor of interest \\[ Y_{ij} = \\mu + \\rho_i + \\alpha_j + e_{ij} \\] where \\(i\\) = replication (block or subject) \\(j\\) = level of Factor A \\(\\mu\\) = overall mean \\(\\rho_i\\) = variation due to the \\(i\\)-th block \\(e_{ij} \\sim N(0, \\sigma^2_e)\\) = whole plot error If B is our factor of interest \\[ Y_{ijk} = \\mu + \\phi_{ij} + \\beta_k + \\epsilon_{ijk} \\] where \\(\\phi_{ij}\\) = variation due to the \\(ij\\)-th main plot \\(\\beta_k\\) = Factor B effect \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_\\epsilon)\\) = subplot error \\(\\phi_{ij} = \\rho_i + \\alpha_j + e_{ij}\\) Together, the split-plot model \\[ Y_{ijk} = \\mu + \\rho_i + \\alpha_j + e_{ij} + \\beta_k + (\\alpha \\beta)_{jk} + \\epsilon_{ijk} \\] where \\(i\\) = replicate (blocks or subjects) \\(j\\) = level of factor A \\(k\\) = level of factor B \\(\\mu\\) = overall mean \\(\\rho_i\\) = effect of the block \\(\\alpha_j\\) = main effect of factor A (fixed) \\(e_{ij} = (\\rho \\alpha)_{ij}\\) = block by factor A interaction (the whole plot error, random) \\(\\beta_k\\) = main effect of factor B (fixed) \\((\\alpha \\beta)_{jk}\\) = interaction between factors A and B (fixed) \\(\\epsilon_{ijk}\\) = subplot error (random) We can approach sub-plot analysis based on the ANOVA perspective Whole plot comparisons Compare factor A to the whole plot error (i.e., \\(\\alpha_j\\) to \\(e_{ij}\\)) Compare the block to the whole plot error (i.e., \\(\\rho_i\\) to \\(e_{ij}\\)) Sub-plot comparisons: Compare factor B to the subplot error (\\(\\beta\\) to \\(\\epsilon_{ijk}\\)) Compare the AB interaction to the subplot error (\\((\\alpha \\beta)_{jk}\\) to \\(\\epsilon_{ijk}\\)) the mixed model perspective \\[ \\mathbf{Y = X \\beta + Zb + \\epsilon} \\] 8.5.1 Application 8.5.1.1 Example 1 \\[ y_{ijk} = \\mu + i_i + v_j + (iv)_{ij} + f_k + \\epsilon_{ijk} \\] where \\(y_{ijk}\\) = observed yield \\(\\mu\\) = overall average yield \\(i_i\\) = irrigation effect \\(v_j\\) = variety effect \\((iv)_{ij}\\) = irrigation by variety interaction \\(f_k\\) = random field (block) effect \\(\\epsilon_{ijk}\\) = residual because variety-field combination is only observed once, we can’t have the random interaction effects between variety and field library(ggplot2) data(irrigation, package = &quot;faraway&quot;) summary(irrigation) #&gt; field irrigation variety yield #&gt; f1 :2 i1:4 v1:8 Min. :34.80 #&gt; f2 :2 i2:4 v2:8 1st Qu.:37.60 #&gt; f3 :2 i3:4 Median :40.15 #&gt; f4 :2 i4:4 Mean :40.23 #&gt; f5 :2 3rd Qu.:42.73 #&gt; f6 :2 Max. :47.60 #&gt; (Other):4 head(irrigation, 4) #&gt; field irrigation variety yield #&gt; 1 f1 i1 v1 35.4 #&gt; 2 f1 i1 v2 37.9 #&gt; 3 f2 i2 v1 36.7 #&gt; 4 f2 i2 v2 38.2 ggplot(irrigation, aes( x = field, y = yield, shape = irrigation, color = variety )) + geom_point(size = 3) sp_model &lt;- lmerTest::lmer(yield ~ irrigation * variety + (1 |field), irrigation) summary(sp_model) #&gt; Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ #&gt; lmerModLmerTest] #&gt; Formula: yield ~ irrigation * variety + (1 | field) #&gt; Data: irrigation #&gt; #&gt; REML criterion at convergence: 45.4 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.7448 -0.5509 0.0000 0.5509 0.7448 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; field (Intercept) 16.200 4.025 #&gt; Residual 2.107 1.452 #&gt; Number of obs: 16, groups: field, 8 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error df t value Pr(&gt;|t|) #&gt; (Intercept) 38.500 3.026 4.487 12.725 0.000109 *** #&gt; irrigationi2 1.200 4.279 4.487 0.280 0.791591 #&gt; irrigationi3 0.700 4.279 4.487 0.164 0.877156 #&gt; irrigationi4 3.500 4.279 4.487 0.818 0.454584 #&gt; varietyv2 0.600 1.452 4.000 0.413 0.700582 #&gt; irrigationi2:varietyv2 -0.400 2.053 4.000 -0.195 0.855020 #&gt; irrigationi3:varietyv2 -0.200 2.053 4.000 -0.097 0.927082 #&gt; irrigationi4:varietyv2 1.200 2.053 4.000 0.584 0.590265 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2 #&gt; irrigation2 -0.707 #&gt; irrigation3 -0.707 0.500 #&gt; irrigation4 -0.707 0.500 0.500 #&gt; varietyv2 -0.240 0.170 0.170 0.170 #&gt; irrgtn2:vr2 0.170 -0.240 -0.120 -0.120 -0.707 #&gt; irrgtn3:vr2 0.170 -0.120 -0.240 -0.120 -0.707 0.500 #&gt; irrgtn4:vr2 0.170 -0.120 -0.120 -0.240 -0.707 0.500 0.500 anova(sp_model, ddf = c(&quot;Kenward-Roger&quot;)) #&gt; Type III Analysis of Variance Table with Kenward-Roger&#39;s method #&gt; Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) #&gt; irrigation 2.4545 0.81818 3 4 0.3882 0.7685 #&gt; variety 2.2500 2.25000 1 4 1.0676 0.3599 #&gt; irrigation:variety 1.5500 0.51667 3 4 0.2452 0.8612 Since p-value of the interaction term is insignificant, we consider fitting without it. library(lme4) sp_model_additive &lt;- lmer(yield ~ irrigation + variety + (1 | field), irrigation) anova(sp_model_additive,sp_model,ddf = &quot;Kenward-Roger&quot;) #&gt; Data: irrigation #&gt; Models: #&gt; sp_model_additive: yield ~ irrigation + variety + (1 | field) #&gt; sp_model: yield ~ irrigation * variety + (1 | field) #&gt; npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) #&gt; sp_model_additive 7 83.959 89.368 -34.980 69.959 #&gt; sp_model 10 88.609 96.335 -34.305 68.609 1.3503 3 0.7172 Since \\(p\\)-value of \\(\\chi^2\\) test is insignificant, we can’t reject the additive model is already sufficient. Looking at AIC and BIC, we can also see that we would prefer the additive model Random Effect Examination exactRLRT test \\(H_0\\): Var(random effect) (i.e., \\(\\sigma^2\\))= 0 \\(H_a\\): Var(random effect) (i.e., \\(\\sigma^2\\)) &gt; 0 sp_model &lt;- lme4::lmer(yield ~ irrigation * variety + (1 | field), irrigation) library(RLRsim) exactRLRT(sp_model) #&gt; #&gt; simulated finite sample distribution of RLRT. #&gt; #&gt; (p-value based on 10000 simulated values) #&gt; #&gt; data: #&gt; RLRT = 6.1118, p-value = 0.0087 Since the p-value is significant, we reject \\(H_0\\) "],["repeated-measures-in-mixed-models.html", "8.6 Repeated Measures in Mixed Models", " 8.6 Repeated Measures in Mixed Models \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\delta_{i(k)}+ \\epsilon_{ijk} \\] where \\(i\\)-th group (fixed) \\(j\\)-th (repeated measure) time effect (fixed) \\(k\\)-th subject \\(\\delta_{i(k)} \\sim N(0,\\sigma^2_\\delta)\\) (k-th subject in the \\(i\\)-th group) and \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent error) are random effects (\\(i = 1,..,n_A, j = 1,..,n_B, k = 1,...,n_i\\)) hence, the variance-covariance matrix of the repeated observations on the k-th subject of the i-th group, \\(\\mathbf{Y}_{ik} = (Y_{i1k},..,Y_{in_Bk})&#39;\\), will be \\[ \\begin{aligned} \\mathbf{\\Sigma}_{subject} &amp;= \\left( \\begin{array} {cccc} \\sigma^2_\\delta + \\sigma^2 &amp; \\sigma^2_\\delta &amp; ... &amp; \\sigma^2_\\delta \\\\ \\sigma^2_\\delta &amp; \\sigma^2_\\delta +\\sigma^2 &amp; ... &amp; \\sigma^2_\\delta \\\\ . &amp; . &amp; . &amp; . \\\\ \\sigma^2_\\delta &amp; \\sigma^2_\\delta &amp; ... &amp; \\sigma^2_\\delta + \\sigma^2 \\\\ \\end{array} \\right) \\\\ &amp;= (\\sigma^2_\\delta + \\sigma^2) \\left( \\begin{array} {cccc} 1 &amp; \\rho &amp; ... &amp; \\rho \\\\ \\rho &amp; 1 &amp; ... &amp; \\rho \\\\ . &amp; . &amp; . &amp; . \\\\ \\rho &amp; \\rho &amp; ... &amp; 1 \\\\ \\end{array} \\right) &amp; \\text{product of a scalar and a correlation matrix} \\end{aligned} \\] where \\(\\rho = \\frac{\\sigma^2_\\delta}{\\sigma^2_\\delta + \\sigma^2}\\), which is the compound symmetry structure that we discussed in Random-Intercepts Model But if you only have repeated measurements on the subject over time, AR(1) structure might be more appropriate Mixed model for a repeated measure \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\] where \\(\\epsilon_{ijk}\\) combines random error of both the whole and subplots. In general, \\[ \\mathbf{Y = X \\beta + \\epsilon} \\] where \\(\\epsilon \\sim N(0, \\sigma^2 \\mathbf{\\Sigma})\\) where \\(\\mathbf{\\Sigma}\\) is block diagonal if the random error covariance is the same for each subject The variance covariance matrix with AR(1) structure is \\[ \\mathbf{\\Sigma}_{subject} = \\sigma^2 \\left( \\begin{array} {ccccc} 1 &amp; \\rho &amp; \\rho^2 &amp; ... &amp; \\rho^{n_B-1} \\\\ \\rho &amp; 1 &amp; \\rho &amp; ... &amp; \\rho^{n_B-2} \\\\ . &amp; . &amp; . &amp; . &amp; . \\\\ \\rho^{n_B-1} &amp; \\rho^{n_B-2} &amp; \\rho^{n_B-3} &amp; ... &amp; 1 \\\\ \\end{array} \\right) \\] Hence, the mixed model for a repeated measure can be written as \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\] where \\(\\epsilon_{ijk}\\) = random error of whole and subplots Generally, \\[ \\mathbf{Y = X \\beta + \\epsilon} \\] where \\(\\epsilon \\sim N(0, \\mathbf{\\sigma^2 \\Sigma})\\) and \\(\\Sigma\\) = block diagonal if the random error covariance is the same for each subject. "],["unbalanced-or-unequally-spaced-data.html", "8.7 Unbalanced or Unequally Spaced Data", " 8.7 Unbalanced or Unequally Spaced Data Consider the model \\[ Y_{ikt} = \\beta_0 + \\beta_{0i} + \\beta_{1}t + \\beta_{1i}t + \\beta_{2} t^2 + \\beta_{2i} t^2 + \\epsilon_{ikt} \\] where \\(i = 1,2\\) (groups) \\(k = 1,…, n_i\\) ( individuals) \\(t = (t_1,t_2,t_3,t_4)\\) (times) \\(\\beta_{2i}\\) = common quadratic term \\(\\beta_{1i}\\) = common linear time trends \\(\\beta_{0i}\\) = common intercepts Then, we assume the variance-covariance matrix of the repeated measurements collected on a particular subject over time has the form \\[ \\mathbf{\\Sigma}_{ik} = \\sigma^2 \\left( \\begin{array} {cccc} 1 &amp; \\rho^{t_2-t_1} &amp; \\rho^{t_3-t_1} &amp; \\rho^{t_4-t_1} \\\\ \\rho^{t_2-t_1} &amp; 1 &amp; \\rho^{t_3-t_2} &amp; \\rho^{t_4-t_2} \\\\ \\rho^{t_3-t_1} &amp; \\rho^{t_3-t_2} &amp; 1 &amp; \\rho^{t_4-t_3} \\\\ \\rho^{t_4-t_1} &amp; \\rho^{t_4-t_2} &amp; \\rho^{t_4-t_3} &amp; 1 \\end{array} \\right) \\] which is called “power” covariance model We can consider \\(\\beta_{2i} , \\beta_{1i}, \\beta_{0i}\\) accordingly to see whether these terms are needed in the final model "],["application-5.html", "8.8 Application", " 8.8 Application R Packages for mixed models nlme has nested structure flexible for complex design not user-friendly lme4 computationally efficient user-friendly can handle non-normal response for more detailed application, check Fitting Linear Mixed-Effects Models Using lme4 Others Bayesian setting: MCMCglmm, brms For genetics: ASReml 8.8.1 Example 1 (Pulps) Model: \\[ y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij} \\] where \\(i = 1,..,a\\) groups for random effect \\(\\alpha_i\\) \\(j = 1,...,n\\) individuals in each group \\(\\alpha_i \\sim N(0, \\sigma^2_\\alpha)\\) is random effects \\(\\epsilon_{ij} \\sim N(0, \\sigma^2_\\epsilon)\\) is random effects Imply compound symmetry model where the intraclass correlation coefficient is: \\(\\rho = \\frac{\\sigma^2_\\alpha}{\\sigma^2_\\alpha + \\sigma^2_\\epsilon}\\) If factor \\(a\\) does not explain much variation, low correlation within the levels: \\(\\sigma^2_\\alpha \\to 0\\) then \\(\\rho \\to 0\\) If factor \\(a\\) explain much variation, high correlation within the levels \\(\\sigma^2_\\alpha \\to \\infty\\) hence, \\(\\rho \\to 1\\) data(pulp, package = &quot;faraway&quot;) plot( y = pulp$bright, x = pulp$operator, xlab = &quot;Operator&quot;, ylab = &quot;Brightness&quot; ) pulp %&gt;% dplyr::group_by(operator) %&gt;% dplyr::summarise(average = mean(bright)) #&gt; # A tibble: 4 × 2 #&gt; operator average #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 a 60.2 #&gt; 2 b 60.1 #&gt; 3 c 60.6 #&gt; 4 d 60.7 lmer application library(lme4) mixed_model &lt;- lmer( # pipe (i..e, | ) denotes random-effect terms formula = bright ~ 1 + (1 |operator), data = pulp) summary(mixed_model) #&gt; Linear mixed model fit by REML [&#39;lmerMod&#39;] #&gt; Formula: bright ~ 1 + (1 | operator) #&gt; Data: pulp #&gt; #&gt; REML criterion at convergence: 18.6 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.4666 -0.7595 -0.1244 0.6281 1.6012 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; operator (Intercept) 0.06808 0.2609 #&gt; Residual 0.10625 0.3260 #&gt; Number of obs: 20, groups: operator, 4 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error t value #&gt; (Intercept) 60.4000 0.1494 404.2 coef(mixed_model) #&gt; $operator #&gt; (Intercept) #&gt; a 60.27806 #&gt; b 60.14088 #&gt; c 60.56767 #&gt; d 60.61340 #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;coef.mer&quot; fixef(mixed_model) # fixed effects #&gt; (Intercept) #&gt; 60.4 confint(mixed_model) # confidence interval #&gt; 2.5 % 97.5 % #&gt; .sig01 0.000000 0.6178987 #&gt; .sigma 0.238912 0.4821845 #&gt; (Intercept) 60.071299 60.7287012 ranef(mixed_model) # random effects #&gt; $operator #&gt; (Intercept) #&gt; a -0.1219403 #&gt; b -0.2591231 #&gt; c 0.1676679 #&gt; d 0.2133955 #&gt; #&gt; with conditional variances for &quot;operator&quot; VarCorr(mixed_model) # random effects standard deviation #&gt; Groups Name Std.Dev. #&gt; operator (Intercept) 0.26093 #&gt; Residual 0.32596 re_dat = as.data.frame(VarCorr(mixed_model)) # rho based on the above formula rho = re_dat[1, &#39;vcov&#39;] / (re_dat[1, &#39;vcov&#39;] + re_dat[2, &#39;vcov&#39;]) rho #&gt; [1] 0.3905354 To Satterthwaite approximation for the denominator df, we use lmerTest library(lmerTest) summary(lmerTest::lmer(bright ~ 1 + (1 | operator), pulp))$coefficients #&gt; Estimate Std. Error df t value Pr(&gt;|t|) #&gt; (Intercept) 60.4 0.1494434 3 404.1664 3.340265e-08 confint(mixed_model)[3, ] #&gt; 2.5 % 97.5 % #&gt; 60.0713 60.7287 In this example, we can see that the confidence interval computed by confint in lmer package is very close is confint in lmerTest model. MCMglmm application under the Bayesian framework library(MCMCglmm) mixed_model_bayes &lt;- MCMCglmm( bright ~ 1, random = ~ operator, data = pulp, verbose = FALSE ) summary(mixed_model_bayes)$solutions #&gt; post.mean l-95% CI u-95% CI eff.samp pMCMC #&gt; (Intercept) 60.40449 60.2055 60.66595 1000 0.001 this method offers the confidence interval slightly more positive than lmer and lmerTest 8.8.1.1 Prediction # random effects prediction (BLUPs) ranef(mixed_model)$operator #&gt; (Intercept) #&gt; a -0.1219403 #&gt; b -0.2591231 #&gt; c 0.1676679 #&gt; d 0.2133955 # prediction for each categories fixef(mixed_model) + ranef(mixed_model)$operator #&gt; (Intercept) #&gt; a 60.27806 #&gt; b 60.14088 #&gt; c 60.56767 #&gt; d 60.61340 # equivalent to the above method predict(mixed_model, newdata = data.frame(operator = c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;))) #&gt; 1 2 3 4 #&gt; 60.27806 60.14088 60.56767 60.61340 use bootMer() to get bootstrap-based confidence intervals for predictions. Another example using GLMM in the context of blocking Penicillin data data(penicillin, package = &quot;faraway&quot;) summary(penicillin) #&gt; treat blend yield #&gt; A:5 Blend1:4 Min. :77 #&gt; B:5 Blend2:4 1st Qu.:81 #&gt; C:5 Blend3:4 Median :87 #&gt; D:5 Blend4:4 Mean :86 #&gt; Blend5:4 3rd Qu.:89 #&gt; Max. :97 library(ggplot2) ggplot(penicillin, aes( y = yield, x = treat, shape = blend, color = blend )) + # treatment = fixed effect # blend = random effects geom_point(size = 3) + xlab(&quot;Treatment&quot;) library(lmerTest) # for p-values mixed_model &lt;- lmerTest::lmer(yield ~ treat + (1 | blend), data = penicillin) summary(mixed_model) #&gt; Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ #&gt; lmerModLmerTest] #&gt; Formula: yield ~ treat + (1 | blend) #&gt; Data: penicillin #&gt; #&gt; REML criterion at convergence: 103.8 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.4152 -0.5017 -0.1644 0.6830 1.2836 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; blend (Intercept) 11.79 3.434 #&gt; Residual 18.83 4.340 #&gt; Number of obs: 20, groups: blend, 5 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error df t value Pr(&gt;|t|) #&gt; (Intercept) 84.000 2.475 11.075 33.941 1.51e-12 *** #&gt; treatB 1.000 2.745 12.000 0.364 0.7219 #&gt; treatC 5.000 2.745 12.000 1.822 0.0935 . #&gt; treatD 2.000 2.745 12.000 0.729 0.4802 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) treatB treatC #&gt; treatB -0.555 #&gt; treatC -0.555 0.500 #&gt; treatD -0.555 0.500 0.500 #The BLUPs for the each blend ranef(mixed_model)$blend #&gt; (Intercept) #&gt; Blend1 4.2878788 #&gt; Blend2 -2.1439394 #&gt; Blend3 -0.7146465 #&gt; Blend4 1.4292929 #&gt; Blend5 -2.8585859 Examine treatment effect anova(mixed_model) # p-value based on lmerTest #&gt; Type III Analysis of Variance Table with Satterthwaite&#39;s method #&gt; Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) #&gt; treat 70 23.333 3 12 1.2389 0.3387 Since the p-value is greater than 0.05, we can’t reject the null hypothesis that there is no treatment effect. library(pbkrtest) # REML is not appropriate for testing fixed effects, it should be ML full_model &lt;- lmer(yield ~ treat + (1 | blend), penicillin, REML = FALSE) null_model &lt;- lmer(yield ~ 1 + (1 | blend), penicillin, REML = FALSE) # use Kenward-Roger approximation for df KRmodcomp(full_model, null_model) #&gt; large : yield ~ treat + (1 | blend) #&gt; small : yield ~ 1 + (1 | blend) #&gt; stat ndf ddf F.scaling p.value #&gt; Ftest 1.2389 3.0000 12.0000 1 0.3387 Since the p-value is greater than 0.05, and consistent with our previous observation, we conclude that we can’t reject the null hypothesis that there is no treatment effect. 8.8.2 Example 2 (Rats) rats &lt;- read.csv( &quot;images/rats.dat&quot;, header = F, sep = &#39; &#39;, col.names = c(&#39;Treatment&#39;, &#39;rat&#39;, &#39;age&#39;, &#39;y&#39;) ) # log transformed age rats$t &lt;- log(1 + (rats$age - 45) / 10) We are interested in whether treatment effect induces changes over time. rat_model &lt;- # treatment = fixed effect, rat = random effects lmerTest::lmer(y ~ t:Treatment + (1 | rat), data = rats) summary(rat_model) #&gt; Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ #&gt; lmerModLmerTest] #&gt; Formula: y ~ t:Treatment + (1 | rat) #&gt; Data: rats #&gt; #&gt; REML criterion at convergence: 932.4 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.25574 -0.65898 -0.01163 0.58356 2.88309 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; rat (Intercept) 3.565 1.888 #&gt; Residual 1.445 1.202 #&gt; Number of obs: 252, groups: rat, 50 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error df t value Pr(&gt;|t|) #&gt; (Intercept) 68.6074 0.3312 89.0275 207.13 &lt;2e-16 *** #&gt; t:Treatmentcon 7.3138 0.2808 247.2762 26.05 &lt;2e-16 *** #&gt; t:Treatmenthig 6.8711 0.2276 247.7097 30.19 &lt;2e-16 *** #&gt; t:Treatmentlow 7.5069 0.2252 247.5196 33.34 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) t:Trtmntc t:Trtmnth #&gt; t:Tretmntcn -0.327 #&gt; t:Tretmnthg -0.340 0.111 #&gt; t:Tretmntlw -0.351 0.115 0.119 anova(rat_model) #&gt; Type III Analysis of Variance Table with Satterthwaite&#39;s method #&gt; Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) #&gt; t:Treatment 3181.9 1060.6 3 223.21 734.11 &lt; 2.2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since the p-value is significant, we can be confident concluding that there is a treatment effect 8.8.3 Example 3 (Agridat) library(agridat) library(latticeExtra) dat &lt;- harris.wateruse # Compare to Schabenberger &amp; Pierce, fig 7.23 useOuterStrips( xyplot( water ~ day | species * age, dat, as.table = TRUE, group = tree, type = c(&#39;p&#39;, &#39;smooth&#39;), main = &quot;harris.wateruse 2 species, 2 ages (10 trees each)&quot; ) ) Remove outliers dat &lt;- subset(dat, day!=268) Plot between age and species xyplot( water ~ day | tree, dat, subset = age == &quot;A2&quot; &amp; species == &quot;S2&quot;, as.table = TRUE, type = c(&#39;p&#39;, &#39;smooth&#39;), ylab = &quot;Water use profiles of individual trees&quot;, main = &quot;harris.wateruse (Age 2, Species 2)&quot; ) # Rescale day for nicer output, # and convergence issues, add quadratic term dat &lt;- transform(dat, ti = day / 100) dat &lt;- transform(dat, ti2 = ti * ti) # Start with a subgroup: age 2, species 2 d22 &lt;- droplevels(subset(dat, age == &quot;A2&quot; &amp; species == &quot;S2&quot;)) lme function from nlme package library(nlme) ## We use pdDiag() to get uncorrelated random effects m1n &lt;- lme( water ~ 1 + ti + ti2, #intercept, time and time-squared = fixed effects data = d22, na.action = na.omit, random = list(tree = pdDiag(~ 1 + ti + ti2)) # random intercept, time # and time squared per tree = random effects ) ranef(m1n) #&gt; (Intercept) ti ti2 #&gt; T04 0.1985796 1.609864e-09 4.990101e-10 #&gt; T05 0.3492827 2.487690e-10 -4.845287e-11 #&gt; T19 -0.1978989 -7.681202e-10 -1.961453e-10 #&gt; T23 0.4519003 -3.270426e-10 -2.413583e-10 #&gt; T38 -0.6457494 -1.608770e-09 -3.298010e-10 #&gt; T40 0.3739432 3.264705e-10 -2.543109e-11 #&gt; T49 0.8620648 9.021831e-10 -5.402247e-12 #&gt; T53 -0.5655049 -8.279040e-10 -4.579291e-11 #&gt; T67 -0.4394623 -3.485113e-10 2.147434e-11 #&gt; T71 -0.3871552 7.930610e-10 3.718993e-10 fixef(m1n) #&gt; (Intercept) ti ti2 #&gt; -10.798799 12.346704 -2.838503 summary(m1n) #&gt; Linear mixed-effects model fit by REML #&gt; Data: d22 #&gt; AIC BIC logLik #&gt; 276.5142 300.761 -131.2571 #&gt; #&gt; Random effects: #&gt; Formula: ~1 + ti + ti2 | tree #&gt; Structure: Diagonal #&gt; (Intercept) ti ti2 Residual #&gt; StdDev: 0.5187869 1.438333e-05 3.864019e-06 0.3836614 #&gt; #&gt; Fixed effects: water ~ 1 + ti + ti2 #&gt; Value Std.Error DF t-value p-value #&gt; (Intercept) -10.798799 0.8814666 227 -12.25094 0 #&gt; ti 12.346704 0.7827112 227 15.77428 0 #&gt; ti2 -2.838503 0.1720614 227 -16.49704 0 #&gt; Correlation: #&gt; (Intr) ti #&gt; ti -0.979 #&gt; ti2 0.970 -0.997 #&gt; #&gt; Standardized Within-Group Residuals: #&gt; Min Q1 Med Q3 Max #&gt; -3.07588246 -0.58531056 0.01210209 0.65402695 3.88777402 #&gt; #&gt; Number of Observations: 239 #&gt; Number of Groups: 10 lmer function from lme4 package m1lmer &lt;- lmer(water ~ 1 + ti + ti2 + (ti + ti2 || tree), data = d22, na.action = na.omit) ranef(m1lmer) #&gt; $tree #&gt; (Intercept) ti ti2 #&gt; T04 0.1985796 0 0 #&gt; T05 0.3492827 0 0 #&gt; T19 -0.1978989 0 0 #&gt; T23 0.4519003 0 0 #&gt; T38 -0.6457494 0 0 #&gt; T40 0.3739432 0 0 #&gt; T49 0.8620648 0 0 #&gt; T53 -0.5655049 0 0 #&gt; T67 -0.4394623 0 0 #&gt; T71 -0.3871552 0 0 #&gt; #&gt; with conditional variances for &quot;tree&quot; Notes: || double pipes= uncorrelated random effects To remove the intercept term: (0+ti|tree) (ti-1|tree) fixef(m1lmer) #&gt; (Intercept) ti ti2 #&gt; -10.798799 12.346704 -2.838503 m1l &lt;- lmer(water ~ 1 + ti + ti2 + (1 | tree) + (0 + ti | tree) + (0 + ti2 | tree), data = d22) ranef(m1l) #&gt; $tree #&gt; (Intercept) ti ti2 #&gt; T04 0.1985796 0 0 #&gt; T05 0.3492827 0 0 #&gt; T19 -0.1978989 0 0 #&gt; T23 0.4519003 0 0 #&gt; T38 -0.6457494 0 0 #&gt; T40 0.3739432 0 0 #&gt; T49 0.8620648 0 0 #&gt; T53 -0.5655049 0 0 #&gt; T67 -0.4394623 0 0 #&gt; T71 -0.3871552 0 0 #&gt; #&gt; with conditional variances for &quot;tree&quot; fixef(m1l) #&gt; (Intercept) ti ti2 #&gt; -10.798799 12.346704 -2.838503 To include structured covariance terms, we can use the following way m2n &lt;- lme( water ~ 1 + ti + ti2, data = d22, random = ~ 1 | tree, cor = corExp(form = ~ day | tree), na.action = na.omit ) ranef(m2n) #&gt; (Intercept) #&gt; T04 0.1929971 #&gt; T05 0.3424631 #&gt; T19 -0.1988495 #&gt; T23 0.4538660 #&gt; T38 -0.6413664 #&gt; T40 0.3769378 #&gt; T49 0.8410043 #&gt; T53 -0.5528236 #&gt; T67 -0.4452930 #&gt; T71 -0.3689358 fixef(m2n) #&gt; (Intercept) ti ti2 #&gt; -11.223310 12.712094 -2.913682 summary(m2n) #&gt; Linear mixed-effects model fit by REML #&gt; Data: d22 #&gt; AIC BIC logLik #&gt; 263.3081 284.0911 -125.654 #&gt; #&gt; Random effects: #&gt; Formula: ~1 | tree #&gt; (Intercept) Residual #&gt; StdDev: 0.5154042 0.3925777 #&gt; #&gt; Correlation Structure: Exponential spatial correlation #&gt; Formula: ~day | tree #&gt; Parameter estimate(s): #&gt; range #&gt; 3.794624 #&gt; Fixed effects: water ~ 1 + ti + ti2 #&gt; Value Std.Error DF t-value p-value #&gt; (Intercept) -11.223310 1.0988725 227 -10.21348 0 #&gt; ti 12.712094 0.9794235 227 12.97916 0 #&gt; ti2 -2.913682 0.2148551 227 -13.56115 0 #&gt; Correlation: #&gt; (Intr) ti #&gt; ti -0.985 #&gt; ti2 0.976 -0.997 #&gt; #&gt; Standardized Within-Group Residuals: #&gt; Min Q1 Med Q3 Max #&gt; -3.04861039 -0.55703950 0.00278101 0.62558762 3.80676991 #&gt; #&gt; Number of Observations: 239 #&gt; Number of Groups: 10 "],["nonlinear-and-generalized-linear-mixed-models.html", "Chapter 9 Nonlinear and Generalized Linear Mixed Models", " Chapter 9 Nonlinear and Generalized Linear Mixed Models NLMMs extend the nonlinear model to include both fixed effects and random effects GLMMs extend the generalized linear model to include both fixed effects and random effects. A nonlinear mixed model has the form of \\[ Y_{ij} = f(\\mathbf{x_{ij} , \\theta, \\alpha_i}) + \\epsilon_{ij} \\] for the j-th response from cluster (or subject) i (\\(i = 1,...,n\\)), where \\(j = 1,...,n_i\\) \\(\\mathbf{\\theta}\\) are the fixed effects \\(\\mathbf{\\alpha}_i\\) are the random effects for cluster i \\(\\mathbf{x}_{ij}\\) are the regressors or design variables \\(f(.)\\) is nonlinear mean response function A GLMM can be written as: we assume \\[ y_i |\\alpha_i \\sim \\text{indep } f(y_i | \\alpha) \\] and \\(f(y_i | \\mathbf{\\alpha})\\) is an exponential family distribution, \\[ f(y_i | \\alpha) = \\exp [\\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)} - c(y_i, \\phi)] \\] The conditional mean of \\(y_i\\) is related to \\(\\theta_i\\) \\[ \\mu_i = \\frac{\\partial b(\\theta_i)}{\\partial \\theta_i} \\] The transformation of this mean will give us the desired linear model to model both the fixed and random effects. \\[ \\begin{aligned} E(y_i |\\alpha) &amp;= \\mu_i \\\\ g(\\mu_i) &amp;= \\mathbf{x_i&#39; \\beta + z&#39;_i \\alpha} \\end{aligned} \\] where \\(g()\\) is a known link function and \\(\\mu_i\\) is the conditional mean. We can see similarity to GLM We also have to specify the random effects distribution \\[ \\alpha \\sim f(\\alpha) \\] which is similar to the specification for mixed models. Moreover, law of large number applies to fixed effects so that you know it is a normal distribution. But here, you can specify \\(\\alpha\\) subjectively. Hence, we can show NLMM is a special case of the GLMM \\[ \\begin{aligned} \\mathbf{Y}_i &amp;= \\mathbf{f}(\\mathbf{x}_i, \\mathbf{\\theta, \\alpha}_i) + \\mathbf{\\epsilon}_i \\\\ \\mathbf{Y}_i &amp;= \\mathbf{g}^{-1} (\\mathbf{x}_i&#39; \\beta + \\mathbf{z}_i&#39; \\mathbf{\\alpha}_i) + \\mathbf{\\epsilon}_i \\end{aligned} \\] where the inverse link function corresponds to a nonlinear transformation of the fixed and random effects. Note: we can’t derive the analytical formulation of the marginal distribution because nonlinear combination of normal variables is not normally distributed, even in the case of additive error (\\(e_i\\)) and random effects (\\(\\alpha_i\\)) are both normal. Consequences of having random effects The marginal mean of \\(y_i\\) is \\[ E(y_i) = E_\\alpha(E(y_i | \\alpha)) = E_\\alpha (\\mu_i) = E(g^{-1}(\\mathbf{x_i&#39; \\beta + z_i&#39; \\alpha})) \\] Because \\(g^{-1}()\\) is nonlinear, this is the most simplified version we can go for. In special cases such as log link (\\(g(\\mu) = \\log \\mu\\) or \\(g^{-1}() = \\exp()\\)) then \\[ E(y_i) = E(\\exp(\\mathbf{x_i&#39; \\beta + z_i&#39; \\alpha})) = \\exp(\\mathbf{x&#39;_i \\beta})E(\\exp(\\mathbf{z}_i&#39;\\alpha)) \\] which is the moment generating function of \\(\\alpha\\) evaluated at \\(\\mathbf{z}_i\\) Marginal variance of \\(y_i\\) \\[ \\begin{aligned} var(y_i) &amp;= var_\\alpha (E(y_i | \\alpha)) + E_\\alpha (var(y_i | \\alpha)) \\\\ &amp;= var(\\mu_i) + E(a(\\phi) V(\\mu_i)) \\\\ &amp;= var(g^{-1} (\\mathbf{x&#39;_i \\beta + z&#39;_i \\alpha})) + E(a(\\phi)V(g^{-1} (\\mathbf{x&#39;_i \\beta + z&#39;_i \\alpha}))) \\end{aligned} \\] Without specific assumption about \\(g()\\) and/or the conditional distribution of \\(\\mathbf{y}\\), this is the most simplified version. Marginal covariance of \\(\\mathbf{y}\\) In a linear mixed model, random effects introduce a dependence among observations which share any random effect in common \\[ \\begin{aligned} cov(y_i, y_j) &amp;= cov_{\\alpha}(E(y_i | \\mathbf{\\alpha}),E(y_j | \\mathbf{\\alpha})) + E_{\\alpha}(cov(y_i, y_j | \\mathbf{\\alpha})) \\\\ &amp;= cov(\\mu_i, \\mu_j) + E(0) \\\\ &amp;= cov(g^{-1}(\\mathbf{x}_i&#39; \\beta + \\mathbf{z}_i&#39; \\mathbf{\\alpha}), g^{-1}(\\mathbf{x}&#39;_j \\beta + \\mathbf{z}_j&#39; \\mathbf{\\alpha})) \\end{aligned} \\] Important: conditioning to induce the covariability Example: Repeated measurements on the subjects. Let \\(y_{ij}\\) be the j-th count taken on the \\(i\\)-th subject. then, the model is \\(y_{ij} | \\mathbf{\\alpha} \\sim \\text{indep } Pois(\\mu_{ij})\\). Here \\[ \\log(\\mu_{ij}) = \\mathbf{x}_{ij}&#39; \\beta + \\alpha_i \\] where \\(\\alpha_i \\sim iid N(0,\\sigma^2_{\\alpha})\\) which is a log-link with a random patient effect. "],["estimation-3.html", "9.1 Estimation", " 9.1 Estimation In linear mixed models, the marginal likelihood for \\(\\mathbf{y}\\) is the integration of the random effects from the hierarchical formulation \\[ f(\\mathbf{y}) = \\int f(\\mathbf{y}| \\alpha) f(\\alpha) d \\alpha \\] For linear mixed models, we assumed that the 2 component distributions were Gaussian with linear relationships, which implied the marginal distribution was also linear and Gaussian and allows us to solve this integral analytically. On the other hand, GLMMs, the distribution for \\(f(\\mathbf{y} | \\alpha)\\) is not Gaussian in general, and for NLMMs, the functional form between the mean response and the random (and fixed) effects is nonlinear. In both cases, we can’t perform the integral analytically, which means we have to solve it numerically and/or linearize the inverse link function. 9.1.1 Estimation by Numerical Integration The marginal likelihood is \\[ L(\\beta; \\mathbf{y}) = \\int f(\\mathbf{y} | \\alpha) f(\\alpha) d \\alpha \\] Estimation fo the fixed effects requires \\(\\frac{\\partial l}{\\partial \\beta}\\), where \\(l\\) is the log-likelihood One way to obtain the marginal inference is to numerically integrate out the random effects through numerical quadrature Laplace approximation Monte Carlo methods When the dimension of \\(\\mathbf{\\alpha}\\) is relatively low, this is easy. But when the dimension of \\(\\alpha\\) is high, additional approximation is required. 9.1.2 Estimation by Linearization Idea: Linearized version of the response (known as working response, or pseudo-response) called \\(\\tilde{y}_i\\) and then the conditional mean is \\[ E(\\tilde{y}_i | \\alpha) = \\mathbf{x}_i&#39; \\beta + \\mathbf{z}_i&#39; \\alpha \\] and also estimate \\(var(\\tilde{y}_i | \\alpha)\\). then, apply Linear Mixed Models estimation as usual. The difference is only in how the linearization is done (i.e., how to expand \\(f(\\mathbf{x, \\theta, \\alpha})\\) or the inverse link function 9.1.2.1 Penalized quasi-likelihood (PQL) This is the more popular method \\[ \\tilde{y}_i^{(k)} = \\hat{\\eta}_i^{(k-1)} + ( y_i - \\hat{\\mu}_i^{(k-1)})\\frac{d \\eta}{d \\mu}| \\hat{\\eta}_i^{(k-1)} \\] where \\(\\eta_i = g(\\mu_i)\\) is the linear predictor \\(k\\) = iteration of the optimization algorithm The algorithm updates \\(\\tilde{y}_i\\) after each linear mixed model fit using \\(E(\\tilde{y}_i | \\alpha)\\) and \\(var(\\tilde{y}_i | \\alpha)\\) Comments: Easy to implement Inference is only asymptotically correct due to the linearizaton Biased estimates are likely for binomial response with small groups and worst for Bernoulli response. Similarly for Poisson models with small counts. (Faraway 2016) Hypothesis testing and confidence intervals also have problems. 9.1.2.2 Generalized Estimating Equations (GEE) Let a marginal generalized linear model for the mean of y as a function of the predictors, which means we linearize the mean response function and assume a dependent error structure Example Binary data: \\[ logit (E(\\mathbf{y})) = \\mathbf{X} \\beta \\] If we assume a “working covariance matrix”, \\(\\mathbf{V}\\) the the elements of \\(\\mathbf{y}\\), then the maximum likelihood equations for estimating \\(\\beta\\) is \\[ \\mathbf{X&#39;V^{-1}y} = \\mathbf{X&#39;V^{-1}} E(\\mathbf{y}) \\] If \\(\\mathbf{V}\\) is correct, then unbiased estimating equations We typically define \\(\\mathbf{V} = \\mathbf{I}\\). Solutions to unbiased estimating equation give consistent estimators. In practice, we assume a covariance structure, and then do a logistic regression, and calculate its large sample variance Let \\(y_{ij} , j = 1,..,n_i, i = 1,..,K\\) be the j-th measurement on the \\(i\\)-th subject. \\[ \\mathbf{y}_i = \\left( \\begin{array} {c} y_{i1} \\\\ . \\\\ y_{in_i} \\end{array} \\right) \\] with mean \\[ \\mathbf{\\mu}_i = \\left( \\begin{array} {c} \\mu_{i1} \\\\ . \\\\ \\mu_{in_i} \\end{array} \\right) \\] and \\[ \\mathbf{x}_{ij} = \\left( \\begin{array} {c} X_{ij1} \\\\ . \\\\ X_{ijp} \\end{array} \\right) \\] Let \\(\\mathbf{V}_i = cov(\\mathbf{y}_i)\\), then based on(Liang and Zeger 1986) GEE estimates for \\(\\beta\\) can be obtained from solving the equation: \\[ S(\\beta) = \\sum_{i=1}^K \\frac{\\partial \\mathbf{\\mu}_i&#39;}{\\partial \\beta} \\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{\\mu}_i) = 0 \\] Let \\(\\mathbf{R}_i (\\mathbf{c})\\) be an \\(n_i \\times n_i\\) “working” correlation matrix specified up to some parameters \\(\\mathbf{c}\\). Then, \\(\\mathbf{V}_i = a(\\phi) \\mathbf{B}_i^{1/2}\\mathbf{R}(\\mathbf{c}) \\mathbf{B}_i^{1/2}\\), where \\(\\mathbf{B}_i\\) is an \\(n_i \\times n_i\\) diagonal matrix with \\(V(\\mu_{ij})\\) on the j-th diagonal If \\(\\mathbf{R}(\\mathbf{c})\\) is the true correlation matrix of \\(\\mathbf{y}_i\\), then \\(\\mathbf{V}_i\\) is the true covariance matrix The working correlation matrix must be estimated iteratively by a fitting algorithm: Compute the initial estimate of \\(\\beta\\) (using GLM under the independence assumption) Compute the working correlation matrix \\(\\mathbf{R}\\) based upon studentized residuals Compute the estimate covariance \\(\\hat{\\mathbf{V}}_i\\) Update \\(\\beta\\) according to \\[ \\beta_{r+1} = \\beta_r + (\\sum_{i=1}^K \\frac{\\partial \\mathbf{\\mu}&#39;_i}{\\partial \\beta} \\hat{\\mathbf{V}}_i^{-1} \\frac{\\partial \\mathbf{\\mu}_i}{\\partial \\beta}) \\] Iterate until the algorithm converges Note: Inference based on likelihoods is not appropriate because this is not a likelihood estimator 9.1.3 Estimation by Bayesian Hierarchical Models Bayesian Estimation \\[ f(\\mathbf{\\alpha}, \\mathbf{\\beta} | \\mathbf{y}) \\propto f(\\mathbf{y} | \\mathbf{\\alpha}, \\mathbf{\\beta}) f(\\mathbf{\\alpha})f(\\mathbf{\\beta}) \\] Numerical techniques (e.g., MCMC) can be used to find posterior distribution. This method is best in terms of not having to make simplifying approximation and fully accounting for uncertainty in estimation and prediction, but it could be complex, time-consuming, and computationally intensive. Implementation Issues: No valid joint distribution can be constructed from the given conditional model and random parameters The mean/ variance relationship and the random effects lead to constraints on the marginal covariance model Difficult to fit computationally 2 types of estimation approaches: Approximate the objective function (marginal likelihood) through integral approximation Laplace methods Quadrature methods Monte Carlo integration Approximate the model (based on Taylor series linearization) Packages in R GLMM: MASS:glmmPQL lme4::glmer glmmTMB NLMM: nlme::nlme; lme4::nlmer brms::brm Bayesian: MCMCglmm ; brms:brm Example: Non-Gaussian Repeated measurements When the data are Gaussian, then Linear Mixed Models When the data are non-Gaussian, then Nonlinear and Generalized Linear Mixed Models References "],["application-6.html", "9.2 Application", " 9.2 Application 9.2.1 Binomial (CBPP Data) data(cbpp,package = &quot;lme4&quot;) head(cbpp) #&gt; herd incidence size period #&gt; 1 1 2 14 1 #&gt; 2 1 3 12 2 #&gt; 3 1 4 9 3 #&gt; 4 1 0 5 4 #&gt; 5 2 3 22 1 #&gt; 6 2 1 18 2 PQL Pro: Linearizes the response to have a pseudo-response as the mean response (like LMM) computationally efficient Cons: biased for binary, Poisson data with small counts random effects have to be interpreted on the link scale can’t interpret AIC/BIC value library(MASS) pql_cbpp &lt;- glmmPQL( cbind(incidence, size - incidence) ~ period, random = ~ 1 | herd, data = cbpp, family = binomial(link = &quot;logit&quot;), verbose = F ) summary(pql_cbpp) #&gt; Linear mixed-effects model fit by maximum likelihood #&gt; Data: cbpp #&gt; AIC BIC logLik #&gt; NA NA NA #&gt; #&gt; Random effects: #&gt; Formula: ~1 | herd #&gt; (Intercept) Residual #&gt; StdDev: 0.5563535 1.184527 #&gt; #&gt; Variance function: #&gt; Structure: fixed weights #&gt; Formula: ~invwt #&gt; Fixed effects: cbind(incidence, size - incidence) ~ period #&gt; Value Std.Error DF t-value p-value #&gt; (Intercept) -1.327364 0.2390194 38 -5.553372 0.0000 #&gt; period2 -1.016126 0.3684079 38 -2.758156 0.0089 #&gt; period3 -1.149984 0.3937029 38 -2.920944 0.0058 #&gt; period4 -1.605217 0.5178388 38 -3.099839 0.0036 #&gt; Correlation: #&gt; (Intr) perid2 perid3 #&gt; period2 -0.399 #&gt; period3 -0.373 0.260 #&gt; period4 -0.282 0.196 0.182 #&gt; #&gt; Standardized Within-Group Residuals: #&gt; Min Q1 Med Q3 Max #&gt; -2.0591168 -0.6493095 -0.2747620 0.5170492 2.6187632 #&gt; #&gt; Number of Observations: 56 #&gt; Number of Groups: 15 exp(0.556) #&gt; [1] 1.743684 is how the herd specific outcome odds varies. We can interpret the fixed effect coefficients just like in GLM. Because we use logit link function here, we can say that the log odds of the probability of having a case in period 2 is -1.016 less than period 1 (baseline). summary(pql_cbpp)$tTable #&gt; Value Std.Error DF t-value p-value #&gt; (Intercept) -1.327364 0.2390194 38 -5.553372 2.333216e-06 #&gt; period2 -1.016126 0.3684079 38 -2.758156 8.888179e-03 #&gt; period3 -1.149984 0.3937029 38 -2.920944 5.843007e-03 #&gt; period4 -1.605217 0.5178388 38 -3.099839 3.637000e-03 Numerical Integration Pro: more accurate Con: computationally expensive won’t work for complex models. library(lme4) numint_cbpp &lt;- glmer( cbind(incidence, size - incidence) ~ period + (1 | herd), data = cbpp, family = binomial(link = &quot;logit&quot;) ) summary(numint_cbpp) #&gt; Generalized linear mixed model fit by maximum likelihood (Laplace #&gt; Approximation) [glmerMod] #&gt; Family: binomial ( logit ) #&gt; Formula: cbind(incidence, size - incidence) ~ period + (1 | herd) #&gt; Data: cbpp #&gt; #&gt; AIC BIC logLik deviance df.resid #&gt; 194.1 204.2 -92.0 184.1 51 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.3816 -0.7889 -0.2026 0.5142 2.8791 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; herd (Intercept) 0.4123 0.6421 #&gt; Number of obs: 56, groups: herd, 15 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.3983 0.2312 -6.048 1.47e-09 *** #&gt; period2 -0.9919 0.3032 -3.272 0.001068 ** #&gt; period3 -1.1282 0.3228 -3.495 0.000474 *** #&gt; period4 -1.5797 0.4220 -3.743 0.000182 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) perid2 perid3 #&gt; period2 -0.363 #&gt; period3 -0.340 0.280 #&gt; period4 -0.260 0.213 0.198 For small data set, the difference between two approaches are minimal library(rbenchmark) benchmark( &quot;MASS&quot; = { pql_cbpp &lt;- glmmPQL( cbind(incidence, size - incidence) ~ period, random = ~ 1 | herd, data = cbpp, family = binomial(link = &quot;logit&quot;), verbose = F ) }, &quot;lme4&quot; = { glmer( cbind(incidence, size - incidence) ~ period + (1 | herd), data = cbpp, family = binomial(link = &quot;logit&quot;) ) }, replications = 50, columns = c(&quot;test&quot;, &quot;replications&quot;, &quot;elapsed&quot;, &quot;relative&quot;), order = &quot;relative&quot; ) #&gt; test replications elapsed relative #&gt; 1 MASS 50 3.79 1.000 #&gt; 2 lme4 50 7.79 2.055 In numerical integration, we can set nAGQ &gt; 1 to switch the method of likelihood evaluation, which might increase accuracy library(lme4) numint_cbpp_GH &lt;- glmer( cbind(incidence, size - incidence) ~ period + (1 | herd), data = cbpp, family = binomial(link = &quot;logit&quot;), nAGQ = 20 ) summary(numint_cbpp_GH)$coefficients[, 1] - summary(numint_cbpp)$coefficients[, 1] #&gt; (Intercept) period2 period3 period4 #&gt; -0.0008808634 0.0005160912 0.0004066218 0.0002644629 Bayesian approach to GLMMs assume the fixed effects parameters have distribution can handle models with intractable result under traditional methods computationally expensive library(MCMCglmm) Bayes_cbpp &lt;- MCMCglmm( cbind(incidence, size - incidence) ~ period, random = ~ herd, data = cbpp, family = &quot;multinomial2&quot;, verbose = FALSE ) summary(Bayes_cbpp) #&gt; #&gt; Iterations = 3001:12991 #&gt; Thinning interval = 10 #&gt; Sample size = 1000 #&gt; #&gt; DIC: 537.9598 #&gt; #&gt; G-structure: ~herd #&gt; #&gt; post.mean l-95% CI u-95% CI eff.samp #&gt; herd 0.03246 1.03e-16 0.2073 105.7 #&gt; #&gt; R-structure: ~units #&gt; #&gt; post.mean l-95% CI u-95% CI eff.samp #&gt; units 1.095 0.3017 2.281 306.9 #&gt; #&gt; Location effects: cbind(incidence, size - incidence) ~ period #&gt; #&gt; post.mean l-95% CI u-95% CI eff.samp pMCMC #&gt; (Intercept) -1.5247 -2.1732 -0.9248 717.5 &lt;0.001 *** #&gt; period2 -1.2812 -2.3489 -0.3661 821.7 0.012 * #&gt; period3 -1.4152 -2.3443 -0.3088 691.5 0.004 ** #&gt; period4 -1.9335 -3.2407 -0.8315 554.9 &lt;0.001 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 MCMCglmm fits a residual variance component (useful with dispersion issues) # explains less variability apply(Bayes_cbpp$VCV,2,sd) #&gt; herd units #&gt; 0.1031743 0.5423514 summary(Bayes_cbpp)$solutions #&gt; post.mean l-95% CI u-95% CI eff.samp pMCMC #&gt; (Intercept) -1.524731 -2.173223 -0.9247605 717.5157 0.001 #&gt; period2 -1.281212 -2.348887 -0.3660568 821.6596 0.012 #&gt; period3 -1.415170 -2.344293 -0.3087640 691.5463 0.004 #&gt; period4 -1.933501 -3.240745 -0.8314840 554.9365 0.001 interpret Bayesian “credible intervals” similarly to confidence intervals Make sure you make post-hoc diagnoses library(lattice) xyplot(as.mcmc(Bayes_cbpp$Sol), layout = c(2, 2)) There is no trend, well-mixed xyplot(as.mcmc(Bayes_cbpp$VCV),layout=c(2,1)) For the herd variable, a lot of them are 0, which suggests problem. To fix the instability in the herd effect sampling, we can either modify the prior distribution on the herd variation increases the number of iteration library(MCMCglmm) Bayes_cbpp2 &lt;- MCMCglmm( cbind(incidence, size - incidence) ~ period, random = ~ herd, data = cbpp, family = &quot;multinomial2&quot;, nitt = 20000, burnin = 10000, prior = list(G = list(list( V = 1, nu = .1 ))), verbose = FALSE ) xyplot(as.mcmc(Bayes_cbpp2$VCV), layout = c(2, 1)) To change the shape of priors, in MCMCglmm use: V controls for the location of the distribution (default = 1) nu controls for the concentration around V (default = 0) 9.2.2 Count (Owl Data) library(glmmTMB) library(dplyr) data(Owls, package = &quot;glmmTMB&quot;) Owls &lt;- Owls %&gt;% rename(Ncalls = SiblingNegotiation) In a typical Poisson model, \\(\\lambda\\) (Poisson mean), is model as \\(\\log(\\lambda) = \\mathbf{x&#39;\\beta}\\) But if the response is the rate (e.g., counts per BroodSize), we could model it as \\(\\log(\\lambda / b) = \\mathbf{x&#39;\\beta}\\) , equivalently \\(\\log(\\lambda) = \\log(b) + \\mathbf{x&#39;\\beta}\\) where \\(b\\) is BroodSize. Hence, we “offset” the mean by the log of this variable. owls_glmer &lt;- glmer( Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent + (1 | Nest), family = poisson, data = Owls ) summary(owls_glmer) #&gt; Generalized linear mixed model fit by maximum likelihood (Laplace #&gt; Approximation) [glmerMod] #&gt; Family: poisson ( log ) #&gt; Formula: Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent + #&gt; (1 | Nest) #&gt; Data: Owls #&gt; #&gt; AIC BIC logLik deviance df.resid #&gt; 5212.8 5234.8 -2601.4 5202.8 594 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.5529 -1.7971 -0.6842 1.2689 11.4312 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; Nest (Intercept) 0.2063 0.4542 #&gt; Number of obs: 599, groups: Nest, 27 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.65585 0.09567 6.855 7.12e-12 *** #&gt; FoodTreatmentSatiated -0.65612 0.05606 -11.705 &lt; 2e-16 *** #&gt; SexParentMale -0.03705 0.04501 -0.823 0.4104 #&gt; FoodTreatmentSatiated:SexParentMale 0.13135 0.07036 1.867 0.0619 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) FdTrtS SxPrnM #&gt; FdTrtmntStt -0.225 #&gt; SexParentMl -0.292 0.491 #&gt; FdTrtmS:SPM 0.170 -0.768 -0.605 nest explains a relatively large proportion of the variability (its standard deviation is larger than some coefficients) the model fit isn’t great (deviance of 5202 on 594 df) # Negative binomial model owls_glmerNB &lt;- glmer.nb(Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent + (1 | Nest), data = Owls) c(Deviance = round(summary(owls_glmerNB)$AICtab[&quot;deviance&quot;], 3), df = summary(owls_glmerNB)$AICtab[&quot;df.resid&quot;]) #&gt; Deviance.deviance df.df.resid #&gt; 3483.616 593.000 There is an improvement using negative binomial considering over-dispersion hist(Owls$Ncalls,breaks=30) To account for too many 0s in these data, we can use zero-inflated Poisson (ZIP) model. glmmTMB can handle ZIP GLMMs since it adds automatic differentiation to existing estimation strategies. library(glmmTMB) owls_glmm &lt;- glmmTMB( Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) + (1 | Nest), ziformula = ~ 0, family = nbinom2(link = &quot;log&quot;), data = Owls ) owls_glmm_zi &lt;- glmmTMB( Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) + (1 | Nest), ziformula = ~ 1, family = nbinom2(link = &quot;log&quot;), data = Owls ) # Scale Arrival time to use as a covariate for zero-inflation parameter Owls$ArrivalTime &lt;- scale(Owls$ArrivalTime) owls_glmm_zi_cov &lt;- glmmTMB( Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) + (1 | Nest), ziformula = ~ ArrivalTime, family = nbinom2(link = &quot;log&quot;), data = Owls ) as.matrix(anova(owls_glmm, owls_glmm_zi)) #&gt; Df AIC BIC logLik deviance Chisq Chi Df #&gt; owls_glmm 6 3495.610 3521.981 -1741.805 3483.610 NA NA #&gt; owls_glmm_zi 7 3431.646 3462.413 -1708.823 3417.646 65.96373 1 #&gt; Pr(&gt;Chisq) #&gt; owls_glmm NA #&gt; owls_glmm_zi 4.592983e-16 as.matrix(anova(owls_glmm_zi, owls_glmm_zi_cov)) #&gt; Df AIC BIC logLik deviance Chisq Chi Df #&gt; owls_glmm_zi 7 3431.646 3462.413 -1708.823 3417.646 NA NA #&gt; owls_glmm_zi_cov 8 3422.532 3457.694 -1703.266 3406.532 11.11411 1 #&gt; Pr(&gt;Chisq) #&gt; owls_glmm_zi NA #&gt; owls_glmm_zi_cov 0.0008567362 summary(owls_glmm_zi_cov) #&gt; Family: nbinom2 ( log ) #&gt; Formula: #&gt; Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) + (1 | Nest) #&gt; Zero inflation: ~ArrivalTime #&gt; Data: Owls #&gt; #&gt; AIC BIC logLik deviance df.resid #&gt; 3422.5 3457.7 -1703.3 3406.5 591 #&gt; #&gt; Random effects: #&gt; #&gt; Conditional model: #&gt; Groups Name Variance Std.Dev. #&gt; Nest (Intercept) 0.07487 0.2736 #&gt; Number of obs: 599, groups: Nest, 27 #&gt; #&gt; Dispersion parameter for nbinom2 family (): 2.22 #&gt; #&gt; Conditional model: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.84778 0.09961 8.511 &lt; 2e-16 *** #&gt; FoodTreatmentSatiated -0.39529 0.13742 -2.877 0.00402 ** #&gt; SexParentMale -0.07025 0.10435 -0.673 0.50079 #&gt; FoodTreatmentSatiated:SexParentMale 0.12388 0.16449 0.753 0.45138 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Zero-inflation model: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.3018 0.1261 -10.32 &lt; 2e-16 *** #&gt; ArrivalTime 0.3545 0.1074 3.30 0.000966 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see ZIP GLMM with an arrival time covariate on the zero is best. arrival time has a positive effect on observing a nonzero number of calls interactions are non significant, the food treatment is significant (fewer calls after eating) nest variability is large in magnitude (without this, the parameter estimates change) 9.2.3 Binomial library(agridat) library(ggplot2) library(lme4) library(spaMM) data(gotway.hessianfly) dat &lt;- gotway.hessianfly dat$prop &lt;- dat$y / dat$n ggplot(dat, aes(x = lat, y = long, fill = prop)) + geom_tile() + scale_fill_gradient(low = &#39;white&#39;, high = &#39;black&#39;) + geom_text(aes(label = gen, color = block)) + ggtitle(&#39;Gotway Hessian Fly&#39;) Fixed effects (\\(\\beta\\)) = genotype Random effects (\\(\\alpha\\)) = block flymodel &lt;- glmer( cbind(y, n - y) ~ gen + (1 | block), data = dat, family = binomial, nAGQ = 5 ) summary(flymodel) #&gt; Generalized linear mixed model fit by maximum likelihood (Adaptive #&gt; Gauss-Hermite Quadrature, nAGQ = 5) [glmerMod] #&gt; Family: binomial ( logit ) #&gt; Formula: cbind(y, n - y) ~ gen + (1 | block) #&gt; Data: dat #&gt; #&gt; AIC BIC logLik deviance df.resid #&gt; 162.2 198.9 -64.1 128.2 47 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.38644 -1.01188 0.09631 1.03468 2.75479 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. #&gt; block (Intercept) 0.001022 0.03196 #&gt; Number of obs: 64, groups: block, 4 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 1.5035 0.3914 3.841 0.000122 *** #&gt; genG02 -0.1939 0.5302 -0.366 0.714644 #&gt; genG03 -0.5408 0.5103 -1.060 0.289260 #&gt; genG04 -1.4342 0.4714 -3.043 0.002346 ** #&gt; genG05 -0.2037 0.5429 -0.375 0.707486 #&gt; genG06 -0.9783 0.5046 -1.939 0.052533 . #&gt; genG07 -0.6041 0.5111 -1.182 0.237235 #&gt; genG08 -1.6774 0.4907 -3.418 0.000630 *** #&gt; genG09 -1.3984 0.4725 -2.960 0.003078 ** #&gt; genG10 -0.6817 0.5333 -1.278 0.201181 #&gt; genG11 -1.4630 0.4843 -3.021 0.002522 ** #&gt; genG12 -1.4591 0.4918 -2.967 0.003010 ** #&gt; genG13 -3.5528 0.6600 -5.383 7.31e-08 *** #&gt; genG14 -2.5073 0.5264 -4.763 1.90e-06 *** #&gt; genG15 -2.0872 0.4851 -4.302 1.69e-05 *** #&gt; genG16 -2.9697 0.5383 -5.517 3.46e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Equivalently, we can use MCMCglmm , for a Bayesian approach library(coda) Bayes_flymodel &lt;- MCMCglmm( cbind(y, n - y) ~ gen , random = ~ block, data = dat, family = &quot;multinomial2&quot;, verbose = FALSE ) plot(Bayes_flymodel$Sol[, 1], main = dimnames(Bayes_flymodel$Sol)[[2]][1]) autocorr.plot(Bayes_flymodel$Sol[, 1], main = dimnames(Bayes_flymodel$Sol)[[2]][1]) 9.2.4 Example from (Schabenberger and Pierce 2001) section 8.4.1 dat2 &lt;- read.table(&quot;images/YellowPoplarData_r.txt&quot;) names(dat2) &lt;- c(&#39;tn&#39;, &#39;k&#39;, &#39;dbh&#39;, &#39;totht&#39;, &#39;dob&#39;, &#39;ht&#39;, &#39;maxd&#39;, &#39;cumv&#39;) dat2$t &lt;- dat2$dob / dat2$dbh dat2$r &lt;- 1 - dat2$dob / dat2$totht The cumulative volume relates to the complementary diameter (subplots were created based on total tree height) library(ggplot2) library(dplyr) dat2 &lt;- dat2 %&gt;% group_by(tn) %&gt;% mutate( z = case_when( totht &lt; 74 &amp; totht &gt;= 0 ~ &#39;a: 0-74ft&#39;, totht &lt; 88 &amp; totht &gt;= 74 ~ &#39;b: 74-88&#39;, totht &lt; 95 &amp; totht &gt;= 88 ~ &#39;c: 88-95&#39;, totht &lt; 99 &amp; totht &gt;= 95 ~ &#39;d: 95-99&#39;, totht &lt; 104 &amp; totht &gt;= 99 ~ &#39;e: 99-104&#39;, totht &lt; 109 &amp; totht &gt;= 104 ~ &#39;f: 104-109&#39;, totht &lt; 115 &amp; totht &gt;= 109 ~ &#39;g: 109-115&#39;, totht &lt; 120 &amp; totht &gt;= 115 ~ &#39;h: 115-120&#39;, totht &lt; 140 &amp; totht &gt;= 120 ~ &#39;i: 120-150&#39;, ) ) ggplot(dat2, aes(x = r, y = cumv)) + geom_point(size = 0.5) + facet_wrap(vars(z)) The proposed non-linear model: \\[ V_{id_j} = (\\beta_0 + (\\beta_1 + b_{1i})\\frac{D^2_i H_i}{1000})(\\exp[-(\\beta_2 + b_{2i})t_{ij} \\exp(\\beta_3 t_{ij})]) + e_{ij} \\] where \\(b_{1i}, b_{2i}\\) are random effects \\(e_{ij}\\) are random errors library(nlme) tmp &lt;- nlme( cumv ~ (b0 + (b1 + u1) * (dbh * dbh * totht / 1000)) * (exp(-(b2 + u2) * (t / 1000) * exp(b3 * t))), data = dat2, fixed = b0 + b1 + b2 + b3 ~ 1, # 1 on the right hand side of the formula indicates # a single fixed effects for the corresponding parameters random = list(pdDiag(u1 + u2 ~ 1)), #uncorrelated random effects groups = ~ tn, #group on trees so each tree w/ have u1 and u2 start = list(fixed = c( b0 = 0.25, b1 = 2.3, b2 = 2.87, b3 = 6.7 )) ) summary(tmp) #&gt; Nonlinear mixed-effects model fit by maximum likelihood #&gt; Model: cumv ~ (b0 + (b1 + u1) * (dbh * dbh * totht/1000)) * (exp(-(b2 + u2) * (t/1000) * exp(b3 * t))) #&gt; Data: dat2 #&gt; AIC BIC logLik #&gt; 31103.73 31151.33 -15544.86 #&gt; #&gt; Random effects: #&gt; Formula: list(u1 ~ 1, u2 ~ 1) #&gt; Level: tn #&gt; Structure: Diagonal #&gt; u1 u2 Residual #&gt; StdDev: 0.1508094 0.447829 2.226361 #&gt; #&gt; Fixed effects: b0 + b1 + b2 + b3 ~ 1 #&gt; Value Std.Error DF t-value p-value #&gt; b0 0.249386 0.12894687 6297 1.9340 0.0532 #&gt; b1 2.288832 0.01266804 6297 180.6776 0.0000 #&gt; b2 2.500497 0.05606685 6297 44.5985 0.0000 #&gt; b3 6.848871 0.02140677 6297 319.9395 0.0000 #&gt; Correlation: #&gt; b0 b1 b2 #&gt; b1 -0.639 #&gt; b2 0.054 0.056 #&gt; b3 -0.011 -0.066 -0.850 #&gt; #&gt; Standardized Within-Group Residuals: #&gt; Min Q1 Med Q3 Max #&gt; -6.694575e+00 -3.081861e-01 -8.904304e-05 3.469469e-01 7.855665e+00 #&gt; #&gt; Number of Observations: 6636 #&gt; Number of Groups: 336 nlme::intervals(tmp) #&gt; Approximate 95% confidence intervals #&gt; #&gt; Fixed effects: #&gt; lower est. upper #&gt; b0 -0.003318061 0.2493855 0.5020892 #&gt; b1 2.264006036 2.2888322 2.3136584 #&gt; b2 2.390620340 2.5004973 2.6103743 #&gt; b3 6.806919342 6.8488713 6.8908232 #&gt; #&gt; Random Effects: #&gt; Level: tn #&gt; lower est. upper #&gt; sd(u1) 0.1376084 0.1508094 0.1652768 #&gt; sd(u2) 0.4056209 0.4478290 0.4944291 #&gt; #&gt; Within-group standard error: #&gt; lower est. upper #&gt; 2.187258 2.226361 2.266162 Little different from the book because of different implementation of nonlinear mixed models. library(cowplot) nlmmfn &lt;- function(fixed,rand,dbh,totht,t){ b0 &lt;- fixed[1] b1 &lt;- fixed[2] b2 &lt;- fixed[3] b3 &lt;- fixed[4] u1 &lt;- rand[1] u2 &lt;- rand[2] #just made so we can predict w/o random effects return((b0+(b1+u1)*(dbh*dbh*totht/1000))*(exp(-(b2+u2)*(t/1000)*exp(b3*t)))) } #Tree 1 pred1 &lt;- data.frame(seq(1, 24, length.out = 100)) names(pred1) &lt;- &#39;dob&#39; pred1$tn &lt;- 1 pred1$dbh &lt;- unique(dat2[dat2$tn == 1, ]$dbh) pred1$t &lt;- pred1$dob / pred1$dbh pred1$totht &lt;- unique(dat2[dat2$tn == 1, ]$totht) pred1$r &lt;- 1 - pred1$dob / pred1$totht pred1$test &lt;- predict(tmp, pred1) pred1$testno &lt;- nlmmfn( fixed = tmp$coefficients$fixed, rand = c(0, 0), pred1$dbh, pred1$totht, pred1$t ) p1 &lt;- ggplot(pred1) + geom_line(aes(x = r, y = test, color = &#39;with random&#39;)) + geom_line(aes(x = r, y = testno, color = &#39;No random&#39;)) + labs(colour = &quot;&quot;) + geom_point(data = dat2[dat2$tn == 1, ], aes(x = r, y = cumv)) + ggtitle(&#39;Tree 1&#39;) + theme(legend.position = &quot;none&quot;) #Tree 151 pred151 &lt;- data.frame(seq(1, 21, length.out = 100)) names(pred151) &lt;- &#39;dob&#39; pred151$tn &lt;- 151 pred151$dbh &lt;- unique(dat2[dat2$tn == 151, ]$dbh) pred151$t &lt;- pred151$dob / pred151$dbh pred151$totht &lt;- unique(dat2[dat2$tn == 151, ]$totht) pred151$r &lt;- 1 - pred151$dob / pred151$totht pred151$test &lt;- predict(tmp, pred151) pred151$testno &lt;- nlmmfn( fixed = tmp$coefficients$fixed, rand = c(0, 0), pred151$dbh, pred151$totht, pred151$t ) p2 &lt;- ggplot(pred151) + geom_line(aes(x = r, y = test, color = &#39;with random&#39;)) + geom_line(aes(x = r, y = testno, color = &#39;No random&#39;)) + labs(colour = &quot;&quot;) + geom_point(data = dat2[dat2$tn == 151,], aes(x = r, y = cumv)) + ggtitle(&#39;Tree 151&#39;) + theme(legend.position = &quot;none&quot;) #Tree 279 pred279 &lt;- data.frame(seq(1, 9, length.out = 100)) names(pred279) &lt;- &#39;dob&#39; pred279$tn &lt;- 279 pred279$dbh &lt;- unique(dat2[dat2$tn == 279, ]$dbh) pred279$t &lt;- pred279$dob / pred279$dbh pred279$totht &lt;- unique(dat2[dat2$tn == 279, ]$totht) pred279$r &lt;- 1 - pred279$dob / pred279$totht pred279$test &lt;- predict(tmp, pred279) pred279$testno &lt;- nlmmfn( fixed = tmp$coefficients$fixed, rand = c(0, 0), pred279$dbh, pred279$totht, pred279$t ) p3 &lt;- ggplot(pred279) + geom_line(aes(x = r, y = test, color = &#39;with random&#39;)) + geom_line(aes(x = r, y = testno, color = &#39;No random&#39;)) + labs(colour = &quot;&quot;) + geom_point(data = dat2[dat2$tn == 279, ], aes(x = r, y = cumv)) + ggtitle(&#39;Tree 279&#39;) + theme(legend.position = &quot;none&quot;) plot_grid(p1, p2, p3) red line = predicted observations based on the common fixed effects teal line = tree-specific predictions with random effects References "],["summary.html", "9.3 Summary", " 9.3 Summary "],["model-specification.html", "Chapter 10 Model Specification", " Chapter 10 Model Specification Test whether underlying assumptions hold true Nested Model (A1/A3) Non-Nested Model (A1/A3) Heteroskedasticity (A4) "],["nested-model.html", "10.1 Nested Model", " 10.1 Nested Model \\[ \\begin{aligned} y &amp;= \\beta_0 + x_1\\beta_1 + x_2\\beta-2 + x_3\\beta_3 + \\epsilon &amp; \\text{unrestricted model} \\\\ y &amp;= \\beta_0 + x_1\\beta_1 + \\epsilon &amp; \\text{restricted model} \\end{aligned} \\] Unrestricted model is always longer than the restricted model The restricted model is “nested” within the unrestricted model To determine which variables should be included or exclude, we could use the same Wald Test Adjusted \\(R^2\\) \\(R^2\\) will always increase with more variables included Adjusted \\(R^2\\) tries to correct by penalizing inclusion of unnecessary variables. \\[ \\begin{aligned} {R}^2 &amp;= 1 - \\frac{SSR/n}{SST/n} \\\\ {R}^2_{adj} &amp;= 1 - \\frac{SSR/(n-k)}{SST/(n-1)} \\\\ &amp;= 1 - \\frac{(n-1)(1-R^2)}{(n-k)} \\end{aligned} \\] \\({R}^2_{adj}\\) increases if and only if the t-statistic on the additional variable is greater than 1 in absolute value. \\({R}^2_{adj}\\) is valid in models where there is no heteroskedasticity there fore it should not be used in determining which variables should be included in the model (the t or F-tests are more appropriate) 10.1.1 Chow test Should we run two different regressions for two groups? "],["non-nested-model.html", "10.2 Non-Nested Model", " 10.2 Non-Nested Model compare models with different non-nested specifications 10.2.1 Davidson-Mackinnon test 10.2.1.1 Independent Variable Should the independent variables be logged? (decide between non-nested alternatives) \\[ \\begin{aligned} y = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\epsilon &amp;&amp; \\text{(level eq)} \\\\ y = \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\epsilon &amp;&amp; \\text{(log eq)} \\end{aligned} \\] Obtain predict outcome when estimating the model in log equation \\(\\check{y}\\) and then estimate the following auxiliary equation, \\[ y = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error \\] and evaluate the t-statistic for the null hypothesis \\(H_0: \\gamma = 0\\) Obtain predict outcome when estimating the model in the level equation \\(\\hat{y}\\), then estimate the following auxiliary equation, \\[ y = \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error \\] and evaluate the t-statistic for the null hypothesis \\(H_0: \\gamma = 0\\) If you reject the null in the (1) step but fail to reject the null in the second step, then the log equation is preferred. If fail to reject the null in the (1) step but reject the null in the (2) step then, level equation is preferred. If reject in both steps, then you have statistical evidence that neither model should be used and should re-evaluate the functional form of your model. If fail to reject in both steps, you do not have sufficient evidence to prefer one model over the other. You can compare the \\(R^2_{adj}\\) to choose between the two models. \\[ \\begin{aligned} y &amp;= \\beta_0 + ln(x)\\beta_1 + \\epsilon \\\\ y &amp;= \\beta_0 + x(\\beta_1) + x^2\\beta_2 + \\epsilon \\end{aligned} \\] Compare which better fits the data Compare standard \\(R^2\\) is unfair because the second model is less parsimonious (more parameters to estimate) The \\(R_{adj}^2\\) will penalize the second model for being less parsimonious + Only valid when there is no heteroskedasticity (A4 holds) Should only compare after a Davidson-Mackinnon test 10.2.1.2 Dependent Variable \\[ \\begin{aligned} y &amp;= \\beta_0 + x_1\\beta_1 + \\epsilon &amp; \\text{level eq} \\\\ ln(y) &amp;= \\beta_0 + x_1\\beta_1 + \\epsilon &amp; \\text{log eq} \\\\ \\end{aligned} \\] In the level model, regardless of how big y is, x has a constant effect (i.e., one unit change in \\(x_1\\) results in a \\(\\beta_1\\) unit change in y) In the log model, the larger in y is, the effect of x is stronger (i.e., one unit change in \\(x_1\\) could increase y from 1 to \\(1+\\beta_1\\) or from 100 to 100+100x\\(\\beta_1\\)) Cannot compare \\(R^2\\) or \\(R^2_{adj}\\) because the outcomes are complement different, the scaling is different (SST is different) We need to “un-transform” the \\(ln(y)\\) back to the same scale as y and then compare, Estimate the model in the log equation to obtain the predicted outcome \\(\\hat{ln(y)}\\) “Un-transform” the predicted outcome \\[ \\hat{m} = exp(\\hat{ln(y)}) \\] Estimate the following model (without an intercept) \\[ y = \\alpha\\hat{m} + error \\] and obtain predicted outcome \\(\\hat{y}\\) Then take the square of the correlation between \\(\\hat{y}\\) and y as a scaled version of the \\(R^2\\) from the log model that can now compare with the usual \\(R^2\\) in the level model. "],["heteroskedasticity-1.html", "10.3 Heteroskedasticity", " 10.3 Heteroskedasticity Using roust standard errors are always valid If there is significant evidence of heteroskedasticity implying A4 does not hold Gauss-Markov Theorem no longer holds, OLS is not BLUE. Should consider using a better linear unbiased estimator (Weighted Least Squares or Generalized Least Squares) 10.3.1 Breusch-Pagan test A4 implies \\[ E(\\epsilon_i^2|\\mathbf{x_i})=\\sigma^2 \\] \\[ \\epsilon_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error \\] and determining whether or not \\(\\mathbf{x}_i\\) has any predictive value if \\(\\mathbf{x}_i\\) has predictive value, then the variance changes over the levels of \\(\\mathbf{x}_i\\) which is evidence of heteroskedasticity if \\(\\mathbf{x}_i\\) does not have predictive value, the variance is constant for all levels of \\(\\mathbf{x}_i\\) The Breusch-Pagan test for heteroskedasticity would compute the F-test of total significance for the following model \\[ e_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error \\] A low p-value means we reject the null of homoskedasticity However, Breusch-Pagan test cannot detect heteroskedasticity in non-linear form 10.3.2 White test test heteroskedasticity would allow for a non-linear relationship by computing the F-test of total significance for the following model (assume there are three independent random variables) \\[ \\begin{aligned} e_i^2 &amp;= \\gamma_0 + x_i \\gamma_1 + x_{i2}\\gamma_2 + x_{i3}\\gamma_3 \\\\ &amp;+ x_{i1}^2\\gamma_4 + x_{i2}^2\\gamma_5 + x_{i3}^2\\gamma_6 \\\\ &amp;+ (x_{i1} \\times x_{i2})\\gamma_7 + (x_{i1} \\times x_{i3})\\gamma_8 + (x_{i2} \\times x_{i3})\\gamma_9 + error \\end{aligned} \\] A low p-value means we reject the null of homoskedasticity Equivalently, we can compute LM as \\(LM = nR^2_{e^2}\\) where the \\(R^2_{e^2}\\) come from the regression with the squared residual as the outcome The LM statistic has a [\\(\\chi_k^2\\)][Chi-squared] distribution "],["imputation-missing-data.html", "Chapter 11 Imputation (Missing Data) ", " Chapter 11 Imputation (Missing Data) "],["introduction-to-missing-data.html", "11.1 Introduction to Missing Data", " 11.1 Introduction to Missing Data Missing data is a common problem in statistical analyses and data science, impacting the quality and reliability of insights derived from datasets. One widely used approach to address this issue is imputation, where missing data is replaced with reasonable estimates. 11.1.1 Types of Imputation Imputation can be categorized into: Unit Imputation: Replacing an entire missing observation (i.e., all features for a single data point are missing). Item Imputation: Replacing missing values for specific variables (features) within a dataset. While imputation offers a means to make use of incomplete datasets, it has historically been viewed skeptically. This skepticism arises from: Frequent misapplication of imputation techniques, which can introduce significant bias to estimates. Limited applicability, as imputation works well only under certain assumptions about the missing data mechanism and research objectives. Biases in imputation can arise from various factors, including: Imputation method: The chosen method can influence the results and introduce biases. Missing data mechanism: The nature of the missing data—whether it is Missing Completely at Random (MCAR) or Missing at Random (MAR)—affects the accuracy of imputation. Proportion of missing data: The amount of missing data significantly impacts the reliability of the imputation. Available information in the dataset: Limited information reduces the robustness of the imputed values. 11.1.2 When and Why to Use Imputation The appropriateness of imputation depends on the nature of the missing data and the research goal: Missing Data in the Outcome Variable (\\(y\\)): Imputation in such cases is generally problematic, as it can distort statistical models and lead to misleading conclusions. For example, imputing outcomes in regression or classification problems can alter the underlying relationship between the dependent and independent variables. Missing Data in Predictive Variables (\\(x\\)): Imputation is more commonly applied here, especially for non-random missing data. Properly handled, imputation can enable the use of incomplete datasets while minimizing bias. 11.1.2.1 Objectives of Imputation The utility of imputation methods differs substantially depending on whether the goal of the analysis is inference/explanation or prediction. Each goal has distinct priorities and tolerances for bias, variance, and assumptions about the missing data mechanism: 11.1.2.1.1 Inference/Explanation In causal inference or explanatory analyses, the primary objective is to ensure valid statistical inference, emphasizing unbiased estimation of parameters and accurate representation of uncertainty. The treatment of missing data must align closely with the assumptions about the mechanism behind the missing data—whether it is Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR): Bias Sensitivity: Inference analyses require that imputed data preserve the integrity of the relationships among variables. Poorly executed imputation can introduce bias, even when it addresses missingness superficially. Variance and Confidence Intervals: For inference, the quality of the standard errors, confidence intervals, and test statistics is critical. Naive imputation methods (e.g., mean imputation) often fail to appropriately reflect the uncertainty due to missingness, leading to overconfidence in parameter estimates. Mechanism Considerations: Imputation methods, such as multiple imputation (MI), attempt to generate values consistent with the observed data distribution while accounting for missing data uncertainty. However, MI’s performance depends heavily on the validity of the MAR assumption. If the missingness mechanism is MNAR and not addressed adequately, the imputed data could yield biased parameter estimates, undermining the purpose of inference. 11.1.2.1.2 Prediction In predictive modeling, the primary goal is to maximize model accuracy (e.g., minimizing mean squared error for continuous outcomes or maximizing classification accuracy). Here, the focus shifts to optimizing predictive performance rather than ensuring unbiased parameter estimates: Loss of Information: Missing data reduces the amount of usable information in a dataset. Imputation allows the model to leverage all available data, rather than excluding incomplete cases via listwise deletion, which can significantly reduce sample size and model performance. Impact on Model Fit: In predictive contexts, imputation can reduce standard errors of the predictions and stabilize model coefficients by incorporating plausible estimates for missing values. Flexibility with Mechanism: Predictive models are less sensitive to the missing data mechanism than inferential models, as long as the imputed values help reduce variability and align with patterns in the observed data. Methods like K-Nearest Neighbors (KNN), iterative imputation, or even machine learning models (e.g., random forests for imputation) can be valuable, regardless of strict adherence to MAR or MCAR assumptions. Trade-offs: Overimputation, where too much noise or complexity is introduced in the imputation process, can harm prediction by introducing artifacts that degrade model generalizability. 11.1.2.1.3 Key Takeaways The usefulness of imputation depends on whether the goal of the analysis is inference or prediction: Inference/Explanation: The primary concern is valid statistical inference, where biased estimates are unacceptable. Imputation is often of limited value for this purpose, as it may not address the underlying missing data mechanism appropriately (Rubin 1996). Prediction: Imputation can be more useful in predictive modeling, as it reduces the loss of information from incomplete cases. By leveraging observed data, imputation can lower standard errors and improve model accuracy. 11.1.3 Importance of Missing Data Treatment in Statistical Modeling Proper handling of missing data ensures: Unbiased Estimates: Avoiding distortions in parameter estimates. Accurate Standard Errors: Ensuring valid hypothesis testing and confidence intervals. Adequate Statistical Power: Maximizing the use of available data. Ignoring or mishandling missing data can lead to: Bias: Systematic errors in parameter estimates, especially under MAR or MNAR mechanisms. Loss of Power: Reduced sample size leads to larger standard errors and weaker statistical significance. Misleading Conclusions: Over-simplistic imputation methods (e.g., mean substitution) can distort relationships among variables. 11.1.4 Prevalence of Missing Data Across Domains Missing data affects virtually all fields: Business: Non-responses in customer surveys, incomplete sales records, and transactional errors. Healthcare: Missing data in electronic health records (EHRs) due to incomplete patient histories or inconsistent data entry. Social Sciences: Non-responses or partial responses in large-scale surveys, leading to biased conclusions. 11.1.5 Practical Considerations for Imputation Diagnostic Checks: Always examine the patterns and mechanisms of missing data before applying imputation (Diagnosing the Missing Data Mechanism). Model Selection: Align the imputation method with the missing data mechanism and research goal. Validation: Assess the impact of imputation on results through sensitivity analyses or cross-validation. References "],["theoretical-foundations-of-missing-data.html", "11.2 Theoretical Foundations of Missing Data", " 11.2 Theoretical Foundations of Missing Data 11.2.1 Definition and Classification of Missing Data Missing data refers to the absence of values for some variables in a dataset. The mechanisms underlying missingness significantly impact the validity of statistical analyses and the choice of handling methods. These mechanisms are classified into three categories: Missing Not at Random (MNAR): Missingness depends on unobserved variables or the missing values themselves. 11.2.1.1 Missing Completely at Random (MCAR) MCAR occurs when the probability of missingness is entirely random and unrelated to either observed or unobserved variables. Under this mechanism, missing data do not introduce bias in parameter estimates when ignored, although statistical efficiency is reduced due to the smaller sample size. Mathematical Definition: The missingness is independent of all data, both observed and unobserved: \\[ P(Y_{\\text{missing}} | Y, X) = P(Y_{\\text{missing}}) \\] Characteristics of MCAR: Missingness is completely unrelated to both observed and unobserved data. Analyses remain unbiased even if missing data are ignored, though they may lack efficiency due to reduced sample size. The missing data points represent a random subset of the overall data. Examples: A sensor randomly fails at specific time points, unrelated to environmental or operational conditions. Survey participants randomly omit responses to certain questions without any systematic pattern. Methods for Testing MCAR: Little’s MCAR Test: A formal statistical test to assess whether data are MCAR. A significant result suggests deviation from MCAR. Mean Comparison Tests: T-tests or similar approaches compare observed and missing data groups on other variables. Significant differences indicate potential bias. Failure to reject the null hypothesis of no difference does not confirm MCAR but suggests consistency with the MCAR assumption. Handling MCAR: Since MCAR data introduce no bias, they can be handled using the following techniques: Complete Case Analysis (Listwise Deletion): Analyses are performed only on cases with complete data. While unbiased under MCAR, this method reduces sample size and efficiency. Universal Singular Value Thresholding (USVT): This technique is effective for MCAR data recovery but can only recover the mean structure, not the entire true distribution (Chatterjee 2015). SoftImpute: A matrix completion method useful for some missing data problems but less effective when missingness is not MCAR (Hastie et al. 2015). Synthetic Nearest Neighbor Imputation: A robust method for imputing missing data. While primarily designed for MCAR, it can also handle certain cases of missing not at random (MNAR) (Agarwal et al. 2023). Available on GitHub: syntheticNN. Notes: The “missingness” on one variable can be correlated with the “missingness” on another variable without violating the MCAR assumption. Absence of evidence for bias (e.g., failing to reject a t-test) does not confirm that the data are MCAR. 11.2.1.2 Missing at Random (MAR) Missing at Random (MAR) occurs when missingness depends on observed variables but not the missing values themselves. This mechanism assumes that observed data provide sufficient information to explain the missingness. In other words, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. Mathematical Definition: The probability of missingness is conditional only on observed data: \\[ P(Y_{\\text{missing}} | Y, X) = P(Y_{\\text{missing}} | X) \\] This implies that whether an observation is missing is unrelated to the missing values themselves but is related to the observed values of other variables. Characteristics of MAR: Missingness is systematically related to observed variables. The propensity for a data point to be missing is not related to the missing data but is related to some of the observed data. Analyses must account for observed data to mitigate bias. Examples: Women are less likely to disclose their weight, but their gender is recorded. In this case, weight is MAR. Missing income data is correlated with education, which is observed. For example, individuals with higher education levels might be less likely to reveal their income. Challenges in MAR: MAR is weaker than Missing Completely at Random (MCAR). It is impossible to directly test for MAR. Evidence for MAR relies on domain expertise and indirect statistical checks rather than direct tests. Handling MAR: Common methods for handling MAR include: Multiple Imputation by Chained Equations (MICE): Iteratively imputes missing values based on observed data. Maximum Likelihood Estimation: Estimates model parameters directly while accounting for MAR assumptions. Regression-Based Imputation: Predicts missing values using observed covariates. These methods assume that observed variables fully explain the missingness. Effective handling of MAR requires careful modeling and often domain-specific knowledge to validate the assumptions underlying the analysis. 11.2.1.3 Missing Not at Random (MNAR) Missing Not at Random (MNAR) is the most complex missing data mechanism. Here, missingness depends on unobserved variables or the values of the missing data themselves. This makes MNAR particularly challenging, as ignoring this dependency introduces significant bias in analyses. Mathematical Definition: The probability of missingness depends on the missing values: \\[ P(Y_{\\text{missing}} | Y, X) \\neq P(Y_{\\text{missing}} | X) \\] Characteristics of MNAR: Missingness cannot be fully explained by observed data. The cause of missingness is directly related to the unobserved values. Ignoring MNAR introduces significant bias in parameter estimates, often leading to invalid conclusions. Examples: High-income individuals are less likely to disclose their income, and income itself is unobserved. Patients with severe symptoms drop out of a clinical study, leaving their health outcomes unrecorded. Challenges in MNAR: MNAR is the most difficult missingness mechanism to address because the missing data mechanism must be explicitly modeled. Identifying MNAR often requires domain knowledge and auxiliary information beyond the observed dataset. Handling MNAR: MNAR requires explicit modeling of the missingness mechanism. Common approaches include: Heckman Selection Models: These models explicitly account for the selection process leading to missing data, adjusting for potential bias (James J. Heckman 1976). Instrumental Variables: Variables predictive of missingness but unrelated to the outcome can be used to mitigate bias (B. Sun et al. 2018; E. J. Tchetgen Tchetgen and Wirth 2017). Pattern-Mixture Models: These models separate the data into groups (patterns) based on missingness and model each group separately. They are particularly useful when the relationship between missingness and missing values is complex. Sensitivity Analysis: Examines how conclusions change under different assumptions about the missing data mechanism. Use of Auxiliary Data Auxiliary data refers to external data sources or variables that can help explain the missingness mechanism. Surrogate Variables: Adding variables that correlate with missing data can improve imputation accuracy and mitigate the MNAR challenge. Linking External Datasets: Merging datasets from different sources can provide additional context or predictors for missingness. Applications in Business: In marketing, customer demographics or transaction histories often serve as auxiliary data to predict missing responses in surveys. Additionally, data collection strategies, such as follow-up surveys or targeted sampling, can help mitigate MNAR effects by collecting information that directly addresses the missingness mechanism. However, such approaches can be resource-intensive and require careful planning. 11.2.2 Missing Data Mechanisms Mechanism Missingness Depends On Implications Examples MCAR Neither observed nor missing data No bias; simplest to handle; decreases efficiency due to data loss. Random sensor failure. MAR Observed data only Requires observed data to explain missingness; common assumption in imputation methods. Gender-based missingness of weight. MNAR Missing data itself or unobserved variables Requires explicit modeling of the missingness mechanism; significant bias if ignored. High-income individuals not disclosing income. 11.2.3 Relationship Between Mechanisms and Ignorability The concept of ignorability is central to determining whether the missingness process must be explicitly modeled. Ignorability impacts the choice of methods for handling missing data and whether the missing data mechanism can be safely disregarded or must be explicitly accounted for. 11.2.3.1 Ignorable Missing Data Missing data is ignorable under the following conditions: The missing data mechanism is MAR or MCAR. The parameters governing the missing data process are unrelated to the parameters of interest in the analysis. In cases of ignorable missing data, there is no need to model the missingness mechanism explicitly unless you aim to improve the efficiency or precision of parameter estimates. Common imputation techniques, such as multiple imputation or maximum likelihood estimation, rely on the assumption of ignorability to produce unbiased parameter estimates. Practical Considerations for Ignorable Missingness Even though ignorable mechanisms simplify analysis, researchers must rigorously assess whether the missingness mechanism meets the MAR or MCAR criteria. Violations can lead to biased results, even if unintentionally overlooked. For example: A survey on income may assume MAR if missingness is associated with respondent age (observed variable) but not income itself (unobserved variable). However, if income directly influences nonresponse, the assumption of MAR is violated. 11.2.3.2 Non-Ignorable Missing Data Missing data is non-ignorable when: The missingness mechanism depends on the values of the missing data themselves or on unobserved variables. The missing data mechanism is related to the parameters of interest, resulting in bias if the mechanism is not modeled explicitly. This type of missingness (i.e., Missing Not at Random (MNAR) requires modeling the missing data mechanism directly to produce unbiased estimates. Characteristics of Non-Ignorable Missingness Dependence on Missing Values: The likelihood of missingness is associated with the missing values themselves. Example: In a study on health, individuals with more severe conditions are more likely to drop out, leading to an underrepresentation of the sickest individuals in the data. Bias in Complete Case Analysis: Analyses based solely on complete cases can lead to substantial bias. Example: In income surveys, if wealthier individuals are less likely to report their income, the estimated mean income will be systematically lower than the true population mean. Need for Explicit Modeling: To address MNAR, the analyst must model the missing data mechanism. This often involves specifying relationships between observed data, missing data, and the missingness process itself. 11.2.3.3 Implications of Non-Ignorable Missingness Non-ignorable mechanisms are often associated with sensitive or personal data: Examples: Individuals with lower education levels may omit their education information. Participants with controversial or stigmatized health conditions might opt out of surveys entirely. Impact on Policy and Decision-Making: Biases introduced by MNAR can have serious consequences for policymaking, such as underestimating the prevalence of poverty or mischaracterizing population health needs. By explicitly addressing non-ignorable missingness, researchers can mitigate biases and ensure that findings accurately reflect the underlying population. References "],["diagnosing-the-missing-data-mechanism.html", "11.3 Diagnosing the Missing Data Mechanism", " 11.3 Diagnosing the Missing Data Mechanism Understanding the mechanism behind missing data is critical to choosing the appropriate methods for handling it. The three main mechanisms for missing data are MCAR (Missing Completely at Random), MAR (Missing at Random), and MNAR (Missing Not at Random). This section discusses methods for diagnosing these mechanisms, including descriptive and inferential approaches. 11.3.1 Descriptive Methods 11.3.1.1 Visualizing Missing Data Patterns Visualization tools are essential for detecting patterns in missing data. Heatmaps and correlation plots can help identify systematic missingness and provide insights into the underlying mechanism. # Example: Visualizing missing data library(Amelia) missmap( airquality, main = &quot;Missing Data Heatmap&quot;, col = c(&quot;yellow&quot;, &quot;black&quot;), legend = TRUE ) Heatmaps: Highlight where missingness occurs in a dataset. Correlation Plots: Show relationships between missingness indicators of different variables. Exploring Univariate and Multivariate Missingness Univariate Analysis: Calculate the proportion of missing data for each variable. # Example: Proportion of missing values missing_proportions &lt;- colSums(is.na(airquality)) / nrow(airquality) print(missing_proportions) #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 0.24183007 0.04575163 0.00000000 0.00000000 0.00000000 0.00000000 Multivariate Analysis: Examine whether missingness in one variable is related to others. This can be visualized using scatterplots of observed vs. missing values. # Example: Missingness correlation library(naniar) vis_miss(airquality) gg_miss_upset(airquality) # Displays a missingness upset plot 11.3.2 Statistical Tests for Missing Data Mechanisms 11.3.2.1 Diagnosing MCAR: Little’s Test Little’s test is a hypothesis test to determine if the missing data mechanism is MCAR. It tests whether the means of observed and missing data are significantly different. The null hypothesis is that the data are MCAR. \\[ \\chi^2 = \\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i} \\] Where: \\(O_i\\)= Observed frequency \\(E_i\\)= Expected frequency under MCAR # Example: Little&#39;s test naniar::mcar_test(airquality) #&gt; # A tibble: 1 × 4 #&gt; statistic df p.value missing.patterns #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 35.1 14 0.00142 4 misty::na.test(airquality) #&gt; Little&#39;s MCAR Test #&gt; #&gt; n nIncomp nPattern chi2 df pval #&gt; 153 42 4 35.15 14 0.001 11.3.2.2 Diagnosing MCAR via Dummy Variables Creating a binary indicator for missingness allows you to test whether the presence of missing data is related to observed data. For instance: Create a dummy variable: 1 = Missing 0 = Observed Conduct a chi-square test or t-test: Chi-square: Compare proportions of missingness across groups. T-test: Compare means of (other) observed variables with missingness indicators. # Example: Chi-square test airquality$missing_var &lt;- as.factor(ifelse(is.na(airquality$Ozone), 1, 0)) # Across groups of months table(airquality$missing_var, airquality$Month) #&gt; #&gt; 5 6 7 8 9 #&gt; 0 26 9 26 26 29 #&gt; 1 5 21 5 5 1 chisq.test(table(airquality$missing_var, airquality$Month)) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: table(airquality$missing_var, airquality$Month) #&gt; X-squared = 44.751, df = 4, p-value = 4.48e-09 # Example: T-test (of other variable) t.test(Wind ~ missing_var, data = airquality) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: Wind by missing_var #&gt; t = -0.60911, df = 63.646, p-value = 0.5446 #&gt; alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -1.6893132 0.8999377 #&gt; sample estimates: #&gt; mean in group 0 mean in group 1 #&gt; 9.862069 10.256757 11.3.3 Assessing MAR and MNAR 11.3.3.1 Sensitivity Analysis Sensitivity analysis involves simulating different scenarios of missing data and assessing how the results change. For example, imputing missing values under different assumptions can provide insight into whether the data are MAR or MNAR. 11.3.3.2 Proxy Variables and External Data Using proxy variables or external data sources can help assess whether missingness depends on unobserved variables (MNAR). For example, in surveys, follow-ups with non-respondents can reveal systematic differences. 11.3.3.3 Practical Challenges in Distinguishing MAR from MNAR Distinguishing between Missing at Random (MAR) and Missing Not at Random (MNAR) is a critical and challenging task in data analysis. Properly identifying the nature of the missing data has significant implications for the choice of imputation strategies, model robustness, and the validity of conclusions. While statistical tests can sometimes aid in this determination, the process often relies heavily on domain knowledge, intuition, and exploratory analysis. Below, we discuss key considerations and examples that highlight these challenges: Sensitive Topics: Missing data related to sensitive or stigmatized topics, such as income, drug use, or health conditions, are often MNAR. For example, individuals with higher incomes might deliberately choose not to report their earnings due to privacy concerns. Similarly, participants in a health survey may avoid answering questions about smoking if they perceive social disapproval. In such cases, the probability of missingness is directly related to the unobserved value itself, making MNAR likely. Field-Specific Norms: Understanding norms and typical data collection practices in a specific field can provide insights into missingness patterns. For instance, in marketing surveys, respondents may skip questions about spending habits if they consider the questions intrusive. Prior research or historical data from the same domain can help infer whether missingness is more likely MAR (e.g., random skipping due to survey fatigue) or MNAR (e.g., deliberate omission by higher spenders). Analyzing Auxiliary Variables: Leveraging auxiliary variables—those correlated with the missing variable—can help infer the missingness mechanism. For example, if missing income data strongly correlates with employment status, this suggests a MAR mechanism, as the missingness depends on observed variables. However, if missingness persists even after accounting for observable predictors, MNAR might be at play. Experimental Design and Follow-Up: In longitudinal studies, dropout rates can signal MAR or MNAR patterns. For example, if dropouts occur disproportionately among participants reporting lower satisfaction in early surveys, this indicates an MNAR mechanism. Designing follow-up surveys to specifically investigate dropout reasons can clarify missingness patterns. Sensitivity Analysis: To account for uncertainty in the missingness mechanism, researchers can conduct sensitivity analyses by comparing results under different assumptions (e.g., imputing data using both MAR and MNAR approaches). This process helps to quantify the potential impact of misclassifying the missingness mechanism on study conclusions. Real-World Examples: In customer feedback surveys, higher ratings might be overrepresented due to non-response bias. Customers with negative experiences might be less likely to complete surveys, leading to an MNAR scenario. In financial reporting, missing audit data might correlate with companies in financial distress, a classic MNAR case where the missingness depends on unobserved financial health metrics. Summary MCAR: No pattern in missingness; use Little’s test or dummy variable analysis. MAR: Missingness related to observed data; requires modeling assumptions or proxy analysis. MNAR: Missingness depends on unobserved data; requires external validation or sensitivity analysis. "],["methods-for-handling-missing-data.html", "11.4 Methods for Handling Missing Data", " 11.4 Methods for Handling Missing Data 11.4.1 Basic Methods 11.4.1.1 Complete Case Analysis (Listwise Deletion) Listwise deletion retains only cases with complete data for all features, discarding rows with any missing values. Advantages: Universally applicable to various statistical tests (e.g., SEM, multilevel regression). When data are Missing Completely at Random (MCAR), parameter estimates and standard errors are unbiased. Under specific Missing at Random (MAR) conditions, such as when the probability of missing data depends only on independent variables, listwise deletion can still yield unbiased estimates. For instance, in the model \\(y = \\beta_{0} + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\\), if missingness in \\(X_1\\) is independent of \\(y\\) but depends on \\(X_1\\) and \\(X_2\\), the estimates remain unbiased (Little 1992). This aligns with principles of stratified sampling, which does not bias estimates. In logistic regression, if missing data depend only on the dependent variable but not on independent variables, listwise deletion produces consistent slope estimates, though the intercept may be biased (Vach and Vach 1994). For regression analysis, listwise deletion is more robust than Maximum Likelihood (ML) or Multiple Imputation (MI) when the MAR assumption is violated. Disadvantages: Results in larger standard errors compared to advanced methods. If data are MAR but not MCAR, biased estimates can occur. In non-regression contexts, more sophisticated methods often outperform listwise deletion. 11.4.1.2 Available Case Analysis (Pairwise Deletion) Pairwise deletion calculates estimates using all available data for each pair of variables, without requiring complete cases. It is particularly suitable for methods like linear regression, factor analysis, and SEM, which rely on correlation or covariance matrices. Advantages: Under MCAR, pairwise deletion produces consistent and unbiased estimates in large samples. Compared to listwise deletion (Glasser 1964): When variable correlations are low, pairwise deletion provides more efficient estimates. When correlations are high, listwise deletion becomes more efficient. Disadvantages: Yields biased estimates under MAR conditions. In small samples, covariance matrices might not be positive definite, rendering coefficient estimation infeasible. Software implementation varies in how sample size is handled, potentially affecting standard errors. Note: Carefully review software documentation to understand how sample size is treated, as this influences standard error calculations. 11.4.1.3 Indicator Method (Dummy Variable Adjustment) Also known as the Missing Indicator Method, this approach introduces an additional variable to indicate missingness in the dataset. Implementation: Create an indicator variable: \\[ D = \\begin{cases} 1 &amp; \\text{if data on } X \\text{ are missing} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Modify the original variable to accommodate missingness: \\[ X^* = \\begin{cases} X &amp; \\text{if data are available} \\\\ c &amp; \\text{if data are missing} \\end{cases} \\] Note: A common choice for \\(c\\) is the mean of \\(X\\). Interpretation: The coefficient of \\(D\\) represents the difference in the expected value of \\(Y\\) between cases with missing data and those without. The coefficient of \\(X^*\\) reflects the effect of \\(X\\) on \\(Y\\) for cases with observed data. Disadvantages: Produces biased estimates of coefficients, even under MCAR conditions (Jones 1996). May lead to overinterpretation of the “missingness effect,” complicating model interpretation. 11.4.1.4 Advantages and Limitations of Basic Methods Method Advantages Disadvantages Listwise Deletion Simple and universally applicable; unbiased under MCAR; robust in certain MAR scenarios. Inefficient (larger standard errors); biased under MAR in many cases; discards potentially useful data. Pairwise Deletion Utilizes all available data; efficient under MCAR with low correlations; avoids discarding all cases. Biased under MAR; prone to non-positive-definite covariance matrices in small samples. Indicator Method Simple implementation; explicitly models missingness effect. Biased even under MCAR; complicates interpretation; may not reflect true underlying relationships. 11.4.2 Single Imputation Techniques Single imputation methods replace missing data with a single value, generating a complete dataset that can be analyzed using standard techniques. While convenient, single imputation generally underestimates variability and risks biasing results. 11.4.2.1 Deterministic Methods 11.4.2.1.1 Mean, Median, Mode Imputation This method replaces missing values with the mean, median, or mode of the observed data. Advantages: Simplicity and ease of implementation. Useful for quick exploratory data analysis. Disadvantages: Bias in Variances and Relationships: Mean imputation reduces variance and disrupts relationships among variables, leading to biased estimates of variances and covariances (Haitovsky 1968). Underestimated Standard Errors: Results in overly optimistic conclusions and increased risk of Type I errors. Dependency Structure Ignored: Particularly problematic in high-dimensional data, as it fails to capture dependencies among features. 11.4.2.1.2 Forward and Backward Filling (Time Series Contexts) Used in time series analysis, this method replaces missing values using the preceding (forward filling) or succeeding (backward filling) values. Advantages: Simple and preserves temporal ordering. Suitable for datasets where adjacent values are strongly correlated. Disadvantages: Biased if missingness spans long gaps or occurs systematically. Cannot capture trends or changes in the underlying process. 11.4.2.2 Statistical Prediction Models 11.4.2.2.1 Linear Regression Imputation Missing values in a variable are imputed based on a linear regression model using observed values of other variables. Advantages: Preserves relationships between variables. More sophisticated than mean or median imputation. Disadvantages: Assumes linear relationships, which may not hold in all datasets. Fails to capture variability, leading to downwardly biased standard errors. 11.4.2.2.2 Logistic Regression for Categorical Variables Similar to linear regression imputation but used for categorical variables. The missing category is predicted using a logistic regression model. Advantages: Useful for binary or multinomial categorical data. Preserves relationships with other variables. Disadvantages: Assumes the underlying logistic model is appropriate. Does not account for uncertainty in the imputed values. 11.4.2.3 Non-Parametric Methods 11.4.2.3.1 Hot Deck Imputation Hot Deck Imputation is a method of handling missing data where missing values are replaced with observed values from “donor” cases that are similar in other characteristics. This technique has been widely used in survey data, including by organizations like the U.S. Census Bureau, due to its flexibility and ability to maintain observed data distributions. Advantages of Hot Deck Imputation Retains observed data distributions: Since missing values are imputed using actual observed data, the overall distribution remains realistic. Flexible: This method is applicable to both categorical and continuous variables. Constrained imputations: Imputed values are always feasible, as they come from observed cases. Adds variability: By randomly selecting donors, this method introduces variability, which can aid in robust standard error estimation. Disadvantages of Hot Deck Imputation Sensitivity to similarity definitions: The quality of imputed values depends on the criteria used to define similarity between cases. Computational intensity: Identifying similar cases and randomly selecting donors can be computationally expensive, especially for large datasets. Subjectivity: Deciding how to define “similar” can introduce subjectivity or bias. Algorithm for Hot Deck Imputation Let \\(n_1\\) represent the number of cases with complete data on the variable \\(Y\\), and \\(n_0\\) represent the number of cases with missing data on \\(Y\\). The steps are as follows: From the \\(n_1\\) cases with complete data, take a random sample (with replacement) of \\(n_1\\) cases. From this sampled pool, take another random sample (with replacement) of size \\(n_0\\). Assign the values from the sampled \\(n_0\\) cases to the cases with missing data in \\(Y\\). Repeat this process for every variable in the dataset. For multiple imputation, repeat the above four steps multiple times to create multiple imputed datasets. Variations and Considerations Skipping Step 1: If Step 1 is skipped, the variability of imputed values is reduced. This approach might not fully account for the uncertainty in missing data, which can underestimate standard errors. Defining similarity: A major challenge in this method is deciding what constitutes “similarity” between cases. Common approaches include matching based on distance metrics (e.g., Euclidean distance) or grouping cases by strata or clusters. Practical Example The U.S. Census Bureau employs an approximate Bayesian bootstrap variation of Hot Deck Imputation. In this approach: Similar cases are identified based on shared characteristics or grouping variables. A randomly chosen value from a similar individual in the sample is used to replace the missing value. This method ensures imputed values are plausible while incorporating variability. Key Notes Good aspects: Imputed values are constrained to observed possibilities. Random selection introduces variability, helpful for multiple imputation scenarios. Challenges: Defining and operationalizing “similarity” remains a critical step in applying this method effectively. Below is an example code snippet illustrating Hot Deck Imputation in R: library(Hmisc) # Example dataset with missing values data &lt;- data.frame( ID = 1:10, Age = c(25, 30, NA, 40, NA, 50, 60, NA, 70, 80), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;) ) # Perform Hot Deck Imputation using Hmisc::impute data$Age_imputed &lt;- impute(data$Age, &quot;random&quot;) # Display the imputed dataset print(data) #&gt; ID Age Gender Age_imputed #&gt; 1 1 25 M 25 #&gt; 2 2 30 F 30 #&gt; 3 3 NA F 25 #&gt; 4 4 40 M 40 #&gt; 5 5 NA M 70 #&gt; 6 6 50 F 50 #&gt; 7 7 60 M 60 #&gt; 8 8 NA F 25 #&gt; 9 9 70 M 70 #&gt; 10 10 80 F 80 This code randomly imputes missing values in the Age column based on observed data using the Hmisc package’s impute function. 11.4.2.3.2 Cold Deck Imputation Cold Deck Imputation is a systematic variant of Hot Deck Imputation where the donor pool is predefined. Instead of selecting donors dynamically from within the same dataset, Cold Deck Imputation relies on an external reference dataset, such as historical data or other high-quality external sources. Advantages of Cold Deck Imputation Utilizes high-quality external data: This method is particularly useful when reliable external reference datasets are available, allowing for accurate and consistent imputations. Consistency: If the same donor pool is used across multiple datasets, imputations remain consistent, which can be advantageous in longitudinal studies or standardized processes. Disadvantages of Cold Deck Imputation Lack of adaptability: External data may not adequately reflect the unique characteristics or variability of the current dataset. Potential for systematic bias: If the donor pool is significantly different from the target dataset, imputations may introduce bias. Reduces variability: Unlike Hot Deck Imputation, Cold Deck Imputation systematically selects values, which removes random variation. This can affect the estimation of standard errors and other inferential statistics. Key Characteristics Systematic Selection: Cold Deck Imputation selects donor values systematically based on predefined rules or matching criteria, rather than using random sampling. External Donor Pool: Donors are typically drawn from a separate dataset or historical records. Algorithm for Cold Deck Imputation Identify an external reference dataset or predefined donor pool. Define the matching criteria to find “similar” cases between the donor pool and the current dataset (e.g., based on covariates or stratification). Systematically assign values from the donor pool to missing values in the current dataset based on the matching criteria. Repeat the process for each variable with missing data. Practical Considerations Cold Deck Imputation works well when external data closely resemble the target dataset. However, when there are significant differences in distributions or relationships between variables, imputations may be biased or unrealistic. This method is less useful for datasets without access to reliable external reference data. Suppose we have a current dataset with missing values and a historical dataset with similar variables. The following example demonstrates how Cold Deck Imputation can be implemented: # Current dataset with missing values current_data &lt;- data.frame( ID = 1:5, Age = c(25, 30, NA, 45, NA), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;) ) # External reference dataset (donor pool) reference_data &lt;- data.frame( Age = c(28, 35, 42, 50), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;) ) # Perform Cold Deck Imputation library(dplyr) # Define a matching function to find closest donor impute_cold_deck &lt;- function(missing_row, reference_data) { # Filter donors with the same gender possible_donors &lt;- reference_data %&gt;% filter(Gender == missing_row$Gender) # Return the mean age of matching donors as an example of systematic imputation return(mean(possible_donors$Age, na.rm = TRUE)) } # Apply Cold Deck Imputation to the missing rows current_data &lt;- current_data %&gt;% rowwise() %&gt;% mutate( Age_imputed = ifelse( is.na(Age), impute_cold_deck(cur_data(), reference_data), Age ) ) # Display the imputed dataset print(current_data) #&gt; # A tibble: 5 × 4 #&gt; # Rowwise: #&gt; ID Age Gender Age_imputed #&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 25 M 25 #&gt; 2 2 30 F 30 #&gt; 3 3 NA F 38.8 #&gt; 4 4 45 M 45 #&gt; 5 5 NA M 38.8 Comparison to Hot Deck Imputation Feature Hot Deck Imputation Cold Deck Imputation Donor Pool Internal (within the dataset) External (predefined dataset) Selection Random Systematic Variability Retained Reduced Bias Potential Lower Higher (if donor pool differs) This method suits situations where external reference datasets are trusted and representative. However, careful consideration is required to ensure alignment between the donor pool and the target dataset to avoid systematic biases. 11.4.2.3.3 Random Draw from Observed Distribution This imputation method replaces missing values by randomly sampling from the observed distribution of the variable with missing data. It is a simple, non-parametric approach that retains the variability of the original data. Advantages Preserves variability: By randomly drawing values from the observed data, this method ensures that the imputed values reflect the inherent variability of the variable. Computational simplicity: The process is straightforward and does not require model fitting or complex calculations. Disadvantages Ignores relationships among variables: Since the imputation is based solely on the observed distribution of the variable, it does not consider relationships or dependencies with other variables. May not align with trends: Imputed values are random and may fail to align with patterns or trends present in the data, such as time series structures or interactions. Steps in Random Draw Imputation Identify the observed (non-missing) values of the variable. For each missing value, randomly sample one value from the observed distribution with or without replacement. Replace the missing value with the randomly sampled value. The following example demonstrates how to use random draw imputation to fill in missing values: # Example dataset with missing values set.seed(123) data &lt;- data.frame( ID = 1:10, Value = c(10, 20, NA, 30, 40, NA, 50, 60, NA, 70) ) # Perform random draw imputation random_draw_impute &lt;- function(data, variable) { observed_values &lt;- data[[variable]][!is.na(data[[variable]])] # Observed values data[[variable]][is.na(data[[variable]])] &lt;- sample(observed_values, sum(is.na(data[[variable]])), replace = TRUE) return(data) } # Apply the imputation imputed_data &lt;- random_draw_impute(data, variable = &quot;Value&quot;) # Display the imputed dataset print(imputed_data) #&gt; ID Value #&gt; 1 1 10 #&gt; 2 2 20 #&gt; 3 3 70 #&gt; 4 4 30 #&gt; 5 5 40 #&gt; 6 6 70 #&gt; 7 7 50 #&gt; 8 8 60 #&gt; 9 9 30 #&gt; 10 10 70 Considerations When to Use: This method is suitable for exploratory analysis or as a quick way to handle missing data in univariate contexts. Limitations: Random draws may result in values that do not fit well in the broader context of the dataset, especially in cases where the variable has strong relationships with others. Feature Random Draw from Observed Distribution Regression-Based Imputation Complexity Simple Moderate to High Preserves Variability Yes Limited in deterministic forms Considers Relationships No Yes Risk of Implausible Values Low (if observed values are plausible) Moderate to High This method is a quick and computationally efficient way to address missing data but is best complemented by more sophisticated methods when relationships between variables are important. 11.4.2.4 Semi-Parametric Methods 11.4.2.4.1 Predictive Mean Matching (PMM) Predictive Mean Matching (PMM) imputes missing values by finding observed values closest in predicted value (based on a regression model) to the missing data. The donor values are then used to fill in the gaps. Advantages: Maintains observed variability in the data. Ensures imputed values are realistic since they are drawn from observed data. Disadvantages: Requires a suitable predictive model. Computationally intensive for large datasets. Steps for PMM: Regress \\(Y\\) on \\(X\\) (matrix of covariates) for the \\(n_1\\) (non-missing cases) to estimate coefficients \\(\\hat{b}\\) and residual variance \\(s^2\\). Draw from the posterior predictive distribution of residual variance: \\[s^2_{[1]} = \\frac{(n_1-k)s^2}{\\chi^2},\\] where \\(\\chi^2\\) is a random draw from \\(\\chi^2_{n_1-k}\\). Randomly sample from the posterior distribution of \\(\\hat{b}\\): \\[b_{[1]} \\sim MVN(\\hat{b}, s^2_{[1]}(X&#39;X)^{-1}).\\] Standardize residuals for \\(n_1\\) cases: \\[e_i = \\frac{y_i - \\hat{b}x_i}{\\sqrt{s^2(1-k/n_1)}}.\\] Randomly draw a sample (with replacement) of \\(n_0\\) residuals from Step 4. Calculate imputed values for \\(n_0\\) missing cases: \\[y_i = b_{[1]}x_i + s_{[1]}e_i.\\] Repeat Steps 2–6 (except Step 4) to create multiple imputations. Notes: PMM can handle heteroskedasticity works for multiple variables, imputing each using all others as predictors. Example: Example from Statistics Globe set.seed(1) # Seed N &lt;- 100 # Sample size y &lt;- round(runif(N,-10, 10)) # Target variable Y x1 &lt;- y + round(runif(N, 0, 50)) # Auxiliary variable 1 x2 &lt;- round(y + 0.25 * x1 + rnorm(N,-3, 15)) # Auxiliary variable 2 x3 &lt;- round(0.1 * x1 + rpois(N, 2)) # Auxiliary variable 3 # (categorical variable) x4 &lt;- as.factor(round(0.02 * y + runif(N))) # Auxiliary variable 4 # Insert 20% missing data in Y y[rbinom(N, 1, 0.2) == 1] &lt;- NA data &lt;- data.frame(y, x1, x2, x3, x4) # Store data in dataset head(data) # First 6 rows of our data #&gt; y x1 x2 x3 x4 #&gt; 1 NA 28 -10 5 0 #&gt; 2 NA 15 -2 2 1 #&gt; 3 1 15 -12 6 1 #&gt; 4 8 58 22 10 1 #&gt; 5 NA 26 -12 7 0 #&gt; 6 NA 19 36 5 1 library(&quot;mice&quot;) # Load mice package ##### Impute data via predictive mean matching (single imputation)##### imp_single &lt;- mice(data, m = 1, method = &quot;pmm&quot;) # Impute missing values #&gt; #&gt; iter imp variable #&gt; 1 1 y #&gt; 2 1 y #&gt; 3 1 y #&gt; 4 1 y #&gt; 5 1 y data_imp_single &lt;- complete(imp_single) # Store imputed data # head(data_imp_single) # Since single imputation underestiamtes stnadard errors, # we use multiple imputaiton ##### Predictive mean matching (multiple imputation) ##### # Impute missing values multiple times imp_multi &lt;- mice(data, m = 5, method = &quot;pmm&quot;) #&gt; #&gt; iter imp variable #&gt; 1 1 y #&gt; 1 2 y #&gt; 1 3 y #&gt; 1 4 y #&gt; 1 5 y #&gt; 2 1 y #&gt; 2 2 y #&gt; 2 3 y #&gt; 2 4 y #&gt; 2 5 y #&gt; 3 1 y #&gt; 3 2 y #&gt; 3 3 y #&gt; 3 4 y #&gt; 3 5 y #&gt; 4 1 y #&gt; 4 2 y #&gt; 4 3 y #&gt; 4 4 y #&gt; 4 5 y #&gt; 5 1 y #&gt; 5 2 y #&gt; 5 3 y #&gt; 5 4 y #&gt; 5 5 y data_imp_multi_all &lt;- # Store multiply imputed data complete(imp_multi, &quot;repeated&quot;, include = TRUE) data_imp_multi &lt;- # Combine imputed Y and X1-X4 (for convenience) data.frame(data_imp_multi_all[, 1:6], data[, 2:5]) head(data_imp_multi) #&gt; y.0 y.1 y.2 y.3 y.4 y.5 x1 x2 x3 x4 #&gt; 1 NA -1 6 -1 -3 3 28 -10 5 0 #&gt; 2 NA -10 10 4 0 2 15 -2 2 1 #&gt; 3 1 1 1 1 1 1 15 -12 6 1 #&gt; 4 8 8 8 8 8 8 58 22 10 1 #&gt; 5 NA 0 -1 -6 2 0 26 -12 7 0 #&gt; 6 NA 4 0 3 3 3 19 36 5 1 Example from UCLA Statistical Consulting library(mice) library(VIM) library(lattice) library(ggplot2) ## set observations to NA anscombe &lt;- within(anscombe, { y1[1:3] &lt;- NA y4[3:5] &lt;- NA }) ## view head(anscombe) #&gt; x1 x2 x3 x4 y1 y2 y3 y4 #&gt; 1 10 10 10 8 NA 9.14 7.46 6.58 #&gt; 2 8 8 8 8 NA 8.14 6.77 5.76 #&gt; 3 13 13 13 8 NA 8.74 12.74 NA #&gt; 4 9 9 9 8 8.81 8.77 7.11 NA #&gt; 5 11 11 11 8 8.33 9.26 7.81 NA #&gt; 6 14 14 14 8 9.96 8.10 8.84 7.04 ## check missing data patterns md.pattern(anscombe) #&gt; x1 x2 x3 x4 y2 y3 y1 y4 #&gt; 6 1 1 1 1 1 1 1 1 0 #&gt; 2 1 1 1 1 1 1 1 0 1 #&gt; 2 1 1 1 1 1 1 0 1 1 #&gt; 1 1 1 1 1 1 1 0 0 2 #&gt; 0 0 0 0 0 0 3 3 6 ## Number of observations per patterns for all pairs of variables p &lt;- md.pairs(anscombe) p #&gt; $rr #&gt; x1 x2 x3 x4 y1 y2 y3 y4 #&gt; x1 11 11 11 11 8 11 11 8 #&gt; x2 11 11 11 11 8 11 11 8 #&gt; x3 11 11 11 11 8 11 11 8 #&gt; x4 11 11 11 11 8 11 11 8 #&gt; y1 8 8 8 8 8 8 8 6 #&gt; y2 11 11 11 11 8 11 11 8 #&gt; y3 11 11 11 11 8 11 11 8 #&gt; y4 8 8 8 8 6 8 8 8 #&gt; #&gt; $rm #&gt; x1 x2 x3 x4 y1 y2 y3 y4 #&gt; x1 0 0 0 0 3 0 0 3 #&gt; x2 0 0 0 0 3 0 0 3 #&gt; x3 0 0 0 0 3 0 0 3 #&gt; x4 0 0 0 0 3 0 0 3 #&gt; y1 0 0 0 0 0 0 0 2 #&gt; y2 0 0 0 0 3 0 0 3 #&gt; y3 0 0 0 0 3 0 0 3 #&gt; y4 0 0 0 0 2 0 0 0 #&gt; #&gt; $mr #&gt; x1 x2 x3 x4 y1 y2 y3 y4 #&gt; x1 0 0 0 0 0 0 0 0 #&gt; x2 0 0 0 0 0 0 0 0 #&gt; x3 0 0 0 0 0 0 0 0 #&gt; x4 0 0 0 0 0 0 0 0 #&gt; y1 3 3 3 3 0 3 3 2 #&gt; y2 0 0 0 0 0 0 0 0 #&gt; y3 0 0 0 0 0 0 0 0 #&gt; y4 3 3 3 3 2 3 3 0 #&gt; #&gt; $mm #&gt; x1 x2 x3 x4 y1 y2 y3 y4 #&gt; x1 0 0 0 0 0 0 0 0 #&gt; x2 0 0 0 0 0 0 0 0 #&gt; x3 0 0 0 0 0 0 0 0 #&gt; x4 0 0 0 0 0 0 0 0 #&gt; y1 0 0 0 0 3 0 0 1 #&gt; y2 0 0 0 0 0 0 0 0 #&gt; y3 0 0 0 0 0 0 0 0 #&gt; y4 0 0 0 0 1 0 0 3 rr = number of observations where both pairs of values are observed rm = the number of observations where both variables are missing values mr = the number of observations where the first variable’s value (e.g. the row variable) is observed and second (or column) variable is missing mm = the number of observations where the second variable’s value (e.g. the col variable) is observed and first (or row) variable is missing ## Margin plot of y1 and y4 marginplot(anscombe[c(5, 8)], col = c(&quot;blue&quot;, &quot;red&quot;, &quot;orange&quot;)) ## 5 imputations for all missing values imp1 &lt;- mice(anscombe, m = 5) #&gt; #&gt; iter imp variable #&gt; 1 1 y1 y4 #&gt; 1 2 y1 y4 #&gt; 1 3 y1 y4 #&gt; 1 4 y1 y4 #&gt; 1 5 y1 y4 #&gt; 2 1 y1 y4 #&gt; 2 2 y1 y4 #&gt; 2 3 y1 y4 #&gt; 2 4 y1 y4 #&gt; 2 5 y1 y4 #&gt; 3 1 y1 y4 #&gt; 3 2 y1 y4 #&gt; 3 3 y1 y4 #&gt; 3 4 y1 y4 #&gt; 3 5 y1 y4 #&gt; 4 1 y1 y4 #&gt; 4 2 y1 y4 #&gt; 4 3 y1 y4 #&gt; 4 4 y1 y4 #&gt; 4 5 y1 y4 #&gt; 5 1 y1 y4 #&gt; 5 2 y1 y4 #&gt; 5 3 y1 y4 #&gt; 5 4 y1 y4 #&gt; 5 5 y1 y4 ## linear regression for each imputed data set - 5 regression are run fitm &lt;- with(imp1, lm(y1 ~ y4 + x1)) summary(fitm) #&gt; # A tibble: 15 × 6 #&gt; term estimate std.error statistic p.value nobs #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 (Intercept) 7.33 2.44 3.01 0.0169 11 #&gt; 2 y4 -0.416 0.223 -1.86 0.0996 11 #&gt; 3 x1 0.371 0.141 2.63 0.0302 11 #&gt; 4 (Intercept) 7.27 2.90 2.51 0.0365 11 #&gt; 5 y4 -0.435 0.273 -1.59 0.150 11 #&gt; 6 x1 0.387 0.160 2.41 0.0422 11 #&gt; 7 (Intercept) 6.54 2.80 2.33 0.0479 11 #&gt; 8 y4 -0.322 0.255 -1.26 0.243 11 #&gt; 9 x1 0.362 0.156 2.32 0.0491 11 #&gt; 10 (Intercept) 5.93 3.08 1.92 0.0907 11 #&gt; 11 y4 -0.286 0.282 -1.02 0.339 11 #&gt; 12 x1 0.418 0.176 2.37 0.0451 11 #&gt; 13 (Intercept) 8.16 2.67 3.05 0.0158 11 #&gt; 14 y4 -0.489 0.251 -1.95 0.0867 11 #&gt; 15 x1 0.326 0.151 2.17 0.0622 11 ## pool coefficients and standard errors across all 5 regression models pool(fitm) #&gt; Class: mipo m = 5 #&gt; term m estimate ubar b t dfcom df #&gt; 1 (Intercept) 5 7.0445966 7.76794670 0.719350800 8.63116766 8 5.805314 #&gt; 2 y4 5 -0.3896685 0.06634920 0.006991497 0.07473900 8 5.706243 #&gt; 3 x1 5 0.3727865 0.02473847 0.001134293 0.02609962 8 6.178032 #&gt; riv lambda fmi #&gt; 1 0.11112601 0.10001207 0.3044313 #&gt; 2 0.12644909 0.11225460 0.3161877 #&gt; 3 0.05502168 0.05215218 0.2586992 ## output parameter estimates summary(pool(fitm)) #&gt; term estimate std.error statistic df p.value #&gt; 1 (Intercept) 7.0445966 2.9378849 2.397846 5.805314 0.05483678 #&gt; 2 y4 -0.3896685 0.2733843 -1.425350 5.706243 0.20638512 #&gt; 3 x1 0.3727865 0.1615538 2.307508 6.178032 0.05923999 11.4.2.4.2 Stochastic Imputation Stochastic Imputation is an enhancement of regression imputation that introduces randomness into the imputation process by adding a random residual to the predicted values from a regression model. This approach aims to retain the variability of the original data while reducing the bias introduced by deterministic regression imputation. Stochastic Imputation can be described as: \\[ \\text{Imputed Value} = \\text{Predicted Value (from regression)} + \\text{Random Residual} \\] This method is commonly used as a foundation for multiple imputation techniques. Advantages of Stochastic Imputation Retains all the benefits of regression imputation: Preserves relationships between variables in the dataset. Utilizes information from observed data to inform imputations. Introduces randomness: Adds variability by including a random residual term, making imputed values more realistic and better representing the uncertainty of missing data. Supports multiple imputation: By generating different random residuals for each iteration, it facilitates the creation of multiple plausible datasets for robust statistical analysis. Disadvantages of Stochastic Imputation Implausible values: Depending on the random residuals, imputed values may fall outside the plausible range (e.g., negative values for variables like age or income). Cannot handle heteroskedasticity: If the data exhibit heteroskedasticity (i.e., non-constant variance of residuals), the randomness added by stochastic imputation may not accurately reflect the underlying variability. Steps in Stochastic Imputation Fit a regression model using cases with complete data for the variable with missing values. Predict missing values using the fitted model. Generate random residuals based on the distribution of residuals from the regression model. Add the random residuals to the predicted values to impute missing values. # Example dataset with missing values set.seed(123) data &lt;- data.frame( X = rnorm(10, mean = 50, sd = 10), Y = c(100, 105, 110, NA, 120, NA, 130, 135, 140, NA) ) # Perform stochastic imputation stochastic_impute &lt;- function(data, predictor, target) { # Subset data with complete cases complete_data &lt;- data[!is.na(data[[target]]), ] # Fit a regression model model &lt;- lm(as.formula(paste(target, &quot;~&quot;, predictor)), data = complete_data) # Predict missing values missing_data &lt;- data[is.na(data[[target]]), ] predictions &lt;- predict(model, newdata = missing_data) # Add random residuals residual_sd &lt;- sd(model$residuals, na.rm = TRUE) stochastic_values &lt;- predictions + rnorm(length(predictions), mean = 0, sd = residual_sd) # Impute missing values data[is.na(data[[target]]), target] &lt;- stochastic_values return(data) } # Apply stochastic imputation imputed_data &lt;- stochastic_impute(data, predictor = &quot;X&quot;, target = &quot;Y&quot;) # Display the imputed dataset print(imputed_data) Notes Multiple Imputation: Most multiple imputation methods are extensions of stochastic regression imputation. By repeating the imputation process with different random seeds, multiple datasets can be generated to account for uncertainty in the imputed values. Dealing with Implausible Values: Additional constraints or transformations (e.g., truncating imputed values to a plausible range) may be necessary to address the issue of implausible values. Comparison to Deterministic Regression Imputation Feature Deterministic Regression Imputation Stochastic Imputation Randomness None Adds random residuals Preserves Variability No Yes Use in Multiple Imputation Limited Well-suited Bias Potential Higher Lower # Income data set.seed(1) # Set seed N &lt;- 1000 # Sample size income &lt;- round(rnorm(N, 0, 500)) # Create some synthetic income data income[income &lt; 0] &lt;- income[income &lt; 0] * (-1) x1 &lt;- income + rnorm(N, 1000, 1500) # Auxiliary variables x2 &lt;- income + rnorm(N,-5000, 2000) # Create 10% missingness in income income[rbinom(N, 1, 0.1) == 1] &lt;- NA data_inc_miss &lt;- data.frame(income, x1, x2) Single stochastic regression imputation imp_inc_sri &lt;- mice(data_inc_miss, method = &quot;norm.nob&quot;, m = 1) #&gt; #&gt; iter imp variable #&gt; 1 1 income #&gt; 2 1 income #&gt; 3 1 income #&gt; 4 1 income #&gt; 5 1 income data_inc_sri &lt;- complete(imp_inc_sri) Single predictive mean matching imp_inc_pmm &lt;- mice(data_inc_miss, method = &quot;pmm&quot;, m = 1) #&gt; #&gt; iter imp variable #&gt; 1 1 income #&gt; 2 1 income #&gt; 3 1 income #&gt; 4 1 income #&gt; 5 1 income data_inc_pmm &lt;- complete(imp_inc_pmm) Stochastic regression imputation contains negative values data_inc_sri$income[data_inc_sri$income &lt; 0] #&gt; [1] -23.85404 -58.37790 -61.86396 -57.47909 -21.29221 -73.26549 #&gt; [7] -61.76194 -42.45942 -351.02991 -317.69090 # No values below 0 data_inc_pmm$income[data_inc_pmm$income &lt; 0] #&gt; numeric(0) Evidence for heteroskadastic data # Heteroscedastic data set.seed(1) # Set seed N &lt;- 1:1000 # Sample size a &lt;- 0 b &lt;- 1 sigma2 &lt;- N^2 eps &lt;- rnorm(N, mean = 0, sd = sqrt(sigma2)) y &lt;- a + b * N + eps # Heteroscedastic variable x &lt;- 30 * N + rnorm(N[length(N)], 1000, 200) # Correlated variable y[rbinom(N[length(N)], 1, 0.3) == 1] &lt;- NA # 30% missing data_het_miss &lt;- data.frame(y, x) Single stochastic regression imputation imp_het_sri &lt;- mice(data_het_miss, method = &quot;norm.nob&quot;, m = 1) #&gt; #&gt; iter imp variable #&gt; 1 1 y #&gt; 2 1 y #&gt; 3 1 y #&gt; 4 1 y #&gt; 5 1 y data_het_sri &lt;- complete(imp_het_sri) Single predictive mean matching imp_het_pmm &lt;- mice(data_het_miss, method = &quot;pmm&quot;, m = 1) #&gt; #&gt; iter imp variable #&gt; 1 1 y #&gt; 2 1 y #&gt; 3 1 y #&gt; 4 1 y #&gt; 5 1 y data_het_pmm &lt;- complete(imp_het_pmm) Comparison between predictive mean matching and stochastic regression imputation par(mfrow = c(1, 2)) # Both plots in one graphic # Plot of observed values plot(x[!is.na(data_het_sri$y)], data_het_sri$y[!is.na(data_het_sri$y)], main = &quot;&quot;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;) # Plot of missing values points(x[is.na(y)], data_het_sri$y[is.na(y)], col = &quot;red&quot;) # Title of plot title(&quot;Stochastic Regression Imputation&quot;, line = 0.5) # Regression line abline(lm(y ~ x, data_het_sri), col = &quot;#1b98e0&quot;, lwd = 2.5) # Legend legend( &quot;topleft&quot;, c(&quot;Observed Values&quot;, &quot;Imputed Values&quot;, &quot;Regression Y ~ X&quot;), pch = c(1, 1, NA), lty = c(NA, NA, 1), col = c(&quot;black&quot;, &quot;red&quot;, &quot;#1b98e0&quot;) ) # Plot of observed values plot(x[!is.na(data_het_pmm$y)], data_het_pmm$y[!is.na(data_het_pmm$y)], main = &quot;&quot;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;) # Plot of missing values points(x[is.na(y)], data_het_pmm$y[is.na(y)], col = &quot;red&quot;) # Title of plot title(&quot;Predictive Mean Matching&quot;, line = 0.5) abline(lm(y ~ x, data_het_pmm), col = &quot;#1b98e0&quot;, lwd = 2.5) # Legend legend( &quot;topleft&quot;, c(&quot;Observed Values&quot;, &quot;Imputed Values&quot;, &quot;Regression Y ~ X&quot;), pch = c(1, 1, NA), lty = c(NA, NA, 1), col = c(&quot;black&quot;, &quot;red&quot;, &quot;#1b98e0&quot;) ) mtext( &quot;Imputation of Heteroscedastic Data&quot;, # Main title of plot side = 3, line = -1.5, outer = TRUE, cex = 2 ) 11.4.2.5 Matrix Completion Matrix completion is a method used to impute missing data in a feature matrix while accounting for dependence between features. This approach leverages principal components to approximate the data matrix, a process referred to as matrix completion (James et al. 2013, Sec 12.3). Problem Setup Consider an \\(n \\times p\\) feature matrix \\(\\mathbf{X}\\), where the element \\(x_{ij}\\) represents the value for the \\(i\\)th observation and \\(j\\)th feature. Some elements of \\(\\mathbf{X}\\) are missing, and we aim to impute these missing values. Similar to the process described in 22.2, the matrix \\(\\mathbf{X}\\) can be approximated using its leading principal components. Specifically, we consider \\(M\\) principal components that minimize the following objective: \\[ \\underset{\\mathbf{A} \\in \\mathbb{R}^{n \\times M}, \\mathbf{B} \\in \\mathbb{R}^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(i,j) \\in \\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\} \\] where \\(\\mathcal{O}\\) is the set of observed indices \\((i,j)\\), which is a subset of the total \\(n \\times p\\) pairs. Here: - \\(\\mathbf{A}\\) is an \\(n \\times M\\) matrix of principal component scores. - \\(\\mathbf{B}\\) is a \\(p \\times M\\) matrix of principal component loadings. Imputation of Missing Values After solving the minimization problem: Missing observations \\(x_{ij}\\) can be imputed using the formula: \\[ \\hat{x}_{ij} = \\sum_{m=1}^M \\hat{a}_{im}\\hat{b}_{jm} \\] where \\(\\hat{a}_{im}\\) and \\(\\hat{b}_{jm}\\) are the estimated elements of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), respectively. The leading \\(M\\) principal component scores and loadings can be approximately recovered, as is done in complete data scenarios. Iterative Algorithm The eigen-decomposition used in standard principal component analysis is not applicable here because of missing values. Instead, an iterative algorithm, as described in (James et al. 2013, Alg 12.1), is employed: Initialize the Complete Matrix: Construct an initial complete matrix \\(\\tilde{\\mathbf{X}}\\) of dimension \\(n \\times p\\) where: \\[ \\tilde{x}_{ij} = \\begin{cases} x_{ij} &amp; \\text{if } (i,j) \\in \\mathcal{O} \\\\ \\bar{x}_j &amp; \\text{if } (i,j) \\notin \\mathcal{O} \\end{cases} \\] Here, \\(\\bar{x}_j\\) is the mean of the observed values for the \\(j\\)th variable in the incomplete data matrix \\(\\mathbf{X}\\). \\(\\mathcal{O}\\) indexes the observed elements of \\(\\mathbf{X}\\). Iterative Steps: Repeat the following steps until convergence: Minimize the Objective: Solve the problem: \\[ \\underset{\\mathbf{A} \\in R^{n \\times M}, \\mathbf{B} \\in R^{p \\times M}}{\\operatorname{min}} \\left\\{ \\sum_{(i,j) \\in \\mathcal{O}} (x_{ij} - \\sum_{m=1}^M a_{im}b_{jm})^2 \\right\\} \\] by computing the principal components of the current \\(\\tilde{\\mathbf{X}}\\). Update Missing Values: For each missing element \\((i,j) \\notin \\mathcal{O}\\), set: \\[ \\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{a}_{im}\\hat{b}_{jm} \\] Recalculate the Objective: Compute the objective: \\[ \\sum_{(i,j) \\in \\mathcal{O}} (x_{ij} - \\sum_{m=1}^M \\hat{a}_{im} \\hat{b}_{jm})^2 \\] Return Imputed Values: Once the algorithm converges, return the estimated missing entries \\(\\tilde{x}_{ij}\\) for \\((i,j) \\notin \\mathcal{O}\\). Key Considerations This approach assumes that the missing data are missing at random (MAR). Convergence criteria for the iterative algorithm often involve achieving a threshold for the change in the objective function or limiting the number of iterations. The choice of \\(M\\), the number of principal components, can be guided by cross-validation or other model selection techniques. 11.4.2.6 Comparison of Single Imputation Techniques Method Advantages Disadvantages Mean, Median, Mode Imputation Simple, quick implementation. Biased variances and covariances; ignores relationships among variables. Forward/Backward Filling Preserves temporal ordering. Biased for systematic gaps or long missing sequences. Linear Regression Imputation Preserves relationships among variables. Fails to capture variability; assumes linearity. Logistic Regression Imputation Handles categorical variables well. Requires appropriate model assumptions; ignores variability. PMM Maintains variability; imputes realistic values. Computationally intensive; requires a good predictive model. Hot Deck Imputation Flexible; maintains data distribution. Sensitive to donor selection; computationally demanding. Cold Deck Imputation Consistent across datasets with predefined donor pools. Risk of bias if donor data are not representative. Random Draw from Observed Simple; retains variability in data. Does not preserve relationships among variables; random imputation may distort trends. Matrix Completion Captures dependencies; imputes structurally consistent values. Computationally intensive; assumes principal components capture data relationships. Single imputation techniques are straightforward and accessible, but they often underestimate uncertainty and fail to fully leverage relationships among variables. These limitations make them less ideal for rigorous analyses compared to multiple imputation or model-based approaches. 11.4.3 Advanced Imputation Techniques 11.4.3.1 Expectation-Maximization (EM) Algorithm 11.4.3.2 Maximum Likelihood Estimation (MLE) 11.4.3.3 Bayesian Methods: - Multiple Imputation by Bayesian Inference - Fully Conditional Specification (FCS) 11.4.3.4 Nonparametric Approaches: - K-Nearest Neighbors (KNN) Imputation - Kernel Density Estimation - Local Regression Smoothing 11.4.4 Machine Learning and Modern Approaches 11.4.4.1 Tree-Based Methods 11.4.4.1.1 Random Forest Imputation (missForest) Random Forest Imputation uses an iterative process where a random forest model predicts missing values for one variable at a time, treating other variables as predictors. This process continues until convergence. Mathematical Framework: For a variable \\(X_j\\) with missing values, treat \\(X_j\\) as the response variable. Fit a random forest model \\(f(X_{-j})\\) using the other variables \\(X_{-j}\\) as predictors. Predict missing values \\(\\hat{X}_j = f(X_{-j})\\). Repeat for all variables with missing data until imputed values stabilize. Advantages: Captures complex interactions and non-linearities. Handles mixed data types seamlessly. Limitations: Computationally intensive for large datasets. Sensitive to the quality of data relationships. 11.4.4.1.2 Gradient Boosting Machines (GBM) Gradient Boosting Machines iteratively build models to minimize loss functions. For imputation, missing values are treated as a target variable to be predicted. Mathematical Framework: The GBM algorithm minimizes the loss function: \\[ L = \\sum_{i=1}^n \\ell(y_i, f(x_i)), \\] where \\(\\ell\\) is the loss function (e.g., mean squared error), \\(y_i\\) are observed values, and \\(f(x_i)\\) are predictions. Missing values are treated as the \\(y_i\\) and predicted iteratively. Advantages: Highly accurate predictions. Captures variable importance. Limitations: Overfitting risks. Requires careful parameter tuning. 11.4.4.2 Neural Network-Based Imputation 11.4.4.2.1 Autoencoders Autoencoders are unsupervised neural networks that compress and reconstruct data. Missing values are estimated during reconstruction. Mathematical Framework: An autoencoder consists of: An encoder function: \\(h = g(Wx + b)\\), which compresses the input \\(x\\). A decoder function: \\(\\hat{x} = g&#39;(W&#39;h + b&#39;)\\), which reconstructs the data. The network minimizes the reconstruction loss: \\[ L = \\sum_{i=1}^n (x_i - \\hat{x}_i)^2. \\] Advantages: Handles high-dimensional and non-linear data. Unsupervised learning. Limitations: Computationally demanding. Requires large datasets for effective training. 11.4.4.2.2 Generative Adversarial Networks (GANs) for Data Imputation GANs consist of a generator and a discriminator. For imputation, the generator fills in missing values, and the discriminator evaluates the quality of the imputations. Mathematical Framework: GAN training involves optimizing: \\[ \\min_G \\max_D \\mathbb{E}[\\log D(x)] + \\mathbb{E}[\\log(1 - D(G(z)))]. \\] \\(D(x)\\): Discriminator’s probability that \\(x\\) is real. \\(G(z)\\): Generator’s output for latent input \\(z\\). Advantages: Realistic imputations that reflect underlying distributions. Handles complex data types. Limitations: Difficult to train and tune. Computationally intensive. 11.4.4.3 Matrix Factorization and Matrix Completion 11.4.4.3.1 Singular Value Decomposition (SVD) SVD decomposes a matrix \\(A\\) into three matrices: \\[ A = U\\Sigma V^T, \\] where \\(U\\) and \\(V\\) are orthogonal matrices, and \\(\\Sigma\\) contains singular values. Missing values are estimated by reconstructing \\(A\\) using a low-rank approximation: \\[ \\hat{A} = U_k \\Sigma_k V_k^T. \\] Advantages: Captures global patterns. Efficient for structured data. Limitations: Assumes linear relationships. Sensitive to sparsity. 11.4.4.3.2 Collaborative Filtering Approaches Collaborative filtering uses similarities between rows (users) or columns (items) to impute missing data. For instance, the value of \\(X_{ij}\\) is predicted as: \\[ \\hat{X}_{ij} = \\frac{\\sum_{k \\in N(i)} w_{ik} X_{kj}}{\\sum_{k \\in N(i)} w_{ik}}, \\] where \\(w_{ik}\\) represents similarity weights and \\(N(i)\\) is the set of neighbors. 11.4.4.4 K-Nearest Neighbor (KNN) Imputation KNN identifies the \\(k\\) nearest observations based on a distance metric and imputes missing values using a weighted average (continuous variables) or mode (categorical variables). Mathematical Framework: For a missing value \\(x\\), its imputed value is: \\[ \\hat{x} = \\frac{\\sum_{i=1}^k w_i x_i}{\\sum_{i=1}^k w_i}, \\] where \\(w_i = \\frac{1}{d(x, x_i)}\\) and \\(d(x, x_i)\\) is a distance metric (e.g., Euclidean or Manhattan). Advantages: Simple and interpretable. Non-parametric. Limitations: Computationally expensive for large datasets. 11.4.4.5 Hybrid Methods Hybrid methods combine statistical and machine learning approaches. For example, mean imputation followed by fine-tuning with machine learning models. These methods aim to leverage the strengths of multiple techniques. 11.4.4.6 Summary Table Method Advantages Limitations Applications Random Forest (missForest) Handles mixed data types, captures interactions Computationally intensive Mixed data types Gradient Boosting Machines High accuracy, feature importance Sensitive to parameters Predictive tasks Autoencoders Handles high-dimensional, non-linear data Computationally expensive Complex datasets GANs Realistic imputations, complex distributions Difficult to train, resource-intensive Healthcare, finance SVD Captures global patterns, efficient Assumes linear relationships Recommendation systems Collaborative Filtering Intuitive for user-item data Struggles with sparse or new data Recommender systems KNN Imputation Simple, interpretable Computationally intensive, sensitive to k General-purpose Hybrid Methods Combines multiple strengths Complexity in design Flexible 11.4.5 Multiple Imputation Multiple Imputation (MI) is a statistical technique for handling missing data by creating several plausible datasets through imputation, analyzing each dataset separately, and then combining the results to account for uncertainty in the imputations. MI operates under the assumption that missing data is either Missing Completely at Random (MCAR) or Missing at Random (MAR). Unlike Single Imputation Techniques, MI reflects the uncertainty inherent in the missing data by introducing variability in the imputed values. It avoids biases introduced by ad hoc methods and produces more reliable statistical inferences. The three fundamental steps in MI are: Imputation: Replace missing values with a set of plausible values to create multiple “completed” datasets. Analysis: Perform the desired statistical analysis on each imputed dataset. Combination: Combine the results using rules to account for within- and between-imputation variability. 11.4.5.1 Why Multiple Imputation is Important Imputed values are estimates and inherently include random error. However, when these estimates are treated as exact values in subsequent analysis, the software may overlook this additional error. This oversight results in underestimated standard errors and overly small p-values, leading to misleading conclusions. Multiple imputation addresses this issue by generating multiple estimates for each missing value. These estimates differ slightly due to their random component, which reintroduces variation. This variation helps the software incorporate the uncertainty of imputed values, resulting in: Unbiased parameter estimates Accurate standard errors Improved p-values Multiple imputation was a significant breakthrough in statistics approximately 20 years ago. It provides solutions for many missing data issues (though not all) and, when applied correctly, leads to reliable parameter estimates. If the proportion of missing data is very small (e.g., 2-3%), the choice of imputation method is less critical. 11.4.5.2 Goals of Multiple Imputation The primary goals of any missing data technique, including multiple imputation, are: Unbiased parameter estimates: Ensuring accurate regression coefficients, group means, odds ratios, etc. Accurate standard errors: This leads to reliable p-values and appropriate statistical inferences. Adequate power: To detect meaningful and significant parameter values. 11.4.5.3 Overview of Rubin’s Framework Rubin’s Framework provides the theoretical foundation for MI. It uses a Bayesian model-based approach for generating imputations and a frequentist approach for evaluating the results. The central goals of Rubin’s framework are to ensure that imputations: Retain the statistical relationships present in the data. Reflect the uncertainty about the true values of the missing data. Under Rubin’s framework, MI offers the following advantages: Generalizability: Unlike Maximum Likelihood Estimation (MLE), MI can be applied to a wide range of models. Statistical Properties: When data is MAR or MCAR, MI estimates are consistent, asymptotically normal, and efficient. Rubin also emphasized the importance of using multiple imputations, as single imputations fail to account for variability in the imputed values, leading to underestimated standard errors and overly optimistic test statistics. 11.4.5.4 Multivariate Imputation via Chained Equations (MICE) Multivariate Imputation via Chained Equations (MICE) is a widely used algorithm for implementing MI, particularly in datasets with mixed variable types. The steps of MICE include: Initialization: Replace missing values with initial guesses, such as the mean or median of the observed data. Iterative Imputation: For each variable with missing values, regress it on all other variables (or a subset of relevant predictors). Use the regression model to predict missing values, adding a random error term drawn from the residual distribution. Convergence: Repeat the imputation process until parameter estimates stabilize. MICE offers flexibility in specifying regression models for each variable, accommodating continuous, categorical, and binary data. 11.4.5.5 Bayesian Ridge Regression for Imputation Bayesian ridge regression is an advanced imputation method that incorporates prior distributions on the regression coefficients, making it particularly useful when: Predictors are highly correlated. Sample sizes are small. Missingness is substantial. This method treats the regression coefficients as random variables and samples from their posterior distribution, introducing variability into the imputation process. Bayesian ridge regression is more computationally intensive than simpler methods like MICE but offers greater robustness. 11.4.5.6 Combining Results from MI (Rubin’s Rules) Once multiple datasets are imputed and analyzed, Rubin’s Rules are used to combine the results. The goal is to properly account for the uncertainty introduced by missing data. For a parameter of interest \\(\\theta\\): Estimate Combination: \\[ \\bar{\\theta} = \\frac{1}{M} \\sum_{m=1}^M \\theta_m \\] where \\(\\theta_m\\) is the estimate from the \\(m\\)th imputed dataset, and \\(M\\) is the number of imputations. Variance Combination: \\[ T = \\bar{W} + \\left(1 + \\frac{1}{M}\\right) B \\] where: \\(\\bar{W}\\) is the average within-imputation variance. \\(B\\) is the between-imputation variance: \\[ B = \\frac{1}{M-1} \\sum_{m=1}^M (\\theta_m - \\bar{\\theta})^2 \\] These formulas adjust the final variance to reflect uncertainty both within and across imputations. 11.4.5.6.1 Challenges Stochastic Variability: MI results vary slightly between runs due to its reliance on random draws. To ensure reproducibility, always set a random seed. Convergence: Iterative algorithms like MICE may struggle to converge, especially with high proportions of missing data. Assumption of MAR: MI assumes that missing data is MAR. If data is Missing Not at Random (MNAR), MI can produce biased results. 11.4.5.6.2 Best Practices Algorithm Selection: Use Multiple Imputation by Chained Equations (MICE) for datasets with mixed data types or when relationships between variables are complex. Apply Bayesian Ridge Regression for small datasets or when predictors are highly correlated. Diagnostic Checks: Evaluate the quality of imputations and assess convergence using trace plots or diagnostic statistics to ensure reliable results. Data Transformations: For skewed or proportion data, consider applying log or logit transformations before imputation and inverse-transforming afterward to preserve the data’s original scale. Handling Non-Linear Relationships: For non-linear relationships or interactions, stratify imputations by the levels of the categorical variable involved to ensure accurate estimates. Number of Imputations: Use at least 20 imputations for small datasets or datasets with high missingness. This ensures robust and reliable results in downstream analyses. Avoid Rounding Imputations for Dummy Variables: Many imputation methods (e.g., Markov Chain Monte Carlo [MCMC]) assume normality, even for dummy variables. While it was historically recommended to round imputed values to 0 or 1 for binary variables, research shows that this introduces bias in parameter estimates. Instead, leave imputed values as fractional, even though this may seem counter-intuitive. Do Not Transform Skewed Variables Before Imputation: Transforming variables to meet normality assumptions before imputation can distort their relationships with other variables, leading to biased imputations and possibly introducing outliers. It is better to directly impute the skewed variable. Use More Imputations: Traditional advice suggests 5–10 imputations are sufficient for unbiased estimates, but inconsistencies may arise in repeated analyses. [@Bodner_2008] suggests using a number of imputations equal to the percentage of missing data. As additional imputations generally do not significantly increase the computational workload, using more imputations is a prudent choice. Create Multiplicative Terms Before Imputation: When your model includes interaction or quadratic terms, generate these terms before imputing missing values. Imputing first and then generating these terms can introduce bias in their regression parameters, as highlighted by [@von_Hippel_2009]. References "],["evaluation-of-imputation-methods.html", "11.5 Evaluation of Imputation Methods", " 11.5 Evaluation of Imputation Methods 11.5.1 Statistical Metrics for Assessing Imputation Quality To evaluate the quality of imputed data, several statistical metrics are commonly used. These metrics compare the imputed values to the observed values (in cases where missingness is simulated or artificially introduced) or assess the overall impact of imputation on the quality of subsequent analyses. Key metrics include: Root Mean Squared Error (RMSE): RMSE is calculated as: \\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\] It measures the average magnitude of errors between the true and imputed values. Lower RMSE indicates better imputation accuracy. Mean Absolute Error (MAE): MAE measures the average absolute difference between observed and imputed values: \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\] MAE provides a straightforward assessment of imputation performance and is less sensitive to outliers than RMSE. Log-Likelihood and Deviance Measures: Log-likelihood can be used to evaluate how well the imputation model fits the data. Deviance measures, based on likelihood comparisons, assess the relative goodness of fit of imputation models. These are particularly useful in evaluating methods like maximum likelihood estimation. In practice, these metrics may be combined with graphical methods such as density plots and residual analysis to understand imputation performance more thoroughly. 11.5.2 Bias-Variance Tradeoff in Imputation Imputation methods must balance bias and variance to achieve reliable results. Simpler methods, such as mean or mode imputation, often lead to biased parameter estimates, particularly if the missingness mechanism is non-random. These methods underestimate variability, shrinking standard errors and potentially leading to overconfidence in statistical inferences. Conversely, advanced methods like Multiple Imputation or Full Information Maximum Likelihood (FIML) typically yield unbiased estimates with appropriately calibrated variances. However, these methods may increase computational complexity and require careful tuning of assumptions and parameters. The tradeoff is summarized as follows: High Bias, Low Variance: Simpler methods (e.g., single imputation, mean imputation). Low Bias, Moderate Variance: Advanced methods (e.g., MI, FIML, Bayesian methods). 11.5.3 Sensitivity Analysis Sensitivity analysis is crucial to assess the robustness of imputation methods under varying assumptions. Two primary areas of focus include: Assessing Robustness to Assumptions: Imputation models often rely on assumptions about the missingness mechanism (See Definition and Classification of Missing Data). Sensitivity analysis involves testing how results vary when these assumptions are slightly relaxed or modified. Impact on Downstream Analysis: The quality of imputation should also be evaluated based on its influence on downstream analyses (Objectives of Imputation). For instance: Does the imputation affect causal inference in regression models? Are the conclusions from hypothesis testing or predictive modeling robust to the imputation technique? 11.5.4 Validation Using Simulated Data and Real-World Case Studies Validation of imputation methods is best performed through a combination of simulated data and real-world examples: Simulated Data: - Create datasets with known missingness patterns and true values. - Apply various imputation methods and assess their performance using RMSE, MAE, and other metrics. Real-World Case Studies: Use datasets from actual studies, such as customer transaction data in marketing or financial data in portfolio analysis. Evaluate the impact of imputation on actionable outcomes (e.g., market segmentation, risk assessment). Combining these approaches ensures that methods generalize well across different contexts and data structures. "],["criteria-for-choosing-an-effective-approach.html", "11.6 Criteria for Choosing an Effective Approach", " 11.6 Criteria for Choosing an Effective Approach Choosing an appropriate imputation method depends on the following criteria: Unbiased Parameter Estimates: The technique should ensure that key estimates, such as means, variances, and regression coefficients, are unbiased, particularly in the presence of MAR or MNAR data. Adequate Power: The method should preserve statistical power, enabling robust hypothesis testing and model estimation. This ensures that important effects are not missed due to inflated type II error. Accurate Standard Errors: Accurate estimation of standard errors is critical for reliable p-values and confidence intervals. Methods like single imputation often underestimate standard errors, leading to overconfident conclusions. Preferred Methods: Multiple Imputation and Full Information Maximum Likelihood Multiple Imputation (MI): MI replaces missing values with multiple plausible values drawn from a predictive distribution. It generates multiple complete datasets, analyzes each dataset, and combines the results. Pros: Handles uncertainty well, provides valid standard errors, and is robust under MAR. Cons: Computationally intensive, sensitive to model mis-specification. Full Information Maximum Likelihood (FIML): FIML uses all available data to estimate parameters directly, avoiding the need to impute missing values explicitly. Pros: Efficient, unbiased under MAR, and computationally elegant. Cons: Requires correctly specified models and may be sensitive to MNAR data. Methods to Avoid Single Imputation (e.g., Mean, Mode): Leads to biased estimates and underestimates variability. Listwise Deletion: Discards rows with missing data, reducing sample size and potentially introducing bias if the data is not MCAR. Practical Considerations Computational efficiency and ease of implementation. Compatibility with downstream analysis methods. Alignment with the data’s missingness mechanism. "],["challenges-and-ethical-considerations.html", "11.7 Challenges and Ethical Considerations", " 11.7 Challenges and Ethical Considerations 11.7.1 Challenges in High-Dimensional Data High-dimensional data, where the number of variables exceeds the number of observations, poses unique challenges for missing data analysis. Curse of Dimensionality: Standard imputation methods, such as mean or regression imputation, struggle with high-dimensional spaces due to sparse data distribution. Regularized Methods: Techniques such as LASSO, Ridge Regression, and Elastic Net can be used to handle high-dimensional missing data. These methods shrink model coefficients, preventing overfitting. Matrix Factorization: Methods like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) are often adapted to impute missing values in high-dimensional datasets by reducing the dimensionality first. 11.7.2 Missing Data in Big Data Contexts The advent of big data introduces additional complexities for missing data handling, including computational scalability and storage constraints. 11.7.2.1 Distributed Imputation Techniques MapReduce Frameworks: Algorithms like k-nearest neighbor (KNN) imputation or multiple imputation can be adapted for distributed environments using MapReduce or similar frameworks. Federated Learning: In scenarios where data is distributed across multiple locations (e.g., in healthcare or banking), federated learning allows imputation without centralizing data, ensuring privacy. 11.7.2.2 Cloud-Based Implementations Cloud-Native Algorithms: Cloud platforms like AWS, Google Cloud, and Azure provide scalable solutions for implementing advanced imputation algorithms on large datasets. AutoML Integration: Automated Machine Learning (AutoML) pipelines often include missing data handling as a preprocessing step, leveraging cloud-based computational power. Real-Time Imputation: In e-commerce, cloud-based solutions enable real-time imputation for recommendation systems or fraud detection, ensuring seamless user experiences. 11.7.3 Ethical Concerns 11.7.3.1 Bias Amplification Introduction of Systematic Bias: Imputation methods can inadvertently reinforce existing biases. For example, imputing salary data based on demographic variables may propagate societal inequalities. Business Implications: In credit scoring, biased imputation of missing financial data can lead to unfair credit decisions, disproportionately affecting marginalized groups. Mitigation Strategies: Techniques such as fairness-aware machine learning and bias auditing can help identify and reduce bias introduced during imputation. 11.7.3.2 Transparency in Reporting Imputation Decisions Reproducibility and Documentation: Transparent reporting of imputation methods and assumptions is essential for reproducibility. Analysts should provide clear documentation of the imputation pipeline. Stakeholder Communication: In business settings, communicating imputation decisions to stakeholders ensures informed decision-making and trust in the results. Ethical Frameworks: Ethical guidelines, such as those provided by the European Union’s GDPR or industry-specific codes, emphasize the importance of transparency in data handling. "],["emerging-trends-in-missing-data-handling.html", "11.8 Emerging Trends in Missing Data Handling", " 11.8 Emerging Trends in Missing Data Handling 11.8.1 Advances in Neural Network Approaches Neural networks have transformed the landscape of missing data imputation, offering flexible, scalable, and powerful solutions that go beyond traditional methods. 11.8.1.1 Variational Autoencoders (VAEs) Overview: Variational Autoencoders (VAEs) are generative models that encode data into a latent space and reconstruct it, filling in missing values during reconstruction. Advantages: Handle complex, non-linear relationships between variables. Scalable to high-dimensional datasets. Generate probabilistic imputations, reflecting uncertainty. Applications: In marketing, VAEs can impute missing customer behavior data while accounting for seasonal and demographic variations. In finance, VAEs assist in imputing missing stock price data by modeling dependencies among assets. 11.8.1.2 GANs for Missing Data Generative Adversarial Networks (GANs): GANs consist of a generator and a discriminator, with the generator imputing missing data and the discriminator evaluating its quality. Advantages: Preserve data distributions and avoid over-smoothing. Suitable for imputation in datasets with complex patterns or multi-modal distributions. Applications: In healthcare, GANs have been used to impute missing patient records while preserving patient privacy and data integrity. In retail, GANs can model missing sales data to predict trends and optimize inventory. 11.8.2 Integration with Reinforcement Learning Reinforcement learning (RL) is increasingly being integrated into missing data strategies, particularly in dynamic or sequential data environments. Markov Decision Processes (MDPs): RL models missing data handling as an MDP, where actions (imputations) are optimized based on rewards (accuracy of predictions or decisions). Active Imputation: RL can be used to actively query for missing data points, prioritizing those with the highest impact on downstream tasks. Example: In customer churn prediction, RL can optimize the imputation of high-value customer records. Applications: Financial forecasting: RL models are used to impute missing transaction data dynamically, optimizing portfolio decisions. Smart cities: RL-based models handle missing sensor data to enhance real-time decision-making in traffic management. 11.8.3 Synthetic Data Generation for Missing Data Synthetic data generation has emerged as a robust solution to address missing data, providing flexibility and privacy. Data Augmentation: Synthetic data is generated to augment datasets with missing values, reducing biases introduced by imputation. Techniques: Simulations: Monte Carlo simulations create plausible data points based on observed distributions. Generative Models: GANs and VAEs generate realistic synthetic data that aligns with existing patterns. Applications: In fraud detection, synthetic datasets balance the impact of missing values on anomaly detection. In insurance, synthetic data supports pricing models by filling in gaps from incomplete policyholder records. 11.8.4 Federated Learning and Privacy-Preserving Imputation Federated learning has gained traction as a method for collaborative analysis while preserving data privacy. Federated Imputation: Distributed imputation algorithms operate on decentralized data, ensuring that sensitive information remains local. Example: Hospitals collaboratively impute missing patient data without sharing individual records. Privacy Mechanisms: Differential privacy adds noise to imputed values, protecting individual-level data. Homomorphic encryption allows computations on encrypted data, ensuring privacy throughout the imputation process. Applications: Healthcare: Federated learning imputes missing diagnostic data across clinics. Banking: Collaborative imputation of financial transaction data supports risk modeling while adhering to regulations. 11.8.5 Imputation in Streaming and Online Data Environments The increasing use of streaming data in business and technology requires real-time imputation methods to ensure uninterrupted analysis. Challenges: Imputation must occur dynamically as data streams in. Low latency and high accuracy are essential to maintain real-time decision-making. Techniques: Online Learning Algorithms: Update imputation models incrementally as new data arrives. Sliding Window Methods: Use recent data to estimate and impute missing values in real time. Applications: IoT devices: Imputation in sensor networks for smart homes or industrial monitoring ensures continuous operation despite data transmission issues. Financial markets: Streaming imputation models predict and fill gaps in real-time stock price feeds to inform trading algorithms. "],["application-of-imputation-in-r.html", "11.9 Application of Imputation in R", " 11.9 Application of Imputation in R This section demonstrates how to visualize missing data and handle it using different imputation techniques. Package Algorithm Cont Var Cate Var Diagnostics Complexity Handling Best Use Case Limitations missForest Random Forest Yes Yes Out-of-bag error (NRMSE, PFC) Handles complex interactions Mixed data types with complex interactions May overfit with small datasets Hmisc Additive Regression, Bootstrap, Predictive Mean Matching Yes Yes \\(R^2\\) for imputed values Basic to intermediate complexity Simple datasets with low complexity Limited to simple imputation methods mi Bayesian Regression Yes Yes Graphical diagnostics,convergence Detects issues like collinearity Datasets with irregularities Computationally intensive for large data MICE Multivariate Imputation via Chained Equations Yes Yes Density plots, pooling of results Handles variable interactions General-purpose imputation for MAR data Requires proper method selection for variable types Amelia Bootstrap-based Expectation Maximization (EMB) Yes Limited (requires normality) Diagnostics supported Works well with large/time-series data Time-series or datasets approximating MVN Assumes MVN, requires transformations for non-MVN 11.9.1 Visualizing Missing Data Visualizing missing data is an essential first step in understanding the patterns and extent of missingness in your dataset. library(visdat) library(naniar) library(ggplot2) # Visualizing missing data vis_miss(airquality) # Missingness patterns using an upset plot gg_miss_upset(airquality) # Scatter plot of missing data with faceting ggplot(airquality, aes(x, y)) + geom_miss_point() + facet_wrap(~ group) # Missing values by variable gg_miss_var(data, facet = group) # Missingness in relation to factors gg_miss_fct(x = variable1, fct = variable2) For more details, read The Missing Book by Nicholas Tierney &amp; Allison Horst. 11.9.2 How Many Imputations? Usually, 5 imputations are sufficient unless there is an extremely high proportion of missing data. High proportions require revisiting data collection processes. Rubin’s Rule for Relative Efficiency According to Rubin, the relative efficiency of an estimate based on \\(m\\) imputations (relative to infinite imputations) is given by: \\[ \\text{Relative Efficiency} = ( 1 + \\frac{\\lambda}{m})^{-1} \\] where \\(\\lambda\\) is the rate of missing data. For example, with 50% missing data ($\\lambda = 0.5$), the standard deviation of an estimate based on 5 imputations is only about 5% wider than that from infinite imputations: \\[ \\sqrt{1 + \\frac{0.5}{5}} = 1.049 \\] 11.9.3 Generating Missing Data for Demonstration library(missForest) # Load the data data &lt;- iris # Generate 10% missing values at random set.seed(1) iris.mis &lt;- prodNA(iris, noNA = 0.1) # Remove categorical variables for numeric imputation iris.mis.cat &lt;- iris.mis iris.mis &lt;- subset(iris.mis, select = -c(Species)) 11.9.4 Imputation with Mean, Median, and Mode Mean, median, or mode imputation is a simple yet commonly used technique. # Imputation for the entire dataset e1071::impute(iris.mis, what = &quot;mean&quot;) # Replace with mean e1071::impute(iris.mis, what = &quot;median&quot;) # Replace with median # Imputation by variable Hmisc::impute(iris.mis$Sepal.Length, mean) # Replace with mean Hmisc::impute(iris.mis$Sepal.Length, median) # Replace with median Hmisc::impute(iris.mis$Sepal.Length, 0) # Replace with a specific value Checking Accuracy Accuracy can be checked by comparing predictions with actual values. # Example data actuals &lt;- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)] predicteds &lt;- rep(mean(iris$Sepal.Width, na.rm = TRUE), length(actuals)) # Using MLmetrics package library(MLmetrics) MAE(predicteds, actuals) #&gt; [1] 0.2870303 MSE(predicteds, actuals) #&gt; [1] 0.1301598 RMSE(predicteds, actuals) #&gt; [1] 0.3607767 11.9.5 K-Nearest Neighbors (KNN) Imputation KNN is a more sophisticated method, leveraging similar observations to fill in missing values. library(DMwR2) knnOutput &lt;- knnImputation(data = iris.mis.cat, meth = &quot;median&quot;) anyNA(knnOutput) # Check for remaining missing values #&gt; [1] FALSE actuals &lt;- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)] predicteds &lt;- knnOutput[is.na(iris.mis$Sepal.Width), &quot;Sepal.Width&quot;] # Using MLmetrics package library(MLmetrics) MAE(predicteds, actuals) #&gt; [1] 0.2318182 MSE(predicteds, actuals) #&gt; [1] 0.1038636 RMSE(predicteds, actuals) #&gt; [1] 0.3222788 KNN typically improves upon mean or median imputation in terms of predictive accuracy. 11.9.6 Imputation with Decision Trees (rpart) Decision trees, such as those implemented in rpart, are effective for both numeric and categorical variables. library(rpart) # Imputation for a categorical variable class_mod &lt;- rpart( Species ~ . - Sepal.Length, data = iris.mis.cat[!is.na(iris.mis.cat$Species), ], method = &quot;class&quot;, na.action = na.omit ) # Imputation for a numeric variable anova_mod &lt;- rpart( Sepal.Width ~ . - Sepal.Length, data = iris.mis[!is.na(iris.mis$Sepal.Width), ], method = &quot;anova&quot;, na.action = na.omit ) # Predictions species_pred &lt;- predict(class_mod, iris.mis.cat[is.na(iris.mis.cat$Species), ]) width_pred &lt;- predict(anova_mod, iris.mis[is.na(iris.mis$Sepal.Width), ]) 11.9.7 MICE (Multivariate Imputation via Chained Equations) MICE assumes that the data are Missing at Random (MAR). It imputes data for each variable by specifying an imputation model tailored to the variable type. 11.9.7.1 How MICE Works For a dataset with variables \\(X_1, X_2, \\dots, X_k\\): If \\(X_1\\) has missing data, it is regressed on the other variables. This process is repeated for all variables with missing data, using the previously predicted values as needed. By default: Continuous variables use linear regression. Categorical variables use logistic regression. 11.9.7.2 Methods Available in MICE pmm (Predictive Mean Matching): For numeric variables. logreg (Logistic Regression): For binary variables (2 levels). polyreg (Bayesian polytomous regression): For factor variables (≥2 levels). Proportional Odds Model: For ordered factor variables (≥2 levels). # Load packages library(mice) library(VIM) # Check missing values pattern md.pattern(iris.mis) #&gt; Sepal.Width Sepal.Length Petal.Length Petal.Width #&gt; 100 1 1 1 1 0 #&gt; 15 1 1 1 0 1 #&gt; 8 1 1 0 1 1 #&gt; 2 1 1 0 0 2 #&gt; 11 1 0 1 1 1 #&gt; 1 1 0 1 0 2 #&gt; 1 1 0 0 1 2 #&gt; 1 1 0 0 0 3 #&gt; 7 0 1 1 1 1 #&gt; 3 0 1 0 1 2 #&gt; 1 0 0 1 1 2 #&gt; 11 15 15 19 60 # Plot missing values aggr( iris.mis, col = c(&#39;navyblue&#39;, &#39;yellow&#39;), numbers = TRUE, sortVars = TRUE, labels = names(iris.mis), cex.axis = 0.7, gap = 3, ylab = c(&quot;Missing data&quot;, &quot;Pattern&quot;) ) #&gt; #&gt; Variables sorted by number of missings: #&gt; Variable Count #&gt; Petal.Width 0.12666667 #&gt; Sepal.Length 0.10000000 #&gt; Petal.Length 0.10000000 #&gt; Sepal.Width 0.07333333 Imputing Data # Perform multiple imputation using MICE imputed_Data &lt;- mice( iris.mis, m = 5, # Number of imputed datasets maxit = 10, # Number of iterations method = &#39;pmm&#39;, # Imputation method seed = 500 # Random seed for reproducibility ) Evaluating Imputed Data # Summary of imputed data summary(imputed_Data) #&gt; Class: mids #&gt; Number of multiple imputations: 5 #&gt; Imputation methods: #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; #&gt; PredictorMatrix: #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Sepal.Length 0 1 1 1 #&gt; Sepal.Width 1 0 1 1 #&gt; Petal.Length 1 1 0 1 #&gt; Petal.Width 1 1 1 0 # Density plot: compare imputed values (red) with observed values (blue) densityplot(imputed_Data) Accessing and Using Imputed Data # Access the complete datasets completeData1 &lt;- complete(imputed_Data, 1) # First imputed dataset completeData2 &lt;- complete(imputed_Data, 2) # Second imputed dataset Regression Model with Imputed Dataset # Fit a regression model using imputed datasets fit &lt;- with(data = imputed_Data, exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width)) # Combine results of all 5 models combine &lt;- pool(fit) summary(combine) #&gt; term estimate std.error statistic df p.value #&gt; 1 (Intercept) 1.9054698 0.33454626 5.695684 105.12438 1.127064e-07 #&gt; 2 Sepal.Length 0.2936285 0.07011405 4.187870 88.69066 6.625536e-05 #&gt; 3 Petal.Width -0.4742921 0.08138313 -5.827892 46.94941 4.915270e-07 11.9.8 Amelia Amelia uses a bootstrap-based Expectation-Maximization with Bootstrapping (EMB) algorithm for imputation, making it faster and suitable for cross-sectional and time-series data. 11.9.8.1 Assumptions All variables must follow a Multivariate Normal Distribution (MVN). Transformations may be required for non-normal data. Data must be Missing at Random (MAR). 11.9.8.2 Comparison: Amelia vs. MICE MICE imputes on a variable-by-variable basis using separate models. Amelia uses a joint modeling approach based on MVN. MICE handles multiple data types, while Amelia requires variables to approximate normality. 11.9.8.3 Imputation with Amelia library(Amelia) data(&quot;iris&quot;) # Seed 10% missing values set.seed(123) iris.mis &lt;- prodNA(iris, noNA = 0.1) # Specify columns and run Amelia amelia_fit &lt;- amelia( iris.mis, m = 5, # Number of imputations parallel = &quot;multicore&quot;, # Use multicore processing noms = &quot;Species&quot; # Nominal variables ) #&gt; -- Imputation 1 -- #&gt; #&gt; 1 2 3 4 5 6 7 #&gt; #&gt; -- Imputation 2 -- #&gt; #&gt; 1 2 3 4 5 #&gt; #&gt; -- Imputation 3 -- #&gt; #&gt; 1 2 3 4 5 #&gt; #&gt; -- Imputation 4 -- #&gt; #&gt; 1 2 3 4 5 6 #&gt; #&gt; -- Imputation 5 -- #&gt; #&gt; 1 2 3 4 5 6 7 8 9 10 # Access imputed outputs # amelia_fit$imputations[[1]] Amelia’s workflow includes bootstrapping multiple imputations to generate robust estimates of means and variances. This process ensures flexibility and speed for large datasets. 11.9.9 missForest The missForest package provides a robust non-parametric imputation method using the Random Forest algorithm. It is versatile, handling both continuous and categorical variables without requiring assumptions about the underlying functional forms. Key Features of missForest Non-Parametric: No assumptions about the functional form. Variable-Specific Models: Builds a random forest model for each variable to impute missing values. Error Estimates: Provides out-of-bag (OOB) imputation error estimates. NRMSE (Normalized Root Mean Squared Error) for continuous variables. PFC (Proportion of Falsely Classified) for categorical variables. High Control: Offers customizable parameters like mtry and ntree. library(missForest) # Impute missing values using default parameters iris.imp &lt;- missForest(iris.mis) # Check imputed values # View the imputed dataset # iris.imp$ximp # Out-of-bag error estimates iris.imp$OOBerror #&gt; NRMSE PFC #&gt; 0.14004144 0.02877698 # Compare imputed data with original data to calculate error iris.err &lt;- mixError(iris.imp$ximp, iris.mis, iris) iris.err #&gt; NRMSE PFC #&gt; 0.14420833 0.09090909 11.9.10 Hmisc The Hmisc package provides a suite of tools for imputing missing data, offering both simple methods (like mean or median imputation) and more advanced approaches like aregImpute. Features of Hmisc impute(): Simple imputation using user-defined methods like mean, median, or a random value. aregImpute(): Combines additive regression, bootstrapping, and predictive mean matching. Handles continuous and categorical variables. Automatically recognizes variable types and applies appropriate methods. Assumptions Linearity in the variables being predicted. Fisher’s optimum scoring is used for categorical variable prediction. library(Hmisc) # Impute using mean iris.mis$imputed_SepalLength &lt;- with(iris.mis, impute(Sepal.Length, mean)) # Impute using random value iris.mis$imputed_SepalLength2 &lt;- with(iris.mis, impute(Sepal.Length, &#39;random&#39;)) # Advanced imputation using aregImpute impute_arg &lt;- aregImpute( ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + Species, data = iris.mis, n.impute = 5 ) #&gt; Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 # Check R-squared values for predicted missing values impute_arg #&gt; #&gt; Multiple Imputation using Bootstrap and PMM #&gt; #&gt; aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + #&gt; Petal.Width + Species, data = iris.mis, n.impute = 5) #&gt; #&gt; n: 150 p: 5 Imputations: 5 nk: 3 #&gt; #&gt; Number of NAs: #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 17 19 12 16 11 #&gt; #&gt; type d.f. #&gt; Sepal.Length s 2 #&gt; Sepal.Width s 2 #&gt; Petal.Length s 2 #&gt; Petal.Width s 2 #&gt; Species c 2 #&gt; #&gt; Transformation of Target Variables Forced to be Linear #&gt; #&gt; R-squares for Predicting Non-Missing Values for Each Variable #&gt; Using Last Imputations of Predictors #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 0.895 0.536 0.987 0.967 0.984 # Access imputed values for Sepal.Length impute_arg$imputed$Sepal.Length #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; 13 4.4 4.9 4.9 5.0 4.9 #&gt; 14 4.8 4.4 5.0 4.5 4.5 #&gt; 23 4.8 5.1 5.1 5.1 4.8 #&gt; 26 5.0 4.8 4.9 4.9 5.0 #&gt; 34 5.0 5.8 6.0 5.7 5.8 #&gt; 39 4.4 4.9 5.0 4.5 4.6 #&gt; 41 5.2 5.1 4.8 5.0 4.8 #&gt; 69 5.8 6.0 6.3 6.0 6.1 #&gt; 72 5.6 5.7 5.7 5.8 6.1 #&gt; 89 6.1 5.7 5.7 5.6 6.9 #&gt; 90 5.5 6.2 5.2 6.0 5.8 #&gt; 91 5.7 6.9 6.0 6.4 6.4 #&gt; 116 5.9 6.8 6.4 6.6 6.9 #&gt; 118 7.9 7.9 7.9 7.9 7.9 #&gt; 135 6.7 6.7 6.7 6.9 6.7 #&gt; 141 7.0 6.3 5.9 6.7 7.0 #&gt; 143 5.7 6.7 5.8 6.3 5.4 Note: While missForest often outperforms Hmisc in terms of accuracy, the latter is useful for datasets with simpler requirements. 11.9.11 mi The mi package is a powerful tool for imputation, using Bayesian methods and providing rich diagnostics for model evaluation and convergence. Features of mi Graphical Diagnostics: Visualize imputation models and convergence. Bayesian Regression: Handles separation and other issues in data. Irregularity Detection: Automatically detects issues like high collinearity. Noise Addition: Adds noise to address additive constraints. library(mi) # Perform imputation using mi mi_data &lt;- mi(iris.mis, seed = 1) # Summary of the imputation process summary(mi_data) #&gt; $Sepal.Length #&gt; $Sepal.Length$is_missing #&gt; missing #&gt; FALSE TRUE #&gt; 133 17 #&gt; #&gt; $Sepal.Length$imputed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.6355172 -0.0703238 -0.0005039 -0.0052716 0.0765631 0.3731257 #&gt; #&gt; $Sepal.Length$observed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.90110 -0.47329 -0.04549 0.00000 0.32120 1.23792 #&gt; #&gt; #&gt; $Sepal.Width #&gt; $Sepal.Width$is_missing #&gt; missing #&gt; FALSE TRUE #&gt; 131 19 #&gt; #&gt; $Sepal.Width$imputed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -2.1083 -0.4216 -0.1925 -0.1940 0.1589 0.7330 #&gt; #&gt; $Sepal.Width$observed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -1.01272 -0.30642 -0.07099 0.00000 0.39988 1.34161 #&gt; #&gt; #&gt; $Petal.Length #&gt; $Petal.Length$is_missing #&gt; missing #&gt; FALSE TRUE #&gt; 138 12 #&gt; #&gt; $Petal.Length$imputed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.86312 -0.58453 0.23556 0.04176 0.48870 0.77055 #&gt; #&gt; $Petal.Length$observed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.7797 -0.6088 0.1459 0.0000 0.3880 0.9006 #&gt; #&gt; #&gt; $Petal.Width #&gt; $Petal.Width$is_missing #&gt; missing #&gt; FALSE TRUE #&gt; 134 16 #&gt; #&gt; $Petal.Width$imputed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.9116177 -0.0000042 0.2520468 0.1734543 0.5147010 0.8411324 #&gt; #&gt; $Petal.Width$observed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.69624 -0.56602 0.08503 0.00000 0.41055 0.86629 #&gt; #&gt; #&gt; $Species #&gt; $Species$crosstab #&gt; #&gt; observed imputed #&gt; setosa 180 16 #&gt; versicolor 192 11 #&gt; virginica 184 17 #&gt; #&gt; #&gt; $imputed_SepalLength #&gt; $imputed_SepalLength$is_missing #&gt; [1] &quot;all values observed&quot; #&gt; #&gt; $imputed_SepalLength$observed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.9574 -0.4379 0.0000 0.0000 0.3413 1.3152 #&gt; #&gt; #&gt; $imputed_SepalLength2 #&gt; $imputed_SepalLength2$is_missing #&gt; [1] &quot;all values observed&quot; #&gt; #&gt; $imputed_SepalLength2$observed #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.90570 -0.48398 -0.06225 0.00000 0.35947 1.20292 "],["data.html", "Chapter 12 Data", " Chapter 12 Data There are multiple ways to categorize data. For example, Qualitative vs. Quantitative: Qualitative Quantitative in-depth interviews, documents, focus groups, case study, ethnography. open-ended questions. observations in words experiments, observation in words, survey with closed-end questions, structured interviews language, descriptive quantities, numbers Text-based Numbers-based Subjective Objectivity "],["cross-sectional.html", "12.1 Cross-Sectional", " 12.1 Cross-Sectional "],["time-series-1.html", "12.2 Time Series", " 12.2 Time Series \\[ y_t = \\beta_0 + x_{t1}\\beta_1 + x_{t2}\\beta_2 + ... + x_{t(k-1)}\\beta_{k-1} + \\epsilon_t \\] Examples Static Model \\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\) Finite Distributed Lag model \\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\) Long Run Propensity (LRP) is \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\) Dynamic Model \\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\) Finite Sample Properties for Time Series: A1-A3: OLS is unbiased A1-A4: usual standard errors are consistent and Gauss-Markov Theorem holds (OLS is BLUE) A1-A6, A6: Finite Sample Wald Test (t-test and F-test) are valid A3 might not hold under time series setting Spurious Time Trend - solvable Strict vs Contemporaneous Exogeneity - not solvable In time series data, there are many processes: Autoregressive model of order p: AR(p) Moving average model of order q: MA(q) Autoregressive model of order p and moving average model of order q: ARMA(p,q) Autoregressive conditional heteroskedasticity model of order p: ARCH(p) Generalized Autoregressive conditional heteroskedasticity of orders p and q; GARCH(p.q) 12.2.1 Deterministic Time trend Both the dependent and independent variables are trending over time Spurious Time Series Regression \\[ y_t = \\alpha_0 + t\\alpha_1 + v_t \\] and x takes the form \\[ x_t = \\lambda_0 + t\\lambda_1 + u_t \\] \\(\\alpha_1 \\neq 0\\) and \\(\\lambda_1 \\neq 0\\) \\(v_t\\) and \\(u_t\\) are independent there is no relationship between \\(y_t\\) and \\(x_t\\) If we estimate the regression, \\[ y_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t \\] so the true \\(\\beta_1=0\\) Inconsistent: \\(plim(\\hat{\\beta}_1)=\\frac{\\alpha_1}{\\lambda_1}\\) Invalid Inference: \\(|t| \\to^d \\infty\\) for \\(H_0: \\beta_1=0\\), will always reject the null as \\(n \\to \\infty\\) Uninformative \\(R^2\\): \\(plim(R^2) = 1\\) will be able to perfectly predict as \\(n \\to \\infty\\) We can rewrite the equation as \\[ \\begin{aligned} y_t &amp;=\\beta_0 + \\beta_1x_t+\\epsilon_t \\\\ \\epsilon_t &amp;= \\alpha_1t + v_t \\end{aligned} \\] where \\(\\beta_0 = \\alpha_0\\) and \\(\\beta_1=0\\). Since \\(x_t\\) is a deterministic function of time, \\(\\epsilon_t\\) is correlated with \\(x_t\\) and we have the usual omitted variable bias. Even when \\(y_t\\) and \\(x_t\\) are related (\\(\\beta_1 \\neq 0\\)) but they are both trending over time, we still get spurious results with the simple regression on \\(y_t\\) on \\(x_t\\) Solutions to Spurious Trend Include time trend \\(t\\) as an additional control consistent parameter estimates and valid inference Detrend both dependent and independent variables and then regress the detrended outcome on detrended independent variables (i.e., regress residuals \\(\\hat{u}_t\\) on residuals \\(\\hat{v}_t\\)) Detrending is the same as partialing out in the Frisch-Waugh-Lovell Theorem Could allow for non-linear time trends by including \\(t\\) \\(t^2\\), and \\(\\exp(t)\\) Allow for seasonality by including indicators for relevant “seasons” (quarters, months, weeks). A3 does not hold under: Feedback Effect \\(\\epsilon_t\\) influences next period’s independent variables Dynamic Specification include last time period outcome as an explanatory variable Dynamically Complete For finite distrusted lag model, the number of lags needs to be absolutely correct. 12.2.2 Feedback Effect \\[ y_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t \\] A3 \\[ E(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T) \\] will not equal 0, because \\(y_t\\) will likely influence \\(x_{t+1},..,x_T\\) A3 is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity) 12.2.3 Dynamic Specification \\[ y_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t \\] \\[ E(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| y_1,y_2, ...,y_t,y_{t+1},...,y_T) \\] will not equal 0, because \\(y_t\\) and \\(\\epsilon_t\\) are inherently correlated A3 is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity) Dynamic Specification is not allowed under A3 12.2.4 Dynamically Complete \\[ y_t = \\beta_0 + x_t\\delta_0 + x_{t-1}\\delta_1 + \\epsilon_t \\] \\[ E(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T) \\] will not equal 0, because if we did not include enough lags, \\(x_{t-2}\\) and \\(\\epsilon_t\\) are correlated A3 is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity) Can be corrected by including more lags (but when stop? ) Without A3 OLS is biased Gauss-Markov Theorem Finite Sample Properties are invalid then, we can Focus on Large Sample Properties Can use A3a instead of A3 A3a in time series become \\[ A3a: E(\\mathbf{x}_t&#39;\\epsilon_t)= 0 \\] only the regressors in this time period need to be independent from the error in this time period (Contemporaneous Exogeneity) \\(\\epsilon_t\\) can be correlated with \\(...,x_{t-2},x_{t-1},x_{t+1}, x_{t+2},...\\) can have a dynamic specification \\(y_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\\) Deriving Large Sample Properties for Time Series Assumptions A1, A2, A3a [Weak Law] and Central Limit Theorem depend on A5 \\(x_t\\) and \\(\\epsilon_t\\) are dependent over t without [Weak Law] or Central Limit Theorem depend on A5, we cannot have Large Sample Properties for OLS Instead of A5, we consider A5a Derivation of the Asymptotic Variance depends on A4 time series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\) under A1, A2, A3a, and A5a, OLS estimator is consistent, and asymptotically normal 12.2.5 Highly Persistent Data If \\(y_t, \\mathbf{x}_t\\) are not weakly dependent stationary process \\(y_t\\) and \\(y_{t-h}\\) are not almost independent for large h A5a does not hold and OLS is not consistent and does not have a limiting distribution. Example + Random Walk \\(y_t = y_{t-1} + u_t\\) + Random Walk with a drift: \\(y_t = \\alpha+ y_{t-1} + u_t\\) Solution First difference is a stationary process \\[ y_t - y_{t-1} = u_t \\] If \\(u_t\\) is a weakly dependent process (also called integrated of order 0) then \\(y_t\\) is said to be difference-stationary process (integrated of order 1) For regression, if \\(\\{y_t, \\mathbf{x}_t \\}\\) are random walks (integrated at order 1), can consistently estimate the first difference equation \\[ \\begin{aligned} y_t - y_{t-1} &amp;= (\\mathbf{x}_t - \\mathbf{x}_{t-1}\\beta + \\epsilon_t - \\epsilon_{t-1}) \\\\ \\Delta y_t &amp;= \\Delta \\mathbf{x}\\beta + \\Delta u_t \\end{aligned} \\] Unit Root Test \\[ y_t = \\alpha + \\alpha y_{t-1} + u_t \\] tests if \\(\\rho=1\\) (integrated of order 1) Under the null \\(H_0: \\rho = 1\\), OLS is not consistent or asymptotically normal. Under the alternative \\(H_a: \\rho &lt; 1\\), OLS is consistent and asymptotically normal. usual t-test is not valid, will need to use the transformed equation to produce a valid test. Dickey-Fuller Test \\[ \\Delta y_t= \\alpha + \\theta y_{t-1} + v_t \\] where \\(\\theta = \\rho -1\\) \\(H_0: \\theta = 0\\) and \\(H_a: \\theta &lt; 0\\) Under the null, \\(\\Delta y_t\\) is weakly dependent but \\(y_{t-1}\\) is not. Dickey and Fuller derived the non-normal asymptotic distribution. If you reject the null then \\(y_t\\) is not a random walk. Concerns with the standard Dickey Fuller Test 1. Only considers a fairly simplistic dynamic relationship \\[ \\Delta y_t = \\alpha + \\theta y_{t-1} + \\gamma_1 \\Delta_{t-1} + ..+ \\gamma_p \\Delta_{t-p} +v_t \\] with one additional lag, under the null \\(\\Delta_{y_t}\\) is an AR(1) process and under the alternative \\(y_t\\) is an AR(2) process. Solution: include lags of \\(\\Delta_{y_t}\\) as controls. Does not allow for time trend \\[ \\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + v_t \\] allows \\(y_t\\) to have a quadratic relationship with \\(t\\) Solution: include time trend (changes the critical values). Adjusted Dickey-Fuller Test \\[ \\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + \\gamma_1 \\Delta y_{t-1} + ... + \\gamma_p \\Delta y_{t-p} + v_t \\] where \\(\\theta = 1 - \\rho\\) \\(H_0: \\theta_1 = 0\\) and \\(H_a: \\theta_1 &lt; 0\\) Under the null, \\(\\Delta y_t\\) is weakly dependent but \\(y_{t-1}\\) is not Critical values are different with the time trend, if you reject the null then \\(y_t\\) is not a random walk. 12.2.5.0.1 Newey West Standard Errors If A4 does not hold, we can use Newey West Standard Errors (HAC - Heteroskedasticity Autocorrelation Consistent) \\[ \\hat{B} = T^{-1} \\sum_{t=1}^{T} e_t^2 \\mathbf{x&#39;_tx_t} + \\sum_{h=1}^{g}(1-\\frac{h}{g+1})T^{-1}\\sum_{t=h+1}^{T} e_t e_{t-h}(\\mathbf{x_t&#39;x_{t-h}+ x_{t-h}&#39;x_t}) \\] estimates the covariances up to a distance g part downweights to insure \\(\\hat{B}\\) is PSD How to choose g: For yearly data: \\(g = 1\\) or 2 is likely to account for most of the correlation For quarterly or monthly data: g should be larger ($g = 4$ or 8 for quarterly and \\(g = 12\\) or 14 for monthly) can also take integer part of \\(4(T/100)^{2/9}\\) or integer part of \\(T^{1/4}\\) Testing for Serial Correlation Run OLS regression of \\(y_t\\) on \\(\\mathbf{x_t}\\) and obtain residuals \\(e_t\\) Run OLS regression of \\(e_t\\) on \\(\\mathbf{x}_t, e_{t-1}\\) and test whether coefficient on \\(e_{t-1}\\) is significant. Reject the null of no serial correlation if the coefficient is significant at the 5% level. Test using heteroskedastic robust standard errors can include \\(e_{t-2},e_{t-3},..\\) in step 2 to test for higher order serial correlation (t-test would now be an F-test of joint significance) "],["repeated-cross-sections.html", "12.3 Repeated Cross Sections", " 12.3 Repeated Cross Sections For each time point (day, month, year, etc.), a set of data is sampled. This set of data can be different among different time points. For example, you can sample different groups of students each time you survey. Allowing structural change in pooled cross section \\[ y_i = \\mathbf{x}_i \\beta + \\delta_1 y_1 + ... + \\delta_T y_T + \\epsilon_i \\] Dummy variables for all but one time period allows different intercept for each time period allows outcome to change on average for each time period Allowing for structural change in pooled cross section \\[ y_i = \\mathbf{x}_i \\beta + \\mathbf{x}_i y_1 \\gamma_1 + ... + \\mathbf{x}_i y_T \\gamma_T + \\delta_1 y_1 + ...+ \\delta_T y_T + \\epsilon_i \\] Interact \\(x_i\\) with time period dummy variables allows different slopes for each time period allows effects to change based on time period (structural break) Interacting all time period dummies with \\(x_i\\) can produce many variables - use hypothesis testing to determine which structural breaks are needed. 12.3.1 Pooled Cross Section \\[ y_i=\\mathbf{x_i\\beta +x_i \\times y1\\gamma_1 + ...+ x_i \\times yT\\gamma_T + \\delta_1y_1+...+ \\delta_Ty_T + \\epsilon_i} \\] Interact \\(x_i\\) with time period dummy variables allows different slopes for each time period allows effect to change based on time period (structural break) interacting all time period dummies with \\(x_i\\) can produce many variables - use hypothesis testing to determine which structural breaks are needed. "],["panel-data.html", "12.4 Panel Data", " 12.4 Panel Data Detail notes in R can be found here Follows an individual over T time periods. Panel data structure is like having n samples of time series data Characteristics Information both across individuals and over time (cross-sectional and time-series) N individuals and T time periods Data can be either Balanced: all individuals are observed in all time periods Unbalanced: all individuals are not observed in all time periods. Assume correlation (clustering) over time for a given individual, with independence over individuals. Types Short panel: many individuals and few time periods. Long panel: many time periods and few individuals Both: many time periods and many individuals Time Trends and Time Effects Nonlinear Seasonality Discontinuous shocks Regressors Time-invariant regressors \\(x_{it}=x_i\\) for all t (e.g., gender, race, education) have zero within variation Individual-invariant regressors \\(x_{it}=x_{t}\\) for all i (e.g., time trend, economy trends) have zero between variation Variation for the dependent variable and regressors Overall variation: variation over time and individuals. Between variation: variation between individuals Within variation: variation within individuals (over time). Estimate Formula Individual mean \\(\\bar{x_i}= \\frac{1}{T} \\sum_{t}x_{it}\\) Overall mean \\(\\bar{x}=\\frac{1}{NT} \\sum_{i} \\sum_t x_{it}\\) Overall Variance \\(s _O^2 = \\frac{1}{NT-1} \\sum_i \\sum_t (x_{it} - \\bar{x})^2\\) Between variance \\(s_B^2 = \\frac{1}{N-1} \\sum_i (\\bar{x_i} -\\bar{x})^2\\) Within variance \\(s_W^2= \\frac{1}{NT-1} \\sum_i \\sum_t (x_{it} - \\bar{x_i})^2 = \\frac{1}{NT-1} \\sum_i \\sum_t (x_{it} - \\bar{x_i} +\\bar{x})^2\\) Note: \\(s_O^2 \\approx s_B^2 + s_W^2\\) Since we have n observation for each time period t, we can control for each time effect separately by including time dummies (time effects) \\[ y_{it}=\\mathbf{x_{it}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + \\epsilon_{it} \\] Note: we cannot use these many time dummies in time series data because in time series data, our n is 1. Hence, there is no variation, and sometimes not enough data compared to variables to estimate coefficients. Unobserved Effects Model Similar to group clustering, assume that there is a random effect that captures differences across individuals but is constant in time. \\[ y_it=\\mathbf{x_{it}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + c_i + u_{it} \\] where \\(c_i + u_{it} = \\epsilon_{it}\\) \\(c_i\\) unobserved individual heterogeneity (effect) \\(u_{it}\\) idiosyncratic shock \\(\\epsilon_{it}\\) unobserved error term. 12.4.1 Pooled OLS Estimator If \\(c_i\\) is uncorrelated with \\(x_{it}\\) \\[ E(\\mathbf{x_{it}&#39;}(c_i+u_{it})) = 0 \\] then A3a still holds. And we have Pooled OLS consistent. If A4 does not hold, OLS is still consistent, but not efficient, and we need cluster robust SE. Sufficient for A3a to hold, we need Exogeneity for \\(u_{it}\\) A3a (contemporaneous exogeneity): \\(E(\\mathbf{x_{it}&#39;}u_{it})=0\\) time varying error Random Effect Assumption (time constant error): \\(E(\\mathbf{x_{it}&#39;}c_{i})=0\\) Pooled OLS will give you consistent coefficient estimates under A1, A2, A3a (for both \\(u_{it}\\) and RE assumption), and A5 (randomly sampling across i). 12.4.2 Individual-specific effects model If we believe that there is unobserved heterogeneity across individual (e.g., unobserved ability of an individual affects \\(y\\)), If the individual-specific effects are correlated with the regressors, then we have the Fixed Effects Estimator. and if they are not correlated we have the Random Effects Estimator. 12.4.2.1 Random Effects Estimator Random Effects estimator is the Feasible GLS estimator that assumes \\(u_{it}\\) is serially uncorrelated and homoskedastic Under A1, A2, A3a (for both \\(u_{it}\\) and RE assumption) and A5 (randomly sampling across i), RE estimator is consistent. If A4 holds for \\(u_{it}\\), RE is the most efficient estimator If A4 fails to hold (may be heteroskedasticity across i, and serial correlation over t), then RE is not the most efficient, but still more efficient than pooled OLS. 12.4.2.2 Fixed Effects Estimator also known as Within Estimator uses within variation (over time) If the RE assumption is not hold (\\(E(\\mathbf{x_{it}&#39;}c_i) \\neq 0\\)), then A3a does not hold (\\(E(\\mathbf{x_{it}&#39;}\\epsilon_i) \\neq 0\\)). Hence, the OLS and RE are inconsistent/biased (because of omitted variable bias) However, FE can only fix bias due to time-invariant factors (both observables and unobservables) correlated with treatment (not time-variant factors that correlated with the treatment). The traditional FE technique is flawed when lagged dependent variables are included in the model. (Nickell 1981) (Narayanan and Nair 2013) With measurement error in the independent, FE will exacerbate the errors-in-the-variables bias. 12.4.2.2.1 Demean Approach To deal with violation in \\(c_i\\), we have \\[ y_{it}= \\mathbf{x_{it} \\beta} + c_i + u_{it} \\] \\[ \\bar{y_i}=\\bar{\\mathbf{x_i}} \\beta + c_i + \\bar{u_i} \\] where the second equation is the time averaged equation using within transformation, we have \\[ y_{it} - \\bar{y_i} = \\mathbf{(x_{it} - \\bar{x_i})}\\beta + u_{it} - \\bar{u_i} \\] because \\(c_i\\) is time constant. The Fixed Effects estimator uses POLS on the transformed equation \\[ y_{it} - \\bar{y_i} = \\mathbf{(x_{it} - \\bar{x_i})} \\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + u_{it} - \\bar{u_i} \\] we need A3 (strict exogeneity) (\\(E((\\mathbf{x_{it}-\\bar{x_i}})&#39;(u_{it}-\\bar{u_i})=0\\)) to have FE consistent. Variables that are time constant will be absorbed into \\(c_i\\). Hence we cannot make inference on time constant independent variables. If you are interested in the effects of time-invariant variables, you could consider the OLS or between estimator It’s recommended that you should still use cluster robust standard errors. 12.4.2.2.2 Dummy Approach Equivalent to the within transformation (i.e., mathematically equivalent to Demean Approach), we can have the fixed effect estimator be the same with the dummy regression \\[ y_{it} = x_{it}\\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + c_1\\gamma_1 + ... + c_{n-1}\\gamma_{n-1} + u_{it} \\] where \\[ c_i = \\begin{cases} 1 &amp;\\text{if observation is i} \\\\ 0 &amp;\\text{otherwise} \\\\ \\end{cases} \\] The standard error is incorrectly calculated. the FE within transformation is controlling for any difference across individual which is allowed to correlated with observables. 12.4.2.2.3 First-difference Approach Economists typically use this approach \\[ y_{it} - y_{i (t-1)} = (\\mathbf{x}_{it} - \\mathbf{x}_{i(t-1)}) \\beta + + (u_{it} - u_{i(t-1)}) \\] 12.4.2.2.4 Fixed Effects Summary The three approaches are almost equivalent. Demean Approach is mathematically equivalent to Dummy Approach If you have only 1 period, all 3 are the same. Since fixed effect is a within estimator, only status changes can contribute to \\(\\beta\\) variation. Hence, with a small number of changes then the standard error for \\(\\beta\\) will explode Status changes mean subjects change from (1) control to treatment group or (2) treatment to control group. Those who have status change, we call them switchers. Treatment effect is typically non-directional. You can give a parameter for the direction if needed. Issues: You could have fundamental difference between switchers and non-switchers. Even though we can’t definitive test this, but providing descriptive statistics on switchers and non-switchers can give us confidence in our conclusion. Because fixed effects focus on bias reduction, you might have larger variance (typically, with fixed effects you will have less df) If the true model is random effect, economists typically don’t care, especially when \\(c_i\\) is the random effect and \\(c_i \\perp x_{it}\\) (because RE assumption is that it is unrelated to \\(x_{it}\\)). The reason why economists don’t care is because RE wouldn’t correct bias, it only improves efficiency over OLS. You can estimate FE for different units (not just individuals). FE removes bias from time invariant factors but not without costs because it uses within variation, which imposes strict exogeneity assumption on \\(u_{it}\\): \\(E[(x_{it} - \\bar{x}_{i})(u_{it} - \\bar{u}_{it})]=0\\) Recall \\[ Y_{it} = \\beta_0 + X_{it}\\beta_1 + \\alpha_i + u_{it} \\] where \\(\\epsilon_{it} = \\alpha_i + u_{it}\\) \\[ \\hat{\\sigma}^2_\\epsilon = \\frac{SSR_{OLS}}{NT - K} \\] \\[ \\hat{\\sigma}^2_u = \\frac{SSR_{FE}}{NT - (N+K)} = \\frac{SSR_{FE}}{N(T-1)-K} \\] It’s ambiguous whether your variance of error changes up or down because SSR can increase while the denominator decreases. FE can be unbiased, but not consistent (i.e., not converging to the true effect) 12.4.2.2.5 FE Examples 12.4.2.2.6 Blau (1999) Intergenerational mobility If we transfer resources to low income family, can we generate upward mobility (increase ability)? Mechanisms for intergenerational mobility Genetic (policy can’t affect) (i.e., ability endowment) Environmental indirect Environmental direct \\[ \\frac{\\% \\Delta \\text{Human capital}}{\\% \\Delta \\text{income}} \\] Financial transfer Income measures: Total household income Wage income Non-wage income Annual versus permanent income Core control variables: Bad controls are those jointly determined with dependent variable Control by mother = choice by mother Uncontrolled by mothers: mother race location of birth education of parents household structure at age 14 \\[ Y_{ijt} = X_{jt} \\beta_i + I_{jt} \\alpha_i + \\epsilon_{ijt} \\] where \\(i\\) = test \\(j\\) = individual (child) \\(t\\) = time Grandmother’s model Since child is nested within mother and mother nested within grandmother, the fixed effect of child is included in the fixed effect of mother, which is included in the fixed-effect of grandmother \\[ Y_{ijgmt} = X_{it} \\beta_{i} + I_{jt} \\alpha_i + \\gamma_g + u_{ijgmt} \\] where \\(i\\) = test, \\(j\\) = kid, \\(m\\) = mother, \\(g\\) = grandmother where \\(\\gamma_g\\) includes \\(\\gamma_m\\) includes \\(\\gamma_j\\) Grandma fixed-effect Pros: control for some genetics + fixed characteristics of how mother are raised can estimate effect of parameter income Con: Might not be a sufficient control Common to cluster a the fixed-effect level (common correlated component) Fixed effect exaggerates attenuation bias Error rate on survey can help you fix this (plug in the number only , but not the uncertainty associated with that number). 12.4.2.2.7 Babcock (2010) \\[ T_{ijct} = \\alpha_0 + S_{jct} \\alpha_1 + X_{ijct} \\alpha_2 + u_{ijct} \\] where \\(S_{jct}\\) is the average class expectation \\(X_{ijct}\\alpha_2\\) is the individual characteristics \\(i\\) student \\(j\\) instructor \\(c\\) course \\(t\\) time \\[ T_{ijct} = \\beta_0+ S_{jct} \\beta_1+ X_{ijct} \\beta_2 +\\mu_{jc} + \\epsilon_{ijct} \\] where \\(\\mu_{jc}\\) is instructor by course fixed effect (unique id), which is different from \\((\\theta_j + \\delta_c)\\) Decrease course shopping because conditioned on available information (\\(\\mu_{jc}\\)) (class grade and instructor’s info). Grade expectation change even though class materials stay the same Identification strategy is Under (fixed) time-varying factor that could bias my coefficient (simultaneity) \\[ Y_{ijt} = X_{it} \\beta_1 + \\text{Teacher Experience}_{jt} \\beta_2 + \\text{Teacher education}_{jt} \\beta_3 + \\text{Teacher score}_{it}\\beta_4 + \\dots + \\epsilon_{ijt} \\] Drop teacher characteristics, and include teacher dummy effect \\[ Y_{ijt} = X_{it} \\alpha + \\Gamma_{it} \\theta_j + u_{ijt} \\] where \\(\\alpha\\) is the within teacher (conditional on teacher fixed effect) and \\(j = 1 \\to (J-1)\\) Nuisance in the sense that we don’t about the interpretation of \\(\\alpha\\) The least we can say about \\(\\theta_j\\) is the teacher effect conditional on student test score. \\[ Y_{ijt} = X_{it} \\gamma + \\epsilon_{ijt} \\] \\(\\gamma\\) is between within (unconditional) and \\(e_{ijt}\\) is the prediction error \\[ e_{ijt} = T_{it} \\delta_j + \\tilde{e}_{ijt} \\] where \\(\\delta_j\\) is the mean for each group \\[ Y_{ijkt} = Y_{ijkt-1} + X_{it} \\beta + T_{it} \\tau_j + (W_i + P_k + \\epsilon_{ijkt}) \\] where \\(Y_{ijkt-1}\\) = lag control \\(\\tau_j\\) = teacher fixed time \\(W_i\\) is the student fixed effect \\(P_k\\) is the school fixed effect \\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\) And we worry about selection on class and school Bias in \\(\\tau\\) (for 1 teacher) is \\[ \\frac{1}{N_j} \\sum_{i = 1}^N (W_i + P_k + \\epsilon_{ijkt}) \\] where \\(N_j\\) = the number of student in class with teacher \\(j\\) then we can \\(P_k + \\frac{1}{N_j} \\sum_{i = 1}^{N_j} (W_i + \\epsilon_{ijkt})\\) Shocks from small class can bias \\(\\tau\\) \\[ \\frac{1}{N_j} \\sum_{i = 1}^{N_j} \\epsilon_{ijkt} \\neq 0 \\] which will inflate the teacher fixed effect Even if we create random teacher fixed effect and put it in the model, it still contains bias mentioned above which can still \\(\\tau\\) (but we do not know the way it will affect - whether more positive or negative). If teachers switch schools, then we can estimate both teacher and school fixed effect (mobility web thin vs. thick) Mobility web refers to the web of switchers (i.e., from one status to another). \\[ Y_{ijkt} = Y_{ijk(t-1)} \\alpha + X_{it}\\beta + T_{it} \\tau + P_k + \\epsilon_{ijkt} \\] If we demean (fixed-effect), \\(\\tau\\) (teacher fixed effect) will go away If you want to examine teacher fixed effect, we have to include teacher fixed effect Control for school, the article argues that there is no selection bias For \\(\\frac{1}{N_j} \\sum_{i =1}^{N_j} \\epsilon_{ijkt}\\) (teacher-level average residuals), \\(var(\\tau)\\) does not change with \\(N_j\\) (Figure 2 in the paper). In words, the quality of teachers is not a function of the number of students If \\(var(\\tau) =0\\) it means that teacher quality does not matter Spin-off of Measurement Error: Sampling error or estimation error \\[ \\hat{\\tau}_j = \\tau_j + \\lambda_j \\] \\[ var(\\hat{\\tau}) = var(\\tau + \\lambda) \\] Assume \\(cov(\\tau_j, \\lambda_j)=0\\) (reasonable) In words, your randomness in getting children does not correlation with teacher quality. Hence, \\[ \\begin{aligned} var(\\hat{\\tau}) &amp;= var(\\tau) + var(\\lambda) \\\\ var(\\tau) &amp;= var(\\hat{\\tau}) - var(\\lambda) \\\\ \\end{aligned} \\] We have \\(var(\\hat{\\tau})\\) and we need to estimate \\(var(\\lambda)\\) \\[ var(\\lambda) = \\frac{1}{J} \\sum_{j=1}^J \\hat{\\sigma}^2_j \\] where \\(\\hat{\\sigma}^2_j\\) is the squared standard error of the teacher \\(j\\) (a function of \\(n\\)) Hence, \\[ \\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{reliability} = \\text{true variance signal} \\] also known as how much noise in \\(\\hat{\\tau}\\) and \\[ 1 - \\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{noise} \\] Even in cases where the true relationship is that \\(\\tau\\) is a function of \\(N_j\\), then our recovery method for \\(\\lambda\\) is still not affected To examine our assumption \\[ \\hat{\\tau}_j = \\beta_0 + X_j \\beta_1 + \\epsilon_j \\] Regressing teacher fixed-effect on teacher characteristics should give us \\(R^2\\) close to 0, because teacher characteristics cannot predict sampling error (\\(\\hat{\\tau}\\) contain sampling error) 12.4.3 Tests for Assumptions We typically don’t test heteroskedasticity because we will use robust covariance matrix estimation anyway. Dataset library(&quot;plm&quot;) data(&quot;EmplUK&quot;, package=&quot;plm&quot;) data(&quot;Produc&quot;, package=&quot;plm&quot;) data(&quot;Grunfeld&quot;, package=&quot;plm&quot;) data(&quot;Wages&quot;, package=&quot;plm&quot;) 12.4.3.1 Poolability also known as an F test of stability (or Chow test) for the coefficients \\(H_0\\): All individuals have the same coefficients (i.e., equal coefficients for all individuals). \\(H_a\\) Different individuals have different coefficients. Notes: Under a within (i.e., fixed) model, different intercepts for each individual are assumed Under random model, same intercept is assumed library(plm) plm::pooltest(inv~value+capital, data=Grunfeld, model=&quot;within&quot;) #&gt; #&gt; F statistic #&gt; #&gt; data: inv ~ value + capital #&gt; F = 5.7805, df1 = 18, df2 = 170, p-value = 1.219e-10 #&gt; alternative hypothesis: unstability Hence, we reject the null hypothesis that coefficients are stable. Then, we should use the random model. 12.4.3.2 Individual and time effects use the Lagrange multiplier test to test the presence of individual or time or both (i.e., individual and time). Types: honda: (Honda 1985) Default bp: (Breusch and Pagan 1980) for unbalanced panels kw: (M. L. King and Wu 1997) unbalanced panels, and two-way effects ghm: (Gourieroux, Holly, and Monfort 1982): two-way effects pFtest(inv~value+capital, data=Grunfeld, effect=&quot;twoways&quot;) #&gt; #&gt; F test for twoways effects #&gt; #&gt; data: inv ~ value + capital #&gt; F = 17.403, df1 = 28, df2 = 169, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: significant effects pFtest(inv~value+capital, data=Grunfeld, effect=&quot;individual&quot;) #&gt; #&gt; F test for individual effects #&gt; #&gt; data: inv ~ value + capital #&gt; F = 49.177, df1 = 9, df2 = 188, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: significant effects pFtest(inv~value+capital, data=Grunfeld, effect=&quot;time&quot;) #&gt; #&gt; F test for time effects #&gt; #&gt; data: inv ~ value + capital #&gt; F = 0.23451, df1 = 19, df2 = 178, p-value = 0.9997 #&gt; alternative hypothesis: significant effects 12.4.3.3 Cross-sectional dependence/contemporaneous correlation Null hypothesis: residuals across entities are not correlated. 12.4.3.3.1 Global cross-sectional dependence pcdtest(inv~value+capital, data=Grunfeld, model=&quot;within&quot;) #&gt; #&gt; Pesaran CD test for cross-sectional dependence in panels #&gt; #&gt; data: inv ~ value + capital #&gt; z = 4.6612, p-value = 3.144e-06 #&gt; alternative hypothesis: cross-sectional dependence 12.4.3.3.2 Local cross-sectional dependence use the same command, but supply matrix w to the argument. pcdtest(inv~value+capital, data=Grunfeld, model=&quot;within&quot;) #&gt; #&gt; Pesaran CD test for cross-sectional dependence in panels #&gt; #&gt; data: inv ~ value + capital #&gt; z = 4.6612, p-value = 3.144e-06 #&gt; alternative hypothesis: cross-sectional dependence 12.4.3.4 Serial Correlation Null hypothesis: there is no serial correlation usually seen in macro panels with long time series (large N and T), not seen in micro panels (small T and large N) Serial correlation can arise from individual effects(i.e., time-invariant error component), or idiosyncratic error terms (e..g, in the case of AR(1) process). But typically, when we refer to serial correlation, we refer to the second one. Can be marginal test: only 1 of the two above dependence (but can be biased towards rejection) joint test: both dependencies (but don’t know which one is causing the problem) conditional test: assume you correctly specify one dependence structure, test whether the other departure is present. 12.4.3.4.1 Unobserved effect test semi-parametric test (the test statistic \\(W \\dot{\\sim} N\\) regardless of the distribution of the errors) with \\(H_0: \\sigma^2_\\mu = 0\\) (i.e., no unobserved effects in the residuals), favors pooled OLS. Under the null, covariance matrix of the residuals = its diagonal (off-diagonal = 0) It is robust against both unobserved effects that are constant within every group, and any kind of serial correlation. pwtest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc) #&gt; #&gt; Wooldridge&#39;s test for unobserved individual effects #&gt; #&gt; data: formula #&gt; z = 3.9383, p-value = 8.207e-05 #&gt; alternative hypothesis: unobserved effect Here, we reject the null hypothesis that the no unobserved effects in the residuals. Hence, we will exclude using pooled OLS. 12.4.3.4.2 Locally robust tests for random effects and serial correlation A joint LM test for random effects and serial correlation assuming normality and homoskedasticity of the idiosyncratic errors [Baltagi and Li (1991)](Baltagi and Li 1995) pbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc, test = &quot;j&quot;) #&gt; #&gt; Baltagi and Li AR-RE joint test #&gt; #&gt; data: formula #&gt; chisq = 4187.6, df = 2, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: AR(1) errors or random effects Here, we reject the null hypothesis that there is no presence of serial correlation, and random effects. But we still do not know whether it is because of serial correlation, of random effects or of both To know the departure from the null assumption, we can use Bera, Sosa-Escudero, and Yoon (2001)’s test for first-order serial correlation or random effects (both under normality and homoskedasticity assumption of the error). BSY for serial correlation pbsytest(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc) #&gt; #&gt; Bera, Sosa-Escudero and Yoon locally robust test #&gt; #&gt; data: formula #&gt; chisq = 52.636, df = 1, p-value = 4.015e-13 #&gt; alternative hypothesis: AR(1) errors sub random effects BSY for random effects pbsytest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, data=Produc, test=&quot;re&quot;) #&gt; #&gt; Bera, Sosa-Escudero and Yoon locally robust test (one-sided) #&gt; #&gt; data: formula #&gt; z = 57.914, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: random effects sub AR(1) errors Since BSY is only locally robust, if you “know” there is no serial correlation, then this test is based on LM test is more superior: plmtest(inv ~ value + capital, data = Grunfeld, type = &quot;honda&quot;) #&gt; #&gt; Lagrange Multiplier Test - (Honda) #&gt; #&gt; data: inv ~ value + capital #&gt; normal = 28.252, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: significant effects On the other hand, if you know there is no random effects, to test for serial correlation, use (Breusch 1978)-(Godfrey 1978)’s test lmtest::bgtest() If you “know” there are random effects, use (Baltagi and Li 1995)’s. to test for serial correlation in both AR(1) and MA(1) processes. \\(H_0\\): Uncorrelated errors. Note: one-sided only has power against positive serial correlation. applicable to only balanced panels. pbltest( log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, data = Produc, alternative = &quot;onesided&quot; ) #&gt; #&gt; Baltagi and Li one-sided LM test #&gt; #&gt; data: log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp #&gt; z = 21.69, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: AR(1)/MA(1) errors in RE panel model General serial correlation tests applicable to random effects model, OLS, and FE (with large T, also known as long panel). can also test higher-order serial correlation plm::pbgtest(plm::plm(inv ~ value + capital, data = Grunfeld, model = &quot;within&quot;), order = 2) #&gt; #&gt; Breusch-Godfrey/Wooldridge test for serial correlation in panel models #&gt; #&gt; data: inv ~ value + capital #&gt; chisq = 42.587, df = 2, p-value = 5.655e-10 #&gt; alternative hypothesis: serial correlation in idiosyncratic errors in the case of short panels (small T and large n), we can use pwartest(log(emp) ~ log(wage) + log(capital), data=EmplUK) #&gt; #&gt; Wooldridge&#39;s test for serial correlation in FE panels #&gt; #&gt; data: plm.model #&gt; F = 312.3, df1 = 1, df2 = 889, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: serial correlation 12.4.3.5 Unit roots/stationarity Dickey-Fuller test for stochastic trends. Null hypothesis: the series is non-stationary (unit root) You would want your test to be less than the critical value (p&lt;.5) so that there is evidence there is not unit roots. 12.4.3.6 Heteroskedasticity Breusch-Pagan test Null hypothesis: the data is homoskedastic If there is evidence for heteroskedasticity, robust covariance matrix is advised. To control for heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator) “white1” - for general heteroskedasticity but no serial correlation (check serial correlation first). Recommended for random effects. “white2” - is “white1” restricted to a common variance within groups. Recommended for random effects. “arellano” - both heteroskedasticity and serial correlation. Recommended for fixed effects 12.4.4 Model Selection 12.4.4.1 POLS vs. RE The continuum between RE (used FGLS which more assumption ) and POLS check back on the section of FGLS Breusch-Pagan LM test Test for the random effect model based on the OLS residual Null hypothesis: variances across entities is zero. In another word, no panel effect. If the test is significant, RE is preferable compared to POLS 12.4.4.2 FE vs. RE RE does not require strict exogeneity for consistency (feedback effect between residual and covariates) Hypothesis If true \\(H_0: Cov(c_i,\\mathbf{x_{it}})=0\\) \\(\\hat{\\beta}_{RE}\\) is consistent and efficient, while \\(\\hat{\\beta}_{FE}\\) is consistent \\(H_0: Cov(c_i,\\mathbf{x_{it}}) \\neq 0\\) \\(\\hat{\\beta}_{RE}\\) is inconsistent, while \\(\\hat{\\beta}_{FE}\\) is consistent Hausman Test For the Hausman test to run, you need to assume that strict exogeneity hold A4 to hold for \\(u_{it}\\) Then, Hausman test statistic: \\(H=(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE})&#39;(V(\\hat{\\beta}_{RE})- V(\\hat{\\beta}_{FE}))(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE}) \\sim \\chi_{n(X)}^2\\) where \\(n(X)\\) is the number of parameters for the time-varying regressors. A low p-value means that we would reject the null hypothesis and prefer FE A high p-value means that we would not reject the null hypothesis and consider RE estimator. gw &lt;- plm(inv ~ value + capital, data = Grunfeld, model = &quot;within&quot;) gr &lt;- plm(inv ~ value + capital, data = Grunfeld, model = &quot;random&quot;) phtest(gw, gr) #&gt; #&gt; Hausman Test #&gt; #&gt; data: inv ~ value + capital #&gt; chisq = 2.3304, df = 2, p-value = 0.3119 #&gt; alternative hypothesis: one model is inconsistent Violation Estimator Basic Estimator Instrumental variable Estimator Variable Coefficients estimator Generalized Method of Moments estimator General FGLS estimator Means groups estimator CCEMG Estimator for limited dependent variables 12.4.5 Summary All three estimators (POLS, RE, FE) require A1, A2, A5 (for individuals) to be consistent. Additionally, POLS is consistent under A3a(for \\(u_{it}\\)): \\(E(\\mathbf{x}_{it}&#39;u_{it})=0\\), and RE Assumption \\(E(\\mathbf{x}_{it}&#39;c_{i})=0\\) If A4 does not hold, use cluster robust SE but POLS is not efficient RE is consistent under A3a(for \\(u_{it}\\)): \\(E(\\mathbf{x}_{it}&#39;u_{it})=0\\), and RE Assumption \\(E(\\mathbf{x}_{it}&#39;c_{i})=0\\) If A4 (for \\(u_{it}\\)) holds then usual SE are valid and RE is most efficient If A4 (for \\(u_{it}\\)) does not hold, use cluster robust SE ,and RE is no longer most efficient (but still more efficient than POLS) FE is consistent under A3 \\(E((\\mathbf{x}_{it}-\\bar{\\mathbf{x}}_{it})&#39;(u_{it} -\\bar{u}_{it}))=0\\) Cannot estimate effects of time constant variables A4 generally does not hold for \\(u_{it} -\\bar{u}_{it}\\) so cluster robust SE are needed Note: A5 for individual (not for time dimension) implies that you have A5a for the entire data set. Estimator / True Model POLS RE FE POLS Consistent Consistent Inconsistent FE Consistent Consistent Consistent RE Consistent Consistent Inconsistent Based on table provided by Ani Katchova 12.4.6 Application 12.4.6.1 plm package Recommended application of plm can be found here and here by Yves Croissant #install.packages(&quot;plm&quot;) library(&quot;plm&quot;) library(foreign) Panel &lt;- read.dta(&quot;http://dss.princeton.edu/training/Panel101.dta&quot;) attach(Panel) Y &lt;- cbind(y) X &lt;- cbind(x1, x2, x3) # Set data as panel data pdata &lt;- pdata.frame(Panel, index = c(&quot;country&quot;, &quot;year&quot;)) # Pooled OLS estimator pooling &lt;- plm(Y ~ X, data = pdata, model = &quot;pooling&quot;) summary(pooling) # Between estimator between &lt;- plm(Y ~ X, data = pdata, model = &quot;between&quot;) summary(between) # First differences estimator firstdiff &lt;- plm(Y ~ X, data = pdata, model = &quot;fd&quot;) summary(firstdiff) # Fixed effects or within estimator fixed &lt;- plm(Y ~ X, data = pdata, model = &quot;within&quot;) summary(fixed) # Random effects estimator random &lt;- plm(Y ~ X, data = pdata, model = &quot;random&quot;) summary(random) # LM test for random effects versus OLS # Accept Null, then OLS, Reject Null then RE plmtest(pooling, effect = &quot;individual&quot;, type = c(&quot;bp&quot;)) # other type: &quot;honda&quot;, &quot;kw&quot;,&quot; &quot;ghm&quot;; other effect : &quot;time&quot; &quot;twoways&quot; # B-P/LM and Pesaran CD (cross-sectional dependence) test # Breusch and Pagan&#39;s original LM statistic pcdtest(fixed, test = c(&quot;lm&quot;)) # Pesaran&#39;s CD statistic pcdtest(fixed, test = c(&quot;cd&quot;)) # Serial Correlation pbgtest(fixed) # stationary library(&quot;tseries&quot;) adf.test(pdata$y, k = 2) # LM test for fixed effects versus OLS pFtest(fixed, pooling) # Hausman test for fixed versus random effects model phtest(random, fixed) # Breusch-Pagan heteroskedasticity library(lmtest) bptest(y ~ x1 + factor(country), data = pdata) # If there is presence of heteroskedasticity ## For RE model coeftest(random) #orginal coef # Heteroskedasticity consistent coefficients coeftest(random, vcovHC) t(sapply(c(&quot;HC0&quot;, &quot;HC1&quot;, &quot;HC2&quot;, &quot;HC3&quot;, &quot;HC4&quot;), function(x) sqrt(diag( vcovHC(random, type = x) )))) #show HC SE of the coef # HC0 - heteroskedasticity consistent. The default. # HC1,HC2, HC3 – Recommended for small samples. # HC3 gives less weight to influential observations. # HC4 - small samples with influential observations # HAC - heteroskedasticity and autocorrelation consistent ## For FE model coeftest(fixed) # Original coefficients coeftest(fixed, vcovHC) # Heteroskedasticity consistent coefficients # Heteroskedasticity consistent coefficients (Arellano) coeftest(fixed, vcovHC(fixed, method = &quot;arellano&quot;)) t(sapply(c(&quot;HC0&quot;, &quot;HC1&quot;, &quot;HC2&quot;, &quot;HC3&quot;, &quot;HC4&quot;), function(x) sqrt(diag( vcovHC(fixed, type = x) )))) #show HC SE of the coef Advanced Other methods to estimate the random model: \"swar\": default (Swamy and Arora 1972) \"walhus\": (Wallace and Hussain 1969) \"amemiya\": (Amemiya 1971) \"nerlove\"” (Nerlove 1971) Other effects: Individual effects: default Time effects: \"time\" Individual and time effects: \"twoways\" Note: no random two-ways effect model for random.method = \"nerlove\" amemiya &lt;- plm( Y ~ X, data = pdata, model = &quot;random&quot;, random.method = &quot;amemiya&quot;, effect = &quot;twoways&quot; ) To call the estimation of the variance of the error components ercomp(Y ~ X, data = pdata, method = &quot;amemiya&quot;, effect = &quot;twoways&quot;) Check for the unbalancedness. Closer to 1 indicates balanced data (Ahrens and Pincus 1981) punbalancedness(random) Instrumental variable \"bvk\": default (Balestra and Varadharajan-Krishnakumar 1987) \"baltagi\": (Baltagi 1981) \"am\" (Amemiya and MaCurdy 1986) \"bms\": (Breusch, Mizon, and Schmidt 1989) instr &lt;- plm( Y ~ X | X_ins, data = pdata, random.method = &quot;ht&quot;, model = &quot;random&quot;, inst.method = &quot;baltagi&quot; ) 12.4.6.1.1 Other Estimators 12.4.6.1.1.1 Variable Coefficients Model fixed_pvcm &lt;- pvcm(Y ~ X, data = pdata, model = &quot;within&quot;) random_pvcm &lt;- pvcm(Y ~ X, data = pdata, model = &quot;random&quot;) More details can be found here 12.4.6.1.1.2 Generalized Method of Moments Estimator Typically use in dynamic models. Example is from plm package z2 &lt;- pgmm( log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + lag(log(capital), 0:1) | lag(log(emp), 2:99) + lag(log(wage), 2:99) + lag(log(capital), 2:99), data = EmplUK, effect = &quot;twoways&quot;, model = &quot;onestep&quot;, transformation = &quot;ld&quot; ) summary(z2, robust = TRUE) 12.4.6.1.1.3 General Feasible Generalized Least Squares Models Assume there is no cross-sectional correlation Robust against intragroup heteroskedasticity and serial correlation. Suited when n is much larger than T (long panel) However, inefficient under group-wise heteorskedasticity. # Random Effects zz &lt;- pggls(log(emp) ~ log(wage) + log(capital), data = EmplUK, model = &quot;pooling&quot;) # Fixed zz &lt;- pggls(log(emp) ~ log(wage) + log(capital), data = EmplUK, model = &quot;within&quot;) 12.4.6.2 fixest package Available functions feols: linear models feglm: generalized linear models femlm: maximum likelihood estimation feNmlm: non-linear in RHS parameters fepois: Poisson fixed-effect fenegbin: negative binomial fixed-effect Notes can only work for fixest object Examples by the package’s authors library(fixest) data(airquality) # Setting a dictionary setFixest_dict( c( Ozone = &quot;Ozone (ppb)&quot;, Solar.R = &quot;Solar Radiation (Langleys)&quot;, Wind = &quot;Wind Speed (mph)&quot;, Temp = &quot;Temperature&quot; ) ) # On multiple estimations: see the dedicated vignette est = feols( Ozone ~ Solar.R + sw0(Wind + Temp) | csw(Month, Day), data = airquality, cluster = ~ Day ) etable(est) #&gt; est.1 est.2 #&gt; Dependent Var.: Ozone (ppb) Ozone (ppb) #&gt; #&gt; Solar Radiation (Langleys) 0.1148*** (0.0234) 0.0522* (0.0202) #&gt; Wind Speed (mph) -3.109*** (0.7986) #&gt; Temperature 1.875*** (0.3671) #&gt; Fixed-Effects: ------------------ ------------------ #&gt; Month Yes Yes #&gt; Day No No #&gt; __________________________ __________________ __________________ #&gt; S.E.: Clustered by: Day by: Day #&gt; Observations 111 111 #&gt; R2 0.31974 0.63686 #&gt; Within R2 0.12245 0.53154 #&gt; #&gt; est.3 est.4 #&gt; Dependent Var.: Ozone (ppb) Ozone (ppb) #&gt; #&gt; Solar Radiation (Langleys) 0.1078** (0.0329) 0.0509* (0.0236) #&gt; Wind Speed (mph) -3.289*** (0.7777) #&gt; Temperature 2.052*** (0.2415) #&gt; Fixed-Effects: ----------------- ------------------ #&gt; Month Yes Yes #&gt; Day Yes Yes #&gt; __________________________ _________________ __________________ #&gt; S.E.: Clustered by: Day by: Day #&gt; Observations 111 111 #&gt; R2 0.58018 0.81604 #&gt; Within R2 0.12074 0.61471 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # in latex etable(est, tex = T) #&gt; \\begingroup #&gt; \\centering #&gt; \\begin{tabular}{lcccc} #&gt; \\tabularnewline \\midrule \\midrule #&gt; Dependent Variable: &amp; \\multicolumn{4}{c}{Ozone (ppb)}\\\\ #&gt; Model: &amp; (1) &amp; (2) &amp; (3) &amp; (4)\\\\ #&gt; \\midrule #&gt; \\emph{Variables}\\\\ #&gt; Solar Radiation (Langleys) &amp; 0.1148$^{***}$ &amp; 0.0522$^{**}$ &amp; 0.1078$^{***}$ &amp; 0.0509$^{**}$\\\\ #&gt; &amp; (0.0234) &amp; (0.0202) &amp; (0.0329) &amp; (0.0236)\\\\ #&gt; Wind Speed (mph) &amp; &amp; -3.109$^{***}$ &amp; &amp; -3.289$^{***}$\\\\ #&gt; &amp; &amp; (0.7986) &amp; &amp; (0.7777)\\\\ #&gt; Temperature &amp; &amp; 1.875$^{***}$ &amp; &amp; 2.052$^{***}$\\\\ #&gt; &amp; &amp; (0.3671) &amp; &amp; (0.2415)\\\\ #&gt; \\midrule #&gt; \\emph{Fixed-effects}\\\\ #&gt; Month &amp; Yes &amp; Yes &amp; Yes &amp; Yes\\\\ #&gt; Day &amp; &amp; &amp; Yes &amp; Yes\\\\ #&gt; \\midrule #&gt; \\emph{Fit statistics}\\\\ #&gt; Observations &amp; 111 &amp; 111 &amp; 111 &amp; 111\\\\ #&gt; R$^2$ &amp; 0.31974 &amp; 0.63686 &amp; 0.58018 &amp; 0.81604\\\\ #&gt; Within R$^2$ &amp; 0.12245 &amp; 0.53154 &amp; 0.12074 &amp; 0.61471\\\\ #&gt; \\midrule \\midrule #&gt; \\multicolumn{5}{l}{\\emph{Clustered (Day) standard-errors in parentheses}}\\\\ #&gt; \\multicolumn{5}{l}{\\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\\\ #&gt; \\end{tabular} #&gt; \\par\\endgroup # get the fixed-effects coefficients for 1 model fixedEffects = fixef(est[[1]]) summary(fixedEffects) #&gt; Fixed_effects coefficients #&gt; Number of fixed-effects for variable Month is 5. #&gt; Mean = 19.6 Variance = 272 #&gt; #&gt; COEFFICIENTS: #&gt; Month: 5 6 7 8 9 #&gt; 3.219 8.288 34.26 40.12 12.13 # see the fixed effects for one dimension fixedEffects$Month #&gt; 5 6 7 8 9 #&gt; 3.218876 8.287899 34.260812 40.122257 12.130971 plot(fixedEffects) For multiple estimation # set up library(fixest) # let R know the base dataset (the biggest/ultimate # dataset that includes everything in your analysis) base = iris # rename variables names(base) = c(&quot;y1&quot;, &quot;y2&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;species&quot;) res_multi = feols( c(y1, y2) ~ x1 + csw(x2, x2 ^ 2) | sw0(species), data = base, fsplit = ~ species, lean = TRUE, vcov = &quot;hc1&quot; # can also clustered at the fixed effect level ) # it&#39;s recommended to use vcov at # estimation stage, not summary stage summary(res_multi, &quot;compact&quot;) #&gt; sample fixef lhs rhs (Intercept) x1 #&gt; 1 Full sample 1 y1 x1 + x2 4.19*** (0.104) 0.542*** (0.076) #&gt; 2 Full sample 1 y1 x1 + x2 + I(x2^2) 4.27*** (0.101) 0.719*** (0.082) #&gt; 3 Full sample 1 y2 x1 + x2 3.59*** (0.103) -0.257*** (0.066) #&gt; 4 Full sample 1 y2 x1 + x2 + I(x2^2) 3.68*** (0.097) -0.030 (0.078) #&gt; 5 Full sample species y1 x1 + x2 0.906*** (0.076) #&gt; 6 Full sample species y1 x1 + x2 + I(x2^2) 0.900*** (0.077) #&gt; 7 Full sample species y2 x1 + x2 0.155* (0.073) #&gt; 8 Full sample species y2 x1 + x2 + I(x2^2) 0.148. (0.075) #&gt; 9 setosa 1 y1 x1 + x2 4.25*** (0.474) 0.399 (0.325) #&gt; 10 setosa 1 y1 x1 + x2 + I(x2^2) 4.00*** (0.504) 0.405 (0.325) #&gt; 11 setosa 1 y2 x1 + x2 2.89*** (0.416) 0.247 (0.305) #&gt; 12 setosa 1 y2 x1 + x2 + I(x2^2) 2.82*** (0.423) 0.248 (0.304) #&gt; 13 setosa species y1 x1 + x2 0.399 (0.325) #&gt; 14 setosa species y1 x1 + x2 + I(x2^2) 0.405 (0.325) #&gt; 15 setosa species y2 x1 + x2 0.247 (0.305) #&gt; 16 setosa species y2 x1 + x2 + I(x2^2) 0.248 (0.304) #&gt; 17 versicolor 1 y1 x1 + x2 2.38*** (0.423) 0.934*** (0.166) #&gt; 18 versicolor 1 y1 x1 + x2 + I(x2^2) 0.323 (1.44) 0.901*** (0.164) #&gt; 19 versicolor 1 y2 x1 + x2 1.25*** (0.275) 0.067 (0.095) #&gt; 20 versicolor 1 y2 x1 + x2 + I(x2^2) 0.097 (1.01) 0.048 (0.099) #&gt; 21 versicolor species y1 x1 + x2 0.934*** (0.166) #&gt; 22 versicolor species y1 x1 + x2 + I(x2^2) 0.901*** (0.164) #&gt; 23 versicolor species y2 x1 + x2 0.067 (0.095) #&gt; 24 versicolor species y2 x1 + x2 + I(x2^2) 0.048 (0.099) #&gt; 25 virginica 1 y1 x1 + x2 1.05. (0.539) 0.995*** (0.090) #&gt; 26 virginica 1 y1 x1 + x2 + I(x2^2) -2.39 (2.04) 0.994*** (0.088) #&gt; 27 virginica 1 y2 x1 + x2 1.06. (0.572) 0.149 (0.107) #&gt; 28 virginica 1 y2 x1 + x2 + I(x2^2) 1.10 (1.76) 0.149 (0.108) #&gt; 29 virginica species y1 x1 + x2 0.995*** (0.090) #&gt; 30 virginica species y1 x1 + x2 + I(x2^2) 0.994*** (0.088) #&gt; 31 virginica species y2 x1 + x2 0.149 (0.107) #&gt; 32 virginica species y2 x1 + x2 + I(x2^2) 0.149 (0.108) #&gt; x2 I(x2^2) #&gt; 1 -0.320. (0.170) #&gt; 2 -1.52*** (0.307) 0.348*** (0.075) #&gt; 3 0.364* (0.142) #&gt; 4 -1.18*** (0.313) 0.446*** (0.074) #&gt; 5 -0.006 (0.163) #&gt; 6 0.290 (0.408) -0.088 (0.117) #&gt; 7 0.623*** (0.114) #&gt; 8 0.951* (0.472) -0.097 (0.125) #&gt; 9 0.712. (0.418) #&gt; 10 2.51. (1.47) -2.91 (2.10) #&gt; 11 0.702 (0.560) #&gt; 12 1.27 (2.39) -0.911 (3.28) #&gt; 13 0.712. (0.418) #&gt; 14 2.51. (1.47) -2.91 (2.10) #&gt; 15 0.702 (0.560) #&gt; 16 1.27 (2.39) -0.911 (3.28) #&gt; 17 -0.320 (0.364) #&gt; 18 3.01 (2.31) -1.24 (0.841) #&gt; 19 0.929*** (0.244) #&gt; 20 2.80. (1.65) -0.695 (0.583) #&gt; 21 -0.320 (0.364) #&gt; 22 3.01 (2.31) -1.24 (0.841) #&gt; 23 0.929*** (0.244) #&gt; 24 2.80. (1.65) -0.695 (0.583) #&gt; 25 0.007 (0.205) #&gt; 26 3.50. (2.09) -0.870 (0.519) #&gt; 27 0.535*** (0.122) #&gt; 28 0.503 (1.56) 0.008 (0.388) #&gt; 29 0.007 (0.205) #&gt; 30 3.50. (2.09) -0.870 (0.519) #&gt; 31 0.535*** (0.122) #&gt; 32 0.503 (1.56) 0.008 (0.388) # call the first 3 estimated models only etable(res_multi[1:3], # customize the headers headers = c(&quot;mod1&quot;, &quot;mod2&quot;, &quot;mod3&quot;)) #&gt; res_multi[1:3].1 res_multi[1:3].2 res_multi[1:3].3 #&gt; mod1 mod2 mod3 #&gt; Dependent Var.: y1 y1 y2 #&gt; #&gt; Constant 4.191*** (0.1037) 4.266*** (0.1007) 3.587*** (0.1031) #&gt; x1 0.5418*** (0.0761) 0.7189*** (0.0815) -0.2571*** (0.0664) #&gt; x2 -0.3196. (0.1700) -1.522*** (0.3072) 0.3640* (0.1419) #&gt; x2 square 0.3479*** (0.0748) #&gt; _______________ __________________ __________________ ___________________ #&gt; S.E. type Heteroskedas.-rob. Heteroskedas.-rob. Heteroskedast.-rob. #&gt; Observations 150 150 150 #&gt; R2 0.76626 0.79456 0.21310 #&gt; Adj. R2 0.76308 0.79034 0.20240 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 12.4.6.2.1 Multiple estimation (Left-hand side) When you have multiple interested dependent variables etable(feols(c(y1, y2) ~ x1 + x2, base)) #&gt; feols(c(y1, y2)..1 feols(c(y1, y2) ..2 #&gt; Dependent Var.: y1 y2 #&gt; #&gt; Constant 4.191*** (0.0970) 3.587*** (0.0937) #&gt; x1 0.5418*** (0.0693) -0.2571*** (0.0669) #&gt; x2 -0.3196* (0.1605) 0.3640* (0.1550) #&gt; _______________ __________________ ___________________ #&gt; S.E. type IID IID #&gt; Observations 150 150 #&gt; R2 0.76626 0.21310 #&gt; Adj. R2 0.76308 0.20240 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To input a list of dependent variable depvars &lt;- c(&quot;y1&quot;, &quot;y2&quot;) res &lt;- lapply(depvars, function(var) { res &lt;- feols(xpd(..lhs ~ x1 + x2, ..lhs = var), data = base) # summary(res) }) etable(res) #&gt; model 1 model 2 #&gt; Dependent Var.: y1 y2 #&gt; #&gt; Constant 4.191*** (0.0970) 3.587*** (0.0937) #&gt; x1 0.5418*** (0.0693) -0.2571*** (0.0669) #&gt; x2 -0.3196* (0.1605) 0.3640* (0.1550) #&gt; _______________ __________________ ___________________ #&gt; S.E. type IID IID #&gt; Observations 150 150 #&gt; R2 0.76626 0.21310 #&gt; Adj. R2 0.76308 0.20240 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 12.4.6.2.2 Multiple estimation (Right-hand side) Options to write the functions sw (stepwise): sequentially analyze each elements y ~ sw(x1, x2) will be estimated as y ~ x1 and y ~ x2 sw0 (stepwise 0): similar to sw but also estimate a model without the elements in the set first y ~ sw(x1, x2) will be estimated as y ~ 1 and y ~ x1 and y ~ x2 csw (cumulative stepwise): sequentially add each element of the set to the formula y ~ csw(x1, x2) will be estimated as y ~ x1 and y ~ x1 + x2 csw0 (cumulative stepwise 0): similar to csw but also estimate a model without the elements in the set first y ~ csw(x1, x2) will be estimated as y~ 1 y ~ x1 and y ~ x1 + x2 mvsw (multiverse stepwise): all possible combination of the elements in the set (it will get large very quick). mvsw(x1, x2, x3) will be sw0(x1, x2, x3, x1 + x2, x1 + x3, x2 + x3, x1 + x2 + x3) 12.4.6.2.3 Split sample estimation etable(feols(y1 ~ x1 + x2, fsplit = ~ species, data = base)) #&gt; feols(y1 ~ x1 +..1 feols(y1 ~ x1 ..2 feols(y1 ~ x1 +..3 #&gt; Sample (species) Full sample setosa versicolor #&gt; Dependent Var.: y1 y1 y1 #&gt; #&gt; Constant 4.191*** (0.0970) 4.248*** (0.4114) 2.381*** (0.4493) #&gt; x1 0.5418*** (0.0693) 0.3990 (0.2958) 0.9342*** (0.1693) #&gt; x2 -0.3196* (0.1605) 0.7121 (0.4874) -0.3200 (0.4024) #&gt; ________________ __________________ _________________ __________________ #&gt; S.E. type IID IID IID #&gt; Observations 150 50 50 #&gt; R2 0.76626 0.11173 0.57432 #&gt; Adj. R2 0.76308 0.07393 0.55620 #&gt; #&gt; feols(y1 ~ x1 +..4 #&gt; Sample (species) virginica #&gt; Dependent Var.: y1 #&gt; #&gt; Constant 1.052* (0.5139) #&gt; x1 0.9946*** (0.0893) #&gt; x2 0.0071 (0.1795) #&gt; ________________ __________________ #&gt; S.E. type IID #&gt; Observations 50 #&gt; R2 0.74689 #&gt; Adj. R2 0.73612 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 12.4.6.2.4 Standard Errors iid: errors are homoskedastic and independent and identically distributed hetero: errors are heteroskedastic using White correction cluster: errors are correlated within the cluster groups newey_west: (Newey and West 1986) use for time series or panel data. Errors are heteroskedastic and serially correlated. vcov = newey_west ~ id + period where id is the subject id and period is time period of the panel. to specify lag period to consider vcov = newey_west(2) ~ id + period where we’re considering 2 lag periods. driscoll_kraay (Driscoll and Kraay 1998) use for panel data. Errors are cross-sectionally and serially correlated. vcov = discoll_kraay ~ period conley: (Conley 1999) for cross-section data. Errors are spatially correlated vcov = conley ~ latitude + longitude to specify the distance cutoff, vcov = vcov_conley(lat = \"lat\", lon = \"long\", cutoff = 100, distance = \"spherical\"), which will use the conley() helper function. hc: from the sandwich package vcov = function(x) sandwich::vcovHC(x, type = \"HC1\")) To let R know which SE estimation you want to use, insert vcov = vcov_type ~ variables 12.4.6.2.5 Small sample correction To specify that R needs to use small sample correction add ssc = ssc(adj = T, cluster.adj = T) References "],["variable-transformation.html", "Chapter 13 Variable Transformation", " Chapter 13 Variable Transformation trafo vignette "],["continuous-variables.html", "13.1 Continuous Variables", " 13.1 Continuous Variables Purposes: To change the scale of the variables To transform skewed data distribution to normal distribution 13.1.1 Standardization \\[ x_i&#39; = \\frac{x_i - \\bar{x}}{s} \\] when you have a few large numbers 13.1.2 Min-max scaling \\[ x_i&#39; = \\frac{x_i - x_{max}}{x_{max} - x_{min}} \\] dependent on the min and max values, which makes it sensitive to outliers. best to use when you have values in a fixed interval. 13.1.3 Square Root/Cube Root When variables have positive skewness or residuals have positive heteroskasticity. Frequency counts variable Data have many 0 or extremely small values. 13.1.4 Logarithmic Variables have positively skewed distribution Formula In case \\(x_i&#39; = \\log(x_i)\\) cannot work zero because log(0) = -Inf \\(x_i&#39; = \\log(x_i + 1)\\) variables with 0 \\(x_i&#39; = \\log(x_i +c)\\) \\(x_i&#39; = \\frac{x_i}{|x_i|}\\log|x_i|\\) variables with negative values \\(x_i&#39;^\\lambda = \\log(x_i + \\sqrt{x_i^2 + \\lambda})\\) generalized log transformation For the general case of \\(\\log(x_i + c)\\), choosing a constant is rather tricky. The choice of the constant is critically important, especially when you want to do inference. It can dramatically change your model fit (see (Ekwaru and Veugelers 2018) for the independent variable case). J. Chen and Roth (2023) show that in causal inference problem, \\(\\log\\) transformation of values with meaningful 0 is problematic. But there are solutions for each approach (e.g., DID, IV). However, assuming that you do not have 0s because of Censoring No measurement errors (stemming from measurement tools) We can proceed choosing c (it’s okay if your 0’s are represent really small values). data(cars) cars$speed %&gt;% head() #&gt; [1] 4 4 7 7 8 9 log(cars$speed) %&gt;% head() #&gt; [1] 1.386294 1.386294 1.945910 1.945910 2.079442 2.197225 # log(x+1) log1p(cars$speed) %&gt;% head() #&gt; [1] 1.609438 1.609438 2.079442 2.079442 2.197225 2.302585 13.1.5 Exponential Negatively skewed data Underlying logarithmic trend (e.g., survival, decay) 13.1.6 Power Variables have negatively skewed distribution 13.1.7 Inverse/Reciprocal Variables have platykurtic distribution Data are positively skewed Ratio data data(cars) head(cars$dist) #&gt; [1] 2 10 4 22 16 10 plot(cars$dist) plot(1/(cars$dist)) 13.1.8 Hyperbolic arcsine Variables with positively skewed distribution 13.1.9 Ordered Quantile Norm (Bartlett 1947) \\[ x_i&#39; = \\Phi^{-1} (\\frac{rank(x_i) - 1/2}{length(x)}) \\] ord_dist &lt;- bestNormalize::orderNorm(cars$dist) ord_dist #&gt; orderNorm Transformation with 50 nonmissing obs and ties #&gt; - 35 unique values #&gt; - Original quantiles: #&gt; 0% 25% 50% 75% 100% #&gt; 2 26 36 56 120 ord_dist$x.t %&gt;% hist() 13.1.10 Arcsinh Proportion variable (0-1) cars$dist %&gt;% hist() # cars$dist %&gt;% MASS::truehist() as_dist &lt;- bestNormalize::arcsinh_x(cars$dist) as_dist #&gt; Standardized asinh(x) Transformation with 50 nonmissing obs.: #&gt; Relevant statistics: #&gt; - mean (before standardization) = 4.230843 #&gt; - sd (before standardization) = 0.7710887 as_dist$x.t %&gt;% hist() \\[ arcsinh(Y) = \\log(\\sqrt{1 + Y^2} + Y) \\] Paper Interpretation Azoulay, Fons-Rosen, and Zivin (2019) Elasticity Faber and Gaubert (2019) Percentage Hjort and Poulsen (2019) Percentage M. S. Johnson (2020) Percentage Beerli et al. (2021) Percentage Norris, Pecenco, and Weaver (2021) Percentage Berkouwer and Dean (2022) Percentage Cabral, Cui, and Dworsky (2022) Elasticity Carranza et al. (2022) Percentage Mirenda, Mocetti, and Rizzica (2022) Percentage For a simple regression model, \\(Y = \\beta X\\) When both \\(Y\\) and \\(X\\) are transformed, the coefficient estimate represents elasticity, indicating the percentage change in \\(Y\\) for a 1% change in \\(X\\). When only \\(Y\\) is in transformed and \\(X\\) is in raw form, the coefficient estimate represents the percentage change in \\(Y\\) for a one-unit change in \\(X\\). 13.1.11 Lambert W x F Transformation LambertW package Using moments to normalize data. Usually need to compare with the Box-Cox Transformation and Yeo-Johnson Transformation Can handle skewness, heavy-tailed. data(cars) head(cars$dist) #&gt; [1] 2 10 4 22 16 10 cars$dist %&gt;% hist() l_dist &lt;- LambertW::Gaussianize(cars$dist) # small fix l_dist %&gt;% hist() 13.1.12 Inverse Hyperbolic Sine (IHS) transformation Proposed by (N. L. Johnson 1949) Can be applied to real numbers. \\[ \\begin{aligned} f(x,\\theta) &amp;= \\frac{\\sinh^{-1} (\\theta x)}{\\theta} \\\\ &amp;= \\frac{\\log(\\theta x + (\\theta^2 x^2 + 1)^{1/2})}{\\theta} \\end{aligned} \\] 13.1.13 Box-Cox Transformation \\[ y^\\lambda = \\beta x+ \\epsilon \\] to fix non-linearity in the error terms work well between (-3,3) (i.e., small transformation). or with independent variables \\[ x_i&#39;^\\lambda = \\begin{cases} \\frac{x_i^\\lambda-1}{\\lambda} &amp; \\text{if } \\lambda \\neq 0\\\\ \\log(x_i) &amp; \\text{if } \\lambda = 0 \\end{cases} \\] And the two-parameter version is \\[ x_i&#39; (\\lambda_1, \\lambda_2) = \\begin{cases} \\frac{(x_i + \\lambda_2)^{\\lambda_1}-1}{} &amp; \\text{if } \\lambda_1 \\neq 0 \\\\ \\log(x_i + \\lambda_2) &amp; \\text{if } \\lambda_1 = 0 \\end{cases} \\] More advances (Manly 1976) (Bickel and Doksum 1981; Box and Cox 1981) library(MASS) data(cars) mod &lt;- lm(cars$speed ~ cars$dist, data = cars) # check residuals plot(mod) bc &lt;- boxcox(mod, lambda = seq(-3, 3)) # best lambda bc$x[which(bc$y == max(bc$y))] #&gt; [1] 1.242424 # model with best lambda mod_lambda = lm(cars$speed ^ (bc$x[which(bc$y == max(bc$y))]) ~ cars$dist, data = cars) plot(mod_lambda) # 2-parameter version two_bc = geoR::boxcoxfit(cars$speed) two_bc #&gt; Fitted parameters: #&gt; lambda beta sigmasq #&gt; 1.028798 15.253008 31.935297 #&gt; #&gt; Convergence code returned by optim: 0 plot(two_bc) # bestNormalize bc_dist &lt;- bestNormalize::boxcox(cars$dist) bc_dist #&gt; Standardized Box Cox Transformation with 50 nonmissing obs.: #&gt; Estimated statistics: #&gt; - lambda = 0.4950628 #&gt; - mean (before standardization) = 10.35636 #&gt; - sd (before standardization) = 3.978036 bc_dist$x.t %&gt;% hist() 13.1.14 Yeo-Johnson Transformation Similar to Box-Cox Transformation (when \\(\\lambda = 1\\)), but allows for negative value \\[ x_i&#39;^\\lambda = \\begin{cases} \\frac{(x_i+1)^\\lambda -1}{\\lambda} &amp; \\text{if } \\lambda \\neq0, x_i \\ge 0 \\\\ \\log(x_i + 1) &amp; \\text{if } \\lambda = 0, x_i \\ge 0 \\\\ \\frac{-[(-x_i+1)^{2-\\lambda}-1]}{2 - \\lambda} &amp; \\text{if } \\lambda \\neq 2, x_i &lt;0 \\\\ -\\log(-x_i + 1) &amp; \\text{if } \\lambda = 2, x_i &lt;0 \\end{cases} \\] data(cars) yj_speed &lt;- bestNormalize::yeojohnson(cars$speed) yj_speed$x.t %&gt;% hist() 13.1.15 RankGauss Turn values into ranks, then ranks to values under normal distribution. 13.1.16 Summary Automatically choose the best method to normalize data (code by bestNormalize) bestdist &lt;- bestNormalize::bestNormalize(cars$dist) bestdist$x.t %&gt;% hist() boxplot(log10(bestdist$oos_preds), yaxt = &quot;n&quot;) # axis(2, at = log10(c(.1, .5, 1, 2, 5, 10)), # labels = c(.1, .5, 1, 2, 5, 10)) References "],["categorical-variables.html", "13.2 Categorical Variables", " 13.2 Categorical Variables Purposes To transform to continuous variable (for machine learning models) (e.g., encoding/ embedding in text mining) Approaches: One-hot encoding Label encoding Feature hashing Binary encoding Base N encoding Frequency encoding Target encoding Ordinal encoding Helmert encoding Mean encoding Weight of evidence encoding Probability ratio encoding Backward difference encoding Leave one out encoding James-Stein encoding M-estimator encoding Thermometer encoding "],["hypothesis-testing.html", "Chapter 14 Hypothesis Testing", " Chapter 14 Hypothesis Testing Error types: Type I Error (False Positive): Reality: nope Diagnosis/Analysis: yes Type II Error (False Negative): Reality: yes Diagnosis/Analysis: nope Power: The probability of rejecting the null hypothesis when it is actually false Note: Always written in terms of the population parameter (\\(\\beta\\)) not the estimator/estimate (\\(\\hat{\\beta}\\)) Sometimes, different disciplines prefer to use \\(\\beta\\) (i.e., standardized coefficient), or \\(\\mathbf{b}\\) (i.e., unstandardized coefficient) \\(\\beta\\) and \\(\\mathbf{b}\\) are similar in interpretation; however, \\(\\beta\\) is scale free. Hence, you can see the relative contribution of \\(\\beta\\) to the dependent variable. On the other hand, \\(\\mathbf{b}\\) can be more easily used in policy decisions. \\[ \\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y} \\] Assuming the null hypothesis is true, what is the (asymptotic) distribution of the estimator Two-sided \\[ \\begin{aligned} &amp;H_0: \\beta_j = 0 \\\\ &amp;H_1: \\beta_j \\neq 0 \\end{aligned} \\] then under the null, the OLS estimator has the following distribution \\[ A1-A3a, A5: \\sqrt{n} \\hat{\\beta_j} \\sim N(0,Avar(\\sqrt{n}\\hat{\\beta}_j)) \\] For the one-sided test, the null is a set of values, so now you choose the worst case single value that is hardest to prove and derive the distribution under the null One-sided \\[ \\begin{aligned} &amp;H_0: \\beta_j\\ge 0 \\\\ &amp;H_1: \\beta_j &lt; 0 \\end{aligned} \\] then the hardest null value to prove is \\(H_0: \\beta_j=0\\). Then under this specific null, the OLS estimator has the following asymptotic distribution \\[ A1-A3a, A5: \\sqrt{n}\\hat{\\beta_j} \\sim N(0,Avar(\\sqrt{n}\\hat{\\beta}_j)) \\] "],["types-of-hypothesis-testing.html", "14.1 Types of hypothesis testing", " 14.1 Types of hypothesis testing \\(H_0 : \\theta = \\theta_0\\) \\(H_1 : \\theta \\neq \\theta_0\\) How far away / extreme \\(\\theta\\) can be if our null hypothesis is true Assume that our likelihood function for q is \\(L(q) = q^{30}(1-q)^{70}\\) Likelihood function q = seq(0, 1, length = 100) L = function(q) { q ^ 30 * (1 - q) ^ 70 } plot(q, L(q), ylab = &quot;L(q)&quot;, xlab = &quot;q&quot;, type = &quot;l&quot;) Log-Likelihood function q = seq(0, 1, length = 100) l = function(q) { 30 * log(q) + 70 * log(1 - q) } plot(q, l(q) - l(0.3), ylab = &quot;l(q) - l(qhat)&quot;, xlab = &quot;q&quot;, type = &quot;l&quot;) abline(v = 0.2) Figure from(Fox 1997) typically, The likelihood ratio test (and Lagrange Multiplier (Score)) performs better with small to moderate sample sizes, but the Wald test only requires one maximization (under the full model). References "],["wald-test.html", "14.2 Wald test", " 14.2 Wald test \\[ \\begin{aligned} W &amp;= (\\hat{\\theta}-\\theta_0)&#39;[cov(\\hat{\\theta})]^{-1}(\\hat{\\theta}-\\theta_0) \\\\ W &amp;\\sim \\chi_q^2 \\end{aligned} \\] where \\(cov(\\hat{\\theta})\\) is given by the inverse Fisher Information matrix evaluated at \\(\\hat{\\theta}\\) and q is the rank of \\(cov(\\hat{\\theta})\\), which is the number of non-redundant parameters in \\(\\theta\\) Alternatively, \\[ t_W=\\frac{(\\hat{\\theta}-\\theta_0)^2}{I(\\theta_0)^{-1}} \\sim \\chi^2_{(v)} \\] where v is the degree of freedom. Equivalently, \\[ s_W= \\frac{\\hat{\\theta}-\\theta_0}{\\sqrt{I(\\hat{\\theta})^{-1}}} \\sim Z \\] How far away in the distribution your sample estimate is from the hypothesized population parameter. For a null value, what is the probability you would have obtained a realization “more extreme” or “worse” than the estimate you actually obtained? Significance Level (\\(\\alpha\\)) and Confidence Level (\\(1-\\alpha\\)) The significance level is the benchmark in which the probability is so low that we would have to reject the null The confidence level is the probability that sets the bounds on how far away the realization of the estimator would have to be to reject the null. Test Statistics Standardized (transform) the estimator and null value to a test statistic that always has the same distribution Test Statistic for the OLS estimator for a single hypothesis \\[ T = \\frac{\\sqrt{n}(\\hat{\\beta}_j-\\beta_{j0})}{\\sqrt{n}SE(\\hat{\\beta_j})} \\sim^a N(0,1) \\] Equivalently, \\[ T = \\frac{(\\hat{\\beta}_j-\\beta_{j0})}{SE(\\hat{\\beta_j})} \\sim^a N(0,1) \\] the test statistic is another random variable that is a function of the data and null hypothesis. T denotes the random variable test statistic t denotes the single realization of the test statistic Evaluating Test Statistic: determine whether or not we reject or fail to reject the null hypothesis at a given significance / confidence level Three equivalent ways Critical Value P-value Confidence Interval Critical Value For a given significance level, will determine the critical value \\((c)\\) One-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\) \\[ P(T&lt;c|H_0)=\\alpha \\] Reject the null if \\(t&lt;c\\) One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\) \\[ P(T&gt;c|H_0)=\\alpha \\] Reject the null if \\(t&gt;c\\) Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\) \\[ P(|T|&gt;c|H_0)=\\alpha \\] Reject the null if \\(|t|&gt;c\\) p-value Calculate the probability that the test statistic was worse than the realization you have One-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\) \\[ \\text{p-value} = P(T&lt;t|H_0) \\] One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\) \\[ \\text{p-value} = P(T&gt;t|H_0) \\] Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\) \\[ \\text{p-value} = P(|T|&lt;t|H_0) \\] reject the null if p-value \\(&lt; \\alpha\\) Confidence Interval Using the critical value associated with a null hypothesis and significance level, create an interval \\[ CI(\\hat{\\beta}_j)_{\\alpha} = [\\hat{\\beta}_j-(c \\times SE(\\hat{\\beta}_j)),\\hat{\\beta}_j+(c \\times SE(\\hat{\\beta}_j))] \\] If the null set lies outside the interval then we reject the null. We are not testing whether the true population value is close to the estimate, we are testing that given a field true population value of the parameter, how like it is that we observed this estimate. Can be interpreted as we believe with \\((1-\\alpha)\\times 100 \\%\\) probability that the confidence interval captures the true parameter value. With stronger assumption (A1-A6), we could consider Finite Sample Properties \\[ T = \\frac{\\hat{\\beta}_j-\\beta_{j0}}{SE(\\hat{\\beta}_j)} \\sim T(n-k) \\] This above distributional derivation is strongly dependent on A4 and A5 T has a student t-distribution because the numerator is normal and the denominator is \\(\\chi^2\\). Critical value and p-values will be calculated from the student t-distribution rather than the standard normal distribution. \\(n \\to \\infty\\), \\(T(n-k)\\) is asymptotically standard normal. Rule of thumb if \\(n-k&gt;120\\): the critical values and p-values from the t-distribution are (almost) the same as the critical values and p-values from the standard normal distribution. if \\(n-k&lt;120\\) if (A1-A6) hold then the t-test is an exact finite distribution test if (A1-A3a, A5) hold, because the t-distribution is asymptotically normal, computing the critical values from a t-distribution is still a valid asymptotic test (i.e., not quite the right critical values and p0values, the difference goes away as \\(n \\to \\infty\\)) 14.2.1 Multiple Hypothesis test multiple parameters as the same time \\(H_0: \\beta_1 = 0\\ \\&amp; \\ \\beta_2 = 0\\) \\(H_0: \\beta_1 = 1\\ \\&amp; \\ \\beta_2 = 0\\) perform a series of simply hypothesis does not answer the question (joint distribution vs. two marginal distributions). The test statistic is based on a restriction written in matrix form. \\[ y=\\beta_0+x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\epsilon \\] Null hypothesis is \\(H_0: \\beta_1 = 0\\) &amp; \\(\\beta_2=0\\) can be rewritten as \\(H_0: \\mathbf{R}\\beta -\\mathbf{q}=0\\) where \\(\\mathbf{R}\\) is a \\(m \\times k\\) matrix where m is the number of restrictions and \\(k\\) is the number of parameters. \\(\\mathbf{q}\\) is a \\(k \\times 1\\) vector \\(\\mathbf{R}\\) “picks up” the relevant parameters while \\(\\mathbf{q}\\) is a the null value of the parameter \\[ \\mathbf{R}= \\left( \\begin{array}{cccc} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{array} \\right), \\mathbf{q} = \\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right) \\] Test Statistic for OLS estimator for a multiple hypothesis \\[ F = \\frac{(\\mathbf{R\\hat{\\beta}-q})\\hat{\\Sigma}^{-1}(\\mathbf{R\\hat{\\beta}-q})}{m} \\sim^a F(m,n-k) \\] \\(\\hat{\\Sigma}^{-1}\\) is the estimator for the asymptotic variance-covariance matrix if A4 holds, both the homoskedastic and heteroskedastic versions produce valid estimator If A4 does not hold, only the heteroskedastic version produces valid estimators. When \\(m = 1\\), there is only a single restriction, then the \\(F\\)-statistic is the \\(t\\)-statistic squared. \\(F\\) distribution is strictly positive, check [F-Distribution] for more details. 14.2.2 Linear Combination Testing multiple parameters as the same time \\[ \\begin{aligned} H_0&amp;: \\beta_1 -\\beta_2 = 0 \\\\ H_0&amp;: \\beta_1 - \\beta_2 &gt; 0 \\\\ H_0&amp;: \\beta_1 - 2\\times\\beta_2 =0 \\end{aligned} \\] Each is a single restriction on a function of the parameters. Null hypothesis: \\[ H_0: \\beta_1 -\\beta_2 = 0 \\] can be rewritten as \\[ H_0: \\mathbf{R}\\beta -\\mathbf{q}=0 \\] where \\(\\mathbf{R}\\)=(0 1 -1 0 0) and \\(\\mathbf{q}=0\\) 14.2.3 Estimate Difference in Coefficients There is no package to estimate for the difference between two coefficients and its CI, but a simple function created by Katherine Zee can be used to calculate this difference. Some modifications might be needed if you don’t use standard lm model in R. difftest_lm &lt;- function(x1, x2, model) { diffest &lt;- summary(model)$coef[x1, &quot;Estimate&quot;] - summary(model)$coef[x2, &quot;Estimate&quot;] vardiff &lt;- (summary(model)$coef[x1, &quot;Std. Error&quot;] ^ 2 + summary(model)$coef[x2, &quot;Std. Error&quot;] ^ 2) - (2 * (vcov(model)[x1, x2])) # variance of x1 + variance of x2 - 2*covariance of x1 and x2 diffse &lt;- sqrt(vardiff) tdiff &lt;- (diffest) / (diffse) ptdiff &lt;- 2 * (1 - pt(abs(tdiff), model$df, lower.tail = T)) upr &lt;- # will usually be very close to 1.96 diffest + qt(.975, df = model$df) * diffse lwr &lt;- diffest + qt(.025, df = model$df) * diffse df &lt;- model$df return(list( est = round(diffest, digits = 2), t = round(tdiff, digits = 2), p = round(ptdiff, digits = 4), lwr = round(lwr, digits = 2), upr = round(upr, digits = 2), df = df )) } 14.2.4 Application library(&quot;car&quot;) # Multiple hypothesis mod.davis &lt;- lm(weight ~ repwt, data=Davis) linearHypothesis(mod.davis, c(&quot;(Intercept) = 0&quot;, &quot;repwt = 1&quot;),white.adjust = TRUE) #&gt; Linear hypothesis test #&gt; #&gt; Hypothesis: #&gt; (Intercept) = 0 #&gt; repwt = 1 #&gt; #&gt; Model 1: restricted model #&gt; Model 2: weight ~ repwt #&gt; #&gt; Note: Coefficient covariance matrix supplied. #&gt; #&gt; Res.Df Df F Pr(&gt;F) #&gt; 1 183 #&gt; 2 181 2 3.3896 0.03588 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Linear Combination mod.duncan &lt;- lm(prestige ~ income + education, data=Duncan) linearHypothesis(mod.duncan, &quot;1*income - 1*education = 0&quot;) #&gt; Linear hypothesis test #&gt; #&gt; Hypothesis: #&gt; income - education = 0 #&gt; #&gt; Model 1: restricted model #&gt; Model 2: prestige ~ income + education #&gt; #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 43 7518.9 #&gt; 2 42 7506.7 1 12.195 0.0682 0.7952 14.2.5 Nonlinear Suppose that we have q nonlinear functions of the parameters \\[ \\mathbf{h}(\\theta) = \\{ h_1 (\\theta), ..., h_q (\\theta)\\}&#39; \\] The,n, the Jacobian matrix (\\(\\mathbf{H}(\\theta)\\)), of rank q is \\[ \\mathbf{H}_{q \\times p}(\\theta) = \\left( \\begin{array} {ccc} \\frac{\\partial h_1(\\theta)}{\\partial \\theta_1} &amp; ... &amp; \\frac{\\partial h_1(\\theta)}{\\partial \\theta_p} \\\\ . &amp; . &amp; . \\\\ \\frac{\\partial h_q(\\theta)}{\\partial \\theta_1} &amp; ... &amp; \\frac{\\partial h_q(\\theta)}{\\partial \\theta_p} \\end{array} \\right) \\] where the null hypothesis \\(H_0: \\mathbf{h} (\\theta) = 0\\) can be tested against the 2-sided alternative with the Wald statistic \\[ W = \\frac{\\mathbf{h(\\hat{\\theta})&#39;\\{H(\\hat{\\theta})[F(\\hat{\\theta})&#39;F(\\hat{\\theta})]^{-1}H(\\hat{\\theta})&#39;\\}^{-1}h(\\hat{\\theta})}}{s^2q} \\sim F_{q,n-p} \\] "],["the-likelihood-ratio-test.html", "14.3 The likelihood ratio test", " 14.3 The likelihood ratio test \\[ t_{LR} = 2[l(\\hat{\\theta})-l(\\theta_0)] \\sim \\chi^2_v \\] where v is the degree of freedom. Compare the height of the log-likelihood of the sample estimate in relation to the height of log-likelihood of the hypothesized population parameter Alternatively, This test considers a ratio of two maximizations, \\[ \\begin{aligned} L_r &amp;= \\text{maximized value of the likelihood under $H_0$ (the reduced model)} \\\\ L_f &amp;= \\text{maximized value of the likelihood under $H_0 \\cup H_a$ (the full model)} \\end{aligned} \\] Then, the likelihood ratio is: \\[ \\Lambda = \\frac{L_r}{L_f} \\] which can’t exceed 1 (since \\(L_f\\) is always at least as large as \\(L-r\\) because \\(L_r\\) is the result of a maximization under a restricted set of the parameter values). The likelihood ratio statistic is: \\[ \\begin{aligned} -2ln(\\Lambda) &amp;= -2ln(L_r/L_f) = -2(l_r - l_f) \\\\ \\lim_{n \\to \\infty}(-2ln(\\Lambda)) &amp;\\sim \\chi^2_v \\end{aligned} \\] where \\(v\\) is the number of parameters in the full model minus the number of parameters in the reduced model. If \\(L_r\\) is much smaller than \\(L_f\\) (the likelihood ratio exceeds \\(\\chi_{\\alpha,v}^2\\)), then we reject he reduced model and accept the full model at \\(\\alpha \\times 100 \\%\\) significance level "],["lagrange-multiplier-score.html", "14.4 Lagrange Multiplier (Score)", " 14.4 Lagrange Multiplier (Score) \\[ t_S= \\frac{S(\\theta_0)^2}{I(\\theta_0)} \\sim \\chi^2_v \\] where \\(v\\) is the degree of freedom. Compare the slope of the log-likelihood of the sample estimate in relation to the slope of the log-likelihood of the hypothesized population parameter "],["two-one-sided-tests-tost-equivalence-testing.html", "14.5 Two One-Sided Tests (TOST) Equivalence Testing", " 14.5 Two One-Sided Tests (TOST) Equivalence Testing This is a good way to test whether your population effect size is within a range of practical interest (e.g., if the effect size is 0). library(TOSTER) "],["marginal-effects.html", "Chapter 15 Marginal Effects", " Chapter 15 Marginal Effects In cases without polynomials or interactions, it can be easy to interpret the marginal effect. For example, \\[ Y = \\beta_1 X_1 + \\beta_2 X_2 \\] where \\(\\beta\\) are the marginal effects. Numerical derivation is easier than analytical derivation. We need to choose values for all the variables to calculate the marginal effect of \\(X\\) Analytical derivation \\[ f&#39;(x) \\equiv \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} \\] E.g., \\(f(x) = X^2\\) \\[ \\begin{aligned} f&#39;(x) &amp;= \\lim_{h \\to 0} \\frac{(x+h)^2 - x^2}{h} \\\\ &amp;= \\frac{x^2 + 2xh + h^2 - x^2}{h} \\\\ &amp;= \\frac{2xh + h^2}{h} \\\\ &amp;= 2x + h \\\\ &amp;= 2x \\end{aligned} \\] For numerically approach, we “just” need to find a small \\(h\\) to plug in our function. However, you also need a large enough \\(h\\) to have numerically accurate computation (Gould, Pitblado, and Poi 2010, chap. 1) Numerically approach One-sided derivative \\[ \\begin{aligned} f&#39;(x) &amp;= \\lim_{h \\to 0} \\frac{(x+h)^2 - x^2}{h} \\\\ &amp; \\approx \\frac{f(x+h) -f(x)}{h} \\end{aligned} \\] Alternatively, two-sided derivative \\[ f&#39;_2(x) \\approx \\frac{f(x+h) - f(x- h)}{2h} \\] Marginal effects for discrete variables (also known as incremental effects) are the change in \\(E[Y|X]\\) for a one unit change in \\(X\\) continuous variables are the change in \\(E[Y|X]\\) for very small changes in \\(X\\) (not unit changes), because it’s a derivative, which is a limit when \\(h \\to 0\\) Analytical derivation Numerical derivation Marginal Effects Rules of expectations Approximate analytical solution Standard Errors Rules of variances Delta method using the asymptotic errors (vcov matrix) References "],["delta-method.html", "15.1 Delta Method", " 15.1 Delta Method approximate the mean and variance of a function of random variables using a first-order Taylor approximation A semi-parametric method Alternative approaches: Analytically derive a probability function for the margin Simulation/Bootstrapping Resources: Advanced: modmarg Intermediate: UCLA stat Simple: Another one Let \\(G(\\beta)\\) be a function of the parameters \\(\\beta\\), then \\[ var(G(\\beta)) \\approx \\nabla G(\\beta) cov (\\beta) \\nabla G(\\beta)&#39; \\] where \\(\\nabla G(\\beta)\\) = the gradient of the partial derivatives of \\(G(\\beta)\\) (also known as the Jacobian) "],["average-marginal-effect-algorithm.html", "15.2 Average Marginal Effect Algorithm", " 15.2 Average Marginal Effect Algorithm For one-sided derivative \\(\\frac{\\partial p(\\mathbf{X},\\beta)}{\\partial X}\\) in the probability scale Estimate your model For each observation \\(i\\) Calculate \\(\\hat{Y}_{i0}\\) which is the prediction in the probability scale using observed values Increase \\(X\\) (variable of interest) by a “small” amount \\(h\\) (\\(X_{new} = X + h\\)) When \\(X\\) is continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) where \\(\\bar{X}\\) is the mean value of \\(X\\) When \\(X\\) is discrete, \\(h = 1\\) Calculate \\(\\hat{Y}_{i1}\\) which is the prediction in the probability scale using new \\(X\\) and other variables’ observed vales. Calculate the difference between the two predictions as fraction of \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}\\) Average numerical derivative is \\(E[\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i0}}{h}] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}\\) Two-sided derivatives Estimate your model For each observation \\(i\\) Calculate \\(\\hat{Y}_{i0}\\) which is the prediction in the probability scale using observed values Increase \\(X\\) (variable of interest) by a “small” amount \\(h\\) (\\(X_{1} = X + h\\)) and decrease \\(X\\) (variable of interest) by a “small” amount \\(h\\) (\\(X_{2} = X - h\\)) When \\(X\\) is continuous, \\(h = (|\\bar{X}| + 0.001) \\times 0.001\\) where \\(\\bar{X}\\) is the mean value of \\(X\\) When \\(X\\) is discrete, \\(h = 1\\) Calculate \\(\\hat{Y}_{i1}\\) which is the prediction in the probability scale using new \\(X_1\\) and other variables’ observed vales. Calculate \\(\\hat{Y}_{i2}\\) which is the prediction in the probability scale using new \\(X_2\\) and other variables’ observed vales. Calculate the difference between the two predictions as fraction of \\(h\\): \\(\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}\\) Average numerical derivative is \\(E[\\frac{\\bar{Y}_{i1} - \\bar{Y}_{i2}}{2h}] \\approx \\frac{\\partial p (Y|\\mathbf{X}, \\beta)}{\\partial X}\\) library(margins) library(tidyverse) data(&quot;mtcars&quot;) mod &lt;- lm(mpg ~ cyl * disp * hp, data = mtcars) margins::margins(mod) %&gt;% summary() #&gt; factor AME SE z p lower upper #&gt; cyl -4.0592 3.7614 -1.0792 0.2805 -11.4313 3.3130 #&gt; disp -0.0350 0.0132 -2.6473 0.0081 -0.0610 -0.0091 #&gt; hp -0.0284 0.0185 -1.5348 0.1248 -0.0647 0.0079 # function for variable get_mae &lt;- function(mod, var = &quot;disp&quot;) { data = mod$model pred &lt;- predict(mod) if (class(mod$model[[{ { var } }]]) == &quot;numeric&quot;) { h = (abs(mean(mod$model[[var]])) + 0.01) * 0.01 } else { h = 1 } data[[{ { var } }]] &lt;- data[[{ { var } }]] - h pred_new &lt;- predict(mod, newdata = data) mean(pred - pred_new) / h } get_mae(mod, var = &quot;disp&quot;) #&gt; [1] -0.03504546 "],["packages.html", "15.3 Packages", " 15.3 Packages 15.3.1 MarginalEffects MarginalEffects package is a successor of margins and emtrends (faster, more efficient, more adaptable). Hence, this is advocated to be used. A limitation is that there is no readily function to correct for multiple comparisons. Hence, one can use the p.adjust function to overcome this disadvantage. Definitions from the package: Marginal effects are slopes or derivatives (i.e., effect of changes in a variable on the outcome) margins package defines marginal effects as “partial derivatives of the regression equation with respect to each variable in the model for each unit in the data.” Marginal means are averages or integrals (i.e., marginalizing across rows of a prediction grid) To customize your plot using plot_cme (which is a ggplot class), you can check this post by the author of the MarginalEffects package Causal inference with the parametric g-formula Because the plug-in g estimator is equivalent to the average contrast in the marginaleffects package. To get predicted values library(marginaleffects) library(tidyverse) data(mtcars) mod &lt;- lm(mpg ~ hp * wt * am, data = mtcars) predictions(mod) %&gt;% head() #&gt; #&gt; Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % #&gt; 22.5 0.884 25.4 &lt;0.001 471.7 20.8 24.2 #&gt; 20.8 1.194 17.4 &lt;0.001 223.3 18.5 23.1 #&gt; 25.3 0.709 35.7 &lt;0.001 922.7 23.9 26.7 #&gt; 20.3 0.704 28.8 &lt;0.001 601.5 18.9 21.6 #&gt; 17.0 0.712 23.9 &lt;0.001 416.2 15.6 18.4 #&gt; 19.7 0.875 22.5 &lt;0.001 368.8 17.9 21.4 #&gt; #&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am # for specific reference values predictions(mod, newdata = datagrid(am = 0, wt = c(2, 4))) #&gt; #&gt; am wt Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % hp #&gt; 0 2 22.0 2.04 10.8 &lt;0.001 87.4 18.0 26.0 147 #&gt; 0 4 16.6 1.08 15.3 &lt;0.001 173.8 14.5 18.7 147 #&gt; #&gt; Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt plot_cap(mod, condition = c(&quot;hp&quot;,&quot;wt&quot;)) # Average Margianl Effects mfx &lt;- marginaleffects(mod, variables = c(&quot;hp&quot;, &quot;wt&quot;)) summary(mfx) #&gt; #&gt; Term Contrast Estimate Std. Error z Pr(&gt;|z|) 2.5 % 97.5 % #&gt; hp mean(dY/dX) -0.0381 0.0128 -2.98 0.00291 -0.0631 -0.013 #&gt; wt mean(dY/dX) -3.9391 1.0858 -3.63 &lt; 0.001 -6.0672 -1.811 #&gt; #&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high # Group-Average Marginal Effects marginaleffects::marginaleffects(mod, by = &quot;hp&quot;, variables = &quot;am&quot;) #&gt; #&gt; Term Contrast hp Estimate Std. Error z Pr(&gt;|z|) S 2.5 % #&gt; am mean(1) - mean(0) 52 3.976 5.20 0.764 0.445 1.2 -6.22 #&gt; am mean(1) - mean(0) 62 -2.774 2.51 -1.107 0.268 1.9 -7.68 #&gt; am mean(1) - mean(0) 65 2.999 4.13 0.725 0.468 1.1 -5.10 #&gt; am mean(1) - mean(0) 66 2.025 3.48 0.582 0.561 0.8 -4.80 #&gt; am mean(1) - mean(0) 91 1.858 2.76 0.674 0.500 1.0 -3.54 #&gt; am mean(1) - mean(0) 93 1.201 2.35 0.511 0.609 0.7 -3.40 #&gt; am mean(1) - mean(0) 95 -1.832 1.97 -0.931 0.352 1.5 -5.69 #&gt; am mean(1) - mean(0) 97 0.708 2.04 0.347 0.728 0.5 -3.28 #&gt; am mean(1) - mean(0) 105 -2.682 2.37 -1.132 0.258 2.0 -7.32 #&gt; am mean(1) - mean(0) 109 -0.237 1.59 -0.149 0.881 0.2 -3.35 #&gt; am mean(1) - mean(0) 110 -0.640 1.57 -0.407 0.684 0.5 -3.73 #&gt; am mean(1) - mean(0) 113 4.081 3.94 1.037 0.300 1.7 -3.63 #&gt; am mean(1) - mean(0) 123 -2.098 2.10 -0.998 0.318 1.7 -6.22 #&gt; am mean(1) - mean(0) 150 -1.429 1.90 -0.753 0.452 1.1 -5.15 #&gt; am mean(1) - mean(0) 175 -0.416 1.56 -0.266 0.790 0.3 -3.48 #&gt; am mean(1) - mean(0) 180 -1.381 2.47 -0.560 0.576 0.8 -6.22 #&gt; am mean(1) - mean(0) 205 -2.873 6.24 -0.460 0.645 0.6 -15.11 #&gt; am mean(1) - mean(0) 215 -2.534 6.95 -0.364 0.716 0.5 -16.16 #&gt; am mean(1) - mean(0) 230 -1.477 7.07 -0.209 0.835 0.3 -15.34 #&gt; am mean(1) - mean(0) 245 1.115 2.28 0.488 0.625 0.7 -3.36 #&gt; am mean(1) - mean(0) 264 2.106 2.29 0.920 0.358 1.5 -2.38 #&gt; am mean(1) - mean(0) 335 4.027 3.24 1.243 0.214 2.2 -2.32 #&gt; 97.5 % #&gt; 14.18 #&gt; 2.14 #&gt; 11.10 #&gt; 8.85 #&gt; 7.26 #&gt; 5.80 #&gt; 2.02 #&gt; 4.70 #&gt; 1.96 #&gt; 2.87 #&gt; 2.45 #&gt; 11.79 #&gt; 2.02 #&gt; 2.29 #&gt; 2.64 #&gt; 3.46 #&gt; 9.36 #&gt; 11.09 #&gt; 12.39 #&gt; 5.59 #&gt; 6.59 #&gt; 10.38 #&gt; #&gt; Columns: term, contrast, hp, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted # Marginal effects at representative values marginaleffects::marginaleffects(mod, newdata = datagrid(am = 0, wt = c(2, 4))) #&gt; #&gt; Term Contrast am wt Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % #&gt; am 1 - 0 0 2 2.5465 2.7860 0.914 0.3607 1.5 -2.9139 8.00694 #&gt; am 1 - 0 0 4 -2.9661 3.0381 -0.976 0.3289 1.6 -8.9207 2.98852 #&gt; hp dY/dX 0 2 -0.0598 0.0283 -2.115 0.0344 4.9 -0.1153 -0.00439 #&gt; hp dY/dX 0 4 -0.0309 0.0187 -1.654 0.0981 3.3 -0.0676 0.00572 #&gt; wt dY/dX 0 2 -2.6762 1.4194 -1.885 0.0594 4.1 -5.4582 0.10587 #&gt; wt dY/dX 0 4 -2.6762 1.4199 -1.885 0.0595 4.1 -5.4591 0.10676 #&gt; #&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, wt, predicted_lo, predicted_hi, predicted, mpg, hp # Marginal Effects at the Mean marginaleffects::marginaleffects(mod, newdata = &quot;mean&quot;) #&gt; #&gt; Term Contrast Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % #&gt; am 1 - 0 -0.8086 1.52383 -0.531 0.59568 0.7 -3.795 2.1781 #&gt; hp dY/dX -0.0323 0.00956 -3.375 &lt; 0.001 10.4 -0.051 -0.0135 #&gt; wt dY/dX -3.7959 1.21310 -3.129 0.00175 9.2 -6.174 -1.4183 #&gt; #&gt; Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, mpg, hp, wt, am # counterfactual comparisons(mod, variables = list(am = 0:1)) %&gt;% summary() #&gt; #&gt; Term Contrast Estimate Std. Error z Pr(&gt;|z|) 2.5 % 97.5 % #&gt; am mean(1) - mean(0) -0.0481 1.85 -0.026 0.979 -3.68 3.58 #&gt; #&gt; Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high 15.3.2 margins Marginal effects are partial derivative of the regression equation with respect to each variable in the model for each unit in the data Average Partial Effects: the contribution of each variable the outcome scale, conditional on the other variables involved in the link function transformation of the linear predictor Average Marginal Effects: the marginal contribution of each variable on the scale of the linear predictor. Average marginal effects are the mean of these unit-specific partial derivatives over some sample margins package gives the marginal effects of models (a replication of the margins command in Stata). prediction package gives the unit-specific and sample average predictions of models (similar to the predictive margins in Stata). library(margins) # examples by the package&#39;s authors mod &lt;- lm(mpg ~ cyl * hp + wt, data = mtcars) summary(mod) #&gt; #&gt; Call: #&gt; lm(formula = mpg ~ cyl * hp + wt, data = mtcars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.3440 -1.4144 -0.6166 1.2160 4.2815 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 52.017520 4.916935 10.579 4.18e-11 *** #&gt; cyl -2.742125 0.800228 -3.427 0.00197 ** #&gt; hp -0.163594 0.052122 -3.139 0.00408 ** #&gt; wt -3.119815 0.661322 -4.718 6.51e-05 *** #&gt; cyl:hp 0.018954 0.006645 2.852 0.00823 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.242 on 27 degrees of freedom #&gt; Multiple R-squared: 0.8795, Adjusted R-squared: 0.8616 #&gt; F-statistic: 49.25 on 4 and 27 DF, p-value: 5.065e-12 In cases where you have interaction or polynomial terms, the coefficient estimates cannot be interpreted as the marginal effects of X on Y. Hence, if you want to know the average marginal effects of each variable then summary(margins(mod)) #&gt; factor AME SE z p lower upper #&gt; cyl 0.0381 0.5999 0.0636 0.9493 -1.1376 1.2139 #&gt; hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179 #&gt; wt -3.1198 0.6613 -4.7176 0.0000 -4.4160 -1.8236 # equivalently margins_summary(mod) #&gt; factor AME SE z p lower upper #&gt; cyl 0.0381 0.5999 0.0636 0.9493 -1.1376 1.2139 #&gt; hp -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179 #&gt; wt -3.1198 0.6613 -4.7176 0.0000 -4.4160 -1.8236 plot(margins(mod)) Marginal effects at the mean (MEM): Marginal effects at the mean values of the sample For discrete variables, it’s called average discrete change (ADC) Average Marginal Effect (AME) An average of the marginal effects at each value of the sample Marginal Effects at representative values (MER) margins(mod, at = list(hp = 150)) #&gt; at(hp) cyl hp wt #&gt; 150 0.1009 -0.04632 -3.12 margins(mod, at = list(hp = 150)) %&gt;% summary() #&gt; factor hp AME SE z p lower upper #&gt; cyl 150.0000 0.1009 0.6128 0.1647 0.8692 -1.1001 1.3019 #&gt; hp 150.0000 -0.0463 0.0145 -3.1909 0.0014 -0.0748 -0.0179 #&gt; wt 150.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236 15.3.3 mfx Works well with Generalized Linear Models/glm package For technical details, see the package vignette Model Dependent Variable Syntax Probit Binary probitmfx Logit Binary logitmfx Poisson Count poissonmfx Negative Binomial Count negbinmfx Beta Rate betamfx library(mfx) data(&quot;mtcars&quot;) poissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars) #&gt; Call: #&gt; poissonmfx(formula = vs ~ mpg * cyl * disp, data = mtcars) #&gt; #&gt; Marginal Effects: #&gt; dF/dx Std. Err. z P&gt;|z| #&gt; mpg 1.4722e-03 8.7531e-03 0.1682 0.8664 #&gt; cyl 6.6420e-03 3.9263e-02 0.1692 0.8657 #&gt; disp 1.5899e-04 9.4555e-04 0.1681 0.8665 #&gt; mpg:cyl -3.4698e-04 2.0564e-03 -0.1687 0.8660 #&gt; mpg:disp -7.6794e-06 4.5545e-05 -0.1686 0.8661 #&gt; cyl:disp -3.3837e-05 1.9919e-04 -0.1699 0.8651 #&gt; mpg:cyl:disp 1.6812e-06 9.8919e-06 0.1700 0.8650 This package can only give the marginal effect for each variable in the glm model, but not the average marginal effect that we might look for. "],["prediction-and-estimation.html", "Chapter 16 Prediction and Estimation", " Chapter 16 Prediction and Estimation Prediction and Estimation (or Causal Inference) serve distinct roles in understanding and modeling data. "],["prediction-2.html", "16.1 Prediction", " 16.1 Prediction Definition: Prediction, denoted as \\(\\hat{y}\\), is about creating an algorithm for predicting the outcome variable \\(y\\) from predictors \\(x\\). Goal: The primary goal is loss minimization, aiming for model accuracy on unseen data: \\[ \\hat{f} \\approx \\min E_{(y,x)} L(f(x), y) \\] Applications in Economics: Measure variables. Embed prediction tasks within parameter estimation or treatment effects. Control for observed confounders. "],["parameter-estimation.html", "16.2 Parameter Estimation", " 16.2 Parameter Estimation Definition: Parameter estimation, represented by \\(\\hat{\\beta}\\), focuses on estimating the relationship between \\(y\\) and \\(x\\). Goal: The aim is consistency, ensuring that models perform well on the training data: \\[ E[\\hat{f}] = f \\] Challenges: High-dimensional spaces can lead to covariance among variables and multicollinearity. This leads to the bias-variance tradeoff (Hastie et al. 2009). References "],["causation-versus-prediction.html", "16.3 Causation versus Prediction", " 16.3 Causation versus Prediction Understanding the relationship between causation and prediction is crucial in statistical modeling. Let \\(Y\\) be an outcome variable dependent on \\(X\\), and our aim is to manipulate \\(X\\) to maximize a payoff function \\(\\pi(X, Y)\\) (Kleinberg et al. 2015). The decision on \\(X\\) hinges on: \\[ \\begin{aligned} \\frac{d\\pi(X, Y)}{d X} &amp;= \\frac{\\partial \\pi}{\\partial X} (Y) + \\frac{\\partial \\pi}{\\partial Y} \\frac{\\partial Y}{\\partial X} \\\\ &amp;= \\frac{\\partial \\pi}{\\partial X} \\text{(Prediction)} + \\frac{\\partial \\pi}{\\partial Y} \\text{(Causation)} \\end{aligned} \\] Empirical work is essential for estimating the derivatives in this equation: \\(\\frac{\\partial Y}{\\partial X}\\) is required for causal inference to determine \\(X\\)’s effect on \\(Y\\), \\(\\frac{\\partial \\pi}{\\partial X}\\) is required for prediction of \\(Y\\). (SICSS 2018 - Sendhil Mullainathan’s presentation slide) References "],["moderation.html", "Chapter 17 Moderation", " Chapter 17 Moderation Spotlight Analysis: Compare the mean of the dependent of the two groups (treatment and control) at every value (Simple Slopes Analysis) Floodlight Analysis: is spotlight analysis on the whole range of the moderator (Johnson-Neyman intervals) Other Resources: BANOVAL : floodlight analysis on Bayesian ANOVA models cSEM : doFloodlightAnalysis in SEM model (Spiller et al. 2013) Terminology: Main effects (slopes): coefficients that do no involve interaction terms Simple slope: when a continuous independent variable interact with a moderating variable, its slope at a particular level of the moderating variable Simple effect: when a categorical independent variable interacts with a moderating variable, its effect at a particular level of the moderating variable. Example: \\[ Y = \\beta_0 + \\beta_1 X + \\beta_2 M + \\beta_3 X \\times M \\] where \\(\\beta_0\\) = intercept \\(\\beta_1\\) = simple effect (slope) of \\(X\\) (independent variable) \\(\\beta_2\\) = simple effect (slope) of \\(M\\) (moderating variable) \\(\\beta_3\\) = interaction of \\(X\\) and \\(M\\) Three types of interactions: Continuous by continuous Continuous by categorical Categorical by categorical When interpreting the three-way interactions, one can use the slope difference test (Dawson and Richter 2006) References "],["emmeans-package.html", "17.1 emmeans package", " 17.1 emmeans package install.packages(&quot;emmeans&quot;) library(emmeans) Data set is from UCLA seminar where gender and prog are categorical dat &lt;- readRDS(&quot;data/exercise.rds&quot;) %&gt;% mutate(prog = factor(prog, labels = c(&quot;jog&quot;, &quot;swim&quot;, &quot;read&quot;))) %&gt;% mutate(gender = factor(gender, labels = c(&quot;male&quot;, &quot;female&quot;))) 17.1.1 Continuous by continuous contcont &lt;- lm(loss~hours*effort,data=dat) summary(contcont) #&gt; #&gt; Call: #&gt; lm(formula = loss ~ hours * effort, data = dat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -29.52 -10.60 -1.78 11.13 34.51 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.79864 11.60362 0.672 0.5017 #&gt; hours -9.37568 5.66392 -1.655 0.0982 . #&gt; effort -0.08028 0.38465 -0.209 0.8347 #&gt; hours:effort 0.39335 0.18750 2.098 0.0362 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 13.56 on 896 degrees of freedom #&gt; Multiple R-squared: 0.07818, Adjusted R-squared: 0.07509 #&gt; F-statistic: 25.33 on 3 and 896 DF, p-value: 9.826e-16 Simple slopes for a continuous by continuous model Spotlight analysis (Aiken and West 2005): usually pick 3 values of moderating variable: Mean Moderating Variable + \\(\\sigma \\times\\) (Moderating variable) Mean Moderating Variable Mean Moderating Variable - \\(\\sigma \\times\\) (Moderating variable) effar &lt;- round(mean(dat$effort) + sd(dat$effort), 1) effr &lt;- round(mean(dat$effort), 1) effbr &lt;- round(mean(dat$effort) - sd(dat$effort), 1) # specify list of points mylist &lt;- list(effort = c(effbr, effr, effar)) # get the estimates emtrends(contcont, ~ effort, var = &quot;hours&quot;, at = mylist) #&gt; effort hours.trend SE df lower.CL upper.CL #&gt; 24.5 0.261 1.352 896 -2.392 2.91 #&gt; 29.7 2.307 0.915 896 0.511 4.10 #&gt; 34.8 4.313 1.308 896 1.745 6.88 #&gt; #&gt; Confidence level used: 0.95 # plot mylist &lt;- list(hours = seq(0, 4, by = 0.4), effort = c(effbr, effr, effar)) emmip(contcont, effort ~ hours, at = mylist, CIs = TRUE) # statistical test for slope difference emtrends( contcont, pairwise ~ effort, var = &quot;hours&quot;, at = mylist, adjust = &quot;none&quot; ) #&gt; $emtrends #&gt; effort hours.trend SE df lower.CL upper.CL #&gt; 24.5 0.261 1.352 896 -2.392 2.91 #&gt; 29.7 2.307 0.915 896 0.511 4.10 #&gt; 34.8 4.313 1.308 896 1.745 6.88 #&gt; #&gt; Results are averaged over the levels of: hours #&gt; Confidence level used: 0.95 #&gt; #&gt; $contrasts #&gt; contrast estimate SE df t.ratio p.value #&gt; effort24.5 - effort29.7 -2.05 0.975 896 -2.098 0.0362 #&gt; effort24.5 - effort34.8 -4.05 1.931 896 -2.098 0.0362 #&gt; effort29.7 - effort34.8 -2.01 0.956 896 -2.098 0.0362 #&gt; #&gt; Results are averaged over the levels of: hours The 3 p-values are the same as the interaction term. For publication, we use library(ggplot2) # data mylist &lt;- list(hours = seq(0, 4, by = 0.4), effort = c(effbr, effr, effar)) contcontdat &lt;- emmip(contcont, effort ~ hours, at = mylist, CIs = TRUE, plotit = FALSE) contcontdat$feffort &lt;- factor(contcontdat$effort) levels(contcontdat$feffort) &lt;- c(&quot;low&quot;, &quot;med&quot;, &quot;high&quot;) # plot p &lt;- ggplot(data = contcontdat, aes(x = hours, y = yvar, color = feffort)) + geom_line() p1 &lt;- p + geom_ribbon(aes(ymax = UCL, ymin = LCL, fill = feffort), alpha = 0.4) p1 + labs(x = &quot;Hours&quot;, y = &quot;Weight Loss&quot;, color = &quot;Effort&quot;, fill = &quot;Effort&quot;) 17.1.2 Continuous by categorical # use Female as basline dat$gender &lt;- relevel(dat$gender, ref = &quot;female&quot;) contcat &lt;- lm(loss ~ hours * gender, data = dat) summary(contcat) #&gt; #&gt; Call: #&gt; lm(formula = loss ~ hours * gender, data = dat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -27.118 -11.350 -1.963 10.001 42.376 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.335 2.731 1.221 0.222 #&gt; hours 3.315 1.332 2.489 0.013 * #&gt; gendermale 3.571 3.915 0.912 0.362 #&gt; hours:gendermale -1.724 1.898 -0.908 0.364 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 14.06 on 896 degrees of freedom #&gt; Multiple R-squared: 0.008433, Adjusted R-squared: 0.005113 #&gt; F-statistic: 2.54 on 3 and 896 DF, p-value: 0.05523 Get simple slopes by each level of the categorical moderator emtrends(contcat, ~ gender, var = &quot;hours&quot;) #&gt; gender hours.trend SE df lower.CL upper.CL #&gt; female 3.32 1.33 896 0.702 5.93 #&gt; male 1.59 1.35 896 -1.063 4.25 #&gt; #&gt; Confidence level used: 0.95 # test difference in slopes emtrends(contcat, pairwise ~ gender, var = &quot;hours&quot;) #&gt; $emtrends #&gt; gender hours.trend SE df lower.CL upper.CL #&gt; female 3.32 1.33 896 0.702 5.93 #&gt; male 1.59 1.35 896 -1.063 4.25 #&gt; #&gt; Confidence level used: 0.95 #&gt; #&gt; $contrasts #&gt; contrast estimate SE df t.ratio p.value #&gt; female - male 1.72 1.9 896 0.908 0.3639 # which is the same as the interaction term # plot (mylist &lt;- list( hours = seq(0, 4, by = 0.4), gender = c(&quot;female&quot;, &quot;male&quot;) )) #&gt; $hours #&gt; [1] 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 4.0 #&gt; #&gt; $gender #&gt; [1] &quot;female&quot; &quot;male&quot; emmip(contcat, gender ~ hours, at = mylist, CIs = TRUE) 17.1.3 Categorical by categorical # relevel baseline dat$prog &lt;- relevel(dat$prog, ref = &quot;read&quot;) dat$gender &lt;- relevel(dat$gender, ref = &quot;female&quot;) catcat &lt;- lm(loss ~ gender * prog, data = dat) summary(catcat) #&gt; #&gt; Call: #&gt; lm(formula = loss ~ gender * prog, data = dat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -19.1723 -4.1894 -0.0994 3.7506 27.6939 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -3.6201 0.5322 -6.802 1.89e-11 *** #&gt; gendermale -0.3355 0.7527 -0.446 0.656 #&gt; progjog 7.9088 0.7527 10.507 &lt; 2e-16 *** #&gt; progswim 32.7378 0.7527 43.494 &lt; 2e-16 *** #&gt; gendermale:progjog 7.8188 1.0645 7.345 4.63e-13 *** #&gt; gendermale:progswim -6.2599 1.0645 -5.881 5.77e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 6.519 on 894 degrees of freedom #&gt; Multiple R-squared: 0.7875, Adjusted R-squared: 0.7863 #&gt; F-statistic: 662.5 on 5 and 894 DF, p-value: &lt; 2.2e-16 Simple effects emcatcat &lt;- emmeans(catcat, ~ gender*prog) # differences in predicted values contrast(emcatcat, &quot;revpairwise&quot;, by = &quot;prog&quot;, adjust = &quot;bonferroni&quot;) #&gt; prog = read: #&gt; contrast estimate SE df t.ratio p.value #&gt; male - female -0.335 0.753 894 -0.446 0.6559 #&gt; #&gt; prog = jog: #&gt; contrast estimate SE df t.ratio p.value #&gt; male - female 7.483 0.753 894 9.942 &lt;.0001 #&gt; #&gt; prog = swim: #&gt; contrast estimate SE df t.ratio p.value #&gt; male - female -6.595 0.753 894 -8.762 &lt;.0001 Plot emmip(catcat, prog ~ gender,CIs=TRUE) Bar graph catcatdat &lt;- emmip(catcat, gender ~ prog, CIs = TRUE, plotit = FALSE) p &lt;- ggplot(data = catcatdat, aes(x = prog, y = yvar, fill = gender)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) p1 &lt;- p + geom_errorbar( position = position_dodge(.9), width = .25, aes(ymax = UCL, ymin = LCL), alpha = 0.3 ) p1 + labs(x = &quot;Program&quot;, y = &quot;Weight Loss&quot;, fill = &quot;Gender&quot;) References "],["probmod-package.html", "17.2 probmod package", " 17.2 probmod package Not recommend: package has serious problem with subscript. install.packages(&quot;probemod&quot;) library(probemod) myModel &lt;- lm(loss ~ hours * gender, data = dat %&gt;% select(loss, hours, gender)) jnresults &lt;- jn(myModel, dv = &#39;loss&#39;, iv = &#39;hours&#39;, mod = &#39;gender&#39;) pickapoint( myModel, dv = &#39;loss&#39;, iv = &#39;hours&#39;, mod = &#39;gender&#39;, alpha = .01 ) plot(jnresults) "],["interactions-package.html", "17.3 interactions package", " 17.3 interactions package Recommend install.packages(&quot;interactions&quot;) 17.3.1 Continuous interaction (at least one of the two variables is continuous) library(interactions) library(jtools) # for summ() states &lt;- as.data.frame(state.x77) fiti &lt;- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states) summ(fiti) Observations 50 Dependent variable Income Type OLS linear regression F(4,45) 10.65 R² 0.49 Adj. R² 0.44 Est. S.E. t val. p (Intercept) 1414.46 737.84 1.92 0.06 Illiteracy 753.07 385.90 1.95 0.06 Murder 130.60 44.67 2.92 0.01 HS Grad 40.76 10.92 3.73 0.00 Illiteracy:Murder -97.04 35.86 -2.71 0.01 Standard errors: OLS For continuous moderator, the three values chosen are: -1 SD above the mean The mean -1 SD below the mean interact_plot(fiti, pred = Illiteracy, modx = Murder, # if you don&#39;t want the plot to mean-center # centered = &quot;none&quot;, # exclude the mean value of the moderator # modx.values = &quot;plus-minus&quot;, # split moderator&#39;s distribution into 3 groups # modx.values = &quot;terciles&quot; plot.points = T, # overlay data # different shape for differennt levels of the moderator point.shape = T, # if two data points are on top one another, # this moves them apart by little jitter = 0.1, # other appearance option x.label = &quot;X label&quot;, y.label = &quot;Y label&quot;, main.title = &quot;Title&quot;, legend.main = &quot;Legend Title&quot;, colors = &quot;blue&quot;, # include confidence band interval = TRUE, int.width = 0.9, robust = TRUE # use robust SE ) To include weights from the regression inn the plot fiti &lt;- lm(Income ~ Illiteracy * Murder, data = states, weights = Population) interact_plot(fiti, pred = Illiteracy, modx = Murder, plot.points = TRUE) Partial Effect Plot library(ggplot2) data(cars) fitc &lt;- lm(cty ~ year + cyl * displ + class + fl + drv, data = mpg) summ(fitc) Observations 234 Dependent variable cty Type OLS linear regression F(16,217) 99.73 R² 0.88 Adj. R² 0.87 Est. S.E. t val. p (Intercept) -200.98 47.01 -4.28 0.00 year 0.12 0.02 5.03 0.00 cyl -1.86 0.28 -6.69 0.00 displ -3.56 0.66 -5.41 0.00 classcompact -2.60 0.93 -2.80 0.01 classmidsize -2.63 0.93 -2.82 0.01 classminivan -4.41 1.04 -4.24 0.00 classpickup -4.37 0.93 -4.68 0.00 classsubcompact -2.38 0.93 -2.56 0.01 classsuv -4.27 0.87 -4.92 0.00 fld 6.34 1.69 3.74 0.00 fle -4.57 1.66 -2.75 0.01 flp -1.92 1.59 -1.21 0.23 flr -0.79 1.57 -0.50 0.61 drvf 1.40 0.40 3.52 0.00 drvr 0.49 0.46 1.06 0.29 cyl:displ 0.36 0.08 4.56 0.00 Standard errors: OLS interact_plot( fitc, pred = displ, modx = cyl, # the observed data is based on displ, cyl, and model error partial.residuals = TRUE, modx.values = c(4, 5, 6, 8) ) Check linearity assumption in the model Plot the lines based on the subsample (red line), and whole sample (black line) x_2 &lt;- runif(n = 200, min = -3, max = 3) w &lt;- rbinom(n = 200, size = 1, prob = 0.5) err &lt;- rnorm(n = 200, mean = 0, sd = 4) y_2 &lt;- 2.5 - x_2 ^ 2 - 5 * w + 2 * w * (x_2 ^ 2) + err data_2 &lt;- as.data.frame(cbind(x_2, y_2, w)) model_2 &lt;- lm(y_2 ~ x_2 * w, data = data_2) summ(model_2) Observations 200 Dependent variable y_2 Type OLS linear regression F(3,196) 1.57 R² 0.02 Adj. R² 0.01 Est. S.E. t val. p (Intercept) -1.12 0.50 -2.27 0.02 x_2 0.28 0.27 1.04 0.30 w 1.42 0.71 2.00 0.05 x_2:w -0.23 0.40 -0.58 0.56 Standard errors: OLS interact_plot( model_2, pred = x_2, modx = w, linearity.check = TRUE, plot.points = TRUE ) 17.3.1.1 Simple Slopes Analysis continuous by continuous variable interaction (still work for binary) conditional slope of the variable of interest (i.e., the slope of \\(X\\) when we hold \\(M\\) constant at a value) Using sim_slopes it will mean-center all variables except the variable of interest For moderator that is Continuous, it will pick mean, and plus/minus 1 SD Categorical, it will use all factor sim_slopes requires A regression model with an interaction term) Variable of interest (pred =) Moderator: (modx =) sim_slopes(fiti, pred = Illiteracy, modx = Murder, johnson_neyman = FALSE) #&gt; SIMPLE SLOPES ANALYSIS #&gt; #&gt; Slope of Illiteracy when Murder = 5.420973 (- 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; -------- -------- -------- ------ #&gt; -71.59 268.65 -0.27 0.79 #&gt; #&gt; Slope of Illiteracy when Murder = 8.685043 (Mean): #&gt; #&gt; Est. S.E. t val. p #&gt; --------- -------- -------- ------ #&gt; -437.12 175.82 -2.49 0.02 #&gt; #&gt; Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; --------- -------- -------- ------ #&gt; -802.66 145.72 -5.51 0.00 # plot the coefficients ss &lt;- sim_slopes(fiti, pred = Illiteracy, modx = Murder, modx.values = c(0, 5, 10)) plot(ss) # table ss &lt;- sim_slopes(fiti, pred = Illiteracy, modx = Murder, modx.values = c(0, 5, 10)) library(huxtable) as_huxtable(ss) Table 17.1: Value of MurderSlope of Illiteracy Value of Murderslope 0.00535.50 (458.77) 5.00-24.44 (282.48) 10.00-584.38 (152.37)*** 17.3.1.2 Johnson-Neyman intervals To know all the values of the moderator for which the slope of the variable of interest will be statistically significant, we can use the Johnson-Neyman interval (P. O. Johnson and Neyman 1936) Even though we kind of know that the alpha level when implementing the Johnson-Neyman interval is not correct (Bauer and Curran 2005), not until recently that there is a correction for the type I and II errors (Esarey and Sumner 2018). Since Johnson-Neyman inflates the type I error (comparisons across all values of the moderator) sim_slopes( fiti, pred = Illiteracy, modx = Murder, johnson_neyman = TRUE, control.fdr = TRUE, # correction for type I and II # include conditional intecepts # cond.int = TRUE, robust = &quot;HC3&quot;, # rubust SE # don&#39;t mean-centered non-focal variables # centered = &quot;none&quot;, jnalpha = 0.05 ) #&gt; JOHNSON-NEYMAN INTERVAL #&gt; #&gt; When Murder is OUTSIDE the interval [-11.70, 8.75], the slope of Illiteracy #&gt; is p &lt; .05. #&gt; #&gt; Note: The range of observed values of Murder is [1.40, 15.10] #&gt; #&gt; Interval calculated using false discovery rate adjusted t = 2.33 #&gt; #&gt; SIMPLE SLOPES ANALYSIS #&gt; #&gt; Slope of Illiteracy when Murder = 5.420973 (- 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; -------- -------- -------- ------ #&gt; -71.59 256.60 -0.28 0.78 #&gt; #&gt; Slope of Illiteracy when Murder = 8.685043 (Mean): #&gt; #&gt; Est. S.E. t val. p #&gt; --------- -------- -------- ------ #&gt; -437.12 191.07 -2.29 0.03 #&gt; #&gt; Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; --------- -------- -------- ------ #&gt; -802.66 178.75 -4.49 0.00 For plotting, we can use johnson_neyman johnson_neyman(fiti, pred = Illiteracy, modx = Murder, # correction for type I and II control.fdr = TRUE, alpha = .05) #&gt; JOHNSON-NEYMAN INTERVAL #&gt; #&gt; When Murder is OUTSIDE the interval [-22.57, 8.52], the slope of Illiteracy #&gt; is p &lt; .05. #&gt; #&gt; Note: The range of observed values of Murder is [1.40, 15.10] #&gt; #&gt; Interval calculated using false discovery rate adjusted t = 2.33 Note: y-axis is the conditional slope of the variable of interest 17.3.1.3 3-way interaction # fita3 &lt;- # lm(rating ~ privileges * critical * learning, # data = attitude) # # probe_interaction( # fita3, # pred = critical, # modx = learning, # mod2 = privileges, # alpha = .1 # ) mtcars$cyl &lt;- factor(mtcars$cyl, labels = c(&quot;4 cylinder&quot;, &quot;6 cylinder&quot;, &quot;8 cylinder&quot;)) fitc3 &lt;- lm(mpg ~ hp * wt * cyl, data = mtcars) interact_plot(fitc3, pred = hp, modx = wt, mod2 = cyl) + theme_apa(legend.pos = &quot;bottomright&quot;) Johnson-Neyman 3-way interaction library(survey) data(api) dstrat &lt;- svydesign( id = ~ 1, strata = ~ stype, weights = ~ pw, data = apistrat, fpc = ~ fpc ) regmodel3 &lt;- survey::svyglm(api00 ~ avg.ed * growth * enroll, design = dstrat) sim_slopes( regmodel3, pred = growth, modx = avg.ed, mod2 = enroll, jnplot = TRUE ) #&gt; ███████████████ While enroll (2nd moderator) = 153.0518 (- 1 SD) ██████████████ #&gt; #&gt; JOHNSON-NEYMAN INTERVAL #&gt; #&gt; When avg.ed is OUTSIDE the interval [2.75, 3.82], the slope of growth is p #&gt; &lt; .05. #&gt; #&gt; Note: The range of observed values of avg.ed is [1.38, 4.44] #&gt; #&gt; SIMPLE SLOPES ANALYSIS #&gt; #&gt; Slope of growth when avg.ed = 2.085002 (- 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; ------ ------ -------- ------ #&gt; 1.25 0.32 3.86 0.00 #&gt; #&gt; Slope of growth when avg.ed = 2.787381 (Mean): #&gt; #&gt; Est. S.E. t val. p #&gt; ------ ------ -------- ------ #&gt; 0.39 0.22 1.75 0.08 #&gt; #&gt; Slope of growth when avg.ed = 3.489761 (+ 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; ------- ------ -------- ------ #&gt; -0.48 0.35 -1.37 0.17 #&gt; #&gt; ████████████████ While enroll (2nd moderator) = 595.2821 (Mean) ███████████████ #&gt; #&gt; JOHNSON-NEYMAN INTERVAL #&gt; #&gt; When avg.ed is OUTSIDE the interval [2.84, 7.83], the slope of growth is p #&gt; &lt; .05. #&gt; #&gt; Note: The range of observed values of avg.ed is [1.38, 4.44] #&gt; #&gt; SIMPLE SLOPES ANALYSIS #&gt; #&gt; Slope of growth when avg.ed = 2.085002 (- 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; ------ ------ -------- ------ #&gt; 0.72 0.22 3.29 0.00 #&gt; #&gt; Slope of growth when avg.ed = 2.787381 (Mean): #&gt; #&gt; Est. S.E. t val. p #&gt; ------ ------ -------- ------ #&gt; 0.34 0.16 2.16 0.03 #&gt; #&gt; Slope of growth when avg.ed = 3.489761 (+ 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; ------- ------ -------- ------ #&gt; -0.04 0.24 -0.16 0.87 #&gt; #&gt; ███████████████ While enroll (2nd moderator) = 1037.5125 (+ 1 SD) ██████████████ #&gt; #&gt; JOHNSON-NEYMAN INTERVAL #&gt; #&gt; The Johnson-Neyman interval could not be found. Is the p value for your #&gt; interaction term below the specified alpha? #&gt; #&gt; SIMPLE SLOPES ANALYSIS #&gt; #&gt; Slope of growth when avg.ed = 2.085002 (- 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; ------ ------ -------- ------ #&gt; 0.18 0.31 0.58 0.56 #&gt; #&gt; Slope of growth when avg.ed = 2.787381 (Mean): #&gt; #&gt; Est. S.E. t val. p #&gt; ------ ------ -------- ------ #&gt; 0.29 0.20 1.49 0.14 #&gt; #&gt; Slope of growth when avg.ed = 3.489761 (+ 1 SD): #&gt; #&gt; Est. S.E. t val. p #&gt; ------ ------ -------- ------ #&gt; 0.40 0.27 1.49 0.14 Report ss3 &lt;- sim_slopes(regmodel3, pred = growth, modx = avg.ed, mod2 = enroll) plot(ss3) as_huxtable(ss3) Table 17.2: enroll = 153 Value of avg.edSlope of growth Value of avg.edslope 2.091.25 (0.32)*** 2.790.39 (0.22)# enroll = 595.28 Value of avg.edSlope of growth 3.49-0.48 (0.35) 2.090.72 (0.22)** 2.790.34 (0.16)* enroll = 1037.51 Value of avg.edSlope of growth 3.49-0.04 (0.24) 2.090.18 (0.31) 2.790.29 (0.20) 3.490.40 (0.27) 17.3.2 Categorical interaction library(ggplot2) mpg2 &lt;- mpg %&gt;% mutate(cyl = factor(cyl)) mpg2[&quot;auto&quot;] &lt;- &quot;auto&quot; mpg2$auto[mpg2$trans %in% c(&quot;manual(m5)&quot;, &quot;manual(m6)&quot;)] &lt;- &quot;manual&quot; mpg2$auto &lt;- factor(mpg2$auto) mpg2[&quot;fwd&quot;] &lt;- &quot;2wd&quot; mpg2$fwd[mpg2$drv == &quot;4&quot;] &lt;- &quot;4wd&quot; mpg2$fwd &lt;- factor(mpg2$fwd) ## Drop the two cars with 5 cylinders (rest are 4, 6, or 8) mpg2 &lt;- mpg2[mpg2$cyl != &quot;5&quot;, ] ## Fit the model fit3 &lt;- lm(cty ~ cyl * fwd * auto, data = mpg2) library(jtools) # for summ() summ(fit3) Observations 230 Dependent variable cty Type OLS linear regression F(11,218) 61.37 R² 0.76 Adj. R² 0.74 Est. S.E. t val. p (Intercept) 21.37 0.39 54.19 0.00 cyl6 -4.37 0.54 -8.07 0.00 cyl8 -8.37 0.67 -12.51 0.00 fwd4wd -2.91 0.76 -3.83 0.00 automanual 1.45 0.57 2.56 0.01 cyl6:fwd4wd 0.59 0.96 0.62 0.54 cyl8:fwd4wd 2.13 0.99 2.15 0.03 cyl6:automanual -0.76 0.90 -0.84 0.40 cyl8:automanual 0.71 1.18 0.60 0.55 fwd4wd:automanual -1.66 1.07 -1.56 0.12 cyl6:fwd4wd:automanual 1.29 1.52 0.85 0.40 cyl8:fwd4wd:automanual -1.39 1.76 -0.79 0.43 Standard errors: OLS cat_plot(fit3, pred = cyl, modx = fwd, plot.points = T) #line plots cat_plot( fit3, pred = cyl, modx = fwd, geom = &quot;line&quot;, point.shape = TRUE, # colors = &quot;Set2&quot;, # choose color vary.lty = TRUE ) # bar plot cat_plot( fit3, pred = cyl, modx = fwd, geom = &quot;bar&quot;, interval = T, plot.points = TRUE ) References "],["interactionr-package.html", "17.4 interactionR package", " 17.4 interactionR package For publication purposes Following (Knol and VanderWeele 2012) for presentation (Hosmer and Lemeshow 1992) for confidence intervals based on the delta method (Zou 2008) for variance recovery “mover” method (Assmann et al. 1996) for bootstrapping install.packages(&quot;interactionR&quot;) References "],["sjplot-package.html", "17.5 sjPlot package", " 17.5 sjPlot package For publication purposes (recommend, but more advanced) link "],["causal-inference.html", "Chapter 18 Causal Inference", " Chapter 18 Causal Inference After all of the mambo jumbo that we have learned so far, I want to now talk about the concept of causality. We usually say that correlation is not causation. Then, what is causation? One of my favorite books has explained this concept beautifully (Pearl and Mackenzie 2018). And I am just going to quickly summarize the gist of it from my understanding. I hope that it can give you an initial grasp on the concept so that later you can continue to read up and develop a deeper understanding. It’s important to have a deep understanding regarding the method research. However, one needs to be aware of its limitation. As mentioned in various sections throughout the book, we see that we need to ask experts for number as our baseline or visit literature to gain insight from past research. Here, we dive in a more conceptual side statistical analysis as a whole, regardless of particular approach. You probably heard scientists say correlation doesn’t mean causation. There are ridiculous spurious correlations that give a firm grip on what the previous phrase means. The pioneer who tried to use regression to infer causation in social science was Yule (1899) (but it was a fatal attempt where he found relief policy increases poverty). To make a causal inference from statistics, the equation (function form) must be stable under intervention (i.e., variables are manipulated). Statistics is used to be a causality-free enterprise in the past. Not until the development of path analysis by Sewall Wright in the 1920s that the discipline started to pay attention to causation. Then, it remained dormant until the Causal Revolution (quoted Judea Pearl’s words). This revolution introduced the calculus of causation which includes (1) causal diagrams), and (2) a symbolic language The world has been using \\(P(Y|X)\\) (statistics use to derive this), but what we want is to compare the difference between \\(P(Y|do(X))\\): treatment group \\(P(Y|do(not-X))\\): control group Hence, we can see a clear difference between \\(P(Y|X) \\neq P(Y|do(X))\\) The conclusion we want to make from data is counterfactuals: What would have happened had we not do X? To teach a robot to make inference, we need inference engine p. 12 (Pearl and Mackenzie 2018) Levels of cognitive ability to be a causal learner: Seeing Doing Imagining Ladder of causation (associated with levels of cognitive ability as well): Association: conditional probability, correlation, regression Intervention Counterfactuals Level Activity Questions Examples Association \\(P(y|x)\\) Seeing What is? How would seeing X change my belief in Y? What does a symptom tell me about a disease? Intervention \\(P(y|do(x),z)\\) Doing Intervening What if? What if I do X? What if I spend more time learning, will my result change? Counterfactuals \\(P(y_x|x&#39;,y&#39;)\\) Imagining Why? was it X that caused Y? What if I had acted differently What if I stopped smoking a year ago? Table by (Pearl 2019, 57) You cannot define causation from probability alone If you say X causes Y if X raises the probability of Y.” On the surface, it might sound intuitively right. But when we translate it to probability notation: \\(P(Y|X) &gt;P(Y)\\) , it can’t be more wrong. Just because you are seeing X (1st level), it doesn’t mean the probability of Y increases. It could be either that (1) X causes Y, or (2) Z affects both X and Y. Hence, people might use control variables, which translate: \\(P(Y|X, Z=z) &gt; P(Y|Z=z)\\), then you can be more confident in your probabilistic observation. However, the question is how can you choose \\(Z\\) With the invention of the do-operator, now you can represent X causes Y as \\[ P(Y|do(X)) &gt; P(Y) \\] and with the help of causal diagram, now you can answer questions at the 2nd level (Intervention) Note: people under econometrics might still use “Granger causality” and “vector autoregression” to use the probability language to represent causality (but it’s not). The 7 tools for Structural Causal Model framework (Pearl 2019): Encoding Causal Assumptions - transparency and testability (with graphical representation) Do-calculus and the control of confounding: “back-door” The algorithmization of Counterfactuals Mediation Analysis and the Assessment of Direct and Indirect Effects Adaptability, External validity and Sample Selection Bias: are still researched under “domain adaptation”, “transfer learning” Recovering from missing data Causal Discovery: d-separation Functional decomposition (Hoyer et al. 2008) Spontaneous local changes (Pearl 2014) List of packages to do causal inference in R Simpson’s Paradox: A statistical association seen in an entire population is reversed in sub-population. Structural Causal Model accompanies graphical causal model to create more efficient language to represent causality Structural Causal Model is the solution to the curse of dimensionality (i.e., large numbers of variable \\(p\\), and small dataset \\(n\\)) thanks to product decomposition. It allows us to solve problems without knowing the function, parameters, or distributions of the error terms. Suppose you have a causal chain \\(X \\to Y \\to Z\\): \\[ P(X=x,Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y) \\] Experimental Design Quasi-experimental Design Experimentalist Observationalist Experimental Data Observational Data Random Assignment (reduce treatment imbalance) Random Sampling (reduce sample selection error) Criticisms of quasi-experimental versus experimental designs: Quasi-experimental methods don’t approximate well experimental results. For example, LaLonde (1986) shows Matching Methods, Difference-in-differences, Tobit-2 (Heckman-type) can’t approximate the experimental estimates. Tools in a hierarchical order Experimental Design: Randomized Control Trials (Gold standard): Tier 1 Quasi-experimental Regression Discontinuity Synthetic Difference-in-Differences Difference-In-Differences Synthetic Control Event Studies Fixed Effects Estimator 12.4.2.2 Endogenous Treatment: mostly Instrumental Variables Matching Methods Interrupted Time Series Endogenous Sample Selection 33.2: mostly Heckman’s correction Internal vs. External Validity Internal Validity: Economists and applied scientists largely care about. External Validity: Localness might affect your external validity. For many economic policies, there is a difference between treatment and intention to treat. For example, we might have an effective vaccine (i.e., intention to treat), but it does not mean that everybody will take it (i.e., treatment). There are four types of subjects that we deal with: Non-switchers: we don’t care about non-switchers because even if we introduce or don’t introduce the intervention, it won’t affect them. Always takers Never takers Switchers Compliers: defined as those who respect the intervention. We only care about compliers because when we introduce the intervention, they will do something. When we don’t have any interventions, they won’t do it. Tools above are used to identify the causal impact of an intervention on compliers If we have only compliers in our dataset, then intention to treatment = treatment effect. Defiers: those who will go to the opposite direction of your treatment. We typically aren’t interested in defiers because they will do the opposite of what we want them to do. And they are typically a small group; hence, we just assume they don’t exist. Treatment Assignment Control Assignment Compliers Treated No Treated Always-takers Treated Treated Never-takers Not treated No treated Defiers Not treated Treated Directional Bias due to selection into treatment comes from 2 general opposite sources Mitigation-based: select into treatment to combat a problem Preference-based: select into treatment because units like that kind of treatment. References "],["treatment-effect-types.html", "18.1 Treatment effect types", " 18.1 Treatment effect types This section is based on Paul Testa’s note Terminology: Quantities of causal interest (i.e., treatment effect types) Estimands: parameters of interest Estimators: procedures to calculate hesitates for the parameters of interest Sources of bias (according to prof. Luke Keele) \\[ \\begin{aligned} &amp;\\text{Estimator - True Causal Effect} \\\\ &amp;= \\text{Hidden bias + Misspecification bias + Statistical Noise} \\\\ &amp;= \\text{Due to design + Due to modeling + Due to finite sample} \\end{aligned} \\] 18.1.1 Average Treatment Effects Average treatment effect (ATE) is the difference in means of the treated and control groups Randomization under Experimental Design can provide an unbiased estimate of ATE. Let \\(Y_i(1)\\) denote the outcome of individual \\(i\\) under treatment and \\(Y_i(0)\\) denote the outcome of individual \\(i\\) under control Then, the treatment effect for individual \\(i\\) is the difference between her outcome under treatment and control \\[ \\tau_i = Y_i(1) - Y_i(0) \\] Without a time machine or dimension portal, we can only observe one of the two event: either individual \\(i\\) experiences the treatment or she doesn’t. Then, the ATE as a quantity of interest can come in handy since we can observe across all individuals \\[ ATE = \\frac{1}{N} \\sum_{i=1}^N \\tau_i = \\frac{\\sum_1^N Y_i(1)}{N} - \\frac{\\sum_i^N Y_i(0)}{N} \\] With random assignment (i.e., treatment assignment is independent of potential outcome and observables and unobservables), the observed means difference between the two groups is an unbiased estimator of the average treatment effect \\[ E(Y_i (1) |D = 1) = E(Y_i(1)|D=0) = E(Y_i(1)) \\\\ E(Y_i(0) |D = 1) = E(Y_i(0)|D = 0 ) = E(Y_i(0)) \\] \\[ ATE = E(Y_i(1)) - E(Y_i(0)) \\] Alternatively, we can write the potential outcomes model in a regression form \\[ Y_i = Y_i(0) + [Y_i (1) - Y_i(0)] D_i \\] Let \\(\\beta_{0i} = Y_i (0) ; \\beta_{1i} = Y_i(1) - Y_i(0)\\), we have \\[ Y_i = \\beta_{0i} + \\beta_{1i} D_i \\] where \\(\\beta_{0i}\\) = outcome if the unit did not receive any treatment \\(\\beta_{1i}\\) = treatment effect (i.e., random coefficients for each unit \\(i\\)) To understand endogeneity (i.e., nonrandom treatment assignment), we can examine a standard linear model \\[ \\begin{aligned} Y_i &amp;= \\beta_{0i} + \\beta_{1i} D_i \\\\ &amp;= ( \\bar{\\beta}_{0} + \\epsilon_{0i} ) + (\\bar{\\beta}_{1} + \\epsilon_{1i} )D_i \\\\ &amp;= \\bar{\\beta}_{0} + \\epsilon_{0i} + \\bar{\\beta}_{1} D_i + \\epsilon_{1i} D_i \\end{aligned} \\] When you have random assignment, \\(E(\\epsilon_{0i}) = E(\\epsilon_{1i}) = 0\\) No selection bias: \\(D_i \\perp e_{0i}\\) Treatment effect is independent of treatment assignment: \\(D_i \\perp e_{1i}\\) But otherwise, residuals can correlate with \\(D_i\\) For estimation, \\(\\hat{\\beta}_1^{OLS}\\) is identical to difference in means (i.e., \\(Y_i(1) - Y_i(0)\\)) In case of heteroskedasticity (i.e., \\(\\epsilon_{0i} + D_i \\epsilon_{1i} \\neq 0\\) ), this residual’s variance depends on \\(X\\) when you have heterogeneous treatment effects (i.e., \\(\\epsilon_{1i} \\neq 0\\)) Robust SE should still give consistent estimate of \\(\\hat{\\beta}_1\\) in this case Alternatively, one can use two-sample t-test on difference in means with unequal variances. 18.1.2 Conditional Average Treatment Effects Treatment effects can be different for different groups of people. In words, treatment effects can vary across subgroups. To examine the heterogeneity across groups (e.g., men vs. women), we can estimate the conditional average treatment effects (CATE) for each subgroup \\[ CATE = E(Y_i(1) - Y_i(0) |D_i, X_i)) \\] 18.1.3 Intent-to-treat Effects When we encounter non-compliance (either people suppose to receive treatment don’t receive it, or people suppose to be in the control group receive the treatment), treatment receipt is not independent of potential outcomes and confounders. In this case, the difference in observed means between the treatment and control groups is not Average Treatment Effects, but Intent-to-treat Effects (ITT). In words, ITT is the treatment effect on those who receive the treatment 18.1.4 Local Average Treatment Effects Instead of estimating the treatment effects of those who receive the treatment (i.e., Intent-to-treat Effects), you want to estimate the treatment effect of those who actually comply with the treatment. This is the local average treatment effects (LATE) or complier average causal effects (CACE). I assume we don’t use CATE to denote complier average treatment effect because it was reserved for conditional average treatment effects. Using random treatment assignment as an instrument, we can recover the effect of treatment on compliers. As the percent of compliers increases, Intent-to-treat Effects and Local Average Treatment Effects converge Rule of thumb: SE(LATE) = SE(ITT)/(share of compliers) LATE estimate is always greater than the ITT estimate LATE can also be estimated using a pure placebo group (Gerber et al. 2010). Partial compliance is hard to study, and IV/2SLS estimator is biased, we have to use Bayesian (Long, Little, and Lin 2010; Jin and Rubin 2009, 2008). 18.1.4.1 One-sided noncompliance One-sided noncompliance is when in the sample, we only have compliers and never-takers With the exclusion restriction (i.e., excludability), never-takers have the same results in the treatment or control group (i.e., never treated) With random assignment, we can have the same number of never-takers in the treatment and control groups Hence, \\[ LATE = \\frac{ITT}{\\text{share of compliers}} \\] 18.1.4.2 Two-sided noncompliance Two-sided noncompliance is when in the sample, we have compliers, never-takers, and always-takers To estimate LATE, beyond excludability like in the One-sided noncompliance case, we need to assume that there is no defiers (i.e., monotonicity assumption) (this is excusable in practical studies) \\[ LATE = \\frac{ITT}{\\text{share of compliers}} \\] 18.1.5 Population vs. Sample Average Treatment Effects See (Imai, King, and Stuart 2008) for when the sample average treatment effect (SATE) diverges from the population average treatment effect (PATE). To stay consistent, this section uses notations from (Imai, King, and Stuart 2008)’s paper. In a finite population \\(N\\), we observe \\(n\\) observations (\\(N&gt;&gt;n\\)), where half is in the control and half is in the treatment group. With unknown data generating process, we have \\[ I_i = \\begin{cases} 1 \\text{ if unit i is in the sample} \\\\ 0 \\text{ otherwise} \\end{cases} \\] \\[ T_i = \\begin{cases} 1 \\text{ if unit i is in the treatment group} \\\\ 0 \\text{ if unit i is in the control group} \\end{cases} \\] \\[ \\text{potential outcome} = \\begin{cases} Y_i(1) \\text{ if } T_i = 1 \\\\ Y_i(0) \\text{ if } T_i = 0 \\end{cases} \\] Observed outcome is \\[ Y_i | I_i = 1= T_i Y_i(1) + (1-T_i)Y_i(0) \\] Since we can never observed both outcome for the same individual, the treatment effect is always unobserved for unit \\(i\\) \\[ TE_i = Y_i(1) - Y_i(0) \\] Sample average treatment effect is \\[ SATE = \\frac{1}{n}\\sum_{i \\in \\{I_i = 1\\}} TE_i \\] Population average treatment effect is \\[ PATE = \\frac{1}{N}\\sum_{i=1}^N TE_i \\] Let \\(X_i\\) be observables and \\(U_i\\) be unobservables for unit \\(i\\) The baseline estimator for SATE and PATE is \\[ \\begin{aligned} D &amp;= \\frac{1}{n/2} \\sum_{i \\in (I_i = 1, T_i = 1)} Y_i - \\frac{1}{n/2} \\sum_{i \\in (I_i = 1 , T_i = 0)} Y_i \\\\ &amp;= \\text{observed sample mean of the treatment group} \\\\ &amp;- \\text{observed sample mean of the control group} \\end{aligned} \\] Let \\(\\Delta\\) be the estimation error (deviation from the truth), under an additive model \\[ Y_i(t) = g_t(X_i) + h_t(U_i) \\] The decomposition of the estimation error is \\[ \\begin{aligned} PATE - D = \\Delta &amp;= \\Delta_S + \\Delta_T \\\\ &amp;= (PATE - SATE) + (SATE - D)\\\\ &amp;= \\text{sample selection}+ \\text{treatment imbalance} \\\\ &amp;= (\\Delta_{S_X} + \\Delta_{S_U}) + (\\Delta_{T_X} + \\Delta_{T_U}) \\\\ &amp;= \\text{(selection on observed + selection on unobserved)} \\\\ &amp;+ (\\text{treatment imbalance in observed + unobserved}) \\end{aligned} \\] 18.1.5.1 Estimation Error from Sample Selection Also known as sample selection error \\[ \\Delta_S = PATE - SATE = \\frac{N - n}{N}(NATE - SATE) \\] where NATE is the non-sample average treatment effect (i.e., average treatment effect for those in the population but not in your sample: \\[ NATE = \\sum_{i\\in (I_i = 0)} \\frac{TE_i}{N-n} \\] From the equation, to have zero sample selection error (i.e., \\(\\Delta_S = 0\\)), we can either Get \\(N = n\\) by redefining your sample as the population of interest \\(NATE = SATE\\) (e.g., \\(TE_i\\) is constant over \\(i\\) in both your selected sample, and those in the population that you did not select) Note When you have heterogeneous treatment effects, random sampling can only warrant sample selection bias, not sample selection error. Since we can rarely know the true underlying distributions of the observables (\\(X\\)) and unobservables (\\(U\\)), we cannot verify whether the empirical distributions of your observables and unobservables for those in your sample is identical to that of your population (to reduce \\(\\Delta_S\\)). For special case, Say you have census of your population, you can adjust for the observables \\(X\\) to reduce \\(\\Delta_{S_X}\\), but still you cannot adjust your unobservables (\\(U\\)) Say you are willing to assume \\(TE_i\\) is constant over \\(X_i\\), then \\(\\Delta_{S_X} = 0\\) \\(U_i\\), then \\(\\Delta_{U}=0\\) 18.1.5.2 Estimation Error from Treatment Imbalance Also known as treatment imbalance error \\[ \\Delta_T = SATE - D \\] \\(\\Delta_T \\to 0\\) when treatment and control groups are balanced (i.e., identical empirical distributions) for both observables (\\(X\\)) and unobservables (\\(U\\)) However, in reality, we can only readjust for observables, not unobservables. Blocking Matching Methods Definition Random assignment within strata based on pre-treatment observables Dropping, repeating or grouping observations to balance covariates between the treatment and control group (Rubin 1973) Time Before randomization of treatments After randomization of treatments What if the set of covariates used to adjust is irrelevant? Nothing happens In the worst case scenario (e.g., these variables are uncorrelated with the treatment assignment, but correlated with the post-treatment variables), matching induces bias that is greater than just using the unadjusted difference in means Benefits \\(\\Delta_{T_X}=0\\) (no imbalance on observables). But we don’t know its effect on unobservables imbalance (might reduce if the unobservables are correlated with the observables) Reduce model dependence, bias, variance, mean-square error 18.1.6 Average Treatment Effects on the Treated and Control Average Effect of treatment on the Treated (ATT) is \\[ \\begin{aligned} ATT &amp;= E(Y_i(1) - Y_i(0)|D_i = 1) \\\\ &amp;= E(Y_i(1)|D_i = 1) - E(Y_i(0) |D_i = 1) \\end{aligned} \\] Average Effect of treatment on the Control (ATC) (i.e., the effect would be for those weren’t treated) is \\[ \\begin{aligned} ATC &amp;= E(Y_i(1) - Y_i (0) |D_i =0) \\\\ &amp;= E(Y_i(1)|D_i = 0) - E(Y_i(0)|D_i = 0) \\end{aligned} \\] Under random assignment and full compliance, \\[ ATE = ATT = ATC \\] Sample average treatment effect on the treated is \\[ SATT = \\frac{1}{n} \\sum_i TE_i \\] where \\(TE_i\\) is the treatment effect for unit \\(i\\) \\(n\\) is the number of treated units in the sample \\(i\\) belongs the subset (i.e., sample) of the population of interest that is treated. Population average treatment effect on the treated is \\[ PATT = \\frac{1}{N} \\sum_i TE_i \\] where \\(TE_i\\) is the treatment effect for unit \\(i\\) \\(N\\) is the number of treated units in the population \\(i\\) belongs to the population of interest that is treated. 18.1.7 Quantile Average Treatment Effects Instead of the middle point estimate (ATE), we can also understand the changes in the distribution the outcome variable due to the treatment. Using quantile regression and more assumptions (Abadie, Angrist, and Imbens 2002; Chernozhukov and Hansen 2005), we can have consistent estimate of quantile treatment effects (QTE), with which we can make inference regarding a given quantile. 18.1.8 Mediation Effects With additional assumptions (i.e., sequential ignorability (Imai, Keele, and Tingley 2010; Bullock and Ha 2011)), we can examine the mechanism of the treatment on the outcome. Under the causal framework, the indirect effect of treatment via a mediator is called average causal mediation effect (ACME) the direct effect of treatment on outcome is the average direct effect (ADE) More in the Mediation Section 36 18.1.9 Log-odds Treatment Effects For binary outcome variable, we might be interested in the log-odds of success. See (Freedman 2008) on how to estimate a consistent causal effect. Alternatively, attributable effects (Rosenbaum 2002) can also be appropriate for binary outcome. References "],["experimental-design.html", "Chapter 19 Experimental Design", " Chapter 19 Experimental Design Randomized Control Trials (RCT) or Experiments have always been and are likely to continue in the future to be the holy grail of causal inference, because of unbiased estimates elimination of confounding factors on average (covariate imbalance is always possible. Hence, you want to do Rerandomization to achieve platinum standard set by (Tukey 1993)) RCT means you have two group treatment (or experimental) gorp and control group. Hence, as you introduce the treatment (your exogenous variable) to the treatment group, the only expected difference in the outcomes of the two group should be due to the treatment. Subjects from the same population will be randomly assigned to either treatment or control group. This random assignment give us the confidence that changes in the outcome variable will be due only the treatment, not any other source (variable). It can be easier for hard science to have RCT because they can introduce the treatment, and have control environments. But it’s hard for social scientists because their subjects are usually human, and some treatment can be hard to introduce, or environments are uncontrollable. Hence, social scientists have to develop different tools (Quasi-experimental) to recover causal inference or to recreate the treatment and control group environment. With RCT, you can easily establish internal validity Even though random assignment is not the same thing as ceteris paribus (i.e., holding everything else constant), it should have the same effect (i.e., under random manipulation, other things equal can be observed, on average, across treatment and control groups). Selection Problem Assume we have binary treatment \\(D_i =(0,1)\\) outcome of interest \\(Y_i\\) for individual \\(i\\) \\(Y_{0i}\\) are those were not treated \\(Y_{1i}\\) are those were treated \\[ \\text{Potential Outcome} = \\begin{cases} Y_{1i} \\text{ if } D_i = 1 \\\\ Y_{0i} \\text{ if } D_i = 0 \\end{cases} \\] Then, what we observe in the outcome variable is \\[ Y_i = Y_{0i} + (Y_{1i} - Y_{0i})D_i \\] It’s likely that \\(Y_{1i}\\) and \\(Y_{0i}\\) both have their own distributions (i.e., different treatment effect for different people). Since we can’t see both outcomes for the same individual (unless we have a time machine), then we can only make inference regarding the average outcome of those who were treated and those who were not. \\[ \\begin{aligned} E[Y_i | D_i = 1] - E[Y_i | D_i = 0] &amp;= (E[Y_{1i} | D_i = 1] - E[Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\ &amp;= (E[Y_{1i}-Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\ \\text{Observed difference in treatment} &amp;= \\text{Average treatment effect on the treated} + \\text{Selection bias} \\end{aligned} \\] The average treatment effect is the average between between a person who is treated and the same person (in another parallel universe) who is not treated The selection bias is the difference between those who were treated and those who weren’t treated With random assignment of treatment (\\(D_i\\)) under Experimental Design, we can have \\(D_i\\) independent of potential outcomes \\[ \\begin{aligned} E[Y_i | D_i = 1] - E[Y_i|D_i = 0] &amp;= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)]\\\\ &amp;= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)] &amp;&amp; D_i \\perp Y_i \\\\ &amp;= E[Y_{1i} - Y_{0i}|D_i = 1] \\\\ &amp;= E[Y_{1i} - Y_{0i}] \\end{aligned} \\] Another representation under regression Suppose that you know the effect is \\[ Y_{1i} - Y_{0i} = \\rho \\] The observed outcome variable (for an individual) can be rewritten as \\[ \\begin{aligned} Y_i &amp;= E(Y_{0i}) + (Y_{1i}-Y_{0i})D_i + [Y_{0i} - E(Y_{0i})]\\\\ &amp;= \\alpha + \\rho D_i + \\eta_i \\end{aligned} \\] where \\(\\eta_i\\) = random variation of \\(Y_{0i}\\) Hence, the conditional expectation of an individual outcome on treatment status is \\[ \\begin{aligned} E[Y_i |D_i = 1] &amp;= \\alpha + \\rho &amp;+ E[\\eta_i |D_i = 1] \\\\ E[Y_i |D_i = 0] &amp;= \\alpha &amp;+ E[\\eta_i |D_i = 0] \\end{aligned} \\] Thus, \\[ E[Y_i |D_i = 1] - E[Y_i |D_i = 0] = \\rho + E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0] \\] where \\(E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\\) is the selection bias - correlation between the regression error term (\\(\\eta_i\\)), and the regressor (\\(D_i\\)) Under regression, we have \\[ E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0] = E[Y_{0i} |D_i = 1] -E[Y_{0i}|D_i = 0] \\] which is the difference in outcomes between those who weren’t treated get treated and those who weren’t treated stay untreated Say you have control variables (\\(X_i\\)), that is uncorrelated with the treatment (\\(D_i\\)), then you can include in your model, and it won’t (in principle) affect your estimate of the treatment effect (\\(\\rho\\)) with an added benefit of reducing the residual variance, which subsequently reduces the standard error of other estimates. \\[ Y_i = \\alpha + \\rho D_i + X_i&#39;\\gamma + \\eta_i \\] Examples: Bertrand and Mullainathan (2004) randomly assign race to a job application to study the effect of race on callbacks. References "],["notes.html", "19.1 Notes", " 19.1 Notes For outcomes with 0s, we can’t use log-like transformation, because it’s sensitive to outcome unit (J. Chen and Roth 2023). For info on this issue, check [Zero-valued Outcomes]. We should use: Percentage changes in the Average: by using Poisson QMLE, we can interpret the coefficients of the effect of treatment on the treated group relative to the mean of the control group. Extensive vs. Intensive Margins: Distinguish the treatment effect on the intensive (outcome: 10 to 11) vs. extensive margins (outcome: 0 to 1). To get the bounds for the intensive-margin, use Lee (2009) (assuming that treatment has a monotonic effect on outcome) set.seed(123) # For reproducibility library(tidyverse) n &lt;- 1000 # Number of observations p_treatment &lt;- 0.5 # Probability of being treated # Step 1: Generate the treatment variable D D &lt;- rbinom(n, 1, p_treatment) # Step 2: Generate potential outcomes # Untreated potential outcome (mostly zeroes) Y0 &lt;- rnorm(n, mean = 0, sd = 1) * (runif(n) &lt; 0.3) # Treated potential outcome (shifting both the probability of being positive - extensive margin and its magnitude - intensive margin) Y1 &lt;- Y0 + rnorm(n, mean = 2, sd = 1) * (runif(n) &lt; 0.7) # Step 3: Combine effects based on treatment Y_observed &lt;- (1 - D) * Y0 + D * Y1 # Add explicit zeroes to model situations with no effect Y_observed[Y_observed &lt; 0] &lt;- 0 data &lt;- data.frame( ID = 1:n, Treatment = D, Outcome = Y_observed, X = rnorm(n) ) |&gt; # whether outcome is positive dplyr::mutate(positive = Outcome &gt; 0) # Viewing the first few rows of the dataset head(data) #&gt; ID Treatment Outcome X positive #&gt; 1 1 0 0.0000000 1.4783345 FALSE #&gt; 2 2 1 2.2369379 -1.4067867 TRUE #&gt; 3 3 0 0.0000000 -1.8839721 FALSE #&gt; 4 4 1 3.2192276 -0.2773662 TRUE #&gt; 5 5 1 0.6649693 0.4304278 TRUE #&gt; 6 6 0 0.0000000 -0.1287867 FALSE hist(data$Outcome) Percentage changes in the Average library(fixest) res_pois &lt;- fepois( fml = Outcome ~ Treatment + X, data = data, vcov = &quot;hetero&quot; ) etable(res_pois) #&gt; res_pois #&gt; Dependent Var.: Outcome #&gt; #&gt; Constant -2.223*** (0.1440) #&gt; Treatment 2.579*** (0.1494) #&gt; X 0.0235 (0.0406) #&gt; _______________ __________________ #&gt; S.E. type Heteroskedas.-rob. #&gt; Observations 1,000 #&gt; Squared Cor. 0.33857 #&gt; Pseudo R2 0.26145 #&gt; BIC 1,927.9 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To calculate the proportional effect # proportional effect exp(coefficients(res_pois)[&quot;Treatment&quot;]) - 1 #&gt; Treatment #&gt; 12.17757 # SE exp(coefficients(res_pois)[&quot;Treatment&quot;]) * sqrt(res_pois$cov.scaled[&quot;Treatment&quot;, &quot;Treatment&quot;]) #&gt; Treatment #&gt; 1.968684 Hence, we conclude that the treatment effect is 1215% higher for the treated group as compared to the control group. Extensive vs. Intensive Margins Here, we can estimate the intensive-margin treatment effect (i.e., the treatment effect for “always-takers”). res &lt;- causalverse::lee_bounds( df = data, d = &quot;Treatment&quot;, m = &quot;positive&quot;, y = &quot;Outcome&quot;, numdraws = 10 ) |&gt; causalverse::nice_tab(2) print(res) #&gt; term estimate std.error #&gt; 1 Lower bound -0.22 0.09 #&gt; 2 Upper bound 2.77 0.14 Since in this case, the bounds contains 0, we can’t say much about the intensive margin for always-takers. If we aim to examine the sensitivity of always-takers, we should consider scenarios where the average outcome of compliers are \\(100 \\times c\\%\\) lower or higher than that of always-takers. We assume that \\(E(Y(1)|Complier) = (1-c)E(Y(1)|Always-taker)\\) set.seed(1) c_values = c(.1, .5, .7) combined_res &lt;- bind_rows(lapply(c_values, function(c) { res &lt;- causalverse::lee_bounds( df = data, d = &quot;Treatment&quot;, m = &quot;positive&quot;, y = &quot;Outcome&quot;, numdraws = 10, c_at_ratio = c ) res$c_value &lt;- as.character(c) return(res) })) combined_res |&gt; dplyr::select(c_value, everything()) |&gt; causalverse::nice_tab() #&gt; c_value term estimate std.error #&gt; 1 0.1 Point estimate 6.60 0.71 #&gt; 2 0.5 Point estimate 2.54 0.13 #&gt; 3 0.7 Point estimate 1.82 0.08 If we assume \\(c = 0.1\\) (i.e., under treatment, compliers would have an outcome equal to 10% of the outcome of always-takers), then the intensive-margin effect for always-takers is 6.6 more in the unit of the outcome. If we assume \\(c = 0.5\\) (i.e., under treatment, compliers would have an outcome equal to 50% of the outcome of always-takers), then the intensive-margin effect for always-takers is 2.54 more in the unit of the outcome. References "],["semi-random-experiment.html", "19.2 Semi-random Experiment", " 19.2 Semi-random Experiment Chicago Open Enrollment Program (Cullen, Jacob, and Levitt 2005) Students can apply to “choice” schools Many schools are oversubscribed (Demand &gt; Supply) Resolve scarcity via random lotteries Non-random enrollment, we only have random lottery which mean the above Let \\[ \\delta_j = E(Y_i | Enroll_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Enroll_{ij} = 0; Apply_{ij} = 1) \\] and \\[ \\theta_j = E(Y_i | Win_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Win_{ij} = 0; Apply_{ij} = 1) \\] Hence, we can clearly see that \\(\\delta_j \\neq \\theta_j\\) because you can only enroll, but you cannot ensure that you will win. Thus, intention to treat is different from treatment effect. Non-random enrollment, we only have random lottery which means we can only estimate \\(\\theta_j\\) To recover the true treatment effect, we can use \\[ \\delta_j = \\frac{E(Y_i|W_{ij} = 1; A_{ij} = 1) - E(Y_i | W_{ij}=0; A_{ij} = 1)}{P(Enroll_{ij} = 1| W_{ij}= 1; A_{ij}=1) - P(Enroll_{ij} = 1| W_{ij}=0; A_{ij}=1)} \\] where \\(\\delta_j\\) = treatment effect \\(W\\) = Whether students win the lottery \\(A\\) = Whether student apply for the lottery i = application j = school Say that we have 10 win Number students Type Selection effect Treatment effect Total effect 1 Always Takers +0.2 +1 +1.2 2 Compliers 0 +1 +1 7 Never Takers -0.1 0 -0.1 10 lose Number students Type Selection effect Treatment effect Total effect 1 Always Takers +0.2 +1 +1.2 2 Compliers 0 0 0 7 Never Takers -0.1 0 -0.1 Intent to treatment = Average effect of who you give option to choose \\[ \\begin{aligned} E(Y_i | W_{ij}=1; A_{ij} = 1) &amp;= \\frac{1*(1.2)+ 2*(1) + 7 * (-0.1)}{10}\\\\ &amp;= 0.25 \\end{aligned} \\] \\[ \\begin{aligned} E(Y_i | W_{ij}=0; A_{ij} = 1) &amp;= \\frac{1*(1.2)+ 2*(0) + 7 * (-0.1)}{10}\\\\ &amp;= 0.05 \\end{aligned} \\] Hence, \\[ \\begin{aligned} \\text{Intent to treatment} &amp;= 0.25 - 0.05 = 0.2 \\\\ \\text{Treatment effect} &amp;= 1 \\end{aligned} \\] \\[ \\begin{aligned} P(Enroll_{ij} = 1 | W_{ij} = 1; A_{ij}=1 ) &amp;= \\frac{1+2}{10} = 0.3 \\\\ P(Enroll_{ij} = 1 | W_{ij} = 0; A_{ij}=1 ) &amp;= \\frac{1}{10} = 0.1 \\end{aligned} \\] \\[ \\text{Treatment effect} = \\frac{0.2}{0.3-0.1} = 1 \\] After knowing how to recover the treatment effect, we turn our attention to the main model \\[ Y_{ia} = \\delta W_{ia} + \\lambda L_{ia} + e_{ia} \\] where \\(W\\) = whether a student wins a lottery \\(L\\) = whether student enrolls in the lottery \\(\\delta\\) = intent to treat Hence, Conditional on lottery, the \\(\\delta\\) is valid But without lottery, your \\(\\delta\\) is not random Winning and losing are only identified within lottery Each lottery has multiple entries. Thus, we can have within estimator We can also include other control variables (\\(X_i \\theta\\)) \\[ Y_{ia} = \\delta_1 W_{ia} + \\lambda_1 L_{ia} + X_i \\theta + u_{ia} \\] \\[ \\begin{aligned} E(\\delta) &amp;= E(\\delta_1) \\\\ E(\\lambda) &amp;\\neq E(\\lambda_1) &amp;&amp; \\text{because choosing a lottery is not random} \\end{aligned} \\] Including \\((X_i \\theta)\\) just shifts around control variables (i.e., reweighting of lottery), which would not affect your treatment effect \\(E(\\delta)\\) References "],["rerandomization.html", "19.3 Rerandomization", " 19.3 Rerandomization Since randomization only balances baseline covariates on average, imbalance in variables due to random chance can still happen. In case that you have a “bad” randomization (i.e., imbalance for important baseline covariates), (Morgan and Rubin 2012) introduce the idea of rerandomization. Rerandomization is checking balance during the randomization process (before the experiment), to eliminate bad allocation (i.e., those with unacceptable balance). The greater the number of variables, the greater the likelihood that at least one covariate would be imbalanced across treatment groups. Example: For 10 covariates, the probability of a significant difference at \\(\\alpha = .05\\) for at least one covariate is \\(1 - (1-.05)^{10} = 0.4 = 40\\%\\) Rerandomization increase treatment effect estimate precision if the covariates are correlated with the outcome. Improvement in precision for treatment effect estimate depends on (1) improvement in covariate balance and (2) correlation between covariates and the outcome. You also need to take into account rerandomization into your analysis when making inference. Rerandomization is equivalent to increasing our sample size. Alternatives include Stratified randomization (Johansson and Schultzberg 2022) Matched randomization (Greevy et al. 2004; Kapelner and Krieger 2014) Minimization (Pocock and Simon 1975) Rerandomization Criterion Acceptable randomization is based on a function of covariate matrix \\(\\mathbf{X}\\) and vector of treatment assignments \\(\\mathbf{W}\\) \\[ W_i = \\begin{cases} 1 \\text{ if treated} \\\\ 0 \\text{ if control} \\end{cases} \\] Mahalanobis Distance, \\(M\\), can be used as criteria for acceptable balance Let \\(M\\) be the multivariate distance between groups means \\[ \\begin{aligned} M &amp;= (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)&#39; cov(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)^{-1} (\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C) \\\\ &amp;= (\\frac{1}{n_T}+ \\frac{1}{n_C})^{-1}(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C)&#39; cov(\\mathbf{X})^{-1}(\\bar{\\mathbf{X}}_T - \\bar{\\mathbf{X}}_C) \\end{aligned} \\] With large sample size and “pure” randomization \\(M \\sim \\chi^2_k\\) where \\(k\\) is the number of covariates to be balanced Then let \\(p_a\\) be the probability of accepting a randomization. Choosing appropriate \\(p_a\\) is a tradeoff between balance and time. Then the rule of thumb is re-randomize when \\(M &gt; a\\) References "],["two-stage-randomized-experiments-with-interference-and-noncompliance.html", "19.4 Two-Stage Randomized Experiments with Interference and Noncompliance", " 19.4 Two-Stage Randomized Experiments with Interference and Noncompliance (Imai, Jiang, and Malani 2021) References "],["sampling.html", "Chapter 20 Sampling ", " Chapter 20 Sampling "],["simple-sampling.html", "20.1 Simple Sampling", " 20.1 Simple Sampling Simple (random) Sampling library(dplyr) iris_df &lt;- iris set.seed(1) sample_n(iris_df, 10) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.8 2.7 4.1 1.0 versicolor #&gt; 2 6.4 2.8 5.6 2.1 virginica #&gt; 3 4.4 3.2 1.3 0.2 setosa #&gt; 4 4.3 3.0 1.1 0.1 setosa #&gt; 5 7.0 3.2 4.7 1.4 versicolor #&gt; 6 5.4 3.0 4.5 1.5 versicolor #&gt; 7 5.4 3.4 1.7 0.2 setosa #&gt; 8 7.6 3.0 6.6 2.1 virginica #&gt; 9 6.1 2.8 4.7 1.2 versicolor #&gt; 10 4.6 3.4 1.4 0.3 setosa library(sampling) # set unique id number for each row iris_df$id = 1:nrow(iris_df) # Simple random sampling with replacement srswor(10, length(iris_df$id)) #&gt; [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 #&gt; [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 #&gt; [75] 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 #&gt; [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [149] 0 0 # Simple random sampling without replacement (sequential method) srswor1(10, length(iris_df$id)) #&gt; [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [75] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [112] 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 #&gt; [149] 0 0 # Simple random sampling with replacement srswr(10, length(iris_df$id)) #&gt; [1] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 #&gt; [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 #&gt; [112] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 #&gt; [149] 0 0 library(survey) data(&quot;api&quot;) srs_design &lt;- svydesign(data = apistrat, weights = ~pw, fpc = ~fpc, id = ~1) library(sampler) rsamp(albania, n = 260, over = 0.1, # desired oversampling proportion rep = F) Identify missing points between sample and collected data alsample &lt;- rsamp(df = albania, 544) alreceived &lt;- rsamp(df = alsample, 390) rmissing(sampdf = alsample, colldf = alreceived, col_name = qvKod) "],["stratified-sampling.html", "20.2 Stratified Sampling", " 20.2 Stratified Sampling A stratum is a subset of the population that has at least one common characteristic. Steps: Identify relevant stratums and their representation in the population. Randomly sample to select a sufficient number of subjects from each stratum. Stratified sampling reduces sampling error. library(dplyr) # by number of rows sample_iris &lt;- iris %&gt;% group_by(Species) %&gt;% sample_n(5) sample_iris #&gt; # A tibble: 15 × 5 #&gt; # Groups: Species [3] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 4.4 3 1.3 0.2 setosa #&gt; 2 5.2 3.5 1.5 0.2 setosa #&gt; 3 5.1 3.8 1.5 0.3 setosa #&gt; 4 5.2 3.4 1.4 0.2 setosa #&gt; 5 4.5 2.3 1.3 0.3 setosa #&gt; 6 5.5 2.5 4 1.3 versicolor #&gt; 7 7 3.2 4.7 1.4 versicolor #&gt; 8 6.7 3 5 1.7 versicolor #&gt; 9 6.1 2.9 4.7 1.4 versicolor #&gt; 10 5.5 2.4 3.8 1.1 versicolor #&gt; 11 6.4 2.7 5.3 1.9 virginica #&gt; 12 6.4 2.8 5.6 2.1 virginica #&gt; 13 6.4 3.2 5.3 2.3 virginica #&gt; 14 6.8 3.2 5.9 2.3 virginica #&gt; 15 7.2 3.6 6.1 2.5 virginica # by fraction sample_iris &lt;- iris %&gt;% group_by(Species) %&gt;% sample_frac(size = .15) sample_iris #&gt; # A tibble: 24 × 5 #&gt; # Groups: Species [3] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.5 4.2 1.4 0.2 setosa #&gt; 2 5 3 1.6 0.2 setosa #&gt; 3 5.2 4.1 1.5 0.1 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.1 3.7 1.5 0.4 setosa #&gt; 6 4.8 3.4 1.9 0.2 setosa #&gt; 7 5.1 3.3 1.7 0.5 setosa #&gt; 8 5.5 3.5 1.3 0.2 setosa #&gt; 9 5 2.3 3.3 1 versicolor #&gt; 10 5.6 2.9 3.6 1.3 versicolor #&gt; # ℹ 14 more rows library(sampler) # Stratified sample using proportional allocation without replacement ssamp(df=albania, n=360, strata=qarku, over=0.1) #&gt; # A tibble: 395 × 45 #&gt; qarku Q_ID bashkia BAS_ID zaz njesiaAdministrative COM_ID qvKod zgjedhes #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Berat 1 Berat 11 ZAZ … &quot;Berat &quot; 1101 &quot;\\&quot;3… 558 #&gt; 2 Berat 1 Berat 11 ZAZ … &quot;Berat &quot; 1101 &quot;\\&quot;3… 815 #&gt; 3 Berat 1 Berat 11 ZAZ … &quot;Sinje&quot; 1108 &quot;\\&quot;3… 419 #&gt; 4 Berat 1 Kucove 13 ZAZ … &quot;Lumas&quot; 1104 &quot;\\&quot;3… 237 #&gt; 5 Berat 1 Kucove 13 ZAZ … &quot;Kucove&quot; 1201 &quot;\\&quot;3… 562 #&gt; 6 Berat 1 Skrapar 17 ZAZ … &quot;Corovode&quot; 1303 &quot;\\&quot;3… 829 #&gt; 7 Berat 1 Berat 11 ZAZ … &quot;Roshnik&quot; 1107 &quot;\\&quot;3… 410 #&gt; 8 Berat 1 Ura Vajg… 19 ZAZ … &quot;Ura Vajgurore&quot; 1110 &quot;\\&quot;3… 708 #&gt; 9 Berat 1 Kucove 13 ZAZ … &quot;Perondi&quot; 1203 &quot;\\&quot;3… 835 #&gt; 10 Berat 1 Kucove 13 ZAZ … &quot;Kucove&quot; 1201 &quot;\\&quot;3… 907 #&gt; # ℹ 385 more rows #&gt; # ℹ 36 more variables: meshkuj &lt;int&gt;, femra &lt;int&gt;, totalSeats &lt;int&gt;, #&gt; # vendndodhja &lt;fct&gt;, ambienti &lt;fct&gt;, totalVoters &lt;int&gt;, femVoters &lt;int&gt;, #&gt; # maleVoters &lt;int&gt;, unusedBallots &lt;int&gt;, damagedBallots &lt;int&gt;, #&gt; # ballotsCast &lt;int&gt;, invalidVotes &lt;int&gt;, validVotes &lt;int&gt;, lsi &lt;int&gt;, #&gt; # ps &lt;int&gt;, pkd &lt;int&gt;, sfida &lt;int&gt;, pr &lt;int&gt;, pd &lt;int&gt;, pbdksh &lt;int&gt;, #&gt; # adk &lt;int&gt;, psd &lt;int&gt;, ad &lt;int&gt;, frd &lt;int&gt;, pds &lt;int&gt;, pdiu &lt;int&gt;, … Identify number of missing points by strata between sample and collected data alsample &lt;- rsamp(df = albania, 544) alreceived &lt;- rsamp(df = alsample, 390) smissing( sampdf = alsample, colldf = alreceived, strata = qarku, col_name = qvKod ) "],["unequal-probability-sampling.html", "20.3 Unequal Probability Sampling", " 20.3 Unequal Probability Sampling UPbrewer() UPmaxentropy() UPmidzuno() UPmidzunopi2() UPmultinomial() UPpivotal() UPrandompivotal() UPpoisson() UPsampford() UPsystematic() UPrandomsystematic() UPsystematicpi2() UPtille() UPtillepi2() "],["balanced-sampling.html", "20.4 Balanced Sampling", " 20.4 Balanced Sampling Purpose: to get the same means in the population and the sample for all the auxiliary variables Balanced sampling is different from purposive selection Balancing equations \\[ \\sum_{k \\in S} \\frac{\\mathbf{x}_k}{\\pi_k} = \\sum_{k \\in U} \\mathbf{x}_k \\] where \\(\\mathbf{x}_k\\) is a vector of auxiliary variables 20.4.1 Cube flight phase landing phase samplecube() fastflightcube() landingcube() 20.4.2 Stratification Try to replicate the population based on the original multivariate histogram library(survey) data(&quot;api&quot;) srs_design &lt;- svydesign(data = apistrat, weights = ~pw, fpc = ~fpc, strata = ~stype, id = ~1) balancedstratification() 20.4.3 Cluster library(survey) data(&quot;api&quot;) srs_design &lt;- svydesign(data = apiclus1, weights = ~pw, fpc = ~fpc, id = ~dnum) balancedcluster() 20.4.4 Two-stage library(survey) data(&quot;api&quot;) srs_design &lt;- svydesign(data = apiclus2, fpc = ~fpc1 + fpc2, id = ~ dnum + snum) balancedtwostage() "],["analysis-of-variance-anova.html", "Chapter 21 Analysis of Variance (ANOVA)", " Chapter 21 Analysis of Variance (ANOVA) ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more useful in the case with qualitative variables and designed experiments. Experimental Design Factor: explanatory or predictor variable to be studied in an investigation Treatment (or Factor Level): “value” of a factor applied to the experimental unit Experimental Unit: person, animal, piece of material, etc. that is subjected to treatment(s) and provides a response Single Factor Experiment: one explanatory variable considered Multifactor Experiment: more than one explanatory variable Classification Factor: A factor that is not under the control of the experimenter (observational data) Experimental Factor: assigned by the experimenter Basics of experimental design: Choices that a statistician has to make: set of treatments set of experimental units treatment assignment (selection bias) measurement (measurement bias, blind experiments) Advancements in experimental design: Factorial Experiments: consider multiple factors at the same time (interaction) Replication: repetition of experiment assess mean squared error control over precision of experiment (power) Randomization Before R.A. Fisher (1900s), treatments were assigned systematically or subjectively randomization: assign treatments to experimental units at random, which averages out systematic effects that cannot be control by the investigator Local control: Blocking or Stratification Reduce experimental errors and increase power by placing restrictions on the randomization of treatments to experimental units. Randomization may also eliminate correlations due to time and space. "],["completely-randomized-design-crd.html", "21.1 Completely Randomized Design (CRD)", " 21.1 Completely Randomized Design (CRD) Treatment factor A with \\(a\\ge2\\) treatments levels. Experimental units are randomly assigned to each treatment. The number of experimental units in each group can be equal (balanced): n unequal (unbalanced): \\(n_i\\) for the i-th group (i = 1,…,a). The total sample size is \\(N=\\sum_{i=1}^{a}n_i\\) Possible assignments of units to treatments are \\(k=\\frac{N!}{n_1!n_2!...n_a!}\\) Each has probability 1/k of being selected. Each experimental unit is measured with a response \\(Y_{ij}\\), in which j denotes unit and i denotes treatment. Treatment 1 2 … a \\(Y_{11}\\) \\(Y_{21}\\) … \\(Y_{a1}\\) \\(Y_{12}\\) … … … … … … … Sample Mean \\(\\bar{Y_{1.}}\\) \\(\\bar{Y_{2.}}\\) … \\(\\bar{Y_{a.}}\\) Sample SD \\(s_1\\) \\(s_2\\) … \\(s_a\\) where \\(\\bar{Y_{i.}}=\\frac{1}{n_i}\\sum_{j=1}^{n_i}Y_{ij}\\) \\(s_i^2=\\frac{1}{n_i-1}\\sum_{j=1}^{n_i}(Y_{ij}-\\bar{Y_i})^2\\) And the grand mean is \\(\\bar{Y_{..}}=\\frac{1}{N}\\sum_{i}\\sum_{j}Y_{ij}\\) 21.1.1 Single Factor Fixed Effects Model also known as Single Factor (One-Way) ANOVA or ANOVA Type I model. Partitioning the Variance The total variability of the \\(Y_{ij}\\) observation can be measured as the deviation of \\(Y_{ij}\\) around the overall mean \\(\\bar{Y_{..}}\\): \\(Y_{ij} - \\bar{Y_{..}}\\) This can be rewritten as: \\[ \\begin{aligned} Y_{ij} - \\bar{Y_{..}}&amp;=Y_{ij} - \\bar{Y_{..}} + \\bar{Y_{i.}} - \\bar{Y_{i.}} \\\\ &amp;= (\\bar{Y_{i.}}-\\bar{Y_{..}})+(Y_{ij}-\\bar{Y_{i.}}) \\end{aligned} \\] where the first term is the between treatment differences (i.e., the deviation of the treatment mean from the overall mean) the second term is within treatment differences (i.e., the deviation of the observation around its treatment mean) \\[ \\begin{aligned} \\sum_{i}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})^2 &amp;= \\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2+\\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2 \\\\ SSTO &amp;= SSTR + SSE \\\\ total~SS &amp;= treatment~SS + error~SS \\\\ (N-1)~d.f. &amp;= (a-1)~d.f. + (N - a) ~ d.f. \\end{aligned} \\] we lose a d.f. for the total corrected SSTO because of the estimation of the mean (\\(\\sum_{i}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})=0\\)) And, for the SSTR \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})=0\\) Accordingly, \\(MSTR= \\frac{SST}{a-1}\\) and \\(MSR=\\frac{SSE}{N-a}\\) ANOVA Table Source of Variation SS df MS Between Treatments \\(\\sum_{i}n_i (\\bar{Y_{i.}}-\\bar{Y_{..}})^2\\) a-1 SSTR/(a-1) Error (within treatments) \\(\\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2\\) N-a SSE/(N-a) Total (corrected) \\(\\sum_{i}n_i (\\bar{Y_{i.}}-\\bar{Y_{..}})^2\\) N-1 Linear Model Explanation of ANOVA 21.1.1.1 Cell means model \\[ Y_{ij}=\\mu_i+\\epsilon\\_{ij} \\] where \\(Y_{ij}\\) response variable in \\(j\\)-th subject for the \\(i\\)-th treatment \\(\\mu_i\\): parameters (fixed) representing the unknown population mean for the i-th treatment \\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors \\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\) All observations have the same variance Example: \\(a = 3\\) (3 treatments) \\(n_1=n_2=n_3=2\\) \\[ \\begin{aligned} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{aligned} \\] \\(X_{k,ij}=1\\) if the \\(k\\)-th treatment is used \\(X_{k,ij}=0\\) Otherwise Note: no intercept term. \\[\\begin{equation} \\begin{aligned} \\mathbf{b}= \\left[\\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} n_1 &amp; 0 &amp; 0\\\\ 0 &amp; n_2 &amp; 0\\\\ 0 &amp; 0 &amp; n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_1\\\\ Y_2\\\\ Y_3\\\\ \\end{array}\\right] \\\\ &amp; = \\left[\\begin{array}{c} \\bar{Y_1}\\\\ \\bar{Y_2}\\\\ \\bar{Y_3}\\\\ \\end{array}\\right] \\end{aligned} \\tag{21.1} \\end{equation}\\] is the BLUE (best linear unbiased estimator) for \\(\\beta=[\\mu_1 \\mu_2\\mu_3]&#39;\\) \\[ E(\\mathbf{b})=\\beta \\] \\[ var(\\mathbf{b})=\\sigma^2(\\mathbf{X&#39;X})^{-1}=\\sigma^2 \\left[\\begin{array}{ccc} 1/n_1 &amp; 0 &amp; 0\\\\ 0 &amp; 1/n_2 &amp; 0\\\\ 0 &amp; 0 &amp; 1/n_3\\\\ \\end{array}\\right] \\] \\(var(b_i)=var(\\hat{\\mu_i})=\\sigma^2/n_i\\) where \\(\\mathbf{b} \\sim N(\\beta,\\sigma^2(\\mathbf{X&#39;X})^{-1})\\) \\[ \\begin{aligned} MSE &amp;= \\frac{1}{N-a} \\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2 \\\\ &amp;= \\frac{1}{N-a} \\sum_{i}[(n_i-1)\\frac{\\sum_{i}(Y_{ij}-\\bar{Y_{i.}})^2}{n_i-1}] \\\\ &amp;= \\frac{1}{N-a} \\sum_{i}(n_i-1)s_1^2 \\end{aligned} \\] We have \\(E(s_i^2)=\\sigma^2\\) \\(E(MSE)=\\frac{1}{N-a}\\sum_{i}(n_i-1)\\sigma^2=\\sigma^2\\) Hence, MSE is an unbiased estimator of \\(\\sigma^2\\), regardless of whether the treatment means are equal or not. \\(E(MSTR)=\\sigma^2+\\frac{\\sum_{i}n_i(\\mu_i-\\mu_.)^2}{a-1}\\) where \\(\\mu_.=\\frac{\\sum_{i=1}^{a}n_i\\mu_i}{\\sum_{i=1}^{a}n_i}\\) If all treatment means are equals (=\\(\\mu_.\\)), \\(E(MSTR)=\\sigma^2\\). Then we can use an \\(F\\)-test for the equality of all treatment means: \\[H_0:\\mu_1=\\mu_2=..=\\mu_a\\] \\[H_a: not~al l~ \\mu_i ~ are ~ equal \\] \\(F=\\frac{MSTR}{MSE}\\) where large values of F support \\(H_a\\) (since MSTR will tend to exceed MSE when \\(H_a\\) holds) and F near 1 support \\(H_0\\) (upper tail test) Equivalently, when \\(H_0\\) is true, \\(F \\sim f_{(a-1,N-a)}\\) If \\(F \\leq f_{(a-1,N-a;1-\\alpha)}\\), we cannot reject \\(H_0\\) If \\(F \\geq f_{(a-1,N-a;1-\\alpha)}\\), we reject \\(H_0\\) Note: If \\(a = 2\\) (2 treatments), \\(F\\)-test = two sample \\(t\\)-test 21.1.1.2 Treatment Effects (Factor Effects) Besides Cell means model, we have another way to formalize one-way ANOVA: \\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\] where \\(Y_{ij}\\) is the \\(j\\)-th response for the \\(i\\)-th treatment \\(\\tau_i\\) is \\(i\\)-th treatment effect \\(\\mu\\) constant component, common to all observations \\(\\epsilon_{ij}\\) independent random errors ~ \\(N(0,\\sigma^2)\\) For example, \\(a = 3\\), \\(n_1=n_2=n_3=2\\) \\[\\begin{equation} \\begin{aligned} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{cccc} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\tau_3\\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{aligned} \\tag{21.2} \\end{equation}\\] However, \\[ \\mathbf{X&#39;X} = \\left( \\begin{array} {cccc} \\sum_{i}n_i &amp; n_1 &amp; n_2 &amp; n_3 \\\\ n_1 &amp; n_1 &amp; 0 &amp; 0 \\\\ n_2 &amp; 0 &amp; n_2 &amp; 0 \\\\ n_3 &amp; 0 &amp; 0 &amp; n_3 \\\\ \\end{array} \\right) \\] is singular thus does not exist, \\(\\mathbf{b}\\) is insolvable (infinite solutions) Hence, we have to impose restrictions on the parameters to a model matrix \\(\\mathbf{X}\\) of full rank. Whatever restriction we use, we still have: \\(E(Y_{ij})=\\mu + \\tau_i = \\mu_i = mean ~ response ~ for ~ i-th ~ treatment\\) 21.1.1.2.1 Restriction on sum of tau \\(\\sum_{i=1}^{a}\\tau_i=0\\) implies \\[ \\mu= \\mu +\\frac{1}{a}\\sum_{i=1}^{a}(\\mu+\\tau_i) \\] is the average of the treatment mean (grand mean) (overall mean) \\[ \\begin{aligned} \\tau_i &amp;=(\\mu+\\tau_i) -\\mu = \\mu_i-\\mu \\\\ &amp;= \\text{treatment mean} - \\text{grand~mean} \\\\ &amp;= \\text{treatment effect} \\end{aligned} \\] \\[ \\tau_a=-\\tau_1-\\tau_2-...-\\tau_{a-1} \\] Hence, the mean for the a-th treatment is \\[ \\mu_a=\\mu+\\tau_a=\\mu-\\tau_1-\\tau_2-...-\\tau_{a-1} \\] Hence, the model need only “a” parameters: \\[ \\mu,\\tau_1,\\tau_2,..,\\tau_{a-1} \\] Equation (21.2) becomes \\[\\begin{equation} \\begin{aligned} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{ccc} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; -1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{aligned} \\end{equation}\\] where \\(\\beta\\equiv[\\mu,\\tau_1,\\tau_2]&#39;\\) Equation (21.1) with \\(\\sum_{i}\\tau_i=0\\) becomes \\[ \\begin{aligned} \\mathbf{b}= \\left[\\begin{array}{c} \\hat{\\mu} \\\\ \\hat{\\tau_1} \\\\ \\hat{\\tau_2} \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} \\sum_{i}n_i &amp; n_1-n_3 &amp; n_2-n_3\\\\ n_1-n_3 &amp; n_1+n_3 &amp; n_3\\\\ n_2-n_3 &amp; n_3 &amp; n_2-n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_{..}\\\\ Y_{1.}-Y_{3.}\\\\ Y_{2.}-Y_{3.}\\\\ \\end{array}\\right] \\\\ &amp; = \\left[\\begin{array}{c} \\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\bar{Y_{1.}}-\\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\bar{Y_{2.}}-\\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\end{array}\\right]\\\\ &amp; = \\left[\\begin{array}{c} \\hat{\\mu}\\\\ \\hat{\\tau_1}\\\\ \\hat{\\tau_2}\\\\ \\end{array}\\right] \\end{aligned} \\] and \\(\\hat{\\tau_3}=-\\hat{\\tau_1}-\\hat{\\tau_2}=\\bar{Y_3}-\\frac{1}{3} \\sum_{i}\\bar{Y_{i.}}\\) 21.1.1.2.2 Restriction on first tau In R, lm() uses the restriction \\(\\tau_1=0\\) For the previous example, for \\(n_1=n_2=n_3=2\\), and \\(\\tau_1=0\\). Then the treatment means can be written as: \\[ \\begin{aligned} \\mu_1 &amp;= \\mu + \\tau_1 = \\mu + 0 = \\mu \\\\ \\mu_2 &amp;= \\mu + \\tau_2 \\\\ \\mu_3 &amp;= \\mu + \\tau_3 \\end{aligned} \\] Hence, \\(\\mu\\) is the mean response for the first treatment In the matrix form, \\[ \\begin{aligned} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_2 \\\\ \\tau_3 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{aligned} \\] \\(\\beta = [\\mu,\\tau_2,\\tau_3]&#39;\\) \\[ \\begin{aligned} \\mathbf{b}= \\left[\\begin{array}{c} \\hat{\\mu} \\\\ \\hat{\\tau_2} \\\\ \\hat{\\tau_3} \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} \\sum_{i}n_i &amp; n_2 &amp; n_3\\\\ n_2 &amp; n_2 &amp; 0\\\\ n_3 &amp; 0 &amp; n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_{..}\\\\ Y_{2.}\\\\ Y_{3.}\\\\ \\end{array}\\right] \\\\ &amp; = \\left[ \\begin{array}{c} \\bar{Y_{1.}} \\\\ \\bar{Y_{2.}} - \\bar{Y_{1.}} \\\\ \\bar{Y_{3.}} - \\bar{Y_{1.}}\\\\ \\end{array}\\right] \\end{aligned} \\] \\[ E(\\mathbf{b})= \\beta = \\left[\\begin{array}{c} {\\mu}\\\\ {\\tau_2}\\\\ {\\tau_3}\\\\ \\end{array}\\right] = \\left[\\begin{array}{c} \\mu_1\\\\ \\mu_2-\\mu_1\\\\ \\mu_3-\\mu_1\\\\ \\end{array}\\right] \\] \\[ \\begin{aligned} var(\\mathbf{b}) &amp;= \\sigma^2(\\mathbf{X&#39;X})^{-1} \\\\ var(\\hat{\\mu}) &amp;= var(\\bar{Y_{1.}})=\\sigma^2/n_1 \\\\ var(\\hat{\\tau_2}) &amp;= var(\\bar{Y_{2.}}-\\bar{Y_{1.}}) = \\sigma^2/n_2 + \\sigma^2/n_1 \\\\ var(\\hat{\\tau_3}) &amp;= var(\\bar{Y_{3.}}-\\bar{Y_{1.}}) = \\sigma^2/n_3 + \\sigma^2/n_1 \\end{aligned} \\] Note For all three parameterization, the ANOVA table is the same Model 1: \\(Y_{ij} = \\mu_i + \\epsilon_{ij}\\) Model 2: \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) where \\(\\sum_{i} \\tau_i=0\\) Model 3: \\(Y_{ij}= \\mu + \\tau_i + \\epsilon_{ij}\\) where \\(\\tau_1=0\\) All models have the same calculation for \\(\\hat{Y}\\) as \\[ \\mathbf{\\hat{Y} = X(X&#39;X)^{-1}X&#39;Y=PY = Xb} \\] ANOVA Table Source of Variation SS df MS F Between Treatments \\(\\sum_{i} n _ i (\\bar { Y_ {i .} } -\\bar{Y_{..}})^2 = \\mathbf{Y &#39; (P-P_1)Y}\\) a-1 \\(\\frac{SSTR}{a-1}\\) \\(\\frac{MSTR}{MSE}\\) Error (within treatments) \\(\\sum_{i}\\sum_{j}(Y_{ij} -\\bar{Y_{i.}})^2=\\mathbf{e&#39;e}\\) N-a \\(\\frac{SSE}{N-a}\\) Total (corrected) \\(\\sum_{i } n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2=\\mathbf{Y&#39;Y - Y&#39;P_1Y}\\) N-1 where \\(\\mathbf{P_1} = \\frac{1}{n}\\mathbf{J}\\) The \\(F\\)-statistic here has \\((a-1,N-a)\\) degrees of freedom, which gives the same value for all three parameterization, but the hypothesis test is written a bit different: \\[ \\begin{aligned} &amp;H_0 : \\mu_1 = \\mu_2 = ... = \\mu_a \\\\ &amp;H_0 : \\mu + \\tau_1 = \\mu + \\tau_2 = ... = \\mu + \\tau_a \\\\ &amp;H_0 : \\tau_1 = \\tau_2 = ...= \\tau_a \\end{aligned} \\] The \\(F\\)-test here serves as a preliminary analysis, to see if there is any difference at different factors. For more in-depth analysis, we consider different testing of treatment effects. 21.1.1.3 Testing of Treatment Effects A Single Treatment Mean \\(\\mu_i\\) A Differences Between Treatment Means A Contrast Among Treatment Means A Linear Combination of Treatment Means 21.1.1.3.1 Single Treatment Mean We have \\(\\hat{\\mu_i}=\\bar{Y_{i.}}\\) where \\(E(\\bar{Y_{i.}})=\\mu_i\\) \\(var(\\bar{Y_{i}})=\\sigma^2/n_i\\) estimated by \\(s^2(\\bar{Y_{i.}})=MSE / n_i\\) Since \\(\\frac{\\bar{Y_{i.}}-\\mu_i}{s(\\bar{Y_{i.}})} \\sim t_{N-a}\\) and the confidence interval for \\(\\mu_i\\) is \\(\\bar{Y_{i.}} \\pm t_{1-\\alpha/2;N-a}s(\\bar{Y_{i.}})\\), then we can do a t-test for the means difference with some constant \\(c\\) \\[ \\begin{aligned} &amp;H_0: \\mu_i = c \\\\ &amp;H_1: \\mu_i \\neq c \\end{aligned} \\] where \\[ T =\\frac{\\bar{Y_{i.}}-c}{s(\\bar{Y_{i.}})} \\] follows \\(t_{N-a}\\) when \\(H_0\\) is true. If \\(|T| &gt; t_{1-\\alpha/2;N-a}\\), we can reject \\(H_0\\) 21.1.1.3.2 Differences Between Treatment Means Let \\(D=\\mu_i - \\mu_i&#39;\\), also known as pairwise comparison \\(D\\) can be estimated by \\(\\hat{D}=\\bar{Y_{i}}-\\bar{Y_{i}}&#39;\\) is unbiased (\\(E(\\hat{D})=\\mu_i-\\mu_i&#39;\\)) Since \\(\\bar{Y_{i}}\\) and \\(\\bar{Y_{i}}&#39;\\) are independent, then \\[ var(\\hat{D})=var(\\bar{Y_{i}}) + var(\\bar{Y_{i&#39;}}) = \\sigma^2(1/n_i + 1/n_i&#39;) \\] can be estimated with \\[ s^2(\\hat{D}) = MSE(1/n_i + 1/n_i&#39;) \\] With the single treatment inference, \\[ \\frac{\\hat{D}-D}{s(\\hat{D})} \\sim t_{N-a} \\] hence, \\[ \\hat{D} \\pm t_{(1-\\alpha/2;N-a)}s(\\hat{D}) \\] Hypothesis tests: \\[ \\begin{aligned} &amp;H_0: \\mu_i = \\mu_i&#39; \\\\ &amp;H_a: \\mu_i \\neq \\mu_i&#39; \\end{aligned} \\] can be tested by the following statistic \\[ T = \\frac{\\hat{D}}{s(\\hat{D})} \\sim t_{1-\\alpha/2;N-a} \\] reject \\(H_0\\) if \\(|T| &gt; t_{1-\\alpha/2;N-a}\\) 21.1.1.3.3 Contrast Among Treatment Means generalize the comparison of two means, we have contrasts A contrast is a linear combination of treatment means: \\[ L = \\sum_{i=1}^{a}c_i \\mu_i \\] where each \\(c_i\\) is non-random constant and sum to 0: \\[ \\sum_{i=1}^{a} c_i = 0 \\] An unbiased estimator of a contrast L is \\[ \\hat{L} = \\sum_{i=1}^{a}c_i \\bar{Y}_{i.} \\] and \\(E(\\hat{L}) = L\\). Since the \\(\\bar{Y}_{i.}\\), i = 1,…, a are independent. \\[ \\begin{aligned} var(\\hat{L}) &amp;= var(\\sum_{i=1}^a c_i \\bar{Y}_{i.}) = \\sum_{i=1}^a var(c_i \\bar{Y}_i) \\\\ &amp;= \\sum_{i=1}^a c_i^2 var(\\bar{Y}_i) = \\sum_{i=1}^a c_i^2 \\sigma^2 /n_i \\\\ &amp;= \\sigma^2 \\sum_{i=1}^{a} c_i^2 /n_i \\end{aligned} \\] Estimation of the variance: \\[ s^2(\\hat{L}) = MSE \\sum_{i=1}^{a} \\frac{c_i^2}{n_i} \\] \\(\\hat{L}\\) is normally distributed (since it is a linear combination of independent normal random variables). Then, since \\(SSE/\\sigma^2\\) is \\(\\chi_{N-a}^2\\) \\[ \\frac{\\hat{L}-L}{s(\\hat{L})} \\sim t_{N-a} \\] A \\(1-\\alpha\\) confidence limits are given by \\[ \\hat{L} \\pm t_{1-\\alpha/2; N-a}s(\\hat{L}) \\] Hypothesis testing \\[ \\begin{aligned} &amp;H_0: L = 0 \\\\ &amp;H_a: L \\neq 0 \\end{aligned} \\] with \\[ T = \\frac{\\hat{L}}{s(\\hat{L})} \\] reject \\(H_0\\) if \\(|T| &gt; t_{1-\\alpha/2;N-a}\\) 21.1.1.3.4 Linear Combination of Treatment Means just like contrast \\(L = \\sum_{i=1}^a c_i \\mu_i\\) but no restrictions on the \\(c_i\\) coefficients. Tests on a single treatment mean, two treatment means, and contrasts can all be considered form the same perspective. \\[ \\begin{aligned} &amp;H_0: \\sum c_i \\mu_i = c \\\\ &amp;H_a: \\sum c_i \\mu_i \\neq c \\end{aligned} \\] The test statistics ( \\(t\\)-stat) can be considered equivalently as \\(F\\)-tests; \\(F = (T)^2\\) where \\(F \\sim F_{1,N-a}\\). Since the numerator degrees of freedom is always 1 in these cases, we refer to them as single-degree-of-freedom tests. Multiple Contrasts To test simultaneously \\(k \\ge 2\\) contrasts, let \\(T_1,...,T_k\\) be the t-stat. The joint distribution of these random variables is a multivariate t-distribution (the tests are dependent since they re based on the same data). Limitations for comparing multiple contrasts: The confidence coefficient \\(1-\\alpha\\) only applies to a particular estimate, not a series of estimates; similarly, the Type I error rate, \\(\\alpha\\), applies to a particular test, not a series of tests. Example: 3 \\(t\\)-tests at \\(\\alpha = 0.05\\), if tests are independent (which they are not), \\(0.95^3 = 0.857\\) (thus \\(\\alpha - 0.143\\) not 0.05) The confidence coefficient \\(1-\\alpha\\) and significance level \\(\\alpha\\) are appropriate only if the test was not suggest by the data. often, the results of an experiment suggest important (i.e.,..g, potential significant) relationships. the process of studying effects suggests by the data is called data snooping Multiple Comparison Procedures: Tukey Scheffe Bonferroni 21.1.1.3.4.1 Tukey All pairwise comparisons of factor level means. All pairs \\(D = \\mu_i - \\mu_i&#39;\\) or all tests of the form: \\[ \\begin{aligned} &amp;H_0: \\mu_i -\\mu_i&#39; = 0 \\\\ &amp;H_a: \\mu_i - \\mu_i&#39; \\neq 0 \\end{aligned} \\] When all sample sizes are equal (\\(n_1 = n_2 = ... = n_a\\)) then the Tukey method family confidence coefficient is exactly \\(1-\\alpha\\) and the significance level is exactly \\(\\alpha\\) When the sample sizes are not equal, the family confidence coefficient is greater than \\(1-\\alpha\\) (i.e., the significance level is less than \\(\\alpha\\)) so the test conservative Tukey considers the studentized range distribution. If we have \\(Y_1,..,Y_r\\), observations from a normal distribution with mean \\(\\alpha\\) and variance \\(\\sigma^2\\). Define: \\[ w = max(Y_i) - min(Y_i) \\] as the range of the observations. Let \\(s^2\\) be an estimate of \\(\\sigma^2\\) with v degrees of freedom. Then, \\[ q(r,v) = \\frac{w}{s} \\] is called the studentized range. The distribution of q uses a special table. Notes when we are not interested in testing all pairwise comparison,s the confidence coefficient for the family of comparisons under consideration will be greater than \\(1-\\alpha\\) (with the significance level less than \\(\\alpha\\)) Tukey can be used for “data snooping” as long as the effects to be studied on the basis of preliminary data analysis are pairwise comparisons. 21.1.1.3.4.2 Scheffe This method applies when the family of interest is the set of possible contrasts among the treatment means: \\[ L = \\sum_{i=1}^a c_i \\mu_i \\] where \\(\\sum_{i=1}^a c_i =0\\) That is, the family of all possible contrasts \\(L\\) or \\[ \\begin{aligned} &amp;H_0: L = 0 \\\\ &amp;H_a: L \\neq 0 \\end{aligned} \\] The family confidence level for the Scheffe procedure is exactly \\(1-\\alpha\\) (i.e., significance level = \\(\\alpha\\)) whether the sample sizes are equal or not. For simultaneous confidence intervals, \\[ \\hat{L} \\pm Ss(\\hat{L}) \\] where \\(\\hat{L}=\\sum c_i \\bar{Y}_{i.},s^2(\\hat{L}) = MSE \\sum c_i^2/n_i\\) and \\(S^2 = (a-1)f_{1-\\alpha;a-1,N-a}\\) The Scheffe procedure considers \\[ F = \\frac{\\hat{L}^2}{(a-1)s^2(\\hat{L})} \\] where we reject \\(H_0\\) at the family significance level \\(\\alpha\\) if \\(F &gt; f_{(1-\\alpha;a-1,N-a)}\\) Note Since applications of the Scheffe never involve all conceivable contrasts, the finite family confidence coefficient will be larger than \\(1-\\alpha\\), so \\(1-\\alpha\\) is a lower bound. Thus, people often consider a larger \\(\\alpha\\) (e.g., 90% confidence interval) Scheffe can be used for “data scooping” since the family of statements contains all possible contrasts. If only pairwise comparisons are to be considered, The Tukey procedure gives narrower confidence limits. 21.1.1.3.4.3 Bonferroni Applicable whether the sample sizes are equal or unequal. For the confidence intervals, \\[ \\hat{L} \\pm B s(\\hat{L}) \\] where \\(B= t_{(1-\\alpha/(2g);N-a)}\\) and g is the number of comparisons in the family. Hypothesis testing \\[ \\begin{aligned} &amp;H_0: L = 0 \\\\ &amp;H_a: L \\neq 0 \\end{aligned} \\] Let \\(T= \\frac{\\hat{L}}{s(\\hat{L})}\\) and reject \\(H_0\\) if \\(|T|&gt;t_{1-\\alpha/(2g),N-a}\\) Notes If all pairwise comparisons are of interest, the Tukey procedure is superior (narrower confidence intervals). If not, Bonferroni may be better. Bonferroni is better than Scheffe when the number of contrasts is about the same as the treatment levels (or less). Recommendation: compute all threes and pick the smallest. Bonferroni can’t be used for data snooping 21.1.1.3.4.4 Fisher’s LSD does not control for family error rate use \\(t\\)-stat for testing \\[ H_0: \\mu_i = \\mu_j \\] t-stat \\[ t = \\frac{\\bar{y}_i - \\bar{y}_j}{\\sqrt{MSE(\\frac{1}{n_i}+ \\frac{1}{n_j})}} \\] 21.1.1.3.4.5 Newman-Keuls Do not recommend using this test since it has less power than ANOVA. 21.1.1.3.5 Multiple comparisons with a control 21.1.1.3.5.1 Dunnett We have \\(a\\) groups where the last group is the control group, and the \\(a-1\\) treatment groups. Then, we compare treatment groups to the control group. Hence, we have \\(a-1\\) contrasts (i.e., \\(a-1\\) pairwise comparisons) 21.1.1.3.6 Summary When choosing a multiple contrast method: Pairwise Equal groups sizes: Tukey Unequal groups sizes: Tukey, Scheffe Not pairwise with control: Dunnett general: Bonferroni, Scheffe 21.1.2 Single Factor Random Effects Model Also known as ANOVA Type II models. Treatments are chosen at from from larger population. We extend inference to all treatments in the population and not restrict our inference to those treatments that happened to be selected for the study. 21.1.2.1 Random Cell Means \\[ Y_{ij} = \\mu_i + \\epsilon_{ij} \\] where \\(\\mu_i \\sim N(\\mu, \\sigma^2_{\\mu})\\) and independent \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) and independent \\(\\mu_i\\) and \\(\\epsilon_{ij}\\) are mutually independent for \\(i =1,...,a; j = 1,...,n\\) With all treatment sample sizes are equal \\[ \\begin{aligned} E(Y_{ij}) &amp;= E(\\mu_i) = \\mu \\\\ var(Y_{ij}) &amp;= var(\\mu_i) + var(\\epsilon_i) = \\sigma^2_{\\mu} + \\sigma^2 \\end{aligned} \\] Since \\(Y_{ij}\\) are not independent \\[ \\begin{aligned} cov(Y_{ij},Y_{ij&#39;}) &amp;= E(Y_{ij}Y_{ij&#39;}) - E(Y_{ij})E(Y_{ij&#39;}) \\\\ &amp;= E(\\mu_i^2 + \\mu_i \\epsilon_{ij&#39;} + \\mu_i \\epsilon_{ij} + \\epsilon_{ij}\\epsilon_{ij&#39;}) - \\mu^2 \\\\ &amp;= \\sigma^2_{\\mu} + \\mu^2 - \\mu^2 &amp; \\text{if} j \\neq j&#39; \\\\ &amp;= \\sigma^2_{\\mu} &amp; \\text{if} j \\neq j&#39; \\end{aligned} \\] \\[ \\begin{aligned} cov(Y_{ij},Y_{i&#39;j&#39;}) &amp;= E(\\mu_i \\mu_{i&#39;} + \\mu_i \\epsilon_{i&#39;j&#39;}+ \\mu_{i&#39;}\\epsilon_{ij}+ \\epsilon_{ij}\\epsilon_{i&#39;j&#39;}) - \\mu^2 \\\\ &amp;= \\mu^2 - \\mu^2 &amp; \\text{if } i \\neq i&#39; \\\\ &amp;= 0 \\\\ \\end{aligned} \\] Hence, all observations have the same variance any two observations from the same treatment have covariance \\(\\sigma^2_{\\mu}\\) The correlation between any two responses from the same treatment: \\[ \\begin{aligned} \\rho(Y_{ij},Y_{ij&#39;}) &amp;= \\frac{\\sigma^2_{\\mu}}{\\sigma^2_{\\mu}+ \\sigma^2} &amp;&amp; \\text{$j \\neq j&#39;$} \\end{aligned} \\] Inference Intraclass Correlation Coefficient \\[ \\frac{\\sigma^2_{\\mu}}{\\sigma^2 + \\sigma^2_{\\mu}} \\] which measures the proportion of total variability of \\(Y_{ij}\\) accounted for by the variance of \\(\\mu_i\\) \\[ \\begin{aligned} &amp;H_0: \\sigma_{\\mu}^2 = 0 \\\\ &amp;H_a: \\sigma_{\\mu}^2 \\neq 0 \\end{aligned} \\] \\(H_0\\) implies \\(\\mu_i = \\mu\\) for all i, which can be tested by the F-test in ANOVA. The understandings of the Single Factor Fixed Effects Model and the Single Factor Random Effects Model are different, the ANOVA is same for the one factor model. The difference is in the expected mean squares Random Effects Model Fixed Effects Model \\(E(MSE) = \\sigma^2\\) \\(E(MSE) = \\sigma^2\\) \\(E(M STR) = \\sigma^2 - n \\sigma^2_\\mu\\) \\(E(MSTR) = \\sigma^2 + \\frac{ \\sum_i n_i (\\mu_i - \\mu)^2}{a-1}\\) If \\(\\sigma^2_\\mu\\), then MSE and MSTR have the same expectation (\\(\\sigma^2\\)). Otherwise, \\(E(MSTR) &gt;E(MSE)\\). Large values of the statistic \\[ F = \\frac{MSTR}{MSE} \\] suggest we reject \\(H_0\\). Since \\(F \\sim F_{(a-1,a(n-1))}\\) when \\(H_0\\) holds. If \\(F &gt; f_{(1-\\alpha;a-1,a(n-1))}\\) we reject \\(H_0\\). If sample sizes are not equal, \\(F\\)-test can still be used, but the df are \\(a-1\\) and \\(N-a\\). 21.1.2.1.1 Estimation of \\(\\mu\\) An unbiased estimator of \\(E(Y_{ij})=\\mu\\) is the grand mean: \\(\\hat{\\mu} = \\hat{Y}_{..}\\) The variance of this estimator is \\[ \\begin{aligned} var(\\bar{Y}_{..}) &amp;= var(\\sum_i \\bar{Y}_{i.}/a) \\\\ &amp;= \\frac{1}{a^2}\\sum_ivar(\\bar{Y}_{i.}) \\\\ &amp;= \\frac{1}{a^2}\\sum_i(\\sigma^2_\\mu+\\sigma^2/n) \\\\ &amp;= \\frac{1}{a^2}(\\sigma^2_{\\mu}+\\sigma^2/n) \\\\ &amp;= \\frac{n\\sigma^2_{\\mu}+ \\sigma^2}{an} \\end{aligned} \\] An unbiased estimator of this variance is \\(s^2(\\bar{Y})=\\frac{MSTR}{an}\\). Thus \\(\\frac{\\bar{Y}_{..}-\\mu}{s(\\bar{Y}_{..})} \\sim t_{a-1}\\) A \\(1-\\alpha\\) confidence interval is \\(\\bar{Y}_{..} \\pm t_{(1-\\alpha/2;a-1)}s(\\bar{Y}_{..})\\) 21.1.2.1.2 Estimation of \\(\\sigma^2_\\mu/(\\sigma^2_{\\mu}+\\sigma^2)\\) In the random and fixed effects model, MSTR and MSE are independent. When the sample sizes are equal (\\(n_i = n\\) for all i), \\[ \\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\sim f_{(a-1,a(n-1))} \\] \\[ P(f_{(\\alpha/2;a-1,a(n-1))}\\le \\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\le f_{(1-\\alpha/2;a-1,a(n-1))}) = 1-\\alpha \\] \\[ \\begin{aligned} L &amp;= \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(1-\\alpha/2;a-1,a(n-1))}})-1) \\\\ U &amp;= \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(\\alpha/2;a-1,a(n-1))}})-1) \\end{aligned} \\] The lower and upper \\((L^*,U^*)\\) confidence limits for \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2}\\) \\[ \\begin{aligned} L^* &amp;= \\frac{L}{1+L} \\\\ U^* &amp;= \\frac{U}{1+U} \\end{aligned} \\] If the lower limit for \\(\\frac{\\sigma^2_\\mu}{\\sigma^2}\\) is negative, it is customary to set \\(L = 0\\). 21.1.2.1.3 Estimation of \\(\\sigma^2\\) \\(a(n-1)MSE/\\sigma^2 \\sim \\chi^2_{a(n-1)}\\), the \\((1-\\alpha)\\) confidence interval for \\(\\sigma^2\\): \\[ \\frac{a(n-1)MSE}{\\chi^2_{1-\\alpha/2;a(n-1)}} \\le \\sigma^2 \\le \\frac{a(n-1)MSE}{\\chi^2_{\\alpha/2;a(n-1)}} \\] can also be used in case sample sizes are not equal - then df is N-a. 21.1.2.1.4 Estimation of \\(\\sigma^2_\\mu\\) \\(E(MSE) = \\sigma^2\\) \\(E(MSTR) = \\sigma^2 + n\\sigma^2_\\mu\\). Hence, \\[ \\sigma^2_{\\mu} = \\frac{E(MSTR)- E(MSE)}{n} \\] An unbiased estimator of \\(\\sigma^2_\\mu\\) is given by \\[ s^2_\\mu =\\frac{MSTR-MSE}{n} \\] if \\(s^2_\\mu &lt; 0\\), set \\(s^2_\\mu = 0\\) If sample sizes are not equal, \\[ s^2_\\mu = \\frac{MSTR - MSE}{n&#39;} \\] where \\(n&#39; = \\frac{1}{a-1}(\\sum_i n_i- \\frac{\\sum_i n^2_i}{\\sum_i n_i})\\) no exact confidence intervals for \\(\\sigma^2_\\mu\\), but we can approximate intervals. Satterthewaite Procedure can be used to construct approximate confidence intervals for linear combination of expected mean squares A linear combination: \\[ \\sigma^2_\\mu = \\frac{1}{n} E(MSTR) + (-\\frac{1}{n}) E(MSE) \\] \\[ S = d_1 E(MS_1) + ..+ d_h E(MS_h) \\] where \\(d_i\\) are coefficients. An unbiased estimator of S is \\[ \\hat{S} = d_1 MS_1 + ...+ d_h MS_h \\] Let \\(df_i\\) be the degrees of freedom associated with the mean square \\(MS_i\\). The Satterthwaite approximation: \\[ \\frac{(df)\\hat{S}}{S} \\sim \\chi^2_{df} \\] where \\[ df = \\frac{(d_1MS_1+...+d_hMS_h)^2}{(d_1MS_1)^2/df_1 + ...+ (d_hMS_h)^2/df_h} \\] An approximate \\(1-\\alpha\\) confidence interval for S: \\[ \\frac{(df)\\hat{S}}{\\chi^2_{1-\\alpha/2;df}} \\le S \\le \\frac{(df)\\hat{S}}{\\chi^2_{\\alpha/2;df}} \\] For the single factor random effects model \\[ \\frac{(df)s^2_\\mu}{\\chi^2_{1-\\alpha/2;df}} \\le \\sigma^2_\\mu \\le \\frac{(df)s^2_\\mu}{\\chi^2_{\\alpha/2;df}} \\] where \\[ df = \\frac{(sn^2_\\mu)^2}{\\frac{(MSTR)^2}{a-1}+ \\frac{(MSE)^2}{a(n-1)}} \\] 21.1.2.2 Random Treatment Effects Model \\[ \\tau_i = \\mu_i - E(\\mu_i) = \\mu_i - \\mu \\] we have \\(\\mu_i = \\mu + \\tau_i\\) and \\[ Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij} \\] where \\(\\mu\\) = constant, common to all observations \\(\\tau_i \\sim N(0,\\sigma^2_\\tau)\\) independent (random variables) \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) independent. \\(\\tau_{i}, \\epsilon_{ij}\\) are independent (i=1,…,a; j =1,..,n) our model is concerned with only balanced single factor ANOVA. Diagnostics Measures Non-constant error variance (plots, Levene test, Hartley test). Non-independence of errors (plots, Durban-Watson test). Outliers (plots, regression methods). Non-normality of error terms (plots, Shapiro-Wilk, Anderson-Darling). Omitted Variable Bias (plots) Remedial Weighted Least Squares Transformations Non-parametric Procedures. Note Fixed effect ANOVA is relatively robust to non-normality unequal variances when sample sizes are approximately equal; at least the F-test and multiple comparisons. However, single comparisons of treatment means are sensitive to unequal variances. Lack of independence can seriously affect both fixed and random effect ANVOA. 21.1.3 Two Factor Fixed Effect ANOVA The multi-factor experiment is more efficient provides more info gives more validity to the findings. 21.1.3.1 Balanced Assumption: All treatment sample sizes are equal All treatment means are of equal importance Assume: Factor \\(A\\) has a levels and Factor \\(B\\) has b levels. All \\(a \\times b\\) factor levels are considered. The number of treatments for each level is n. \\(N = abn\\) observations in the study. 21.1.3.1.1 Cell Means Model \\[ Y_{ijk} = \\mu_{ij} + \\epsilon_{ijk} \\] where \\(\\mu_{ij}\\) are fixed parameters (cell means) \\(i = 1,...,a\\) = the levels of Factor A \\(j = 1,...,b\\) = the levels of Factor B. \\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) for \\(i = 1,...,a\\), \\(j = 1,..,b\\) and \\(k = 1,..,n\\) And \\[ \\begin{aligned} E(Y_{ijk}) &amp;= \\mu_{ij} \\\\ var(Y_{ijk}) &amp;= var(\\epsilon_{ijk}) = \\sigma^2 \\end{aligned} \\] Hence, \\[ Y_{ijk} \\sim \\text{indep } N(\\mu_{ij},\\sigma^2) \\] And the model is \\[ \\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon \\] Thus, \\[ \\begin{aligned} E(\\mathbf{Y}) &amp;= \\mathbf{X}\\beta \\\\ var(\\mathbf{Y}) &amp;= \\sigma^2 \\mathbf{I} \\end{aligned} \\] Interaction \\[ (\\alpha \\beta)_{ij} = \\mu_{ij} - (\\mu_{..}+ \\alpha_i + \\beta_j) \\] where \\(\\mu_{..} = \\sum_i \\sum_j \\mu_{ij}/ab\\) is the grand mean \\(\\alpha_i = \\mu_{i.}-\\mu_{..}\\) is the main effect for factor \\(A\\) at the \\(i\\)-th level \\(\\beta_j = \\mu_{.j} - \\mu_{..}\\) is the main effect for factor \\(B\\) at the \\(j\\)-th level \\((\\alpha \\beta)_{ij}\\) is the interaction effect when factor \\(A\\) is at the \\(i\\)-th level and factor \\(B\\) is at the \\(j\\)-th level. \\((\\alpha \\beta)_{ij} = \\mu_{ij} - \\mu_{i.}-\\mu_{.j}+ \\mu_{..}\\) Examine interactions: Examine whether all \\(\\mu_{ij}\\) can be expressed as the sums \\(\\mu_{..} + \\alpha_i + \\beta_j\\) Examine whether the difference between the mean responses for any two levels of factor \\(B\\) is the same for all levels of factor \\(A\\). Examine whether the difference between the mean response for any two levels of factor \\(A\\) is the same for all levels of factor \\(B\\) Examine whether the treatment mean curves for the different factor levels in a treatment plot are parallel. For \\(j = 1,...,b\\) \\[ \\begin{aligned} \\sum_i(\\alpha \\beta)_{ij} &amp;= \\sum_i (\\mu_{ij} - \\mu_{..} - \\alpha_i - \\beta_j) \\\\ &amp;= \\sum_i \\mu_{ij} - a \\mu_{..} - \\sum_i \\alpha_i - a \\beta_j \\\\ &amp;= a \\mu_{.j} - a \\mu_{..}- \\sum_i (\\mu_{i.} - \\mu_{..}) - a(\\mu_{.j}-\\mu_{..}) \\\\ &amp;= a \\mu_{.j} - a \\mu_{..} - a \\mu_{..}+ a \\mu_{..} - a (\\mu_{.j} - \\mu_{..}) \\\\ &amp;= 0 \\end{aligned} \\] Similarly, \\(\\sum_j (\\alpha \\beta) = 0, i = 1,...,a\\) and \\(\\sum_i \\sum_j (\\alpha \\beta)_{ij} =0\\), \\(\\sum_i \\alpha_i = 0\\), \\(\\sum_j \\beta_j = 0\\) 21.1.3.1.2 Factor Effects Model \\[ \\begin{aligned} \\mu_{ij} &amp;= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} \\\\ Y_{ijk} &amp;= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\end{aligned} \\] where \\(\\mu_{..}\\) is a constant \\(\\alpha_i\\) are constants subject to the restriction \\(\\sum_i \\alpha_i=0\\) \\(\\beta_j\\) are constants subject to the restriction \\(\\sum_j \\beta_j = 0\\) \\((\\alpha \\beta)_{ij}\\) are constants subject to the restriction \\(\\sum_i(\\alpha \\beta)_{ij} = 0\\) for \\(j=1,...,b\\) and \\(\\sum_j(\\alpha \\beta)_{ij} = 0\\) for \\(i = 1,...,a\\) \\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) for \\(k = 1,..,n\\) We have \\[ \\begin{aligned} E(Y_{ijk}) &amp;= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}\\\\ var(Y_{ijk}) &amp;= \\sigma^2 \\\\ Y_{ijk} &amp;\\sim N (\\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}, \\sigma^2) \\end{aligned} \\] We have \\(1+a+b+ab\\) parameters. But there are \\(ab\\) parameters in the Cell Means Model. In the Factor Effects Model, the restrictions limit the number of parameters that can be estimated: \\[ \\begin{aligned} 1 &amp;\\text{ for } \\mu_{..} \\\\ (a-1) &amp;\\text{ for } \\alpha_i \\\\ (b-1) &amp;\\text{ for } \\beta_j \\\\ (a-1)(b-1) &amp;\\text{ for } (\\alpha \\beta)_{ij} \\end{aligned} \\] Hence, there are \\[ 1 + a - 1 + b - 1 + ab - a- b + 1 = ab \\] parameters in the model. We can have several restrictions when considering the model in the form \\(\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\\) One way: \\[ \\begin{aligned} \\alpha_a &amp;= \\alpha_1 - \\alpha_2 - ... - \\alpha_{a-1} \\\\ \\beta_b &amp;= -\\beta_1 - \\beta_2 - ... - \\beta_{b-1} \\\\ (\\alpha \\beta)_{ib} &amp;= -(\\alpha \\beta)_{i1} -(\\alpha \\beta)_{i2} -...-(\\alpha \\beta)_{i,b-1} ; i = 1,..,a \\\\ (\\alpha \\beta)_{aj}&amp; = -(\\alpha \\beta)_{1j}-(\\alpha \\beta)_{2j} - ... -(\\alpha \\beta)_{a-1,j}; j = 1,..,b \\end{aligned} \\] We can fit the model by least squares or maximum likelihood Cell Means Model minimize \\[ Q = \\sum_i \\sum_j \\sum_k (Y_{ijk}-\\mu_{ij})^2 \\] estimators \\[ \\begin{aligned} \\hat{\\mu}_{ij} &amp;= \\bar{Y}_{ij} \\\\ \\hat{Y}_{ijk} &amp;= \\bar{Y}_{ij} \\\\ e_{ijk} = Y_{ijk} - \\hat{Y}_{ijk} &amp;= Y_{ijk} - \\bar{Y}_{ij} \\end{aligned} \\] Factor Effects Model \\[ Q = \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\mu_{..}-\\alpha_i = \\beta_j - (\\alpha \\beta)_{ij})^2 \\] subject to the restrictions \\[ \\begin{aligned} \\sum_i \\alpha_i &amp;= 0 \\\\ \\sum_j \\beta_j &amp;= 0 \\\\ \\sum_i (\\alpha \\beta)_{ij} &amp;= 0 \\\\ \\sum_j (\\alpha \\beta)_{ij} &amp;= 0 \\end{aligned} \\] estimators \\[ \\begin{aligned} \\hat{\\mu}_{..} &amp;= \\bar{Y}_{...} \\\\ \\hat{\\alpha}_i &amp;= \\bar{Y}_{i..} - \\bar{Y}_{...} \\\\ \\hat{\\beta}_j &amp;= \\bar{Y}_{.j.}-\\bar{Y}_{...} \\\\ (\\hat{\\alpha \\beta})_{ij} &amp;= \\bar{Y}_{ij.} - \\bar{Y}_{i..} - \\bar{Y}_{.j.}+ \\bar{Y}_{...} \\end{aligned} \\] The fitted values \\[ \\hat{Y}_{ijk} = \\bar{Y}_{...}+ (\\bar{Y}_{i..}- \\bar{Y}_{...})+ (\\bar{Y}_{.j.}- \\bar{Y}_{...}) + (\\bar{Y}_{ij.} - \\bar{Y}_{i..}-\\bar{Y}_{.j.}+\\bar{Y}_{...}) = \\bar{Y}_{ij.} \\] where \\[ \\begin{aligned} e_{ijk} &amp;= Y_{ijk} - \\bar{Y}_{ij.} \\\\ e_{ijk} &amp;\\sim \\text{ indep } (0,\\sigma^2) \\end{aligned} \\] and \\[ \\begin{aligned} s^2_{\\hat{\\mu}..} &amp;= \\frac{MSE}{nab} \\\\ s^2_{\\hat{\\alpha}_i} &amp;= MSE(\\frac{1}{nb} - \\frac{1}{nab}) \\\\ s^2_{\\hat{\\beta}_j} &amp;= MSE(\\frac{1}{na} - \\frac{1}{nab}) \\\\ s^2_{(\\hat{\\alpha\\beta})_{ij}} &amp;= MSE (\\frac{1}{n} - \\frac{1}{na}- \\frac{1}{nb} + \\frac{1}{nab}) \\end{aligned} \\] 21.1.3.1.2.1 Partitioning the Total Sum of Squares \\[ Y_{ijk} - \\bar{Y}_{...} = \\bar{Y}_{ij.} - \\bar{Y}_{...} + Y_{ijk} - \\bar{Y}_{ij.} \\] \\(Y_{ijk} - \\bar{Y}_{...}\\): Total deviation \\(\\bar{Y}_{ij.} - \\bar{Y}_{...}\\): Deviation of treatment mean from overall mean \\(Y_{ijk} - \\bar{Y}_{ij.}\\): Deviation of observation around treatment mean (residual). \\[ \\begin{aligned} \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\bar{Y}_{...})^2 &amp;= n \\sum_i \\sum_j (\\bar{Y}_{ij.}- \\bar{Y}_{...})^2+ \\sum_i \\sum_j sum_k (Y_{ijk} - \\bar{ij.})^2 \\\\ SSTO &amp;= SSTR + SSE \\end{aligned} \\] (cross product terms are 0) \\[ \\bar{Y}_{ij.}- \\bar{Y}_{...} = \\bar{Y}_{i..}-\\bar{Y}_{...} + \\bar{Y}_{.j.}-\\bar{Y}_{...} + \\bar{Y}_{ij.} - \\bar{Y}_{i..} - \\bar{Y}_{.j.} + \\bar{Y}_{...} \\] squaring and summing: \\[ \\begin{aligned} n\\sum_i \\sum_j (\\bar{Y}_{ij.}-\\bar{Y}_{...})^2 &amp;= nb\\sum_i (\\bar{Y}_{i..}-\\bar{Y}_{...})^2 + na \\sum_j (\\bar{Y}_{.j.}-\\bar{Y}_{...})^2 \\\\ &amp;+ n \\sum_i \\sum_j (\\bar{Y}_{ij.}-\\bar{Y}_{i..}- \\bar{Y}_{.j.}+ \\bar{Y}_{...})^2 \\\\ SSTR &amp;= SSA + SSB + SSAB \\end{aligned} \\] The interaction term from \\[ \\begin{aligned} SSAB &amp;= SSTO - SSE - SSA - SSB \\\\ SSAB &amp;= SSTR - SSA - SSB \\end{aligned} \\] where \\(SSA\\) is the factor \\(A\\) sum of squares (measures the variability of the estimated factor \\(A\\) level means \\(\\bar{Y}_{i..}\\))- the more variable, the larger \\(SSA\\) \\(SSB\\) is the factor \\(B\\) sum of squares \\(SSAB\\) is the interaction sum of squares, measuring the variability of the estimated interactions. 21.1.3.1.2.2 Partitioning the df \\(N = abn\\) cases and \\(ab\\) treatments. For one-way ANOVA and regression, the partition has df: \\[ SS: SSTO = SSTR + SSE \\] \\[ df: N-1 = (ab-1) + (N-ab) \\] we must further partition the \\(ab-1\\) df with SSTR \\[ SSTR = SSA + SSB + SSAB \\] \\[ ab-1 = (a-1) + (b-1) + (a-1)(b-1) \\] \\(df_{SSA} = a-1\\): a treatment deviations but 1 df is lost due to the restriction \\(\\sum (\\bar{Y}_{i..}- \\bar{Y}_{...})=0\\) \\(df_{SSB} = b-1\\): b treatment deviations but 1 df is lost due to the restriction \\(\\sum (\\bar{Y}_{.j.}- \\bar{Y}_{...})=0\\) \\(df_{SSAB} = (a-1)(b-1)= (ab-1)-(a-1)-(b-1)\\): ab interactions, there are (a+b-1) restrictions, so df = ab-a-(b-1)= (a-1)(b-1) 21.1.3.1.2.3 Mean Squares \\[ \\begin{aligned} MSA &amp;= \\frac{SSA}{a-1}\\\\ MSB &amp;= \\frac{SSB}{b-1}\\\\ MSAB &amp;= \\frac{SSAB}{(a-1)(b-1)} \\end{aligned} \\] The expected mean squares are \\[ \\begin{aligned} E(MSE) &amp;= \\sigma^2 \\\\ E(MSA) &amp;= \\sigma^2 + nb \\frac{\\sum \\alpha_i^2}{a-1} = \\sigma^2 + nb \\frac{\\sum(\\sum_{i.}-\\mu_{..})^2}{a-1} \\\\ E(MSB) &amp;= \\sigma^2 + na \\frac{\\sum \\beta_i^2}{b-1} = \\sigma^2 + na \\frac{\\sum(\\sum_{.j}-\\mu_{..})^2}{b-1} \\\\ E(MSAB) &amp;= \\sigma^2 + n \\frac{\\sum \\sum (\\alpha \\beta)_{ij}^2}{(a-1)(b-1)} = \\sigma^2 + n \\frac{\\sum (\\mu_{ij}- \\mu_{i.}- \\mu_{.j}+ \\mu_{..} )^2}{(a-1)(b-1)} \\end{aligned} \\] If there are no factor A main effects (all \\(\\mu_{i.} = 0\\) or \\(\\alpha_i = 0\\)) the MSA and MSE have the same expectation; otherwise MSA &gt; MSE. Same for factor B, and interaction effects. which case we can examine F-statistics. Interaction \\[ \\begin{aligned} H_0: \\mu_{ij}- \\mu_{i.} - \\mu_{.j} + \\mu_{..} = 0 &amp;&amp; \\text{for all i,j} \\\\ H_a: \\mu_{ij}- \\mu_{i.} - \\mu_{.j} + \\mu_{..} \\neq 0 &amp;&amp; \\text{for some i,j} \\end{aligned} \\] or \\[ \\begin{aligned} &amp;H_0: \\text{All}(\\alpha \\beta)_{ij} = 0 \\\\ &amp;H_a: \\text{Not all} (\\alpha \\beta) = 0 \\end{aligned} \\] Let \\(F = \\frac{MSAB}{MSE}\\). When \\(H_0\\) is true \\(F \\sim f_{((a-1)(b-1),ab(n-1))}\\). So reject \\(H_0\\) when \\(F &gt; f_{((a-1)(b-1),ab(n-1))}\\) Factor A main effects: \\[ \\begin{aligned} &amp;H_0: \\mu_{1.} = \\mu_{2.} = ... = \\mu_{a.} \\\\ &amp;H_a: \\text{Not all $\\mu_{i.}$ are equal} \\end{aligned} \\] or \\[ \\begin{aligned} &amp;H_0: \\alpha_1 = ... = \\alpha_a = 0 \\\\ &amp;H_a: \\text{Not all $\\alpha_i$ are equal to 0} \\end{aligned} \\] \\(F= \\frac{MSA}{MSE}\\) and reject \\(H_0\\) if \\(F&gt;f_{(1-\\alpha;a-1,ab(n-1))}\\) 21.1.3.1.2.4 Two-way ANOVA Source of Variation SS df MS F Factor A \\(SSA\\) \\(a-1\\) \\(MSA = SSA/(a-1)\\) \\(MSA/MSE\\) Factor B \\(SSB\\) \\(b-1\\) \\(MSB = SSB/(b-1)\\) \\(MSB/MSE\\) AB interactions \\(SSAB\\) \\((a-1)(b-1)\\) \\(MSAB = SSAB /MSE\\) Error \\(SSE\\) \\(ab(n-1)\\) \\(MSE = SSE/ab(n-1)\\) Total (corrected) \\(SSTO\\) \\(abn - 1\\) Doing 2-way ANOVA means you always check interaction first, because if there are significant interactions, checking the significance of the main effects becomes moot. The main effects concern the mean responses for levels of one factor averaged over the levels of the other factor. When interaction is present, we can’t conclude that a given factor has no effect, even if these averages are the same. It means that the effect of the factor depends on the level of the other factor. On the other hand, if you can establish that there is no interaction, then you can consider inference on the factor main effects, which are then said to be additive. And we can also compare factor means like the Single Factor Fixed Effects Model using Tukey, Scheffe, Bonferroni. We can also consider contrasts in the 2-way model \\[ L = \\sum c_i \\mu_i \\] where \\(\\sum c_i =0\\) which is estimated by \\[ \\hat{L} = \\sum c_i \\bar{Y}_{i..} \\] with variance \\[ \\sigma^2(\\hat{L}) = \\frac{\\sigma^2}{bn} \\sum c_i^2 \\] and variance estimate \\[ \\frac{MSE}{bn} \\sum c_i^2 \\] Orthogonal Contrasts \\[ \\begin{aligned} L_1 &amp;= \\sum c_i \\mu_i, \\sum c_i = 0 \\\\ L_2 &amp;= \\sum d_i \\mu_i , \\sum d_i = 0 \\end{aligned} \\] these contrasts are said to be orthogonal if \\[ \\sum \\frac{c_i d_i}{n_i} = 0 \\] in balanced case \\(\\sum c_i d_i =0\\) \\[ \\begin{aligned} cov(\\hat{L}_1, \\hat{L}_2) &amp;= cov(\\sum_i c_i \\bar{Y}_{i..}, \\sum_l d_l \\bar{Y}_{l..}) \\\\ &amp;= \\sum_i \\sum_l c_i d_l cov(\\bar{Y}_{i..},\\bar{Y}_{l..}) \\\\ &amp;= \\sum_i c_i d_i \\frac{\\sigma^2}{bn} = 0 \\end{aligned} \\] Orthogonal contrasts can be used to further partition the model sum of squares. There are many sets of orthogonal contrasts and thus, many ways to partition the sum of squares. A special set of orthogonal contrasts that are used when the levels of a factor can be assigned values on a metric scale are called orthogonal polynomials Coefficients can be found for the special case of equal spaced levels (e.g., (0 15 30 45 60)) equal sample sizes (\\(n_1 = n_2 = ... = n_{ab}\\)) We can define the SS for a given contrast: \\[ SS_L = \\frac{\\hat{L}^2}{\\sum_{i=1}^a (c^2_i/bn_i)} \\] \\[ T = \\frac{\\hat{L}}{\\sqrt{MSE\\sum_{i=1}^a(c_i^2/bn_i)}} \\sim t \\] Moreover, \\[ t^2_{(1-\\alpha/2;df)}=F_{(1-\\alpha;1,df)} \\] So, \\[ \\frac{SS_L}{MSE} \\sim F_{(1-\\alpha;1,df_{MSE})} \\] all contrasts have d.f = 1 21.1.3.2 Unbalanced We could have unequal numbers of replications for all treatment combinations: Observational studies Dropouts in designed studies Larger sample sizes for inexpensive treatments Sample sizes to match population makeup. Assume that each factor combination has at least 1 observation (no empty cells) Consider the same model as: \\[ Y_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\] where sample sizes are: \\(n_{ij}\\): \\[ \\begin{aligned} n_{i.} &amp;= \\sum_j n_{ij} \\\\ n_{.j} &amp;= \\sum_i n_{ij} \\\\ n_T &amp;= \\sum_i \\sum_j n_{ij} \\end{aligned} \\] Problem here is that \\[ SSTO \\neq SSA + SSB + SSAB + SSE \\] (the design is non-orthogonal) For \\(i = 1,...,a-1,\\) \\[ u_i = \\begin{cases} +1 &amp; \\text{if the obs is from the i-th level of Factor 1} \\\\ -1 &amp; \\text{if the obs is from the a-th level of Factor 1} \\\\ 0 &amp; \\text{otherwise} \\\\ \\end{cases} \\] For \\(j=1,...,b-1\\) \\[ v_i = \\begin{cases} +1 &amp; \\text{if the obs is from the j-th level of Factor 1} \\\\ -1 &amp; \\text{if the obs is from the b-th level of Factor 1} \\\\ 0 &amp; \\text{otherwise} \\\\ \\end{cases} \\] We can use these indicator variables as predictor variables and \\(\\mu_{..}, \\alpha_i ,\\beta_j, (\\alpha \\beta)_{ij}\\) as unknown parameters. \\[ Y = \\mu_{..} + \\sum_{i=1}^{a-1} \\alpha_i u_i + \\sum_{j=1}^{b-1} \\beta_j v_j + \\sum_{i=1}^{a-1} \\sum_{j=1}^{b-1}(\\alpha \\beta)_{ij} u_i v_j + \\epsilon \\] To test hypotheses, we use the extra sum of squares idea. For interaction effects \\[ \\begin{aligned} &amp;H_0: all (\\alpha \\beta)_{ij} = 0 \\\\ &amp;H_a: \\text{not all }(\\alpha \\beta)_{ij} =0 \\end{aligned} \\] Or to test \\[ \\begin{aligned} &amp;H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0 \\\\ &amp;H_a: \\text{not all } \\beta_j = 0 \\end{aligned} \\] Analysis of Factor Means (e.g., contrasts) is analogous to the balanced case, with modifications in the formulas for means and standard errors to account for unequal sample sizes. Or , we can fit the cell means model and consider it from a regression perspective If you have empty cells (i.e., some factor combinations have no observation), then the equivalent regression approach can’t be used. But you can still do partial analyses 21.1.4 Two-Way Random Effects ANOVA \\[ Y_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ij} \\] where \\(\\mu_{..}\\): constant \\(\\alpha_i \\sim N(0,\\sigma^2_{\\alpha}), i = 1,..,a\\) (independent) \\(\\beta_j \\sim N(0,\\sigma^2_{\\beta}), j = 1,..,b\\) (independent) \\((\\alpha \\beta)_{ij} \\sim N(0,\\sigma^2_{\\alpha \\beta}),i=1,...,a,j=1,..,b\\) (independent) \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent) All \\(\\alpha_i, \\beta_j, (\\alpha \\beta)_{ij}\\) are pairwise independent Theoretical means, variances, and covariances are \\[ \\begin{aligned} E(Y_{ijk}) &amp;= \\mu_{..} \\\\ var(Y_{ijk}) &amp;= \\sigma^2_Y= \\sigma^2_\\alpha + \\sigma^2_\\beta + \\sigma^2_{\\alpha \\beta} + \\sigma^2 \\end{aligned} \\] So \\(Y_{ijk} \\sim N(\\mu_{..},\\sigma^2_\\alpha + \\sigma^2_\\beta + \\sigma^2_{\\alpha \\beta} + \\sigma^2)\\) \\[ \\begin{aligned} cov(Y_{ijk},Y_{ij&#39;k&#39;}) &amp;= \\sigma^2_{\\alpha}, j \\neq j&#39; \\\\ cov(Y_{ijk},Y_{i&#39;jk&#39;}) &amp;= \\sigma^2_{\\beta}, i \\neq i&#39;\\\\ cov(Y_{ijk},Y_{ijk&#39;}) &amp;= \\sigma^2_\\alpha + \\sigma^2_{\\beta} + \\sigma^2_{\\alpha \\beta}, k \\neq k&#39; \\\\ cov(Y_{ijk},Y_{i&#39;j&#39;k&#39;}) &amp;= , i \\neq i&#39;, j \\neq j&#39; \\end{aligned} \\] 21.1.5 Two-Way Mixed Effects ANOVA 21.1.5.1 Balanced One fixed factor, while other is random treatment levels, we have a mixed effects model or a mixed model Restricted mixed model for 2-way ANOVA: \\[ Y_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\] where \\(\\mu_{..}\\): constant \\(\\alpha_i\\): fixed effects with constraints subject to restriction \\(\\sum \\alpha_i = 0\\) \\(\\beta_j \\sim indep N(0,\\sigma^2_\\beta)\\) \\((\\alpha \\beta)_{ij} \\sim N(0,\\frac{a-1}{a}\\sigma^2_{\\alpha \\beta})\\) subject to restriction \\(\\sum_i (\\alpha \\beta)_{ij} = 0\\) for all j, the variance here is written as the proportion for convenience; it makes the expected mean squares simpler (other assumed \\(var((\\alpha \\beta)_{ij}= \\sigma^2_{\\alpha \\beta}\\)) \\(cov((\\alpha \\beta)_{ij},(\\alpha \\beta)_{i&#39;j&#39;}) = - \\frac{1}{a} \\sigma^2_{\\alpha \\beta}, i \\neq i&#39;\\) \\(\\epsilon_{ijk}\\sim indepN(0,\\sigma^2)\\) \\(\\beta_j, (\\alpha \\beta)_{ij}, \\epsilon_{ijk}\\) are pairwise independent Two-way mixed models are written in an “unrestricted” form, with no restrictions on the interaction effects \\((\\alpha \\beta)_{ij}\\), they are pairwise independent. Let \\(\\beta^*, (\\alpha \\beta)^*_{ij}\\) be the unrestricted random effects, and \\((\\bar{\\alpha \\beta})_{ij}^*\\) the means averaged over the fixed factor for each level of random factor B. \\[ \\begin{aligned} \\beta_j &amp;= \\beta_j^* + (\\bar{\\alpha \\beta})_{ij}^* \\\\ (\\alpha \\beta)_{ij} &amp;= (\\alpha \\beta)_{ij}^* - (\\bar{\\alpha \\beta})_{ij}^* \\end{aligned} \\] Some consider the restricted model to be more general. but here we consider the restricted form. \\[ \\begin{aligned} E(Y_{ijk}) &amp;= \\mu_{..} + \\alpha_i \\\\ var(Y_{ijk}) &amp;= \\sigma^2_\\beta + \\frac{a-1}{a} \\sigma^2_{\\alpha \\beta} + \\sigma^2 \\end{aligned} \\] Responses from the same random factor \\((B)\\) level are correlated \\[ \\begin{aligned} cov(Y_{ijk},Y_{ijk&#39;}) &amp;= E(Y_{ijk}Y_{ijk&#39;}) - E(Y_{ijk})E(Y_{ijk&#39;}) \\\\ &amp;= \\sigma^2_\\beta + \\frac{a-1}{a} \\sigma^2_{\\alpha \\beta} , k \\neq k&#39; \\end{aligned} \\] Similarly, \\[ \\begin{aligned} cov(Y_{ijk},Y_{i&#39;jk&#39;}) &amp;= \\sigma^2_\\beta - \\frac{1}{a} \\sigma^2_{\\alpha\\ \\beta}, i \\neq i&#39; \\\\ cov(Y_{ijk},Y_{i&#39;j&#39;k&#39;}) &amp;= 0, j \\neq j&#39; \\end{aligned} \\] Hence, you can see that the only way you don’t have dependence in the \\(Y\\) is when they don’t share the same random effect. An advantage of the restricted mixed model is that 2 observations from the same random factor b level can be positively or negatively correlated. In the unrestricted model, they can only be positively correlated. Mean Square Fixed ANOVA (A, B Fixed) Random ANOVA (A,B random) Mixed ANVOA (A fixed, B random) MSA a - 1 \\(\\sigma ^2+ n b \\frac{\\sum\\alpha_i^2}{a-1}\\) \\(\\sigma^2 + nb\\sigma^ 2_ \\alpha +n \\sigma^ 2_{\\alpha \\beta}\\) MSB b-1 \\(\\sigma^2 + n a \\frac{\\sum\\beta ^2_j}{b-1}\\) \\(\\sigma^ 2 + na\\sigma^2_ \\beta +n \\sigma^ 2_{\\alpha \\beta}\\) MSAB ( a-1)(b-1) \\(\\sigma^2 + n \\frac{\\sum \\sum(\\alpha \\beta )^2_ {ij}} { ( a-1)(b-1)}\\) \\(\\sigma^2+n \\sigma^2_{\\alpha \\beta}\\) MSE (n-1)ab \\(\\sigma^2\\) \\(\\sigma^2\\) For fixed, random, and mixed models (balanced), the ANOVA table sums of squares calculations are identical. (also true for df and mean squares). The only difference is with the expected mean squares, thus the test statistics. In Random ANOVA, we test \\[ \\begin{aligned} &amp;H_0: \\sigma^2 = 0 \\\\ &amp;H_a: \\sigma^2 &gt; 0 \\end{aligned} \\] by considering \\(F= \\frac{MSA}{MSAB} \\sim F_{a-1;(a-1)(b-1)}\\) The same test statistic is used for mixed models, but in that case we are testing null hypothesis that all of the \\(\\alpha_i = 0\\) The test statistic different for the same null hypothesis under the fixed effects model. Test for effects of Fixed ANOVA (A&amp;B fixed) Random ANOVA (A&amp;B random) Mixed ANOVA (A fixed, B random) Factor A \\(\\frac{MSA}{MSE}\\) \\(\\frac{MSA}{MSAB}\\) \\(\\frac{MSA}{MSAB}\\) Factor B \\(\\frac{MSB}{MSE}\\) \\(\\frac{MSB}{MSAB}\\) \\(\\frac{MSB}{MSE}\\) AB interactions \\(\\frac{MSAB}{MSE}\\) \\(\\frac{MSAB}{MSE}\\) \\(\\frac{MSAB}{MSE}\\) Estimation Of Variance Components In random and mixed effects models, we are interested in estimating the variance components Variance component \\(\\sigma^2_\\beta\\) in the mixed ANOVA. \\[ E(\\sigma^2_\\beta) = \\frac{E(MSB)-E(MSE)}{na} = \\frac{\\sigma^2 + na \\sigma^2_\\beta - \\sigma^2}{na} = \\sigma^2_\\beta \\] which can be estimated with \\[ \\hat{\\sigma}^2_\\beta = \\frac{MSB - MSE}{na} \\] Confidence intervals for variance components can be constructed (approximately) by using the Satterthwaite procedure or the MLS procedure (like the 1-way random effects) Estimation of Fixed Effects in Mixed Models \\[ \\begin{aligned} \\hat{\\alpha}_i &amp;= \\bar{Y}_{i..} - \\bar{Y}_{...} \\\\ \\hat{\\mu}_{i.} &amp;= \\bar{Y}_{...} + (\\bar{Y}_{i..}- \\bar{Y}_{...}) = \\bar{Y}_{i..} \\\\ \\sigma^2(\\hat{\\alpha}_i) &amp;= \\frac{\\sigma^2 + n \\sigma^2_{\\alpha \\beta}}{bn} = \\frac{E(MSAB)}{bn} \\\\ s^2(\\hat{\\alpha}_i) &amp;= \\frac{MSAB}{bn} \\end{aligned} \\] Contrasts on the Fixed Effects \\[ \\begin{aligned} L &amp;= \\sum c_i \\alpha_i \\\\ \\sum c_i &amp;= 0 \\\\ \\hat{L} &amp;= \\sum c_i \\hat{\\alpha}_i \\\\ \\sigma^2(\\hat{L}) &amp;= \\sum c^2_i \\sigma^2 (\\hat{\\alpha}_i) \\\\ s^2(\\hat{L}) &amp;= \\frac{MSAB}{bn} \\sum c^2_i \\end{aligned} \\] Confidence intervals and tests can be constructed as usual 21.1.5.2 Unbalanced For a mixed model with a = 2, b = 4 \\[ \\begin{aligned} Y_{ijk} &amp;= \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\\\ var(\\beta_j)&amp;= \\sigma^2_\\beta \\\\ var((\\alpha \\beta)_{ij})&amp;= \\frac{2-1}{2}\\sigma^2_{\\alpha \\beta} = \\frac{\\sigma^2_{\\alpha \\beta}}{2} \\\\ var(\\epsilon_{ijk}) &amp;= \\sigma^2 \\\\ E(Y_{ijk}) &amp;= \\mu_{..} + \\alpha_i \\\\ var(Y_{ijk}) &amp;= \\sigma^2_{\\beta} + \\frac{\\sigma^2_{\\alpha \\beta}}{2} + \\sigma^2 \\\\ cov(Y_{ijk},Y_{ijk&#39;}) &amp;= \\sigma^2 + \\frac{\\sigma^2_{\\alpha \\beta}}{2}, k \\neq k&#39; \\\\ cov(Y_{ijk},Y_{i&#39;jk&#39;}) &amp;= \\sigma^2_{\\beta} - \\frac{\\sigma^2_{\\alpha \\beta}}{2}, i \\neq i&#39; \\\\ cov(Y_{ijk},Y_{i&#39;j&#39;k&#39;}) &amp;= 0, j \\neq j&#39; \\end{aligned} \\] assume \\[ \\mathbf{Y} \\sim N(\\mathbf{X}\\beta, M) \\] where \\(M\\) is block diagonal density function \\[ f(\\mathbf{Y}) = \\frac{1}{(2\\pi)^{N/2}|M|^{1/2}}exp(-\\frac{1}{2}\\mathbf{(Y - X \\beta)&#39; M^{-1}(Y-X\\beta)}) \\] if we knew the variance components, we could use GLS: \\[ \\hat{\\beta}_{GLS} = \\mathbf{(X&#39;M^{-1}X)^{-1}X&#39;M^{-1}Y} \\] but we usually don’t know the variance components \\(\\sigma^2, \\sigma^2_\\beta, \\sigma^2_{\\alpha \\beta}\\) that make up \\(M\\) Another way to get estimates is by Maximum likelihood estimation we try to maximize its log \\[ \\ln L = - \\frac{N}{2} \\ln (2\\pi) - \\frac{1}{2}\\ln|M| - \\frac{1}{2} \\mathbf{(Y-X \\beta)&#39;\\Sigma^{-1}(Y-X\\beta)} \\] "],["nonparametric-anova.html", "21.2 Nonparametric ANOVA", " 21.2 Nonparametric ANOVA 21.2.1 Kruskal-Wallis Generalization of independent samples Wilcoxon Rank sum test for 2 independent samples (like F-test of one-way ANOVA is a generalization to several independent samples of the two sample t-test) Consider the one-way case: We have \\(a\\ge2\\) treatments \\(n_i\\) is the sample size for the \\(i\\)-th treatment \\(Y_{ij}\\) is the \\(j\\)-th observation from the \\(i\\)-th treatment. we make no assumption of normality We only assume that observations on the \\(i\\)-th treatment are a random sample from the continuous CDF \\(F_i\\), i = 1,..,n, and are mutually independent. \\[ \\begin{aligned} &amp;H_0: F_1 = F_2 = ... = F_a \\\\ &amp;H_a: F_i &lt; F_j \\text{ for some } i \\neq j \\end{aligned} \\] or if distribution is from the location-scale family, \\(H_0: \\theta_1 = \\theta_2 = ... = \\theta_a\\)) Procedure Rank all \\(N = \\sum_{i=1}^a n_i\\) observations in ascending order. Let \\(r_{ij} = rank(Y_{ij})\\), note \\(\\sum_i \\sum_j r_{ij} = 1 + 2 .. + N = \\frac{N(N+1)}{2}\\) Calculate the rank sums and averages: \\[ r_{i.} = \\sum_{j=1}^{n_i} r_{ij} \\] and \\[ \\bar{r}_{i.} = \\frac{r_{i.}}{n_i}, i = 1,..,a \\] Calculate the test statistic on the ranks: \\[ \\chi_{KW}^2 = \\frac{SSTR}{\\frac{SSTO}{N-1}} \\] where \\(SSTR = \\sum n_i (\\bar{r}_{i.}- \\bar{r}_{..})^2\\) and \\(SSTO = \\sum \\sum (\\bar{r}_{ij}- \\bar{r}_{..})^2\\) For large \\(n_i\\) (\\(\\ge 5\\) observations) the Kruskal-Wallis statistic is approximated by a \\(\\chi^2_{a-1}\\) distribution when all the treatment means are equal. Hence, reject \\(H_0\\) if \\(\\chi^2_{KW} &gt; \\chi^2_{(1-\\alpha;a-1)}\\). If sample sizes are small, one can exhaustively work out all possible distinct ways of assigning N ranks to the observations from a treatments and calculate the value of the KW statistic in each case (\\(\\frac{N!}{n_1!..n_a!}\\) possible combinations). Under \\(H_0\\) all of these assignments are equally likely. 21.2.2 Friedman Test When the responses \\(Y_{ij} = 1,..,n, j = 1,..,r\\) in a randomized complete block design are not normally distributed (or do not have constant variance), a nonparametric test is more helpful. A distribution-free rank-based test for comparing the treatments in this setting is the Friedman test. Let \\(F_{ij}\\) be the CDF of random \\(Y_{ij}\\), corresponding to the observed value \\(y_{ij}\\) Under the null hypothesis, \\(F_{ij}\\) are identical for all treatments j separately for each block i. \\[ \\begin{aligned} &amp;H_0: F_{i1} = F_{i2} = ... = F_{ir} \\text{ for all i} \\\\ &amp;H_a: F_{ij} &lt; F_{ij&#39;} \\text{ for some } j \\neq j&#39; \\text{ for all } i \\end{aligned} \\] For location parameter distributions, treatment effects can be tested: \\[ \\begin{aligned} &amp;H_0: \\tau_1 = \\tau_2 = ... = \\tau_r \\\\ &amp;H_a: \\tau_j &gt; \\tau_{j&#39;} \\text{ for some } j \\neq j&#39; \\end{aligned} \\] Procedure Rank observations from the r treatments separately within each block (in ascending order; if ties, each tied observation is given the mean of ranks involved). Let the ranks be called \\(r_{ij}\\) Calculate the Friedman test statistic \\[ \\chi^2_F = \\frac{SSTR}{\\frac{SSTR + SSE}{n(r-1)}} \\] where \\[ \\begin{aligned} SSTR &amp;= n \\sum (\\bar{r}_{.j}-\\bar{r}_{..})^2 \\\\ SSE &amp;= \\sum \\sum (r_{ij} - \\bar{r}_{.j})^2 \\\\ \\bar{r}_{.j} &amp;= \\frac{\\sum_i r_{ij}}{n}\\\\ \\bar{r}_{..} &amp;= \\frac{r+1}{2} \\end{aligned} \\] If there is no ties, it can be rewritten as \\[ \\chi^2_{F} = [\\frac{12}{nr(n+1)}\\sum_j r_{.j}^2] - 3n(r+1) \\] with large number of blocks, \\(\\chi^2_F\\) is approximately \\(\\chi^2_{r-1}\\) under \\(H_0\\). Hence, we reject \\(H_0\\) if \\(\\chi^2_F &gt; \\chi^2_{(1-\\alpha;r-1)}\\) The exact null distribution for \\(\\chi^2_F\\) can be derived since there are r! possible ways of assigning ranks 1,2,…,r to the r observations within each block. There are n blocks and thus \\((r!)^n\\) possible assignments to the ranks, which are equally likely when \\(H_0\\) is true. "],["sample-size-planning-for-anova.html", "21.3 Sample Size Planning for ANOVA", " 21.3 Sample Size Planning for ANOVA 21.3.1 Balanced Designs 21.3.1.1 Single Factor Studies 21.3.1.1.1 Fixed cell means \\[ P(F&gt;f_{(1-\\alpha;a-1,N-a)}|\\phi) = 1 - \\beta \\] where \\(\\phi\\) is the non-centrality parameter (measures how unequal the treatment means \\(\\mu_i\\) are) \\[ \\phi = \\frac{1}{\\sigma}\\sqrt{\\frac{n}{a}\\sum_i (\\mu_i - \\mu_.)^2} , (n_i \\equiv n) \\] and \\[ \\mu_. = \\frac{\\sum \\mu_i}{a} \\] To decide on the power probabilities we use the non-central F distribution. We could use the power table directly when effects are fixed and design is balanced by using minimum range of factor level means for your desired differences \\[ \\Delta = \\max(\\mu_i) - \\min(\\mu_i) \\] Hence, we need \\(\\alpha\\) level \\(\\Delta\\) \\(\\sigma\\) \\(\\beta\\) Notes: When \\(\\Delta/\\sigma\\) is small greatly affects sample size, but if \\(\\Delta/\\sigma\\) is large. Reducing \\(\\alpha\\) or \\(\\beta\\) increases the required sample sizes. Error in estimating \\(\\sigma\\) can make a large difference. 21.3.1.2 Multi-factor Studies The same noncentral \\(F\\) tables can be used here For two-factor fixed effect model Test for interactions: \\[ \\begin{aligned} \\phi &amp;= \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\alpha \\beta_{ij})^2}{(a-1)(b-1)+1}} = \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\mu_{ij}- \\mu_{i.} - \\mu_{.j} + \\mu_{..})^2}{(a-1)(b-1)+1}} \\\\ \\upsilon_1 &amp;= (a-1)(b-1) \\\\ \\upsilon_2 &amp;= ab(n-1) \\end{aligned} \\] Test for Factor \\(A\\) main effects: \\[ \\begin{aligned} \\phi &amp;= \\frac{1}{\\sigma} \\sqrt{\\frac{nb \\sum \\alpha_i^2}{a}} = \\frac{1}{\\sigma}\\sqrt{\\frac{nb \\sum (\\mu_{i.}- \\mu_{..})^2}{a}} \\\\ \\upsilon_1 &amp;= a-1 \\\\ \\upsilon_2 &amp;= ab(n-1) \\end{aligned} \\] Test for Factor \\(B\\) main effects: \\[ \\begin{aligned} \\phi &amp;= \\frac{1}{\\sigma} \\sqrt{\\frac{na \\sum \\beta_j^2}{b}} = \\frac{1}{\\sigma}\\sqrt{\\frac{na \\sum (\\mu_{.j}- \\mu_{..})^2}{b}} \\\\ \\upsilon_1 &amp;= b-1 \\\\ \\upsilon_2 &amp;= ab(n-1) \\end{aligned} \\] Procedure: Specify the minimum range of Factor \\(A\\) means Obtain sample sizes with \\(r = a\\). The resulting sample size is \\(bn\\), from which \\(n\\) can be obtained. Repeat the first 2 steps for Factor \\(B\\) minimum range. Choose the greater number of sample size between \\(A\\) and \\(B\\). 21.3.2 Randomized Block Experiments Analogous to completely randomized designs . The power of the F-test for treatment effects for randomized block design uses the same non-centrality parameter as completely randomized design: \\[ \\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n}{r} \\sum (\\mu_i - \\mu_.)^2} \\] However, the power level is different from the randomized block design because error variance \\(\\sigma^2\\) is different df(MSE) is different. "],["randomized-block-designs.html", "21.4 Randomized Block Designs", " 21.4 Randomized Block Designs To improve the precision of treatment comparisons, we can reduce variability among the experimental units. We can group experimental units into blocks so that each block contains relatively homogeneous units. Within each block, random assignment treatments to units (separate random assignment for each block) The number of units per block is a multiple of the number of factor combinations. Commonly, use each treatment once in each block. Benefits of Blocking Reduction in variability of estimators for treatment means Improved power for t-tests and F-tests Narrower confidence intervals Smaller MSE Compare treatments under different conditions (related to different blocks). Loss from Blocking (little to lose) If you don’t do blocking well, you waste df on negligible block effects that could have been used to estimate \\(\\sigma^2\\) Hence, the df for \\(t\\)-tests and denominator df for \\(F\\)-tests will be reduced without reducing MSE and small loss of power for both tests. Consider \\[ Y_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij} \\] where \\(i = 1, 2, \\dots, n\\) \\(j = 1, 2, \\dots, r\\) \\(\\mu_{..}\\): overall mean response, averaging across all blocks and treatments \\(\\rho_i\\): block effect, average difference in response for i-th block (\\(\\sum \\rho_i =0\\)) \\(\\tau_j\\) treatment effect, average across blocks (\\(\\sum \\tau_j = 0\\)) \\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random experimental error. Here, we assume that the block and treatment effects are additive. The difference in average response for any pair of treatments i the same within each block \\[ (\\mu_{..} + \\rho_i + \\tau_j) - (\\mu_{..} + \\rho_i + \\tau_j&#39;) = \\tau_j - \\tau_j&#39; \\] for all \\(i=1,..,n\\) blocks \\[ \\begin{aligned} \\hat{\\mu} &amp;= \\bar{Y}_{..} \\\\ \\hat{\\rho}_i &amp;= \\bar{Y}_{i.} - \\bar{Y}_{..} \\\\ \\hat{\\tau}_j &amp;= \\bar{Y}_{.j} - \\bar{Y}_{..} \\end{aligned} \\] Hence, \\[ \\begin{aligned} \\hat{Y}_{ij} &amp;= \\bar{Y}_{..} + (\\bar{Y}_{i.} - \\bar{Y}_{..}) + (\\bar{Y}_{.j}- \\bar{Y}_{..}) = \\bar{Y}_{i.} + \\bar{Y}_{.j} - \\bar{Y}_{..} \\\\ e_{ij} &amp;= Y_{ij} - \\hat{Y}_{ij} = Y_{ij}- \\bar{Y}_{i.} - \\bar{Y}_{.j} + \\bar{Y}_{..} \\end{aligned} \\] ANOVA table Source of Variation SS df Fixed Treatments E(MS) Random Treatments E(MS) Blocks \\(r \\sum_i(\\bar{Y}_{i.}-\\bar{Y}_{..})^2\\) \\(n - 1\\) \\(\\sigma^2 +r \\frac{\\sum \\rho^2_i}{n-1}\\) \\(\\sigma^2 + r \\frac{\\sum \\rho^2_i}{n-1}\\) Treatments \\(n\\sum_ j (\\bar{Y} _ {.j}-\\bar{ Y}_{..})^2\\) \\(r - 1\\) \\(\\sigma^2 + n \\frac{\\sum \\tau^2_j}{r-1}\\) \\(\\sigma^2 + n \\sigma^2_\\tau\\) Error \\(\\sum_i \\sum _j ( Y_{ ij } - \\bar { Y}_{i.} - \\bar{Y}_{.j} + \\bar{ Y}_{..})^2\\) \\((n-1)(r-1)\\) \\(\\sigma^2\\) \\(\\sigma^2\\) Total \\(SSTO\\) \\(nr-1\\) F-tests \\[ \\begin{aligned} H_0: \\tau_1 = \\tau_2 = ... = \\tau_r = 0 &amp;&amp; \\text{Fixed Treatment Effects} \\\\ H_a: \\text{not all } \\tau_j = 0 \\\\ \\\\ H_0: \\sigma^2_{\\tau} = 0 &amp;&amp; \\text{Random Treatment Effects} \\\\ H_a: \\sigma^2_{\\tau} \\neq 0 \\end{aligned} \\] In both cases \\(F = \\frac{MSTR}{MSE}\\), reject \\(H_0\\) if \\(F &gt; f_{(1-\\alpha; r-1,(n-1)(r-1))}\\) we don’t use F-test to compare blocks, because We have a priori that blocs are different Randomization is done “within” block. To estimate the efficiency that was gained by blocking (relative to completely randomized design). \\[ \\begin{aligned} \\hat{\\sigma}^2_{CR} &amp;= \\frac{(n-1)MSBL + n(r-1)MSE}{nr-1} \\\\ \\hat{\\sigma}^2_{RB} &amp;= MSE \\\\ \\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}} &amp;= \\text{above 1} \\\\ \\end{aligned} \\] then a completely randomized experiment would \\[ (\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}}-1)\\%% \\] more observations than the randomized block design to get the same MSE If batches are randomly selected then they are random effects. That is , if the experiment was repeated, a new sample of i batches would be selected,d yielding new values for \\(\\rho_1, \\rho_2,...,\\rho_i\\) then. \\[ \\rho_1, \\rho_2,...,\\rho_j \\sim N(0,\\sigma^2_\\rho) \\] Then, \\[ Y_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij} \\] where \\(\\mu_{..}\\) fixed \\(\\rho_i\\): random iid \\(N(0,\\sigma^2_p)\\) \\(\\tau_j\\) fixed (or random) \\(\\sum \\tau_j = 0\\) \\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\) Fixed Treatment \\[ \\begin{aligned} E(Y_{ij}) &amp;= \\mu_{..} + \\tau_j \\\\ var(Y_{ij}) &amp;= \\sigma^2_{\\rho} + \\sigma^2 \\end{aligned} \\] \\[ \\begin{aligned} cov(Y_{ij},Y_{ij&#39;}) &amp;= \\sigma^2 , j \\neq j&#39; \\text{ treatments within same block are correlated} \\\\ cov(Y_{ij},Y_{i&#39;j&#39;}) &amp;= 0 , i \\neq i&#39; , j \\neq j&#39; \\end{aligned} \\] Correlation between 2 observations in the same block \\[ \\frac{\\sigma^2_{\\rho}}{\\sigma^2 + \\sigma^2_{\\rho}} \\] The expected MS for the additive fixed treatment effect, random block effect is Source SS E(MS) Blocks SSBL \\(\\sigma^2 + r \\sigma^2_\\rho\\) Treatment SSTR \\(\\sigma^2 + n \\frac{\\sum \\tau^2_j}{r-1}\\) Error SSE \\(\\sigma^2\\) Interactions and Blocks without replications within each block for each treatment, we can’t consider interaction between block and treatment when the block effect is fixed. Hence, only in the random block effect, we have \\[ Y_{ij} = \\mu_{..} + \\rho_i + \\tau_j + (\\rho \\tau)_{ij} + \\epsilon_{ij} \\] where \\(\\mu_{..}\\) constant \\(\\rho_i \\sim idd N(0,\\sigma^2_{\\rho})\\) random \\(\\tau_j\\) fixed (\\(\\sum \\tau_j = 0\\)) \\((\\rho \\tau)_{ij} \\sim N(0,\\frac{r-1}{r}\\sigma^2_{\\rho \\tau})\\) with \\(\\sum_j (\\rho \\tau)_{ij}=0\\) for all i \\(cov((\\rho \\tau)_{ij},(\\rho \\tau)_{ij&#39;})= -\\frac{1}{r} \\sigma^2_{\\rho \\tau}\\) for \\(j \\neq j&#39;\\) \\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\) random Note: a special case of mixed 2-factor model with 1 observation per “cell” \\[ \\begin{aligned} E(Y_{ij}) &amp;= \\mu_{..} + \\tau_j \\\\ var(Y_{ij}) &amp;= \\sigma^2_\\rho + \\frac{r-1}{r} \\sigma^2_{\\rho \\tau} + \\sigma^2 \\end{aligned} \\] \\[ \\begin{aligned} cov(Y_{ij},Y_{ij&#39;}) &amp;= \\sigma^2_\\rho - \\frac{1}{r} \\sigma^2_{\\rho \\tau}, j \\neq j&#39; \\text{ obs from the same block are correlated} \\\\ cov(Y_{ij},Y_{i&#39;j&#39;}) &amp;= 0, i \\neq i&#39;, j \\neq j&#39; \\text{ obs from different blocks are independent} \\end{aligned} \\] The sum of squares and degrees of freedom for interaction model are the same as those for the additive model. The difference exists only with the expected mean squares Source SS df E(MS) Blocks \\(SSBL\\) \\(n-1\\) \\(\\sigma^2 + r \\sigma^2_\\rho\\) Treatment \\(SSTR\\) \\(r -1\\) \\(\\sigma^2 + \\sigma ^2_{\\rho \\tau} + n \\frac{\\sum \\tau_j^2}{r-1}\\) Error \\(SSE\\) \\((n-1)(r-1)\\) \\(\\sigma^2 + \\sigma ^2_{\\rho \\tau}\\) No exact test is possible for block effects when interaction is present (Not important if blocks are used primarily to reduce experimental error variability) \\(E(MSE) = \\sigma^2 + \\sigma^2_{\\rho \\tau}\\) the error term variance and interaction variance \\(\\sigma^2_{\\rho \\tau}\\). We can’t estimate these components separately with this model. The two are confounded. If more than 1 observation per treatment block combination, one can consider interaction with fixed block effects, which is called generalized randomized block designs (multifactor analysis). 21.4.1 Tukey Test of Additivity (Tukey’s 1 df test for additivity) formal test of interaction effects between blocks and treatments for a randomized block design. can also considered for testing additivity in 2-way analyses when there is only one observation per cell. we consider a less restricted interaction term \\[ (\\rho \\tau)_{ij} = D\\rho_i \\tau_j \\text{(D: Constant)} \\] So, \\[ Y_{ij} = \\mu_{..} + \\rho_i + \\tau_j + D\\rho_i \\tau_j + \\epsilon_{ij} \\] the least square estimate or MLE for D \\[ \\hat{D} = \\frac{\\sum_i \\sum_j \\rho_i \\tau_j Y_{ij}}{\\sum_i \\rho_i^2 \\sum_j \\tau^2_j} \\] replacing the parameters by their estimates \\[ \\hat{D} = \\frac{\\sum_i \\sum_j (\\bar{Y}_{i.}- \\bar{Y}_{..})(\\bar{Y}_{.j}- \\bar{Y}_{..})Y_{ij}}{\\sum_i (\\bar{Y}_{i.}- \\bar{Y}_{..})^2 \\sum_j(\\bar{Y}_{.j}- \\bar{Y}_{..})^2} \\] Thus, the interaction sum of squares \\[ SSint = \\sum_i \\sum_j \\hat{D}^2(\\bar{Y}_{i.}- \\bar{Y}_{..})^2(\\bar{Y}_{.j}- \\bar{Y}_{..})^2 \\] The ANOVA decomposition \\[ SSTO = SSBL + SSTR + SSint + SSRem \\] where \\(SSRem\\): remainder sum of squares \\[ SSRem = SSTO - SSBL - SSTR - SSint \\] if \\(D = 0\\) (i.e., no interactions of the type \\(D \\rho_i \\tau_j\\)). \\(SSint\\) and \\(SSRem\\) are independent \\(\\chi^2_{1,rn-r-n}\\). If \\(D = 0\\), \\[ F = \\frac{SSint/1}{SSRem/(rn-r-n)} \\sim f_{(1-\\alpha;rn-r-n)} \\] if \\[ \\begin{aligned} &amp;H_0: D = 0 \\text{ no interaction present} \\\\ &amp;H_a: D \\neq 0 \\text{ interaction of form $D \\rho_i \\tau_j$ present} \\end{aligned} \\] we reject \\(H_0\\) if \\(F &gt; f_{(1-\\alpha;1,nr-r-n)}\\) "],["nested-designs.html", "21.5 Nested Designs", " 21.5 Nested Designs Let \\(\\mu_{ij}\\) be the mean response when factor A is at the i-th level and factor B is at the j-th level. If the factors are crossed, the \\(j\\)-th level of B is the same for all levels of A. If factor B is nested within A, the j-th level of B when A is at level 1 has nothing in common with the j-th level of B when A is at level 2. Factors that can’t be manipulated are designated as classification factors, as opposed to experimental factors (i.e., you assign to the experimental units). 21.5.1 Two-Factor Nested Designs Consider B is nested within A. both factors are fixed All treatment means are equally important. Mean responses \\[ \\mu_{i.} = \\sum_j \\mu_{ij}/b \\] Main effect factor A \\[ \\alpha_i = \\mu_{i.} - \\mu_{..} \\] where \\(\\mu_{..} = \\frac{\\mu_{ij}}{ab} = \\frac{\\sum_i \\mu_{i.}}{a}\\) and \\(\\sum_i \\alpha_i = 0\\) Individual effects of \\(B\\) is denoted as \\(\\beta_{j(i)}\\) where \\(j(i)\\) indicates the \\(j\\)-th level of factor \\(B\\) is nested within the it-h level of factor A \\[ \\begin{aligned} \\beta_{j(i)} &amp;= \\mu_{ij} - \\mu_{i.} \\\\ &amp;= \\mu_{ij} - \\alpha_i - \\mu_{..} \\\\ \\sum_j \\beta_{j(i)}&amp;=0 , i = 1,...,a \\end{aligned} \\] \\(\\beta_{j(i)}\\) is the specific effect of the \\(j\\)-th level of factor \\(B\\) nested within the \\(i\\)-th level of factor \\(A\\). Hence, \\[ \\mu_{ij} \\equiv \\mu_{..} + \\alpha_i + \\beta_{j(i)} \\equiv \\mu_{..} + (\\mu_{i.} - \\mu_{..}) + (\\mu_{ij} - \\mu_{i.}) \\] Model \\[ Y_{ijk} = \\mu_{..} + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk} \\] where \\(Y_{ijk}\\) response for the \\(k\\)-th treatment when factor \\(A\\) is at the \\(i\\)-th level and factor \\(B\\) is at the \\(j\\)-th level \\((i = 1,..,a; j = 1,..,b; k = 1,..n)\\) \\(\\mu_{..}\\) constant \\(\\alpha_i\\) constants subject to restriction \\(\\sum_i \\alpha_i = 0\\) \\(\\beta_{j(i)}\\) constants subject to restriction \\(\\sum_j \\beta_{j(i)} = 0\\) for all \\(i\\) \\(\\epsilon_{ijk} \\sim iid N(0,\\sigma^2)\\) \\[ \\begin{aligned} E(Y_{ijk}) &amp;= \\mu_{..} + \\alpha_i + \\beta_{j(i)} \\\\ var(Y_{ijk}) &amp;= \\sigma^2 \\end{aligned} \\] there is no interaction term in a nested model ANOVA for Two-Factor Nested Designs Least Squares and MLE estimates Parameter Estimator \\(\\mu_{..}\\) \\(\\bar{Y}_{...}\\) \\(\\alpha_i\\) \\(\\bar{Y}_{i..} - \\bar{Y}_{...}\\) \\(\\beta_{j(i)}\\) \\(\\bar{Y}_{ij.} - \\bar{Y}_{i..}\\) \\(\\hat{Y}_{ijk}\\) \\(\\bar{Y}_{ij.}\\) residual \\(e_{ijk} = Y_{ijk} - \\bar{Y}_{ijk}\\) \\[ \\begin{aligned} SSTO &amp;= SSA + SSB(A) + SSE \\\\ \\sum_i \\sum_j \\sum_k (Y_{ijk}- \\bar{Y}_{...})^2 &amp;= bn \\sum_i (\\bar{Y}_{i..}- \\bar{Y}_{...})^2 + n \\sum_i \\sum_j (\\bar{Y}_{ij.}- \\bar{Y}_{i..})^2 \\\\ &amp;+ \\sum_i \\sum_j \\sum_k (Y_{ijk} -\\bar{Y}_{ij.})^2 \\end{aligned} \\] ANOVA Table Source of Variation SS df MS E(MS) Factor A \\(SSA\\) \\(a-1\\) \\(MSA\\) \\(\\sigma^2 + bn \\frac{\\sum \\alpha_i^2}{a-1}\\) Factor B \\(SSB(A)\\) \\(a(b-1)\\) \\(MSB(A)\\) \\(\\sigma^2 + n \\frac{\\ | | | | um \\sum e ta_{i)}^ 2}{a(b-1)}\\) Error \\(SSE\\) \\(ab(n-1)\\) \\(MSE\\) \\(\\sigma^2\\) Total \\(SSTO\\) \\(abn -1\\) Tests For Factor Effects \\[ \\begin{aligned} &amp;H_0: \\text{ All } \\alpha_i =0 \\\\ &amp;H_a: \\text{ not all } \\alpha_i = 0 \\end{aligned} \\] \\(F = \\frac{MSA}{MSE} \\sim f_{(1-\\alpha;a-1,(n-1)ab)}\\) reject if \\(F &gt; f\\) \\[ \\begin{aligned} &amp;H_0: \\text{ All } \\beta_{j(i)} =0 \\\\ &amp;H_a: \\text{ not all } \\beta_{j(i)} = 0 \\end{aligned} \\] \\(F = \\frac{MSB(A)}{MSE} \\sim f_{(1-\\alpha;a(b-1),(n-1)ab)}\\) reject \\(F&gt;f\\) Testing Factor Effect Contrasts \\(L = \\sum c_i \\mu_i\\) where \\(\\sum c_i =0\\) \\[ \\begin{aligned} \\hat{L} &amp;= \\sum c_i \\bar{Y}_{i..} \\\\ \\hat{L} &amp;\\pm t_{(1-\\alpha/2;df)}s(\\hat{L}) \\end{aligned} \\] where \\(s^2(\\hat{L}) = \\sum c_i^2 s^2(\\bar{Y}_{i..})\\), where \\(s^2(\\bar{Y}_{i..}) = \\frac{MSE}{bn}, df = ab(n-1)\\) Testing Treatment Means \\(L = \\sum c_i \\mu_{.j}\\) estimated by \\(\\hat{L} = \\sum c_i \\bar{Y}_{ij}\\) with confidence limits: \\[ \\hat{L} \\pm t_{(1-\\alpha/2;(n-1)ab)}s(\\hat{L}) \\] where \\[ s^2(\\hat{L}) = \\frac{MSE}{n}\\sum c^2_i \\] Unbalanced Nested Two-Factor Designs If there are different number of levels of factor \\(B\\) for different levels of factor \\(A\\), then the design is called unbalanced The model \\[ \\begin{aligned} Y_{ijk} &amp;= \\mu_{..} + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk} \\\\ \\sum_{i=1}^2 \\alpha_i &amp;=0 \\\\ \\sum_{j=1}^3 \\beta_{j(1)} &amp;= 0 \\\\ \\sum_{j=1}^2 \\beta_{j(2)}&amp;=0 \\end{aligned} \\] where \\(i = 1,2;j =1,..,b_i;k=1,..,n_{ij}\\) \\(b_1 = 3, b_2= 2, n_{11} = n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\\) \\(\\alpha_1,\\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\) are parameters. And constraints: \\(\\alpha_2 = - \\alpha_1, \\beta_{3(1)}= - \\beta_{1(1)}-\\beta_{2(1)}, \\beta_{2(2)}=-\\beta_{1(2)}\\) 4 indicator variables \\[\\begin{equation} X_1 = \\begin{cases} 1&amp;\\text{if obs from school 1}\\\\ -1&amp;\\text{if obs from school 2}\\\\ \\end{cases} \\end{equation}\\] \\[\\begin{equation} X_2 = \\begin{cases} 1&amp;\\text{if obs from instructor 1 in school 1}\\\\ -1&amp;\\text{if obs from instructor 3 in school 1}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases} \\end{equation}\\] \\[\\begin{equation} X_3 = \\begin{cases} 1&amp;\\text{if obs from instructor 2 in school 1}\\\\ -1&amp;\\text{if obs from instructor 3 in school 1}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases} \\end{equation}\\] \\[\\begin{equation} X_4 = \\begin{cases} 1&amp;\\text{if obs from instructor 1 in school 1}\\\\ -1&amp;\\text{if obs from instructor 2 in school 1}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases} \\end{equation}\\] Regression Full Model \\[ Y_{ijk} = \\mu_{..} + \\alpha_1 X_{ijk1} + \\beta_{1(1)}X_{ijk2} + \\beta_{2(1)}X_{ijk3} + \\beta_{1(2)}X_{ijk4} + \\epsilon_{ijk} \\] Random Factor Effects If \\[ \\begin{aligned} \\alpha_1 &amp;\\sim iid N(0,\\sigma^2_\\alpha) \\\\ \\beta_{j(i)} &amp;\\sim iid N(0,\\sigma^2_\\beta) \\end{aligned} \\] Mean Square Expected Mean Squares A fixed, B random Expected Mean Squares A random, B random MSA \\(\\sigma^ 2 + n \\sigma^2_\\beta + bn \\frac{\\sum \\alpha_i^2}{a-1}\\) \\(\\sigma^2 + bn \\sigma^2_{\\alpha} + n \\sigma^2_\\beta\\) MSB(A) \\(\\sigma^2 + n \\sigma^2_\\beta\\) \\(\\sigma^2 + n \\sigma^2_\\beta\\) MSE \\(\\sigma^2\\) \\(\\sigma^2\\) Test Statistics Factor A \\(\\frac{MSA}{MSB(A)}\\) \\(\\frac{MSA}{MSB(A)}\\) Factor B(A) \\(\\frac{MSB(A)}{MSE}\\) \\(\\frac{MSB(A)}{MSE}\\) Another way to increase the precision of treatment comparisons by reducing variability is to use regression models to adjust for differences among experimental units (also known as analysis of covariance). "],["single-factor-covariance-model.html", "21.6 Single Factor Covariance Model", " 21.6 Single Factor Covariance Model \\[ Y_{ij} = \\mu_{.} + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) + \\epsilon_{ij} \\] for \\(i = 1,...,r;j=1,..,n_i\\) where \\(\\mu_.\\) overall mean \\(\\tau_i\\): fixed treatment effects (\\(\\sum \\tau_i =0\\)) \\(\\gamma\\): fixed regression coefficient effect between X and Y \\(X_{ij}\\) covariate (not random) \\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random errors If we just use \\(\\gamma X_{ij}\\) as the regression term (rather than \\(\\gamma(X_{ij}-\\bar{X}_{..})\\)), then \\(\\mu_.\\) is no longer the overall mean; thus we need to centered mean. \\[ \\begin{aligned} E(Y_{ij}) &amp;= \\mu_. + \\tau_i + \\gamma(X_{ij}-\\bar{X}_{..}) \\\\ var(Y_{ij}) &amp;= \\sigma^2 \\end{aligned} \\] \\(Y_{ij} \\sim N(\\mu_{ij},\\sigma^2)\\), where \\[ \\begin{aligned} \\mu_{ij} &amp;= \\mu_. + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) \\\\ \\sum \\tau_i &amp;=0 \\end{aligned} \\] Thus, the mean response (\\(\\mu_{ij}\\)) is a regression line with intercept \\(\\mu_. + \\tau_i\\) and slope \\(\\gamma\\) for each treatment $$i. Assumption: All treatment regression lines have the same slope when treatment interact with covariate \\(X\\) (non-parallel slopes), covariance analysis is not appropriate. in which case we should use separate regression lines. More complicated regression features (e.g., quadratic, cubic) or additional covariates e.g., \\[ Y_{ij} = \\mu_. + \\tau_i + \\gamma_1(X_{ij1}-\\bar{X}_{..2}) + \\gamma_2(X_{ij2}-\\bar{X}_{..2}) + \\epsilon_{ij} \\] Regression Formulation We can use indicator variables for treatments \\[ l_1 = \\begin{cases} 1 &amp; \\text{if case is from treatment 1}\\\\ -1 &amp; \\text{if case is from treatment r}\\\\ 0 &amp;\\text{otherwise}\\\\ \\end{cases} \\] \\[ . \\] \\[ . \\] \\[ l_{r-1} = \\begin{cases} 1 &amp; \\text{if case is from treatment r-1}\\\\ -1 &amp; \\text{if case is from treatment r}\\\\ 0 &amp;\\text{otherwise}\\\\ \\end{cases} \\] Let \\(x_{ij} = X_{ij}- \\bar{X}_{..}\\). the regression model is \\[ Y_{ij} = \\mu_. + \\tau_1l_{ij,1} + .. + \\tau_{r-1}l_{ij,r-1} + \\gamma x_{ij}+\\epsilon_{ij} \\] where \\(I_{ij,1}\\) is the indicator variable \\(l_1\\) for the j-th case from treatment i. The treatment effect \\(\\tau_1,..\\tau_{r-1}\\) are just regression coefficients for the indicator variables. We could use the same diagnostic tools for this case. Inference Treatment effects \\[ \\begin{aligned} &amp;H_0: \\tau_1 = \\tau_2 = ...= 0 \\\\ &amp;H_a: \\text{not all } \\tau_i =0 \\end{aligned} \\] \\[ \\begin{aligned} &amp;\\text{Full Model}: Y_{ij} = \\mu_. + \\tau_i + \\gamma X_{ij} +\\epsilon_{ij} \\\\ &amp;\\text{Reduced Model}: Y_{ij} = \\mu_. + \\gamma X_{ij} + \\epsilon_{ij} \\end{aligned} \\] \\[ F = \\frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} / \\frac{SSE(F)}{N-(r+1)} \\sim F_{(r-1,N-(r+1))} \\] If we are interested in comparisons of treatment effects. For example, r - 3. We estimate \\(\\tau_1,\\tau_2, \\tau_3 = -\\tau_1 - \\tau_2\\) Comparison Estimate Variance of Estimator \\(\\tau_1 - \\tau_2\\) \\(\\hat{\\tau}_1 - \\hat{\\tau}_2\\) \\(var(\\hat {\\tau}_1) + var(\\hat{\\tau}_2) - 2cov(\\hat{ \\tau}_1\\hat{\\tau}_2)\\) \\(\\tau_1 - \\tau_3\\) \\(2 \\hat{\\tau}_1 + \\hat{\\tau}_2\\) \\(4var(\\hat {\\tau}_1) + var(\\hat{\\tau}_2) - 4cov(\\hat{ \\tau}_1\\hat{\\tau}_2)\\) \\(\\tau_2 - \\tau_3\\) \\(\\hat{\\tau}_1 + 2 \\hat{\\tau}_2\\) \\(var(\\hat{\\tau}_1) + 4var(\\hat{\\tau}_2) - 4cov(\\hat{\\tau}_1\\hat{\\tau}_2)\\) Testing for Parallel Slopes Example: r = 3 \\[ Y_{ij} = \\mu_{.} + \\tau_1 I_{ij,1} + \\tau_2 I_{ij,2} + \\gamma X_{ij} + \\beta_1 I_{ij,1}X_{ij} + \\beta_2 I_{ij,2}X_{ij} + \\epsilon_{ij} \\] where \\(\\beta_1,\\beta_2\\): interaction coefficients. \\[ \\begin{aligned} &amp;H_0: \\beta_1 = \\beta_2 = 0 \\\\ &amp;H_a: \\text{at least one} \\beta \\neq 0 \\end{aligned} \\] If we can’t reject \\(H_0\\) using F-test then we have evidence that the slopes are parallel Adjusted Means The means in response after adjusting for the covariate effect \\[ Y_{i.}(adj) = \\bar{Y}_{i.} - \\hat{\\gamma}(\\bar{X}_{i.} - \\bar{X}_{..}) \\] "],["multivariate-methods.html", "Chapter 22 Multivariate Methods", " Chapter 22 Multivariate Methods \\(y_1,...,y_p\\) are possibly correlated random variables with means \\(\\mu_1,...,\\mu_p\\) \\[ \\mathbf{y} = \\left( \\begin{array} {c} y_1 \\\\ . \\\\ y_p \\\\ \\end{array} \\right) \\] \\[ E(\\mathbf{y}) = \\left( \\begin{array} {c} \\mu_1 \\\\ . \\\\ \\mu_p \\\\ \\end{array} \\right) \\] Let \\(\\sigma_{ij} = cov(y_i, y_j)\\) for \\(i,j = 1,…,p\\) \\[ \\mathbf{\\Sigma} = (\\sigma_{ij}) = \\left( \\begin{array} {cccc} \\sigma_{11} &amp; \\sigma_{22} &amp; ... &amp; \\sigma_{1p} \\\\ \\sigma_{21} &amp; \\sigma_{22} &amp; ... &amp; \\sigma_{2p} \\\\ . &amp; . &amp; . &amp; . \\\\ \\sigma_{p1} &amp; \\sigma_{p2} &amp; ... &amp; \\sigma_{pp} \\end{array} \\right) \\] where \\(\\mathbf{\\Sigma}\\) (symmetric) is the variance-covariance or dispersion matrix Let \\(\\mathbf{u}_{p \\times 1}\\) and \\(\\mathbf{v}_{q \\times 1}\\) be random vectors with means \\(\\mu_u\\) and \\(\\mu_v\\) . Then \\[ \\mathbf{\\Sigma}_{uv} = cov(\\mathbf{u,v}) = E[(\\mathbf{u} - \\mu_u)(\\mathbf{v} - \\mu_v)&#39;] \\] in which \\(\\mathbf{\\Sigma}_{uv} \\neq \\mathbf{\\Sigma}_{vu}\\) and \\(\\mathbf{\\Sigma}_{uv} = \\mathbf{\\Sigma}_{vu}&#39;\\) Properties of Covariance Matrices Symmetric \\(\\mathbf{\\Sigma}&#39; = \\mathbf{\\Sigma}\\) Non-negative definite \\(\\mathbf{a&#39;\\Sigma a} \\ge 0\\) for any \\(\\mathbf{a} \\in R^p\\), which is equivalent to eigenvalues of \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p \\ge 0\\) \\(|\\mathbf{\\Sigma}| = \\lambda_1 \\lambda_2 ... \\lambda_p \\ge 0\\) (generalized variance) (the bigger this number is, the more variation there is \\(trace(\\mathbf{\\Sigma}) = tr(\\mathbf{\\Sigma}) = \\lambda_1 + ... + \\lambda_p = \\sigma_{11} + ... + \\sigma_{pp} =\\) sum of variance (total variance) Note: \\(\\mathbf{\\Sigma}\\) is typically required to be positive definite, which means all eigenvalues are positive, and \\(\\mathbf{\\Sigma}\\) has an inverse \\(\\mathbf{\\Sigma}^{-1}\\) such that \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma} = \\mathbf{I}_{p \\times p} = \\mathbf{\\Sigma \\Sigma}^{-1}\\) Correlation Matrices \\[ \\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}} \\] \\[ \\mathbf{R} = \\left( \\begin{array} {cccc} \\rho_{11} &amp; \\rho_{12} &amp; ... &amp; \\rho_{1p} \\\\ \\rho_{21} &amp; \\rho_{22} &amp; ... &amp; \\rho_{2p} \\\\ . &amp; . &amp; . &amp;. \\\\ \\rho_{p1} &amp; \\rho_{p2} &amp; ... &amp; \\rho_{pp} \\\\ \\end{array} \\right) \\] where \\(\\rho_{ij}\\) is the correlation, and \\(\\rho_{ii} = 1\\) for all i Alternatively, \\[ \\mathbf{R} = [diag(\\mathbf{\\Sigma})]^{-1/2}\\mathbf{\\Sigma}[diag(\\mathbf{\\Sigma})]^{-1/2} \\] where \\(diag(\\mathbf{\\Sigma})\\) is the matrix which has the \\(\\sigma_{ii}\\)’s on the diagonal and 0’s elsewhere and \\(\\mathbf{A}^{1/2}\\) (the square root of a symmetric matrix) is a symmetric matrix such as \\(\\mathbf{A} = \\mathbf{A}^{1/2}\\mathbf{A}^{1/2}\\) Equalities Let \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) be random vectors with means \\(\\mu_x\\) and \\(\\mu_y\\) and variance -variance matrices \\(\\mathbf{\\Sigma}_x\\) and \\(\\mathbf{\\Sigma}_y\\). \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices of constants and \\(\\mathbf{c}\\) and \\(\\mathbf{d}\\) be vectors of constants Then \\(E(\\mathbf{Ay + c} ) = \\mathbf{A} \\mu_y + c\\) \\(var(\\mathbf{Ay + c}) = \\mathbf{A} var(\\mathbf{y})\\mathbf{A}&#39; = \\mathbf{A \\Sigma_y A}&#39;\\) \\(cov(\\mathbf{Ay + c, By+ d}) = \\mathbf{A\\Sigma_y B}&#39;\\) \\(E(\\mathbf{Ay + Bx + c}) = \\mathbf{A \\mu_y + B \\mu_x + c}\\) \\(var(\\mathbf{Ay + Bx + c}) = \\mathbf{A \\Sigma_y A&#39; + B \\Sigma_x B&#39; + A \\Sigma_{yx}B&#39; + B\\Sigma&#39;_{yx}A&#39;}\\) Multivariate Normal Distribution Let \\(\\mathbf{y}\\) be a multivariate normal (MVN) random variable with mean \\(\\mu\\) and variance \\(\\mathbf{\\Sigma}\\). Then the density of \\(\\mathbf{y}\\) is \\[ f(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2} \\mathbf{(y-\\mu)&#39;\\Sigma^{-1}(y-\\mu)} ) \\] \\(\\mathbf{y} \\sim N_p(\\mu, \\mathbf{\\Sigma})\\) 22.0.1 Properties of MVN Let \\(\\mathbf{A}_{r \\times p}\\) be a fixed matrix. Then \\(\\mathbf{Ay} \\sim N_r (\\mathbf{A \\mu, A \\Sigma A&#39;})\\) . \\(r \\le p\\) and all rows of \\(\\mathbf{A}\\) must be linearly independent to guarantee that \\(\\mathbf{A \\Sigma A}&#39;\\) is non-singular. Let \\(\\mathbf{G}\\) be a matrix such that \\(\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}&#39;\\). Then \\(\\mathbf{G&#39;y} \\sim N_p(\\mathbf{G&#39; \\mu, I})\\) and \\(\\mathbf{G&#39;(y-\\mu)} \\sim N_p (0,\\mathbf{I})\\) Any fixed linear combination of \\(y_1,...,y_p\\) (say \\(\\mathbf{c&#39;y}\\)) follows \\(\\mathbf{c&#39;y} \\sim N_1 (\\mathbf{c&#39; \\mu, c&#39; \\Sigma c})\\) Define a partition, \\([\\mathbf{y}&#39;_1,\\mathbf{y}_2&#39;]&#39;\\) where \\(\\mathbf{y}_1\\) is \\(p_1 \\times 1\\) \\(\\mathbf{y}_2\\) is \\(p_2 \\times 1\\), \\(p_1 + p_2 = p\\) \\(p_1,p_2 \\ge 1\\) Then \\[ \\left( \\begin{array} {c} \\mathbf{y}_1 \\\\ \\mathbf{y}_2 \\\\ \\end{array} \\right) \\sim N \\left( \\left( \\begin{array} {c} \\mu_1 \\\\ \\mu_2 \\\\ \\end{array} \\right), \\left( \\begin{array} {cc} \\mathbf{\\Sigma}_{11} &amp; \\mathbf{\\Sigma}_{12} \\\\ \\mathbf{\\Sigma}_{21} &amp; \\mathbf{\\Sigma}_{22}\\\\ \\end{array} \\right) \\right) \\] The marginal distributions of \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\) are \\(\\mathbf{y}_1 \\sim N_{p1}(\\mathbf{\\mu_1, \\Sigma_{11}})\\) and \\(\\mathbf{y}_2 \\sim N_{p2}(\\mathbf{\\mu_2, \\Sigma_{22}})\\) Individual components \\(y_1,...,y_p\\) are all normally distributed \\(y_i \\sim N_1(\\mu_i, \\sigma_{ii})\\) The conditional distribution of \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\) is normal \\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\) In this formula, we see if we know (have info about) \\(\\mathbf{y}_2\\), we can re-weight \\(\\mathbf{y}_1\\) ’s mean, and the variance is reduced because we know more about \\(\\mathbf{y}_1\\) because we know \\(\\mathbf{y}_2\\) which is analogous to \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). And \\(\\mathbf{y}_1\\) and \\(\\mathbf{y}_2\\) are independently distrusted only if \\(\\mathbf{\\Sigma}_{12} = 0\\) If \\(\\mathbf{y} \\sim N(\\mathbf{\\mu, \\Sigma})\\) and \\(\\mathbf{\\Sigma}\\) is positive definite, then \\(\\mathbf{(y-\\mu)&#39; \\Sigma^{-1} (y - \\mu)} \\sim \\chi^2_{(p)}\\) If \\(\\mathbf{y}_i\\) are independent \\(N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i)\\) random variables, then for fixed matrices \\(\\mathbf{A}_{i(m \\times p)}\\), \\(\\sum_{i=1}^k \\mathbf{A}_i \\mathbf{y}_i \\sim N_m (\\sum_{i=1}^{k} \\mathbf{A}_i \\mathbf{\\mu}_i, \\sum_{i=1}^k \\mathbf{A}_i \\mathbf{\\Sigma}_i \\mathbf{A}_i)\\) Multiple Regression \\[ \\left( \\begin{array} {c} Y \\\\ \\mathbf{x} \\end{array} \\right) \\sim N_{p+1} \\left( \\left[ \\begin{array} {c} \\mu_y \\\\ \\mathbf{\\mu}_x \\end{array} \\right] , \\left[ \\begin{array} {cc} \\sigma^2_Y &amp; \\mathbf{\\Sigma}_{yx} \\\\ \\mathbf{\\Sigma}_{yx} &amp; \\mathbf{\\Sigma}_{xx} \\end{array} \\right] \\right) \\] The conditional distribution of Y given x follows a univariate normal distribution with \\[ \\begin{aligned} E(Y| \\mathbf{x}) &amp;= \\mu_y + \\mathbf{\\Sigma}_{yx} \\Sigma_{xx}^{-1} (\\mathbf{x}- \\mu_x) \\\\ &amp;= \\mu_y - \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mu_x + \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mathbf{x} \\\\ &amp;= \\beta_0 + \\mathbf{\\beta&#39;x} \\end{aligned} \\] where \\(\\beta = (\\beta_1,...,\\beta_p)&#39; = \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{yx}&#39;\\) (e.g., analogous to \\(\\mathbf{(x&#39;x)^{-1}x&#39;y}\\) but not the same if we consider \\(Y_i\\) and \\(\\mathbf{x}_i\\), \\(i = 1,..,n\\) and use the empirical covariance formula: \\(var(Y|\\mathbf{x}) = \\sigma^2_Y - \\mathbf{\\Sigma_{yx}\\Sigma^{-1}_{xx} \\Sigma&#39;_{yx}}\\)) Samples from Multivariate Normal Populations A random sample of size n, \\(\\mathbf{y}_1,.., \\mathbf{y}_n\\) from \\(N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). Then Since \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) are iid, their sample mean, \\(\\bar{\\mathbf{y}} = \\sum_{i=1}^n \\mathbf{y}_i/n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}/n)\\). that is, \\(\\bar{\\mathbf{y}}\\) is an unbiased estimator of \\(\\mathbf{\\mu}\\) The \\(p \\times p\\) sample variance-covariance matrix, \\(\\mathbf{S}\\) is \\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{i=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})&#39; = \\frac{1}{n-1} (\\sum_{i=1}^n \\mathbf{y}_i \\mathbf{y}_i&#39; - n \\bar{\\mathbf{y}}\\bar{\\mathbf{y}}&#39;)\\) where \\(\\mathbf{S}\\) is symmetric, unbiased estimator of \\(\\mathbf{\\Sigma}\\) and has \\(p(p+1)/2\\) random variables. \\((n-1)\\mathbf{S} \\sim W_p (n-1, \\mathbf{\\Sigma})\\) is a Wishart distribution with n-1 degrees of freedom and expectation \\((n-1) \\mathbf{\\Sigma}\\). The Wishart distribution is a multivariate extension of the Chi-squared distribution. \\(\\bar{\\mathbf{y}}\\) and \\(\\mathbf{S}\\) are independent \\(\\bar{\\mathbf{y}}\\) and \\(\\mathbf{S}\\) are sufficient statistics. (All of the info in the data about \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\) is contained in \\(\\bar{\\mathbf{y}}\\) and \\(\\mathbf{S}\\) , regardless of sample size). Large Sample Properties \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) are a random sample from some population with mean \\(\\mathbf{\\mu}\\) and variance-covariance matrix \\(\\mathbf{\\Sigma}\\) \\(\\bar{\\mathbf{y}}\\) is a consistent estimator for \\(\\mu\\) \\(\\mathbf{S}\\) is a consistent estimator for \\(\\mathbf{\\Sigma}\\) Multivariate Central Limit Theorem: Similar to the univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0,\\Sigma})\\) where n is large relative to p (\\(n \\ge 25p\\)), which is equivalent to \\(\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mu, \\mathbf{\\Sigma}/n)\\) Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)&#39; \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mu)\\) when n is large relative to p. Maximum Likelihood Estimation for MVN Suppose iid \\(\\mathbf{y}_1 ,... \\mathbf{y}_n \\sim N_p (\\mu, \\mathbf{\\Sigma})\\), the likelihood function for the data is \\[ \\begin{aligned} L(\\mu, \\mathbf{\\Sigma}) &amp;= \\prod_{j=1}^n (\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2}(\\mathbf{y}_j -\\mu)&#39;\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu)) \\\\ &amp;= \\frac{1}{(2\\pi)^{np/2}|\\mathbf{\\Sigma}|^{n/2}} \\exp(-\\frac{1}{2} \\sum_{j=1}^n(\\mathbf{y}_j -\\mu)&#39;\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu) \\end{aligned} \\] Then, the MLEs are \\[ \\hat{\\mu} = \\bar{\\mathbf{y}} \\] \\[ \\hat{\\mathbf{\\Sigma}} = \\frac{n-1}{n} \\mathbf{S} \\] using derivatives of the log of the likelihood function with respect to \\(\\mu\\) and \\(\\mathbf{\\Sigma}\\) Properties of MLEs Invariance: If \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\), then the MLE of \\(h(\\theta)\\) is \\(h(\\hat{\\theta})\\) for any function h(.) Consistency: MLEs are consistent estimators, but they are usually biased Efficiency: MLEs are efficient estimators (no other estimator has a smaller variance for large samples) Asymptotic normality: Suppose that \\(\\hat{\\theta}_n\\) is the MLE for \\(\\theta\\) based upon n independent observations. Then \\(\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1})\\) \\(\\mathbf{H}\\) is the Fisher Information Matrix, which contains the expected values of the second partial derivatives fo the log-likelihood function. the (i,j)th element of \\(\\mathbf{H}\\) is \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\) we can estimate \\(\\mathbf{H}\\) by finding the form determined above, and evaluate it at \\(\\theta = \\hat{\\theta}_n\\) Likelihood ratio testing: for some null hypothesis, \\(H_0\\) we can form a likelihood ratio test The statistic is: \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\) For large n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) where v is the number of parameters in the unrestricted space minus the number of parameters under \\(H_0\\) Test of Multivariate Normality Check univariate normality for each trait (X) separately Can check \\[Normality Assessment\\] The good thing is that if any of the univariate trait is not normal, then the joint distribution is not normal (see again [m]). If a joint multivariate distribution is normal, then the marginal distribution has to be normal. However, marginal normality of all traits does not imply joint MVN Easily rule out multivariate normality, but not easy to prove it Mardia’s tests for multivariate normality Multivariate skewness is\\[ \\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})&#39; \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3 \\] where \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are independent, but have the same distribution (note: \\(\\beta\\) here is not regression coefficient) Multivariate kurtosis is defined as \\[ \\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})&#39; \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2 \\] For the MVN distribution, we have \\(\\beta_{1,p} = 0\\) and \\(\\beta_{2,p} = p(p+2)\\) For a sample of size n, we can estimate \\[ \\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n g^2_{ij} \\] \\[ \\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{i=1}^n g^2_{ii} \\] where \\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})&#39; \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) where \\(d^2_i\\) is the Mahalanobis distance (Mardia 1970) shows for large n \\[ \\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6} \\] \\[ \\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1) \\] Hence, we can use \\(\\kappa_1\\) and \\(\\kappa_2\\) to test the null hypothesis of MVN. When the data are non-normal, normal theory tests on the mean are sensitive to \\(\\beta_{1,p}\\) , while tests on the covariance are sensitive to \\(\\beta_{2,p}\\) Alternatively, Doornik-Hansen test for multivariate normality (Doornik and Hansen 2008) Chi-square Q-Q plot Let \\(\\mathbf{y}_i, i = 1,...,n\\) be a random sample sample from \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\) Then \\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), i = 1,...,n\\) are iid \\(N_p (\\mathbf{0}, \\mathbf{I})\\). Thus, \\(d_i^2 = \\mathbf{z}_i&#39; \\mathbf{z}_i \\sim \\chi^2_p , i = 1,...,n\\) plot the ordered \\(d_i^2\\) values against the qualities of the \\(\\chi^2_p\\) distribution. When normality holds, the plot should approximately resemble a straight lien passing through the origin at a 45 degree it requires large sample size (i.e., sensitive to sample size). Even if we generate data from a MVN, the tail of the Chi-square Q-Q plot can still be out of line. If the data are not normal, we can ignore it use nonparametric methods use models based upon an approximate distribution (e.g., GLMM) try performing a transformation library(heplots) library(ICSNP) library(MVN) library(tidyverse) trees = read.table(&quot;images/trees.dat&quot;) names(trees) &lt;- c(&quot;Nitrogen&quot;,&quot;Phosphorous&quot;,&quot;Potassium&quot;,&quot;Ash&quot;,&quot;Height&quot;) str(trees) #&gt; &#39;data.frame&#39;: 26 obs. of 5 variables: #&gt; $ Nitrogen : num 2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ... #&gt; $ Phosphorous: num 0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ... #&gt; $ Potassium : num 1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ... #&gt; $ Ash : num 1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ... #&gt; $ Height : int 351 249 171 373 321 191 225 291 284 213 ... summary(trees) #&gt; Nitrogen Phosphorous Potassium Ash #&gt; Min. :1.130 Min. :0.1570 Min. :0.3800 Min. :0.4500 #&gt; 1st Qu.:1.532 1st Qu.:0.1963 1st Qu.:0.6050 1st Qu.:0.6375 #&gt; Median :1.855 Median :0.2250 Median :0.7150 Median :0.9300 #&gt; Mean :1.896 Mean :0.2506 Mean :0.7619 Mean :0.8873 #&gt; 3rd Qu.:2.160 3rd Qu.:0.2975 3rd Qu.:0.8975 3rd Qu.:0.9825 #&gt; Max. :2.880 Max. :0.4170 Max. :1.3500 Max. :1.7900 #&gt; Height #&gt; Min. : 65.0 #&gt; 1st Qu.:122.5 #&gt; Median :181.0 #&gt; Mean :196.6 #&gt; 3rd Qu.:276.0 #&gt; Max. :373.0 cor(trees, method = &quot;pearson&quot;) # correlation matrix #&gt; Nitrogen Phosphorous Potassium Ash Height #&gt; Nitrogen 1.0000000 0.6023902 0.5462456 0.6509771 0.8181641 #&gt; Phosphorous 0.6023902 1.0000000 0.7037469 0.6707871 0.7739656 #&gt; Potassium 0.5462456 0.7037469 1.0000000 0.6710548 0.7915683 #&gt; Ash 0.6509771 0.6707871 0.6710548 1.0000000 0.7676771 #&gt; Height 0.8181641 0.7739656 0.7915683 0.7676771 1.0000000 # qq-plot gg &lt;- trees %&gt;% pivot_longer(everything(), names_to = &quot;Var&quot;, values_to = &quot;Value&quot;) %&gt;% ggplot(aes(sample = Value)) + geom_qq() + geom_qq_line() + facet_wrap(&quot;Var&quot;, scales = &quot;free&quot;) gg # Univariate normality sw_tests &lt;- apply(trees, MARGIN = 2, FUN = shapiro.test) sw_tests #&gt; $Nitrogen #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: newX[, i] #&gt; W = 0.96829, p-value = 0.5794 #&gt; #&gt; #&gt; $Phosphorous #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: newX[, i] #&gt; W = 0.93644, p-value = 0.1104 #&gt; #&gt; #&gt; $Potassium #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: newX[, i] #&gt; W = 0.95709, p-value = 0.3375 #&gt; #&gt; #&gt; $Ash #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: newX[, i] #&gt; W = 0.92071, p-value = 0.04671 #&gt; #&gt; #&gt; $Height #&gt; #&gt; Shapiro-Wilk normality test #&gt; #&gt; data: newX[, i] #&gt; W = 0.94107, p-value = 0.1424 # Kolmogorov-Smirnov test ks_tests &lt;- map(trees, ~ ks.test(scale(.x),&quot;pnorm&quot;)) ks_tests #&gt; $Nitrogen #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: scale(.x) #&gt; D = 0.12182, p-value = 0.8351 #&gt; alternative hypothesis: two-sided #&gt; #&gt; #&gt; $Phosphorous #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: scale(.x) #&gt; D = 0.17627, p-value = 0.3944 #&gt; alternative hypothesis: two-sided #&gt; #&gt; #&gt; $Potassium #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: scale(.x) #&gt; D = 0.10542, p-value = 0.9348 #&gt; alternative hypothesis: two-sided #&gt; #&gt; #&gt; $Ash #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: scale(.x) #&gt; D = 0.14503, p-value = 0.6449 #&gt; alternative hypothesis: two-sided #&gt; #&gt; #&gt; $Height #&gt; #&gt; Asymptotic one-sample Kolmogorov-Smirnov test #&gt; #&gt; data: scale(.x) #&gt; D = 0.1107, p-value = 0.9076 #&gt; alternative hypothesis: two-sided # Mardia&#39;s test, need large sample size for power mardia_test &lt;- mvn( trees, mvnTest = &quot;mardia&quot;, covariance = FALSE, multivariatePlot = &quot;qq&quot; ) mardia_test$multivariateNormality #&gt; Test Statistic p value Result #&gt; 1 Mardia Skewness 29.7248528871795 0.72054426745778 YES #&gt; 2 Mardia Kurtosis -1.67743173185383 0.0934580886477281 YES #&gt; 3 MVN &lt;NA&gt; &lt;NA&gt; YES # Doornik-Hansen&#39;s test dh_test &lt;- mvn( trees, mvnTest = &quot;dh&quot;, covariance = FALSE, multivariatePlot = &quot;qq&quot; ) dh_test$multivariateNormality #&gt; Test E df p value MVN #&gt; 1 Doornik-Hansen 161.9446 10 1.285352e-29 NO # Henze-Zirkler&#39;s test hz_test &lt;- mvn( trees, mvnTest = &quot;hz&quot;, covariance = FALSE, multivariatePlot = &quot;qq&quot; ) hz_test$multivariateNormality #&gt; Test HZ p value MVN #&gt; 1 Henze-Zirkler 0.7591525 0.6398905 YES # The last column indicates whether dataset follows a multivariate normality or not (i.e, YES or NO) at significance level 0.05. # Royston&#39;s test # can only apply for 3 &lt; obs &lt; 5000 (because of Shapiro-Wilk&#39;s test) royston_test &lt;- mvn( trees, mvnTest = &quot;royston&quot;, covariance = FALSE, multivariatePlot = &quot;qq&quot; ) royston_test$multivariateNormality #&gt; Test H p value MVN #&gt; 1 Royston 9.064631 0.08199215 YES # E-statistic estat_test &lt;- mvn( trees, mvnTest = &quot;energy&quot;, covariance = FALSE, multivariatePlot = &quot;qq&quot; ) estat_test$multivariateNormality #&gt; Test Statistic p value MVN #&gt; 1 E-statistic 1.091101 0.532 YES 22.0.2 Mean Vector Inference In the univariate normal distribution, we test \\(H_0: \\mu =\\mu_0\\) by using \\[ T = \\frac{\\bar{y}- \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1} \\] under the null hypothesis. And reject the null if \\(|T|\\) is large relative to \\(t_{(1-\\alpha/2,n-1)}\\) because it means that seeing a value as large as what we observed is rare if the null is true Equivalently, \\[ T^2 = \\frac{(\\bar{y}- \\mu_0)^2}{s^2/n} = n(\\bar{y}- \\mu_0)(s^2)^{-1}(\\bar{y}- \\mu_0) \\sim f_{(1,n-1)} \\] 22.0.2.1 Natural Multivariate Generalization \\[ \\begin{aligned} &amp;H_0: \\mathbf{\\mu} = \\mathbf{\\mu}_0 \\\\ &amp;H_a: \\mathbf{\\mu} \\neq \\mathbf{\\mu}_0 \\end{aligned} \\] Define Hotelling’s \\(T^2\\) by \\[ T^2 = n(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)&#39;\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0) \\] which can be viewed as a generalized distance between \\(\\bar{\\mathbf{y}}\\) and \\(\\mathbf{\\mu}_0\\) Under the assumption of normality, \\[ F = \\frac{n-p}{(n-1)p} T^2 \\sim f_{(p,n-p)} \\] and reject the null hypothesis when \\(F &gt; f_{(1-\\alpha, p, n-p)}\\) The \\(T^2\\) test is invariant to changes in measurement units. If \\(\\mathbf{z = Cy + d}\\) where \\(\\mathbf{C}\\) and \\(\\mathbf{d}\\) do not depend on \\(\\mathbf{y}\\), then \\(T^2(\\mathbf{z}) - T^2(\\mathbf{y})\\) The \\(T^2\\) test can be derived as a likelihood ratio test of \\(H_0: \\mu = \\mu_0\\) 22.0.2.2 Confidence Intervals 22.0.2.2.1 Confidence Region An “exact” \\(100(1-\\alpha)\\%\\) confidence region for \\(\\mathbf{\\mu}\\) is the set of all vectors, \\(\\mathbf{v}\\), which are “close enough” to the observed mean vector, \\(\\bar{\\mathbf{y}}\\) to satisfy \\[ n(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)&#39;\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0) \\le \\frac{(n-1)p}{n-p} f_{(1-\\alpha, p, n-p)} \\] \\(\\mathbf{v}\\) are just the mean vectors that are not rejected by the \\(T^2\\) test when \\(\\mathbf{\\bar{y}}\\) is observed. In case that you have 2 parameters, the confidence region is a “hyper-ellipsoid”. In this region, it consists of all \\(\\mathbf{\\mu}_0\\) vectors for which the \\(T^2\\) test would not reject \\(H_0\\) at significance level \\(\\alpha\\) Even though the confidence region better assesses the joint knowledge concerning plausible values of \\(\\mathbf{\\mu}\\) , people typically include confidence statement about the individual component means. We’d like all of the separate confidence statements to hold simultaneously with a specified high probability. Simultaneous confidence intervals: intervals against any statement being incorrect 22.0.2.2.1.1 Simultaneous Confidence Statements Intervals based on a rectangular confidence region by projecting the previous region onto the coordinate axes: \\[ \\bar{y}_{i} \\pm \\sqrt{\\frac{(n-1)p}{n-p}f_{(1-\\alpha, p,n-p)}\\frac{s_{ii}}{n}} \\] for all \\(i = 1,..,p\\) which implied confidence region is conservative; it has at least \\(100(1- \\alpha)\\%\\) Generally, simultaneous \\(100(1-\\alpha) \\%\\) confidence intervals for all linear combinations , \\(\\mathbf{a}\\) of the elements of the mean vector are given by \\[ \\mathbf{a&#39;\\bar{y}} \\pm \\sqrt{\\frac{(n-1)p}{n-p}f_{(1-\\alpha, p,n-p)}\\frac{\\mathbf{a&#39;Sa}}{n}} \\] works for any arbitrary linear combination \\(\\mathbf{a&#39;\\mu} = a_1 \\mu_1 + ... + a_p \\mu_p\\), which is a projection onto the axis in the direction of \\(\\mathbf{a}\\) These intervals have the property that the probability that at least one such interval does not contain the appropriate \\(\\mathbf{a&#39; \\mu}\\) is no more than \\(\\alpha\\) These types of intervals can be used for “data snooping” (like \\[Scheffe\\]) 22.0.2.2.1.2 One \\(\\mu\\) at a time One at a time confidence intervals: \\[ \\bar{y}_i \\pm t_{(1 - \\alpha/2, n-1} \\sqrt{\\frac{s_{ii}}{n}} \\] Each of these intervals has a probability of \\(1-\\alpha\\) of covering the appropriate \\(\\mu_i\\) But they ignore the covariance structure of the \\(p\\) variables If we only care about \\(k\\) simultaneous intervals, we can use “one at a time” method with the \\[Bonferroni\\] correction. This method gets more conservative as the number of intervals \\(k\\) increases. 22.0.3 General Hypothesis Testing 22.0.3.1 One-sample Tests \\[ H_0: \\mathbf{C \\mu= 0} \\] where \\(\\mathbf{C}\\) is a \\(c \\times p\\) matrix of rank c where \\(c \\le p\\) We can test this hypothesis using the following statistic \\[ F = \\frac{n - c}{(n-1)c} T^2 \\] where \\(T^2 = n(\\mathbf{C\\bar{y}})&#39; (\\mathbf{CSC&#39;})^{-1} (\\mathbf{C\\bar{y}})\\) Example: \\[ H_0: \\mu_1 = \\mu_2 = ... = \\mu_p \\] Equivalently, \\[ \\begin{aligned} \\mu_1 - \\mu_2 &amp;= 0 \\\\ &amp;\\vdots \\\\ \\mu_{p-1} - \\mu_p &amp;= 0 \\end{aligned} \\] a total of \\(p-1\\) tests. Hence, we have \\(\\mathbf{C}\\) as the \\(p - 1 \\times p\\) matrix \\[ \\mathbf{C} = \\left( \\begin{array} {ccccc} 1 &amp; -1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; -1 \\end{array} \\right) \\] number of rows = \\(c = p -1\\) Equivalently, we can also compare all of the other means to the first mean. Then, we test \\(\\mu_1 - \\mu_2 = 0, \\mu_1 - \\mu_3 = 0,..., \\mu_1 - \\mu_p = 0\\), the \\((p-1) \\times p\\) matrix \\(\\mathbf{C}\\) is \\[ \\mathbf{C} = \\left( \\begin{array} {ccccc} -1 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ -1 &amp; 0 &amp; 1 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ -1 &amp; 0 &amp; \\ldots &amp; 0 &amp; 1 \\end{array} \\right) \\] The value of \\(T^2\\) is invariant to these equivalent choices of \\(\\mathbf{C}\\) This is often used for repeated measures designs, where each subject receives each treatment once over successive periods of time (all treatments are administered to each unit). Example: Let \\(y_{ij}\\) be the response from subject i at time j for \\(i = 1,..,n, j = 1,...,T\\). In this case, \\(\\mathbf{y}_i = (y_{i1}, ..., y_{iT})&#39;, i = 1,...,n\\) are a random sample from \\(N_T (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\) Let \\(n=8\\) subjects, \\(T = 6\\). We are interested in \\(\\mu_1, .., \\mu_6\\) \\[ H_0: \\mu_1 = \\mu_2 = ... = \\mu_6 \\] Equivalently, \\[ \\begin{aligned} \\mu_1 - \\mu_2 &amp;= 0 \\\\ \\mu_2 - \\mu_3 &amp;= 0 \\\\ &amp;... \\\\ \\mu_5 - \\mu_6 &amp;= 0 \\end{aligned} \\] We can test orthogonal polynomials for 4 equally spaced time points. To test for example the null hypothesis that quadratic and cubic effects are jointly equal to 0, we would define \\(\\mathbf{C}\\) \\[ \\mathbf{C} = \\left( \\begin{array} {cccc} 1 &amp; -1 &amp; -1 &amp; 1 \\\\ -1 &amp; 3 &amp; -3 &amp; 1 \\end{array} \\right) \\] 22.0.3.2 Two-Sample Tests Consider the analogous two sample multivariate tests. Example: we have data on two independent random samples, one sample from each of two populations \\[ \\begin{aligned} \\mathbf{y}_{1i} &amp;\\sim N_p (\\mathbf{\\mu_1, \\Sigma}) \\\\ \\mathbf{y}_{2j} &amp;\\sim N_p (\\mathbf{\\mu_2, \\Sigma}) \\end{aligned} \\] We assume normality equal variance-covariance matrices independent random samples We can summarize our data using the sufficient statistics \\(\\mathbf{\\bar{y}}_1, \\mathbf{S}_1, \\mathbf{\\bar{y}}_2, \\mathbf{S}_2\\) with respective sample sizes, \\(n_1,n_2\\) Since we assume that \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}\\), compute a pooled estimate of the variance-covariance matrix on \\(n_1 + n_2 - 2\\) df \\[ \\mathbf{S} = \\frac{(n_1 - 1)\\mathbf{S}_1 + (n_2-1) \\mathbf{S}_2}{(n_1 -1) + (n_2 - 1)} \\] \\[ \\begin{aligned} &amp;H_0: \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2 \\\\ &amp;H_a: \\mathbf{\\mu}_1 \\neq \\mathbf{\\mu}_2 \\end{aligned} \\] At least one element of the mean vectors is different We use \\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) to estimate \\(\\mu_1 - \\mu_2\\) \\(\\mathbf{S}\\) to estimate \\(\\mathbf{\\Sigma}\\) Note: because we assume the two populations are independent, there is no covariance \\(cov(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = var(\\mathbf{\\bar{y}}_1) + var(\\mathbf{\\bar{y}}_2) = \\frac{\\mathbf{\\Sigma_1}}{n_1} + \\frac{\\mathbf{\\Sigma_2}}{n_2} = \\mathbf{\\Sigma}(\\frac{1}{n_1} + \\frac{1}{n_2})\\) Reject \\(H_0\\) if \\[ \\begin{aligned} T^2 &amp;= (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)&#39;\\{ \\mathbf{S} (\\frac{1}{n_1} + \\frac{1}{n_2})\\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\ &amp;= \\frac{n_1 n_2}{n_1 +n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)&#39;\\{ \\mathbf{S} \\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\ &amp; \\ge \\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \\alpha,n_1 + n_2 - p -1)} \\end{aligned} \\] or equivalently, if \\[ F = \\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \\ge f_{(1- \\alpha, p , n_1 + n_2 -p -1)} \\] A \\(100(1-\\alpha) \\%\\) confidence region for \\(\\mu_1 - \\mu_2\\) consists of all vector \\(\\delta\\) which satisfy \\[ \\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta})&#39; \\mathbf{S}^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta}) \\le \\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\\alpha, p , n_1 + n_2 - p -1)} \\] The simultaneous confidence intervals for all linear combinations of \\(\\mu_1 - \\mu_2\\) have the form \\[ \\mathbf{a&#39;}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\pm \\sqrt{\\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\\alpha, p, n_1 + n_2 -p -1)} \\times \\sqrt{\\mathbf{a&#39;Sa}(\\frac{1}{n_1} + \\frac{1}{n_2})} \\] Bonferroni intervals, for k combinations \\[ (\\bar{y}_{1i} - \\bar{y}_{2i}) \\pm t_{(1-\\alpha/2k, n_1 + n_2 - 2)}\\sqrt{(\\frac{1}{n_1} + \\frac{1}{n_2})s_{ii}} \\] 22.0.3.3 Model Assumptions If model assumption are not met Unequal Covariance Matrices If \\(n_1 = n_2\\) (large samples) there is little effect on the Type I error rate and power fo the two sample test If \\(n_1 &gt; n_2\\) and the eigenvalues of \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) are less than 1, the Type I error level is inflated If \\(n_1 &gt; n_2\\) and some eigenvalues of \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) are greater than 1, the Type I error rate is too small, leading to a reduction in power Sample Not Normal Type I error level of the two sample \\(T^2\\) test isn’t much affect by moderate departures from normality if the two populations being sampled have similar distributions One sample \\(T^2\\) test is much more sensitive to lack of normality, especially when the distribution is skewed. Intuitively, you can think that in one sample your distribution will be sensitive, but the distribution of the difference between two similar distributions will not be as sensitive. Solutions: Transform to make the data more normal Large large samples, use the \\(\\chi^2\\) (Wald) test, in which populations don’t need to be normal, or equal sample sizes, or equal variance-covariance matrices \\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)&#39;( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\) 22.0.3.3.1 Equal Covariance Matrices Tests With independent random samples from k populations of \\(p\\)-dimensional vectors. We compute the sample covariance matrix for each, \\(\\mathbf{S}_i\\), where \\(i = 1,...,k\\) \\[ \\begin{aligned} &amp;H_0: \\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\ldots = \\mathbf{\\Sigma}_k = \\mathbf{\\Sigma} \\\\ &amp;H_a: \\text{at least 2 are different} \\end{aligned} \\] Assume \\(H_0\\) is true, we would use a pooled estimate of the common covariance matrix, \\(\\mathbf{\\Sigma}\\) \\[ \\mathbf{S} = \\frac{\\sum_{i=1}^k (n_i -1)\\mathbf{S}_i}{\\sum_{i=1}^k (n_i - 1)} \\] with \\(\\sum_{i=1}^k (n_i -1)\\) 22.0.3.3.1.1 Bartlett’s Test (a modification of the likelihood ratio test). Define \\[ N = \\sum_{i=1}^k n_i \\] and (note: \\(| |\\) are determinants here, not absolute value) \\[ M = (N - k) \\log|\\mathbf{S}| - \\sum_{i=1}^k (n_i - 1) \\log|\\mathbf{S}_i| \\] \\[ C^{-1} = 1 - \\frac{2p^2 + 3p - 1}{6(p+1)(k-1)} \\{\\sum_{i=1}^k (\\frac{1}{n_i - 1}) - \\frac{1}{N-k} \\} \\] Reject \\(H_0\\) when \\(MC^{-1} &gt; \\chi^2_{1- \\alpha, (k-1)p(p+1)/2}\\) If not all samples are from normal populations, \\(MC^{-1}\\) has a distribution which is often shifted to the right of the nominal \\(\\chi^2\\) distribution, which means \\(H_0\\) is often rejected even when it is true (the Type I error level is inflated). Hence, it is better to test individual normality first, or then multivariate normality before you do Bartlett’s test. 22.0.3.4 Two-Sample Repeated Measurements Define \\(\\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})&#39;\\) to be the observations from the i-th subject in the h-th group for times 1 through T Assume that \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1}\\) are iid \\(N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) and that \\(\\mathbf{y}_{21},...,\\mathbf{y}_{2n_2}\\) are iid \\(N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\) \\(H_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c\\) where \\(\\mathbf{C}\\) is a \\(c \\times t\\) matrix of rank \\(c\\) where \\(c \\le t\\) The test statistic has the form \\[ T^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)&#39; \\mathbf{C}&#39;(\\mathbf{CSC}&#39;)^{-1}\\mathbf{C} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\] where \\(\\mathbf{S}\\) is the pooled covariance estimate. Then, \\[ F = \\frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \\sim f_{(c, n_1 + n_2 - c-1)} \\] when \\(H_0\\) is true If the null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) is rejected. A weaker hypothesis is that the profiles for the two groups are parallel. \\[ \\begin{aligned} \\mu_{11} - \\mu_{21} &amp;= \\mu_{12} - \\mu_{22} \\\\ &amp;\\vdots \\\\ \\mu_{1t-1} - \\mu_{2t-1} &amp;= \\mu_{1t} - \\mu_{2t} \\end{aligned} \\] The null hypothesis matrix term is then \\(H_0: \\mathbf{C}(\\mu_1 - \\mu_2) = \\mathbf{0}_c\\) , where \\(c = t - 1\\) and \\[ \\mathbf{C} = \\left( \\begin{array} {ccccc} 1 &amp; -1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; -1 \\end{array} \\right)_{(t-1) \\times t} \\] # One-sample Hotelling&#39;s T^2 test # Create data frame plants &lt;- data.frame( y1 = c(2.11, 2.36, 2.13, 2.78, 2.17), y2 = c(10.1, 35.0, 2.0, 6.0, 2.0), y3 = c(3.4, 4.1, 1.9, 3.8, 1.7) ) # Center the data with # the hypothesized means and make a matrix plants_ctr &lt;- plants %&gt;% transmute(y1_ctr = y1 - 2.85, y2_ctr = y2 - 15.0, y3_ctr = y3 - 6.0) %&gt;% as.matrix() # Use anova.mlm to calculate Wilks&#39; lambda onesamp_fit &lt;- anova(lm(plants_ctr ~ 1), test = &quot;Wilks&quot;) onesamp_fit #&gt; Analysis of Variance Table #&gt; #&gt; Df Wilks approx F num Df den Df Pr(&gt;F) #&gt; (Intercept) 1 0.054219 11.629 3 2 0.08022 . #&gt; Residuals 4 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 can’t reject the null of hypothesized vector of means # Paired-Sample Hotelling&#39;s T^2 test library(ICSNP) # Create data frame waste &lt;- data.frame( case = 1:11, com_y1 = c(6, 6, 18, 8, 11, 34, 28, 71, 43, 33, 20), com_y2 = c(27, 23, 64, 44, 30, 75, 26, 124, 54, 30, 14), state_y1 = c(25, 28, 36, 35, 15, 44, 42, 54, 34, 29, 39), state_y2 = c(15, 13, 22, 29, 31, 64, 30, 64, 56, 20, 21) ) # Calculate the difference between commercial and state labs waste_diff &lt;- waste %&gt;% transmute(y1_diff = com_y1 - state_y1, y2_diff = com_y2 - state_y2) # Run the test paired_fit &lt;- HotellingsT2(waste_diff) # value T.2 in the output corresponds to # the approximate F-value in the output from anova.mlm paired_fit #&gt; #&gt; Hotelling&#39;s one sample T2-test #&gt; #&gt; data: waste_diff #&gt; T.2 = 6.1377, df1 = 2, df2 = 9, p-value = 0.02083 #&gt; alternative hypothesis: true location is not equal to c(0,0) reject the null that the two labs’ measurements are equal # Independent-Sample Hotelling&#39;s T^2 test with Bartlett&#39;s test # Read in data steel &lt;- read.table(&quot;images/steel.dat&quot;) names(steel) &lt;- c(&quot;Temp&quot;, &quot;Yield&quot;, &quot;Strength&quot;) str(steel) #&gt; &#39;data.frame&#39;: 12 obs. of 3 variables: #&gt; $ Temp : int 1 1 1 1 1 2 2 2 2 2 ... #&gt; $ Yield : int 33 36 35 38 40 35 36 38 39 41 ... #&gt; $ Strength: int 60 61 64 63 65 57 59 59 61 63 ... # Plot the data ggplot(steel, aes(x = Yield, y = Strength)) + geom_text(aes(label = Temp), size = 5) + geom_segment(aes( x = 33, y = 57.5, xend = 42, yend = 65 ), col = &quot;red&quot;) # Bartlett&#39;s test for equality of covariance matrices # same thing as Box&#39;s M test in the multivariate setting bart_test &lt;- boxM(steel[, -1], steel$Temp) bart_test # fail to reject the null of equal covariances #&gt; #&gt; Box&#39;s M-test for Homogeneity of Covariance Matrices #&gt; #&gt; data: steel[, -1] #&gt; Chi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442 # anova.mlm twosamp_fit &lt;- anova(lm(cbind(Yield, Strength) ~ factor(Temp), data = steel), test = &quot;Wilks&quot;) twosamp_fit #&gt; Analysis of Variance Table #&gt; #&gt; Df Wilks approx F num Df den Df Pr(&gt;F) #&gt; (Intercept) 1 0.001177 3818.1 2 9 6.589e-14 *** #&gt; factor(Temp) 1 0.294883 10.8 2 9 0.004106 ** #&gt; Residuals 10 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # ICSNP package twosamp_fit2 &lt;- HotellingsT2(cbind(steel$Yield, steel$Strength) ~ factor(steel$Temp)) twosamp_fit2 #&gt; #&gt; Hotelling&#39;s two sample T2-test #&gt; #&gt; data: cbind(steel$Yield, steel$Strength) by factor(steel$Temp) #&gt; T.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106 #&gt; alternative hypothesis: true location difference is not equal to c(0,0) reject null. Hence, there is a difference in the means of the bivariate normal distributions References "],["manova.html", "22.1 MANOVA", " 22.1 MANOVA Multivariate Analysis of Variance One-way MANOVA Compare treatment means for h different populations Population 1: \\(\\mathbf{y}_{11}, \\mathbf{y}_{12}, \\dots, \\mathbf{y}_{1n_1} \\sim idd N_p (\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) \\(\\vdots\\) Population h: \\(\\mathbf{y}_{h1}, \\mathbf{y}_{h2}, \\dots, \\mathbf{y}_{hn_h} \\sim idd N_p (\\mathbf{\\mu}_h, \\mathbf{\\Sigma})\\) Assumptions Independent random samples from \\(h\\) different populations Common covariance matrices Each population is multivariate normal Calculate the summary statistics \\(\\mathbf{\\bar{y}}_i, \\mathbf{S}\\) and the pooled estimate of the covariance matrix \\(\\mathbf{S}\\) Similar to the univariate one-way ANVOA, we can use the effects model formulation \\(\\mathbf{\\mu}_i = \\mathbf{\\mu} + \\mathbf{\\tau}_i\\), where \\(\\mathbf{\\mu}_i\\) is the population mean for population i \\(\\mathbf{\\mu}\\) is the overall mean effect \\(\\mathbf{\\tau}_i\\) is the treatment effect of the i-th treatment. For the one-way model: \\(\\mathbf{y}_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) for \\(i = 1,..,h; j = 1,..., n_i\\) and \\(\\epsilon_{ij} \\sim N_p(\\mathbf{0, \\Sigma})\\) However, the above model is over-parameterized (i.e., infinite number of ways to define \\(\\mathbf{\\mu}\\) and the \\(\\mathbf{\\tau}_i\\)’s such that they add up to \\(\\mu_i\\). Thus we can constrain by having \\[ \\sum_{i=1}^h n_i \\tau_i = 0 \\] or \\[ \\mathbf{\\tau}_h = 0 \\] The observational equivalent of the effects model is \\[ \\begin{aligned} \\mathbf{y}_{ij} &amp;= \\mathbf{\\bar{y}} + (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}}) + (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i) \\\\ &amp;= \\text{overall sample mean} + \\text{treatement effect} + \\text{residual} \\text{ (under univariate ANOVA)} \\end{aligned} \\] After manipulation \\[ \\sum_{i = 1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})&#39; = \\sum_{i = 1}^h n_i (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})&#39; + \\sum_{i=1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}}_i)&#39; \\] LHS = Total corrected sums of squares and cross products (SSCP) matrix RHS = 1st term = Treatment (or between subjects) sum of squares and cross product matrix (denoted H;B) 2nd term = residual (or within subject) SSCP matrix denoted (E;W) Note: \\[ \\mathbf{E} = (n_1 - 1)\\mathbf{S}_1 + ... + (n_h -1) \\mathbf{S}_h = (n-h) \\mathbf{S} \\] MANOVA table MONOVA table Source SSCP df Treatment \\(\\mathbf{H}\\) \\(h -1\\) Residual (error) \\(\\mathbf{E}\\) \\(\\sum_{i= 1}^h n_i - h\\) Total Corrected \\(\\mathbf{H + E}\\) \\(\\sum_{i=1}^h n_i -1\\) \\[ H_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h = \\mathbf{0} \\] We consider the relative “sizes” of \\(\\mathbf{E}\\) and \\(\\mathbf{H+E}\\) Wilk’s Lambda Define Wilk’s Lambda \\[ \\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H+E}|} \\] Properties: Wilk’s Lambda is equivalent to the F-statistic in the univariate case The exact distribution of \\(\\Lambda^*\\) can be determined for especial cases. For large sample sizes, reject \\(H_0\\) if \\[ -(\\sum_{i=1}^h n_i - 1 - \\frac{p+h}{2}) \\log(\\Lambda^*) &gt; \\chi^2_{(1-\\alpha, p(h-1))} \\] 22.1.1 Testing General Hypotheses \\(h\\) different treatments with the i-th treatment applied to \\(n_i\\) subjects that are observed for \\(p\\) repeated measures. Consider this a \\(p\\) dimensional obs on a random sample from each of \\(h\\) different treatment populations. \\[ \\mathbf{y}_{ij} = \\mathbf{\\mu} + \\mathbf{\\tau}_i + \\mathbf{\\epsilon}_{ij} \\] for \\(i = 1,..,h\\) and \\(j = 1,..,n_i\\) Equivalently, \\[ \\mathbf{Y} = \\mathbf{XB} + \\mathbf{\\epsilon} \\] where \\(n = \\sum_{i = 1}^h n_i\\) and with restriction \\(\\mathbf{\\tau}_h = 0\\) \\[ \\mathbf{Y}_{(n \\times p)} = \\left[ \\begin{array} {c} \\mathbf{y}_{11}&#39; \\\\ \\vdots \\\\ \\mathbf{y}_{1n_1}&#39; \\\\ \\vdots \\\\ \\mathbf{y}_{hn_h}&#39; \\end{array} \\right], \\mathbf{B}_{(h \\times p)} = \\left[ \\begin{array} {c} \\mathbf{\\mu}&#39; \\\\ \\mathbf{\\tau}_1&#39; \\\\ \\vdots \\\\ \\mathbf{\\tau}_{h-1}&#39; \\end{array} \\right], \\mathbf{\\epsilon}_{(n \\times p)} = \\left[ \\begin{array} {c} \\epsilon_{11}&#39; \\\\ \\vdots \\\\ \\epsilon_{1n_1}&#39; \\\\ \\vdots \\\\ \\epsilon_{hn_h}&#39; \\end{array} \\right] \\] \\[ \\mathbf{X}_{(n \\times h)} = \\left[ \\begin{array} {ccccc} 1 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ 1 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\end{array} \\right] \\] Estimation \\[ \\mathbf{\\hat{B}} = (\\mathbf{X&#39;X})^{-1} \\mathbf{X&#39;Y} \\] Rows of \\(\\mathbf{Y}\\) are independent (i.e., \\(var(\\mathbf{Y}) = \\mathbf{I}_n \\otimes \\mathbf{\\Sigma}\\) , an \\(np \\times np\\) matrix, where \\(\\otimes\\) is the Kronecker product). \\[ \\begin{aligned} &amp;H_0: \\mathbf{LBM} = 0 \\\\ &amp;H_a: \\mathbf{LBM} \\neq 0 \\end{aligned} \\] where \\(\\mathbf{L}\\) is a \\(g \\times h\\) matrix of full row rank (\\(g \\le h\\)) = comparisons across groups \\(\\mathbf{M}\\) is a \\(p \\times u\\) matrix of full column rank (\\(u \\le p\\)) = comparisons across traits The general treatment corrected sums of squares and cross product is \\[ \\mathbf{H} = \\mathbf{M&#39;Y&#39;X(X&#39;X)^{-1}L&#39;[L(X&#39;X)^{-1}L&#39;]^{-1}L(X&#39;X)^{-1}X&#39;YM} \\] or for the null hypothesis \\(H_0: \\mathbf{LBM} = \\mathbf{D}\\) \\[ \\mathbf{H} = (\\mathbf{\\hat{LBM}} - \\mathbf{D})&#39;[\\mathbf{X(X&#39;X)^{-1}L}]^{-1}(\\mathbf{\\hat{LBM}} - \\mathbf{D}) \\] The general matrix of residual sums of squares and cross product \\[ \\mathbf{E} = \\mathbf{M&#39;Y&#39;[I-X(X&#39;X)^{-1}X&#39;]YM} = \\mathbf{M&#39;[Y&#39;Y - \\hat{B}&#39;(X&#39;X)^{-1}\\hat{B}]M} \\] We can compute the following statistic eigenvalues of \\(\\mathbf{HE}^{-1}\\) Wilk’s Criterion: \\(\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) . The df depend on the rank of \\(\\mathbf{L}, \\mathbf{M}, \\mathbf{X}\\) Lawley-Hotelling Trace: \\(U = tr(\\mathbf{HE}^{-1})\\) Pillai Trace: \\(V = tr(\\mathbf{H}(\\mathbf{H}+ \\mathbf{E}^{-1})\\) Roy’s Maximum Root: largest eigenvalue of \\(\\mathbf{HE}^{-1}\\) If \\(H_0\\) is true and n is large, \\(-(n-1- \\frac{p+h}{2})\\ln \\Lambda^* \\sim \\chi^2_{p(h-1)}\\). Some special values of p and h can give exact F-dist under \\(H_0\\) # One-way MANOVA library(car) library(emmeans) library(profileR) library(tidyverse) ## Read in the data gpagmat &lt;- read.table(&quot;images/gpagmat.dat&quot;) ## Change the variable names names(gpagmat) &lt;- c(&quot;y1&quot;, &quot;y2&quot;, &quot;admit&quot;) ## Check the structure str(gpagmat) #&gt; &#39;data.frame&#39;: 85 obs. of 3 variables: #&gt; $ y1 : num 2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ... #&gt; $ y2 : int 596 473 482 527 505 693 626 663 447 588 ... #&gt; $ admit: int 1 1 1 1 1 1 1 1 1 1 ... ## Plot the data gg &lt;- ggplot(gpagmat, aes(x = y1, y = y2)) + geom_text(aes(label = admit, col = as.character(admit))) + scale_color_discrete(name = &quot;Admission&quot;, labels = c(&quot;Admit&quot;, &quot;Do not admit&quot;, &quot;Borderline&quot;)) + scale_x_continuous(name = &quot;GPA&quot;) + scale_y_continuous(name = &quot;GMAT&quot;) ## Fit one-way MANOVA oneway_fit &lt;- manova(cbind(y1, y2) ~ admit, data = gpagmat) summary(oneway_fit, test = &quot;Wilks&quot;) #&gt; Df Wilks approx F num Df den Df Pr(&gt;F) #&gt; admit 1 0.6126 25.927 2 82 1.881e-09 *** #&gt; Residuals 83 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 reject the null of equal multivariate mean vectors between the three admmission groups # Repeated Measures MANOVA ## Create data frame stress &lt;- data.frame( subject = 1:8, begin = c(3, 2, 5, 6, 1, 5, 1, 5), middle = c(3, 4, 3, 7, 4, 7, 1, 2), final = c(6, 7, 4, 7, 6, 7, 3, 5) ) If independent = time with 3 levels -&gt; univariate ANOVA (require sphericity assumption (i.e., the variances for all differences are equal)) If each level of independent time as a separate variable -&gt; MANOVA (does not require sphericity assumption) ## MANOVA stress_mod &lt;- lm(cbind(begin, middle, final) ~ 1, data = stress) idata &lt;- data.frame(time = factor( c(&quot;begin&quot;, &quot;middle&quot;, &quot;final&quot;), levels = c(&quot;begin&quot;, &quot;middle&quot;, &quot;final&quot;) )) repeat_fit &lt;- Anova( stress_mod, idata = idata, idesign = ~ time, icontrasts = &quot;contr.poly&quot; ) summary(repeat_fit) #&gt; #&gt; Type III Repeated Measures MANOVA Tests: #&gt; #&gt; ------------------------------------------ #&gt; #&gt; Term: (Intercept) #&gt; #&gt; Response transformation matrix: #&gt; (Intercept) #&gt; begin 1 #&gt; middle 1 #&gt; final 1 #&gt; #&gt; Sum of squares and products for the hypothesis: #&gt; (Intercept) #&gt; (Intercept) 1352 #&gt; #&gt; Multivariate Tests: (Intercept) #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; Pillai 1 0.896552 60.66667 1 7 0.00010808 *** #&gt; Wilks 1 0.103448 60.66667 1 7 0.00010808 *** #&gt; Hotelling-Lawley 1 8.666667 60.66667 1 7 0.00010808 *** #&gt; Roy 1 8.666667 60.66667 1 7 0.00010808 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; ------------------------------------------ #&gt; #&gt; Term: time #&gt; #&gt; Response transformation matrix: #&gt; time.L time.Q #&gt; begin -7.071068e-01 0.4082483 #&gt; middle -7.850462e-17 -0.8164966 #&gt; final 7.071068e-01 0.4082483 #&gt; #&gt; Sum of squares and products for the hypothesis: #&gt; time.L time.Q #&gt; time.L 18.062500 6.747781 #&gt; time.Q 6.747781 2.520833 #&gt; #&gt; Multivariate Tests: time #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; Pillai 1 0.7080717 7.276498 2 6 0.024879 * #&gt; Wilks 1 0.2919283 7.276498 2 6 0.024879 * #&gt; Hotelling-Lawley 1 2.4254992 7.276498 2 6 0.024879 * #&gt; Roy 1 2.4254992 7.276498 2 6 0.024879 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Univariate Type III Repeated-Measures ANOVA Assuming Sphericity #&gt; #&gt; Sum Sq num Df Error SS den Df F value Pr(&gt;F) #&gt; (Intercept) 450.67 1 52.00 7 60.6667 0.0001081 *** #&gt; time 20.58 2 24.75 14 5.8215 0.0144578 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; #&gt; Mauchly Tests for Sphericity #&gt; #&gt; Test statistic p-value #&gt; time 0.7085 0.35565 #&gt; #&gt; #&gt; Greenhouse-Geisser and Huynh-Feldt Corrections #&gt; for Departure from Sphericity #&gt; #&gt; GG eps Pr(&gt;F[GG]) #&gt; time 0.77429 0.02439 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; HF eps Pr(&gt;F[HF]) #&gt; time 0.9528433 0.01611634 can’t reject the null hypothesis of sphericity, hence univariate ANOVA is also appropriate.We also see linear significant time effect, but no quadratic time effect ## Polynomial contrasts # What is the reference for the marginal means? ref_grid(stress_mod, mult.name = &quot;time&quot;) #&gt; &#39;emmGrid&#39; object with variables: #&gt; 1 = 1 #&gt; time = multivariate response levels: begin, middle, final # marginal means for the levels of time contr_means &lt;- emmeans(stress_mod, ~ time, mult.name = &quot;time&quot;) contrast(contr_means, method = &quot;poly&quot;) #&gt; contrast estimate SE df t.ratio p.value #&gt; linear 2.12 0.766 7 2.773 0.0276 #&gt; quadratic 1.38 0.944 7 1.457 0.1885 # MANOVA ## Read in Data heart &lt;- read.table(&quot;images/heart.dat&quot;) names(heart) &lt;- c(&quot;drug&quot;, &quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;) ## Create a subject ID nested within drug heart &lt;- heart %&gt;% group_by(drug) %&gt;% mutate(subject = row_number()) %&gt;% ungroup() str(heart) #&gt; tibble [24 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ drug : chr [1:24] &quot;ax23&quot; &quot;ax23&quot; &quot;ax23&quot; &quot;ax23&quot; ... #&gt; $ y1 : int [1:24] 72 78 71 72 66 74 62 69 85 82 ... #&gt; $ y2 : int [1:24] 86 83 82 83 79 83 73 75 86 86 ... #&gt; $ y3 : int [1:24] 81 88 81 83 77 84 78 76 83 80 ... #&gt; $ y4 : int [1:24] 77 82 75 69 66 77 70 70 80 84 ... #&gt; $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ... ## Create means summary for profile plot, # pivot longer for plotting with ggplot heart_means &lt;- heart %&gt;% group_by(drug) %&gt;% summarize_at(vars(starts_with(&quot;y&quot;)), mean) %&gt;% ungroup() %&gt;% pivot_longer(-drug, names_to = &quot;time&quot;, values_to = &quot;mean&quot;) %&gt;% mutate(time = as.numeric(as.factor(time))) gg_profile &lt;- ggplot(heart_means, aes(x = time, y = mean)) + geom_line(aes(col = drug)) + geom_point(aes(col = drug)) + ggtitle(&quot;Profile Plot&quot;) + scale_y_continuous(name = &quot;Response&quot;) + scale_x_discrete(name = &quot;Time&quot;) gg_profile ## Fit model heart_mod &lt;- lm(cbind(y1, y2, y3, y4) ~ drug, data = heart) man_fit &lt;- car::Anova(heart_mod) summary(man_fit) #&gt; #&gt; Type II MANOVA Tests: #&gt; #&gt; Sum of squares and products for error: #&gt; y1 y2 y3 y4 #&gt; y1 641.00 601.750 535.250 426.00 #&gt; y2 601.75 823.875 615.500 534.25 #&gt; y3 535.25 615.500 655.875 555.25 #&gt; y4 426.00 534.250 555.250 674.50 #&gt; #&gt; ------------------------------------------ #&gt; #&gt; Term: drug #&gt; #&gt; Sum of squares and products for the hypothesis: #&gt; y1 y2 y3 y4 #&gt; y1 567.00 335.2500 42.7500 387.0 #&gt; y2 335.25 569.0833 404.5417 367.5 #&gt; y3 42.75 404.5417 391.0833 171.0 #&gt; y4 387.00 367.5000 171.0000 316.0 #&gt; #&gt; Multivariate Tests: drug #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; Pillai 2 1.283456 8.508082 8 38 1.5010e-06 *** #&gt; Wilks 2 0.079007 11.509581 8 36 6.3081e-08 *** #&gt; Hotelling-Lawley 2 7.069384 15.022441 8 34 3.9048e-09 *** #&gt; Roy 2 6.346509 30.145916 4 19 5.4493e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 reject the null hypothesis of no difference in means between treatments ## Contrasts heart$drug &lt;- factor(heart$drug) L &lt;- matrix(c(0, 2, 1, -1,-1, -1), nrow = 3, byrow = T) colnames(L) &lt;- c(&quot;bww9:ctrl&quot;, &quot;ax23:rest&quot;) rownames(L) &lt;- unique(heart$drug) contrasts(heart$drug) &lt;- L contrasts(heart$drug) #&gt; bww9:ctrl ax23:rest #&gt; ax23 0 2 #&gt; bww9 1 -1 #&gt; ctrl -1 -1 # do not set contrast L if you do further analysis (e.g., Anova, lm) # do M matrix instead M &lt;- matrix(c(1, -1, 0, 0, 0, 1, -1, 0, 0, 0, 1, -1), nrow = 4) ## update model to test contrasts heart_mod2 &lt;- update(heart_mod) coef(heart_mod2) #&gt; y1 y2 y3 y4 #&gt; (Intercept) 75.00 78.9583333 77.041667 74.75 #&gt; drugbww9:ctrl 4.50 5.8125000 3.562500 4.25 #&gt; drugax23:rest -2.25 0.7708333 1.979167 -0.75 # Hypothesis test for bww9 vs control after transformation M # same as linearHypothesis(heart_mod, hypothesis.matrix = c(0,1,-1), P = M) bww9vctrl &lt;- car::linearHypothesis(heart_mod2, hypothesis.matrix = c(0, 1, 0), P = M) bww9vctrl #&gt; #&gt; Response transformation matrix: #&gt; [,1] [,2] [,3] #&gt; y1 1 0 0 #&gt; y2 -1 1 0 #&gt; y3 0 -1 1 #&gt; y4 0 0 -1 #&gt; #&gt; Sum of squares and products for the hypothesis: #&gt; [,1] [,2] [,3] #&gt; [1,] 27.5625 -47.25 14.4375 #&gt; [2,] -47.2500 81.00 -24.7500 #&gt; [3,] 14.4375 -24.75 7.5625 #&gt; #&gt; Sum of squares and products for error: #&gt; [,1] [,2] [,3] #&gt; [1,] 261.375 -141.875 28.000 #&gt; [2,] -141.875 248.750 -19.375 #&gt; [3,] 28.000 -19.375 219.875 #&gt; #&gt; Multivariate Tests: #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; Pillai 1 0.2564306 2.184141 3 19 0.1233 #&gt; Wilks 1 0.7435694 2.184141 3 19 0.1233 #&gt; Hotelling-Lawley 1 0.3448644 2.184141 3 19 0.1233 #&gt; Roy 1 0.3448644 2.184141 3 19 0.1233 bww9vctrl &lt;- car::linearHypothesis(heart_mod, hypothesis.matrix = c(0, 1, -1), P = M) bww9vctrl #&gt; #&gt; Response transformation matrix: #&gt; [,1] [,2] [,3] #&gt; y1 1 0 0 #&gt; y2 -1 1 0 #&gt; y3 0 -1 1 #&gt; y4 0 0 -1 #&gt; #&gt; Sum of squares and products for the hypothesis: #&gt; [,1] [,2] [,3] #&gt; [1,] 27.5625 -47.25 14.4375 #&gt; [2,] -47.2500 81.00 -24.7500 #&gt; [3,] 14.4375 -24.75 7.5625 #&gt; #&gt; Sum of squares and products for error: #&gt; [,1] [,2] [,3] #&gt; [1,] 261.375 -141.875 28.000 #&gt; [2,] -141.875 248.750 -19.375 #&gt; [3,] 28.000 -19.375 219.875 #&gt; #&gt; Multivariate Tests: #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; Pillai 1 0.2564306 2.184141 3 19 0.1233 #&gt; Wilks 1 0.7435694 2.184141 3 19 0.1233 #&gt; Hotelling-Lawley 1 0.3448644 2.184141 3 19 0.1233 #&gt; Roy 1 0.3448644 2.184141 3 19 0.1233 there is no significant difference in means between the control and bww9 drug # Hypothesis test for ax23 vs rest after transformation M axx23vrest &lt;- car::linearHypothesis(heart_mod2, hypothesis.matrix = c(0, 0, 1), P = M) axx23vrest #&gt; #&gt; Response transformation matrix: #&gt; [,1] [,2] [,3] #&gt; y1 1 0 0 #&gt; y2 -1 1 0 #&gt; y3 0 -1 1 #&gt; y4 0 0 -1 #&gt; #&gt; Sum of squares and products for the hypothesis: #&gt; [,1] [,2] [,3] #&gt; [1,] 438.0208 175.20833 -395.7292 #&gt; [2,] 175.2083 70.08333 -158.2917 #&gt; [3,] -395.7292 -158.29167 357.5208 #&gt; #&gt; Sum of squares and products for error: #&gt; [,1] [,2] [,3] #&gt; [1,] 261.375 -141.875 28.000 #&gt; [2,] -141.875 248.750 -19.375 #&gt; [3,] 28.000 -19.375 219.875 #&gt; #&gt; Multivariate Tests: #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; Pillai 1 0.855364 37.45483 3 19 3.5484e-08 *** #&gt; Wilks 1 0.144636 37.45483 3 19 3.5484e-08 *** #&gt; Hotelling-Lawley 1 5.913921 37.45483 3 19 3.5484e-08 *** #&gt; Roy 1 5.913921 37.45483 3 19 3.5484e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 axx23vrest &lt;- car::linearHypothesis(heart_mod, hypothesis.matrix = c(2, -1, 1), P = M) axx23vrest #&gt; #&gt; Response transformation matrix: #&gt; [,1] [,2] [,3] #&gt; y1 1 0 0 #&gt; y2 -1 1 0 #&gt; y3 0 -1 1 #&gt; y4 0 0 -1 #&gt; #&gt; Sum of squares and products for the hypothesis: #&gt; [,1] [,2] [,3] #&gt; [1,] 402.5208 127.41667 -390.9375 #&gt; [2,] 127.4167 40.33333 -123.7500 #&gt; [3,] -390.9375 -123.75000 379.6875 #&gt; #&gt; Sum of squares and products for error: #&gt; [,1] [,2] [,3] #&gt; [1,] 261.375 -141.875 28.000 #&gt; [2,] -141.875 248.750 -19.375 #&gt; [3,] 28.000 -19.375 219.875 #&gt; #&gt; Multivariate Tests: #&gt; Df test stat approx F num Df den Df Pr(&gt;F) #&gt; Pillai 1 0.842450 33.86563 3 19 7.9422e-08 *** #&gt; Wilks 1 0.157550 33.86563 3 19 7.9422e-08 *** #&gt; Hotelling-Lawley 1 5.347205 33.86563 3 19 7.9422e-08 *** #&gt; Roy 1 5.347205 33.86563 3 19 7.9422e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 there is a significant difference in means between ax23 drug treatment and the rest of the treatments 22.1.2 Profile Analysis Examine similarities between the treatment effects (between subjects), which is useful for longitudinal analysis. Null is that all treatments have the same average effect. \\[ H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_h \\] Equivalently, \\[ H_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h \\] The exact nature of the similarities and differences between the treatments can be examined under this analysis. Sequential steps in profile analysis: Are the profiles parallel? (i.e., is there no interaction between treatment and time) Are the profiles coincidental? (i.e., are the profiles identical?) Are the profiles horizontal? (i.e., are there no differences between any time points?) If we reject the null hypothesis that the profiles are parallel, we can test Are there differences among groups within some subset of the total time points? Are there differences among time points in a particular group (or groups)? Are there differences within some subset of the total time points in a particular group (or groups)? Example 4 times (p = 4) 3 treatments (h=3) 22.1.2.1 Parallel Profile Are the profiles for each population identical expect for a mean shift? \\[ \\begin{aligned} H_0: \\mu_{11} - \\mu_{21} - \\mu_{12} - \\mu_{22} = &amp;\\dots = \\mu_{1t} - \\mu_{2t} \\\\ \\mu_{11} - \\mu_{31} - \\mu_{12} - \\mu_{32} = &amp;\\dots = \\mu_{1t} - \\mu_{3t} \\\\ &amp;\\dots \\end{aligned} \\] for \\(h-1\\) equations Equivalently, \\[ H_0: \\mathbf{LBM = 0} \\] \\[ \\mathbf{LBM} = \\left[ \\begin{array} {ccc} 1 &amp; -1 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\end{array} \\right] \\left[ \\begin{array} {ccc} \\mu_{11} &amp; \\dots &amp; \\mu_{14} \\\\ \\mu_{21} &amp; \\dots &amp; \\mu_{24} \\\\ \\mu_{31} &amp; \\dots &amp; \\mu_{34} \\end{array} \\right] \\left[ \\begin{array} {ccc} 1 &amp; 1 &amp; 1 \\\\ -1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\end{array} \\right] = \\mathbf{0} \\] where this is the cell means parameterization of \\(\\mathbf{B}\\) The multiplication of the first 2 matrices \\(\\mathbf{LB}\\) is \\[ \\left[ \\begin{array} {cccc} \\mu_{11} - \\mu_{21} &amp; \\mu_{12} - \\mu_{22} &amp; \\mu_{13} - \\mu_{23} &amp; \\mu_{14} - \\mu_{24}\\\\ \\mu_{11} - \\mu_{31} &amp; \\mu_{12} - \\mu_{32} &amp; \\mu_{13} - \\mu_{33} &amp; \\mu_{14} - \\mu_{34} \\end{array} \\right] \\] which is the differences in treatment means at the same time Multiplying by \\(\\mathbf{M}\\), we get the comparison across time \\[ \\left[ \\begin{array} {ccc} (\\mu_{11} - \\mu_{21}) - (\\mu_{12} - \\mu_{22}) &amp; (\\mu_{11} - \\mu_{21}) -(\\mu_{13} - \\mu_{23}) &amp; (\\mu_{11} - \\mu_{21}) - (\\mu_{14} - \\mu_{24}) \\\\ (\\mu_{11} - \\mu_{31}) - (\\mu_{12} - \\mu_{32}) &amp; (\\mu_{11} - \\mu_{31}) - (\\mu_{13} - \\mu_{33}) &amp; (\\mu_{11} - \\mu_{31}) -(\\mu_{14} - \\mu_{34}) \\end{array} \\right] \\] Alternatively, we can also use the effects parameterization \\[ \\mathbf{LBM} = \\left[ \\begin{array} {cccc} 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\end{array} \\right] \\left[ \\begin{array} {c} \\mu&#39; \\\\ \\tau&#39;_1 \\\\ \\tau_2&#39; \\\\ \\tau_3&#39; \\end{array} \\right] \\left[ \\begin{array} {ccc} 1 &amp; 1 &amp; 1 \\\\ -1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\end{array} \\right] = \\mathbf{0} \\] In both parameterizations, \\(rank(\\mathbf{L}) = h-1\\) and \\(rank(\\mathbf{M}) = p-1\\) We could also choose \\(\\mathbf{L}\\) and \\(\\mathbf{M}\\) in other forms \\[ \\mathbf{L} = \\left[ \\begin{array} {cccc} 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{array} \\right] \\] and \\[ \\mathbf{M} = \\left[ \\begin{array} {ccc} 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; -1 \\end{array} \\right] \\] and still obtain the same result. 22.1.2.2 Coincidental Profiles After we have evidence that the profiles are parallel (i.e., fail to reject the parallel profile test), we can ask whether they are identical? Given profiles are parallel, then if the sums of the components of \\(\\mu_i\\) are identical for all the treatments, then the profiles are identical. \\[ H_0: \\mathbf{1&#39;}_p \\mu_1 = \\mathbf{1&#39;}_p \\mu_2 = \\dots = \\mathbf{1&#39;}_p \\mu_h \\] Equivalently, \\[ H_0: \\mathbf{LBM} = \\mathbf{0} \\] where for the cell means parameterization \\[ \\mathbf{L} = \\left[ \\begin{array} {ccc} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 \\end{array} \\right] \\] and \\[ \\mathbf{M} = \\left[ \\begin{array} {cccc} 1 &amp; 1 &amp; 1 &amp; 1 \\end{array} \\right]&#39; \\] multiplication yields \\[ \\left[ \\begin{array} {c} (\\mu_{11} + \\mu_{12} + \\mu_{13} + \\mu_{14}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34}) \\\\ (\\mu_{21} + \\mu_{22} + \\mu_{23} + \\mu_{24}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34}) \\end{array} \\right] = \\left[ \\begin{array} {c} 0 \\\\ 0 \\end{array} \\right] \\] Different choices of \\(\\mathbf{L}\\) and \\(\\mathbf{M}\\) can yield the same result 22.1.2.3 Horizontal Profiles Given that we can’t reject the null hypothesis that all \\(h\\) profiles are the same, we can ask whether all of the elements of the common profile equal? (i.e., horizontal) \\[ H_0: \\mathbf{LBM} = \\mathbf{0} \\] \\[ \\mathbf{L} = \\left[ \\begin{array} {ccc} 1 &amp; 0 &amp; 0 \\end{array} \\right] \\] and \\[ \\mathbf{M} = \\left[ \\begin{array} {ccc} 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; -1 \\end{array} \\right] \\] hence, \\[ \\left[ \\begin{array} {ccc} (\\mu_{11} - \\mu_{12}) &amp; (\\mu_{12} - \\mu_{13}) &amp; (\\mu_{13} + \\mu_{14}) \\end{array} \\right] = \\left[ \\begin{array} {ccc} 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] Note: If we fail to reject all 3 hypotheses, then we fail to reject the null hypotheses of both no difference between treatments and no differences between traits. Test Equivalent test for Parallel profile Interaction Coincidental profile main effect of between-subjects factor Horizontal profile main effect of repeated measures factor profile_fit &lt;- pbg( data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, 1]), original.names = TRUE, profile.plot = FALSE ) summary(profile_fit) #&gt; Call: #&gt; pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, #&gt; 1]), original.names = TRUE, profile.plot = FALSE) #&gt; #&gt; Hypothesis Tests: #&gt; $`Ho: Profiles are parallel` #&gt; Multivariate.Test Statistic Approx.F num.df den.df p.value #&gt; 1 Wilks 0.1102861 12.737599 6 38 7.891497e-08 #&gt; 2 Pillai 1.0891707 7.972007 6 40 1.092397e-05 #&gt; 3 Hotelling-Lawley 6.2587852 18.776356 6 36 9.258571e-10 #&gt; 4 Roy 5.9550887 39.700592 3 20 1.302458e-08 #&gt; #&gt; $`Ho: Profiles have equal levels` #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; group 2 328.7 164.35 5.918 0.00915 ** #&gt; Residuals 21 583.2 27.77 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; $`Ho: Profiles are flat` #&gt; F df1 df2 p-value #&gt; 1 14.30928 3 19 4.096803e-05 # reject null hypothesis of parallel profiles # reject the null hypothesis of coincidental profiles # reject the null hypothesis that the profiles are flat 22.1.3 Summary "],["principal-components.html", "22.2 Principal Components", " 22.2 Principal Components Unsupervised learning find important features reduce the dimensions of the data set “decorrelate” multivariate vectors that have dependence. uses eigenvector/eigvenvalue decomposition of covariance (correlation) matrices. According to the “spectral decomposition theorem”, if \\(\\mathbf{\\Sigma}_{p \\times p}\\) i s a positive semi-definite, symmetric, real matrix, then there exists an orthogonal matrix \\(\\mathbf{A}\\) such that \\(\\mathbf{A&#39;\\Sigma A} = \\Lambda\\) where \\(\\Lambda\\) is a diagonal matrix containing the eigenvalues \\(\\mathbf{\\Sigma}\\) \\[ \\mathbf{\\Lambda} = \\left( \\begin{array} {cccc} \\lambda_1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; \\lambda_p \\end{array} \\right) \\] \\[ \\mathbf{A} = \\left( \\begin{array} {cccc} \\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\ldots &amp; \\mathbf{a}_p \\end{array} \\right) \\] the i-th column of \\(\\mathbf{A}\\) , \\(\\mathbf{a}_i\\), is the i-th \\(p \\times 1\\) eigenvector of \\(\\mathbf{\\Sigma}\\) that corresponds to the eigenvalue, \\(\\lambda_i\\) , where \\(\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_p\\) . Alternatively, express in matrix decomposition: \\[ \\mathbf{\\Sigma} = \\mathbf{A \\Lambda A}&#39; \\] \\[ \\mathbf{\\Sigma} = \\mathbf{A} \\left( \\begin{array} {cccc} \\lambda_1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots&amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; \\lambda_p \\end{array} \\right) \\mathbf{A}&#39; = \\sum_{i=1}^p \\lambda_i \\mathbf{a}_i \\mathbf{a}_i&#39; \\] where the outer product \\(\\mathbf{a}_i \\mathbf{a}_i&#39;\\) is a \\(p \\times p\\) matrix of rank 1. For example, \\(\\mathbf{x} \\sim N_2(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\) \\[ \\mathbf{\\mu} = \\left( \\begin{array} {c} 5 \\\\ 12 \\end{array} \\right); \\mathbf{\\Sigma} = \\left( \\begin{array} {cc} 4 &amp; 1 \\\\ 1 &amp; 2 \\end{array} \\right) \\] library(MASS) mu = as.matrix(c(5, 12)) Sigma = matrix(c(4, 1, 1, 2), nrow = 2, byrow = T) sim &lt;- mvrnorm(n = 1000, mu = mu, Sigma = Sigma) plot(sim[, 1], sim[, 2]) Here, \\[ \\mathbf{A} = \\left( \\begin{array} {cc} 0.9239 &amp; -0.3827 \\\\ 0.3827 &amp; 0.9239 \\\\ \\end{array} \\right) \\] Columns of \\(\\mathbf{A}\\) are the eigenvectors for the decomposition Under matrix multiplication (\\(\\mathbf{A&#39;\\Sigma A}\\) or \\(\\mathbf{A&#39;A}\\) ), the off-diagonal elements equal to 0 Multiplying data by this matrix (i.e., projecting the data onto the orthogonal axes); the distribution of the resulting data (i.e., “scores”) is \\[ N_2 (\\mathbf{A&#39;\\mu,A&#39;\\Sigma A}) = N_2 (\\mathbf{A&#39;\\mu, \\Lambda}) \\] Equivalently, \\[ \\mathbf{y} = \\mathbf{A&#39;x} \\sim N \\left[ \\left( \\begin{array} {c} 9.2119 \\\\ 9.1733 \\end{array} \\right), \\left( \\begin{array} {cc} 4.4144 &amp; 0 \\\\ 0 &amp; 1.5859 \\end{array} \\right) \\right] \\] A_matrix = matrix(c(0.9239, -0.3827, 0.3827, 0.9239), nrow = 2, byrow = T) t(A_matrix) %*% A_matrix #&gt; [,1] [,2] #&gt; [1,] 1.000051 0.000000 #&gt; [2,] 0.000000 1.000051 sim1 &lt;- mvrnorm( n = 1000, mu = t(A_matrix) %*% mu, Sigma = t(A_matrix) %*% Sigma %*% A_matrix ) plot(sim1[, 1], sim1[, 2]) No more dependence in the data structure, plot Notes: The i-th eigenvalue is the variance of a linear combination of the elements of \\(\\mathbf{x}\\) ; \\(var(y_i) = var(\\mathbf{a&#39;_i x}) = \\lambda_i\\) The values on the transformed set of axes (i.e., the \\(y_i\\)’s) are called the scores. These are the orthogonal projections of the data onto the “new principal component axes Variances of \\(y_1\\) are greater than those for any other possible projection Covariance matrix decomposition and projection onto orthogonal axes = PCA 22.2.1 Population Principal Components \\(p \\times 1\\) vectors \\(\\mathbf{x}_1, \\dots , \\mathbf{x}_n\\) which are iid with \\(var(\\mathbf{x}_i) = \\mathbf{\\Sigma}\\) The first PC is the linear combination \\(y_1 = \\mathbf{a}_1&#39; \\mathbf{x} = a_{11}x_1 + \\dots + a_{1p}x_p\\) with \\(\\mathbf{a}_1&#39; \\mathbf{a}_1 = 1\\) such that \\(var(y_1)\\) is the maximum of all linear combinations of \\(\\mathbf{x}\\) which have unit length The second PC is the linear combination \\(y_1 = \\mathbf{a}_2&#39; \\mathbf{x} = a_{21}x_1 + \\dots + a_{2p}x_p\\) with \\(\\mathbf{a}_2&#39; \\mathbf{a}_2 = 1\\) such that \\(var(y_1)\\) is the maximum of all linear combinations of \\(\\mathbf{x}\\) which have unit length and uncorrelated with \\(y_1\\) (i.e., \\(cov(\\mathbf{a}_1&#39; \\mathbf{x}, \\mathbf{a}&#39;_2 \\mathbf{x}) =0\\) continues for all \\(y_i\\) to \\(y_p\\) \\(\\mathbf{a}_i\\)’s are those that make up the matrix \\(\\mathbf{A}\\) in the symmetric decomposition \\(\\mathbf{A&#39;\\Sigma A} = \\mathbf{\\Lambda}\\) , where \\(var(y_1) = \\lambda_1, \\dots , var(y_p) = \\lambda_p\\) And the total variance of \\(\\mathbf{x}\\) is \\[ \\begin{aligned} var(x_1) + \\dots + var(x_p) &amp;= tr(\\Sigma) = \\lambda_1 + \\dots + \\lambda_p \\\\ &amp;= var(y_1) + \\dots + var(y_p) \\end{aligned} \\] Data Reduction To reduce the dimension of data from p (original) to k dimensions without much “loss of information”, we can use properties of the population principal components Suppose \\(\\mathbf{\\Sigma} \\approx \\sum_{i=1}^k \\lambda_i \\mathbf{a}_i \\mathbf{a}_i&#39;\\) . Even thought the true variance-covariance matrix has rank \\(p\\) , it can be be well approximate by a matrix of rank k (k &lt;p) New “traits” are linear combinations of the measured traits. We can attempt to make meaningful interpretation fo the combinations (with orthogonality constraints). The proportion of the total variance accounted for by the j-th principal component is \\[ \\frac{var(y_j)}{\\sum_{i=1}^p var(y_i)} = \\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i} \\] The proportion of the total variation accounted for by the first k principal components is \\(\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}\\) Above example , we have \\(4.4144/(4+2) = .735\\) of the total variability can be explained by the first principal component 22.2.2 Sample Principal Components Since \\(\\mathbf{\\Sigma}\\) is unknown, we use \\[ \\mathbf{S} = \\frac{1}{n-1}\\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})&#39; \\] Let \\(\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots \\ge \\hat{\\lambda}_p \\ge 0\\) be the eigenvalues of \\(\\mathbf{S}\\) and \\(\\hat{\\mathbf{a}}_1, \\hat{\\mathbf{a}}_2, \\dots, \\hat{\\mathbf{a}}_p\\) denote the eigenvectors of \\(\\mathbf{S}\\) Then, the i-th sample principal component score (or principal component or score) is \\[ \\hat{y}_{ij} = \\sum_{k=1}^p \\hat{a}_{ik}x_{kj} = \\hat{\\mathbf{a}}_i&#39;\\mathbf{x}_j \\] Properties of Sample Principal Components The estimated variance of \\(y_i = \\hat{\\mathbf{a}}_i&#39;\\mathbf{x}_j\\) is \\(\\hat{\\lambda}_i\\) The sample covariance between \\(\\hat{y}_i\\) and \\(\\hat{y}_{i&#39;}\\) is 0 when \\(i \\neq i&#39;\\) The proportion of the total sample variance accounted for by the i-th sample principal component is \\(\\frac{\\hat{\\lambda}_i}{\\sum_{k=1}^p \\hat{\\lambda}_k}\\) The estimated correlation between the \\(i\\)-th principal component score and the \\(l\\)-th attribute of \\(\\mathbf{x}\\) is \\[ r_{x_l , \\hat{y}_i} = \\frac{\\hat{a}_{il}\\sqrt{\\lambda_i}}{\\sqrt{s_{ll}}} \\] The correlation coefficient is typically used to interpret the components (i.e., if this correlation is high then it suggests that the l-th original trait is important in the i-th principle component). According to R. A. Johnson, Wichern, et al. (2002), pp.433-434, \\(r_{x_l, \\hat{y}_i}\\) only measures the univariate contribution of an individual X to a component Y without taking into account the presence of the other X’s. Hence, some prefer \\(\\hat{a}_{il}\\) coefficient to interpret the principal component. \\(r_{x_l, \\hat{y}_i} ; \\hat{a}_{il}\\) are referred to as “loadings” To use k principal components, we must calculate the scores for each data vector in the sample \\[ \\mathbf{y}_j = \\left( \\begin{array} {c} y_{1j} \\\\ y_{2j} \\\\ \\vdots \\\\ y_{kj} \\end{array} \\right) = \\left( \\begin{array} {c} \\hat{\\mathbf{a}}_1&#39; \\mathbf{x}_j \\\\ \\hat{\\mathbf{a}}_2&#39; \\mathbf{x}_j \\\\ \\vdots \\\\ \\hat{\\mathbf{a}}_k&#39; \\mathbf{x}_j \\end{array} \\right) = \\left( \\begin{array} {c} \\hat{\\mathbf{a}}_1&#39; \\\\ \\hat{\\mathbf{a}}_2&#39; \\\\ \\vdots \\\\ \\hat{\\mathbf{a}}_k&#39; \\end{array} \\right) \\mathbf{x}_j \\] Issues: Large sample theory exists for eigenvalues and eigenvectors of sample covariance matrices if inference is necessary. But we do not do inference with PCA, we only use it as exploratory or descriptive analysis. PC is not invariant to changes in scale (Exception: if all trait are rescaled by multiplying by the same constant, such as feet to inches). PCA based on the correlation matrix \\(\\mathbf{R}\\) is different than that based on the covariance matrix \\(\\mathbf{\\Sigma}\\) PCA for the correlation matrix is just rescaling each trait to have unit variance Transform \\(\\mathbf{x}\\) to \\(\\mathbf{z}\\) where \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) where the denominator affects the PCA After transformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\) PCA on \\(\\mathbf{R}\\) is calculated in the same way as that on \\(\\mathbf{S}\\) (where \\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) ) The use of \\(\\mathbf{R}, \\mathbf{S}\\) depends on the purpose of PCA. If the scale of the observations if different, covariance matrix is more preferable. but if they are dramatically different, analysis can still be dominated by the large variance traits. How many PCs to use can be guided by Scree Graphs: plot the eigenvalues against their indices. Look for the “elbow” where the steep decline in the graph suddenly flattens out; or big gaps. minimum Percent of total variation (e.g., choose enough components to have 50% or 90%). can be used for interpretations. Kaiser’s rule: use only those PC with eigenvalues larger than 1 (applied to PCA on the correlation matrix) - ad hoc Compare to the eigenvalue scree plot of data to the scree plot when the data are randomized. 22.2.3 Application PCA on the covariance matrix is usually not preferred due to the fact that PCA is not invariant to changes in scale. Hence, PCA on the correlation matrix is more preferred This also addresses the problem of multicollinearity The eigvenvectors may differ by a multiplication of -1 for different implementation, but same interpretation. library(tidyverse) ## Read in and check data stock &lt;- read.table(&quot;images/stock.dat&quot;) names(stock) &lt;- c(&quot;allied&quot;, &quot;dupont&quot;, &quot;carbide&quot;, &quot;exxon&quot;, &quot;texaco&quot;) str(stock) #&gt; &#39;data.frame&#39;: 100 obs. of 5 variables: #&gt; $ allied : num 0 0.027 0.1228 0.057 0.0637 ... #&gt; $ dupont : num 0 -0.04485 0.06077 0.02995 -0.00379 ... #&gt; $ carbide: num 0 -0.00303 0.08815 0.06681 -0.03979 ... #&gt; $ exxon : num 0.0395 -0.0145 0.0862 0.0135 -0.0186 ... #&gt; $ texaco : num 0 0.0435 0.0781 0.0195 -0.0242 ... ## Covariance matrix of data cov(stock) #&gt; allied dupont carbide exxon texaco #&gt; allied 0.0016299269 0.0008166676 0.0008100713 0.0004422405 0.0005139715 #&gt; dupont 0.0008166676 0.0012293759 0.0008276330 0.0003868550 0.0003109431 #&gt; carbide 0.0008100713 0.0008276330 0.0015560763 0.0004872816 0.0004624767 #&gt; exxon 0.0004422405 0.0003868550 0.0004872816 0.0008023323 0.0004084734 #&gt; texaco 0.0005139715 0.0003109431 0.0004624767 0.0004084734 0.0007587370 ## Correlation matrix of data cor(stock) #&gt; allied dupont carbide exxon texaco #&gt; allied 1.0000000 0.5769244 0.5086555 0.3867206 0.4621781 #&gt; dupont 0.5769244 1.0000000 0.5983841 0.3895191 0.3219534 #&gt; carbide 0.5086555 0.5983841 1.0000000 0.4361014 0.4256266 #&gt; exxon 0.3867206 0.3895191 0.4361014 1.0000000 0.5235293 #&gt; texaco 0.4621781 0.3219534 0.4256266 0.5235293 1.0000000 # cov(scale(stock)) # give the same result ## PCA with covariance cov_pca &lt;- prcomp(stock) # uses singular value decomposition for calculation and an N -1 divisor # alternatively, princomp can do PCA via spectral decomposition, # but it has worse numerical accuracy # eigen values cov_results &lt;- data.frame(eigen_values = cov_pca$sdev ^ 2) cov_results %&gt;% mutate(proportion = eigen_values / sum(eigen_values), cumulative = cumsum(proportion)) #&gt; eigen_values proportion cumulative #&gt; 1 0.0035953867 0.60159252 0.6015925 #&gt; 2 0.0007921798 0.13255027 0.7341428 #&gt; 3 0.0007364426 0.12322412 0.8573669 #&gt; 4 0.0005086686 0.08511218 0.9424791 #&gt; 5 0.0003437707 0.05752091 1.0000000 # first 2 PCs account for 73% variance in the data # eigen vectors cov_pca$rotation # prcomp calls rotation #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; allied 0.5605914 0.73884565 -0.1260222 0.28373183 -0.20846832 #&gt; dupont 0.4698673 -0.09286987 -0.4675066 -0.68793190 0.28069055 #&gt; carbide 0.5473322 -0.65401929 -0.1140581 0.50045312 -0.09603973 #&gt; exxon 0.2908932 -0.11267353 0.6099196 -0.43808002 -0.58203935 #&gt; texaco 0.2842017 0.07103332 0.6168831 0.06227778 0.72784638 # princomp calls loadings. # first PC = overall average # second PC compares Allied to Carbide ## PCA with correlation #same as scale(stock) %&gt;% prcomp cor_pca &lt;- prcomp(stock, scale = T) # eigen values cor_results &lt;- data.frame(eigen_values = cor_pca$sdev ^ 2) cor_results %&gt;% mutate(proportion = eigen_values / sum(eigen_values), cumulative = cumsum(proportion)) #&gt; eigen_values proportion cumulative #&gt; 1 2.8564869 0.57129738 0.5712974 #&gt; 2 0.8091185 0.16182370 0.7331211 #&gt; 3 0.5400440 0.10800880 0.8411299 #&gt; 4 0.4513468 0.09026936 0.9313992 #&gt; 5 0.3430038 0.06860076 1.0000000 # first egiven values corresponds to less variance # than PCA based on the covariance matrix # eigen vectors cor_pca$rotation #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; allied 0.4635405 -0.2408499 0.6133570 -0.3813727 0.4532876 #&gt; dupont 0.4570764 -0.5090997 -0.1778996 -0.2113068 -0.6749814 #&gt; carbide 0.4699804 -0.2605774 -0.3370355 0.6640985 0.3957247 #&gt; exxon 0.4216770 0.5252647 -0.5390181 -0.4728036 0.1794482 #&gt; texaco 0.4213291 0.5822416 0.4336029 0.3812273 -0.3874672 # interpretation of PC2 is different from above: # it is a comparison of Allied, Dupont and Carbid to Exxon and Texaco Covid Example To reduce collinearity problem in this dataset, we can use principal components as regressors. load(&#39;images/MOcovid.RData&#39;) covidpca &lt;- prcomp(ndat[,-1],scale = T,center = T) covidpca$rotation[,1:2] #&gt; PC1 PC2 #&gt; X..Population.in.Rural.Areas 0.32865838 0.05090955 #&gt; Area..sq..miles. 0.12014444 -0.28579183 #&gt; Population.density..sq..miles. -0.29670124 0.28312922 #&gt; Literacy.rate -0.12517700 -0.08999542 #&gt; Families -0.25856941 0.16485752 #&gt; Area.of.farm.land..sq..miles. 0.02101106 -0.31070363 #&gt; Number.of.farms -0.03814582 -0.44809679 #&gt; Average.value.of.all.property.per.farm..dollars. -0.05410709 0.14404306 #&gt; Estimation.of.rurality.. -0.19040210 0.12089501 #&gt; Male.. 0.02182394 -0.09568768 #&gt; Number.of.Physcians.per.100.000 -0.31451606 0.13598026 #&gt; average.age 0.29414708 0.35593459 #&gt; X0.4.age.proportion -0.11431336 -0.23574057 #&gt; X20.44.age.proportion -0.32802128 -0.22718550 #&gt; X65.and.over.age.proportion 0.30585033 0.32201626 #&gt; prop..White..nonHisp 0.35627561 -0.14142646 #&gt; prop..Hispanic -0.16655381 -0.15105342 #&gt; prop..Black -0.33333359 0.24405802 # Variability of each principal component: pr.var pr.var &lt;- covidpca$sdev ^ 2 # Variance explained by each principal component: pve pve &lt;- pr.var / sum(pr.var) plot( pve, xlab = &quot;Principal Component&quot;, ylab = &quot;Proportion of Variance Explained&quot;, ylim = c(0, 0.5), type = &quot;b&quot; ) plot( cumsum(pve), xlab = &quot;Principal Component&quot;, ylab = &quot;Cumulative Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &quot;b&quot; ) # the first six principe account for around 80% of the variance. #using base lm function for PC regression pcadat &lt;- data.frame(covidpca$x[, 1:6]) pcadat$y &lt;- ndat$Y pcr.man &lt;- lm(log(y) ~ ., pcadat) mean(pcr.man$residuals ^ 2) #&gt; [1] 0.03453371 #comparison to lm w/o prin comps lm.fit &lt;- lm(log(Y) ~ ., data = ndat) mean(lm.fit$residuals ^ 2) #&gt; [1] 0.02335128 MSE for the PC-based model is larger than regular regression, because models with a large degree of collinearity can still perform well. pcr function in pls can be used for fitting PC regression (it will select the optimal number of components in the model). References "],["factor-analysis.html", "22.3 Factor Analysis", " 22.3 Factor Analysis Purpose Using a few linear combinations of underlying unobservable (latent) traits, we try to describe the covariance relationship among a large number of measured traits Similar to PCA, but factor analysis is model based More details can be found on PSU stat or UMN stat Let \\(\\mathbf{y}\\) be the set of \\(p\\) measured variables \\(E(\\mathbf{y}) = \\mathbf{\\mu}\\) \\(var(\\mathbf{y}) = \\mathbf{\\Sigma}\\) We have \\[ \\begin{aligned} \\mathbf{y} - \\mathbf{\\mu} &amp;= \\mathbf{Lf} + \\epsilon \\\\ &amp;= \\left( \\begin{array} {c} l_{11}f_1 + l_{12}f_2 + \\dots + l_{tm}f_m \\\\ \\vdots \\\\ l_{p1}f_1 + l_{p2}f_2 + \\dots + l_{pm} f_m \\end{array} \\right) + \\left( \\begin{array} {c} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_p \\end{array} \\right) \\end{aligned} \\] where \\(\\mathbf{y} - \\mathbf{\\mu}\\) = the p centered measurements \\(\\mathbf{L}\\) = \\(p \\times m\\) matrix of factor loadings \\(\\mathbf{f}\\) = unobserved common factors for the population \\(\\mathbf{\\epsilon}\\) = random errors (i.e., variation that is not accounted for by the common factors). We want \\(m\\) (the number of factors) to be much smaller than \\(p\\) (the number of measured attributes) Restrictions on the model \\(E(\\epsilon) = \\mathbf{0}\\) \\(var(\\epsilon) = \\Psi_{p \\times p} = diag( \\psi_1, \\dots, \\psi_p)\\) \\(\\mathbf{\\epsilon}, \\mathbf{f}\\) are independent Additional assumption could be \\(E(\\mathbf{f}) = \\mathbf{0}, var(\\mathbf{f}) = \\mathbf{I}_{m \\times m}\\) (known as the orthogonal factor model) , which imposes the following covariance structure on \\(\\mathbf{y}\\) \\[ \\begin{aligned} var(\\mathbf{y}) = \\mathbf{\\Sigma} &amp;= var(\\mathbf{Lf} + \\mathbf{\\epsilon}) \\\\ &amp;= var(\\mathbf{Lf}) + var(\\epsilon) \\\\ &amp;= \\mathbf{L} var(\\mathbf{f}) \\mathbf{L}&#39; + \\mathbf{\\Psi} \\\\ &amp;= \\mathbf{LIL}&#39; + \\mathbf{\\Psi} \\\\ &amp;= \\mathbf{LL}&#39; + \\mathbf{\\Psi} \\end{aligned} \\] Since \\(\\mathbf{\\Psi}\\) is diagonal, the off-diagonal elements of \\(\\mathbf{LL}&#39;\\) are \\(\\sigma_{ij}\\), the co variances in \\(\\mathbf{\\Sigma}\\), which means \\(cov(y_i, y_j) = \\sum_{k=1}^m l_{ik}l_{jk}\\) and the covariance of \\(\\mathbf{y}\\) is completely determined by the m factors ( \\(m &lt;&lt;p\\)) \\(var(y_i) = \\sum_{k=1}^m l_{ik}^2 + \\psi_i\\) where \\(\\psi_i\\) is the specific variance and the summation term is the i-th communality (i.e., portion of the variance of the i-th variable contributed by the \\(m\\) common factors (\\(h_i^2 = \\sum_{k=1}^m l_{ik}^2\\)) The factor model is only uniquely determined up to an orthogonal transformation of the factors. Let \\(\\mathbf{T}_{m \\times m}\\) be an orthogonal matrix \\(\\mathbf{TT}&#39; = \\mathbf{T&#39;T} = \\mathbf{I}\\) then \\[ \\begin{aligned} \\mathbf{y} - \\mathbf{\\mu} &amp;= \\mathbf{Lf} + \\epsilon \\\\ &amp;= \\mathbf{LTT&#39;f} + \\epsilon \\\\ &amp;= \\mathbf{L}^*(\\mathbf{T&#39;f}) + \\epsilon &amp; \\text{where } \\mathbf{L}^* = \\mathbf{LT} \\end{aligned} \\] and \\[ \\begin{aligned} \\mathbf{\\Sigma} &amp;= \\mathbf{LL}&#39; + \\mathbf{\\Psi} \\\\ &amp;= \\mathbf{LTT&#39;L} + \\mathbf{\\Psi} \\\\ &amp;= (\\mathbf{L}^*)(\\mathbf{L}^*)&#39; + \\mathbf{\\Psi} \\end{aligned} \\] Hence, any orthogonal transformation of the factors is an equally good description of the correlations among the observed traits. Let \\(\\mathbf{y} = \\mathbf{Cx}\\) , where \\(\\mathbf{C}\\) is any diagonal matrix, then \\(\\mathbf{L}_y = \\mathbf{CL}_x\\) and \\(\\mathbf{\\Psi}_y = \\mathbf{C\\Psi}_x\\mathbf{C}\\) Hence, we can see that factor analysis is also invariant to changes in scale 22.3.1 Methods of Estimation To estimate \\(\\mathbf{L}\\) Principal Component Method Principal Factor Method 22.3.1.3 22.3.1.1 Principal Component Method Spectral decomposition \\[ \\begin{aligned} \\mathbf{\\Sigma} &amp;= \\lambda_1 \\mathbf{a}_1 \\mathbf{a}_1&#39; + \\dots + \\lambda_p \\mathbf{a}_p \\mathbf{a}_p&#39; \\\\ &amp;= \\mathbf{A\\Lambda A}&#39; \\\\ &amp;= \\sum_{k=1}^m \\lambda+k \\mathbf{a}_k \\mathbf{a}_k&#39; + \\sum_{k= m+1}^p \\lambda_k \\mathbf{a}_k \\mathbf{a}_k&#39; \\\\ &amp;= \\sum_{k=1}^m l_k l_k&#39; + \\sum_{k=m+1}^p \\lambda_k \\mathbf{a}_k \\mathbf{a}_k&#39; \\end{aligned} \\] where \\(l_k = \\mathbf{a}_k \\sqrt{\\lambda_k}\\) and the second term is not diagonal in general. Assume \\[ \\psi_i = \\sigma_{ii} - \\sum_{k=1}^m l_{ik}^2 = \\sigma_{ii} - \\sum_{k=1}^m \\lambda_i a_{ik}^2 \\] then \\[ \\mathbf{\\Sigma} \\approx \\mathbf{LL}&#39; + \\mathbf{\\Psi} \\] To estimate \\(\\mathbf{L}\\) and \\(\\Psi\\) , we use the expected eigenvalues and eigenvectors from \\(\\mathbf{S}\\) or \\(\\mathbf{R}\\) The estimated factor loadings don’t change as the number of actors increases The diagonal elements of \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}&#39; + \\hat{\\mathbf{\\Psi}}\\) are equal to the diagonal elements of \\(\\mathbf{S}\\) and \\(\\mathbf{R}\\), but the covariances may not be exactly reproduced We select \\(m\\) so that the off-diagonal elements close to the values in \\(\\mathbf{S}\\) (or to make the off-diagonal elements of \\(\\mathbf{S} - \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}&#39; + \\hat{\\mathbf{\\Psi}}\\) small) 22.3.1.2 Principal Factor Method Consider modeling the correlation matrix, \\(\\mathbf{R} = \\mathbf{L} \\mathbf{L}&#39; + \\mathbf{\\Psi}\\) . Then \\[ \\mathbf{L} \\mathbf{L}&#39; = \\mathbf{R} - \\mathbf{\\Psi} = \\left( \\begin{array} {cccc} h_1^2 &amp; r_{12} &amp; \\dots &amp; r_{1p} \\\\ r_{21} &amp; h_2^2 &amp; \\dots &amp; r_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{p1} &amp; r_{p2} &amp; \\dots &amp; h_p^2 \\end{array} \\right) \\] where \\(h_i^2 = 1- \\psi_i\\) (the communality) Suppose that initial estimates are available for the communalities, \\((h_1^*)^2,(h_2^*)^2, \\dots , (h_p^*)^2\\), then we can regress each trait on all the others, and then use the \\(r^2\\) as \\(h^2\\) The estimate of \\(\\mathbf{R} - \\mathbf{\\Psi}\\) at step k is \\[ (\\mathbf{R} - \\mathbf{\\Psi})_k = \\left( \\begin{array} {cccc} (h_1^*)^2 &amp; r_{12} &amp; \\dots &amp; r_{1p} \\\\ r_{21} &amp; (h_2^*)^2 &amp; \\dots &amp; r_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{p1} &amp; r_{p2} &amp; \\dots &amp; (h_p^*)^2 \\end{array} \\right) = \\mathbf{L}_k^*(\\mathbf{L}_k^*)&#39; \\] where \\[ \\mathbf{L}_k^* = (\\sqrt{\\hat{\\lambda}_1^*\\hat{\\mathbf{a}}_1^* , \\dots \\hat{\\lambda}_m^*\\hat{\\mathbf{a}}_m^*}) \\] and \\[ \\hat{\\psi}_{i,k}^* = 1 - \\sum_{j=1}^m \\hat{\\lambda}_i^* (\\hat{a}_{ij}^*)^2 \\] we used the spectral decomposition on the estimated matrix \\((\\mathbf{R}- \\mathbf{\\Psi})\\) to calculate the \\(\\hat{\\lambda}_i^* s\\) and the \\(\\mathbf{\\hat{a}}_i^* s\\) After updating the values of \\((\\hat{h}_i^*)^2 = 1 - \\hat{\\psi}_{i,k}^*\\) we will use them to form a new \\(\\mathbf{L}_{k+1}^*\\) via another spectral decomposition. Repeat the process Notes: The matrix \\((\\mathbf{R} - \\mathbf{\\Psi})_k\\) is not necessarily positive definite The principal component method is similar to principal factor if one considers the initial communalities are \\(h^2 = 1\\) if \\(m\\) is too large, some communalities may become larger than 1, causing the iterations to terminate. To combat, we can fix any communality that is greater than 1 at 1 and then continues. continue iterations regardless of the size of the communalities. However, results can be outside fo the parameter space. 22.3.1.3 Maximum Likelihood Method Since we need the likelihood function, we make the additional (critical) assumption that \\(\\mathbf{y}_j \\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) for \\(j = 1,..,n\\) \\(\\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{I})\\) \\(\\epsilon_j \\sim N(\\mathbf{0}, \\mathbf{\\Psi})\\) and restriction \\(\\mathbf{L}&#39; \\mathbf{\\Psi}^{-1}\\mathbf{L} = \\mathbf{\\Delta}\\) where \\(\\mathbf{\\Delta}\\) is a diagonal matrix. (since the factor loading matrix is not unique, we need this restriction). Notes: Finding MLE can be computationally expensive we typically use other methods for exploratory data analysis Likelihood ratio tests could be used for testing hypotheses in this framework (i.e., Confirmatory Factor Analysis) 22.3.2 Factor Rotation \\(\\mathbf{T}_{m \\times m}\\) is an orthogonal matrix that has the property that \\[ \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}&#39; + \\hat{\\mathbf{\\Psi}} = \\hat{\\mathbf{L}}^*(\\hat{\\mathbf{L}}^*)&#39; + \\hat{\\mathbf{\\Psi}} \\] where \\(\\mathbf{L}^* = \\mathbf{LT}\\) This means that estimated specific variances and communalities are not altered by the orthogonal transformation. Since there are an infinite number of choices for \\(\\mathbf{T}\\), some selection criterion is necessary For example, we can find the orthogonal transformation that maximizes the objective function \\[ \\sum_{j = 1}^m [\\frac{1}{p}\\sum_{i=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 - \\{\\frac{\\gamma}{p} \\sum_{i=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 \\}^2] \\] where \\(\\frac{l_{ij}^{*2}}{h_i}\\) are “scaled loadings”, which gives variables with small communalities more influence. Different choices of \\(\\gamma\\) in the objective function correspond to different orthogonal rotation found in the literature; Varimax \\(\\gamma = 1\\) (rotate the factors so that each of the \\(p\\) variables should have a high loading on only one factor, but this is not always possible). Quartimax \\(\\gamma = 0\\) Equimax \\(\\gamma = m/2\\) Parsimax \\(\\gamma = \\frac{p(m-1)}{p+m-2}\\) Promax: non-orthogonal or olique transformations Harris-Kaiser (HK): non-orthogonal or oblique transformations 22.3.3 Estimation of Factor Scores Recall \\[ (\\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}_{p \\times m}\\mathbf{f}_j + \\epsilon_j \\] If the factor model is correct then \\[ var(\\epsilon_j) = \\mathbf{\\Psi} = diag (\\psi_1, \\dots , \\psi_p) \\] Thus we could consider using weighted least squares to estimate \\(\\mathbf{f}_j\\) , the vector of factor scores for the j-th sampled unit by \\[ \\begin{aligned} \\hat{\\mathbf{f}} &amp;= (\\mathbf{L}&#39;\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}&#39; \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\mu}) \\\\ &amp; \\approx (\\mathbf{L}&#39;\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}&#39; \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}}) \\end{aligned} \\] 22.3.3.1 The Regression Method Alternatively, we can use the regression method to estimate the factor scores Consider the joint distribution of \\((\\mathbf{y}_j - \\mathbf{\\mu})\\) and \\(\\mathbf{f}_j\\) assuming multivariate normality, as in the maximum likelihood approach. then, \\[ \\left( \\begin{array} {c} \\mathbf{y}_j - \\mathbf{\\mu} \\\\ \\mathbf{f}_j \\end{array} \\right) \\sim N_{p + m} \\left( \\left[ \\begin{array} {cc} \\mathbf{LL}&#39; + \\mathbf{\\Psi} &amp; \\mathbf{L} \\\\ \\mathbf{L}&#39; &amp; \\mathbf{I}_{m\\times m} \\end{array} \\right] \\right) \\] when the \\(m\\) factor model is correct Hence, \\[ E(\\mathbf{f}_j | \\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}&#39; (\\mathbf{LL}&#39; + \\mathbf{\\Psi})^{-1}(\\mathbf{y}_j - \\mathbf{\\mu}) \\] notice that \\(\\mathbf{L}&#39; (\\mathbf{LL}&#39; + \\mathbf{\\Psi})^{-1}\\) is an \\(m \\times p\\) matrix of regression coefficients Then, we use the estimated conditional mean vector to estimate the factor scores \\[ \\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}&#39;(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}&#39; + \\mathbf{\\hat{\\Psi}})^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}}) \\] Alternatively, we could reduce the effect of possible incorrect determination fo the number of factors \\(m\\) by using \\(\\mathbf{S}\\) as a substitute for \\(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}&#39; + \\mathbf{\\hat{\\Psi}}\\) then \\[ \\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}&#39;\\mathbf{S}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}}) \\] where \\(j = 1,\\dots,n\\) 22.3.4 Model Diagnostic Plots Check for outliers (recall that \\(\\mathbf{f}_j \\sim iid N(\\mathbf{0}, \\mathbf{I}_{m \\times m})\\)) Check for multivariate normality assumption Use univariate tests for normality to check the factor scores Confirmatory Factor Analysis: formal testing of hypotheses about loadings, use MLE and full/reduced model testing paradigm and measures of model fit 22.3.5 Application In the psych package, h2 = the communalities u2 = the uniqueness com = the complexity library(psych) library(tidyverse) ## Load the data from the psych package data(Harman.5) Harman.5 #&gt; population schooling employment professional housevalue #&gt; Tract1 5700 12.8 2500 270 25000 #&gt; Tract2 1000 10.9 600 10 10000 #&gt; Tract3 3400 8.8 1000 10 9000 #&gt; Tract4 3800 13.6 1700 140 25000 #&gt; Tract5 4000 12.8 1600 140 25000 #&gt; Tract6 8200 8.3 2600 60 12000 #&gt; Tract7 1200 11.4 400 10 16000 #&gt; Tract8 9100 11.5 3300 60 14000 #&gt; Tract9 9900 12.5 3400 180 18000 #&gt; Tract10 9600 13.7 3600 390 25000 #&gt; Tract11 9600 9.6 3300 80 12000 #&gt; Tract12 9400 11.4 4000 100 13000 # Correlation matrix cor_mat &lt;- cor(Harman.5) cor_mat #&gt; population schooling employment professional housevalue #&gt; population 1.00000000 0.00975059 0.9724483 0.4388708 0.02241157 #&gt; schooling 0.00975059 1.00000000 0.1542838 0.6914082 0.86307009 #&gt; employment 0.97244826 0.15428378 1.0000000 0.5147184 0.12192599 #&gt; professional 0.43887083 0.69140824 0.5147184 1.0000000 0.77765425 #&gt; housevalue 0.02241157 0.86307009 0.1219260 0.7776543 1.00000000 ## Principal Component Method with Correlation cor_pca &lt;- prcomp(Harman.5, scale = T) # eigen values cor_results &lt;- data.frame(eigen_values = cor_pca$sdev ^ 2) cor_results &lt;- cor_results %&gt;% mutate( proportion = eigen_values / sum(eigen_values), cumulative = cumsum(proportion), number = row_number() ) cor_results #&gt; eigen_values proportion cumulative number #&gt; 1 2.87331359 0.574662719 0.5746627 1 #&gt; 2 1.79666009 0.359332019 0.9339947 2 #&gt; 3 0.21483689 0.042967377 0.9769621 3 #&gt; 4 0.09993405 0.019986811 0.9969489 4 #&gt; 5 0.01525537 0.003051075 1.0000000 5 # Scree plot of Eigenvalues scree_gg &lt;- ggplot(cor_results, aes(x = number, y = eigen_values)) + geom_line(alpha = 0.5) + geom_text(aes(label = number)) + scale_x_continuous(name = &quot;Number&quot;) + scale_y_continuous(name = &quot;Eigenvalue&quot;) + theme_bw() scree_gg screeplot(cor_pca, type = &#39;lines&#39;) ## Keep 2 factors based on scree plot and eigenvalues factor_pca &lt;- principal(Harman.5, nfactors = 2, rotate = &quot;none&quot;) factor_pca #&gt; Principal Components Analysis #&gt; Call: principal(r = Harman.5, nfactors = 2, rotate = &quot;none&quot;) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; PC1 PC2 h2 u2 com #&gt; population 0.58 0.81 0.99 0.012 1.8 #&gt; schooling 0.77 -0.54 0.89 0.115 1.8 #&gt; employment 0.67 0.73 0.98 0.021 2.0 #&gt; professional 0.93 -0.10 0.88 0.120 1.0 #&gt; housevalue 0.79 -0.56 0.94 0.062 1.8 #&gt; #&gt; PC1 PC2 #&gt; SS loadings 2.87 1.80 #&gt; Proportion Var 0.57 0.36 #&gt; Cumulative Var 0.57 0.93 #&gt; Proportion Explained 0.62 0.38 #&gt; Cumulative Proportion 0.62 1.00 #&gt; #&gt; Mean item complexity = 1.7 #&gt; Test of the hypothesis that 2 components are sufficient. #&gt; #&gt; The root mean square of the residuals (RMSR) is 0.03 #&gt; with the empirical chi square 0.29 with prob &lt; 0.59 #&gt; #&gt; Fit based upon off diagonal values = 1 # factor 1 = overall socioeconomic health # factor 2 = contrast of the population and employment against school and house value ## Ssquared multiple correlation (SMC) prior, no rotation factor_pca_smc &lt;- fa( Harman.5, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;none&quot;, SMC = TRUE ) factor_pca_smc #&gt; Factor Analysis using method = pa #&gt; Call: fa(r = Harman.5, nfactors = 2, rotate = &quot;none&quot;, SMC = TRUE, fm = &quot;pa&quot;) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; PA1 PA2 h2 u2 com #&gt; population 0.62 0.78 1.00 -0.0027 1.9 #&gt; schooling 0.70 -0.53 0.77 0.2277 1.9 #&gt; employment 0.70 0.68 0.96 0.0413 2.0 #&gt; professional 0.88 -0.15 0.80 0.2017 1.1 #&gt; housevalue 0.78 -0.60 0.96 0.0361 1.9 #&gt; #&gt; PA1 PA2 #&gt; SS loadings 2.76 1.74 #&gt; Proportion Var 0.55 0.35 #&gt; Cumulative Var 0.55 0.90 #&gt; Proportion Explained 0.61 0.39 #&gt; Cumulative Proportion 0.61 1.00 #&gt; #&gt; Mean item complexity = 1.7 #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; #&gt; df null model = 10 with the objective function = 6.38 with Chi Square = 54.25 #&gt; df of the model are 1 and the objective function was 0.34 #&gt; #&gt; The root mean square of the residuals (RMSR) is 0.01 #&gt; The df corrected root mean square of the residuals is 0.03 #&gt; #&gt; The harmonic n.obs is 12 with the empirical chi square 0.02 with prob &lt; 0.88 #&gt; The total n.obs was 12 with Likelihood Chi Square = 2.44 with prob &lt; 0.12 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.596 #&gt; RMSEA index = 0.336 and the 90 % confidence intervals are 0 0.967 #&gt; BIC = -0.04 #&gt; Fit based upon off diagonal values = 1 ## SMC prior, Promax rotation factor_pca_smc_pro &lt;- fa( Harman.5, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;Promax&quot;, SMC = TRUE ) factor_pca_smc_pro #&gt; Factor Analysis using method = pa #&gt; Call: fa(r = Harman.5, nfactors = 2, rotate = &quot;Promax&quot;, SMC = TRUE, #&gt; fm = &quot;pa&quot;) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; PA1 PA2 h2 u2 com #&gt; population -0.11 1.02 1.00 -0.0027 1.0 #&gt; schooling 0.90 -0.11 0.77 0.2277 1.0 #&gt; employment 0.02 0.97 0.96 0.0413 1.0 #&gt; professional 0.75 0.33 0.80 0.2017 1.4 #&gt; housevalue 1.01 -0.14 0.96 0.0361 1.0 #&gt; #&gt; PA1 PA2 #&gt; SS loadings 2.38 2.11 #&gt; Proportion Var 0.48 0.42 #&gt; Cumulative Var 0.48 0.90 #&gt; Proportion Explained 0.53 0.47 #&gt; Cumulative Proportion 0.53 1.00 #&gt; #&gt; With factor correlations of #&gt; PA1 PA2 #&gt; PA1 1.00 0.25 #&gt; PA2 0.25 1.00 #&gt; #&gt; Mean item complexity = 1.1 #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; #&gt; df null model = 10 with the objective function = 6.38 with Chi Square = 54.25 #&gt; df of the model are 1 and the objective function was 0.34 #&gt; #&gt; The root mean square of the residuals (RMSR) is 0.01 #&gt; The df corrected root mean square of the residuals is 0.03 #&gt; #&gt; The harmonic n.obs is 12 with the empirical chi square 0.02 with prob &lt; 0.88 #&gt; The total n.obs was 12 with Likelihood Chi Square = 2.44 with prob &lt; 0.12 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.596 #&gt; RMSEA index = 0.336 and the 90 % confidence intervals are 0 0.967 #&gt; BIC = -0.04 #&gt; Fit based upon off diagonal values = 1 ## SMC prior, varimax rotation factor_pca_smc_var &lt;- fa( Harman.5, nfactors = 2, fm = &quot;pa&quot;, rotate = &quot;varimax&quot;, SMC = TRUE ) ## Make a data frame of the loadings for ggplot2 factors_df &lt;- bind_rows( data.frame( y = rownames(factor_pca_smc$loadings), unclass(factor_pca_smc$loadings) ), data.frame( y = rownames(factor_pca_smc_pro$loadings), unclass(factor_pca_smc_pro$loadings) ), data.frame( y = rownames(factor_pca_smc_var$loadings), unclass(factor_pca_smc_var$loadings) ), .id = &quot;Rotation&quot; ) flag_gg &lt;- ggplot(factors_df) + geom_vline(aes(xintercept = 0)) + geom_hline(aes(yintercept = 0)) + geom_point(aes( x = PA2, y = PA1, col = y, shape = y ), size = 2) + scale_x_continuous(name = &quot;Factor 2&quot;, limits = c(-1.1, 1.1)) + scale_y_continuous(name = &quot;Factor1&quot;, limits = c(-1.1, 1.1)) + facet_wrap(&quot;Rotation&quot;, labeller = labeller(Rotation = c( &quot;1&quot; = &quot;Original&quot;, &quot;2&quot; = &quot;Promax&quot;, &quot;3&quot; = &quot;Varimax&quot; ))) + coord_fixed(ratio = 1) # make aspect ratio of each facet 1 flag_gg # promax and varimax did a good job to assign trait to a particular factor factor_mle_1 &lt;- fa( Harman.5, nfactors = 1, fm = &quot;mle&quot;, rotate = &quot;none&quot;, SMC = TRUE ) factor_mle_1 #&gt; Factor Analysis using method = ml #&gt; Call: fa(r = Harman.5, nfactors = 1, rotate = &quot;none&quot;, SMC = TRUE, fm = &quot;mle&quot;) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; ML1 h2 u2 com #&gt; population 0.97 0.950 0.0503 1 #&gt; schooling 0.14 0.021 0.9791 1 #&gt; employment 1.00 0.995 0.0049 1 #&gt; professional 0.51 0.261 0.7388 1 #&gt; housevalue 0.12 0.014 0.9864 1 #&gt; #&gt; ML1 #&gt; SS loadings 2.24 #&gt; Proportion Var 0.45 #&gt; #&gt; Mean item complexity = 1 #&gt; Test of the hypothesis that 1 factor is sufficient. #&gt; #&gt; df null model = 10 with the objective function = 6.38 with Chi Square = 54.25 #&gt; df of the model are 5 and the objective function was 3.14 #&gt; #&gt; The root mean square of the residuals (RMSR) is 0.41 #&gt; The df corrected root mean square of the residuals is 0.57 #&gt; #&gt; The harmonic n.obs is 12 with the empirical chi square 39.41 with prob &lt; 2e-07 #&gt; The total n.obs was 12 with Likelihood Chi Square = 24.56 with prob &lt; 0.00017 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.022 #&gt; RMSEA index = 0.564 and the 90 % confidence intervals are 0.374 0.841 #&gt; BIC = 12.14 #&gt; Fit based upon off diagonal values = 0.5 #&gt; Measures of factor score adequacy #&gt; ML1 #&gt; Correlation of (regression) scores with factors 1.00 #&gt; Multiple R square of scores with factors 1.00 #&gt; Minimum correlation of possible factor scores 0.99 factor_mle_2 &lt;- fa( Harman.5, nfactors = 2, fm = &quot;mle&quot;, rotate = &quot;none&quot;, SMC = TRUE ) factor_mle_2 #&gt; Factor Analysis using method = ml #&gt; Call: fa(r = Harman.5, nfactors = 2, rotate = &quot;none&quot;, SMC = TRUE, fm = &quot;mle&quot;) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; ML2 ML1 h2 u2 com #&gt; population -0.03 1.00 1.00 0.005 1.0 #&gt; schooling 0.90 0.04 0.81 0.193 1.0 #&gt; employment 0.09 0.98 0.96 0.036 1.0 #&gt; professional 0.78 0.46 0.81 0.185 1.6 #&gt; housevalue 0.96 0.05 0.93 0.074 1.0 #&gt; #&gt; ML2 ML1 #&gt; SS loadings 2.34 2.16 #&gt; Proportion Var 0.47 0.43 #&gt; Cumulative Var 0.47 0.90 #&gt; Proportion Explained 0.52 0.48 #&gt; Cumulative Proportion 0.52 1.00 #&gt; #&gt; Mean item complexity = 1.1 #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; #&gt; df null model = 10 with the objective function = 6.38 with Chi Square = 54.25 #&gt; df of the model are 1 and the objective function was 0.31 #&gt; #&gt; The root mean square of the residuals (RMSR) is 0.01 #&gt; The df corrected root mean square of the residuals is 0.05 #&gt; #&gt; The harmonic n.obs is 12 with the empirical chi square 0.05 with prob &lt; 0.82 #&gt; The total n.obs was 12 with Likelihood Chi Square = 2.22 with prob &lt; 0.14 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.658 #&gt; RMSEA index = 0.307 and the 90 % confidence intervals are 0 0.945 #&gt; BIC = -0.26 #&gt; Fit based upon off diagonal values = 1 #&gt; Measures of factor score adequacy #&gt; ML2 ML1 #&gt; Correlation of (regression) scores with factors 0.98 1.00 #&gt; Multiple R square of scores with factors 0.95 1.00 #&gt; Minimum correlation of possible factor scores 0.91 0.99 factor_mle_3 &lt;- fa( Harman.5, nfactors = 3, fm = &quot;mle&quot;, rotate = &quot;none&quot;, SMC = TRUE ) factor_mle_3 #&gt; Factor Analysis using method = ml #&gt; Call: fa(r = Harman.5, nfactors = 3, rotate = &quot;none&quot;, SMC = TRUE, fm = &quot;mle&quot;) #&gt; Standardized loadings (pattern matrix) based upon correlation matrix #&gt; ML2 ML1 ML3 h2 u2 com #&gt; population -0.12 0.98 -0.11 0.98 0.0162 1.1 #&gt; schooling 0.89 0.15 0.29 0.90 0.0991 1.3 #&gt; employment 0.00 1.00 0.04 0.99 0.0052 1.0 #&gt; professional 0.72 0.52 -0.10 0.80 0.1971 1.9 #&gt; housevalue 0.97 0.13 -0.09 0.97 0.0285 1.1 #&gt; #&gt; ML2 ML1 ML3 #&gt; SS loadings 2.28 2.26 0.11 #&gt; Proportion Var 0.46 0.45 0.02 #&gt; Cumulative Var 0.46 0.91 0.93 #&gt; Proportion Explained 0.49 0.49 0.02 #&gt; Cumulative Proportion 0.49 0.98 1.00 #&gt; #&gt; Mean item complexity = 1.2 #&gt; Test of the hypothesis that 3 factors are sufficient. #&gt; #&gt; df null model = 10 with the objective function = 6.38 with Chi Square = 54.25 #&gt; df of the model are -2 and the objective function was 0 #&gt; #&gt; The root mean square of the residuals (RMSR) is 0 #&gt; The df corrected root mean square of the residuals is NA #&gt; #&gt; The harmonic n.obs is 12 with the empirical chi square 0 with prob &lt; NA #&gt; The total n.obs was 12 with Likelihood Chi Square = 0 with prob &lt; NA #&gt; #&gt; Tucker Lewis Index of factoring reliability = 1.318 #&gt; Fit based upon off diagonal values = 1 #&gt; Measures of factor score adequacy #&gt; ML2 ML1 ML3 #&gt; Correlation of (regression) scores with factors 0.99 1.00 0.82 #&gt; Multiple R square of scores with factors 0.98 1.00 0.68 #&gt; Minimum correlation of possible factor scores 0.96 0.99 0.36 The output info for the null hypothesis of no common factors is in the statement “The degrees of freedom for the null model ..” The output info for the null hypothesis that number of factors is sufficient is in the statement “The total number of observations was …” One factor is not enough, two is sufficient, and not enough data for 3 factors (df of -2 and NA for p-value). Hence, we should use 2-factor model. "],["discriminant-analysis.html", "22.4 Discriminant Analysis", " 22.4 Discriminant Analysis Suppose we have two or more different populations from which observations could come from. Discriminant analysis seeks to determine which of the possible population an observation comes from while making as few mistakes as possible This is an alternative to logistic approaches with the following advantages: when there is clear separation between classes, the parameter estimates for the logic regression model can be surprisingly unstable, while discriminant approaches do not suffer If X is normal in each of the classes and the sample size is small, then discriminant approaches can be more accurate Notation Similar to MANOVA, let \\(\\mathbf{y}_{j1},\\mathbf{y}_{j2},\\dots, \\mathbf{y}_{in_j} \\sim iid f_j (\\mathbf{y})\\) for \\(j = 1,\\dots, h\\) Let \\(f_j(\\mathbf{y})\\) be the density function for population j . Note that each vector \\(\\mathbf{y}\\) contain measurements on all \\(p\\) traits Assume that each observation is from one of \\(h\\) possible populations. We want to form a discriminant rule that will allocate an observation \\(\\mathbf{y}\\) to population j when \\(\\mathbf{y}\\) is in fact from this population 22.4.1 Known Populations The maximum likelihood discriminant rule for assigning an observation \\(\\mathbf{y}\\) to one of the \\(h\\) populations allocates \\(\\mathbf{y}\\) to the population that gives the largest likelihood to \\(\\mathbf{y}\\) Consider the likelihood for a single observation \\(\\mathbf{y}\\), which has the form \\(f_j (\\mathbf{y})\\) where j is the true population. Since \\(j\\) is unknown, to make the likelihood as large as possible, we should choose the value j which causes \\(f_j (\\mathbf{y})\\) to be as large as possible Consider a simple univariate example. Suppose we have data from one of two binomial populations. The first population has \\(n= 10\\) trials with success probability \\(p = .5\\) The second population has \\(n= 10\\) trials with success probability \\(p = .7\\) to which population would we assign an observation of \\(y = 7\\) Note: \\(f(y = 7|n = 10, p = .5) = .117\\) \\(f(y = 7|n = 10, p = .7) = .267\\) where \\(f(.)\\) is the binomial likelihood. Hence, we choose the second population Another example We have 2 populations, where First population: \\(N(\\mu_1, \\sigma^2_1)\\) Second population: \\(N(\\mu_2, \\sigma^2_2)\\) The likelihood for a single observation is \\[ f_j (y) = (2\\pi \\sigma^2_j)^{-1/2} \\exp\\{ -\\frac{1}{2}(\\frac{y - \\mu_j}{\\sigma_j})^2\\} \\] Consider a likelihood ratio rule \\[ \\begin{aligned} \\Lambda &amp;= \\frac{\\text{likelihood of y from pop 1}}{\\text{likelihood of y from pop 2}} \\\\ &amp;= \\frac{f_1(y)}{f_2(y)} \\\\ &amp;= \\frac{\\sigma_2}{\\sigma_1} \\exp\\{-\\frac{1}{2}[(\\frac{y - \\mu_1}{\\sigma_1})^2- (\\frac{y - \\mu_2}{\\sigma_2})^2] \\} \\end{aligned} \\] Hence, we classify into pop 1 if \\(\\Lambda &gt;1\\) pop 2 if \\(\\Lambda &lt;1\\) for ties, flip a coin Another way to think: we classify into population 1 if the “standardized distance” of y from \\(\\mu_1\\) is less than the “standardized distance” of y from \\(\\mu_2\\) which is referred to as a quadratic discriminant rule. (Significant simplification occurs in th special case where \\(\\sigma_1 = \\sigma_2 = \\sigma^2\\)) Thus, we classify into population 1 if \\[ (y - \\mu_2)^2 &gt; (y - \\mu_1)^2 \\] or \\[ |y- \\mu_2| &gt; |y - \\mu_1| \\] and \\[ -2 \\log (\\Lambda) = -2y \\frac{(\\mu_1 - \\mu_2)}{\\sigma^2} + \\frac{(\\mu_1^2 - \\mu_2^2)}{\\sigma^2} = \\beta y + \\alpha \\] Thus, we classify into population 1 if this is less than 0. Discriminant classification rule is linear in y in this case. 22.4.1.1 Multivariate Expansion Suppose that there are 2 populations \\(N_p(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\) \\(N_p(\\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2)\\) \\[ \\begin{aligned} -2 \\log(\\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})}) &amp;= \\log|\\mathbf{\\Sigma}_1| + (\\mathbf{x} - \\mathbf{\\mu}_1)&#39; \\mathbf{\\Sigma}^{-1}_1 (\\mathbf{x} - \\mathbf{\\mu}_1) \\\\ &amp;- [\\log|\\mathbf{\\Sigma}_2|+ (\\mathbf{x} - \\mathbf{\\mu}_2)&#39; \\mathbf{\\Sigma}^{-1}_2 (\\mathbf{x} - \\mathbf{\\mu}_2) ] \\end{aligned} \\] Again, we classify into population 1 if this is less than 0, otherwise, population 2. And like the univariate case with non-equal variances, this is a quadratic discriminant rule. And if the covariance matrices are equal: \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}_1\\) classify into population 1 if \\[ (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)&#39; \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)&#39; \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) \\ge 0 \\] This linear discriminant rule is also referred to as Fisher’s linear discriminant function By assuming the covariance matrices are equal, we assume that the shape and orientation fo the two populations must be the same (which can be a strong restriction) In other words, for each variable, it can have different mean but the same variance. Note: LDA Bayes decision boundary is linear. Hence, quadratic decision boundary might lead to better classification. Moreover, the assumption of same variance/covariance matrix across all classes for Gaussian densities imposes the linear rule, if we allow the predictors in each class to follow MVN distribution with class-specific mean vectors and variance/covariance matrices, then it is Quadratic Discriminant Analysis. But then, you will have more parameters to estimate (which gives more flexibility than LDA) at the cost of more variance (bias -variance tradeoff). When \\(\\mathbf{\\mu}_1, \\mathbf{\\mu}_2, \\mathbf{\\Sigma}\\) are known, the probability of misclassification can be determined: \\[ \\begin{aligned} P(2|1) &amp;= P(\\text{calssify into pop 2| x is from pop 1}) \\\\ &amp;= P((\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)&#39; \\mathbf{\\Sigma}^{-1} \\mathbf{x} \\le \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)&#39; \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)|\\mathbf{x} \\sim N(\\mu_1, \\mathbf{\\Sigma}) \\\\ &amp;= \\Phi(-\\frac{1}{2} \\delta) \\end{aligned} \\] where \\(\\delta^2 = (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)&#39; \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\) \\(\\Phi\\) is the standard normal CDF Suppose there are \\(h\\) possible populations, which are distributed as \\(N_p (\\mathbf{\\mu}_p, \\mathbf{\\Sigma})\\). Then, the maximum likelihood (linear) discriminant rule allocates \\(\\mathbf{y}\\) to population j where j minimizes the squared Mahalanobis distance \\[ (\\mathbf{y} - \\mathbf{\\mu}_j)&#39; \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{\\mu}_j) \\] 22.4.1.2 Bayes Discriminant Rules If we know that population j has prior probabilities \\(\\pi_j\\) (assume \\(\\pi_j &gt;0\\)) we can form the Bayes discriminant rule. This rule allocates an observation \\(\\mathbf{y}\\) to the population for which \\(\\pi_j f_j (\\mathbf{y})\\) is maximized. Note: Maximum likelihood discriminant rule is a special case of the Bayes discriminant rule, where it sets all the \\(\\pi_j = 1/h\\) Optimal Properties of Bayes Discriminant Rules let \\(p_{ii}\\) be the probability of correctly assigning an observation from population i then one rule (with probabilities \\(p_{ii}\\) ) is as good as another rule (with probabilities \\(p_{ii}&#39;\\) ) if \\(p_{ii} \\ge p_{ii}&#39;\\) for all \\(i = 1,\\dots, h\\) The first rule is better than the alternative if \\(p_{ii} &gt; p_{ii}&#39;\\) for at least one i. A rule for which there is no better alternative is called admissible Bayes Discriminant Rules are admissible If we utilized prior probabilities, then we can form the posterior probability of a correct allocation, \\(\\sum_{i=1}^h \\pi_i p_{ii}\\) Bayes Discriminant Rules have the largest possible posterior probability of correct allocation with respect to the prior These properties show that Bayes Discriminant rule is our best approach. Unequal Cost We want to consider the cost misallocation Define \\(c_{ij}\\) to be the cost associated with allocation a member of population j to population i. Assume that \\(c_{ij} &gt;0\\) for all \\(i \\neq j\\) \\(c_{ij} = 0\\) if \\(i = j\\) We could determine the expected amount of loss for an observation allocated to population i as \\(\\sum_j c_{ij} p_{ij}\\) where the \\(p_{ij}s\\) are the probabilities of allocating an observation from population j into population i We want to minimize the amount of loss expected for our rule. Using a Bayes Discrimination, allocate \\(\\mathbf{y}\\) to the population j which minimizes \\(\\sum_{k \\neq j} c_{ij} \\pi_k f_k(\\mathbf{y})\\) We could assign equal probabilities to each group and get a maximum likelihood type rule. here, we would allocate \\(\\mathbf{y}\\) to population j which minimizes \\(\\sum_{k \\neq j}c_{jk} f_k(\\mathbf{y})\\) Example: Two binomial populations, each of size 10, with probabilities \\(p_1 = .5\\) and \\(p_2 = .7\\) And the probability of being in the first population is .9 However, suppose the cost of inappropriately allocating into the first population is 1 and the cost of incorrectly allocating into the second population is 5. In this case, we pick population 1 over population 2 In general, we consider two regions, \\(R_1\\) and \\(R_2\\) associated with population 1 and 2: \\[ R_1: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} \\ge \\frac{c_{12} \\pi_2}{c_{21} \\pi_1} \\] \\[ R_2: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} &lt; \\frac{c_{12} \\pi_2}{c_{21} \\pi_1} \\] where \\(c_{12}\\) is the cost of assigning a member of population 2 to population 1. 22.4.1.3 Discrimination Under Estimation Suppose we know the form of the distributions for populations of interests, but we still have to estimate the parameters. Example: we know the distributions are multivariate normal, but we have to estimate the means and variances The maximum likelihood discriminant rule allocates an observation \\(\\mathbf{y}\\) to population j when j maximizes the function \\[ f_j (\\mathbf{y} |\\hat{\\theta}) \\] where \\(\\hat{\\theta}\\) are the maximum likelihood estimates of the unknown parameters For instance, we have 2 multivariate normal populations with distinct means, but common variance covariance matrix MLEs for \\(\\mathbf{\\mu}_1\\) and \\(\\mathbf{\\mu}_2\\) are \\(\\mathbf{\\bar{y}}_1\\) and \\(\\mathbf{\\bar{y}}_2\\)and common \\(\\mathbf{\\Sigma}\\) is \\(\\mathbf{S}\\). Thus, an estimated discriminant rule could be formed by substituting these sample values for the population values 22.4.1.4 Native Bayes The challenge with classification using Bayes’ is that we don’t know the (true) densities, \\(f_k, k = 1, \\dots, K\\), while LDA and QDA make strong multivariate normality assumptions to deal with this. Naive Bayes makes only one assumption: within the k-th class, the p predictors are independent (i.e,, for \\(k = 1,\\dots, K\\) \\[ f_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\dots \\times f_{kp}(x_p) \\] where \\(f_{kj}\\) is the density function of the j-th predictor among observation in the k-th class. This assumption allows the use of joint distribution without the need to account for dependence between observations. However, this (native) assumption can be unrealistic, but still works well in cases where the number of sample (n) is not large relative to the number of features (p). With this assumption, we have \\[ P(Y=k|X=x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K \\pi_l \\times f_{l1}(x_1)\\times \\dots f_{lp}(x_p)} \\] we only need to estimate the one-dimensional density function \\(f_{kj}\\) with either of these approaches: When \\(X_j\\) is quantitative, assume it has a univariate normal distribution (with independence): \\(X_j | Y = k \\sim N(\\mu_{jk}, \\sigma^2_{jk})\\) which is more restrictive than QDA because it assumes predictors are independent (e.g., a diagonal covariance matrix) When \\(X_j\\) is quantitative, use a kernel density estimator Kernel Methods ; which is a smoothed histogram When \\(X_j\\) is qualitative, we count the promotion of training observations for the j-th predictor corresponding to each class. 22.4.1.5 Comparison of Classification Methods Assuming we have K classes and K is the baseline from (James , Witten, Hastie, and Tibshirani book) Comparing the log odds relative to the K class 22.4.1.5.1 Logistic Regression \\[ \\log(\\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \\beta_{k0} + \\sum_{j=1}^p \\beta_{kj}x_j \\] 22.4.1.5.2 LDA \\[ \\log(\\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \\sum_{j=1}^p b_{kj} x_j \\] where \\(a_k\\) and \\(b_{kj}\\) are functions of \\(\\pi_k, \\pi_K, \\mu_k , \\mu_K, \\mathbf{\\Sigma}\\) Similar to logistic regression, LDA assumes the log odds is linear in \\(x\\) Even though they look like having the same form, the parameters in logistic regression are estimated by MLE, where as LDA linear parameters are specified by the prior and normal distributions We expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and logistic regression to perform better when it does not 22.4.1.5.3 QDA \\[ \\log(\\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \\sum_{j=1}^{p}b_{kj}x_{j} + \\sum_{j=1}^p \\sum_{l=1}^p c_{kjl}x_j x_l \\] where \\(a_k, b_{kj}, c_{kjl}\\) are functions \\(\\pi_k , \\pi_K, \\mu_k, \\mu_K ,\\mathbf{\\Sigma}_k, \\mathbf{\\Sigma}_K\\) 22.4.1.5.4 Naive Bayes \\[ \\log (\\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \\sum_{j=1}^p g_{kj} (x_j) \\] where \\(a_k = \\log (\\pi_k / \\pi_K)\\) and \\(g_{kj}(x_j) = \\log(\\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\\) which is the form of generalized additive model 22.4.1.5.5 Summary LDA is a special case of QDA LDA is robust when it comes to high dimensions Any classifier with a linear decision boundary is a special case of naive Bayes with \\(g_{kj}(x_j) = b_{kj} x_j\\), which means LDA is a special case of naive Bayes. LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes assumes independence of the features. Naive bayes is also a special case of LDA with \\(\\mathbf{\\Sigma}\\) restricted to a diagonal matrix with diagonals, \\(\\sigma^2\\) (another notation \\(diag (\\mathbf{\\Sigma})\\) ) assuming \\(f_{kj}(x_j) = N(\\mu_{kj}, \\sigma^2_j)\\) QDA and naive Bayes are not special case of each other. In principal,e naive Bayes can produce a more flexible fit by the choice of \\(g_{kj}(x_j)\\) , but it’s restricted to only purely additive fit, but QDA includes multiplicative terms of the form \\(c_{kjl}x_j x_l\\) None of these methods uniformly dominates the others: the choice of method depends on the true distribution of the predictors in each of the K classes, n and p (i.e., related to the bias-variance tradeoff). Compare to the non-parametric method (KNN) KNN would outperform both LDA and logistic regression when the decision boundary is highly nonlinear, but can’t say which predictors are most important, and requires many observations KNN is also limited in high-dimensions due to the curse of dimensionality Since QDA is a special type of nonlinear decision boundary (quadratic), it can be considered as a compromise between the linear methods and KNN classification. QDA can have fewer training observations than KNN but not as flexible. From simulation: True decision boundary Best performance Linear LDA + Logistic regression Moderately nonlinear QDA + Naive Bayes Highly nonlinear (many training, p is not large) KNN like linear regression, we can also introduce flexibility by including transformed features \\(\\sqrt{X}, X^2, X^3\\) 22.4.2 Probabilities of Misclassification When the distribution are exactly known, we can determine the misclassification probabilities exactly. however, when we need to estimate the population parameters, we have to estimate the probability of misclassification Naive method Plugging the parameters estimates into the form for the misclassification probabilities results to derive at the estimates of the misclassification probability. But this will tend to be optimistic when the number of samples in one or more populations is small. Resubstitution method Use the proportion of the samples from population i that would be allocated to another population as an estimate of the misclassification probability But also optimistic when the number of samples is small Jack-knife estimates: The above two methods use observation to estimate both parameters and also misclassification probabilities based upon the discriminant rule Alternatively, we determine the discriminant rule based upon all of the data except the k-th observation from the j-th population then, determine if the k-th observation would be misclassified under this rule perform this process for all \\(n_j\\) observation in population j . An estimate fo the misclassification probability would be the fraction of \\(n_j\\) observations which were misclassified repeat the process for other \\(i \\neq j\\) populations This method is more reliable than the others, but also computationally intensive Cross-Validation Summary Consider the group-specific densities \\(f_j (\\mathbf{x})\\) for multivariate vector \\(\\mathbf{x}\\). Assume equal misclassifications costs, the Bayes classification probability of \\(\\mathbf{x}\\) belonging to the j-th population is \\[ p(j |\\mathbf{x}) = \\frac{\\pi_j f_j (\\mathbf{x})}{\\sum_{k=1}^h \\pi_k f_k (\\mathbf{x})} \\] \\(j = 1,\\dots, h\\) where there are \\(h\\) possible groups. We then classify into the group for which this probability of membership is largest Alternatively, we can write this in terms of a generalized squared distance formation \\[ D_j^2 (\\mathbf{x}) = d_j^2 (\\mathbf{x})+ g_1(j) + g_2 (j) \\] where \\(d_j^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu}_j)&#39; \\mathbf{V}_j^{-1} (\\mathbf{x} - \\mathbf{\\mu}_j)\\) is the squared Mahalanobis distance from \\(\\mathbf{x}\\) to the centroid of group j, and \\(\\mathbf{V}_j = \\mathbf{S}_j\\) if the within group covariance matrices are not equal \\(\\mathbf{V}_j = \\mathbf{S}_p\\) if a pooled covariance estimate is appropriate and \\[ g_1(j) = \\begin{cases} \\ln |\\mathbf{S}_j| &amp; \\text{within group covariances are not equal} \\\\ 0 &amp; \\text{pooled covariance} \\end{cases} \\] \\[ g_2(j) = \\begin{cases} -2 \\ln \\pi_j &amp; \\text{prior probabilities are not equal} \\\\ 0 &amp; \\text{prior probabilities are equal} \\end{cases} \\] then, the posterior probability of belonging to group j is \\[ p(j| \\mathbf{x}) = \\frac{\\exp(-.5 D_j^2(\\mathbf{x}))}{\\sum_{k=1}^h \\exp(-.5 D^2_k (\\mathbf{x}))} \\] where \\(j = 1,\\dots , h\\) and \\(\\mathbf{x}\\) is classified into group j if \\(p(j | \\mathbf{x})\\) is largest for \\(j = 1,\\dots,h\\) (or, \\(D_j^2(\\mathbf{x})\\) is smallest). 22.4.2.1 Assessing Classification Performance For binary classification, confusion matrix Predicted class - or Null + or Null Total True Class - or Null True Neg (TN) False Pos (FP) N + or Null False Neg (FN) True Pos (TP) P Total N* P* and table 4.6 from (James et al. 2013) Name Definition Synonyms False Pos rate FP/N Type I error, 1 0 Specificity True Pos. rate TP/P 1 - Type II error, power, sensitivity, recall Pos Pred. value TP/P* Precision, 1 - false discovery promotion Neg. Pred. value TN/N* ROC curve (receiver Operating Characteristics) is a graphical comparison between sensitivity (true positive) and specificity ( = 1 - false positive) y-axis = true positive rate x-axis = false positive rate as we change the threshold rate for classifying an observation as from 0 to 1 AUC (area under the ROC) ideally would equal to 1, a bad classifier would have AUC = 0.5 (pure chance) 22.4.3 Unknown Populations/ Nonparametric Discrimination When your multivariate data are not Gaussian, or known distributional form at all, we can use the following methods 22.4.3.1 Kernel Methods We approximate \\(f_j (\\mathbf{x})\\) by a kernel density estimate \\[ \\hat{f}_j(\\mathbf{x}) = \\frac{1}{n_j} \\sum_{i = 1}^{n_j} K_j (\\mathbf{x} - \\mathbf{x}_i) \\] where \\(K_j (.)\\) is a kernel function satisfying \\(\\int K_j(\\mathbf{z})d\\mathbf{z} =1\\) \\(\\mathbf{x}_i\\) , \\(i = 1,\\dots , n_j\\) is a random sample from the j-th population. Thus, after finding \\(\\hat{f}_j (\\mathbf{x})\\) for each of the \\(h\\) populations, the posterior probability of group membership is \\[ p(j |\\mathbf{x}) = \\frac{\\pi_j \\hat{f}_j (\\mathbf{x})}{\\sum_{k-1}^h \\pi_k \\hat{f}_k (\\mathbf{x})} \\] where \\(j = 1,\\dots, h\\) There are different choices for the kernel function: Uniform Normal Epanechnikov Biweight Triweight We these kernels, we have to pick the “radius” (or variance, width, window width, bandwidth) of the kernel, which is a smoothing parameter (the larger the radius, the more smooth the kernel estimate of the density). To select the smoothness parameter, we can use the following method If we believe the populations were close to multivariate normal, then \\[ R = (\\frac{4/(2p+1)}{n_j})^{1/(p+1} \\] But since we do not know for sure, we might choose several different values and select one that vies the best out of sample or cross-validation discrimination. Moreover, you also have to decide whether to use different kernel smoothness for different populations, which is similar to the individual and pooled covariances in the classical methodology. 22.4.3.2 Nearest Neighbor Methods The nearest neighbor (also known as k-nearest neighbor) method performs the classification of a new observation vector based on the group membership of its nearest neighbors. In practice, we find \\[ d_{ij}^2 (\\mathbf{x}, \\mathbf{x}_i) = (\\mathbf{x}, \\mathbf{x}_i) V_j^{-1}(\\mathbf{x}, \\mathbf{x}_i) \\] which is the distance between the vector \\(\\mathbf{x}\\) and the \\(i\\)-th observation in group \\(j\\) We consider different choices for \\(\\mathbf{V}_j\\) For example, \\[ \\begin{aligned} \\mathbf{V}_j &amp;= \\mathbf{S}_p \\\\ \\mathbf{V}_j &amp;= \\mathbf{S}_j \\\\ \\mathbf{V}_j &amp;= \\mathbf{I} \\\\ \\mathbf{V}_j &amp;= diag (\\mathbf{S}_p) \\end{aligned} \\] We find the \\(k\\) observations that are closest to \\(\\mathbf{x}\\) (where users pick \\(k\\)). Then we classify into the most common population, weighted by the prior. 22.4.3.3 Modern Discriminant Methods Note: Logistic regression (with or without random effects) is a flexible model-based procedure for classification between two populations. The extension of logistic regression to the multi-group setting is polychotomous logistic regression (or, mulinomial regression). The machine learning and pattern recognition are growing with strong focus on nonlinear discriminant analysis methods such as: radial basis function networks support vector machines multiplayer perceptrons (neural networks) The general framework \\[ g_j (\\mathbf{x}) = \\sum_{l = 1}^m w_{jl}\\phi_l (\\mathbf{x}; \\mathbf{\\theta}_l) + w_{j0} \\] where \\(j = 1,\\dots, h\\) \\(m\\) nonlinear basis functions \\(\\phi_l\\), each of which has \\(n_m\\) parameters given by \\(\\theta_l = \\{ \\theta_{lk}: k = 1, \\dots , n_m \\}\\) We assign \\(\\mathbf{x}\\) to the \\(j\\)-th population if \\(g_j(\\mathbf{x})\\) is the maximum for all \\(j = 1,\\dots, h\\) Development usually focuses on the choice and estimation of the basis functions, \\(\\phi_l\\) and the estimation of the weights \\(w_{jl}\\) More details can be found (Webb, Copsey, and Cawley 2011) 22.4.4 Application library(class) library(klaR) library(MASS) library(tidyverse) ## Read in the data crops &lt;- read.table(&quot;images/crops.txt&quot;) names(crops) &lt;- c(&quot;crop&quot;, &quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;) str(crops) #&gt; &#39;data.frame&#39;: 36 obs. of 5 variables: #&gt; $ crop: chr &quot;Corn&quot; &quot;Corn&quot; &quot;Corn&quot; &quot;Corn&quot; ... #&gt; $ y1 : int 16 15 16 18 15 15 12 20 24 21 ... #&gt; $ y2 : int 27 23 27 20 15 32 15 23 24 25 ... #&gt; $ y3 : int 31 30 27 25 31 32 16 23 25 23 ... #&gt; $ y4 : int 33 30 26 23 32 15 73 25 32 24 ... ## Read in test data crops_test &lt;- read.table(&quot;images/crops_test.txt&quot;) names(crops_test) &lt;- c(&quot;crop&quot;, &quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;) str(crops_test) #&gt; &#39;data.frame&#39;: 5 obs. of 5 variables: #&gt; $ crop: chr &quot;Corn&quot; &quot;Soybeans&quot; &quot;Cotton&quot; &quot;Sugarbeets&quot; ... #&gt; $ y1 : int 16 21 29 54 32 #&gt; $ y2 : int 27 25 24 23 32 #&gt; $ y3 : int 31 23 26 21 62 #&gt; $ y4 : int 33 24 28 54 16 22.4.4.1 LDA Default prior is proportional to sample size and lda and qda do not fit a constant or intercept term ## Linear discriminant analysis lda_mod &lt;- lda(crop ~ y1 + y2 + y3 + y4, data = crops) lda_mod #&gt; Call: #&gt; lda(crop ~ y1 + y2 + y3 + y4, data = crops) #&gt; #&gt; Prior probabilities of groups: #&gt; Clover Corn Cotton Soybeans Sugarbeets #&gt; 0.3055556 0.1944444 0.1666667 0.1666667 0.1666667 #&gt; #&gt; Group means: #&gt; y1 y2 y3 y4 #&gt; Clover 46.36364 32.63636 34.18182 36.63636 #&gt; Corn 15.28571 22.71429 27.42857 33.14286 #&gt; Cotton 34.50000 32.66667 35.00000 39.16667 #&gt; Soybeans 21.00000 27.00000 23.50000 29.66667 #&gt; Sugarbeets 31.00000 32.16667 20.00000 40.50000 #&gt; #&gt; Coefficients of linear discriminants: #&gt; LD1 LD2 LD3 LD4 #&gt; y1 -6.147360e-02 0.009215431 -0.02987075 -0.014680566 #&gt; y2 -2.548964e-02 0.042838972 0.04631489 0.054842132 #&gt; y3 1.642126e-02 -0.079471595 0.01971222 0.008938745 #&gt; y4 5.143616e-05 -0.013917423 0.05381787 -0.025717667 #&gt; #&gt; Proportion of trace: #&gt; LD1 LD2 LD3 LD4 #&gt; 0.7364 0.1985 0.0576 0.0075 ## Look at accuracy on the training data lda_fitted &lt;- predict(lda_mod,newdata = crops) # Contingency table lda_table &lt;- table(truth = crops$crop, fitted = lda_fitted$class) lda_table #&gt; fitted #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 6 0 3 0 2 #&gt; Corn 0 6 0 1 0 #&gt; Cotton 3 0 1 2 0 #&gt; Soybeans 0 1 1 3 1 #&gt; Sugarbeets 1 1 0 2 2 # accuracy of 0.5 is just random (not good) ## Posterior probabilities of membership crops_post &lt;- cbind.data.frame(crops, crop_pred = lda_fitted$class, lda_fitted$posterior) crops_post &lt;- crops_post %&gt;% mutate(missed = crop != crop_pred) head(crops_post) #&gt; crop y1 y2 y3 y4 crop_pred Clover Corn Cotton Soybeans #&gt; 1 Corn 16 27 31 33 Corn 0.08935164 0.4054296 0.1763189 0.2391845 #&gt; 2 Corn 15 23 30 30 Corn 0.07690181 0.4558027 0.1420920 0.2530101 #&gt; 3 Corn 16 27 27 26 Corn 0.09817815 0.3422454 0.1365315 0.3073105 #&gt; 4 Corn 18 20 25 23 Corn 0.10521511 0.3633673 0.1078076 0.3281477 #&gt; 5 Corn 15 15 31 32 Corn 0.05879921 0.5753907 0.1173332 0.2086696 #&gt; 6 Corn 15 32 32 15 Soybeans 0.09723648 0.3278382 0.1318370 0.3419924 #&gt; Sugarbeets missed #&gt; 1 0.08971545 FALSE #&gt; 2 0.07219340 FALSE #&gt; 3 0.11573442 FALSE #&gt; 4 0.09546233 FALSE #&gt; 5 0.03980738 FALSE #&gt; 6 0.10109590 TRUE # posterior shows that posterior of corn membership is much higher than the prior ## LOOCV # leave-one-out cross validation for linear discriminant analysis # cannot run the predict function using the object with CV = TRUE # because it returns the within sample predictions lda_cv &lt;- lda(crop ~ y1 + y2 + y3 + y4, data = crops, CV = TRUE) # Contingency table lda_table_cv &lt;- table(truth = crops$crop, fitted = lda_cv$class) lda_table_cv #&gt; fitted #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 4 3 1 0 3 #&gt; Corn 0 4 1 2 0 #&gt; Cotton 3 0 0 2 1 #&gt; Soybeans 0 1 1 3 1 #&gt; Sugarbeets 2 1 0 2 1 ## Predict the test data lda_pred &lt;- predict(lda_mod, newdata = crops_test) ## Make a contingency table with truth and most likely class table(truth=crops_test$crop, predict=lda_pred$class) #&gt; predict #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 0 0 1 0 0 #&gt; Corn 0 1 0 0 0 #&gt; Cotton 0 0 0 1 0 #&gt; Soybeans 0 0 0 1 0 #&gt; Sugarbeets 1 0 0 0 0 LDA didn’t do well on both within sample and out-of-sample data. 22.4.4.2 QDA ## Quadratic discriminant analysis qda_mod &lt;- qda(crop ~ y1 + y2 + y3 + y4, data = crops) ## Look at accuracy on the training data qda_fitted &lt;- predict(qda_mod, newdata = crops) # Contingency table qda_table &lt;- table(truth = crops$crop, fitted = qda_fitted$class) qda_table #&gt; fitted #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 9 0 0 0 2 #&gt; Corn 0 7 0 0 0 #&gt; Cotton 0 0 6 0 0 #&gt; Soybeans 0 0 0 6 0 #&gt; Sugarbeets 0 0 1 1 4 ## LOOCV qda_cv &lt;- qda(crop ~ y1 + y2 + y3 + y4, data = crops, CV = TRUE) # Contingency table qda_table_cv &lt;- table(truth = crops$crop, fitted = qda_cv$class) qda_table_cv #&gt; fitted #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 9 0 0 0 2 #&gt; Corn 3 2 0 0 2 #&gt; Cotton 3 0 2 0 1 #&gt; Soybeans 3 0 0 2 1 #&gt; Sugarbeets 3 0 1 1 1 ## Predict the test data qda_pred &lt;- predict(qda_mod, newdata = crops_test) ## Make a contingency table with truth and most likely class table(truth = crops_test$crop, predict = qda_pred$class) #&gt; predict #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 1 0 0 0 0 #&gt; Corn 0 1 0 0 0 #&gt; Cotton 0 0 1 0 0 #&gt; Soybeans 0 0 0 1 0 #&gt; Sugarbeets 0 0 0 0 1 22.4.4.3 KNN knn uses design matrices of the features. ## Design matrices X_train &lt;- crops %&gt;% dplyr::select(-crop) X_test &lt;- crops_test %&gt;% dplyr::select(-crop) Y_train &lt;- crops$crop Y_test &lt;- crops_test$crop ## Nearest neighbors with 2 neighbors knn_2 &lt;- knn(X_train, X_train, Y_train, k = 2) table(truth = Y_train, fitted = knn_2) #&gt; fitted #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 7 0 2 1 1 #&gt; Corn 0 7 0 0 0 #&gt; Cotton 0 0 4 0 2 #&gt; Soybeans 0 0 0 4 2 #&gt; Sugarbeets 1 0 2 0 3 ## Accuracy mean(Y_train==knn_2) #&gt; [1] 0.6944444 ## Performance on test data knn_2_test &lt;- knn(X_train, X_test, Y_train, k = 2) table(truth = Y_test, predict = knn_2_test) #&gt; predict #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 1 0 0 0 0 #&gt; Corn 0 1 0 0 0 #&gt; Cotton 0 0 0 0 1 #&gt; Soybeans 0 0 0 1 0 #&gt; Sugarbeets 0 0 0 0 1 ## Accuracy mean(Y_test==knn_2_test) #&gt; [1] 0.8 ## Nearest neighbors with 3 neighbors knn_3 &lt;- knn(X_train, X_train, Y_train, k = 3) table(truth = Y_train, fitted = knn_3) #&gt; fitted #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 8 0 1 1 1 #&gt; Corn 0 4 1 2 0 #&gt; Cotton 1 1 3 0 1 #&gt; Soybeans 0 1 1 4 0 #&gt; Sugarbeets 0 0 0 2 4 ## Accuracy mean(Y_train==knn_3) #&gt; [1] 0.6388889 ## Performance on test data knn_3_test &lt;- knn(X_train, X_test, Y_train, k = 3) table(truth = Y_test, predict = knn_3_test) #&gt; predict #&gt; truth Clover Corn Cotton Soybeans Sugarbeets #&gt; Clover 1 0 0 0 0 #&gt; Corn 0 1 0 0 0 #&gt; Cotton 0 0 1 0 0 #&gt; Soybeans 0 0 0 1 0 #&gt; Sugarbeets 0 0 0 0 1 ## Accuracy mean(Y_test==knn_3_test) #&gt; [1] 1 22.4.4.4 Stepwise Stepwise discriminant analysis using the stepclass in function in the klaR package. step &lt;- stepclass( crop ~ y1 + y2 + y3 + y4, data = crops, method = &quot;qda&quot;, improvement = 0.15 ) #&gt; correctness rate: 0.45; in: &quot;y1&quot;; variables (1): y1 #&gt; #&gt; hr.elapsed min.elapsed sec.elapsed #&gt; 0.00 0.00 0.16 step$process #&gt; step var varname result.pm #&gt; 0 start 0 -- 0.00 #&gt; 1 in 1 y1 0.45 step$performance.measure #&gt; [1] &quot;correctness rate&quot; Iris Data library(dplyr) data(&#39;iris&#39;) set.seed(1) samp &lt;- sample.int(nrow(iris), size = floor(0.70 * nrow(iris)), replace = F) train.iris &lt;- iris[samp,] %&gt;% mutate_if(is.numeric,scale) test.iris &lt;- iris[-samp,] %&gt;% mutate_if(is.numeric,scale) library(ggplot2) iris.model &lt;- lda(Species ~ ., data = train.iris) #pred pred.lda &lt;- predict(iris.model, test.iris) table(truth = test.iris$Species, prediction = pred.lda$class) #&gt; prediction #&gt; truth setosa versicolor virginica #&gt; setosa 15 0 0 #&gt; versicolor 0 17 0 #&gt; virginica 0 0 13 plot(iris.model) iris.model.qda &lt;- qda(Species~.,data=train.iris) #pred pred.qda &lt;- predict(iris.model.qda,test.iris) table(truth=test.iris$Species,prediction=pred.qda$class) #&gt; prediction #&gt; truth setosa versicolor virginica #&gt; setosa 15 0 0 #&gt; versicolor 0 16 1 #&gt; virginica 0 0 13 22.4.4.5 PCA with Discriminant Analysis we can use both PCA for dimension reduction in discriminant analysis zeros &lt;- as.matrix(read.table(&quot;images/mnist0_train_b.txt&quot;)) nines &lt;- as.matrix(read.table(&quot;images/mnist9_train_b.txt&quot;)) train &lt;- rbind(zeros[1:1000, ], nines[1:1000, ]) train &lt;- train / 255 #divide by 255 per notes (so ranges from 0 to 1) train &lt;- t(train) #each column is an observation image(matrix(train[, 1], nrow = 28), main = &#39;Example image, unrotated&#39;) test &lt;- rbind(zeros[2501:3000, ], nines[2501:3000, ]) test &lt;- test / 255 test &lt;- t(test) y.train &lt;- c(rep(0, 1000), rep(9, 1000)) y.test &lt;- c(rep(0, 500), rep(9, 500)) library(MASS) pc &lt;- prcomp(t(train)) train.large &lt;- data.frame(cbind(y.train, pc$x[, 1:10])) large &lt;- lda(y.train ~ ., data = train.large) #the test data set needs to be constucted w/ the same 10 princomps test.large &lt;- data.frame(cbind(y.test, predict(pc, t(test))[, 1:10])) pred.lda &lt;- predict(large, test.large) table(truth = test.large$y.test, prediction = pred.lda$class) #&gt; prediction #&gt; truth 0 9 #&gt; 0 491 9 #&gt; 9 5 495 large.qda &lt;- qda(y.train~.,data=train.large) #prediction pred.qda &lt;- predict(large.qda,test.large) table(truth=test.large$y.test,prediction=pred.qda$class) #&gt; prediction #&gt; truth 0 9 #&gt; 0 493 7 #&gt; 9 3 497 References "],["quasi-experimental.html", "Chapter 23 Quasi-experimental", " Chapter 23 Quasi-experimental In most cases, it means that you have pre- and post-intervention data. Great resources for causal inference include Causal Inference Mixtape and Recent Advances in Micro, especially if you like to read about the history of causal inference as a field as well (codes for Stata, R, and Python). Libraries in R: Econometrics Causal Inference Identification strategy for any quasi-experiment (No ways to prove or formal statistical test, but you can provide plausible argument and evidence) Where the exogenous variation comes from (by argument and institutional knowledge) Exclusion restriction: Evidence that the variation in the exogenous shock and the outcome is due to no other factors The stable unit treatment value assumption (SUTVA) states that the treatment of unit \\(i\\) affect only the outcome of unit \\(i\\) (i.e., no spillover to the control groups) All quasi-experimental methods involve a tradeoff between power and support for the exogeneity assumption (i.e., discard variation in the data that is not exogenous). Consequently, we don’t usually look at \\(R^2\\) (Ebbes, Papies, and Van Heerde 2011). And it can even be misleading to use \\(R^2\\) as the basis for model comparison. Clustering should be based on the design, not the expectations of correlation (Abadie et al. 2023). With a small sample, you should use the wild bootstrap procedure (Cameron, Gelbach, and Miller 2008) to correct for the downward bias (see (Cai et al. 2022)for additional assumptions). Typical robustness check: recommended by (Goldfarb, Tucker, and Wang 2022) Different controls: show models with and without controls. Typically, we want to see the change in the estimate of interest. See (Altonji, Elder, and Taber 2005) for a formal assessment based on Rosenbaum bounds (i.e., changes in the estimate and threat of Omitted variables on the estimate). For specific applications in marketing, see (Manchanda, Packard, and Pattabhiramaiah 2015) (Shin, Sudhir, and Yoon 2012) Different functional forms Different window of time (in longitudinal setting) Different dependent variables (those that are related) or different measures of the dependent variables Different control group size (matched vs. un-matched samples) Placebo tests: see each placebo test for each setting below. Showing the mechanism: Mediation analysis Moderation analysis Estimate the model separately (for different groups) Assess whether the three-way interaction between the source of variation (e.g., under DID, cross-sectional and time series) and group membership is significant. External Validity: Assess how representative your sample is Explain the limitation of the design. Use quasi-experimental results in conjunction with structural models: see (J. E. Anderson, Larch, and Yotov 2015; Einav, Finkelstein, and Levin 2010; Chung, Steenburgh, and Sudhir 2014) Limitation What is your identifying assumptions or identification strategy What are threats to the validity of your assumptions? What you do to address it? And maybe how future research can do to address it. References "],["natural-experiments.html", "23.1 Natural Experiments", " 23.1 Natural Experiments Reusing the same natural experiments for research, particularly when employing identical methods to determine the treatment effect in a given setting, can pose problems for hypothesis testing. Simulations show that when \\(N_{\\text{Outcome}} &gt;&gt; N_{\\text{True effect}}\\), more than 50% of statistically significant findings may be false positives (Heath et al. 2023, 2331). Solutions: Bonferroni correction Romano and Wolf (2005) and Romano and Wolf (2016) correction: recommended Benjamini and Yekutieli (2001) correction Alternatively, refer to the rules of thumb from Table AI (Heath et al. 2023, 2356). When applying multiple testing corrections, we can either use (but they will give similar results anyway (Heath et al. 2023, 2335)): Chronological Sequencing: Outcomes are ordered by the date they were first reported, with multiple testing corrections applied in this sequence. This method progressively raises the statistical significance threshold as more outcomes are reviewed over time. Best Foot Forward Policy: Outcomes are ordered from most to least likely to be rejected based on experimental data. Used primarily in clinical trials, this approach gives priority to intended treatment effects, which are subjected to less stringent statistical requirements. New outcomes are added to the sequence as they are linked to the primary treatment effect. # Romano-Wolf correction library(fixest) library(wildrwolf) head(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa fit1 &lt;- feols(Sepal.Width ~ Sepal.Length , data = iris) fit2 &lt;- feols(Petal.Length ~ Sepal.Length, data = iris) fit3 &lt;- feols(Petal.Width ~ Sepal.Length, data = iris) res &lt;- rwolf( models = list(fit1, fit2, fit3), param = &quot;Sepal.Length&quot;, B = 500 ) #&gt; | | | 0% | |======================= | 33% | |=============================================== | 67% | |======================================================================| 100% res #&gt; model Estimate Std. Error t value Pr(&gt;|t|) RW Pr(&gt;|t|) #&gt; 1 1 -0.0618848 0.04296699 -1.440287 0.1518983 0.139720559 #&gt; 2 2 1.858433 0.08585565 21.64602 1.038667e-47 0.001996008 #&gt; 3 3 0.7529176 0.04353017 17.29645 2.325498e-37 0.001996008 For all other tests, one can use multtest::mt.rawp2adjp which includes: Bonferroni Holm (1979) Šidák (1967) Hochberg (1988) Benjamini and Hochberg (1995) Benjamini and Yekutieli (2001) Adaptive Benjamini and Hochberg (2000) Two-stage Benjamini, Krieger, and Yekutieli (2006) Permutation adjusted p-values for simple multiple testing procedures # BiocManager::install(&quot;multtest&quot;) library(multtest) procs &lt;- c(&quot;Bonferroni&quot;, &quot;Holm&quot;, &quot;Hochberg&quot;, &quot;SidakSS&quot;, &quot;SidakSD&quot;, &quot;BH&quot;, &quot;BY&quot;, &quot;ABH&quot;, &quot;TSBH&quot;) mt.rawp2adjp( # p-values runif(10), procs) |&gt; causalverse::nice_tab() #&gt; adjp.rawp adjp.Bonferroni adjp.Holm adjp.Hochberg adjp.SidakSS adjp.SidakSD #&gt; 1 0.12 1 1 0.75 0.72 0.72 #&gt; 2 0.22 1 1 0.75 0.92 0.89 #&gt; 3 0.24 1 1 0.75 0.94 0.89 #&gt; 4 0.29 1 1 0.75 0.97 0.91 #&gt; 5 0.36 1 1 0.75 0.99 0.93 #&gt; 6 0.38 1 1 0.75 0.99 0.93 #&gt; 7 0.44 1 1 0.75 1.00 0.93 #&gt; 8 0.59 1 1 0.75 1.00 0.93 #&gt; 9 0.65 1 1 0.75 1.00 0.93 #&gt; 10 0.75 1 1 0.75 1.00 0.93 #&gt; adjp.BH adjp.BY adjp.ABH adjp.TSBH_0.05 index h0.ABH h0.TSBH #&gt; 1 0.63 1 0.63 0.63 2 10 10 #&gt; 2 0.63 1 0.63 0.63 6 10 10 #&gt; 3 0.63 1 0.63 0.63 8 10 10 #&gt; 4 0.63 1 0.63 0.63 3 10 10 #&gt; 5 0.63 1 0.63 0.63 10 10 10 #&gt; 6 0.63 1 0.63 0.63 1 10 10 #&gt; 7 0.63 1 0.63 0.63 7 10 10 #&gt; 8 0.72 1 0.72 0.72 9 10 10 #&gt; 9 0.72 1 0.72 0.72 5 10 10 #&gt; 10 0.75 1 0.75 0.75 4 10 10 References "],["regression-discontinuity.html", "Chapter 24 Regression Discontinuity", " Chapter 24 Regression Discontinuity A regression discontinuity occurs when there is a discrete change (jump) in treatment likelihood in the distribution of a continuous (or roughly continuous) variable (i.e., running/forcing/assignment variable). Running variable can also be time, but the argument for time to be continuous is hard to argue because usually we do not see increment of time (e.g., quarterly or annual data). Unless we have minute or hour data, then we might be able to argue for it. Review paper (G. Imbens and Lemieux 2008; Lee and Lemieux 2010) Other readings: https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdf https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf (Thistlethwaite and Campbell 1960): first paper to use RD in the context of merit awards on future academic outcomes. RD is a localized experiment at the cutoff point Hence, we always have to qualify (perfunctory) our statement in research articles that “our research might not generalize to beyond the bandwidth.” In reality, RD and experimental (from random assignment) estimates are very similar ((Chaplin et al. 2018); Mathematica). But still, it’s hard to prove empirically for every context (there might be future study that finds a huge difference between local estimate - causal - and overall estimate - random assignment. Threats: only valid near threshold: inference at threshold is valid on average. Interestingly, random experiment showed the validity already. Tradeoff between efficiency and bias Regression discontinuity is under the framework of Instrumental Variable (structural IV) argued by (J. D. Angrist and Lavy 1999) and a special case of the Matching Methods (matching at one point) argued by (James J. Heckman, LaLonde, and Smith 1999). The hard part is to find a setting that can apply, but once you find one, it’s easy to apply We can also have multiple cutoff lines. However, for each cutoff line, there can only be one breakup point RD can have multiple coinciding effects (i.e., joint distribution or bundled treatment), then RD effect in this case would be the joint effect. As the running variable becomes more discrete your framework should be Interrupted Time Series, but for more granular levels you can use RD. When you have infinite data (or substantially large) the two frameworks are identical. RD is always better than Interrupted Time Series Multiple alternative model specifications that produce consistent results are more reliable (parametric - linear regression with polynomials terms, and non-parametric - local linear regression). This is according to (Lee and Lemieux 2010), one straightforward method to ease the linearity assumption is by incorporating polynomial functions of the forcing variable. The choice of polynomial terms can be determined based on the data. . According to (Gelman and Imbens 2019), accounting for global high-order polynomials presents three issues: (1) imprecise estimates due to noise, (2) sensitivity to the polynomial’s degree, and (3) inadequate coverage of confidence intervals. To address this, researchers should instead employ estimators that rely on local linear or quadratic polynomials or other smooth functions. RD should be viewed more as a description of a data generating process, rather than a method or approach (similar to a randomized experiment) RD is close to other quasi-experimental methods in the sense that it’s based on the discontinuity at a threshold randomized experiments in the sense that it’s local randomization. There are several types of Regression Discontinuity: Sharp RD: Change in treatment probability at the cutoff point is 1 Kink design: Instead of a discontinuity in the level of running variable, we have a discontinuity in the slope of the function (while the function/level can remain continuous) (Nielsen, Sørensen, and Taber 2010). See (Böckerman, Kanninen, and Suoniemi 2018) for application, and (Card et al. 2015) for theory. Kink RD Fuzzy RD: Change in treatment probability less than 1 Fuzzy Kink RD RDiT: running variable is time. Others: Multiple cutoff Multiple Scores Geographic RD Dynamic Treatments Continuous Treatments Consider \\[ D_i = 1_{X_i &gt; c} \\] \\[ D_i = \\begin{cases} D_i = 1 \\text{ if } X_i &gt; C \\\\ D_i = 0 \\text{ if } X_i &lt; C \\end{cases} \\] where \\(D_i\\) = treatment effect \\(X_i\\) = score variable (continuous) \\(c\\) = cutoff point Identification (Identifying assumptions) of RD: Average Treatment Effect at the cutoff (Continuity-based) \\[ \\begin{aligned} \\alpha_{SRDD} &amp;= E[Y_{1i} - Y_{0i} | X_i = c] \\\\ &amp;= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c]\\\\ &amp;= \\lim_{x \\to c^+} E[Y_{1i}|X_i = c] - \\lim_{x \\to c^=} E[Y_{0i}|X_i = c] \\end{aligned} \\] Average Treatment Effect in a neighborhood (Local Randomization-based): \\[ \\begin{aligned} \\alpha_{LR} &amp;= E[Y_{1i} - Y_{0i}|X_i \\in W] \\\\ &amp;= \\frac{1}{N_1} \\sum_{X_i \\in W, T_i = 1}Y_i - \\frac{1}{N_0}\\sum_{X_i \\in W, T_i =0} Y_i \\end{aligned} \\] RDD estimates the local average treatment effect (LATE), at the cutoff point which is not at the individual or population levels. Since researchers typically care more about the internal validity, than external validity, localness affects only external validity. Assumptions: Independent assignment Continuity of conditional regression functions \\(E[Y(0)|X=x]\\) and \\(E[Y(1)|X=x]\\) are continuous in x. RD is valid if cutpoint is exogenous (i.e., no endogenous selection) and running variable is not manipulable Only treatment(s) (e.g., could be joint distribution of multiple treatments) cause discontinuity or jump in the outcome variable All other factors are smooth through the cutoff (i.e., threshold) value. (we can also test this assumption by seeing no discontinuity in other factors). If they “jump”, they will bias your causal estimate Threats to RD Variables (other than treatment) change discontinuously at the cutoff We can test for jumps in these variables (including pre-treatment outcome) Multiple discontinuities for the assignment variable Manipulation of the assignment variable At the cutoff point, check for continuity in the density of the assignment variable. References "],["estimation-and-inference.html", "24.1 Estimation and Inference", " 24.1 Estimation and Inference 24.1.1 Local Randomization-based Additional Assumption: Local Randomization approach assumes that inside the chosen window \\(W = [c-w, c+w]\\) are assigned to treatment as good as random: Joint probability distribution of scores for units inside the chosen window \\(W\\) is known Potential outcomes are not affected by value of the score This approach is stronger than the Continuity-based because we assume the regressions are continuously at \\(c\\) and unaffected by the running variable within window \\(W\\) Because we can choose the window \\(W\\) (within which random assignment is plausible), the sample size can typically be small. To choose the window \\(W\\), we can base on either where the pre-treatment covariate-balance is observed independent tests between outcome and score domain knowledge To make inference, we can either use (Fisher) randomization inference (Neyman) design-based 24.1.2 Continuity-based also known as the local polynomial method as the name suggests, global polynomial regression is not recommended (because of lack of robustness, and over-fitting and Runge’s phenomenon) Step to estimate local polynomial regression Choose polynomial order and weighting scheme Choose bandwidth that has optimal MSE or coverage error Estimate the parameter of interest Examine robust bias-correct inference "],["specification-checks.html", "24.2 Specification Checks", " 24.2 Specification Checks Balance Checks Sorting/Bunching/Manipulation Placebo Tests Sensitivity to Bandwidth Choice 24.2.1 Balance Checks Also known as checking for Discontinuities in Average Covariates Null Hypothesis: The average effect of covariates on pseudo outcomes (i.e., those qualitatively cannot be affected by the treatment) is 0. If this hypothesis is rejected, you better have a good reason to why because it can cast serious doubt on your RD design. 24.2.2 Sorting/Bunching/Manipulation Also known as checking for A Discontinuity in the Distribution of the Forcing Variable Also known as clustering or density test Formal test is McCrary sorting test (McCrary 2008) or (Cattaneo, Idrobo, and Titiunik 2019) Since human subjects can manipulate the running variable to be just above or below the cutoff (assuming that the running variable is manipulable), especially when the cutoff point is known in advance for all subjects, this can result in a discontinuity in the distribution of the running variable at the cutoff (i.e., we will see “bunching” behavior right before or after the cutoff)&gt; People would like to sort into treatment if it’s desirable. The density of the running variable would be 0 just below the threshold People would like to be out of treatment if it’s undesirable (McCrary 2008) proposes a density test (i.e., a formal test for manipulation of the assignment variable). \\(H_0\\): The continuity of the density of the running variable (i.e., the covariate that underlies the assignment at the discontinuity point) \\(H_a\\): A jump in the density function at that point Even though it’s not a requirement that the density of the running must be continuous at the cutoff, but a discontinuity can suggest manipulations. (J. L. Zhang and Rubin 2003; Lee 2009; Aronow, Baron, and Pinson 2019) offers a guide to know when you should warrant the manipulation Usually it’s better to know your research design inside out so that you can suspect any manipulation attempts. We would suspect the direction of the manipulation. And typically, it’s one-way manipulation. In cases where we might have both ways, theoretically they would cancel each other out. We could also observe partial manipulation in reality (e.g., when subjects can only imperfectly manipulate). But typically, as we treat it like fuzzy RD, we would not have identification problems. But complete manipulation would lead to serious identification issues. Remember: even in cases where we fail to reject the null hypothesis for the density test, we could not rule out completely that identification problem exists (just like any other hypotheses) Bunching happens when people self-select to a specific value in the range of a variable (e.g., key policy thresholds). Review paper (Kleven 2016) This test can only detect manipulation that changes the distribution of the running variable. If you can choose the cutoff point or you have 2-sided manipulation, this test will fail to detect it. Histogram in bunching is similar to a density curve (we want narrower bins, wider bins bias elasticity estimates) We can also use bunching method to study individuals’ or firm’s responsiveness to changes in policy. Under RD, we assume that we don’t have any manipulation in the running variable. However, bunching behavior is a manipulation by firms or individuals. Thus, violating this assumption. Bunching can fix this problem by estimating what densities of individuals would have been without manipulation (i.e., manipulation-free counterfactual). The fraction of persons who manipulated is then calculated by comparing the observed distribution to manipulation-free counterfactual distributions. Under RD, we do not need this step because the observed and manipulation-free counterfactual distributions are assumed to be the same. RD assume there is no manipulation (i.e., assume the manipulation-free counterfactual distribution) When running variable and outcome variable are simultaneously determined, we can use a modified RDD estimator to have consistent estimate. (Bajari et al. 2011) Assumptions: Manipulation is one-sided: People move one way (i.e., either below the threshold to above the threshold or vice versa, but not to or away the threshold), which is similar to the monotonicity assumption under instrumental variable 33.1.3.1 Manipulation is bounded (also known as regularity assumption): so that we can use people far away from this threshold to derive at our counterfactual distribution [Blomquist et al. (2021)](Bertanha, McCallum, and Seegert 2021) Steps: Identify the window in which the running variable contains bunching behavior. We can do this step empirically based on Bosch, Dekker, and Strohmaier (2020). Additionally robustness test is needed (i.e., varying the manipulation window). Estimate the manipulation-free counterfactual Calculating the standard errors for inference can follow (Chetty, Hendren, and Katz 2016) where we bootstrap re-sampling residuals in the estimation of the counts of individuals within bins (large data can render this step unnecessary). If we pass the bunching test, we can move on to the Placebo Test McCrary (2008) test A jump in the density at the threshold (i.e., discontinuity) hold can serve as evidence for sorting around the cutoff point library(rdd) # you only need the runing variable and the cutoff point # Example by the package&#39;s authors #No discontinuity x&lt;-runif(1000,-1,1) DCdensity(x,0) #&gt; [1] 0.6126802 #Discontinuity x&lt;-runif(1000,-1,1) x&lt;-x+2*(runif(1000,-1,1)&gt;0&amp;x&lt;0) DCdensity(x,0) #&gt; [1] 0.0008519227 Cattaneo, Idrobo, and Titiunik (2019) test library(rddensity) # Example by the package&#39;s authors # Continuous Density set.seed(1) x &lt;- rnorm(2000, mean = -0.5) rdd &lt;- rddensity(X = x, vce = &quot;jackknife&quot;) summary(rdd) #&gt; #&gt; Manipulation testing using local polynomial density estimation. #&gt; #&gt; Number of obs = 2000 #&gt; Model = unrestricted #&gt; Kernel = triangular #&gt; BW method = estimated #&gt; VCE method = jackknife #&gt; #&gt; c = 0 Left of c Right of c #&gt; Number of obs 1376 624 #&gt; Eff. Number of obs 354 345 #&gt; Order est. (p) 2 2 #&gt; Order bias (q) 3 3 #&gt; BW est. (h) 0.514 0.609 #&gt; #&gt; Method T P &gt; |T| #&gt; Robust -0.6798 0.4966 #&gt; #&gt; #&gt; P-values of binomial tests (H0: p=0.5). #&gt; #&gt; Window Length / 2 &lt;c &gt;=c P&gt;|T| #&gt; 0.036 28 20 0.3123 #&gt; 0.072 46 39 0.5154 #&gt; 0.107 68 59 0.4779 #&gt; 0.143 94 79 0.2871 #&gt; 0.179 122 103 0.2301 #&gt; 0.215 145 130 0.3986 #&gt; 0.250 163 156 0.7370 #&gt; 0.286 190 176 0.4969 #&gt; 0.322 214 200 0.5229 #&gt; 0.358 249 218 0.1650 # you have to specify your own plot (read package manual) 24.2.3 Placebo Tests Also known as Discontinuities in Average Outcomes at Other Values We should not see any jumps at other values (either \\(X_i &lt;c\\) or \\(X_i \\ge c\\)) Use the same bandwidth you use for the cutoff, and move it along the running variable: testing for a jump in the conditional mean of the outcome at the median of the running variable. Also known as falsification checks Before and after the cutoff point, we can run the placebo test to see whether X’s are different). The placebo test is where you expect your coefficients to be not different from 0. This test can be used for Testing no discontinuity in predetermined variables: Testing other discontinuities Placebo outcomes: we should see any changes in other outcomes that shouldn’t have changed. Inclusion and exclusion of covariates: RDD parameter estimates should not be sensitive to the inclusion or exclusion of other covariates. This is analogous to Experimental Design where we cannot only test whether the observables are similar in both treatment and control groups (if we reject this, then we don’t have random assignment), but we cannot test unobservables. Balance on observable characteristics on both sides \\[ Z_i = \\alpha_0 + \\alpha_1 f(x_i) + [I(x_i \\ge c)] \\alpha_2 + [f(x_i) \\times I(x_i \\ge c)]\\alpha_3 + u_i \\] where \\(x_i\\) is the running variable \\(Z_i\\) is other characteristics of people (e.g., age, etc) Theoretically, \\(Z_i\\) should no be affected by treatment. Hence, \\(E(\\alpha_2) = 0\\) Moreover, when you have multiple \\(Z_i\\), you typically have to simulate joint distribution (to avoid having significant coefficient based on chance). The only way that you don’t need to generate joint distribution is when all \\(Z_i\\)’s are independent (unlikely in reality). Under RD, you shouldn’t have to do any Matching Methods. Because just like when you have random assignment, there is no need to make balanced dataset before and after the cutoff. If you have to do balancing, then your RD assumptions are probably wrong in the first place. 24.2.4 Sensitivity to Bandwidth Choice Methods for bandwidth selection Ad-hoc or substantively driven Data driven: cross validation Conservative approach: (Calonico, Cattaneo, and Farrell 2020) The objective is to minimize the mean squared error between the estimated and actual treatment effects. Then, we need to see how sensitive our results will be dependent on the choice of bandwidth. In some cases, the best bandwidth for testing covariates may not be the best bandwidth for treating them, but it may be close. # find optimal bandwidth by Imbens-Kalyanaraman rdd::IKbandwidth(running_var, outcome_var, cutpoint = &quot;&quot;, kernel = &quot;triangular&quot;) # can also pick other kernels 24.2.5 Manipulation Robust Regression Discontinuity Bounds McCrary (2008) linked density jumps at cutoffs in RD studies to potential manipulation. If no jump is detected, researchers proceed with RD analysis; if detected, they halt using the cutoff for inference. Some studies use the “doughnut-hole” method, excluding near-cutoff observations and extrapolating, which contradicts RD principles. False negative could be due to a small sample size and can lead to biased estimates, as units near the cutoff may still differ in unobserved ways. Even correct rejections of no manipulation may overlook that the data can still be informative despite modest manipulation. Gerard, Rokkanen, and Rothe (2020) introduces a systematic approach to handle potentially manipulated variables in RD designs, addressing both concerns. The model introduces two types of unobservable units in RD designs: always-assigned units, which are always on one side of the cutoff, potentially-assigned units, which fit traditional RD assumptions. The standard RD model is a subset of this broader model, which assumes no always-assigned units. Identifying assumption: manipulation occurs through one-sided selection. The approach does not make a binary decision on manipulation in RD designs but assesses its extent and worst-case impact. Two steps are used: Determining the proportion of always-assigned units using the discontinuity at the cutoff Bounding treatment effects based on the most extreme feasible outcomes for these units. For sharp RD designs, bounds are established by trimming extreme outcomes near the cutoff; for fuzzy designs, the process involves more complex adjustments due to additional model constraints. Extensions of the study use covariate information and economic behavior assumptions to refine these bounds and identify covariate distributions among unit types at the cutoff. Setup Independent data points \\((X_i, Y_i, D_i)\\), where \\(X_i\\) is the running variable, \\(Y_i\\) is the outcome, and \\(D_i\\) indicates treatment status (1 if treated, 0 otherwise). Treatment is assigned based on \\(X_i \\geq c\\). The design is sharp if \\(D_i = I(X_i \\geq c)\\) and fuzzy otherwise. The population is divided into: Potentially-assigned units (\\(M_i = 0\\)): Follow the standard RD framework, with potential outcomes \\(Y_i(d)\\) and potential treatment states \\(D_i(x)\\). Always-assigned units (\\(M_i = 1\\)): These units do not require potential outcomes or states, and always have \\(X_i\\) values beyond the cutoff. Assumptions Local Independence and Continuity: \\(P(D = 1|X = c^+, M = 0) &gt; P(D = 1|X = c^-, M = 0)\\) No defiers: \\(P(D^+ \\geq D^-|X = c, M = 0) = 1\\) Continuity in potential outcomes and states at \\(c\\). \\(F_{X|M=0}(x)\\) is differentiable at \\(c\\), with a positive derivative. Smoothness of the Running Variable among Potentially-Assigned Units: The derivative of \\(F_{X|M=0}(x)\\) is continuous at \\(c\\). Restrictions on Always-Assigned Units: \\(P(X \\geq c|M = 1) = 1\\) and \\(F_{X|M=1}(x)\\) is right-differentiable (or left-differentiable) at \\(c\\). This (local) one-sided manipulation assumption allows identification of the proportion of always-assigned units among all units close to the cutoff. When always-assigned unit exist, the RD design is fuzzy because we have Treated and untreated units among the potentially-assigned (below and above the cutoff) Always-assigned units (above the cutoff). Causal Effects of Interest causal effects among potentially-assigned units: \\[ \\Gamma = E[Y(1) - Y(0) | X = c, D^+ &gt; D^-, M = 0] \\] This parameter represents the local average treatment effect (LATE) for the subgroup of “compliers”—units that receive treatment if and only if their running variable \\(X_i\\) exceeds a certain cutoff. The parameter \\(\\Gamma\\) captures the causal effect of changes in the cutoff level on treatment status among potentially-assigned compliers. RD designs with a manipulated running variable “Doughnut-Hole” RD Designs: Focuses on actual observations at the cutoff, not hypothetical true values. Provides a direct and observable estimate of causal effects, without reliance on hypothetical constructs. Exclude observations around the cutoff and use extrapolation from the trends outside this excluded range to infer causal effects at the cutoff Assumes a hypothetical population existing in a counterfactual scenario without manipulation. Requires strong assumptions about the nature of manipulation and the minimal impact of extrapolation biases. Identification of \\(\\tau\\) in RD Designs Identification challenges arise due to the inability to distinguish always-assigned from potentially-assigned units, thus Γ is not point identified. We establish sharp bounds on Γ These bounds are supported by the stochastic dominance of the potential outcome CDFs over observed distributions. Unit Types and Notation: \\(C_0\\): Potentially-assigned compliers. \\(A_0\\): Potentially-assigned always-takers. \\(N_0\\): Potentially-assigned never-takers. \\(T_1\\): Always-assigned treated units. \\(U_1\\): Always-assigned untreated units. The measure \\(\\tau\\) , representing the proportion of always-assigned units near the cutoff, is point identified by the discontinuity in the observed running variable density \\(f_X\\) at the cutoff Sharp RD: Units to the left of the cutoff are potentially assigned units. The distribution of their observed outcomes (\\(Y\\)) are the outcomes \\(Y(0)\\) of potentially-assigned compliers (\\(C_0\\)) at the cutoff. To determine the bounds on the treatment effect (\\(\\Gamma\\)), we need to assess the distribution of treated outcomes (\\(Y(1)\\)) for the same potentially-assigned compliers at the cutoff. Information regarding the treated outcomes (\\(Y(1)\\)) comes exclusively from the subpopulation of treated units, which includes both potentially-assigned compliers (\\(C_0\\)) and those always assigned units (\\(T_1\\)). With \\(\\tau\\) point identified, we can estimate sharp bounds on \\(\\Gamma\\). Fuzzy RD: Note: Table on page 848 (Gerard, Rokkanen, and Rothe 2020) Subpopulation Types of units \\(X = c^+, D = 1\\) \\(C_0, A_0, T_1\\) \\(X = c^-, D = 1\\) \\(A_0\\) \\(X= c^+, D = 0\\) \\(N_0, U_1\\) \\(X = c^-, D = 0\\) \\(C_0, N_0\\) Unit Types and Combinations: There are five distinct unit types and four combinations of treatment assignments and decisions relevant to the analysis. These distinctions are important because they affect how potential outcomes are analyzed and bounded. Outcome Distributions: The analysis involves estimating the distribution of potential outcomes (both treated and untreated) among potentially-assigned compliers at the cutoff. Three-Step Process: Potential Outcomes Under Treatment: Bounds on the distribution of treated outcomes are determined using data from treated units. Potential Outcomes Under Non-Treatment: Bounds on the distribution of untreated outcomes are derived using data from untreated units. Bounds on Parameters of Interest: Using the bounds from the first two steps, sharp upper and lower bounds on the local average treatment effect are derived. Extreme Value Consideration: The bounds for treatment effects are based on “extreme” scenarios under worst-case assumptions about the distribution of potential outcomes, making them sharp but empirically relevant within the data constraints. Extensions: Quantile Treatment Effects: alternative to average effects by focusing on different quantiles of the outcome distribution, which are less affected by extreme values. Applicability to Discrete Outcomes Behavioral Assumptions Impact: Assuming a high likelihood of treatment among always-assigned units can narrow the bounds of treatment effects by refining the analysis of potential outcomes. Utilization of Covariates: Incorporating covariates measured prior to treatment can refine the bounds on treatment effects and help target policies by identifying covariate distributions among different unit types. Notes: Quantile Treatment Effects (QTEs): QTE bounds are less sensitive to the tails of the outcome distribution, making them tighter than ATE bounds. Inference on ATEs is sensitive to the extent of manipulation, with confidence intervals widening significantly with small degrees of assumed manipulation. Inference on QTEs is less affected by manipulation, remaining meaningful even with larger degrees of manipulation. Alternative Inference Strategy when manipulation is believed to be unlikely. Try different hypothetical values of \\(\\tau\\) devtools::install_github(&quot;francoisgerard/rdbounds/R&quot;) library(formattable) library(data.table) library(rdbounds) set.seed(123) df &lt;- rdbounds_sampledata(10000, covs = FALSE) #&gt; [1] &quot;True tau: 0.117999815082062&quot; #&gt; [1] &quot;True treatment effect on potentially-assigned: 2&quot; #&gt; [1] &quot;True treatment effect on right side of cutoff: 2.35399944524618&quot; head(df) #&gt; x y treatment #&gt; 1 -1.2532616 3.489563 0 #&gt; 2 -0.5146925 3.365232 0 #&gt; 3 3.4853777 6.193533 0 #&gt; 4 0.1576616 8.820440 1 #&gt; 5 0.2890962 4.791972 0 #&gt; 6 3.8350019 7.316907 0 rdbounds_est &lt;- rdbounds( y = df$y, x = df$x, # covs = as.factor(df$cov), treatment = df$treatment, c = 0, discrete_x = FALSE, discrete_y = FALSE, bwsx = c(.2, .5), bwy = 1, # for median effect use # type = &quot;qte&quot;, # percentiles = .5, kernel = &quot;epanechnikov&quot;, orders = 1, evaluation_ys = seq(from = 0, to = 15, by = 1), refinement_A = TRUE, refinement_B = TRUE, right_effects = TRUE, yextremes = c(0, 15), num_bootstraps = 5 ) #&gt; [1] &quot;The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.04209&quot; #&gt; [1] &quot;2024-05-13 19:12:33 Estimating CDFs for point estimates&quot; #&gt; [1] &quot;2024-05-13 19:12:33 .....Estimating CDFs for units just to the right of the cutoff&quot; #&gt; [1] &quot;2024-05-13 19:12:35 Estimating CDFs with nudged tau (tau_star)&quot; #&gt; [1] &quot;2024-05-13 19:12:35 .....Estimating CDFs for units just to the right of the cutoff&quot; #&gt; [1] &quot;2024-05-13 19:12:38 Beginning parallelized output by bootstrap..&quot; #&gt; [1] &quot;2024-05-13 19:12:42 Computing Confidence Intervals&quot; #&gt; [1] &quot;2024-05-13 19:12:51 Time taken:0.3 minutes&quot; rdbounds_summary(rdbounds_est, title_prefix = &quot;Sample Data Results&quot;) #&gt; [1] &quot;Time taken: 0.3 minutes&quot; #&gt; [1] &quot;Sample size: 10000&quot; #&gt; [1] &quot;Local Average Treatment Effect:&quot; #&gt; $tau_hat #&gt; [1] 0.04209028 #&gt; #&gt; $tau_hat_CI #&gt; [1] 0.1671043 0.7765031 #&gt; #&gt; $takeup_increase #&gt; [1] 0.7521208 #&gt; #&gt; $takeup_increase_CI #&gt; [1] 0.7065353 0.7977063 #&gt; #&gt; $TE_SRD_naive #&gt; [1] 1.770963 #&gt; #&gt; $TE_SRD_naive_CI #&gt; [1] 1.541314 2.000612 #&gt; #&gt; $TE_SRD_bounds #&gt; [1] 1.569194 1.912681 #&gt; #&gt; $TE_SRD_CI #&gt; [1] -0.1188634 3.5319468 #&gt; #&gt; $TE_SRD_covs_bounds #&gt; [1] NA NA #&gt; #&gt; $TE_SRD_covs_CI #&gt; [1] NA NA #&gt; #&gt; $TE_FRD_naive #&gt; [1] 2.356601 #&gt; #&gt; $TE_FRD_naive_CI #&gt; [1] 1.995430 2.717772 #&gt; #&gt; $TE_FRD_bounds #&gt; [1] 1.980883 2.362344 #&gt; #&gt; $TE_FRD_CI #&gt; [1] -0.6950823 4.6112538 #&gt; #&gt; $TE_FRD_bounds_refinementA #&gt; [1] 1.980883 2.357499 #&gt; #&gt; $TE_FRD_refinementA_CI #&gt; [1] -0.6950823 4.6112538 #&gt; #&gt; $TE_FRD_bounds_refinementB #&gt; [1] 1.980883 2.351411 #&gt; #&gt; $TE_FRD_refinementB_CI #&gt; [1] -0.6152215 4.2390830 #&gt; #&gt; $TE_FRD_covs_bounds #&gt; [1] NA NA #&gt; #&gt; $TE_FRD_covs_CI #&gt; [1] NA NA #&gt; #&gt; $TE_SRD_CIs_manipulation #&gt; [1] NA NA #&gt; #&gt; $TE_FRD_CIs_manipulation #&gt; [1] NA NA #&gt; #&gt; $TE_SRD_right_bounds #&gt; [1] 1.376392 2.007746 #&gt; #&gt; $TE_SRD_right_CI #&gt; [1] -5.036752 5.889137 #&gt; #&gt; $TE_FRD_right_bounds #&gt; [1] 1.721121 2.511504 #&gt; #&gt; $TE_FRD_right_CI #&gt; [1] -6.663269 7.414185 rdbounds_est_tau &lt;- rdbounds( y = df$y, x = df$x, # covs = as.factor(df$cov), treatment = df$treatment, c = 0, discrete_x = FALSE, discrete_y = FALSE, bwsx = c(.2, .5), bwy = 1, kernel = &quot;epanechnikov&quot;, orders = 1, evaluation_ys = seq(from = 0, to = 15, by = 1), refinement_A = TRUE, refinement_B = TRUE, right_effects = TRUE, potential_taus = c(.025, .05, .1, .2), yextremes = c(0, 15), num_bootstraps = 5 ) #&gt; [1] &quot;The proportion of always-assigned units just to the right of the cutoff is estimated to be 0.04209&quot; #&gt; [1] &quot;2024-05-13 19:12:52 Estimating CDFs for point estimates&quot; #&gt; [1] &quot;2024-05-13 19:12:52 .....Estimating CDFs for units just to the right of the cutoff&quot; #&gt; [1] &quot;2024-05-13 19:12:53 Estimating CDFs with nudged tau (tau_star)&quot; #&gt; [1] &quot;2024-05-13 19:12:53 .....Estimating CDFs for units just to the right of the cutoff&quot; #&gt; [1] &quot;2024-05-13 19:12:56 Beginning parallelized output by bootstrap..&quot; #&gt; [1] &quot;2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.025&quot; #&gt; [1] &quot;2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.05&quot; #&gt; [1] &quot;2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.1&quot; #&gt; [1] &quot;2024-05-13 19:13:02 Estimating CDFs with fixed tau value of: 0.2&quot; #&gt; [1] &quot;2024-05-13 19:13:03 Beginning parallelized output by bootstrap x fixed tau..&quot; #&gt; [1] &quot;2024-05-13 19:13:09 Computing Confidence Intervals&quot; #&gt; [1] &quot;2024-05-13 19:13:19 Time taken:0.46 minutes&quot; causalverse::plot_rd_aa_share(rdbounds_est_tau) # For SRD (default) # causalverse::plot_rd_aa_share(rdbounds_est_tau, rd_type = &quot;FRD&quot;) # For FRD References "],["fuzzy-rd-design.html", "24.3 Fuzzy RD Design", " 24.3 Fuzzy RD Design When you have cutoff that does not perfectly determine treatment, but creates a discontinuity in the likelihood of receiving the treatment, you need another instrument For those that are close to the cutoff, we create an instrument for \\(D_i\\) \\[ Z_i= \\begin{cases} 1 &amp; \\text{if } X_i \\ge c \\\\ 0 &amp; \\text{if } X_c &lt; c \\end{cases} \\] Then, we can estimate the effect of the treatment for compliers only (i.e., those treatment \\(D_i\\) depends on \\(Z_i\\)) The LATE parameter \\[ \\lim_{c - \\epsilon \\le X \\le c + \\epsilon, \\epsilon \\to 0}( \\frac{E(Y |Z = 1) - E(Y |Z=0)}{E(D|Z = 1) - E(D|Z = 0)}) \\] equivalently, the canonical parameter: \\[ \\frac{lim_{x \\downarrow c}E(Y|X = x) - \\lim_{x \\uparrow c} E(Y|X = x)}{\\lim_{x \\downarrow c } E(D |X = x) - \\lim_{x \\uparrow c}E(D |X=x)} \\] Two equivalent ways to estimate First Sharp RDD for \\(Y\\) Sharp RDD for \\(D\\) Take the estimate from step 1 divide by that of step 2 Second: Subset those observations that are close to \\(c\\) and run instrumental variable \\(Z\\) "],["regression-kink-design.html", "24.4 Regression Kink Design", " 24.4 Regression Kink Design If the slope of the treatment intensity changes at the cutoff (instead of the level of treatment assignment), we can have regression kink design Example: unemployment benefits Sharp Kink RD parameter \\[ \\alpha_{KRD} = \\frac{\\lim_{x \\downarrow c} \\frac{d}{dx}E[Y_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[Y_i |X_i = x]}{\\lim_{x \\downarrow c} \\frac{d}{dx}b(x) - \\lim_{x \\uparrow c} \\frac{d}{dx}b(x)} \\] where \\(b(x)\\) is a known function inducing “kink” Fuzzy Kink RD parameter \\[ \\alpha_{KRD} = \\frac{\\lim_{x \\downarrow c} \\frac{d}{dx}E[Y_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[Y_i |X_i = x]}{\\lim_{x \\downarrow c} \\frac{d}{dx}E[D_i |X_i = x]- \\lim_{x \\uparrow c} \\frac{d}{dx}E[D_i |X_i = x]} \\] "],["multi-cutoff.html", "24.5 Multi-cutoff", " 24.5 Multi-cutoff \\[ \\tau (x,c)= E[Y_{1i} - Y_{0i}|X_i = x, C_i = c] \\] "],["multi-score.html", "24.6 Multi-score", " 24.6 Multi-score Multi-score (in multiple dimensions) (e.g., math and English cutoff for certain honor class): \\[ \\tau (x_1, x_2) = E[Y_{1i} - Y_{0i}|X_{1i} = x_1, X_{2i} = x] \\] "],["steps-for-sharp-rd.html", "24.7 Steps for Sharp RD", " 24.7 Steps for Sharp RD Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear). Run regression on both sides of the cutoff to get the treatment effect Robustness checks: Assess possible jumps in other variables around the cutoff Hypothesis testing for bunching Placebo tests Varying bandwidth "],["steps-for-fuzzy-rd.html", "24.8 Steps for Fuzzy RD", " 24.8 Steps for Fuzzy RD Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear). Graph the probability of treatment Estimate the treatment effect using 2SLS Robustness checks: Assess possible jumps in other variables around the cutoff Hypothesis testing for bunching Placebo tests Varying bandwidth "],["steps-for-rdit-regression-discontinuity-in-time.html", "24.9 Steps for RDiT (Regression Discontinuity in Time)", " 24.9 Steps for RDiT (Regression Discontinuity in Time) Notes: Additional assumption: Time-varying confounders change smoothly across the cutoff date Typically used in policy implementation in the same date for all subjects, but can also be used for cases where implementation dates are different between subjects. In the second case, researchers typically use different RDiT specification for each time series. Sometimes the date of implementation is not randomly assigned by chosen strategically. Hence, RDiT should be thought of as the “discontinuity at a threshold” interpretation of RD (not as “local randomization”). (C. Hausman and Rapson 2018, 8) Normal RD uses variation in the \\(N\\) dimension, while RDiT uses variation in the \\(T\\) dimension Choose polynomials based on BIC typically. And can have either global polynomial or pre-period and post-period polynomial for each time series (but usually the global one will perform better) Could use augmented local linear outlined by (C. Hausman and Rapson 2018, 12), where estimate the model with all the control first then take the residuals to include in the model with the RDiT treatment (remember to use bootstrapping method to account for the first-stage variance in the second stage). Pros: can overcome cases where there is no cross-sectional variation in treatment implementation (DID is not feasible) There are papers that use both RDiT and DID to (1) see the differential treatment effects across individuals/ space (Auffhammer and Kellogg 2011) or (2) compare the 2 estimates where the control group’s validity is questionable (Gallego, Montero, and Salas 2013). Better than pre/post comparison because it can include flexible controls Better than event studies because it can use long-time horizons (may not be too relevant now since the development long-time horizon event studies), and it can use higher-order polynomials time control variables. Cons: Taking observation for from the threshold (in time) can bias your estimates because of unobservables and time-series properties of the data generating process. (McCrary 2008) test is not possible (see Sorting/Bunching/Manipulation) because when the density of the running (time) is uniform, you can’t use the test. Time-varying unobservables may impact the dependent variable discontinuously Error terms are likely to include persistence (serially correlated errors) Researchers cannot model time-varying treatment under RDiT In a small enough window, the local linear specification is fine, but the global polynomials can either be too big or too small (C. Hausman and Rapson 2018) Biases Time-Varying treatment Effects increase sample size either by more granular data (greater frequency): will not increase power because of the problem of serial correlation increasing time window: increases bias from other confounders 2 additional assumption: Model is correctly specified (with all confoudners or global polynomial approximation) Treatment effect is correctly specified (whether it’s smooth and constant, or varies) These 2 assumptions do not interact ( we don’t want them to interact - i.e., we don’t want the polynomial correlated with the unobserved variation in the treatment effect) There usually a difference between short-run and long-run treatment effects, but it’s also possibly that the bias can stem from the over-fitting problem of the polynomial specification. (C. Hausman and Rapson 2018, 544) Autoregression (serial dependence) Need to use clustered standard errors to account for serial dependence in the residuals In the case of serial dependence in \\(\\epsilon_{it}\\), we don’t have a solution, including a lagged dependent variable would misspecify the model (probably find another research project) In the case of serial dependence in \\(y_{it}\\), with long window, it becomes fuzzy to what you try to recover. You can include the lagged dependent variable (bias can still come from the time-varying treatment or over-fitting of the global polynomial) Sorting and Anticipation Effects Cannot run the (McCrary 2008) because the density of the time running variable is uniform Can still run tests to check discontinuities in other covariates (you want no discontinuities) and discontinuities in the outcome variable at other placebo thresholds ( you don’t want discontinuities) Hence, it’s hard to argue for the causal effect here because it could be the total effect of the causal treatment and the unobserved sorting/anticipation/adaptation/avoidance effects. You can only argue that there is no such behavior Recommendations for robustness check following (C. Hausman and Rapson 2018, 549) Plot the raw data and residuals (after removing confounders or trend). With varying polynomial and local linear controls, inconsistent results can be a sign of time-varying treatment effects. Using global polynomial, you could overfit, then show polynomial with different order and alternative local linear bandwidths. If the results are consistent, you’re okay Placebo Tests: estimate another RD (1) on another location or subject (that did not receive the treatment) or (2) use another date. Plot RD discontinuity on continuous controls Donut RD to see if avoiding the selection close to the cutoff would yield better results (Barreca et al. 2011) Test for auto-regression (using only pre-treatment data). If there is evidence for autoregression, include the lagged dependent variable Augmented local linear (no need to use global polynomial and avoid over-fitting) Use full sample to exclude the effect of important predictors Estimate the conditioned second stage on a smaller sample bandwidth Examples from (C. Hausman and Rapson 2018, 534) in econ (Davis 2008): Air quality (Auffhammer and Kellogg 2011): Air quality (H. Chen et al. 2018): Air quality (De Paola, Scoppa, and Falcone 2013): car accidents (Gallego, Montero, and Salas 2013): air quality (Bento et al. 2014): Traffic (M. L. Anderson 2014): Traffic (Burger, Kaffine, and Yu 2014): Car accidents (Brodeur et al. 2021): Covid19 lock-downs on well-being marketing M. R. Busse et al. (2013): Vehicle prices (X. Chen et al. 2009): Customer Satisfaction (M. R. Busse, Simester, and Zettelmeyer 2010): Vehicle prices (Davis and Kahn 2010): vehicle prices References "],["evaluation-of-an-rd.html", "24.10 Evaluation of an RD", " 24.10 Evaluation of an RD Evidence for (either formal tests or graphs) Treatment and outcomes change discontinuously at the cutoff, while other variables and pre-treatment outcomes do not. No manipulation of the assignment variable. Results are robust to various functional forms of the forcing variable Is there any other (unobserved) confound that could cause the discontinuous change at the cutoff (i.e., multiple forcing variables / bundling of institutions)? External Validity: How likely the result at the cutoff will generalize? General Model \\[ Y_i = \\beta_0 + f(x_i) \\beta_1 + [I(x_i \\ge c)]\\beta_2 + \\epsilon_i \\] where \\(f(x_i)\\) is any functional form of \\(x_i\\) Simple case When \\(f(x_i) = x_i\\) (linear function) \\[ Y_i = \\beta_0 + x_i \\beta_1 + [I(x_i \\ge c)]\\beta_2 + \\epsilon_i \\] RD gives you \\(\\beta_2\\) (causal effect) of \\(X\\) on \\(Y\\) at the cutoff point In practice, everyone does \\[ Y_i = \\alpha_0 + f(x) \\alpha _1 + [I(x_i \\ge c)]\\alpha_2 + [f(x_i)\\times [I(x_i \\ge c)]\\alpha_3 + u_i \\] where we estimate different slope on different sides of the line and if you estimate \\(\\alpha_3\\) to be no different from 0 then we return to the simple case Notes: Sparse data can make \\(\\alpha_3\\) large differential effect People are very skeptical when you have complex \\(f(x_i)\\), usual simple function forms (e.g., linear, squared term, etc.) should be good. However, if you still insist, then non-parametric estimation can be your best bet. Bandwidth of \\(c\\) (window) Closer to \\(c\\) can give you lower bias, but also efficiency Wider \\(c\\) can increase bias, but higher efficiency. Optimal bandwidth is very controversial, but usually we have to do it in the appendix for research article anyway. We can either drop observations outside of bandwidth or weight depends on how far and close to \\(c\\) "],["applications.html", "24.11 Applications", " 24.11 Applications Examples in marketing: (Narayanan and Kalyanam 2015) (Hartmann, Nair, and Narayanan 2011): nonparametric estimation and guide to identifying causal marketing mix effects Packages in R (see (Thoemmes, Liao, and Jin 2017) for detailed comparisons): all can handle both sharp and fuzzy RD rdd rdrobust estimation, inference and plot rddensity discontinuity in density tests (Sorting/Bunching/Manipulation) using local polynomials and binomial test rdlocrand covariate balance, binomial tests, window selection rdmulti multiple cutoffs and multiple scores rdpower power, sample selection rddtools Package rdd rdrobust rddtools Coefficient estimator Local linear regression local polynomial regression local polynomial regression bandwidth selectors (G. Imbens and Kalyanaraman 2012) (Calonico, Cattaneo, and Farrell 2020) (G. Imbens and Kalyanaraman 2012) (Calonico, Cattaneo, and Titiunik 2014) (G. Imbens and Kalyanaraman 2012) Kernel functions Triangular Rectangular Epanechnikov Gaussian Epanechnikov Gaussian Bias Correction Local polynomial regression Covariate options Include Include Include Residuals Assumptions testing McCrary sorting McCrary sorting Equality of covariates distribution and mean based on table 1 (Thoemmes, Liao, and Jin 2017) (p. 347) 24.11.1 Example 1 Example by Leihua Ye \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i \\] \\[ X_i = \\begin{cases} 1, W_i \\ge c \\\\ 0, W_i &lt; c \\end{cases} \\] #cutoff point = 3.5 GPA &lt;- runif(1000, 0, 4) future_success &lt;- 10 + 2 * GPA + 10 * (GPA &gt;= 3.5) + rnorm(1000) #install and load the package ‘rddtools’ #install.packages(“rddtools”) library(rddtools) data &lt;- rdd_data(future_success, GPA, cutpoint = 3.5) # plot the dataset plot( data, col = &quot;red&quot;, cex = 0.1, xlab = &quot;GPA&quot;, ylab = &quot;future_success&quot; ) # estimate the sharp RDD model rdd_mod &lt;- rdd_reg_lm(rdd_object = data, slope = &quot;same&quot;) summary(rdd_mod) #&gt; #&gt; Call: #&gt; lm(formula = y ~ ., data = dat_step1, weights = weights) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.90364 -0.70348 0.00278 0.66828 3.00603 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 16.90704 0.06637 254.75 &lt;2e-16 *** #&gt; D 10.09058 0.11063 91.21 &lt;2e-16 *** #&gt; x 1.97078 0.03281 60.06 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.9908 on 997 degrees of freedom #&gt; Multiple R-squared: 0.9654, Adjusted R-squared: 0.9654 #&gt; F-statistic: 1.392e+04 on 2 and 997 DF, p-value: &lt; 2.2e-16 # plot the RDD model along with binned observations plot( rdd_mod, cex = 0.1, col = &quot;red&quot;, xlab = &quot;GPA&quot;, ylab = &quot;future_success&quot; ) 24.11.2 Example 2 Bowblis and Smith (2021) Occupational licensing can either increase or decrease market efficiency: More information means more efficiency Increased entry barriers (i.e., friction) increase efficiency Components of RD Running variable Cutoff: 120 beds or above Treatment: you have to have the treatment before the cutoff point. Under OLS \\[ Y_i = \\alpha_0 + X_i \\alpha_1 + LW_i \\alpha_2 + \\epsilon_i \\] where \\(LW_i\\) Licensed/certified workers (in fraction format for each center). \\(Y_i\\) = Quality of service Bias in \\(\\alpha_2\\) Mitigation-based: terrible quality can lead to more hiring, which negatively bias \\(\\alpha_2\\) Preference-based: places that have higher quality staff want to keep high quality staffs. Under RD \\[ \\begin{aligned} Y_{ist} &amp;= \\beta_0 + [I(Bed \\ge121)_{ist}]\\beta_1 + f(Size_{ist}) \\beta_2\\\\ &amp;+ [f(Size_{ist}) \\times I(Bed \\ge 121)_{ist}] \\beta_3 \\\\ &amp;+ X_{it} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist} \\end{aligned} \\] where \\(s\\) = state \\(t\\) = year \\(i\\) = hospital This RD is fuzzy If right near the threshold (bandwidth), we have states with different sorting (i.e., non-random), then we need the fixed-effect for state \\(s\\). But then your RD assumption wrong anyway, then you won’t do it in the first place Technically, we could also run the fixed-effect regression, but because it’s lower in the causal inference hierarchy. Hence, we don’t do it. Moreover, in the RD framework, we don’t include \\(t\\) before treatment (but in the FE we have to include before and after) If we include \\(\\pi_i\\) for each hospital, then we don’t have variation in the causal estimates (because hardly any hospital changes their bed size in the panel) When you have \\(\\beta_1\\) as the intent to treat (because the treatment effect does not coincide with the intent to treat) You cannot take those fuzzy cases out, because it will introduce the selection bias. Note that we cannot drop cases based on behavioral choice (because we will exclude non-compliers), but we can drop when we have particular behaviors ((e.g., people like round numbers). Thus, we have to use Instrument variable 33.1.3.1 Stage 1: \\[ \\begin{aligned} QSW_{ist} &amp;= \\alpha_0 + [I(Bed \\ge121)_{ist}]\\alpha_1 + f(Size_{ist}) \\alpha_2\\\\ &amp;+ [f(Size_{ist}) \\times I(Bed \\ge 121)_{ist}] \\alpha_3 \\\\ &amp;+ X_{it} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist} \\end{aligned} \\] (Note: you should have different fixed effects and error term - \\(\\delta, \\gamma_s, \\theta_t, \\epsilon_{ist}\\) from the first equation, but I ran out of Greek letters) Stage 2: \\[ \\begin{aligned} Y_{ist} &amp;= \\gamma_0 + \\gamma_1 \\hat{QWS}_{ist} + f(Size_{ist}) \\delta_2 \\\\ &amp;+ [f(Size_{ist}) \\times I(Bed \\ge 121)] \\delta_3 \\\\ &amp;+ X_{it} \\lambda + \\eta_s + \\tau_t + u_{ist} \\end{aligned} \\] The bigger the jump (discontinuity), the more similar the 2 coefficients (\\(\\gamma_1 \\approx \\beta_1\\)) where \\(\\gamma_1\\) is the average treatment effect (of exposing to the policy) \\(\\beta_1\\) will always be closer to 0 than \\(\\gamma_1\\) Figure 1 shows bunching at every 5 units cutoff, but 120 is still out there. If we have manipulable bunching, there should be decrease at 130 Since we have limited number of mass points (at the round numbers), we should clustered standard errors by the mass point 24.11.3 Example 3 Replication of (Carpenter and Dobkin 2009) by Philipp Leppert, dataset from here 24.11.4 Example 4 For a detailed application, see (Thoemmes, Liao, and Jin 2017) where they use rdd, rdrobust, rddtools References "],["synthetic-difference-in-differences.html", "Chapter 25 Synthetic Difference-in-Differences", " Chapter 25 Synthetic Difference-in-Differences by (Arkhangelsky et al. 2021) also known as weighted double-differencing estimators Setting: Researchers use panel data to study effects of policy changes. Panel data: repeated observations across time for various units. Some units exposed to policy at different times than others. Policy changes often aren’t random across units or time. Challenge: Observed covariates might not lead to credible conclusions of no confounding (G. W. Imbens and Rubin 2015) To estimate the effects, either Difference-in-differences (DID) method widely used in applied economics. Synthetic Control (SC) methods offer alternative approach for comparative case studies. Difference between DID and SC: DID: used with many policy-exposed units; relies on “parallel trends” assumption. SC: used with few policy-exposed units; compensates lack of parallel trends by reweighting units based on pre-exposure trends. New proposition: Synthetic Difference in Differences (SDID). Combines features of DID and SC. Reweights and matches pre-exposure trends (similar to SC). Invariant to additive unit-level shifts, valid for large-panel inference (like DID). Attractive features: SDID provides consistent and asymptotically normal estimates. SDID performs on par with or better than DID in traditional DID settings. where DID can only handle completely random treatment assignment, SDID can handle cases where treatment assignment is correlated with some time or unit latent factors. Similarly, SDID is as good as or better than SC in traditional SC settings. Uniformly random treatment assignment results in unbiased outcomes for all methods, but SDID is more precise. SDID reduces bias effectively for non-uniformly random assignments. SDID’s double robustness is akin to the augmented inverse probability weighting estimator Scharfstein, Rotnitzky, and Robins (1999). Very much similar to augmented SC estimator by (Ben-Michael, Feller, and Rothstein 2021; Arkhangelsky et al. 2021, 4112) Ideal case to use SDID estimator is when \\(N_{ctr} \\approx T_{pre}\\) Small \\(T_{post}\\) \\(N_{tr} &lt;\\sqrt{N_{ctr}}\\) Applications in marketing: Lambrecht, Tucker, and Zhang (2024): TV ads on online browsing and sales. Keller, Guyt, and Grewal (2024): soda tax on marketing effectiveness. References "],["understanding.html", "25.1 Understanding", " 25.1 Understanding Consider a traditional time-series cross-sectional data Let \\(Y_{it}\\) denote the outcome for unit \\(i\\) in period \\(t\\) A balanced panel of \\(N\\) units and \\(T\\) time periods \\(W_{it} \\in \\{0, 1\\}\\) is the binary treatment \\(N_c\\) never-treated units (control) \\(N_t\\) treated units after time \\(T_{pre}\\) Steps: Find unit weights \\(\\hat{w}^{sdid}\\) such that \\(\\sum_{i = 1}^{N_c} \\hat{w}_i^{sdid} Y_{it} \\approx N_t^{-1} \\sum_{i = N_c + 1}^N Y_{it} \\forall t = 1, \\dots, T_{pre}\\) (i.e., pre-treatment trends in outcome of the treated similar to those of control units) (similar to SC). Find time weights \\(\\hat{\\lambda}_t\\) such that we have a balanced window (i.e., posttreatment outcomes for control units differ consistently from their weighted average pretreatment outcomes). Estimate the average causal effect of treatment \\[ (\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\{ \\sum_{i = 1}^N \\sum_{t = 1}^T (Y_{it} - \\mu - \\alpha_i - \\beta_ t - W_{it} \\tau)^2 \\hat{w}_i^{sdid} \\hat{\\lambda}_t^{sdid} \\} \\] Better than DiD estimator because \\(\\tau^{did}\\) does not consider time or unit weights \\[ (\\hat{\\tau}^{did}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta} \\{ \\sum_{i = 1}^N \\sum_{t = 1}^T (Y_{it} - \\mu - \\alpha_i - \\beta_ t - W_{it} \\tau)^2 \\} \\] Better than SC estimator because \\(\\tau^{sc}\\) lacks unit fixed effete and time weights \\[ (\\hat{\\tau}^{sc}, \\hat{\\mu}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\beta} \\{ \\sum_{i = 1}^N \\sum_{t = 1}^T (Y_{it} - \\mu - \\beta_ t - W_{it} \\tau)^2 \\hat{w}_i^{sdid} \\} \\] DID SC SDID Primary Assumption Absence of intervention leads to parallel evolution across states. Reweights unexposed states to match pre-intervention outcomes of treated state. Reweights control units to ensure a parallel time trend with the treated pre-intervention trend. Reliability Concern Can be unreliable when pre-intervention trends aren’t parallel. Accounts for non-parallel pre-intervention trends by reweighting. Uses reweighting to adjust for non-parallel pre-intervention trends. Treatment of Time Periods All pre-treatment periods are given equal weight. Doesn’t specifically emphasize equal weight for pre-treatment periods. Focuses only on a subset of pre-intervention time periods, selected based on historical outcomes. Goal with Reweighting N/A (doesn’t use reweighting). To match treated state as closely as possible before the intervention. Make trends of control units parallel (not necessarily identical) to the treated pre-intervention. Alternatively, think of our parameter of interest as: \\[ \\hat{\\tau} = \\hat{\\delta}_t - \\sum_{i = 1}^{N_c} \\hat{w}_i \\hat{\\delta}_i \\] where \\(\\hat{\\delta}_t = \\frac{1}{N_t} \\sum_{i = N_c + 1}^N \\hat{\\delta}_i\\) Method Sample Weight Adjusted outcomes (\\(\\hat{\\delta}_i\\)) Interpretation SC \\(\\hat{w}^{sc} = \\min_{w \\in R}l_{unit}(w)\\) \\(\\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{it}\\) Unweighted treatment period averages DID \\(\\hat{w}_i^{did} = N_c^{-1}\\) \\(\\frac{1}{T_{post}} \\sum_{t = T_{pre}+ 1}^T Y_{it} - \\frac{1}{T_{pre}} \\sum_{t = 1}^{T_{pre}}Y_{it}\\) Unweighted differences between average treatment period and pretreatment outcome SDID \\((\\hat{w}_0, \\hat{w}^{sdid}) = \\min l_{unit}(w_0, w)\\) \\(\\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{it} - \\sum_{t = 1}^{T_{pre}} \\hat{\\lambda}_t^{sdid} Y_{it}\\) Weighted differences between average treatment period and pretreatment outcome The SDID estimator uses weights: Makes two-way fixed effect regression “local.” Emphasizes units similar in their past to treated units. Prioritizes periods resembling treated periods. Benefits of this localization: Robustness: Using similar units and periods boosts estimator’s robustness. Improved Precision: Weights can eliminate predictable outcome components. The SEs of SDID are smaller than those of SC and DID Caveat: If there’s minor systematic heterogeneity in outcomes, unequal weighting might reduce precision compared to standard DID. Weight Design: Unit Weights: Makes average outcome for treated units roughly parallel to the weighted average for control units. Time Weights: Ensures posttreatment outcomes for control units differ consistently from their weighted average pretreatment outcomes. Weights enhance DID’s plausibility: Raw data often lacks parallel time trends for treated/control units. Similar techniques (e.g., adjusting for covariates or selecting specific time periods) were used before (Callaway and Sant’Anna 2021). SDID automates this process, applying a similar logic to weight both units and time periods. Time Weights in SDID: Removes bias and boosts precision (i.e., minimizes the influence of time periods vastly different from posttreatment periods). Argument for Unit Fixed Effects: Flexibility: Increases model flexibility and thereby bolsters robustness. Enhanced Precision: Unit fixed effects explain a significant portion of outcome variation. SC Weighting &amp; Unit Fixed Effects: Under certain conditions, SC weighting can inherently account for unit fixed effects. For example, when the weighted average outcome for control units in pretreatment is the same as that of the treated units. (unlikely in reality) The use of unit fixed effect in synthetic control regression (i.e., synthetic control with intercept) was proposed before in Doudchenko and Imbens (2016) and Ferman and Pinto (2021) (called DIFP) More details on application Choose unit weights Regularization Parameter: Equal to the size of a typical one-period outcome change for control units in the pre-period, then multiplied by a scaling factor (Arkhangelsky et al. 2021, 4092). Relation to SC Weights: SDID weights are similar to those used in (Abadie, Diamond, and Hainmueller 2010) except two distinctions: Inclusion of an Intercept Term: The weights in SynthDiD do not necessarily make the control pre-trends perfectly match the treated trends, just make them parallel. This flexibility comes from the use of unit fixed effects, which can absorb any consistent differences between units. Regularization Penalty: Adopted from Doudchenko and Imbens (2016) . Enhances the dispersion and ensures the uniqueness of the weights. DID weights are identical to those used in (Abadie, Diamond, and Hainmueller 2010) without intercept and regularization penalty and 1 treated unit. Choose time weights Also include an intercept term, but no regularization (because correlated observations within time periods for the same unit is plausible, but not across units within the same period). Note: To account for time-varying variables in the weights, one can use the residuals of the regression of the observed outcome on these time-varying variables, instead of the observed outcomes themselves (\\(Y_{it}^{res} = Y_{it} - X_{it} \\hat{\\beta}\\), where \\(\\hat{\\beta}\\) come from \\(Y = \\beta X_{it}\\)). The SDID method can account for systematic effects, often referred to as unit effects or unit heterogeneity, which influence treatment assignment (i.e., when treatment assignment is correlated with these systematic effects). Consequently, it provides unbiased estimates, especially valuable when there’s a suspicion that the treatment might be influenced by persistent, unit-specific attributes. Even in cases where we have completely random assignment, SDID, DiD, and SC are unbiased, but SynthDiD has the smallest SE. References "],["application-12.html", "25.2 Application", " 25.2 Application SDID Algorithm Compute regularization parameter \\(\\zeta\\) \\[ \\zeta = (N_{t}T_{post})^{1/4} \\hat{\\sigma} \\] where \\[ \\hat{\\sigma}^2 = \\frac{1}{N_c(T_{pre}- 1)} \\sum_{i = 1}^{N_c} \\sum_{t = 1}^{T_{re}-1}(\\Delta_{it} - \\hat{\\Delta})^2 \\] \\(\\Delta_{it} = Y_{i(t + 1)} - Y_{it}\\) \\(\\hat{\\Delta} = \\frac{1}{N_c(T_{pre} - 1)}\\sum_{i = 1}^{N_c}\\sum_{t = 1}^{T_{pre}-1} \\Delta_{it}\\) Compute unit weights \\(\\hat{w}^{sdid}\\) \\[ (\\hat{w}_0, \\hat{w}^{sidid}) = \\arg \\min_{w_0 \\in R, w \\in \\Omega}l_{unit}(w_0, w) \\] where \\(l_{unit} (w_0, w) = \\sum_{t = 1}^{T_{pre}}(w_0 + \\sum_{i = 1}^{N_c}w_i Y_{it} - \\frac{1}{N_t}\\sum_{i = N_c + 1}^NY_{it})^2 + \\zeta^2 T_{pre}||w||_2^2\\) \\(\\Omega = \\{w \\in R_+^N: \\sum_{i = 1}^{N_c} w_i = 1, w_i = N_t^{-1} \\forall i = N_c + 1, \\dots, N \\}\\) Compute time weights \\(\\hat{\\lambda}^{sdid}\\) \\[ (\\hat{\\lambda}_0 , \\hat{\\lambda}^{sdid}) = \\arg \\min_{\\lambda_0 \\in R, \\lambda \\in \\Lambda} l_{time}(\\lambda_0, \\lambda) \\] where \\(l_{time} (\\lambda_0, \\lambda) = \\sum_{i = 1}^{N_c}(\\lambda_0 + \\sum_{t = 1}^{T_{pre}} \\lambda_t Y_{it} - \\frac{1}{T_{post}} \\sum_{t = T_{pre} + 1}^T Y_{it})^2\\) \\(\\Lambda = \\{ \\lambda \\in R_+^T: \\sum_{t = 1}^{T_{pre}} \\lambda_t = 1, \\lambda_t = T_{post}^{-1} \\forall t = T_{pre} + 1, \\dots, T\\}\\) Compute the SDID estimator \\[ (\\hat{\\tau}^{sdid}, \\hat{\\mu}, \\hat{\\alpha}, \\hat{\\beta}) = \\arg \\min_{\\tau, \\mu, \\alpha, \\beta}\\{ \\sum_{i = 1}^N \\sum_{t = 1}^T (Y_{it} - \\mu - \\alpha_i - \\beta_t - W_{it} \\tau)^2 \\hat{w}_i^{sdid}\\hat{\\lambda}_t^{sdid} \\] SE Estimation Under certain assumptions (errors, samples, and interaction properties between time and unit fixed effects) detailed in (Arkhangelsky et al. 2019, 4107), SDID is asymptotically normal and zero-centered Using its asymptotic variance, conventional confidence intervals can be applied to SDID. \\[ \\tau \\in \\hat{\\tau}^{sdid} \\pm z_{\\alpha/2}\\sqrt{\\hat{V}_\\tau} \\] There are 3 approaches for variance estimation in confidence intervals: Clustered Bootstrap (Efron 1992): Independently resample units. Advantages: Simple to use; robust performance in large panels due to natural approach to inference with panel data where observations of the same unit might be correlated. Disadvantage: Computationally expensive. Jackknife (Miller 1974): Applied to weighted SDID regression with fixed weights. Generally conservative and precise when treated and control units are sufficiently similar. Not recommended for some methods, like the SC estimator, due to potential biases. Appropriate for jackknifing DID without random weights. Placebo Variance Estimation: Can used in cases with only one treated unit or large panels. Placebo evaluations swap out the treated unit for untreated ones to estimate noise. Relies on homoskedasticity across units. Depends on homoskedasticity across units. It hinges on the empirical distribution of residuals from placebo estimators on control units. The validity of the placebo method hinges on consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity for feasible inference. Detailed analysis available in Conley and Taber (2011). All algorithms are from Arkhangelsky et al. (2021), p. 4109: Bootstrap Variance Estimation For each \\(b\\) from \\(1 \\to B\\): Sample \\(N\\) rows from \\((\\mathbf{Y}, \\mathbf{W})\\) to get (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)) with replacement. If the sample lacks treated or control units, resample. Calculate \\(\\tau^{(b)}\\) using (\\(\\mathbf{Y}^{(b)}, \\mathbf{W}^{(b)}\\)). Calculate variance: \\(\\hat{V}_\\tau = \\frac{1}{B} \\sum_{b = 1}^B (\\hat{\\tau}^{b} - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\) Jackknife Variance Estimation For each \\(i\\) from \\(1 \\to N\\): Calculate \\(\\hat{\\tau}^{(-i)}\\): \\(\\arg\\min_{\\tau, \\{\\alpha_j, \\beta_t\\}} \\sum_{j \\neq, i, t}(\\mathbf{Y}_{jt} - \\alpha_j - \\beta_t - \\tau \\mathbf{W}_{it})^2 \\hat{w}_j \\hat{\\lambda}_t\\) Calculate: \\(\\hat{V}_{\\tau} = (N - 1) N^{-1} \\sum_{i = 1}^N (\\hat{\\tau}^{(-i)} - \\hat{\\tau})^2\\) Placebo Variance Estimation For each \\(b\\) from \\(1 \\to B\\) Sample \\(N_t\\) out of \\(N_c\\) without replacement to get the “placebo” treatment Construct a placebo treatment matrix \\(\\mathbf{W}_c^b\\) for the controls Calculate \\(\\hat{\\tau}\\) based on \\((\\mathbf{Y}_c, \\mathbf{W}_c^b)\\) Calculate \\(\\hat{V}_\\tau = \\frac{1}{B}\\sum_{b = 1}^B (\\hat{\\tau}^b - \\frac{1}{B} \\sum_{b = 1}^B \\hat{\\tau}^b)^2\\) 25.2.1 Block Treatment Code provided by the synthdid package library(synthdid) library(tidyverse) # Estimate the effect of California Proposition 99 on cigarette consumption data(&#39;california_prop99&#39;) setup = synthdid::panel.matrices(synthdid::california_prop99) tau.hat = synthdid::synthdid_estimate(setup$Y, setup$N0, setup$T0) # se = sqrt(vcov(tau.hat, method = &#39;placebo&#39;)) plot(tau.hat) + causalverse::ama_theme() setup = synthdid::panel.matrices(synthdid::california_prop99) # Run for specific estimators results_selected = causalverse::panel_estimate(setup, selected_estimators = c(&quot;synthdid&quot;, &quot;did&quot;, &quot;sc&quot;)) results_selected #&gt; $synthdid #&gt; $synthdid$estimate #&gt; synthdid: -15.604 +- NA. Effective N0/N0 = 16.4/38~0.4. Effective T0/T0 = 2.8/19~0.1. N1,T1 = 1,12. #&gt; #&gt; $synthdid$std.error #&gt; [1] 10.05324 #&gt; #&gt; #&gt; $did #&gt; $did$estimate #&gt; synthdid: -27.349 +- NA. Effective N0/N0 = 38.0/38~1.0. Effective T0/T0 = 19.0/19~1.0. N1,T1 = 1,12. #&gt; #&gt; $did$std.error #&gt; [1] 15.81479 #&gt; #&gt; #&gt; $sc #&gt; $sc$estimate #&gt; synthdid: -19.620 +- NA. Effective N0/N0 = 3.8/38~0.1. Effective T0/T0 = Inf/19~Inf. N1,T1 = 1,12. #&gt; #&gt; $sc$std.error #&gt; [1] 11.16422 # to access more details in the estimate object summary(results_selected$did$estimate) #&gt; $estimate #&gt; [1] -27.34911 #&gt; #&gt; $se #&gt; [,1] #&gt; [1,] NA #&gt; #&gt; $controls #&gt; estimate 1 #&gt; Wyoming 0.026 #&gt; Wisconsin 0.026 #&gt; West Virginia 0.026 #&gt; Virginia 0.026 #&gt; Vermont 0.026 #&gt; Utah 0.026 #&gt; Texas 0.026 #&gt; Tennessee 0.026 #&gt; South Dakota 0.026 #&gt; South Carolina 0.026 #&gt; Rhode Island 0.026 #&gt; Pennsylvania 0.026 #&gt; Oklahoma 0.026 #&gt; Ohio 0.026 #&gt; North Dakota 0.026 #&gt; North Carolina 0.026 #&gt; New Mexico 0.026 #&gt; New Hampshire 0.026 #&gt; Nevada 0.026 #&gt; Nebraska 0.026 #&gt; Montana 0.026 #&gt; Missouri 0.026 #&gt; Mississippi 0.026 #&gt; Minnesota 0.026 #&gt; Maine 0.026 #&gt; Louisiana 0.026 #&gt; Kentucky 0.026 #&gt; Kansas 0.026 #&gt; Iowa 0.026 #&gt; Indiana 0.026 #&gt; Illinois 0.026 #&gt; Idaho 0.026 #&gt; Georgia 0.026 #&gt; Delaware 0.026 #&gt; Connecticut 0.026 #&gt; #&gt; $periods #&gt; estimate 1 #&gt; 1988 0.053 #&gt; 1987 0.053 #&gt; 1986 0.053 #&gt; 1985 0.053 #&gt; 1984 0.053 #&gt; 1983 0.053 #&gt; 1982 0.053 #&gt; 1981 0.053 #&gt; 1980 0.053 #&gt; 1979 0.053 #&gt; 1978 0.053 #&gt; 1977 0.053 #&gt; 1976 0.053 #&gt; 1975 0.053 #&gt; 1974 0.053 #&gt; 1973 0.053 #&gt; 1972 0.053 #&gt; 1971 0.053 #&gt; #&gt; $dimensions #&gt; N1 N0 N0.effective T1 T0 T0.effective #&gt; 1 38 38 12 19 19 causalverse::process_panel_estimate(results_selected) #&gt; Method Estimate SE #&gt; 1 SYNTHDID -15.60 10.05 #&gt; 2 DID -27.35 15.81 #&gt; 3 SC -19.62 11.16 25.2.2 Staggered Adoption To apply to staggered adoption settings using the SDID estimator (see examples in Arkhangelsky et al. (2021), p. 4115 similar to Ben-Michael, Feller, and Rothstein (2022)), we can: Apply the SDID estimator repeatedly, once for every adoption date. Using Ben-Michael, Feller, and Rothstein (2022) ’s method, form matrices for each adoption date. Apply SDID and average based on treated unit/time-period fractions. Create multiple samples by splitting the data up by time periods. Each sample should have a consistent adoption date. For a formal note on this special case, see Porreca (2022). It compares the outcomes from using SynthDiD with those from other estimators: Two-Way Fixed Effects (TWFE), The group time average treatment effect estimator from Callaway and Sant’Anna (2021), The partially pooled synthetic control method estimator from Ben-Michael, Feller, and Rothstein (2021), in a staggered treatment adoption context. The findings reveal that SynthDiD produces a different estimate of the average treatment effect compared to the other methods. Simulation results suggest that these differences could be due to the SynthDiD’s data generating process assumption (a latent factor model) aligning more closely with the actual data than the additive fixed effects model assumed by traditional DiD methods. To explore heterogeneity of treatment effect, we can do subgroup analysis (Berman and Israeli 2022, 1092) Method Advantages Disadvantages Procedure Split Data into Subsets Compares treated units to control units within the same subgroup. Each subset uses a different synthetic control, making it challenging to compare effects across subgroups. Split the data into separate subsets for each subgroup. Compute synthetic DID effects for each subset. Control Group Comprising All Non-adopters Control weights match pretrends well for each treated subgroup. Each control unit receives a different weight for each treatment subgroup, making it difficult to compare results due to varying synthetic controls. Use a control group consisting of all non-adopters in each balanced panel cohort analysis. Switch treatment units to the subgroup being analyzed. Perform synthdid analysis. Use All Data to Estimate Synthetic Control Weights (recommend) All units have the same synthetic control. Pretrend match may not be as accurate since it aims to match the average outcome of all treated units, not just a specific subgroup. Use all the data to estimate the synthetic DID control weights. Compute treatment effects using only the treated subgroup units as the treatment units. library(tidyverse) df &lt;- fixest::base_stagg |&gt; dplyr::mutate(treatvar = if_else(time_to_treatment &gt;= 0, 1, 0)) |&gt; dplyr::mutate(treatvar = as.integer(if_else(year_treated &gt; (5 + 2), 0, treatvar))) est &lt;- causalverse::synthdid_est_ate( data = df, adoption_cohorts = 5:7, lags = 2, leads = 2, time_var = &quot;year&quot;, unit_id_var = &quot;id&quot;, treated_period_var = &quot;year_treated&quot;, treat_stat_var = &quot;treatvar&quot;, outcome_var = &quot;y&quot; ) #&gt; adoption_cohort: 5 #&gt; Treated units: 5 Control units: 65 #&gt; adoption_cohort: 6 #&gt; Treated units: 5 Control units: 60 #&gt; adoption_cohort: 7 #&gt; Treated units: 5 Control units: 55 data.frame( Period = names(est$TE_mean_w), ATE = est$TE_mean_w, SE = est$SE_mean_w ) |&gt; causalverse::nice_tab() #&gt; Period ATE SE #&gt; 1 -2 -0.05 0.22 #&gt; 2 -1 0.05 0.22 #&gt; 3 0 -5.07 0.80 #&gt; 4 1 -4.68 0.51 #&gt; 5 2 -3.70 0.79 #&gt; 6 cumul.0 -5.07 0.80 #&gt; 7 cumul.1 -4.87 0.55 #&gt; 8 cumul.2 -4.48 0.53 causalverse::synthdid_plot_ate(est) est_sub &lt;- causalverse::synthdid_est_ate( data = df, adoption_cohorts = 5:7, lags = 2, leads = 2, time_var = &quot;year&quot;, unit_id_var = &quot;id&quot;, treated_period_var = &quot;year_treated&quot;, treat_stat_var = &quot;treatvar&quot;, outcome_var = &quot;y&quot;, # a vector of subgroup id (from unit id) subgroup = c( # some are treated &quot;11&quot;, &quot;30&quot;, &quot;49&quot; , # some are control within this period &quot;20&quot;, &quot;25&quot;, &quot;21&quot;) ) #&gt; adoption_cohort: 5 #&gt; Treated units: 3 Control units: 65 #&gt; adoption_cohort: 6 #&gt; Treated units: 0 Control units: 60 #&gt; adoption_cohort: 7 #&gt; Treated units: 0 Control units: 55 data.frame( Period = names(est_sub$TE_mean_w), ATE = est_sub$TE_mean_w, SE = est_sub$SE_mean_w ) |&gt; causalverse::nice_tab() #&gt; Period ATE SE #&gt; 1 -2 0.32 0.44 #&gt; 2 -1 -0.32 0.44 #&gt; 3 0 -4.29 1.68 #&gt; 4 1 -4.00 1.52 #&gt; 5 2 -3.44 2.90 #&gt; 6 cumul.0 -4.29 1.68 #&gt; 7 cumul.1 -4.14 1.52 #&gt; 8 cumul.2 -3.91 1.82 causalverse::synthdid_plot_ate(est) Plot different estimators library(causalverse) methods &lt;- c(&quot;synthdid&quot;, &quot;did&quot;, &quot;sc&quot;, &quot;sc_ridge&quot;, &quot;difp&quot;, &quot;difp_ridge&quot;) estimates &lt;- lapply(methods, function(method) { synthdid_est_ate( data = df, adoption_cohorts = 5:7, lags = 2, leads = 2, time_var = &quot;year&quot;, unit_id_var = &quot;id&quot;, treated_period_var = &quot;year_treated&quot;, treat_stat_var = &quot;treatvar&quot;, outcome_var = &quot;y&quot;, method = method ) }) plots &lt;- lapply(seq_along(estimates), function(i) { causalverse::synthdid_plot_ate(estimates[[i]], title = methods[i], theme = causalverse::ama_theme(base_size = 6)) }) gridExtra::grid.arrange(grobs = plots, ncol = 2) References "],["difference-in-differences.html", "Chapter 26 Difference-in-differences", " Chapter 26 Difference-in-differences List of packages Examples in marketing (Liaukonyte, Teixeira, and Wilbur 2015): TV ad on online shopping (Yanwen Wang, Lewis, and Schweidel 2018): political ad source and message tone on vote shares and turnout using discontinuities in the level of political ads at the borders (Datta, Knox, and Bronnenberg 2018): streaming service on total music consumption using timing of users adoption of a music streaming service (Janakiraman, Lim, and Rishika 2018): data breach announcement affect customer spending using timing of data breach and variation whether customer info was breached in that event (Israeli 2018): digital monitoring and enforcement on violations using enforcement of min ad price policies (Ramani and Srinivasan 2019): firms respond to foreign direct investment liberalization using India’s reform in 1991. (Pattabhiramaiah, Sriram, and Manchanda 2019): paywall affects readership (Akca and Rao 2020): aggregators for airlines business effect (Lim et al. 2020): nutritional labels on nutritional quality for other brands in a category using variation in timing of adoption of nutritional labels across categories (Guo, Sriram, and Manchanda 2020): payment disclosure laws effect on physician prescription behavior using Timing of the Massachusetts open payment law as the exogenous shock (S. He, Hollenbeck, and Proserpio 2022): using Amazon policy change to examine the causal impact of fake reviews on sales, average ratings. (Peukert et al. 2022): using European General data protection Regulation, examine the impact of policy change on website usage. Examples in econ: (Rosenzweig and Wolpin 2000) (J. D. Angrist and Krueger 2001) (Fuchs-Schündeln and Hassan 2016): macro Show the mechanism via Mediation Under DiD analysis: see (Habel, Alavi, and Linsenmayer 2021) Moderation analysis: see (Goldfarb and Tucker 2011) Steps to trust DID: Visualize the treatment rollout (e.g., panelView). Document the number of treated units in each cohort (e.g., control and treated). Visualize the trajectory of average outcomes across cohorts (if you have multiple periods). Parallel Trends Conduct an event-study analysis with and without covariates. For the case with covariates, check for overlap in covariates between treated and control groups to ensure control group validity (e.g., if the control is relatively small than the treated group, you might not have overlap, and you have to make extrapolation). Conduct sensitivity analysis for parallel trend violations (e.g., honestDiD). References "],["visualization.html", "26.1 Visualization", " 26.1 Visualization library(panelView) library(fixest) library(tidyverse) base_stagg &lt;- fixest::base_stagg |&gt; # treatment status dplyr::mutate(treat_stat = dplyr::if_else(time_to_treatment &lt; 0, 0, 1)) |&gt; select(id, year, treat_stat, y) head(base_stagg) #&gt; id year treat_stat y #&gt; 2 90 1 0 0.01722971 #&gt; 3 89 1 0 -4.58084528 #&gt; 4 88 1 0 2.73817174 #&gt; 5 87 1 0 -0.65103066 #&gt; 6 86 1 0 -5.33381664 #&gt; 7 85 1 0 0.49562631 panelView::panelview( y ~ treat_stat, data = base_stagg, index = c(&quot;id&quot;, &quot;year&quot;), xlab = &quot;Year&quot;, ylab = &quot;Unit&quot;, display.all = F, gridOff = T, by.timing = T ) # alternatively specification panelView::panelview( Y = &quot;y&quot;, D = &quot;treat_stat&quot;, data = base_stagg, index = c(&quot;id&quot;, &quot;year&quot;), xlab = &quot;Year&quot;, ylab = &quot;Unit&quot;, display.all = F, gridOff = T, by.timing = T ) # Average outcomes for each cohort panelView::panelview( data = base_stagg, Y = &quot;y&quot;, D = &quot;treat_stat&quot;, index = c(&quot;id&quot;, &quot;year&quot;), by.timing = T, display.all = F, type = &quot;outcome&quot;, by.cohort = T ) #&gt; Number of unique treatment histories: 10 "],["simple-dif-n-dif.html", "26.2 Simple Dif-n-dif", " 26.2 Simple Dif-n-dif A tool developed intuitively to study “natural experiment”, but its uses are much broader. Fixed Effects Estimator is the foundation for DID Why is dif-in-dif attractive? Identification strategy: Inter-temporal variation between groups Cross-sectional estimator helps avoid omitted (unobserved) common trends Time-series estimator helps overcome omitted (unobserved) cross-sectional differences Consider \\(D_i = 1\\) treatment group \\(D_i = 0\\) control group \\(T= 1\\) After the treatment \\(T =0\\) Before the treatment After (T = 1) Before (T = 0) Treated \\(D_i =1\\) \\(E[Y_{1i}(1)|D_i = 1]\\) \\(E[Y_{0i}(0)|D)i=1]\\) Control \\(D_i = 0\\) \\(E[Y_{0i}(1) |D_i =0]\\) \\(E[Y_{0i}(0)|D_i=0]\\) missing \\(E[Y_{0i}(1)|D=1]\\) The Average Treatment Effect on Treated (ATT) \\[ \\begin{aligned} E[Y_1(1) - Y_0(1)|D=1] &amp;= \\{E[Y(1)|D=1] - E[Y(1)|D=0] \\} \\\\ &amp;- \\{E[Y(0)|D=1] - E[Y(0)|D=0] \\} \\end{aligned} \\] More elaboration: For the treatment group, we isolate the difference between being treated and not being treated. If the untreated group would have been affected in a different way, the DiD design and estimate would tell us nothing. Alternatively, because we can’t observe treatment variation in the control group, we can’t say anything about the treatment effect on this group. Extension More than 2 groups (multiple treatments and multiple controls), and more than 2 period (pre and post) \\[ Y_{igt} = \\alpha_g + \\gamma_t + \\beta I_{gt} + \\delta X_{igt} + \\epsilon_{igt} \\] where \\(\\alpha_g\\) is the group-specific fixed effect \\(\\gamma_t\\) = time specific fixed effect \\(\\beta\\) = dif-in-dif effect \\(I_{gt}\\) = interaction terms (n treatment indicators x n post-treatment dummies) (capture effect heterogeneity over time) This specification is the “two-way fixed effects DiD” - TWFE (i.e., 2 sets of fixed effects: group + time). However, if you have Staggered Dif-n-dif (i.e., treatment is applied at different times to different groups). TWFE is really bad. Long-term Effects To examine the dynamic treatment effects (that are not under rollout/staggered design), we can create a centered time variable, Centered Time Variable Period … \\(t = -1\\) 2 periods before treatment period \\(t = 0\\) Last period right before treatment period Remember to use this period as reference group \\(t = 1\\) Treatment period … By interacting this factor variable, we can examine the dynamic effect of treatment (i.e., whether it’s fading or intensifying) \\[ \\begin{aligned} Y &amp;= \\alpha_0 + \\alpha_1 Group + \\alpha_2 Time \\\\ &amp;+ \\beta_{-T_1} Treatment+ \\beta_{-(T_1 -1)} Treatment + \\dots + \\beta_{-1} Treatment \\\\ &amp;+ \\beta_1 + \\dots + \\beta_{T_2} Treatment \\end{aligned} \\] where \\(\\beta_0\\) is used as the reference group (i.e., drop from the model) \\(T_1\\) is the pre-treatment period \\(T_2\\) is the post-treatment period With more variables (i.e., interaction terms), coefficients estimates can be less precise (i.e., higher SE). DiD on the relationship, not levels. Technically, we can apply DiD research design not only on variables, but also on coefficients estimates of some other regression models with before and after a policy is implemented. Goal: Pre-treatment coefficients should be non-significant \\(\\beta_{-T_1}, \\dots, \\beta_{-1} = 0\\) (similar to the Placebo Test) Post-treatment coefficients are expected to be significant \\(\\beta_1, \\dots, \\beta_{T_2} \\neq0\\) You can now examine the trend in post-treatment coefficients (i.e., increasing or decreasing) library(tidyverse) library(fixest) od &lt;- causaldata::organ_donations %&gt;% # Treatment variable dplyr::mutate(California = State == &#39;California&#39;) %&gt;% # centered time variable dplyr::mutate(center_time = as.factor(Quarter_Num - 3)) # where 3 is the reference period precedes the treatment period class(od$California) #&gt; [1] &quot;logical&quot; class(od$State) #&gt; [1] &quot;character&quot; cali &lt;- feols(Rate ~ i(center_time, California, ref = 0) | State + center_time, data = od) etable(cali) #&gt; cali #&gt; Dependent Var.: Rate #&gt; #&gt; California x center_time = -2 -0.0029 (0.0051) #&gt; California x center_time = -1 0.0063** (0.0023) #&gt; California x center_time = 1 -0.0216*** (0.0050) #&gt; California x center_time = 2 -0.0203*** (0.0045) #&gt; California x center_time = 3 -0.0222* (0.0100) #&gt; Fixed-Effects: ------------------- #&gt; State Yes #&gt; center_time Yes #&gt; _____________________________ ___________________ #&gt; S.E.: Clustered by: State #&gt; Observations 162 #&gt; R2 0.97934 #&gt; Within R2 0.00979 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 iplot(cali, pt.join = T) coefplot(cali) "],["notes-1.html", "26.3 Notes", " 26.3 Notes Matching Methods Match treatment and control based on pre-treatment observables Modify SEs appropriately (James J. Heckman, Ichimura, and Todd 1997). It’s might be easier to just use the Doubly Robust DiD (Sant’Anna and Zhao 2020) where you just need either matching or regression to work in order to identify your treatment effect Whereas the group fixed effects control for the group time-invariant effects, it does not control for selection bias (i.e., certain groups are more likely to be treated than others). Hence, with these backdoor open (i.e., selection bias) between (1) propensity to be treated and (2) dynamics evolution of the outcome post-treatment, matching can potential close these backdoor. Be careful when matching time-varying covariates because you might encounter “regression to the mean” problem, where pre-treatment periods can have an unusually bad or good time (that is out of the ordinary), then the post-treatment period outcome can just be an artifact of the regression to the mean (Daw and Hatfield 2018). This problem is not of concern to time-invariant variables. Matching and DiD can use pre-treatment outcomes to correct for selection bias. From real world data and simulation, (Chabé-Ferret 2015) found that matching generally underestimates the average causal effect and gets closer to the true effect with more number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is still consistent when implemented symmetrically (i.e., the same number of period before and after treatment). In cases where selection bias is asymmetric, the MC simulations show that Symmetric DiD still performs better than Matching. It’s always good to show results with and without controls because If the controls are fixed within group or within time, then those should be absorbed under those fixed effects If the controls are dynamic across group and across, then your parallel trends assumption is not plausible. Under causal inference, \\(R^2\\) is not so important. For count data, one can use the fixed-effects Poisson pseudo-maximum likelihood estimator (PPML) Puhani (2012) (For applied papers, see Burtch, Carnahan, and Greenwood (2018) in management and C. He et al. (2021) in marketing). This also allows for robust standard errors under over-dispersion (Wooldridge 1999). This estimator outperforms a log OLS when data have many 0s(Silva and Tenreyro 2011), since log-OLS can produce biased estimates (O’Hara and Kotze 2010) under heteroskedascity (Silva and Tenreyro 2006). For those thinking of negative binomial with fixed effects, there isn’t an estimator right now (Allison and Waterman 2002). For [Zero-valued Outcomes], we have to distinguish the treatment effect on the intensive (outcome: 10 to 11) vs. extensive margins (outcome: 0 to 1), and we can’t readily interpret the treatment coefficient of log-transformed outcome regression as percentage change (J. Chen and Roth 2023). Alternatively, we can either focus on Proportional treatment effects: \\(\\theta_{ATT\\%} = \\frac{E(Y_{it}(1) | D_i = 1, Post_t = 1) - E(Y_{it}(0) |D_i = 1, Post_t = 1)}{E(Y_{it}(0) | D_i = 1 , Post_t = 1}\\) (i.e., percentage change in treated group’s average post-treatment outcome). Instead of relying on the parallel trends assumption in levels, we could also rely on parallel trends assumption in ratio (Wooldridge 2023). We can use Poisson QMLE to estimate the treatment effect: \\(Y_{it} = \\exp(\\beta_0 + D_i \\times \\beta_1 Post_t + \\beta_2 D_i + \\beta_3 Post_t + X_{it}) \\epsilon_{it}\\) and \\(\\hat{\\theta}_{ATT \\%} = \\exp(\\hat{\\beta}_1-1)\\). To examine the parallel trends assumption in ratio holds, we can also estimate a dynamic version of the Poisson QMLE: \\(Y_{it} = \\exp(\\lambda_t + \\beta_2 D_i + \\sum_{r \\neq -1} \\beta_r D_i \\times (RelativeTime_t = r)\\), we would expect \\(\\exp(\\hat{\\beta_r}) - 1 = 0\\) for \\(r &lt; 0\\). Even if we see the plot of these coefficients are 0, we still should run sensitivity analysis (Rambachan and Roth 2023) to examine violation of this assumption (see Prior Parallel Trends Test). Log Effects with Calibrated Extensive-margin value: due to problem with the mean value interpretation of the proportional treatment effects with outcomes that are heavy-tailed, we might be interested in the extensive margin effect. Then, we can explicit model how much weight we put on the intensive vs. extensive margin (J. Chen and Roth 2023, 39). Proportional treatment effects set.seed(123) # For reproducibility n &lt;- 500 # Number of observations per group (treated and control) # Generating IDs for a panel setup ID &lt;- rep(1:n, times = 2) # Defining groups and periods Group &lt;- rep(c(&quot;Control&quot;, &quot;Treated&quot;), each = n) Time &lt;- rep(c(&quot;Before&quot;, &quot;After&quot;), times = n) Treatment &lt;- ifelse(Group == &quot;Treated&quot;, 1, 0) Post &lt;- ifelse(Time == &quot;After&quot;, 1, 0) # Step 1: Generate baseline outcomes with a zero-inflated model lambda &lt;- 20 # Average rate of occurrence zero_inflation &lt;- 0.5 # Proportion of zeros Y_baseline &lt;- ifelse(runif(2 * n) &lt; zero_inflation, 0, rpois(2 * n, lambda)) # Step 2: Apply DiD treatment effect on the treated group in the post-treatment period Treatment_Effect &lt;- Treatment * Post Y_treatment &lt;- ifelse(Treatment_Effect == 1, rpois(n, lambda = 2), 0) # Incorporating a simple time trend, ensuring outcomes are non-negative Time_Trend &lt;- ifelse(Time == &quot;After&quot;, rpois(2 * n, lambda = 1), 0) # Step 3: Combine to get the observed outcomes Y_observed &lt;- Y_baseline + Y_treatment + Time_Trend # Ensure no negative outcomes after the time trend Y_observed &lt;- ifelse(Y_observed &lt; 0, 0, Y_observed) # Create the final dataset data &lt;- data.frame( ID = ID, Treatment = Treatment, Period = Post, Outcome = Y_observed ) # Viewing the first few rows of the dataset head(data) #&gt; ID Treatment Period Outcome #&gt; 1 1 0 0 0 #&gt; 2 2 0 1 25 #&gt; 3 3 0 0 0 #&gt; 4 4 0 1 20 #&gt; 5 5 0 0 19 #&gt; 6 6 0 1 0 library(fixest) res_pois &lt;- fepois(Outcome ~ Treatment + Period + Treatment * Period, data = data, vcov = &quot;hetero&quot;) etable(res_pois) #&gt; res_pois #&gt; Dependent Var.: Outcome #&gt; #&gt; Constant 2.249*** (0.0717) #&gt; Treatment 0.1743. (0.0932) #&gt; Period 0.0662 (0.0960) #&gt; Treatment x Period 0.0314 (0.1249) #&gt; __________________ _________________ #&gt; S.E. type Heteroskeda.-rob. #&gt; Observations 1,000 #&gt; Squared Cor. 0.01148 #&gt; Pseudo R2 0.00746 #&gt; BIC 15,636.8 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Average percentage change exp(coefficients(res_pois)[&quot;Treatment:Period&quot;]) - 1 #&gt; Treatment:Period #&gt; 0.03191643 # SE using delta method exp(coefficients(res_pois)[&quot;Treatment:Period&quot;]) * sqrt(res_pois$cov.scaled[&quot;Treatment:Period&quot;, &quot;Treatment:Period&quot;]) #&gt; Treatment:Period #&gt; 0.1288596 In this example, the DID coefficient is not significant. However, say that it’s significant, we can interpret the coefficient as 3 percent increase in posttreatment period due to the treatment. library(fixest) base_did_log0 &lt;- base_did |&gt; mutate(y = if_else(y &gt; 0, y, 0)) res_pois_es &lt;- fepois(y ~ x1 + i(period, treat, 5) | id + period, data = base_did_log0, vcov = &quot;hetero&quot;) etable(res_pois_es) #&gt; res_pois_es #&gt; Dependent Var.: y #&gt; #&gt; x1 0.1895*** (0.0108) #&gt; treat x period = 1 -0.2769 (0.3545) #&gt; treat x period = 2 -0.2699 (0.3533) #&gt; treat x period = 3 0.1737 (0.3520) #&gt; treat x period = 4 -0.2381 (0.3249) #&gt; treat x period = 6 0.3724 (0.3086) #&gt; treat x period = 7 0.7739* (0.3117) #&gt; treat x period = 8 0.5028. (0.2962) #&gt; treat x period = 9 0.9746** (0.3092) #&gt; treat x period = 10 1.310*** (0.3193) #&gt; Fixed-Effects: ------------------ #&gt; id Yes #&gt; period Yes #&gt; ___________________ __________________ #&gt; S.E. type Heteroskedas.-rob. #&gt; Observations 1,080 #&gt; Squared Cor. 0.51131 #&gt; Pseudo R2 0.34836 #&gt; BIC 5,868.8 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 iplot(res_pois_es) This parallel trend is the “ratio” version as in Wooldridge (2023) : \\[ \\frac{E(Y_{it}(0) |D_i = 1, Post_t = 1)}{E(Y_{it}(0) |D_i = 1, Post_t = 0)} = \\frac{E(Y_{it}(0) |D_i = 0, Post_t = 1)}{E(Y_{it}(0) |D_i =0, Post_t = 0)} \\] which means without treatment, the average percentage change in the mean outcome for treated group is identical to that of the control group. Log Effects with Calibrated Extensive-margin value If we want to study the treatment effect on a concave transformation of the outcome that is less influenced by those in the distribution’s tail, then we can perform this analysis. Steps: Normalize the outcomes such that 1 represents the minimum non-zero and positve value (i.e., divide the outcome by its minimum non-zero and positive value). Estimate the treatment effects for the new outcome \\[ m(y) = \\begin{cases} \\log(y) &amp; \\text{for } y &gt;0 \\\\ -x &amp; \\text{for } y = 0 \\end{cases} \\] The choice of \\(x\\) depends on what the researcher is interested in: Value of \\(x\\) Interest \\(x = 0\\) The treatment effect in logs where all zero-valued outcomes are set to equal the minimum non-zero value (i.e., we exclude the extensive-margin change between 0 and \\(y_{min}\\) ) \\(x&gt;0\\) Setting the change between 0 and \\(y_{min}\\) to be valued as the equivalent of a \\(x\\) log point change along the intensive margin. library(fixest) base_did_log0_cali &lt;- base_did_log0 |&gt; # get min mutate(min_y = min(y[y &gt; 0])) |&gt; # normalized the outcome mutate(y_norm = y / min_y) my_regression &lt;- function(x) { base_did_log0_cali &lt;- base_did_log0_cali %&gt;% mutate(my = ifelse(y_norm == 0,-x, log(y_norm))) my_reg &lt;- feols( fml = my ~ x1 + i(period, treat, 5) | id + period, data = base_did_log0_cali, vcov = &quot;hetero&quot; ) return(my_reg) } xvec &lt;- c(0, .1, .5, 1, 3) reg_list &lt;- purrr::map(.x = xvec, .f = my_regression) iplot(reg_list, pt.col = 1:length(xvec), pt.pch = 1:length(xvec)) legend(&quot;topleft&quot;, col = 1:length(xvec), pch = 1:length(xvec), legend = as.character(xvec)) etable( reg_list, headers = list(&quot;Extensive-margin value (x)&quot; = as.character(xvec)), digits = 2, digits.stats = 2 ) #&gt; model 1 model 2 model 3 #&gt; Extensive-margin value (x) 0 0.1 0.5 #&gt; Dependent Var.: my my my #&gt; #&gt; x1 0.43*** (0.02) 0.44*** (0.02) 0.46*** (0.03) #&gt; treat x period = 1 -0.92 (0.67) -0.94 (0.69) -1.0 (0.73) #&gt; treat x period = 2 -0.41 (0.66) -0.42 (0.67) -0.43 (0.71) #&gt; treat x period = 3 -0.34 (0.67) -0.35 (0.68) -0.38 (0.73) #&gt; treat x period = 4 -1.0 (0.67) -1.0 (0.68) -1.1 (0.73) #&gt; treat x period = 6 0.44 (0.66) 0.44 (0.67) 0.45 (0.72) #&gt; treat x period = 7 1.1. (0.64) 1.1. (0.65) 1.2. (0.70) #&gt; treat x period = 8 1.1. (0.64) 1.1. (0.65) 1.1 (0.69) #&gt; treat x period = 9 1.7** (0.65) 1.7** (0.66) 1.8* (0.70) #&gt; treat x period = 10 2.4*** (0.62) 2.4*** (0.63) 2.5*** (0.68) #&gt; Fixed-Effects: -------------- -------------- -------------- #&gt; id Yes Yes Yes #&gt; period Yes Yes Yes #&gt; __________________________ ______________ ______________ ______________ #&gt; S.E. type Heterosk.-rob. Heterosk.-rob. Heterosk.-rob. #&gt; Observations 1,080 1,080 1,080 #&gt; R2 0.43 0.43 0.43 #&gt; Within R2 0.26 0.26 0.25 #&gt; #&gt; model 4 model 5 #&gt; Extensive-margin value (x) 1 3 #&gt; Dependent Var.: my my #&gt; #&gt; x1 0.49*** (0.03) 0.62*** (0.04) #&gt; treat x period = 1 -1.1 (0.79) -1.5 (1.0) #&gt; treat x period = 2 -0.44 (0.77) -0.51 (0.99) #&gt; treat x period = 3 -0.43 (0.78) -0.60 (1.0) #&gt; treat x period = 4 -1.2 (0.78) -1.5 (1.0) #&gt; treat x period = 6 0.45 (0.77) 0.46 (1.0) #&gt; treat x period = 7 1.2 (0.75) 1.3 (0.97) #&gt; treat x period = 8 1.2 (0.74) 1.3 (0.96) #&gt; treat x period = 9 1.8* (0.75) 2.1* (0.97) #&gt; treat x period = 10 2.7*** (0.73) 3.2*** (0.94) #&gt; Fixed-Effects: -------------- -------------- #&gt; id Yes Yes #&gt; period Yes Yes #&gt; __________________________ ______________ ______________ #&gt; S.E. type Heterosk.-rob. Heterosk.-rob. #&gt; Observations 1,080 1,080 #&gt; R2 0.42 0.41 #&gt; Within R2 0.25 0.24 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We have the dynamic treatment effects for different hypothesized extensive-margin value of \\(x \\in (0, .1, .5, 1, 3, 5)\\) The first column is when the zero-valued outcome equal to \\(y_{min, y&gt;0}\\) (i.e., there is no different between the minimum outcome and zero outcome - \\(x = 0\\)) For this particular example, as the extensive margin increases, we see an increase in the effect magnitude. The second column is when we assume an extensive-margin change from 0 to \\(y_{min, y &gt;0}\\) is equivalent to a 10 (i.e., \\(0.1 \\times 100\\)) log point change along the intensive margin. References "],["standard-errors-2.html", "26.4 Standard Errors", " 26.4 Standard Errors Serial correlation is a big problem in DiD because (Bertrand, Duflo, and Mullainathan 2004) DiD often uses long time series Outcomes are often highly positively serially correlated Minimal variation in the treatment variable over time within a group (e.g., state). To overcome this problem: Using parametric correction (standard AR correction) is not good. Using nonparametric (e.g., block bootstrap- keep all obs from the same group such as state together) is good when number of groups is large. Remove time series dimension (i.e., aggregate data into 2 periods: pre and post). This still works with small number of groups (See (Donald and Lang 2007) for more notes on small-sample aggregation). Empirical and arbitrary variance-covariance matrix corrections work only in large samples. References "],["examples.html", "26.5 Examples", " 26.5 Examples Example by Philipp Leppert replicating Card and Krueger (1994) Example by Anthony Schmidt 26.5.1 Example by Doleac and Hansen (2020) The purpose of banning a checking box for ex-criminal was banned because we thought that it gives more access to felons Even if we ban the box, employers wouldn’t just change their behaviors. But then the unintended consequence is that employers statistically discriminate based on race 3 types of ban the box Public employer only Private employer with government contract All employers Main identification strategy If any county in the Metropolitan Statistical Area (MSA) adopts ban the box, it means the whole MSA is treated. Or if the state adopts “ban the ban,” every county is treated Under Simple Dif-n-dif \\[ Y_{it} = \\beta_0 + \\beta_1 Post_t + \\beta_2 treat_i + \\beta_2 (Post_t \\times Treat_i) + \\epsilon_{it} \\] But if there is no common post time, then we should use Staggered Dif-n-dif \\[ \\begin{aligned} E_{imrt} &amp;= \\alpha + \\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\\\ &amp;+ \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + \\delta_m\\times f(t) \\beta_7 + e_{imrt} \\end{aligned} \\] where \\(i\\) = person; \\(m\\) = MSA; \\(r\\) = region (US regions e.g., Midwest) ; \\(r\\) = region; \\(t\\) = year \\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic \\(\\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\) are the 3 dif-n-dif variables (\\(BTB\\) = “ban the box”) \\(\\delta_m\\) = dummy for MSI \\(D_{imt}\\) = control for people \\(\\lambda_{rt}\\) = region by time fixed effect \\(\\delta_m \\times f(t)\\) = linear time trend within MSA (but we should not need this if we have good pre-trend) If we put \\(\\lambda_r - \\lambda_t\\) (separately) we will more broad fixed effect, while \\(\\lambda_{rt}\\) will give us deeper and narrower fixed effect. Before running this model, we have to drop all other races. And \\(\\beta_1, \\beta_2, \\beta_3\\) are not collinear because there are all interaction terms with \\(BTB_{mt}\\) If we just want to estimate the model for black men, we will modify it to be \\[ E_{imrt} = \\alpha + BTB_{mt} \\beta_1 + \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + (\\delta_m \\times f(t)) \\beta_7 + e_{imrt} \\] \\[ \\begin{aligned} E_{imrt} &amp;= \\alpha + BTB_{m (t - 3t)} \\theta_1 + BTB_{m(t-2)} \\theta_2 + BTB_{mt} \\theta_4 \\\\ &amp;+ BTB_{m(t+1)}\\theta_5 + BTB_{m(t+2)}\\theta_6 + BTB_{m(t+3t)}\\theta_7 \\\\ &amp;+ [\\delta_m + D_{imt}\\beta_5 + \\lambda_r + (\\delta_m \\times (f(t))\\beta_7 + e_{imrt}] \\end{aligned} \\] We have to leave \\(BTB_{m(t-1)}\\theta_3\\) out for the category would not be perfect collinearity So the year before BTB (\\(\\theta_1, \\theta_2, \\theta_3\\)) should be similar to each other (i.e., same pre-trend). Remember, we only run for places with BTB. If \\(\\theta_2\\) is statistically different from \\(\\theta_3\\) (baseline), then there could be a problem, but it could also make sense if we have pre-trend announcement. 26.5.2 Example from Princeton library(foreign) mydata = read.dta(&quot;http://dss.princeton.edu/training/Panel101.dta&quot;) %&gt;% # create a dummy variable to indicate the time when the treatment started dplyr::mutate(time = ifelse(year &gt;= 1994, 1, 0)) %&gt;% # create a dummy variable to identify the treatment group dplyr::mutate(treated = ifelse(country == &quot;E&quot; | country == &quot;F&quot; | country == &quot;G&quot; , 1, 0)) %&gt;% # create an interaction between time and treated dplyr::mutate(did = time * treated) estimate the DID estimator didreg = lm(y ~ treated + time + did, data = mydata) summary(didreg) #&gt; #&gt; Call: #&gt; lm(formula = y ~ treated + time + did, data = mydata) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -9.768e+09 -1.623e+09 1.167e+08 1.393e+09 6.807e+09 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.581e+08 7.382e+08 0.485 0.6292 #&gt; treated 1.776e+09 1.128e+09 1.575 0.1200 #&gt; time 2.289e+09 9.530e+08 2.402 0.0191 * #&gt; did -2.520e+09 1.456e+09 -1.731 0.0882 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.953e+09 on 66 degrees of freedom #&gt; Multiple R-squared: 0.08273, Adjusted R-squared: 0.04104 #&gt; F-statistic: 1.984 on 3 and 66 DF, p-value: 0.1249 The did coefficient is the differences-in-differences estimator. Treat has a negative effect 26.5.3 Example by Card and Krueger (1993) found that increase in minimum wage increases employment Experimental Setting: New Jersey (treatment) increased minimum wage Penn (control) did not increase minimum wage After Before Treatment NJ A B A - B Control PA C D C - D A - C B - D (A - B) - (C - D) where A - B = treatment effect + effect of time (additive) C - D = effect of time (A - B) - (C - D) = dif-n-dif The identifying assumptions: Can’t have switchers PA is the control group is a good counter factual is what NJ would look like if they hadn’t had the treatment \\[ Y_{jt} = \\beta_0 + NJ_j \\beta_1 + POST_t \\beta_2 + (NJ_j \\times POST_t)\\beta_3+ X_{jt}\\beta_4 + \\epsilon_{jt} \\] where \\(j\\) = restaurant \\(NJ\\) = dummy where \\(1 = NJ\\), and \\(0 = PA\\) \\(POST\\) = dummy where \\(1 = post\\), and \\(0 = pre\\) Notes: We don’t need \\(\\beta_4\\) in our model to have unbiased \\(\\beta_3\\), but including it would give our coefficients efficiency If we use \\(\\Delta Y_{jt}\\) as the dependent variable, we don’t need \\(POST_t \\beta_2\\) anymore Alternative model specification is that the authors use NJ high wage restaurant as control group (still choose those that are close to the border) The reason why they can’t control for everything (PA + NJ high wage) is because it’s hard to interpret the causal treatment Dif-n-dif utilizes similarity in pretrend of the dependent variables. However, this is neither a necessary nor sufficient for the identifying assumption. It’s not sufficient because they can have multiple treatments (technically, you could include more control, but your treatment can’t interact) It’s not necessary because trends can be parallel after treatment However, we can’t never be certain; we just try to find evidence consistent with our theory so that dif-n-dif can work. Notice that we don’t need before treatment the levels of the dependent variable to be the same (e.g., same wage average in both NJ and PA), dif-n-dif only needs pre-trend (i.e., slope) to be the same for the two groups. 26.5.4 Example by Butcher, McEwan, and Weerapana (2014) Theory: Highest achieving students are usually in hard science. Why? Hard to give students students the benefit of doubt for hard science How unpleasant and how easy to get a job. Degrees with lower market value typically want to make you feel more pleasant Under OLS \\[ E_{ij} = \\beta_0 + X_i \\beta_1 + G_j \\beta_2 + \\epsilon_{ij} \\] where \\(X_i\\) = student attributes \\(\\beta_2\\) = causal estimate (from grade change) \\(E_{ij}\\) = Did you choose to enroll in major \\(j\\) \\(G_j\\) = grade given in major \\(j\\) Examine \\(\\hat{\\beta}_2\\) Negative bias: Endogenous response because department with lower enrollment rate will give better grade Positive bias: hard science is already having best students (i.e., ability), so if they don’t their grades can be even lower Under dif-n-dif \\[ Y_{idt} = \\beta_0 + POST_t \\beta_1 + Treat_d \\beta_2 + (POST_t \\times Treat_d)\\beta_3 + X_{idt} + \\epsilon_{idt} \\] where \\(Y_{idt}\\) = grade average Intercept Treat Post Treat*Post Treat Pre 1 1 0 0 Treat Post 1 1 1 1 Control Pre 1 0 0 0 Control Post 1 0 1 0 Average for pre-control \\(\\beta_0\\) A more general specification of the dif-n-dif is that \\[ Y_{idt} = \\alpha_0 + (POST_t \\times Treat_d) \\alpha_1 + \\theta_d + \\delta_t + X_{idt} + u_{idt} \\] where \\((\\theta_d + \\delta_t)\\) richer , more df than \\(Treat_d \\beta_2 + Post_t \\beta_1\\) (because fixed effects subsume Post and treat) \\(\\alpha_1\\) should be equivalent to \\(\\beta_3\\) (if your model assumptions are correct) References "],["one-difference.html", "26.6 One Difference", " 26.6 One Difference The regression formula is as follows (Liaukonytė, Tuchman, and Zhu 2023): \\[ y_{ut} = \\beta \\text{Post}_t + \\gamma_u + \\gamma_w(t) + \\gamma_l + \\gamma_g(u)p(t) + \\epsilon_{ut} \\] where \\(y_{ut}\\): Outcome of interest for unit u in time t. \\(\\text{Post}_t\\): Dummy variable representing a specific post-event period. \\(\\beta\\): Coefficient measuring the average change in the outcome after the event relative to the pre-period. \\(\\gamma_u\\): Fixed effects for each unit. \\(\\gamma_w(t)\\): Time-specific fixed effects to account for periodic variations. \\(\\gamma_l\\): Dummy variable for a specific significant period (e.g., a major event change). \\(\\gamma_g(u)p(t)\\): Group x period fixed effects for flexible trends that may vary across different categories (e.g., geographical regions) and periods. \\(\\epsilon_{ut}\\): Error term. This model can be used to analyze the impact of an event on the outcome of interest while controlling for various fixed effects and time-specific variations, but using units themselves pre-treatment as controls. References "],["two-way-fixed-effects.html", "26.7 Two-way Fixed-effects", " 26.7 Two-way Fixed-effects A generalization of the dif-n-dif model is the two-way fixed-effects models where you have multiple groups and time effects. But this is not a designed-based, non-parametric causal estimator (Imai and Kim 2021) When applying TWFE to multiple groups and multiple periods, the supposedly causal coefficient is the weighted average of all two-group/two-period DiD estimators in the data where some of the weights can be negative. More specifically, the weights are proportional to group sizes and treatment indicator’s variation in each pair, where units in the middle of the panel have the highest weight. The canonical/standard TWFE only works when Effects are homogeneous across units and across time periods (i.e., no dynamic changes in the effects of treatment). See (Goodman-Bacon 2021; Clément De Chaisemartin and d’Haultfoeuille 2020; L. Sun and Abraham 2021; Borusyak, Jaravel, and Spiess 2021) for details. Similarly, it relies on the assumption of linear additive effects (Imai and Kim 2021) Have to argue why treatment heterogeneity is not a problem (e.g., plot treatment timing and decompose treatment coefficient using Goodman-Bacon Decomposition) know the percentage of observation are never treated (because as the never-treated group increases, the bias of TWFE decreases, with 80% sample to be never-treated, bias is negligible). The problem is worsen when you have long-run effects. Need to manually drop two relative time periods if everyone is eventually treated (to avoid multicollinearity). Programs might do this randomly and if it chooses to drop a post-treatment period, it will create biases. The choice usually -1, and -2 periods. Treatment heterogeneity can come in because (1) it might take some time for a treatment to have measurable changes in outcomes or (2) for each period after treatment, the effect can be different (phase in or increasing effects). 2 time periods. Within this setting, TWFE works because, using the baseline (e.g., control units where their treatment status is unchanged across time periods), the comparison can be Good for Newly treated units vs. control Newly treated units vs not-yet treated Bad for Newly treated vs. already treated (because already treated cannot serve as the potential outcome for the newly treated). Strict exogeneity (i.e., time-varying confounders, feedback from past outcome to treatment) (Imai and Kim 2019) Specific functional forms (i.e., treatment effect homogeneity and no carryover effects or anticipation effects) (Imai and Kim 2019) Note: Notation for this section is consistent with (2020) \\[ Y_{it} = \\alpha_i + \\lambda_t + \\tau W_{it} + \\beta X_{it} + \\epsilon_{it} \\] where \\(Y_{it}\\) is the outcome \\(\\alpha_i\\) is the unit FE \\(\\lambda_t\\) is the time FE \\(\\tau\\) is the causal effect of treatment \\(W_{it}\\) is the treatment indicator \\(X_{it}\\) are covariates When \\(T = 2\\), the TWFE is the traditional DiD model Under the following assumption, \\(\\hat{\\tau}_{OLS}\\) is unbiased: homogeneous treatment effect parallel trends assumptions linear additive effects (Imai and Kim 2021) Remedies for TWFE’s shortcomings (Goodman-Bacon 2021): diagnostic robustness tests of the TWFE DiD and identify influential observations to the DiD estimate (Goodman-Bacon Decomposition) (Callaway and Sant’Anna 2021): 2-step estimation with a bootstrap procedure that can account for autocorrelation and clustering, the parameters of interest are the group-time average treatment effects, where each group is defined by when it was first treated (Multiple periods and variation in treatment timing) Comparing post-treatment outcomes fo groups treated in a period against a similar group that is never treated (using matching). Treatment status cannot switch (once treated, stay treated for the rest of the panel) Package: did (L. Sun and Abraham 2021): a specialization of (Callaway and Sant’Anna 2021) in the event-study context. They include lags and leads in their design have cohort-specific estimates (similar to group-time estimates in (Callaway and Sant’Anna 2021) They propose the “interaction-weighted” estimator. Package: fixest (Imai and Kim 2021) Different from (Callaway and Sant’Anna 2021) because they allow units to switch in and out of treatment. Based on matching methods, to have weighted TWFE Package: wfe and PanelMatch (Gardner 2022): two-stage DiD did2s In cases with an unaffected unit (i.e., never-treated), using the exposure-adjusted difference-in-differences estimators can recover the average treatment effect (Clément De Chaisemartin and d’Haultfoeuille 2020). However, if you want to see the treatment effect heterogeneity (in cases where the true heterogeneous treatment effects vary by the exposure rate), exposure-adjusted did still fails (L. Sun and Shapiro 2022). (2020): see below To be robust against time- and unit-varying effects We can use the reshaped inverse probability weighting (RIPW)- TWFE estimator With the following assumptions: SUTVA Binary treatment: \\(\\mathbf{W}_i = (W_{i1}, \\dots, W_{it})\\) where \\(\\mathbf{W}_i \\sim \\mathbf{\\pi}_i\\) generalized propensity score (i.e., each person treatment likelihood follow \\(\\pi\\) regardless of the period) Then, the unit-time specific effect is \\(\\tau_{it} = Y_{it}(1) - Y_{it}(0)\\) Then the Doubly Average Treatment Effect (DATE) is \\[ \\tau(\\xi) = \\sum_{T=1}^T \\xi_t \\left(\\frac{1}{n} \\sum_{i = 1}^n \\tau_{it} \\right) \\] where \\(\\frac{1}{n} \\sum_{i = 1}^n \\tau_{it}\\) is the unweighted effect of treatment across units (i.e., time-specific ATE). \\(\\xi = (\\xi_1, \\dots, \\xi_t)\\) are user-specific weights for each time period. This estimand is called DATE because it’s weighted (averaged) across both time and units. A special case of DATE is when both time and unit-weights are equal \\[ \\tau_{eq} = \\frac{1}{nT} \\sum_{t=1}^T \\sum_{i = 1}^n \\tau_{it} \\] Borrowing the idea of inverse propensity-weighted least squares estimator in the cross-sectional case that we reweight the objective function via the treatment assignment mechanism: \\[ \\hat{\\tau} \\triangleq \\arg \\min_{\\tau} \\sum_{i = 1}^n (Y_i -\\mu - W_i \\tau)^2 \\frac{1}{\\pi_i (W_i)} \\] where the first term is the least squares objective the second term is the propensity score In the panel data case, the IPW estimator will be \\[ \\hat{\\tau}_{IPW} \\triangleq \\arg \\min_{\\tau} \\sum_{i = 1}^n \\sum_{t =1}^T (Y_{i t}-\\alpha_i - \\lambda_t - W_{it} \\tau)^2 \\frac{1}{\\pi_i (W_i)} \\] Then, to have DATE that users can specify the structure of time weight, we use reshaped IPW estimator (2020) \\[ \\hat{\\tau}_{RIPW} (\\Pi) \\triangleq \\arg \\min_{\\tau} \\sum_{i = 1}^n \\sum_{t =1}^T (Y_{i t}-\\alpha_i - \\lambda_t - W_{it} \\tau)^2 \\frac{\\Pi(W_i)}{\\pi_i (W_i)} \\] where it’s a function of a data-independent distribution \\(\\Pi\\) that depends on the support of the treatment path \\(\\mathbb{S} = \\cup_i Supp(W_i)\\) This generalization can transform to IPW-TWFE estimator when \\(\\Pi \\sim Unif(\\mathbb{S})\\) randomized experiment when \\(\\Pi = \\pi_i\\) To choose \\(\\Pi\\), we don’t need to data, we just need possible assignments in your setting. For most practical problems (DiD, staggered, transient), we have closed form solutions For generic solver, we can use nonlinear programming (e..g, BFGS algorithm) As argued in (Imai and Kim 2021) that TWFE is not a non-parametric approach, it can be subjected to incorrect model assumption (i.e., model dependence). Hence, they advocate for matching methods for time-series cross-sectional data (Imai and Kim 2021) Use wfe and PanelMatch to apply their paper. This package is based on (Somaini and Wolak 2016) # dataset library(bacondecomp) df &lt;- bacondecomp::castle # devtools::install_github(&quot;paulosomaini/xtreg2way&quot;) library(xtreg2way) # output &lt;- xtreg2way(y, # data.frame(x1, x2), # iid, # tid, # w, # noise = &quot;1&quot;, # se = &quot;1&quot;) # equilvalently output &lt;- xtreg2way(l_homicide ~ post, df, iid = df$state, # group id tid = df$year, # time id # w, # vector of weight se = &quot;1&quot;) output$betaHat #&gt; [,1] #&gt; l_homicide 0.08181162 output$aVarHat #&gt; [,1] #&gt; [1,] 0.003396724 # to save time, you can use your structure in the # last output for a new set of variables # output2 &lt;- xtreg2way(y, x1, struc=output$struc) Standard errors estimation options Set Estimation se = \"0\" Assume homoskedasticity and no within group correlation or serial correlation se = \"1\" (default) robust to heteroskadasticity and serial correlation (Arellano 1987) se = \"2\" robust to heteroskedasticity, but assumes no correlation within group or serial correlation se = \"11\" Aerllano SE with df correction performed by Stata xtreg (Somaini and Wolak 2021) Alternatively, you can also do it manually or with the plm package, but you have to be careful with how the SEs are estimated library(multiwayvcov) # get vcov matrix library(lmtest) # robust SEs estimation # manual output3 &lt;- lm(l_homicide ~ post + factor(state) + factor(year), data = df) # get variance-covariance matrix vcov_tw &lt;- multiwayvcov::cluster.vcov(output3, cbind(df$state, df$year), use_white = F, df_correction = F) # get coefficients coeftest(output3, vcov_tw)[2,] #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; 0.08181162 0.05671410 1.44252696 0.14979397 # using the plm package library(plm) output4 &lt;- plm(l_homicide ~ post, data = df, index = c(&quot;state&quot;, &quot;year&quot;), model = &quot;within&quot;, effect = &quot;twoways&quot;) # get coefficients coeftest(output4, vcov = vcovHC, type = &quot;HC1&quot;) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; post 0.081812 0.057748 1.4167 0.1572 As you can see, differences stem from SE estimation, not the coefficient estimate. References "],["multiple-periods-and-variation-in-treatment-timing.html", "26.8 Multiple periods and variation in treatment timing", " 26.8 Multiple periods and variation in treatment timing This is an extension of the DiD framework to settings where you have more than 2 time periods different treatment timing When treatment effects are heterogeneous across time or units, the standard Two-way Fixed-effects is inappropriate. Notation is consistent with did package (Callaway and Sant’Anna 2021) \\(Y_{it}(0)\\) is the potential outcome for unit \\(i\\) \\(Y_{it}(g)\\) is the potential outcome for unit \\(i\\) in time period \\(t\\) if it’s treated in period \\(g\\) \\(Y_{it}\\) is the observed outcome for unit \\(i\\) in time period \\(t\\) \\[ Y_{it} = \\begin{cases} Y_{it} = Y_{it}(0) &amp; \\forall i \\in \\text{never-treated group} \\\\ Y_{it} = 1\\{G_i &gt; t\\} Y_{it}(0) + 1\\{G_i \\le t \\}Y_{it}(G_i) &amp; \\forall i \\in \\text{other groups} \\end{cases} \\] \\(G_i\\) is the time period when \\(i\\) is treated \\(C_i\\) is a dummy when \\(i\\) belongs to the never-treated group \\(D_{it}\\) is a dummy for whether \\(i\\) is treated in period \\(t\\) Assumptions: Staggered treatment adoption: once treated, a unit cannot be untreated (revert) Parallel trends assumptions (conditional on covariates): Based on never-treated units: \\(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\\) Without treatment, the average potential outcomes for group \\(g\\) equals the average potential outcomes for the never-treated group (i.e., control group), which means that we have (1) enough data on the never-treated group (2) the control group is similar to the eventually treated group. Based on not-yet treated units: \\(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \\neq g]\\) Not-yet treated units by time \\(s\\) ( \\(s \\ge t\\)) can be used as comparison groups to calculate the average treatment effects for the group first treated in time \\(g\\) Additional assumption: pre-treatment trends across groups (Marcus and Sant’Anna 2021) Random sampling Irreversibility of treatment (once treated, cannot be untreated) Overlap (the treatment propensity \\(e \\in [0,1]\\)) Group-Time ATE This is the equivalent of the average treatment effect in the standard case (2 groups, 2 periods) under multiple time periods. \\[ ATT(g,t) = E[Y_t(g) - Y_t(0) |G = g] \\] which is the average treatment effect for group \\(g\\) in period \\(t\\) Identification: When the parallel trends assumption based on Never-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \\forall t \\ge g\\) Not-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \\neq g] \\forall t \\ge g\\) Identification: when the parallel trends assumption only holds conditional on covariates and based on Never-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \\forall t \\ge g\\) Not-yet-treated units: \\(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \\neq g] \\forall t \\ge g\\) This is plausible when you have suspected selection bias that can be corrected by using covariates (i.e., very much similar to matching methods to have plausible parallel trends). Possible parameters of interest are: Average treatment effect per group \\[ \\theta_S(g) = \\frac{1}{\\tau - g + 1} \\sum_{t = 2}^\\tau \\mathbb{1} \\{ \\le t \\} ATT(g,t) \\] Average treatment effect across groups (that were treated) (similar to average treatment effect on the treated in the canonical case) \\[ \\theta_S^O := \\sum_{g=2}^\\tau \\theta_S(g) P(G=g) \\] Average treatment effect dynamics (i.e., average treatment effect for groups that have been exposed to the treatment for \\(e\\) time periods): \\[ \\theta_D(e) := \\sum_{g=2}^\\tau \\mathbb{1} \\{g + e \\le \\tau \\}ATT(g,g + e) P(G = g|G + e \\le \\tau) \\] Average treatment effect in period \\(t\\) for all groups that have treated by period \\(t\\)) \\[ \\theta_C(t) = \\sum_{g=2}^\\tau \\mathbb{1}\\{g \\le t\\} ATT(g,t) P(G = g|g \\le t) \\] Average treatment effect by calendar time \\[ \\theta_C = \\frac{1}{\\tau-1}\\sum_{t=2}^\\tau \\theta_C(t) \\] References "],["staggered-dif-n-dif.html", "26.9 Staggered Dif-n-dif", " 26.9 Staggered Dif-n-dif See Wing et al. (2024) checklist. Recommendations by Baker, Larcker, and Wang (2022) TWFE DiD regressions are suitable for single treatment periods or when treatment effects are homogeneous, provided there’s a solid rationale for effect homogeneity. For TWFE staggered DiD, researchers should evaluate bias risks, plot treatment timings to check for variations, and use decompositions like Goodman-Bacon (2021) when possible. If decompositions aren’t feasible (e.g., unbalanced panel), the percentage of never-treated units can indicate bias severity. Expected treatment effect variability should also be discussed. In TWFE staggered DiD event studies, avoid binning time periods without evidence of uniform effects. Use full relative-time indicators, justify reference periods, and be wary of multicollinearity causing bias. To address treatment timing and bias concerns, use alternative estimators like stacked regressions, L. Sun and Abraham (2021), Callaway and Sant’Anna (2021), or separate regressions for each event with “clean” controls. Justify the selection of comparison groups (not-yet treated, last treated, never treated) and ensure the parallel-trends assumption holds, especially when anticipating no effects for certain groups. Notes: When subjects are treated at different point in time (variation in treatment timing across units), we have to use staggered DiD (also known as DiD event study or dynamic DiD). For design where a treatment is applied and units are exposed to this treatment at all time afterward, see (Athey and Imbens 2022) For example, basic design (Stevenson and Wolfers 2006) \\[ \\begin{aligned} Y_{it} &amp;= \\sum_k \\beta_k Treatment_{it}^k + \\sum_i \\eta_i State_i \\\\ &amp;+ \\sum_t \\lambda_t Year_t + Controls_{it} + \\epsilon_{it} \\end{aligned} \\] where \\(Treatment_{it}^k\\) is a series of dummy variables equal to 1 if state \\(i\\) is treated \\(k\\) years ago in period \\(t\\) SE is usually clustered at the group level (occasionally time level). To avoid collinearity, the period right before treatment is usually chosen to drop. The more general form of TWFE (L. Sun and Abraham 2021): First, define the relative period bin indicator as \\[ D_{it}^l = \\mathbf{1}(t - E_i = l) \\] where it’s an indicator function of unit \\(i\\) being \\(l\\) periods from its first treatment at time \\(t\\) Static specification \\[ Y_{it} = \\alpha_i + \\lambda_t + \\mu_g \\sum_{l \\ge0} D_{it}^l + \\epsilon_{it} \\] where \\(\\alpha_i\\) is the the unit FE \\(\\lambda_t\\) is the time FE \\(\\mu_g\\) is the coefficient of interest \\(g = [0,T)\\) we exclude all periods before first adoption. Dynamic specification \\[ Y_{it} = \\alpha_i + \\lambda_t + \\sum_{\\substack{l = -K \\\\ l \\neq -1}}^{L} \\mu_l D_{it}^l + \\epsilon_{it} \\] where we have to exclude some relative periods to avoid multicollinearity problem (e.g., either period right before treatment, or the treatment period). In this setting, we try to show that the treatment and control groups are not statistically different (i.e., the coefficient estimates before treatment are not different from 0) to show pre-treatment parallel trends. However, this two-way fixed effects design has been criticized by L. Sun and Abraham (2021); Callaway and Sant’Anna (2021); Goodman-Bacon (2021). When researchers include leads and lags of the treatment to see the long-term effects of the treatment, these leads and lags can be biased by effects from other periods, and pre-trends can falsely arise due to treatment effects heterogeneity. Applying the new proposed method, finance and accounting researchers find that in many cases, the causal estimates turn out to be null (Baker, Larcker, and Wang 2022). Assumptions of Staggered DID Rollout Exogeneity (i.e., exogeneity of treatment adoption): if the treatment is randomly implemented over time (i.e., unrelated to variables that could also affect our dependent variables) Evidence: Regress adoption on pre-treatment variables. And if you find evidence of correlation, include linear trends interacted with pre-treatment variables (Hoynes and Schanzenbach 2009) Evidence: (Deshpande and Li 2019, 223) Treatment is random: Regress treatment status at the unit level to all pre-treatment observables. If you have some that are predictive of treatment status, you might have to argue why it’s not a worry. At best, you want this. Treatment timing is random: Conditional on treatment, regress timing of the treatment on pre-treatment observables. At least, you want this. No confounding events Exclusion restrictions No-anticipation assumption: future treatment time do not affect current outcomes Invariance-to-history assumption: the time a unit under treatment does not affect the outcome (i.e., the time exposed does not matter, just whether exposed or not). This presents causal effect of early or late adoption on the outcome. And all the assumptions in listed in the Multiple periods and variation in treatment timing Auxiliary assumptions: Constant treatment effects across units Constant treatment effect over time Random sampling Effect Additivity Remedies for staggered DiD (Baker, Larcker, and Wang 2022): Each treated cohort is compared to appropriate controls (not-yet-treated, never-treated) (Goodman-Bacon 2021) (Callaway and Sant’Anna 2021) consistent for average ATT. more complicated but also more flexible than (L. Sun and Abraham 2021) (L. Sun and Abraham 2021) (a special case of (Callaway and Sant’Anna 2021)) (Clément De Chaisemartin and d’Haultfoeuille 2020) (Borusyak, Jaravel, and Spiess 2021) Stacked DID (biased but simple): (Gormley and Matsa 2011) (Cengiz et al. 2019) (Deshpande and Li 2019) 26.9.1 Stacked DID Notations following these slides \\[ Y_{it} = \\beta_{FE} D_{it} + A_i + B_t + \\epsilon_{it} \\] where \\(A_i\\) is the group fixed effects \\(B_t\\) is the period fixed effects Steps Choose Event Window Enumerate Sub-experiments Define Inclusion Criteria Stack Data Specify Estimating Equation Event Window Let \\(\\kappa_a\\) be the length of the pre-event window \\(\\kappa_b\\) be the length of the post-event window By setting a common event window for the analysis, we essentially exclude all those events that do not meet this criteria. Sub-experiments Let \\(T_1\\) be the earliest period in the dataset \\(T_T\\) be the last period in the dataset Then, the collection of all policy adoption periods that are under our event window is \\[ \\Omega_A = \\{ A_i |T_1 + \\kappa_a \\le A_i \\le T_T - \\kappa_b\\} \\] where these events exist at least \\(\\kappa_a\\) periods after the earliest period at least \\(\\kappa_b\\) periods before the last period Let \\(d = 1, \\dots, D\\) be the index column of the sub-experiments in \\(\\Omega_A\\) and \\(\\omega_d\\) be the event date of the d-th sub-experiment (e.g., \\(\\omega_1\\) = adoption date of the 1st experiment) Inclusion Criteria Valid treated Units Within sub-experiment \\(d\\), all treated units have the same adoption date This makes sure a unit can only serve as a treated unit in only 1 sub-experiment Clean controls Only units satisfying \\(A_i &gt;\\omega_d + \\kappa_b\\) are included as controls in sub-experiment d This ensures controls are only never treated units units that are treated in far future But a unit can be control unit in multiple sub-experiments (need to correct SE) Valid Time Periods All observations within sub-experiment d are from time periods within the sub-experiment’s event window This ensures in sub-experiment d, only observations satisfying \\(\\omega_d - \\kappa_a \\le t \\le \\omega_d + \\kappa_b\\) are included library(did) library(tidyverse) library(fixest) data(base_stagg) # first make the stacked datasets # get the treatment cohorts cohorts &lt;- base_stagg %&gt;% select(year_treated) %&gt;% # exclude never-treated group filter(year_treated != 10000) %&gt;% unique() %&gt;% pull() # make formula to create the sub-datasets getdata &lt;- function(j, window) { #keep what we need base_stagg %&gt;% # keep treated units and all units not treated within -5 to 5 # keep treated units and all units not treated within -window to window filter(year_treated == j | year_treated &gt; j + window) %&gt;% # keep just year -window to window filter(year &gt;= j - window &amp; year &lt;= j + window) %&gt;% # create an indicator for the dataset mutate(df = j) } # get data stacked stacked_data &lt;- map_df(cohorts, ~ getdata(., window = 5)) %&gt;% mutate(rel_year = if_else(df == year_treated, time_to_treatment, NA_real_)) %&gt;% fastDummies::dummy_cols(&quot;rel_year&quot;, ignore_na = TRUE) %&gt;% mutate(across(starts_with(&quot;rel_year_&quot;), ~ replace_na(., 0))) # get stacked value stacked &lt;- feols( y ~ `rel_year_-5` + `rel_year_-4` + `rel_year_-3` + `rel_year_-2` + rel_year_0 + rel_year_1 + rel_year_2 + rel_year_3 + rel_year_4 + rel_year_5 | id ^ df + year ^ df, data = stacked_data )$coefficients stacked_se = feols( y ~ `rel_year_-5` + `rel_year_-4` + `rel_year_-3` + `rel_year_-2` + rel_year_0 + rel_year_1 + rel_year_2 + rel_year_3 + rel_year_4 + rel_year_5 | id ^ df + year ^ df, data = stacked_data )$se # add in 0 for omitted -1 stacked &lt;- c(stacked[1:4], 0, stacked[5:10]) stacked_se &lt;- c(stacked_se[1:4], 0, stacked_se[5:10]) cs_out &lt;- att_gt( yname = &quot;y&quot;, data = base_stagg, gname = &quot;year_treated&quot;, idname = &quot;id&quot;, # xformla = &quot;~x1&quot;, tname = &quot;year&quot; ) cs &lt;- aggte( cs_out, type = &quot;dynamic&quot;, min_e = -5, max_e = 5, bstrap = FALSE, cband = FALSE ) res_sa20 = feols(y ~ sunab(year_treated, year) | id + year, base_stagg) sa = tidy(res_sa20)[5:14, ] %&gt;% pull(estimate) sa = c(sa[1:4], 0, sa[5:10]) sa_se = tidy(res_sa20)[6:15, ] %&gt;% pull(std.error) sa_se = c(sa_se[1:4], 0, sa_se[5:10]) compare_df_est = data.frame( period = -5:5, cs = cs$att.egt, sa = sa, stacked = stacked ) compare_df_se = data.frame( period = -5:5, cs = cs$se.egt, sa = sa_se, stacked = stacked_se ) compare_df_longer &lt;- compare_df_est %&gt;% pivot_longer(!period, names_to = &quot;estimator&quot;, values_to = &quot;est&quot;) %&gt;% full_join(compare_df_se %&gt;% pivot_longer(!period, names_to = &quot;estimator&quot;, values_to = &quot;se&quot;)) %&gt;% mutate(upper = est + 1.96 * se, lower = est - 1.96 * se) ggplot(compare_df_longer) + geom_ribbon(aes( x = period, ymin = lower, ymax = upper, group = estimator )) + geom_line(aes( x = period, y = est, group = estimator, col = estimator ), linewidth = 1) + causalverse::ama_theme() Stack Data Estimating Equation \\[ Y_{itd} = \\beta_0 + \\beta_1 T_{id} + \\beta_2 P_{td} + \\beta_3 (T_{id} \\times P_{td}) + \\epsilon_{itd} \\] where \\(T_{id}\\) = 1 if unit \\(i\\) is treated in sub-experiment \\(d\\), 0 if control \\(P_{td}\\) = 1 if it’s the period after the treatment in sub-experiment \\(d\\) Equivalently, \\[ Y_{itd} = \\beta_3 (T_{id} \\times P_{td}) + \\theta_{id} + \\gamma_{td} + \\epsilon_{itd} \\] \\(\\beta_3\\) averages all the time-varying effects into a single number (can’t see the time-varying effects) Stacked Event Study Let \\(YSE_{td} = t - \\omega_d\\) be the “time since event” variable in sub-experiment \\(d\\) Then, \\(YSE_{td} = -\\kappa_a, \\dots, 0, \\dots, \\kappa_b\\) in every sub-experiment In each sub-experiment, we can fit \\[ Y_{it}^d = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j^d \\times 1(TSE_{td} = j) + \\sum_{m = -\\kappa_a}^{\\kappa_b} \\delta_j^d (T_{id} \\times 1 (TSE_{td} = j)) + \\theta_i^d + \\epsilon_{it}^d \\] Different set of event study coefficients in each sub-experiment \\[ Y_{itd} = \\sum_{j = -\\kappa_a}^{\\kappa_b} \\beta_j \\times 1(TSE_{td} = j) + \\sum_{m = -\\kappa_a}^{\\kappa_b} \\delta_j (T_{id} \\times 1 (TSE_{td} = j)) + \\theta_{id} + \\epsilon_{itd} \\] Clustering Clustered at the unit x sub-experiment level (Cengiz et al. 2019) Clustered at the unit level (Deshpande and Li 2019) 26.9.2 Goodman-Bacon Decomposition Paper: (Goodman-Bacon 2021) For an excellent explanation slides by the author, see Takeaways: A pairwise DID (\\(\\tau\\)) gets more weight if the change is close to the middle of the study window A pairwise DID (\\(\\tau\\)) gets more weight if it includes more observations. Code from bacondecomp vignette library(bacondecomp) library(tidyverse) data(&quot;castle&quot;) castle &lt;- bacondecomp::castle %&gt;% dplyr::select(&quot;l_homicide&quot;, &quot;post&quot;, &quot;state&quot;, &quot;year&quot;) head(castle) #&gt; l_homicide post state year #&gt; 1 2.027356 0 Alabama 2000 #&gt; 2 2.164867 0 Alabama 2001 #&gt; 3 1.936334 0 Alabama 2002 #&gt; 4 1.919567 0 Alabama 2003 #&gt; 5 1.749841 0 Alabama 2004 #&gt; 6 2.130440 0 Alabama 2005 df_bacon &lt;- bacon( l_homicide ~ post, data = castle, id_var = &quot;state&quot;, time_var = &quot;year&quot; ) #&gt; type weight avg_est #&gt; 1 Earlier vs Later Treated 0.05976 -0.00554 #&gt; 2 Later vs Earlier Treated 0.03190 0.07032 #&gt; 3 Treated vs Untreated 0.90834 0.08796 # weighted average of the decomposition sum(df_bacon$estimate * df_bacon$weight) #&gt; [1] 0.08181162 Two-way Fixed effect estimate library(broom) fit_tw &lt;- lm(l_homicide ~ post + factor(state) + factor(year), data = bacondecomp::castle) head(tidy(fit_tw)) #&gt; # A tibble: 6 × 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 1.95 0.0624 31.2 2.84e-118 #&gt; 2 post 0.0818 0.0317 2.58 1.02e- 2 #&gt; 3 factor(state)Alaska -0.373 0.0797 -4.68 3.77e- 6 #&gt; 4 factor(state)Arizona 0.0158 0.0797 0.198 8.43e- 1 #&gt; 5 factor(state)Arkansas -0.118 0.0810 -1.46 1.44e- 1 #&gt; 6 factor(state)California -0.108 0.0810 -1.34 1.82e- 1 Hence, naive TWFE fixed effect equals the weighted average of the Bacon decomposition (= 0.08). library(ggplot2) ggplot(df_bacon) + aes( x = weight, y = estimate, # shape = factor(type), color = type ) + labs(x = &quot;Weight&quot;, y = &quot;Estimate&quot;, shape = &quot;Type&quot;) + geom_point() + causalverse::ama_theme() With time-varying controls that can identify variation within-treatment timing group, the”early vs. late” and “late vs. early” estimates collapse to just one estimate (i.e., both treated). 26.9.3 DID with in and out treatment condition 26.9.3.1 Panel Match Imai and Kim (2021) This case generalizes the staggered adoption setting, allowing units to vary in treatment over time. For \\(N\\) units across \\(T\\) time periods (with potentially unbalanced panels), let \\(X_{it}\\) represent treatment and \\(Y_{it}\\) the outcome for unit \\(i\\) at time \\(t\\). We use the two-way linear fixed effects model: \\[ Y_{it} = \\alpha_i + \\gamma_t + \\beta X_{it} + \\epsilon_{it} \\] for \\(i = 1, \\dots, N\\) and \\(t = 1, \\dots, T\\). Here, \\(\\alpha_i\\) and \\(\\gamma_t\\) are unit and time fixed effects. They capture time-invariant unit-specific and unit-invariant time-specific unobserved confounders, respectively. We can express these as \\(\\alpha_i = h(\\mathbf{U}_i)\\) and \\(\\gamma_t = f(\\mathbf{V}_t)\\), with \\(\\mathbf{U}_i\\) and \\(\\mathbf{V}_t\\) being the confounders. The model doesn’t assume a specific form for \\(h(.)\\) and \\(f(.)\\), but that they’re additive and separable given binary treatment. The least squares estimate of \\(\\beta\\) leverages the covariance in outcome and treatment (Imai and Kim 2021, 406). Specifically, it uses the within-unit and within-time variations. Many researchers prefer the two fixed effects (2FE) estimator because it adjusts for both types of unobserved confounders without specific functional-form assumptions, but this is wrong (Imai and Kim 2019). We do need functional-form assumption (i.e., linearity assumption) for the 2FE to work (Imai and Kim 2021, 406) Two-Way Matching Estimator: It can lead to mismatches; units with the same treatment status get matched when estimating counterfactual outcomes. Observations need to be matched with opposite treatment status for correct causal effects estimation. Mismatches can cause attenuation bias. The 2FE estimator adjusts for this bias using the factor \\(K\\), which represents the net proportion of proper matches between observations with opposite treatment status. Weighting in 2FE: Observation \\((i,t)\\) is weighted based on how often it acts as a control unit. The weighted 2FE estimator still has mismatches, but fewer than the standard 2FE estimator. Adjustments are made based on observations that neither belong to the same unit nor the same time period as the matched observation. This means there are challenges in adjusting for unit-specific and time-specific unobserved confounders under the two-way fixed effect framework. Equivalence &amp; Assumptions: Equivalence between the 2FE estimator and the DID estimator is dependent on the linearity assumption. The multi-period DiD estimator is described as an average of two-time-period, two-group DiD estimators applied during changes from control to treatment. Comparison with DiD: In simple settings (two time periods, treatment given to one group in the second period), the standard nonparametric DiD estimator equals the 2FE estimator. This doesn’t hold in multi-period DiD designs where units change treatment status multiple times at different intervals. Contrary to popular belief, the unweighted 2FE estimator isn’t generally equivalent to the multi-period DiD estimator. While the multi-period DiD can be equivalent to the weighted 2FE, some control observations may have negative regression weights. Conclusion: Justifying the 2FE estimator as the DID estimator isn’t warranted without imposing the linearity assumption. Application (Imai, Kim, and Wang 2021) Matching Methods: Enhance the validity of causal inference. Reduce model dependence and provide intuitive diagnostics (Ho et al. 2007) Rarely utilized in analyzing time series cross-sectional data. The proposed matching estimators are more robust than the standard two-way fixed effects estimator, which can be biased if mis-specified Better than synthetic controls (e.g., (Xu 2017)) because it needs less data to achieve good performance and and adapt the the context of unit switching treatment status multiple times. Notes: Potential carryover effects (treatment may have a long-term effect), leading to post-treatment bias. Proposed Approach: Treated observations are matched with control observations from other units in the same time period with the same treatment history up to a specified number of lags. Standard matching and weighting techniques are employed to further refine the matched set. Apply a DiD estimator to adjust for time trend. The goal is to have treated and matched control observations with similar covariate values. Assessment: The quality of matches is evaluated through covariate balancing. Estimation: Both short-term and long-term average treatment effects on the treated (ATT) are estimated. library(PanelMatch) Treatment Variation plot Visualize the variation of the treatment across space and time Aids in discerning whether the treatment fluctuates adequately over time and units or if the variation is primarily clustered in a subset of data. DisplayTreatment( unit.id = &quot;wbcode2&quot;, time.id = &quot;year&quot;, legend.position = &quot;none&quot;, xlab = &quot;year&quot;, ylab = &quot;Country Code&quot;, treatment = &quot;dem&quot;, hide.x.tick.label = TRUE, hide.y.tick.label = TRUE, # dense.plot = TRUE, data = dem ) Select \\(F\\) (i.e., the number of leads - time periods after treatment). Driven by what authors are interested in estimating: \\(F = 0\\) is the contemporaneous effect (short-term effect) \\(F = n\\) is the the treatment effect on the outcome two time periods after the treatment. (cumulative or long-term effect) Select \\(L\\) (number of lags to adjust). Driven by the identification assumption. Balances bias-variance tradeoff. Higher \\(L\\) values increase credibility but reduce efficiency by limiting potential matches. Model assumption: No spillover effect assumed. Carryover effect allowed up to \\(L\\) periods. Potential outcome for a unit depends neither on others’ treatment status nor on its past treatment after \\(L\\) periods. After defining causal quantity with parameters \\(L\\) and \\(F\\). Focus on the average treatment effect of treatment status change. \\(\\delta(F,L)\\) is the average causal effect of treatment change (ATT), \\(F\\) periods post-treatment, considering treatment history up to \\(L\\) periods. Causal quantity considers potential future treatment reversals, meaning treatment could revert to control before outcome measurement. Also possible to estimate the average treatment effect of treatment reversal on the reversed (ART). Choose \\(L,F\\) based on specific needs. A large \\(L\\) value: Increases the credibility of the limited carryover effect assumption. Allows more past treatments (up to \\(t−L\\)) to influence the outcome \\(Y_{i,t+F}\\). Might reduce the number of matches and lead to less precise estimates. Selecting an appropriate number of lags Researchers should base this choice on substantive knowledge. Sensitivity of empirical results to this choice should be examined. The choice of \\(F\\) should be: Substantively motivated. Decides whether the interest lies in short-term or long-term causal effects. A large \\(F\\) value can complicate causal effect interpretation, especially if many units switch treatment status during the \\(F\\) lead time period. Identification Assumption Parallel trend assumption conditioned on treatment, outcome (excluding immediate lag), and covariate histories. Doesn’t require strong unconfoundedness assumption. Cannot account for unobserved time-varying confounders. Essential to examine outcome time trends. Check if they’re parallel between treated and matched control units using pre-treatment data Constructing the Matched Sets: For each treated observation, create matched control units with identical treatment history from \\(t−L\\) to \\(t−1\\). Matching based on treatment history helps control for carryover effects. Past treatments often act as major confounders, but this method can correct for it. Exact matching on time period adjusts for time-specific unobserved confounders. Unlike staggered adoption methods, units can change treatment status multiple times. Matched set allows treatment switching in and out of treatment Refining the Matched Sets: Initially, matched sets adjust only for treatment history. Parallel trend assumption requires adjustments for other confounders like past outcomes and covariates. Matching methods: Match each treated observation with up to \\(J\\) control units. Distance measures like Mahalanobis distance or propensity score can be used. Match based on estimated propensity score, considering pretreatment covariates. Refined matched set selects most similar control units based on observed confounders. Weighting methods: Assign weight to each control unit in a matched set. Weights prioritize more similar units. Inverse propensity score weighting method can be applied. Weighting is a more generalized method than matching. The Difference-in-Differences Estimator: Using refined matched sets, the ATT (Average Treatment Effect on the Treated) of policy change is estimated. For each treated observation, estimate the counterfactual outcome using the weighted average of control units in the refined set. The DiD estimate of the ATT is computed for each treated observation, then averaged across all such observations. For noncontemporaneous treatment effects where \\(F &gt; 0\\): The ATT doesn’t specify future treatment sequence. Matched control units might have units receiving treatment between time \\(t\\) and \\(t + F\\). Some treated units could return to control conditions between these times. Checking Covariate Balance: The proposed methodology offers the advantage of checking covariate balance between treated and matched control observations. This check helps to see if treated and matched control observations are comparable with respect to observed confounders. Once matched sets are refined, covariate balance examination becomes straightforward. Examine the mean difference of each covariate between a treated observation and its matched controls for each pretreatment time period. Standardize this difference using the standard deviation of each covariate across all treated observations in the dataset. Aggregate this covariate balance measure across all treated observations for each covariate and pretreatment time period. Examine balance for lagged outcome variables over multiple pretreatment periods and time-varying covariates. This helps evaluate the validity of the parallel trend assumption underlying the proposed DiD estimator. Relations with Linear Fixed Effects Regression Estimators: The standard DiD estimator is equivalent to the linear two-way fixed effects regression estimator when: Only two time periods exist. Treatment is given to some units exclusively in the second period. This equivalence doesn’t extend to multiperiod DiD designs, where: More than two time periods are considered. Units might receive treatment multiple times. Despite this, many researchers relate the use of the two-way fixed effects estimator to the DiD design. Standard Error Calculation: Approach: Condition on the weights implied by the matching process. These weights denote how often an observation is utilized in matching (G. W. Imbens and Rubin 2015) Context: Analogous to the conditional variance seen in regression models. Resulting standard errors don’t factor in uncertainties around the matching procedure. They can be viewed as a measure of uncertainty conditional upon the matching process (Ho et al. 2007). Key Findings: Even in conditions favoring OLS, the proposed matching estimator displayed higher robustness to omitted relevant lags than the linear regression model with fixed effects. The robustness offered by matching came at a cost - reduced statistical power. This emphasizes the classic statistical tradeoff between bias (where matching has an advantage) and variance (where regression models might be more efficient). Data Requirements The treatment variable is binary: 0 signifies “assignment” to control. 1 signifies assignment to treatment. Variables identifying units in the data must be: Numeric or integer. Variables identifying time periods should be: Consecutive numeric/integer data. Data format requirement: Must be provided as a standard data.frame object. Basic functions: Utilize treatment histories to create matching sets of treated and control units. Refine these matched sets by determining weights for each control unit in the set. Units with higher weights have a larger influence during estimations. Matching on Treatment History: Goal is to match units transitioning from untreated to treated status with control units that have similar past treatment histories. Setting the Quantity of Interest (qoi =) att average treatment effect on treated units atc average treatment effect of treatment on the control units art average effect of treatment reversal for units that experience treatment reversal ate average treatment effect library(PanelMatch) # All examples follow the package&#39;s vignette # Create the matched sets PM.results.none &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;none&quot;, data = dem, match.missing = TRUE, size.match = 5, qoi = &quot;att&quot;, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, use.diagonal.variance.matrix = TRUE ) # visualize the treated unit and matched controls DisplayTreatment( unit.id = &quot;wbcode2&quot;, time.id = &quot;year&quot;, legend.position = &quot;none&quot;, xlab = &quot;year&quot;, ylab = &quot;Country Code&quot;, treatment = &quot;dem&quot;, data = dem, matched.set = PM.results.none$att[1], # highlight the particular set show.set.only = TRUE ) Control units and the treated unit have identical treatment histories over the lag window (1988-1991) DisplayTreatment( unit.id = &quot;wbcode2&quot;, time.id = &quot;year&quot;, legend.position = &quot;none&quot;, xlab = &quot;year&quot;, ylab = &quot;Country Code&quot;, treatment = &quot;dem&quot;, data = dem, matched.set = PM.results.none$att[2], # highlight the particular set show.set.only = TRUE ) This set is more limited than the first one, but we can still see that we have exact past histories. Refining Matched Sets Refinement involves assigning weights to control units. Users must: Specify a method for calculating unit similarity/distance. Choose variables for similarity/distance calculations. Select a Refinement Method Users determine the refinement method via the refinement.method argument. Options include: mahalanobis ps.match CBPS.match ps.weight CBPS.weight ps.msm.weight CBPS.msm.weight none Methods with “match” in the name and Mahalanobis will assign equal weights to similar control units. “Weighting” methods give higher weights to control units more similar to treated units. Variable Selection Users need to define which covariates will be used through the covs.formula argument, a one-sided formula object. Variables on the right side of the formula are used for calculations. “Lagged” versions of variables can be included using the format: I(lag(name.of.var, 0:n)). Understanding PanelMatch and matched.set objects The PanelMatch function returns a PanelMatch object. The most crucial element within the PanelMatch object is the matched.set object. Within the PanelMatch object, the matched.set object will have names like att, art, or atc. If qoi = ate, there will be two matched.set objects: att and atc. Matched.set Object Details matched.set is a named list with added attributes. Attributes include: Lag Names of treatment Unit and time variables Each list entry represents a matched set of treated and control units. Naming follows a structure: [id variable].[time variable]. Each list element is a vector of control unit ids that match the treated unit mentioned in the element name. Since it’s a matching method, weights are only given to the size.match most similar control units based on distance calculations. # PanelMatch without any refinement PM.results.none &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;none&quot;, data = dem, match.missing = TRUE, size.match = 5, qoi = &quot;att&quot;, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, use.diagonal.variance.matrix = TRUE ) # Extract the matched.set object msets.none &lt;- PM.results.none$att # PanelMatch with refinement PM.results.maha &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;mahalanobis&quot;, # use Mahalanobis distance data = dem, match.missing = TRUE, covs.formula = ~ tradewb, size.match = 5, qoi = &quot;att&quot; , outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, use.diagonal.variance.matrix = TRUE ) msets.maha &lt;- PM.results.maha$att # these 2 should be identical because weights are not shown msets.none |&gt; head() #&gt; wbcode2 year matched.set.size #&gt; 1 4 1992 74 #&gt; 2 4 1997 2 #&gt; 3 6 1973 63 #&gt; 4 6 1983 73 #&gt; 5 7 1991 81 #&gt; 6 7 1998 1 msets.maha |&gt; head() #&gt; wbcode2 year matched.set.size #&gt; 1 4 1992 74 #&gt; 2 4 1997 2 #&gt; 3 6 1973 63 #&gt; 4 6 1983 73 #&gt; 5 7 1991 81 #&gt; 6 7 1998 1 # summary(msets.none) # summary(msets.maha) Visualizing Matched Sets with the plot method Users can visualize the distribution of the matched set sizes. A red line, by default, indicates the count of matched sets where treated units had no matching control units (i.e., empty matched sets). Plot adjustments can be made using graphics::plot. plot(msets.none) Comparing Methods of Refinement Users are encouraged to: Use substantive knowledge for experimentation and evaluation. Consider the following when configuring PanelMatch: The number of matched sets. The number of controls matched to each treated unit. Achieving covariate balance. Note: Large numbers of small matched sets can lead to larger standard errors during the estimation stage. Covariates that aren’t well balanced can lead to undesirable comparisons between treated and control units. Aspects to consider include: Refinement method. Variables for weight calculation. Size of the lag window. Procedures for addressing missing data (refer to match.missing and listwise.delete arguments). Maximum size of matched sets (for matching methods). Supportive Features: print, plot, and summary methods assist in understanding matched sets and their sizes. get_covariate_balance helps evaluate covariate balance: Lower values in the covariate balance calculations are preferred. PM.results.none &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;none&quot;, data = dem, match.missing = TRUE, size.match = 5, qoi = &quot;att&quot;, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, use.diagonal.variance.matrix = TRUE ) PM.results.maha &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;mahalanobis&quot;, data = dem, match.missing = TRUE, covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)), size.match = 5, qoi = &quot;att&quot;, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, use.diagonal.variance.matrix = TRUE ) # listwise deletion used for missing data PM.results.listwise &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;mahalanobis&quot;, data = dem, match.missing = FALSE, listwise.delete = TRUE, covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)), size.match = 5, qoi = &quot;att&quot;, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, use.diagonal.variance.matrix = TRUE ) # propensity score based weighting method PM.results.ps.weight &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;ps.weight&quot;, data = dem, match.missing = FALSE, listwise.delete = TRUE, covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)), size.match = 5, qoi = &quot;att&quot;, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE ) get_covariate_balance( PM.results.none$att, data = dem, covariates = c(&quot;tradewb&quot;, &quot;y&quot;), plot = FALSE ) #&gt; tradewb y #&gt; t_4 -0.07245466 0.291871990 #&gt; t_3 -0.20930129 0.208654876 #&gt; t_2 -0.24425207 0.107736647 #&gt; t_1 -0.10806125 -0.004950238 get_covariate_balance( PM.results.maha$att, data = dem, covariates = c(&quot;tradewb&quot;, &quot;y&quot;), plot = FALSE ) #&gt; tradewb y #&gt; t_4 0.04558637 0.09701606 #&gt; t_3 -0.03312750 0.10844046 #&gt; t_2 -0.01396793 0.08890753 #&gt; t_1 0.10474894 0.06618865 get_covariate_balance( PM.results.listwise$att, data = dem, covariates = c(&quot;tradewb&quot;, &quot;y&quot;), plot = FALSE ) #&gt; tradewb y #&gt; t_4 0.05634922 0.05223623 #&gt; t_3 -0.01104797 0.05217896 #&gt; t_2 0.01411473 0.03094133 #&gt; t_1 0.06850180 0.02092209 get_covariate_balance( PM.results.ps.weight$att, data = dem, covariates = c(&quot;tradewb&quot;, &quot;y&quot;), plot = FALSE ) #&gt; tradewb y #&gt; t_4 0.014362590 0.04035905 #&gt; t_3 0.005529734 0.04188731 #&gt; t_2 0.009410044 0.04195008 #&gt; t_1 0.027907540 0.03975173 get_covariate_balance Function Options: Allows for the generation of plots displaying covariate balance using plot = TRUE. Plots can be customized using arguments typically used with the base R plot method. Option to set use.equal.weights = TRUE for: Obtaining the balance of unrefined sets. Facilitating understanding of the refinement’s impact. # Use equal weights get_covariate_balance( PM.results.ps.weight$att, data = dem, use.equal.weights = TRUE, covariates = c(&quot;tradewb&quot;, &quot;y&quot;), plot = TRUE, # visualize by setting plot to TRUE ylim = c(-1, 1) ) # Compare covariate balance to refined sets # See large improvement in balance get_covariate_balance( PM.results.ps.weight$att, data = dem, covariates = c(&quot;tradewb&quot;, &quot;y&quot;), plot = TRUE, # visualize by setting plot to TRUE ylim = c(-1, 1) ) balance_scatter( matched_set_list = list(PM.results.maha$att, PM.results.ps.weight$att), data = dem, covariates = c(&quot;y&quot;, &quot;tradewb&quot;) ) PanelEstimate Standard Error Calculation Methods There are different methods available: Bootstrap (default method with 1000 iterations). Conditional: Assumes independence across units, but not time. Unconditional: Doesn’t make assumptions of independence across units or time. For qoi values set to att, art, or atc (Imai, Kim, and Wang 2021): You can use analytical methods for calculating standard errors, which include both “conditional” and “unconditional” methods. PE.results &lt;- PanelEstimate( sets = PM.results.ps.weight, data = dem, se.method = &quot;bootstrap&quot;, number.iterations = 1000, confidence.level = .95 ) # point estimates PE.results[[&quot;estimates&quot;]] #&gt; t+0 t+1 t+2 t+3 t+4 #&gt; 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846 # standard errors PE.results[[&quot;standard.error&quot;]] #&gt; t+0 t+1 t+2 t+3 t+4 #&gt; 0.6399349 1.0304938 1.3825265 1.7625951 2.1672629 # use conditional method PE.results &lt;- PanelEstimate( sets = PM.results.ps.weight, data = dem, se.method = &quot;conditional&quot;, confidence.level = .95 ) # point estimates PE.results[[&quot;estimates&quot;]] #&gt; t+0 t+1 t+2 t+3 t+4 #&gt; 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846 # standard errors PE.results[[&quot;standard.error&quot;]] #&gt; t+0 t+1 t+2 t+3 t+4 #&gt; 0.4844805 0.8170604 1.1171942 1.4116879 1.7172143 summary(PE.results) #&gt; Weighted Difference-in-Differences with Propensity Score #&gt; Matches created with 4 lags #&gt; #&gt; Standard errors computed with conditional method #&gt; #&gt; Estimate of Average Treatment Effect on the Treated (ATT) by Period: #&gt; $summary #&gt; estimate std.error 2.5% 97.5% #&gt; t+0 0.2609565 0.4844805 -0.6886078 1.210521 #&gt; t+1 0.9630847 0.8170604 -0.6383243 2.564494 #&gt; t+2 1.2851017 1.1171942 -0.9045586 3.474762 #&gt; t+3 1.7370930 1.4116879 -1.0297644 4.503950 #&gt; t+4 1.4871846 1.7172143 -1.8784937 4.852863 #&gt; #&gt; $lag #&gt; [1] 4 #&gt; #&gt; $qoi #&gt; [1] &quot;att&quot; plot(PE.results) Moderating Variables # moderating variable dem$moderator &lt;- 0 dem$moderator &lt;- ifelse(dem$wbcode2 &gt; 100, 1, 2) PM.results &lt;- PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, refinement.method = &quot;mahalanobis&quot;, data = dem, match.missing = TRUE, covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)), size.match = 5, qoi = &quot;att&quot;, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, use.diagonal.variance.matrix = TRUE ) PE.results &lt;- PanelEstimate(sets = PM.results, data = dem, moderator = &quot;moderator&quot;) # Each element in the list corresponds to a level in the moderator plot(PE.results[[1]]) plot(PE.results[[2]]) To write up for journal submission, you can follow the following report: In this study, closely aligned with the research by (Acemoglu et al. 2019), two key effects of democracy on economic growth are estimated: the impact of democratization and that of authoritarian reversal. The treatment variable, \\(X_{it}\\), is defined to be one if country \\(i\\) is democratic in year \\(t\\), and zero otherwise. The Average Treatment Effect for the Treated (ATT) under democratization is formulated as follows: \\[ \\begin{aligned} \\delta(F, L) &amp;= \\mathbb{E} \\left\\{ Y_{i, t + F} (X_{it} = 1, X_{i, t - 1} = 0, \\{X_{i,t-l}\\}_{l=2}^L) \\right. \\\\ &amp;\\left. - Y_{i, t + F} (X_{it} = 0, X_{i, t - 1} = 0, \\{X_{i,t-l}\\}_{l=2}^L) | X_{it} = 1, X_{i, t - 1} = 0 \\right\\} \\end{aligned} \\] In this framework, the treated observations are countries that transition from an authoritarian regime \\(X_{it-1} = 0\\) to a democratic one \\(X_{it} = 1\\). The variable \\(F\\) represents the number of leads, denoting the time periods following the treatment, and \\(L\\) signifies the number of lags, indicating the time periods preceding the treatment. The ATT under authoritarian reversal is given by: \\[ \\begin{aligned} &amp;\\mathbb{E} \\left[ Y_{i, t + F} (X_{it} = 0, X_{i, t - 1} = 1, \\{ X_{i, t - l}\\}_{l=2}^L ) \\right. \\\\ &amp;\\left. - Y_{i, t + F} (X_{it} = 1, X_{it-1} = 1, \\{X_{i, t - l} \\}_{l=2}^L ) | X_{it} = 0, X_{i, t - 1} = 1 \\right] \\end{aligned} \\] The ATT is calculated conditioning on 4 years of lags (\\(L = 4\\)) and up to 4 years following the policy change \\(F = 1, 2, 3, 4\\). Matched sets for each treated observation are constructed based on its treatment history, with the number of matched control units generally decreasing when considering a 4-year treatment history as compared to a 1-year history. To enhance the quality of matched sets, methods such as Mahalanobis distance matching, propensity score matching, and propensity score weighting are utilized. These approaches enable us to evaluate the effectiveness of each refinement method. In the process of matching, we employ both up-to-five and up-to-ten matching to investigate how sensitive our empirical results are to the maximum number of allowed matches. For more information on the refinement process, please see the Web Appendix The Mahalanobis distance is expressed through a specific formula. We aim to pair each treated unit with a maximum of \\(J\\) control units, permitting replacement, denoted as \\(| \\mathcal{M}_{it} \\le J|\\). The average Mahalanobis distance between a treated and each control unit over time is computed as: \\[ S_{it} (i&#39;) = \\frac{1}{L} \\sum_{l = 1}^L \\sqrt{(\\mathbf{V}_{i, t - l} - \\mathbf{V}_{i&#39;, t -l})^T \\mathbf{\\Sigma}_{i, t - l}^{-1} (\\mathbf{V}_{i, t - l} - \\mathbf{V}_{i&#39;, t -l})} \\] For a matched control unit \\(i&#39; \\in \\mathcal{M}_{it}\\), \\(\\mathbf{V}_{it&#39;}\\) represents the time-varying covariates to adjust for, and \\(\\mathbf{\\Sigma}_{it&#39;}\\) is the sample covariance matrix for \\(\\mathbf{V}_{it&#39;}\\). Essentially, we calculate a standardized distance using time-varying covariates and average this across different time intervals. In the context of propensity score matching, we employ a logistic regression model with balanced covariates to derive the propensity score. Defined as the conditional likelihood of treatment given pre-treatment covariates (Rosenbaum and Rubin 1983), the propensity score is estimated by first creating a data subset comprised of all treated and their matched control units from the same year. This logistic regression model is then fitted as follows: \\[ \\begin{aligned} &amp; e_{it} (\\{\\mathbf{U}_{i, t - l} \\}^L_{l = 1}) \\\\ &amp;= Pr(X_{it} = 1| \\mathbf{U}_{i, t -1}, \\ldots, \\mathbf{U}_{i, t - L}) \\\\ &amp;= \\frac{1}{1 = \\exp(- \\sum_{l = 1}^L \\beta_l^T \\mathbf{U}_{i, t - l})} \\end{aligned} \\] where \\(\\mathbf{U}_{it&#39;} = (X_{it&#39;}, \\mathbf{V}_{it&#39;}^T)^T\\). Given this model, the estimated propensity score for all treated and matched control units is then computed. This enables the adjustment for lagged covariates via matching on the calculated propensity score, resulting in the following distance measure: \\[ S_{it} (i&#39;) = | \\text{logit} \\{ \\hat{e}_{it} (\\{ \\mathbf{U}_{i, t - l}\\}^L_{l = 1})\\} - \\text{logit} \\{ \\hat{e}_{i&#39;t}( \\{ \\mathbf{U}_{i&#39;, t - l} \\}^L_{l = 1})\\} | \\] Here, \\(\\hat{e}_{i&#39;t} (\\{ \\mathbf{U}_{i, t - l}\\}^L_{l = 1})\\) represents the estimated propensity score for each matched control unit \\(i&#39; \\in \\mathcal{M}_{it}\\). Once the distance measure \\(S_{it} (i&#39;)\\) has been determined for all control units in the original matched set, we fine-tune this set by selecting up to \\(J\\) closest control units, which meet a researcher-defined caliper constraint \\(C\\). All other control units receive zero weight. This results in a refined matched set for each treated unit \\((i, t)\\): \\[ \\mathcal{M}_{it}^* = \\{i&#39; : i&#39; \\in \\mathcal{M}_{it}, S_{it} (i&#39;) &lt; C, S_{it} \\le S_{it}^{(J)}\\} \\] \\(S_{it}^{(J)}\\) is the \\(J\\)th smallest distance among the control units in the original set \\(\\mathcal{M}_{it}\\). For further refinement using weighting, a weight is assigned to each control unit \\(i&#39;\\) in a matched set corresponding to a treated unit \\((i, t)\\), with greater weight accorded to more similar units. We utilize inverse propensity score weighting, based on the propensity score model mentioned earlier: \\[ w_{it}^{i&#39;} \\propto \\frac{\\hat{e}_{i&#39;t} (\\{ \\mathbf{U}_{i, t-l} \\}^L_{l = 1} )}{1 - \\hat{e}_{i&#39;t} (\\{ \\mathbf{U}_{i, t-l} \\}^L_{l = 1} )} \\] In this model, \\(\\sum_{i&#39; \\in \\mathcal{M}_{it}} w_{it}^{i&#39;} = 1\\) and \\(w_{it}^{i&#39;} = 0\\) for \\(i&#39; \\notin \\mathcal{M}_{it}\\). The model is fitted to the complete sample of treated and matched control units. Checking Covariate Balance A distinct advantage of the proposed methodology over regression methods is the ability it offers researchers to inspect the covariate balance between treated and matched control observations. This facilitates the evaluation of whether treated and matched control observations are comparable regarding observed confounders. To investigate the mean difference of each covariate (e.g., \\(V_{it&#39;j}\\), representing the \\(j\\)-th variable in \\(\\mathbf{V}_{it&#39;}\\)) between the treated observation and its matched control observation at each pre-treatment time period (i.e., \\(t&#39; &lt; t\\)), we further standardize this difference. For any given pretreatment time period, we adjust by the standard deviation of each covariate across all treated observations in the dataset. Thus, the mean difference is quantified in terms of standard deviation units. Formally, for each treated observation \\((i,t)\\) where \\(D_{it} = 1\\), we define the covariate balance for variable \\(j\\) at the pretreatment time period \\(t - l\\) as: \\[\\begin{equation} B_{it}(j, l) = \\frac{V_{i, t- l,j}- \\sum_{i&#39; \\in \\mathcal{M}_{it}}w_{it}^{i&#39;}V_{i&#39;, t-l,j}}{\\sqrt{\\frac{1}{N_1 - 1} \\sum_{i&#39;=1}^N \\sum_{t&#39; = L+1}^{T-F}D_{i&#39;t&#39;}(V_{i&#39;, t&#39;-l, j} - \\bar{V}_{t&#39; - l, j})^2}} \\label{eq:covbalance} \\end{equation}\\] where \\(N_1 = \\sum_{i&#39;= 1}^N \\sum_{t&#39; = L+1}^{T-F} D_{i&#39;t&#39;}\\) denotes the total number of treated observations and \\(\\bar{V}_{t-l,j} = \\sum_{i=1}^N D_{i,t-l,j}/N\\). We then aggregate this covariate balance measure across all treated observations for each covariate and pre-treatment time period: \\[\\begin{equation} \\bar{B}(j, l) = \\frac{1}{N_1} \\sum_{i=1}^N \\sum_{t = L+ 1}^{T-F}D_{it} B_{it}(j,l) \\label{eq:aggbalance} \\end{equation}\\] Lastly, we evaluate the balance of lagged outcome variables over several pre-treatment periods and that of time-varying covariates. This examination aids in assessing the validity of the parallel trend assumption integral to the DiD estimator justification. In Figure ??, we demonstrate the enhancement of covariate balance thank to the refinement of matched sets. Each scatter plot contrasts the absolute standardized mean difference, as detailed in Equation (??), before (horizontal axis) and after (vertical axis) this refinement. Points below the 45-degree line indicate an improved standardized mean balance for certain time-varying covariates post-refinement. The majority of variables benefit from this refinement process. Notably, the propensity score weighting (bottom panel) shows the most significant improvement, whereas Mahalanobis matching (top panel) yields a more modest improvement. library(PanelMatch) library(causalverse) runPanelMatch &lt;- function(method, lag, size.match=NULL, qoi=&quot;att&quot;) { # Default parameters for PanelMatch common.args &lt;- list( lag = lag, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, data = dem, covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)), qoi = qoi, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, size.match = size.match # setting size.match here for all methods ) if(method == &quot;mahalanobis&quot;) { common.args$refinement.method &lt;- &quot;mahalanobis&quot; common.args$match.missing &lt;- TRUE common.args$use.diagonal.variance.matrix &lt;- TRUE } else if(method == &quot;ps.match&quot;) { common.args$refinement.method &lt;- &quot;ps.match&quot; common.args$match.missing &lt;- FALSE common.args$listwise.delete &lt;- TRUE } else if(method == &quot;ps.weight&quot;) { common.args$refinement.method &lt;- &quot;ps.weight&quot; common.args$match.missing &lt;- FALSE common.args$listwise.delete &lt;- TRUE } return(do.call(PanelMatch, common.args)) } methods &lt;- c(&quot;mahalanobis&quot;, &quot;ps.match&quot;, &quot;ps.weight&quot;) lags &lt;- c(1, 4) sizes &lt;- c(5, 10) You can either do it sequentailly res_pm &lt;- list() for(method in methods) { for(lag in lags) { for(size in sizes) { name &lt;- paste0(method, &quot;.&quot;, lag, &quot;lag.&quot;, size, &quot;m&quot;) res_pm[[name]] &lt;- runPanelMatch(method, lag, size) } } } # Now, you can access res_pm using res_pm[[&quot;mahalanobis.1lag.5m&quot;]] etc. # for treatment reversal res_pm_rev &lt;- list() for(method in methods) { for(lag in lags) { for(size in sizes) { name &lt;- paste0(method, &quot;.&quot;, lag, &quot;lag.&quot;, size, &quot;m&quot;) res_pm_rev[[name]] &lt;- runPanelMatch(method, lag, size, qoi = &quot;art&quot;) } } } or in parallel library(foreach) library(doParallel) registerDoParallel(cores = 4) # Initialize an empty list to store results res_pm &lt;- list() # Replace nested for-loops with foreach results &lt;- foreach( method = methods, .combine = &#39;c&#39;, .multicombine = TRUE, .packages = c(&quot;PanelMatch&quot;, &quot;causalverse&quot;) ) %dopar% { tmp &lt;- list() for (lag in lags) { for (size in sizes) { name &lt;- paste0(method, &quot;.&quot;, lag, &quot;lag.&quot;, size, &quot;m&quot;) tmp[[name]] &lt;- runPanelMatch(method, lag, size) } } tmp } # Collate results for (name in names(results)) { res_pm[[name]] &lt;- results[[name]] } # Treatment reversal # Initialize an empty list to store results res_pm_rev &lt;- list() # Replace nested for-loops with foreach results_rev &lt;- foreach( method = methods, .combine = &#39;c&#39;, .multicombine = TRUE, .packages = c(&quot;PanelMatch&quot;, &quot;causalverse&quot;) ) %dopar% { tmp &lt;- list() for (lag in lags) { for (size in sizes) { name &lt;- paste0(method, &quot;.&quot;, lag, &quot;lag.&quot;, size, &quot;m&quot;) tmp[[name]] &lt;- runPanelMatch(method, lag, size, qoi = &quot;art&quot;) } } tmp } # Collate results for (name in names(results_rev)) { res_pm_rev[[name]] &lt;- results_rev[[name]] } stopImplicitCluster() library(gridExtra) # Updated plotting function create_balance_plot &lt;- function(method, lag, sizes, res_pm, dem) { matched_set_lists &lt;- lapply(sizes, function(size) { res_pm[[paste0(method, &quot;.&quot;, lag, &quot;lag.&quot;, size, &quot;m&quot;)]]$att }) return( balance_scatter_custom( matched_set_list = matched_set_lists, legend.title = &quot;Possible Matches&quot;, set.names = as.character(sizes), legend.position = c(0.2, 0.8), # for compiled plot, you don&#39;t need x,y, or main labs x.axis.label = &quot;&quot;, y.axis.label = &quot;&quot;, main = &quot;&quot;, data = dem, dot.size = 5, # show.legend = F, them_use = causalverse::ama_theme(base_size = 32), covariates = c(&quot;y&quot;, &quot;tradewb&quot;) ) ) } plots &lt;- list() for (method in methods) { for (lag in lags) { plots[[paste0(method, &quot;.&quot;, lag, &quot;lag&quot;)]] &lt;- create_balance_plot(method, lag, sizes, res_pm, dem) } } # # Arranging plots in a 3x2 grid # grid.arrange(plots[[&quot;mahalanobis.1lag&quot;]], # plots[[&quot;mahalanobis.4lag&quot;]], # plots[[&quot;ps.match.1lag&quot;]], # plots[[&quot;ps.match.4lag&quot;]], # plots[[&quot;ps.weight.1lag&quot;]], # plots[[&quot;ps.weight.4lag&quot;]], # ncol=2, nrow=3) # Standardized Mean Difference of Covariates library(gridExtra) library(grid) # Create column and row labels using textGrob col_labels &lt;- c(&quot;1-year Lag&quot;, &quot;4-year Lag&quot;) row_labels &lt;- c(&quot;Maha Matching&quot;, &quot;PS Matching&quot;, &quot;PS Weigthing&quot;) major.axes.fontsize = 40 minor.axes.fontsize = 30 png( file.path(getwd(), &quot;images&quot;, &quot;did_balance_scatter.png&quot;), width = 1200, height = 1000 ) # Create a list-of-lists, where each inner list represents a row grid_list &lt;- list( list( nullGrob(), textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)) ), list(textGrob( row_labels[1], gp = gpar(fontsize = minor.axes.fontsize), rot = 90 ), plots[[&quot;mahalanobis.1lag&quot;]], plots[[&quot;mahalanobis.4lag&quot;]]), list(textGrob( row_labels[2], gp = gpar(fontsize = minor.axes.fontsize), rot = 90 ), plots[[&quot;ps.match.1lag&quot;]], plots[[&quot;ps.match.4lag&quot;]]), list(textGrob( row_labels[3], gp = gpar(fontsize = minor.axes.fontsize), rot = 90 ), plots[[&quot;ps.weight.1lag&quot;]], plots[[&quot;ps.weight.4lag&quot;]]) ) # &quot;Flatten&quot; the list-of-lists into a single list of grobs grobs &lt;- do.call(c, grid_list) grid.arrange( grobs = grobs, ncol = 3, nrow = 4, widths = c(0.15, 0.42, 0.42), heights = c(0.15, 0.28, 0.28, 0.28) ) grid.text( &quot;Before Refinement&quot;, x = 0.5, y = 0.03, gp = gpar(fontsize = major.axes.fontsize) ) grid.text( &quot;After Refinement&quot;, x = 0.03, y = 0.5, rot = 90, gp = gpar(fontsize = major.axes.fontsize) ) dev.off() #&gt; png #&gt; 2 Note: Scatter plots display the standardized mean difference of each covariate \\(j\\) and lag year \\(l\\) as defined in Equation (??) before (x-axis) and after (y-axis) matched set refinement. Each plot includes varying numbers of possible matches for each matching method. Rows represent different matching/weighting methods, while columns indicate adjustments for various lag lengths. # Step 1: Define configurations configurations &lt;- list( list(refinement.method = &quot;none&quot;, qoi = &quot;att&quot;), list(refinement.method = &quot;none&quot;, qoi = &quot;art&quot;), list(refinement.method = &quot;mahalanobis&quot;, qoi = &quot;att&quot;), list(refinement.method = &quot;mahalanobis&quot;, qoi = &quot;art&quot;), list(refinement.method = &quot;ps.match&quot;, qoi = &quot;att&quot;), list(refinement.method = &quot;ps.match&quot;, qoi = &quot;art&quot;), list(refinement.method = &quot;ps.weight&quot;, qoi = &quot;att&quot;), list(refinement.method = &quot;ps.weight&quot;, qoi = &quot;art&quot;) ) # Step 2: Use lapply or loop to generate results results &lt;- lapply(configurations, function(config) { PanelMatch( lag = 4, time.id = &quot;year&quot;, unit.id = &quot;wbcode2&quot;, treatment = &quot;dem&quot;, data = dem, match.missing = FALSE, listwise.delete = TRUE, size.match = 5, outcome.var = &quot;y&quot;, lead = 0:4, forbid.treatment.reversal = FALSE, refinement.method = config$refinement.method, covs.formula = ~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)), qoi = config$qoi ) }) # Step 3: Get covariate balance and plot plots &lt;- mapply(function(result, config) { df &lt;- get_covariate_balance( if (config$qoi == &quot;att&quot;) result$att else result$art, data = dem, covariates = c(&quot;tradewb&quot;, &quot;y&quot;), plot = F ) causalverse::plot_covariate_balance_pretrend(df, main = &quot;&quot;, show_legend = F) }, results, configurations, SIMPLIFY = FALSE) # Set names for plots names(plots) &lt;- sapply(configurations, function(config) { paste(config$qoi, config$refinement.method, sep = &quot;.&quot;) }) To export library(gridExtra) library(grid) # Column and row labels col_labels &lt;- c(&quot;None&quot;, &quot;Mahalanobis&quot;, &quot;Propensity Score Matching&quot;, &quot;Propensity Score Weighting&quot;) row_labels &lt;- c(&quot;ATT&quot;, &quot;ART&quot;) # Specify your desired fontsize for labels minor.axes.fontsize &lt;- 16 major.axes.fontsize &lt;- 20 png(file.path(getwd(), &quot;images&quot;, &quot;p_covariate_balance.png&quot;), width=1200, height=1000) # Create a list-of-lists, where each inner list represents a row grid_list &lt;- list( list( nullGrob(), textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize)) ), list( textGrob( row_labels[1], gp = gpar(fontsize = minor.axes.fontsize), rot = 90 ), plots$att.none, plots$att.mahalanobis, plots$att.ps.match, plots$att.ps.weight ), list( textGrob( row_labels[2], gp = gpar(fontsize = minor.axes.fontsize), rot = 90 ), plots$art.none, plots$art.mahalanobis, plots$art.ps.match, plots$art.ps.weight ) ) # &quot;Flatten&quot; the list-of-lists into a single list of grobs grobs &lt;- do.call(c, grid_list) # Arrange your plots with text labels grid.arrange( grobs = grobs, ncol = 5, nrow = 3, widths = c(0.1, 0.225, 0.225, 0.225, 0.225), heights = c(0.1, 0.45, 0.45) ) # Add main x and y axis titles grid.text( &quot;Refinement Methods&quot;, x = 0.5, y = 0.01, gp = gpar(fontsize = major.axes.fontsize) ) grid.text( &quot;Quantities of Interest&quot;, x = 0.02, y = 0.5, rot = 90, gp = gpar(fontsize = major.axes.fontsize) ) dev.off() library(knitr) include_graphics(file.path(getwd(), &quot;images&quot;, &quot;p_covariate_balance.png&quot;)) Note: Each graph displays the standardized mean difference, as outlined in Equation (??), plotted on the vertical axis across a pre-treatment duration of four years represented on the horizontal axis. The leftmost column illustrates the balance prior to refinement, while the subsequent three columns depict the covariate balance post the application of distinct refinement techniques. Each individual line signifies the balance of a specific variable during the pre-treatment phase.The red line is tradewb and blue line is the lagged outcome variable. In Figure ??, we observe a marked improvement in covariate balance due to the implemented matching procedures during the pre-treatment period. Our analysis prioritizes methods that adjust for time-varying covariates over a span of four years preceding the treatment initiation. The two rows delineate the standardized mean balance for both treatment modalities, with individual lines representing the balance for each covariate. Across all scenarios, the refinement attributed to matched sets significantly enhances balance. Notably, using propensity score weighting considerably mitigates imbalances in confounders. While some degree of imbalance remains evident in the Mahalanobis distance and propensity score matching techniques, the standardized mean difference for the lagged outcome remains stable throughout the pre-treatment phase. This consistency lends credence to the validity of the proposed DiD estimator. Estimation Results We now detail the estimated ATTs derived from the matching techniques. Figure below offers visual representations of the impacts of treatment initiation (upper panel) and treatment reversal (lower panel) on the outcome variable for a duration of 5 years post-transition, specifically, (F = 0, 1, …, 4). Across the five methods (columns), it becomes evident that the point estimates of effects associated with treatment initiation consistently approximate zero over the 5-year window. In contrast, the estimated outcomes of treatment reversal are notably negative and maintain statistical significance through all refinement techniques during the initial year of transition and the 1 to 4 years that follow, provided treatment reversal is permissible. These effects are notably pronounced, pointing to an estimated reduction of roughly X% in the outcome variable. Collectively, these findings indicate that the transition into the treated state from its absence doesn’t invariably lead to a heightened outcome. Instead, the transition from the treated state back to its absence exerts a considerable negative effect on the outcome variable in both the short and intermediate terms. Hence, the positive effect of the treatment (if we were to use traditional DiD) is actually driven by the negative effect of treatment reversal. # sequential # Step 1: Apply PanelEstimate function # Initialize an empty list to store results res_est &lt;- vector(&quot;list&quot;, length(res_pm)) # Iterate over each element in res_pm for (i in 1:length(res_pm)) { res_est[[i]] &lt;- PanelEstimate( res_pm[[i]], data = dem, se.method = &quot;bootstrap&quot;, number.iterations = 1000, confidence.level = .95 ) # Transfer the name of the current element to the res_est list names(res_est)[i] &lt;- names(res_pm)[i] } # Step 2: Apply plot_PanelEstimate function # Initialize an empty list to store plot results res_est_plot &lt;- vector(&quot;list&quot;, length(res_est)) # Iterate over each element in res_est for (i in 1:length(res_est)) { res_est_plot[[i]] &lt;- plot_PanelEstimate(res_est[[i]], main = &quot;&quot;, theme_use = causalverse::ama_theme(base_size = 14)) # Transfer the name of the current element to the res_est_plot list names(res_est_plot)[i] &lt;- names(res_est)[i] } # check results # res_est_plot$mahalanobis.1lag.5m # Step 1: Apply PanelEstimate function for res_pm_rev # Initialize an empty list to store results res_est_rev &lt;- vector(&quot;list&quot;, length(res_pm_rev)) # Iterate over each element in res_pm_rev for (i in 1:length(res_pm_rev)) { res_est_rev[[i]] &lt;- PanelEstimate( res_pm_rev[[i]], data = dem, se.method = &quot;bootstrap&quot;, number.iterations = 1000, confidence.level = .95 ) # Transfer the name of the current element to the res_est_rev list names(res_est_rev)[i] &lt;- names(res_pm_rev)[i] } # Step 2: Apply plot_PanelEstimate function for res_est_rev # Initialize an empty list to store plot results res_est_plot_rev &lt;- vector(&quot;list&quot;, length(res_est_rev)) # Iterate over each element in res_est_rev for (i in 1:length(res_est_rev)) { res_est_plot_rev[[i]] &lt;- plot_PanelEstimate(res_est_rev[[i]], main = &quot;&quot;, theme_use = causalverse::ama_theme(base_size = 14)) # Transfer the name of the current element to the res_est_plot_rev list names(res_est_plot_rev)[i] &lt;- names(res_est_rev)[i] } # parallel library(doParallel) library(foreach) # Detect the number of cores to use for parallel processing num_cores &lt;- 4 # Register the parallel backend cl &lt;- makeCluster(num_cores) registerDoParallel(cl) # Step 1: Apply PanelEstimate function in parallel res_est &lt;- foreach(i = 1:length(res_pm), .packages = &quot;PanelMatch&quot;) %dopar% { PanelEstimate( res_pm[[i]], data = dem, se.method = &quot;bootstrap&quot;, number.iterations = 1000, confidence.level = .95 ) } # Transfer names from res_pm to res_est names(res_est) &lt;- names(res_pm) # Step 2: Apply plot_PanelEstimate function in parallel res_est_plot &lt;- foreach( i = 1:length(res_est), .packages = c(&quot;PanelMatch&quot;, &quot;causalverse&quot;, &quot;ggplot2&quot;) ) %dopar% { plot_PanelEstimate(res_est[[i]], main = &quot;&quot;, theme_use = causalverse::ama_theme(base_size = 10)) } # Transfer names from res_est to res_est_plot names(res_est_plot) &lt;- names(res_est) # Step 1: Apply PanelEstimate function for res_pm_rev in parallel res_est_rev &lt;- foreach(i = 1:length(res_pm_rev), .packages = &quot;PanelMatch&quot;) %dopar% { PanelEstimate( res_pm_rev[[i]], data = dem, se.method = &quot;bootstrap&quot;, number.iterations = 1000, confidence.level = .95 ) } # Transfer names from res_pm_rev to res_est_rev names(res_est_rev) &lt;- names(res_pm_rev) # Step 2: Apply plot_PanelEstimate function for res_est_rev in parallel res_est_plot_rev &lt;- foreach( i = 1:length(res_est_rev), .packages = c(&quot;PanelMatch&quot;, &quot;causalverse&quot;, &quot;ggplot2&quot;) ) %dopar% { plot_PanelEstimate(res_est_rev[[i]], main = &quot;&quot;, theme_use = causalverse::ama_theme(base_size = 10)) } # Transfer names from res_est_rev to res_est_plot_rev names(res_est_plot_rev) &lt;- names(res_est_rev) # Stop the cluster stopCluster(cl) To export library(gridExtra) library(grid) # Column and row labels col_labels &lt;- c(&quot;Mahalanobis 5m&quot;, &quot;Mahalanobis 10m&quot;, &quot;PS Matching 5m&quot;, &quot;PS Matching 10m&quot;, &quot;PS Weighting 5m&quot;) row_labels &lt;- c(&quot;ATT&quot;, &quot;ART&quot;) # Specify your desired fontsize for labels minor.axes.fontsize &lt;- 16 major.axes.fontsize &lt;- 20 png(file.path(getwd(), &quot;images&quot;, &quot;p_did_est_in_n_out.png&quot;), width=1200, height=1000) # Create a list-of-lists, where each inner list represents a row grid_list &lt;- list( list( nullGrob(), textGrob(col_labels[1], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[2], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[3], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[4], gp = gpar(fontsize = minor.axes.fontsize)), textGrob(col_labels[5], gp = gpar(fontsize = minor.axes.fontsize)) ), list( textGrob(row_labels[1], gp = gpar(fontsize = minor.axes.fontsize), rot = 90), res_est_plot$mahalanobis.1lag.5m, res_est_plot$mahalanobis.1lag.10m, res_est_plot$ps.match.1lag.5m, res_est_plot$ps.match.1lag.10m, res_est_plot$ps.weight.1lag.5m ), list( textGrob(row_labels[2], gp = gpar(fontsize = minor.axes.fontsize), rot = 90), res_est_plot_rev$mahalanobis.1lag.5m, res_est_plot_rev$mahalanobis.1lag.10m, res_est_plot_rev$ps.match.1lag.5m, res_est_plot_rev$ps.match.1lag.10m, res_est_plot_rev$ps.weight.1lag.5m ) ) # &quot;Flatten&quot; the list-of-lists into a single list of grobs grobs &lt;- do.call(c, grid_list) # Arrange your plots with text labels grid.arrange( grobs = grobs, ncol = 6, nrow = 3, widths = c(0.1, 0.18, 0.18, 0.18, 0.18, 0.18), heights = c(0.1, 0.45, 0.45) ) # Add main x and y axis titles grid.text( &quot;Methods&quot;, x = 0.5, y = 0.02, gp = gpar(fontsize = major.axes.fontsize) ) grid.text( &quot;&quot;, x = 0.02, y = 0.5, rot = 90, gp = gpar(fontsize = major.axes.fontsize) ) dev.off() library(knitr) include_graphics(file.path(getwd(), &quot;images&quot;, &quot;p_did_est_in_n_out.png&quot;)) 26.9.3.2 Counterfactual Estimators Also known as imputation approach (Liu, Wang, and Xu 2022) This class of estimator consider observation treatment as missing data. Models are built using data from the control units to impute conterfactuals for the treated observations. It’s called counterfactual estimators because they predict outcomes as if the treated observations had not received the treatment. Advantages: Avoids negative weights and biases by not using treated observations for modeling and applying uniform weights. Supports various models, including those that may relax strict exogeneity assumptions. Methods including Fixed-effects conterfactual estimator (FEct) (DiD is a special case): Based on the Two-way Fixed-effects, where assumes linear additive functional form of unobservables based on unit and time FEs. But FEct fixes the improper weighting of TWFE by comparing within each matched pair (where each pair is the treated observation and its predicted counterfactual that is the weighted sum of all untreated observations). Interactive Fixed Effects conterfactual estimator (IFEct) Xu (2017): When we suspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses the factor-augmented models to relax the strict exogeneity assumption where the effects of unobservables can be decomposed to unit FE + time FE + unit x time FE. Generalized Synthetic Controls are a subset of IFEct when treatments don’t revert. Matrix completion (MC) (Athey et al. 2021): Generalization of factor-augmented models. Different from IFEct which uses hard impute, MC uses soft impute to regularize the singular values when decomposing the residual matrix. Only when latent factors (of unobservables) are strong and sparse, IFEct outperforms MC. [Synthetic Controls] (case studies) Identifying Assumptions: Function Form: Additive separability of observables, unobservables, and idiosyncratic error term. Hence, these models are scale dependent (Athey and Imbens 2006) (e.g., log-transform outcome can invadiate this assumption). Strict Exogeneity: Conditional on observables and unobservables, potential outcomes are independent of treatment assignment (i.e., baseline quasi-randomization) In DiD, where unobservables = unit + time FEs, this assumption is the parallel trends assumption Low-dimensional Decomposition (Feasibility Assumption): Unobservable effects can be decomposed in low-dimension. For the case that \\(U_{it} = f_t \\times \\lambda_i\\) where \\(f_t\\) = common time trend (time FE), and \\(\\lambda_i\\) = unit heterogeneity (unit FE). If \\(U_{it} = f_t \\times \\lambda_i\\) , DiD can satisfy this assumption. But this assumption is weaker than that of DID, and allows us to control for unobservables based on data. Estimation Procedure: Using all control observations, estimate the functions of both observable and unobservable variables (relying on Assumptions 1 and 3). Predict the counterfactual outcomes for each treated unit using the obtained functions. Calculate the difference in treatment effect for each treated individual. By averaging over all treated individuals, you can obtain the Average Treatment Effect on the Treated (ATT). Notes: Use jackknife when number of treated units is small (Liu, Wang, and Xu 2022, 166). 26.9.3.2.1 Imputation Method Liu, Wang, and Xu (2022) can also account for treatment reversals and heterogeneous treatment effects. Other imputation estimators include [@gardner2022two and @borusyak2021revisiting] N. Brown, Butts, and Westerlund (2023) library(fect) PanelMatch::dem model.fect &lt;- fect( Y = &quot;y&quot;, D = &quot;dem&quot;, X = &quot;tradewb&quot;, data = na.omit(PanelMatch::dem), method = &quot;fe&quot;, index = c(&quot;wbcode2&quot;, &quot;year&quot;), se = TRUE, parallel = TRUE, seed = 1234, # twfe force = &quot;two-way&quot; ) print(model.fect$est.avg) plot(model.fect) plot(model.fect, stats = &quot;F.p&quot;) F-test \\(H_0\\): residual averages in the pre-treatment periods = 0 To see treatment reversal effects plot(model.fect, stats = &quot;F.p&quot;, type = &#39;exit&#39;) 26.9.3.2.2 Placebo Test By selecting a part of the data and excluding observations within a specified range to improve the model fitting, we then evaluate whether the estimated Average Treatment Effect (ATT) within this range significantly differs from zero. This approach helps us analyze the periods before treatment. If this test fails, either the functional form or strict exogeneity assumption is problematic. out.fect.p &lt;- fect( Y = &quot;y&quot;, D = &quot;dem&quot;, X = &quot;tradewb&quot;, data = na.omit(PanelMatch::dem), method = &quot;fe&quot;, index = c(&quot;wbcode2&quot;, &quot;year&quot;), se = TRUE, placeboTest = TRUE, # using 3 periods placebo.period = c(-2, 0) ) plot(out.fect.p, proportion = 0.1, stats = &quot;placebo.p&quot;) 26.9.3.2.3 (No) Carryover Effects Test The placebo test can be adapted to assess carryover effects by masking several post-treatment periods instead of pre-treatment ones. If no carryover effects are present, the average prediction error should approximate zero. For the carryover test, set carryoverTest = TRUE. Specify a post-treatment period range in carryover.period to exclude observations for model fitting, then evaluate if the estimated ATT significantly deviates from zero. Even if we have carryover effects, in most cases of the staggered adoption setting, researchers are interested in the cumulative effects, or aggregated treatment effects, so it’s okay. out.fect.c &lt;- fect( Y = &quot;y&quot;, D = &quot;dem&quot;, X = &quot;tradewb&quot;, data = na.omit(PanelMatch::dem), method = &quot;fe&quot;, index = c(&quot;wbcode2&quot;, &quot;year&quot;), se = TRUE, carryoverTest = TRUE, # how many periods of carryover carryover.period = c(1, 3) ) plot(out.fect.c, stats = &quot;carryover.p&quot;) We have evidence of carryover effects. 26.9.3.3 Matrix Completion Applications in marketing: Bronnenberg, Dubé, and Sanders (2020) To estimate average causal effects in panel data with units exposed to treatment intermittently, two literatures are pivotal: Unconfoundedness (G. W. Imbens and Rubin 2015): Imputes missing potential control outcomes for treated units using observed outcomes from similar control units in previous periods. Synthetic Control (Abadie, Diamond, and Hainmueller 2010): Imputes missing control outcomes for treated units using weighted averages from control units, matching lagged outcomes between treated and control units. Both exploit missing potential outcomes under different assumptions: Unconfoundedness assumes time patterns are stable across units. Synthetic control assumes unit patterns are stable over time. Once regularization is applied, both approaches are applicable in similar settings (Athey et al. 2021). Matrix Completion method, nesting both, is based on matrix factorization, focusing on imputing missing matrix elements assuming: Complete matrix = low-rank matrix + noise. Missingness is completely at random. It’s distinguished by not imposing factorization restrictions but utilizing regularization to define the estimator, particularly effective with the nuclear norm as a regularizer for complex missing patterns (Athey et al. 2021). Contributions of Athey et al. (2021) matrix completion include: Recognizing structured missing patterns allowing time correlation, enabling staggered adoption. Modifying estimators for unregularized unit and time fixed effects. Performing well across various \\(T\\) and \\(N\\) sizes, unlike unconfoundedness and synthetic control, which falter when \\(T &gt;&gt; N\\) or \\(N &gt;&gt; T\\), respectively. Identifying Assumptions: SUTVA: Potential outcomes indexed only by the unit’s contemporaneous treatment. No dynamic effects (it’s okay under staggered adoption, it gives a different interpretation of estimand). Setup: \\(Y_{it}(0)\\) and \\(Y_{it}(1)\\) represent potential outcomes of \\(Y_{it}\\). \\(W_{it}\\) is a binary treatment indicator. Aim to estimate the average effect for the treated: \\[ \\tau = \\frac{\\sum_{(i,t): W_{it} = 1}[Y_{it}(1) - Y_{it}(0)]}{\\sum_{i,t}W_{it}} \\] We observe all relevant values for \\(Y_{it}(1)\\) We want to impute missing entries in the \\(Y(0)\\) matrix for treated units with \\(W_{it} = 1\\). Define \\(\\mathcal{M}\\) as the set of pairs of indices \\((i,t)\\), where \\(i \\in N\\) and \\(t \\in T\\), corresponding to missing entries with \\(W_{it} = 1\\); \\(\\mathcal{O}\\) as the set of pairs of indices corresponding to observed entries in \\(Y(0)\\) with \\(W_{it} = 0\\). Data is conceptualized as two \\(N \\times T\\) matrices, one incomplete and one complete: \\[ Y = \\begin{pmatrix} Y_{11} &amp; Y_{12} &amp; ? &amp; \\cdots &amp; Y_{1T} \\\\ ? &amp; ? &amp; Y_{23} &amp; \\cdots &amp; ? \\\\ Y_{31} &amp; ? &amp; Y_{33} &amp; \\cdots &amp; ? \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Y_{N1} &amp; ? &amp; Y_{N3} &amp; \\cdots &amp; ? \\end{pmatrix}, \\] and \\[ W = \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix}, \\] where \\[ W_{it} = \\begin{cases} 1 &amp; \\text{if } (i,t) \\in \\mathcal{M}, \\\\ 0 &amp; \\text{if } (i,t) \\in \\mathcal{O}, \\end{cases} \\] is an indicator for the event that the corresponding component of \\(Y\\), that is \\(Y_{it}\\), is missing. Patterns of missing data in \\(\\mathbf{Y}\\): Block (treatment) structure with 2 special cases Single-treated-period block structure (G. W. Imbens and Rubin 2015) Single-treated-unit block structure (Abadie, Diamond, and Hainmueller 2010) Staggered Adoption Shape of matrix \\(\\mathbf{Y}\\): Thin (\\(N &gt;&gt; T\\)) Fat (\\(T &gt;&gt; N\\)) Square (\\(N \\approx T\\)) Combinations of patterns of missingness and shape create different literatures: Horizontal Regression = Thin matrix + single-treated-period block (focusing on cross-section correlation patterns) Vertical Regression = Fat matrix + single-treated-unit block (focusing on time-series correlation patterns) TWFE = Square matrix To combine, we can exploit both stable patterns over time, and across units (e.g., TWFE, interactive FEs or matrix completion). For the same factor model \\[ \\mathbf{Y = UV}^T + \\mathbf{\\epsilon} \\] where \\(\\mathbf{U}\\) is \\(N \\times R\\) and \\(\\mathbf{V}\\) is \\(T\\times R\\) The interactive FE literature focuses on a fixed number of factors \\(R\\) in \\(\\mathbf{U, V}\\), while matrix completion focuses on impute \\(\\mathbf{Y}\\) using some forms regularization (e.g., nuclear norm). We can also estimate the number of factors \\(R\\) Moon and Weidner (2015) To use the nuclear norm minimization estimator, we must add a penalty term to regularize the objective function. However, before doing so, we need to explicitly estimate the time (\\(\\lambda_t\\)) and unit (\\(\\mu_i\\)) fixed effects implicitly embedded in the missing data matrix to reduce the bias of the regularization term. Specifically, \\[ Y_{it} =L_{it} + \\sum_{p = 1}^P \\sum_{q= 1}^Q X_{ip} H_{pq}Z_{qt} + \\mu_i + \\lambda_t + V_{it} \\beta + \\epsilon_{it} \\] where \\(X_{ip}\\) is a matrix of \\(p\\) variables for unit \\(i\\) \\(Z_{qt}\\) is a matrix of \\(q\\) variables for time \\(t\\) \\(V_{it}\\) is a matrix of time-varying variables. Lasso-type \\(l_1\\) norm (\\(||H|| = \\sum_{p = 1}^p \\sum_{q = 1}^Q |H_{pq}|\\)) is used to shrink \\(H \\to 0\\) There are several options to regularize \\(L\\): Frobenius (i.e., Ridge): not informative since it imputes missing values as 0. Nuclear Norm (i.e., Lasso): computationally feasible (using SOFT-IMPUTE algorithm (Mazumder, Hastie, and Tibshirani 2010)). Rank (i.e., Subset selection): not computationally feasible This method allows to use more covariates leverage data from treated units (can be used when treatment effect is constant and pattern of missing is not complex). have autocorrelated errors have weighted loss function (i.e., take into account the probability of outcomes for a unit being missing) 26.9.4 Gardner (2022) and Borusyak, Jaravel, and Spiess (2021) Estimate the time and unit fixed effects separately Known as the imputation method (Borusyak, Jaravel, and Spiess 2021) or two-stage DiD (Gardner 2022) # remotes::install_github(&quot;kylebutts/did2s&quot;) library(did2s) library(ggplot2) library(fixest) library(tidyverse) data(base_stagg) est &lt;- did2s( data = base_stagg |&gt; mutate(treat = if_else(time_to_treatment &gt;= 0, 1, 0)), yname = &quot;y&quot;, first_stage = ~ x1 | id + year, second_stage = ~ i(time_to_treatment, ref = c(-1,-1000)), treatment = &quot;treat&quot; , cluster_var = &quot;id&quot; ) fixest::esttable(est) #&gt; est #&gt; Dependent Var.: y #&gt; #&gt; time_to_treatment = -9 0.3518** (0.1332) #&gt; time_to_treatment = -8 -0.3130* (0.1213) #&gt; time_to_treatment = -7 0.0894 (0.2367) #&gt; time_to_treatment = -6 0.0312 (0.2176) #&gt; time_to_treatment = -5 -0.2079 (0.1519) #&gt; time_to_treatment = -4 -0.1152 (0.1438) #&gt; time_to_treatment = -3 -0.0127 (0.1483) #&gt; time_to_treatment = -2 0.1503 (0.1440) #&gt; time_to_treatment = 0 -5.139*** (0.3680) #&gt; time_to_treatment = 1 -3.480*** (0.3784) #&gt; time_to_treatment = 2 -2.021*** (0.3055) #&gt; time_to_treatment = 3 -0.6965. (0.3947) #&gt; time_to_treatment = 4 1.070** (0.3501) #&gt; time_to_treatment = 5 2.173*** (0.4456) #&gt; time_to_treatment = 6 4.449*** (0.3680) #&gt; time_to_treatment = 7 4.864*** (0.3698) #&gt; time_to_treatment = 8 6.187*** (0.2702) #&gt; ______________________ __________________ #&gt; S.E. type Custom #&gt; Observations 950 #&gt; R2 0.62486 #&gt; Adj. R2 0.61843 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fixest::iplot( est, main = &quot;Event study&quot;, xlab = &quot;Time to treatment&quot;, ref.line = -1 ) coefplot(est) mult_est &lt;- did2s::event_study( data = fixest::base_stagg |&gt; dplyr::mutate(year_treated = dplyr::if_else(year_treated == 10000, 0, year_treated)), gname = &quot;year_treated&quot;, idname = &quot;id&quot;, tname = &quot;year&quot;, yname = &quot;y&quot;, estimator = &quot;all&quot; ) #&gt; Error in purrr::map(., function(y) { : ℹ In index: 1. #&gt; ℹ With name: y. #&gt; Caused by error in `.subset2()`: #&gt; ! no such index at level 1 did2s::plot_event_study(mult_est) Borusyak, Jaravel, and Spiess (2021) didimputation This version is currently not working library(didimputation) library(fixest) data(&quot;base_stagg&quot;) did_imputation( data = base_stagg, yname = &quot;y&quot;, gname = &quot;year_treated&quot;, tname = &quot;year&quot;, idname = &quot;id&quot; ) 26.9.5 Clément De Chaisemartin and d’Haultfoeuille (2020) use twowayfeweights from GitHub (Clément De Chaisemartin and d’Haultfoeuille 2020) Average instant treatment effect of changes in the treatment This relaxes the no-carryover-effect assumption. Drawbacks: Cannot observe treatment effects that manifest over time. There still isn’t a good package for this estimator. # remotes::install_github(&quot;shuo-zhang-ucsb/did_multiplegt&quot;) library(DIDmultiplegt) library(fixest) library(tidyverse) data(&quot;base_stagg&quot;) res &lt;- did_multiplegt( df = base_stagg |&gt; dplyr::mutate(treatment = dplyr::if_else(time_to_treatment &lt; 0, 0, 1)), Y = &quot;y&quot;, G = &quot;year_treated&quot;, T = &quot;year&quot;, D = &quot;treatment&quot;, controls = &quot;x1&quot;, # brep = 20, # getting SE will take forever placebo = 5, dynamic = 5, average_effect = &quot;simple&quot; ) head(res) #&gt; $effect #&gt; treatment #&gt; -5.214207 #&gt; #&gt; $N_effect #&gt; [1] 675 #&gt; #&gt; $N_switchers_effect #&gt; [1] 45 #&gt; #&gt; $dynamic_1 #&gt; [1] -3.63556 #&gt; #&gt; $N_dynamic_1 #&gt; [1] 580 #&gt; #&gt; $N_switchers_effect_1 #&gt; [1] 40 I don’t recommend the TwoWayFEWeights since it only gives the aggregated average treatment effect over all post-treatment periods, but not for each period. library(TwoWayFEWeights) res &lt;- twowayfeweights( data = base_stagg |&gt; dplyr::mutate(treatment = dplyr::if_else(time_to_treatment &lt; 0, 0, 1)), Y = &quot;y&quot;, G = &quot;year_treated&quot;, T = &quot;year&quot;, D = &quot;treatment&quot;, summary_measures = T ) print(res) #&gt; Under the common trends assumption, beta estimates a weighted sum of 45 ATTs. #&gt; 41 ATTs receive a positive weight, and 4 receive a negative weight. #&gt; #&gt; ────────────────────────────────────────── #&gt; Treat. var: treatment ATTs Σ weights #&gt; ────────────────────────────────────────── #&gt; Positive weights 41 1.0238 #&gt; Negative weights 4 -0.0238 #&gt; ────────────────────────────────────────── #&gt; Total 45 1 #&gt; ────────────────────────────────────────── #&gt; #&gt; Summary Measures: #&gt; TWFE Coefficient (β_fe): -3.4676 #&gt; min σ(Δ) compatible with β_fe and Δ_TR = 0: 4.8357 #&gt; min σ(Δ) compatible with β_fe and Δ_TR of a different sign: 36.1549 #&gt; Reference: Corollary 1, de Chaisemartin, C and D&#39;Haultfoeuille, X (2020a) #&gt; #&gt; The development of this package was funded by the European Union (ERC, REALLYCREDIBLE,GA N. 101043899). 26.9.6 Callaway and Sant’Anna (2021) staggered package Group-time average treatment effect library(staggered) library(fixest) data(&quot;base_stagg&quot;) # simple weighted average staggered( df = base_stagg, i = &quot;id&quot;, t = &quot;year&quot;, g = &quot;year_treated&quot;, y = &quot;y&quot;, estimand = &quot;simple&quot; ) #&gt; estimate se se_neyman #&gt; 1 -0.7110941 0.2211943 0.2214245 # cohort weighted average staggered( df = base_stagg, i = &quot;id&quot;, t = &quot;year&quot;, g = &quot;year_treated&quot;, y = &quot;y&quot;, estimand = &quot;cohort&quot; ) #&gt; estimate se se_neyman #&gt; 1 -2.724242 0.2701093 0.2701745 # calendar weighted average staggered( df = base_stagg, i = &quot;id&quot;, t = &quot;year&quot;, g = &quot;year_treated&quot;, y = &quot;y&quot;, estimand = &quot;calendar&quot; ) #&gt; estimate se se_neyman #&gt; 1 -0.5861831 0.1768297 0.1770729 res &lt;- staggered( df = base_stagg, i = &quot;id&quot;, t = &quot;year&quot;, g = &quot;year_treated&quot;, y = &quot;y&quot;, estimand = &quot;eventstudy&quot;, eventTime = -9:8 ) head(res) #&gt; estimate se se_neyman eventTime #&gt; 1 0.20418779 0.1045821 0.1045821 -9 #&gt; 2 -0.06215104 0.1669703 0.1670886 -8 #&gt; 3 0.02744671 0.1413273 0.1420377 -7 #&gt; 4 -0.02131747 0.2203695 0.2206338 -6 #&gt; 5 -0.30690897 0.2015697 0.2036412 -5 #&gt; 6 0.05594029 0.1908101 0.1921745 -4 ggplot( res |&gt; mutate( ymin_ptwise = estimate + 1.96 * se, ymax_ptwise = estimate - 1.96 * se ), aes(x = eventTime, y = estimate) ) + geom_pointrange(aes(ymin = ymin_ptwise, ymax = ymax_ptwise)) + geom_hline(yintercept = 0) + xlab(&quot;Event Time&quot;) + ylab(&quot;Estimate&quot;) + causalverse::ama_theme() # Callaway and Sant&#39;Anna estimator for the simple weighted average staggered_cs( df = base_stagg, i = &quot;id&quot;, t = &quot;year&quot;, g = &quot;year_treated&quot;, y = &quot;y&quot;, estimand = &quot;simple&quot; ) #&gt; estimate se se_neyman #&gt; 1 -0.7994889 0.4484987 0.4486122 # Sun and Abraham estimator for the simple weighted average staggered_sa( df = base_stagg, i = &quot;id&quot;, t = &quot;year&quot;, g = &quot;year_treated&quot;, y = &quot;y&quot;, estimand = &quot;simple&quot; ) #&gt; estimate se se_neyman #&gt; 1 -0.7551901 0.4407818 0.4409525 Fisher’s Randomization Test (i.e., permutation test) \\(H_0\\): \\(TE = 0\\) staggered( df = base_stagg, i = &quot;id&quot;, t = &quot;year&quot;, g = &quot;year_treated&quot;, y = &quot;y&quot;, estimand = &quot;simple&quot;, compute_fisher = T, num_fisher_permutations = 100 ) #&gt; estimate se se_neyman fisher_pval fisher_pval_se_neyman #&gt; 1 -0.7110941 0.2211943 0.2214245 0 0 #&gt; num_fisher_permutations #&gt; 1 100 26.9.7 L. Sun and Abraham (2021) This paper utilizes the Cohort Average Treatment Effects on the Treated (CATT), which measures the cohort-specific average difference in outcomes relative to those never treated, offering a more detailed analysis than Goodman-Bacon (2021). In scenarios lacking a never-treated group, this method designates the last cohort to be treated as the control group. Parameter of interest is the cohort-specific ATT \\(l\\) periods from int ital treatment period \\(e\\) \\[ CATT = E[Y_{i, e + I} - Y_{i, e + I}^\\infty|E_i = e] \\] This paper uses an interaction-weighted estimator in a panel data setting, where the original paper Gibbons, Suárez Serrato, and Urbancic (2018) used the same idea in a cross-sectional setting. Callaway and Sant’Anna (2021) explores group-time average treatment effects, employing cohorts that have not yet been treated as controls, and permits conditioning on time-varying covariates. Athey and Imbens (2022) examines the treatment effect in relation to the counterfactual outcome of the always-treated group, diverging from the conventional focus on the never-treated. Borusyak, Jaravel, and Spiess (2021) presumes a uniform treatment effect across cohorts, effectively simplifying CATT to ATT. Identifying Assumptions for dynamic TWFE: Parallel Trends: Baseline outcomes follow parallel trends across cohorts before treatment. This gives us all CATT (including own, included bins, and excluded bins) No Anticipatory Behavior: There is no effect of the treatment during pre-treatment periods, indicating that outcomes are not influenced by the anticipation of treatment. Treatment Effect Homogeneity: The treatment effect is consistent across cohorts for each relative period. Each adoption cohort should have the same path of treatment effects. In other words, the trajectory of each treatment cohort is similar. Compare to other designs: Athey and Imbens (2022) assume heterogeneity of treatment effects vary over adoption cohorts, but not over time. Borusyak, Jaravel, and Spiess (2021) assume heterogeneity of treatment effects vary over time, but not over adoption cohorts. Callaway and Sant’Anna (2021) assume heterogeneity of treatment effects vary over time and across cohorts. Clement De Chaisemartin and D’haultfœuille (2023) assume heterogeneity of treatment effects vary across groups and over time. Goodman-Bacon (2021) assume heterogeneity either “vary across units but not over time” or “vary over time but not across units”. L. Sun and Abraham (2021) allows for treatment effect heterogeneity across units and time. Sources of Heterogeneous Treatment Effects Adoption cohorts can differ based on certain covariates. Similarly, composition of units within each adoption cohort is different. The response to treatment varies among cohorts if units self-select their initial treatment timing based on anticipated treatment effects. However, this self-selection is still compatible with the parallel trends assumption. This is true if units choose based on an evaluation of baseline outcomes - that is, if baseline outcomes are similar (following parallel trends), then we might not see selection into treatment based on the evaluation of the baseline outcome. Treatment effects can vary across cohorts due to calendar time-varying effects, such as changes in economic conditions. Notes: If you do TWFE, you actually have to drop 2 terms to avoid multicollinearity: Period right before treatment (this one was known before this paper) Drop or bin or trim a distant lag period (this one was clarified by the paper). The reason is before of the multicollinearity in the linear relationship between TWFE and the relative period indicators. Contamination of the treatment effect estimates from excluded periods is a type of “normalization”. To avoid this, we have to assume that all pre-treatment periods have the same CATT. L. Sun and Abraham (2021) estimation method gives reasonable weights to CATT (i..e, weights that sum to 1, and are non negative). They estimate the weighted average of CATT where the weights are shares of cohorts that experience at least \\(l\\) periods after to treatment, normalized by the size of total periods \\(g\\). Aggregation of CATT is similar to that of Callaway and Sant’Anna (2021) Application can use fixest in r with sunab function library(fixest) data(&quot;base_stagg&quot;) res_sa20 = feols(y ~ x1 + sunab(year_treated, year) | id + year, base_stagg) iplot(res_sa20) summary(res_sa20, agg = &quot;att&quot;) #&gt; OLS estimation, Dep. Var.: y #&gt; Observations: 950 #&gt; Fixed-effects: id: 95, year: 10 #&gt; Standard-errors: Clustered (id) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; x1 0.994678 0.018378 54.12293 &lt; 2.2e-16 *** #&gt; ATT -1.133749 0.205070 -5.52858 2.882e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; RMSE: 0.921817 Adj. R2: 0.887984 #&gt; Within R2: 0.876406 summary(res_sa20, agg = c(&quot;att&quot; = &quot;year::[^-]&quot;)) #&gt; OLS estimation, Dep. Var.: y #&gt; Observations: 950 #&gt; Fixed-effects: id: 95, year: 10 #&gt; Standard-errors: Clustered (id) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; x1 0.994678 0.018378 54.122928 &lt; 2.2e-16 *** #&gt; year::-9:cohort::10 0.351766 0.359073 0.979649 3.2977e-01 #&gt; year::-8:cohort::9 0.033914 0.471437 0.071937 9.4281e-01 #&gt; year::-8:cohort::10 -0.191932 0.352896 -0.543876 5.8781e-01 #&gt; year::-7:cohort::8 -0.589387 0.736910 -0.799809 4.2584e-01 #&gt; year::-7:cohort::9 0.872995 0.493427 1.769249 8.0096e-02 . #&gt; year::-7:cohort::10 0.019512 0.603411 0.032336 9.7427e-01 #&gt; year::-6:cohort::7 -0.042147 0.865736 -0.048683 9.6127e-01 #&gt; year::-6:cohort::8 -0.657571 0.573257 -1.147078 2.5426e-01 #&gt; year::-6:cohort::9 0.877743 0.533331 1.645775 1.0315e-01 #&gt; year::-6:cohort::10 -0.403635 0.347412 -1.161832 2.4825e-01 #&gt; year::-5:cohort::6 -0.658034 0.913407 -0.720418 4.7306e-01 #&gt; year::-5:cohort::7 -0.316974 0.697939 -0.454158 6.5076e-01 #&gt; year::-5:cohort::8 -0.238213 0.469744 -0.507113 6.1326e-01 #&gt; year::-5:cohort::9 0.301477 0.604201 0.498968 6.1897e-01 #&gt; year::-5:cohort::10 -0.564801 0.463214 -1.219308 2.2578e-01 #&gt; year::-4:cohort::5 -0.983453 0.634492 -1.549984 1.2451e-01 #&gt; year::-4:cohort::6 0.360407 0.858316 0.419900 6.7552e-01 #&gt; year::-4:cohort::7 -0.430610 0.661356 -0.651102 5.1657e-01 #&gt; year::-4:cohort::8 -0.895195 0.374901 -2.387816 1.8949e-02 * #&gt; year::-4:cohort::9 -0.392478 0.439547 -0.892914 3.7418e-01 #&gt; year::-4:cohort::10 0.519001 0.597880 0.868069 3.8757e-01 #&gt; year::-3:cohort::4 0.591288 0.680169 0.869324 3.8688e-01 #&gt; year::-3:cohort::5 -1.000650 0.971741 -1.029749 3.0577e-01 #&gt; year::-3:cohort::6 0.072188 0.652641 0.110609 9.1216e-01 #&gt; year::-3:cohort::7 -0.836820 0.804275 -1.040465 3.0079e-01 #&gt; year::-3:cohort::8 -0.783148 0.701312 -1.116691 2.6697e-01 #&gt; year::-3:cohort::9 0.811285 0.564470 1.437251 1.5397e-01 #&gt; year::-3:cohort::10 0.527203 0.320051 1.647250 1.0285e-01 #&gt; year::-2:cohort::3 0.036941 0.673771 0.054828 9.5639e-01 #&gt; year::-2:cohort::4 0.832250 0.859544 0.968246 3.3541e-01 #&gt; year::-2:cohort::5 -1.574086 0.525563 -2.995051 3.5076e-03 ** #&gt; year::-2:cohort::6 0.311758 0.832095 0.374666 7.0875e-01 #&gt; year::-2:cohort::7 -0.558631 0.871993 -0.640638 5.2332e-01 #&gt; year::-2:cohort::8 0.429591 0.305270 1.407250 1.6265e-01 #&gt; year::-2:cohort::9 1.201899 0.819186 1.467188 1.4566e-01 #&gt; year::-2:cohort::10 -0.002429 0.682087 -0.003562 9.9717e-01 #&gt; att -1.133749 0.205070 -5.528584 2.8820e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; RMSE: 0.921817 Adj. R2: 0.887984 #&gt; Within R2: 0.876406 # alternatively summary(res_sa20, agg = c(&quot;att&quot; = &quot;year::[012345678]&quot;)) |&gt; etable(digits = 2) #&gt; summary(res_.. #&gt; Dependent Var.: y #&gt; #&gt; x1 0.99*** (0.02) #&gt; year = -9 x cohort = 10 0.35 (0.36) #&gt; year = -8 x cohort = 9 0.03 (0.47) #&gt; year = -8 x cohort = 10 -0.19 (0.35) #&gt; year = -7 x cohort = 8 -0.59 (0.74) #&gt; year = -7 x cohort = 9 0.87. (0.49) #&gt; year = -7 x cohort = 10 0.02 (0.60) #&gt; year = -6 x cohort = 7 -0.04 (0.87) #&gt; year = -6 x cohort = 8 -0.66 (0.57) #&gt; year = -6 x cohort = 9 0.88 (0.53) #&gt; year = -6 x cohort = 10 -0.40 (0.35) #&gt; year = -5 x cohort = 6 -0.66 (0.91) #&gt; year = -5 x cohort = 7 -0.32 (0.70) #&gt; year = -5 x cohort = 8 -0.24 (0.47) #&gt; year = -5 x cohort = 9 0.30 (0.60) #&gt; year = -5 x cohort = 10 -0.56 (0.46) #&gt; year = -4 x cohort = 5 -0.98 (0.63) #&gt; year = -4 x cohort = 6 0.36 (0.86) #&gt; year = -4 x cohort = 7 -0.43 (0.66) #&gt; year = -4 x cohort = 8 -0.90* (0.37) #&gt; year = -4 x cohort = 9 -0.39 (0.44) #&gt; year = -4 x cohort = 10 0.52 (0.60) #&gt; year = -3 x cohort = 4 0.59 (0.68) #&gt; year = -3 x cohort = 5 -1.0 (0.97) #&gt; year = -3 x cohort = 6 0.07 (0.65) #&gt; year = -3 x cohort = 7 -0.84 (0.80) #&gt; year = -3 x cohort = 8 -0.78 (0.70) #&gt; year = -3 x cohort = 9 0.81 (0.56) #&gt; year = -3 x cohort = 10 0.53 (0.32) #&gt; year = -2 x cohort = 3 0.04 (0.67) #&gt; year = -2 x cohort = 4 0.83 (0.86) #&gt; year = -2 x cohort = 5 -1.6** (0.53) #&gt; year = -2 x cohort = 6 0.31 (0.83) #&gt; year = -2 x cohort = 7 -0.56 (0.87) #&gt; year = -2 x cohort = 8 0.43 (0.31) #&gt; year = -2 x cohort = 9 1.2 (0.82) #&gt; year = -2 x cohort = 10 -0.002 (0.68) #&gt; att -1.1*** (0.21) #&gt; Fixed-Effects: -------------- #&gt; id Yes #&gt; year Yes #&gt; _______________________ ______________ #&gt; S.E.: Clustered by: id #&gt; Observations 950 #&gt; R2 0.90982 #&gt; Within R2 0.87641 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using the same syntax as fixest # devtools::install_github(&quot;kylebutts/fwlplot&quot;) library(fwlplot) fwl_plot(y ~ x1, data = base_stagg) fwl_plot(y ~ x1 | id + year, data = base_stagg, n_sample = 100) fwl_plot(y ~ x1 | id + year, data = base_stagg, n_sample = 100, fsplit = ~ treated) 26.9.8 Wooldridge (2022) use etwfe(Extended two-way Fixed Effects) (Wooldridge 2022) 26.9.9 Doubly Robust DiD Also known as the locally efficient doubly robust DiD (Sant’Anna and Zhao 2020) Code example by the authors The package (not method) is rather limited application: Use OLS (cannot handle glm) Canonical DiD only (cannot handle DDD). library(DRDID) data(&quot;nsw_long&quot;) eval_lalonde_cps &lt;- subset(nsw_long, nsw_long$treated == 0 | nsw_long$sample == 2) head(eval_lalonde_cps) #&gt; id year treated age educ black married nodegree dwincl re74 hisp #&gt; 1 1 1975 NA 42 16 0 1 0 NA 0.000 0 #&gt; 2 1 1978 NA 42 16 0 1 0 NA 0.000 0 #&gt; 3 2 1975 NA 20 13 0 0 0 NA 2366.794 0 #&gt; 4 2 1978 NA 20 13 0 0 0 NA 2366.794 0 #&gt; 5 3 1975 NA 37 12 0 1 0 NA 25862.322 0 #&gt; 6 3 1978 NA 37 12 0 1 0 NA 25862.322 0 #&gt; early_ra sample experimental re #&gt; 1 NA 2 0 0.0000 #&gt; 2 NA 2 0 100.4854 #&gt; 3 NA 2 0 3317.4678 #&gt; 4 NA 2 0 4793.7451 #&gt; 5 NA 2 0 22781.8555 #&gt; 6 NA 2 0 25564.6699 # locally efficient doubly robust DiD Estimators for the ATT out &lt;- drdid( yname = &quot;re&quot;, tname = &quot;year&quot;, idname = &quot;id&quot;, dname = &quot;experimental&quot;, xformla = ~ age + educ + black + married + nodegree + hisp + re74, data = eval_lalonde_cps, panel = TRUE ) summary(out) #&gt; Call: #&gt; drdid(yname = &quot;re&quot;, tname = &quot;year&quot;, idname = &quot;id&quot;, dname = &quot;experimental&quot;, #&gt; xformla = ~age + educ + black + married + nodegree + hisp + #&gt; re74, data = eval_lalonde_cps, panel = TRUE) #&gt; ------------------------------------------------------------------ #&gt; Further improved locally efficient DR DID estimator for the ATT: #&gt; #&gt; ATT Std. Error t value Pr(&gt;|t|) [95% Conf. Interval] #&gt; -901.2703 393.6247 -2.2897 0.022 -1672.7747 -129.766 #&gt; ------------------------------------------------------------------ #&gt; Estimator based on panel data. #&gt; Outcome regression est. method: weighted least squares. #&gt; Propensity score est. method: inverse prob. tilting. #&gt; Analytical standard error. #&gt; ------------------------------------------------------------------ #&gt; See Sant&#39;Anna and Zhao (2020) for details. # Improved locally efficient doubly robust DiD estimator # for the ATT, with panel data # drdid_imp_panel() # Locally efficient doubly robust DiD estimator for the ATT, # with panel data # drdid_panel() # Locally efficient doubly robust DiD estimator for the ATT, # with repeated cross-section data # drdid_rc() # Improved locally efficient doubly robust DiD estimator for the ATT, # with repeated cross-section data # drdid_imp_rc() 26.9.10 Augmented/Forward DID DID Methods for Limited Pre-Treatment Periods: Method Scenario Approach Augmented DID (K. T. Li and Van den Bulte 2023) Treatment outcome is outside the range of control units Constructs the treatment counterfactual using a scaled average of control units Forward DID (K. T. Li 2024) Treatment outcome is within the range of control units Uses a forward selection algorithm to choose relevant control units before applying DID References "],["multiple-treatments.html", "26.10 Multiple Treatments", " 26.10 Multiple Treatments When you have 2 treatments in a setting, you should always try to model both of them under one regression to see whether they are significantly different. Never use one treated groups as control for the other, and run separate regression. Could check this answer \\[ \\begin{aligned} Y_{it} &amp;= \\alpha + \\gamma_1 Treat1_{i} + \\gamma_2 Treat2_{i} + \\lambda Post_t \\\\ &amp;+ \\delta_1(Treat1_i \\times Post_t) + \\delta_2(Treat2_i \\times Post_t) + \\epsilon_{it} \\end{aligned} \\] (Fricke 2017) (Clement De Chaisemartin and D’haultfœuille 2023) video code References "],["mediation-under-did.html", "26.11 Mediation Under DiD", " 26.11 Mediation Under DiD Check this post "],["assumptions-2.html", "26.12 Assumptions", " 26.12 Assumptions Parallel Trends: Difference between the treatment and control groups remain constant if there were no treatment. should be used in cases where you observe before and after an event you have treatment and control groups not in cases where treatment is not random confounders. To support we use Placebo test Prior Parallel Trends Test Linear additive effects (of group/unit specific and time-specific): If they are not additively interact, we have to use the weighted 2FE estimator (Imai and Kim 2021) Typically seen in the Staggered Dif-n-dif No anticipation: There is no causal effect of the treatment before its implementation. Possible issues Estimate dependent on functional form: When the size of the response depends (nonlinearly) on the size of the intervention, we might want to look at the the difference in the group with high intensity vs. low. Selection on (time–varying) unobservables Can use the overall sensitivity of coefficient estimates to hidden bias using Rosenbaum Bounds Long-term effects Parallel trends are more likely to be observed over shorter period (window of observation) Heterogeneous effects Different intensity (e.g., doses) for different groups. Ashenfelter dip (Ashenfelter and Card 1985) (job training program participant are more likely to experience an earning drop prior enrolling in these programs) Participants are systemically different from nonparticipants before the treatment, leading to the question of permanent or transitory changes. A fix to this transient endogeneity is to calculate long-run differences (exclude a number of periods symmetrically around the adoption/ implementation date). If we see a sustained impact, then we have strong evidence for the causal impact of a policy. (Proserpio and Zervas 2017b) (James J. Heckman and Smith 1999) (Jepsen, Troske, and Coomes 2014) (X. Li, Gan, and Hu 2011) Response to event might not be immediate (can’t be observed right away in the dependent variable) Using lagged dependent variable \\(Y_{it-1}\\) might be more appropriate (Blundell and Bond 1998) Other factors that affect the difference in trends between the two groups (i.e., treatment and control) will bias your estimation. Correlated observations within a group or time Incidental parameters problems (Lancaster 2000): it’s always better to use individual and time fixed effect. When examining the effects of variation in treatment timing, we have to be careful because negative weights (per group) can be negative if there is a heterogeneity in the treatment effects over time. Example: [Athey and Imbens (2022)](Borusyak, Jaravel, and Spiess 2021)(Goodman-Bacon 2021). In this case you should use new estimands proposed by @callaway2021difference(Clément De Chaisemartin and d’Haultfoeuille 2020), in the did package. If you expect lags and leads, see (L. Sun and Abraham 2021) (Gibbons, Suárez Serrato, and Urbancic 2018) caution when we suspect the treatment effect and treatment variance vary across groups 26.12.1 Prior Parallel Trends Test Plot the average outcomes over time for both treatment and control group before and after the treatment in time. Statistical test for difference in trends (using data from before the treatment period) \\[ Y = \\alpha_g + \\beta_1 T + \\beta_2 T\\times G + \\epsilon \\] where \\(Y\\) = the outcome variable \\(\\alpha_g\\) = group fixed effects \\(T\\) = time (e.g., specific year, or month) \\(\\beta_2\\) = different time trends for each group Hence, if \\(\\beta_2 =0\\) provides evidence that there are no differences in the trend for the two groups prior the time treatment. You can also use different functional forms (e..g, polynomial or nonlinear). If \\(\\beta_2 \\neq 0\\) statistically, possible reasons can be: Statistical significance can be driven by large sample Or the trends are so consistent, and just one period deviation can throw off the trends. Hence, statistical statistical significance. Technically, we can still salvage the research by including time fixed effects, instead of just the before-and-after time fixed effect (actually, most researchers do this mechanically anyway nowadays). However, a side effect can be that the time fixed effects can also absorb some part your treatment effect as well, especially in cases where the treatment effects vary with time (i.e., stronger or weaker over time) (Wolfers 2003). Debate: (Kahn-Lang and Lang 2020) argue that DiD will be more plausible when the treatment and control groups are similar not only in trends, but also in levels. Because when we observe dissimilar in levels prior to the treatment, why is it okay to think that this will not affect future trends? Show a plot of the dependent variable’s time series for treated and control groups and also a similar plot with matched sample. (Ryan et al. 2019) show evidence of matched DiD did well in the setting of non-parallel trends (at least in health care setting). In the case that we don’t have similar levels ex ante between treatment and control groups, functional form assumptions matter and we need justification for our choice. Pre-trend statistical tests: (Roth 2022) provides evidence that these test are usually under powered. See PretrendsPower and pretrends packages for correcting this. Parallel trends assumption is specific to both the transformation and units of the outcome (Roth and Sant’Anna 2023) See falsification test (\\(H_0\\): parallel trends is insensitive to functional form). library(tidyverse) library(fixest) od &lt;- causaldata::organ_donations %&gt;% # Use only pre-treatment data filter(Quarter_Num &lt;= 3) %&gt;% # Treatment variable dplyr::mutate(California = State == &#39;California&#39;) # use my package causalverse::plot_par_trends( data = od, metrics_and_names = list(&quot;Rate&quot; = &quot;Rate&quot;), treatment_status_var = &quot;California&quot;, time_var = list(Quarter_Num = &quot;Time&quot;), display_CI = F ) #&gt; [[1]] # do it manually # always good but plot the dependent out od |&gt; # group by treatment status and time dplyr::group_by(California, Quarter) |&gt; dplyr::summarize_all(mean) |&gt; dplyr::ungroup() |&gt; # view() ggplot2::ggplot(aes(x = Quarter_Num, y = Rate, color = California)) + ggplot2::geom_line() + causalverse::ama_theme() # but it&#39;s also important to use statistical test prior_trend &lt;- fixest::feols(Rate ~ i(Quarter_Num, California) | State + Quarter, data = od) fixest::coefplot(prior_trend, grid = F) fixest::iplot(prior_trend, grid = F) This is alarming since one of the periods is significantly different from 0, which means that our parallel trends assumption is not plausible. In cases where you might have violations of parallel trends assumption, check (Rambachan and Roth 2023) Impose restrictions on how different the post-treatment violations of parallel trends can be from the pre-trends. Partial identification of causal parameter Sensitivity analysis # https://github.com/asheshrambachan/HonestDiD # remotes::install_github(&quot;asheshrambachan/HonestDiD&quot;) # library(HonestDiD) Alternatively, Ban and Kedagni (2022) propose a method that with an information set (i.e., pre-treatment covariates), and an assumption on the selection bias in the post-treatment period (i.e., lies within the convex hull of all selection biases), they can still identify a set of ATT, and with stricter assumption on selection bias from the policymakers perspective, they can also have a point estimate. Alternatively, we can use the pretrends package to examine our assumptions (Roth 2022) 26.12.2 Placebo Test Procedure: Sample data only in the period before the treatment in time. Consider different fake cutoff in time, either Try the whole sequence in time Generate random treatment period, and use randomization inference to account for sampling distribution of the fake effect. Estimate the DiD model but with the post-time = 1 with the fake cutoff A significant DiD coefficient means that you violate the parallel trends! You have a big problem. Alternatively, When data have multiple control groups, drop the treated group, and assign another control group as a “fake” treated group. But even if it fails (i.e., you find a significant DiD effect) among the control groups, it can still be fine. However, this method is used under Synthetic Control Code by theeffectbook.net library(tidyverse) library(fixest) od &lt;- causaldata::organ_donations %&gt;% # Use only pre-treatment data dplyr::filter(Quarter_Num &lt;= 3) %&gt;% # Create fake treatment variables dplyr::mutate( FakeTreat1 = State == &#39;California&#39; &amp; Quarter %in% c(&#39;Q12011&#39;, &#39;Q22011&#39;), FakeTreat2 = State == &#39;California&#39; &amp; Quarter == &#39;Q22011&#39; ) clfe1 &lt;- fixest::feols(Rate ~ FakeTreat1 | State + Quarter, data = od) clfe2 &lt;- fixest::feols(Rate ~ FakeTreat2 | State + Quarter, data = od) fixest::etable(clfe1,clfe2) #&gt; clfe1 clfe2 #&gt; Dependent Var.: Rate Rate #&gt; #&gt; FakeTreat1TRUE 0.0061 (0.0051) #&gt; FakeTreat2TRUE -0.0017 (0.0028) #&gt; Fixed-Effects: --------------- ---------------- #&gt; State Yes Yes #&gt; Quarter Yes Yes #&gt; _______________ _______________ ________________ #&gt; S.E.: Clustered by: State by: State #&gt; Observations 81 81 #&gt; R2 0.99377 0.99376 #&gt; Within R2 0.00192 0.00015 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We would like the “supposed” DiD to be insignificant. 26.12.3 Assumption Violations Endogenous Timing If the timing of units can be influenced by strategic decisions in a DID analysis, an instrumental variable approach with a control function can be used to control for endogeneity in timing. Questionable Counterfactuals In situations where the control units may not serve as a reliable counterfactual for the treated units, matching methods such as propensity score matching or generalized random forest can be utilized. Additional methods can be found in Matching Methods. 26.12.4 Robustness Checks Placebo DiD (if the DiD estimate \\(\\neq 0\\), parallel trend is violated, and original DiD is biased): Group: Use fake treatment groups: A population that was not affect by the treatment Time: Redo the DiD analysis for period before the treatment (expected treatment effect is 0) (e.g., for previous year or period). Possible alternative control group: Expected results should be similar Try different windows (further away from the treatment point, other factors can creep in and nullify your effect). Treatment Reversal (what if we don’t see the treatment event) Higher-order polynomial time trend (to relax linearity assumption) Test whether other dependent variables that should not be affected by the event are indeed unaffected. Use the same control and treatment period (DiD \\(\\neq0\\), there is a problem) The triple-difference strategy involves examining the interaction between the treatment variable and the probability of being affected by the program, and the group-level participation rate. The identification assumption is that there are no differential trends between high and low participation groups in early versus late implementing countries. References "],["changes-in-changes.html", "Chapter 27 Changes-in-Changes", " Chapter 27 Changes-in-Changes Introduction The Changes-in-Changes (CiC) estimator, introduced by Athey and Imbens (2006), is an alternative to the Difference-in-Differences (DiD) strategy. Unlike traditional DiD, which estimates the Average Treatment Effect on the Treated (ATT), CiC focuses on the Quantile Treatment Effect on the Treated (QTT). QTT captures the difference between potential outcome distributions for treated units at a specific quantile. Beyond Averages: Policymakers often look beyond average program impacts, considering how benefits are distributed across different groups. Job Training Example: Two programs with the same negative average impact may be treated differently: one benefiting high earners might be rejected, while one benefiting low earners could be approved. Traditional Methods’ Limitations: Methods like linear regression, which assume uniform effects, fail to capture important distributional differences. QTEs’ Advantage: QTE methods are tailored for analyzing how treatment effects vary across different segments of a population. QTE vs. ATE: While QTEs provide detailed insights into distributional impacts, they also allow for the recovery of ATEs. However, ATEs are usually identified under weaker assumptions, making QTEs more suitable for exploring the shape of treatment effects rather than just their central tendency. Key Concepts Quantile Treatment Effect on the Treated (QTT): Difference in quantiles of treated units’ potential outcome distributions. Rank Preservation: Assumes each unit’s rank remains constant across potential outcome distributions—this is a strong assumption. Counterfactual Distribution: Estimation focuses on determining this distribution for the treated units in period 1. Estimating QTT CiC uses four distributions from a 2x2 DiD design: \\(F_{Y(0),00}\\): CDF of \\(Y(0)\\) for control units in period 0. \\(F_{Y(0),10}\\): CDF of \\(Y(0)\\) for treatment units in period 0. \\(F_{Y(0),01}\\): CDF of \\(Y(0)\\) for control units in period 1. \\(F_{Y(1),11}\\): CDF of \\(Y(1)\\) for treatment units in period 1. QTT is defined as the difference between the inverses of \\(F_{Y(1),11}\\) and the counterfactual distribution \\(F_{Y(0),11}\\) at quantile \\(q\\): \\[ \\Delta_\\theta^{QTT} = F_{Y(1), 11}^{-1} (\\theta) - F_{Y (0), 11}^{-1} (\\theta) \\] Estimation Process Counterfactual CDF: \\[ \\hat{F}_{Y(0),11}(y) = F_{y,01}\\left(F^{-1}_{y,00}\\left(F_{y,10}(y)\\right)\\right) \\] Equivalent Expression: \\[ \\hat{F}^{-1}_{Y(0),11}(\\theta) = F^{-1}_{y,01}\\left(F_{y,00}\\left(F^{-1}_{y,10}(\\theta)\\right)\\right) \\] Treatment Effect Estimate: \\[ \\hat{\\Delta}^{CIC}_{\\theta} = F^{-1}_{Y(1),11}(\\theta) - \\hat{F}^{-1}_{Y(0),11}(\\theta) \\] Equivalently: \\(\\Delta^{CIC}_{\\theta}\\) is the difference between two QTE estimates: \\[ \\Delta^{CIC}_{\\theta} = \\Delta^{QTE}_{\\theta,1} - \\Delta^{QTE}_{\\theta&#39;,0} \\] where: \\(\\Delta^{QTT}_{\\theta,1}\\) = change over time in \\(y\\) at quantile \\(\\theta\\) for \\(D = 1\\) group. \\(\\Delta^{QTU}_{\\theta&#39;,0}\\) = change over time in \\(y\\) at quantile \\(\\theta&#39;\\) for \\(D = 0\\) group, where \\(q&#39;\\) is the quantile in the \\(D = 0, T = 0\\) distribution corresponding to the value of \\(y\\) associated with quantile \\(\\theta\\) in the \\(D = 1, T = 0\\) distribution. Marketing Example Suppose a company implements a new online marketing strategy aimed at improving customer retention rates. QTT: The goal is to estimate the effect of the strategy on customer retention rates at different quantiles (e.g., median retention rate). Rank Preservation: Assumes customers’ rank in retention distribution remains the same, regardless of the strategy—this assumption is strong and should be carefully considered. Counterfactual: CiC helps estimate how retention rates would have changed without the new strategy by comparing it with a control group. References Athey and Imbens (2006) Frölich and Melly (2013): IV-based Callaway and Li (2019): panel data Huber, Schelker, and Strittmatter (2022) Additional Resources Code examples available in Stata. References "],["application-13.html", "27.1 Application", " 27.1 Application 27.1.1 ECIC package library(ecic) data(dat, package = &quot;ecic&quot;) mod = ecic( yvar = lemp, # dependent variable gvar = first.treat, # group indicator tvar = year, # time indicator ivar = countyreal, # unit ID dat = dat, # dataset boot = &quot;weighted&quot;, # bootstrap proceduce (&quot;no&quot;, &quot;normal&quot;, or &quot;weighted&quot;) nReps = 3 # number of bootstrap runs ) mod_res &lt;- summary(mod) mod_res #&gt; perc coefs se #&gt; 1 0.1 1.206140 0.021351711 #&gt; 2 0.2 1.316599 0.009225026 #&gt; 3 0.3 1.449963 0.001859468 #&gt; 4 0.4 1.583415 0.015296156 #&gt; 5 0.5 1.739932 0.011240454 #&gt; 6 0.6 1.915558 0.013060348 #&gt; 7 0.7 2.114966 0.014482208 #&gt; 8 0.8 2.363105 0.005173865 #&gt; 9 0.9 2.779202 0.020831180 ecic_plot(mod_res) 27.1.2 QTE package library(qte) data(lalonde) # randomized setting # qte is identical to qtet jt.rand &lt;- ci.qtet( re78 ~ treat, data = lalonde.exp, iters = 10 ) summary(jt.rand) #&gt; #&gt; Quantile Treatment Effect: #&gt; #&gt; tau QTE Std. Error #&gt; 0.05 0.00 0.00 #&gt; 0.1 0.00 0.00 #&gt; 0.15 0.00 0.00 #&gt; 0.2 0.00 18.33 #&gt; 0.25 338.65 377.74 #&gt; 0.3 846.40 470.45 #&gt; 0.35 1451.51 515.86 #&gt; 0.4 1177.72 869.19 #&gt; 0.45 1396.08 918.39 #&gt; 0.5 1123.55 925.74 #&gt; 0.55 1181.54 938.82 #&gt; 0.6 1466.51 951.64 #&gt; 0.65 2115.04 892.16 #&gt; 0.7 1795.12 842.66 #&gt; 0.75 2347.49 678.45 #&gt; 0.8 2278.12 971.21 #&gt; 0.85 2178.28 973.90 #&gt; 0.9 3239.60 1889.23 #&gt; 0.95 3979.62 2872.52 #&gt; #&gt; Average Treatment Effect: 1794.34 #&gt; Std. Error: 665.59 ggqte(jt.rand) # conditional independence assumption (CIA) jt.cia &lt;- ci.qte( re78 ~ treat, xformla = ~ age + education, data = lalonde.psid, iters = 10 ) summary(jt.cia) #&gt; #&gt; Quantile Treatment Effect: #&gt; #&gt; tau QTE Std. Error #&gt; 0.05 0.00 0.00 #&gt; 0.1 0.00 0.00 #&gt; 0.15 -4433.18 710.76 #&gt; 0.2 -8219.15 419.90 #&gt; 0.25 -10435.74 793.20 #&gt; 0.3 -12232.03 1037.03 #&gt; 0.35 -12428.30 1425.39 #&gt; 0.4 -14195.24 1793.20 #&gt; 0.45 -14248.66 1907.98 #&gt; 0.5 -15538.67 2095.11 #&gt; 0.55 -16550.71 2329.67 #&gt; 0.6 -15595.02 2686.45 #&gt; 0.65 -15827.52 2745.62 #&gt; 0.7 -16090.32 3390.26 #&gt; 0.75 -16091.49 3376.67 #&gt; 0.8 -17864.76 3245.52 #&gt; 0.85 -16756.71 3533.91 #&gt; 0.9 -17914.99 2305.10 #&gt; 0.95 -23646.22 2003.55 #&gt; #&gt; Average Treatment Effect: -13435.40 #&gt; Std. Error: 1259.01 ggqte(jt.cia) jt.ciat &lt;- ci.qtet( re78 ~ treat, xformla = ~ age + education, data = lalonde.psid, iters = 10 ) summary(jt.ciat) #&gt; #&gt; Quantile Treatment Effect: #&gt; #&gt; tau QTE Std. Error #&gt; 0.05 0.00 0.00 #&gt; 0.1 -1018.15 614.29 #&gt; 0.15 -3251.00 1557.37 #&gt; 0.2 -7240.86 1433.54 #&gt; 0.25 -8379.94 475.33 #&gt; 0.3 -8758.82 345.53 #&gt; 0.35 -9897.44 606.54 #&gt; 0.4 -10239.57 747.91 #&gt; 0.45 -10751.39 736.39 #&gt; 0.5 -10570.14 899.75 #&gt; 0.55 -11348.96 898.80 #&gt; 0.6 -11550.84 687.20 #&gt; 0.65 -12203.56 780.92 #&gt; 0.7 -13277.72 979.47 #&gt; 0.75 -14011.74 993.28 #&gt; 0.8 -14373.95 706.69 #&gt; 0.85 -14499.18 1048.62 #&gt; 0.9 -15008.63 2201.11 #&gt; 0.95 -15954.05 2655.30 #&gt; #&gt; Average Treatment Effect: 4266.19 #&gt; Std. Error: 600.51 ggqte(jt.ciat) QTE compares quantiles of the entire population under treatment and control, whereas QTET compares quantiles within the treated group itself. This difference means that QTE reflects the overall population-level impact, while QTET focuses on the treated group’s specific impact. CIA enables identification of both QTE and QTET, but since QTET is conditional on treatment, it might reflect different effects than QTE, especially when the treatment effect is heterogeneous across different subpopulations. For example, the QTE could show a more generalized effect across all individuals, while the QTET may reveal stronger or weaker effects for the subgroup that actually received the treatment. These are DID-like models With the distributional difference-in-differences assumption Callaway and Li (2019), which is an extension of the parallel trends assumption, we can estimate QTET. # distributional DiD assumption jt.pqtet &lt;- panel.qtet( re ~ treat, t = 1978, tmin1 = 1975, tmin2 = 1974, tname = &quot;year&quot;, idname = &quot;id&quot;, data = lalonde.psid.panel, iters = 10 ) summary(jt.pqtet) #&gt; #&gt; Quantile Treatment Effect: #&gt; #&gt; tau QTE Std. Error #&gt; 0.05 4779.21 1222.37 #&gt; 0.1 1987.35 776.82 #&gt; 0.15 842.95 3332.09 #&gt; 0.2 -7366.04 4852.87 #&gt; 0.25 -8449.96 3522.70 #&gt; 0.3 -7992.15 1201.51 #&gt; 0.35 -7429.21 1161.43 #&gt; 0.4 -6597.37 1288.64 #&gt; 0.45 -5519.45 1391.04 #&gt; 0.5 -4702.88 1129.80 #&gt; 0.55 -3904.52 1131.23 #&gt; 0.6 -2741.80 1157.60 #&gt; 0.65 -1507.31 1223.03 #&gt; 0.7 -771.12 1264.45 #&gt; 0.75 707.81 1280.34 #&gt; 0.8 580.00 793.09 #&gt; 0.85 821.75 969.38 #&gt; 0.9 -250.77 1662.49 #&gt; 0.95 -1874.54 2706.67 #&gt; #&gt; Average Treatment Effect: 2326.51 #&gt; Std. Error: 795.44 ggqte(jt.pqtet) With 2 periods, the distributional DiD assumption can partially identify QTET with bounds (Fan and Yu 2012) res_bound &lt;- bounds( re ~ treat, t = 1978, tmin1 = 1975, data = lalonde.psid.panel, idname = &quot;id&quot;, tname = &quot;year&quot; ) summary(res_bound) #&gt; #&gt; Bounds on the Quantile Treatment Effect on the Treated: #&gt; #&gt; tau Lower Bound Upper Bound #&gt; tau Lower Bound Upper Bound #&gt; 0.05 -51.72 0 #&gt; 0.1 -1220.84 0 #&gt; 0.15 -1881.9 0 #&gt; 0.2 -2601.32 0 #&gt; 0.25 -2916.38 485.23 #&gt; 0.3 -3080.16 943.05 #&gt; 0.35 -3327.89 1505.98 #&gt; 0.4 -3240.59 2133.59 #&gt; 0.45 -2982.51 2616.84 #&gt; 0.5 -3108.01 2566.2 #&gt; 0.55 -3342.66 2672.82 #&gt; 0.6 -3491.4 3065.7 #&gt; 0.65 -3739.74 3349.74 #&gt; 0.7 -4647.82 2992.03 #&gt; 0.75 -4826.78 3219.32 #&gt; 0.8 -5801.7 2702.33 #&gt; 0.85 -6588.61 2499.41 #&gt; 0.9 -8953.84 2020.84 #&gt; 0.95 -14283.61 397.04 #&gt; #&gt; Average Treatment Effect on the Treated: 2326.51 plot(res_bound) With a restrictive assumption that difference in the quantiles of the distribution of potential outcomes for the treated and untreated groups be the same for all values of quantiles, we can have the mean DiD model jt.mdid &lt;- ddid2( re ~ treat, t = 1978, tmin1 = 1975, tname = &quot;year&quot;, idname = &quot;id&quot;, data = lalonde.psid.panel, iters = 10 ) summary(jt.mdid) #&gt; #&gt; Quantile Treatment Effect: #&gt; #&gt; tau QTE Std. Error #&gt; 0.05 10616.61 744.99 #&gt; 0.1 5019.83 447.82 #&gt; 0.15 2388.12 334.57 #&gt; 0.2 1033.23 365.01 #&gt; 0.25 485.23 445.95 #&gt; 0.3 943.05 631.10 #&gt; 0.35 931.45 756.72 #&gt; 0.4 945.35 888.69 #&gt; 0.45 1205.88 903.88 #&gt; 0.5 1362.11 778.89 #&gt; 0.55 1279.05 871.73 #&gt; 0.6 1618.13 734.08 #&gt; 0.65 1834.30 674.83 #&gt; 0.7 1326.06 793.46 #&gt; 0.75 1586.35 714.42 #&gt; 0.8 1256.09 591.37 #&gt; 0.85 723.10 871.86 #&gt; 0.9 251.36 1703.13 #&gt; 0.95 -1509.92 2033.88 #&gt; #&gt; Average Treatment Effect: 2326.51 #&gt; Std. Error: 514.81 plot(jt.mdid) On top of the distributional DiD assumption, we need copula stability assumption (i.e., If, before the treatment, the units with the highest outcomes were improving the most, we would expect to see them improving the most in the current period too.) for these models: Aspect QDiD CiC Treatment of Time and Group Symmetric Asymmetric QTET Computation Not inherently scale-invariant Outcome Variable Scale-Invariant jt.qdid &lt;- QDiD( re ~ treat, t = 1978, tmin1 = 1975, tname = &quot;year&quot;, idname = &quot;id&quot;, data = lalonde.psid.panel, iters = 10, panel = T ) jt.cic &lt;- CiC( re ~ treat, t = 1978, tmin1 = 1975, tname = &quot;year&quot;, idname = &quot;id&quot;, data = lalonde.psid.panel, iters = 10, panel = T ) References "],["synthetic-control.html", "Chapter 28 Synthetic Control", " Chapter 28 Synthetic Control Examples in marketing: (Tirunillai and Tellis 2017): offline TV ad on Online Chatter (Yanwen Wang, Wu, and Zhu 2019): mobile hailing technology adoption on drivers’ hourly earnings (Guo, Sriram, and Manchanda 2020): payment disclosure laws effect on physician prescription behavior using Timing of the Massachusetts open payment law as the exogenous shock (Adalja et al. 2023): mandatory GMO labels had no impact on consumer demand (Using Vermont as a mandatory state) Notes The SC method provides asymptotically normal estimators for various linear panel data models, given sufficiently large pre-treatment periods, making it a natural alternative to the Difference-in-differences model (Arkhangelsky and Hirshberg 2023). SCM is superior than Matching Methods because it not only matches on covariates (i.e., pre-treatment variables), but also outcomes. For a review of the method, see (Abadie 2021) SCMs can also be used under the Bayesian framework (Bayesian Synthetic Control) where we do not have to impose any restrictive priori (S. Kim, Lee, and Gupta 2020) Different from Matching Methods because SCMs match on the pre-treatment outcomes in each period while Matching Methods match on the number of covariates. A data driven procedure to construct more comparable control groups (i.e., black box). To do causal inference with control and treatment group using Matching Methods, you typically have to have similar covariates in the control and the treated groups. However, if you don’t methods like Propensity Scores and DID can perform rather poorly (i.e., large bias). Advantages over Difference-in-differences Maximization of the observable similarity between control and treatment (maybe also unobservables) Can also be used in cases where no untreated case with similar on matching dimensions with treated cases Objective selection of controls. Advantages over linear regression Regression weights for the estimator will be outside of [0,1] (because regression allows extrapolation), and it will not be sparse (i.e., can be less than 0). No extrapolation under SCMs Explicitly state the fit (i.e., the weight) Can be estimated without the post-treatment outcomes for the control group (can’t p-hack) Advantages: From the selection criteria, researchers can understand the relative importance of each candidate Post-intervention outcomes are not used in synthetic. Hence, you can’t retro-fit. Observable similarity between control and treatment cases is maximized Disadvantages: It’s hard to argue for the weights you use to create the “synthetic control” SCM is recommended when Social events to evaluate large-scale program or policy Only one treated case with several control candidates. Assumptions Donor subject is a good match for the synthetic control (i.e., gap between the dependent of the donor subject and that of the synthetic control should be 0 before treatment) Only the treated subject undergoes the treatment and not any of the subjects in the donor pool. No other changes to the subjects during the whole window. The counterfactual outcome of the treatment group can be imputed in a linear combination of control groups. Identification: The exclusion restriction is met conditional on the pre-treatment outcomes. Synth provides an algorithm that finds weighted combination of the comparison units where the weights are chosen such that it best resembles the values of predictors of the outcome variable for the affected units before the intervention Setting (notation followed professor Alberto Abadie) \\(J + 1\\) units in periods \\(1, \\dots, T\\) The first unit is the treated one during \\(T_0 + 1, \\dots, T\\) \\(J\\) units are called a donor pool \\(Y_{it}^I\\) is the outcome for unit \\(i\\) if it’s exposed to the treatment during \\(T_0 + 1 , \\dots T\\) \\(Y_{it}^N\\) is the outcome for unit \\(i\\) if it’s not exposed to the treatment We try to estimate the effect of treatment on the treated unit \\[ \\tau_{1t} = Y_{1t}^I - Y_{1t}^N \\] where we observe the first treated unit already \\(Y_{1t}^I = Y_{1t}\\) To construct the synthetic control unit, we have to find appropriate weight for each donor in the donor pool by finding \\(\\mathbf{W} = (w_2, \\dots, w_{J=1})&#39;\\) where \\(w_j \\ge 0\\) for \\(j = 2, \\dots, J+1\\) \\(w_2 + \\dots + w_{J+1} = 1\\) The “appropriate” vector \\(\\mathbf{W}\\) here is constrained to \\[ \\min||\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W}|| \\] where \\(\\mathbf{X}_1\\) is the \\(k \\times 1\\) vector of pre-treatment characteristics for the treated unit \\(\\mathbf{X}_0\\) is the \\(k \\times J\\) matrix of pre-treatment characteristics for the untreated units For simplicity, researchers usually use \\[ \\begin{aligned} &amp;\\min||\\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W}|| \\\\ &amp;= (\\sum_{h=1}^k v_h(X_{h1}- w_2 X-{h2} - \\dots - w_{J+1} X_{hJ +1})^{1/2} \\end{aligned} \\] where \\(v_1, \\dots, v_k\\) is a vector positive constants that represent the predictive power of the \\(k\\) predictors on \\(Y_{1t}^N\\) (i.e., the potential outcome of the treated without treatment) and it can be chosen either explicitly by the researcher or by data-driven methods For penalized synthetic control (Abadie and L’hour 2021), the minimization problem becomes \\[ \\min_{\\mathbf{W}} ||\\mathbf{X}_1 - \\sum_{j=2}^{J + 1}W_j \\mathbf{X}_j ||^2 + \\lambda \\sum_{j=2}^{J+1} W_j ||\\mathbf{X}_1 - \\mathbf{X}_j||^2 \\] where \\(W_j \\ge 0\\) and \\(\\sum_{j=2}^{J+1} W_j = 1\\) \\(\\lambda &gt;0\\) balances over-fitting of the treated and minimize the sum of pairwise distances \\(\\lambda \\to 0\\): pure synthetic control (i.e solution for the unpenalized estimator) \\(\\lambda \\to \\infty\\): nearest neighbor matching Advantages: For \\(\\lambda &gt;0\\), you have unique and sparse solution Reduces the interpolation bias when averaging dissimilar units Penalized SC never uses dissimilar units Then the synthetic control estimator is \\[ \\hat{\\tau}_{1t} = Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt} \\] where \\(Y_{jt}\\) is the outcome for unit \\(j\\) at time \\(t\\) Consideration Under the factor model (Abadie, Diamond, and Hainmueller 2010) \\[ Y_{it}^N = \\mathbf{\\theta}_t \\mathbf{Z}_i + \\mathbf{\\lambda}_t \\mathbf{\\mu}_i + \\epsilon_{it} \\] where \\(Z_i\\) = observables \\(\\mu_i\\) = unobservables \\(\\epsilon_{it}\\) = unit-level transitory shock (i.e., random noise) with assumptions of \\(\\mathbf{W}^*\\) such that \\[ \\begin{aligned} \\sum_{j=2}^{J+1} w_j^* \\mathbf{Z}_j &amp;= \\mathbf{Z}_1 \\\\ &amp;\\dots \\\\ \\sum_{j=2}^{J+1} w_j^* Y_{j1} &amp;= Y_{11} \\\\ \\sum_{j=2}^{J+1} w_j^* Y_{jT_0} &amp;= Y_{1T_0} \\end{aligned} \\] Basically, we assume that the synthetic control is a good counterfactual when the treated unit is not exposed to the treatment. Then, the bias bound depends on close fit, which is controlled by the ratio between \\(\\epsilon_{it}\\) (transitory shock) and \\(T_0\\) (the number of pre-treatment periods). In other words, you should have good fit for \\(Y_{1t}\\) for pre-treatment period (i.e., \\(T_0\\) should be large while small variance in \\(\\epsilon_{it}\\)) When you have poor fit, you have to use bias correction version of the synthetic control. See Ben-Michael, Feller, and Rothstein (2020) Overfitting can be the result of small \\(T_0\\) (the number of pre-treatment periods), large \\(J\\) (the number of units in the donor pool), and large \\(\\epsilon_{it}\\) (noise) Mitigation: put only similar units (to the treated one) in the donor pool To make inference, we have to create a permutation distribution (by iteratively reassigning the treatment to the units in the donor pool and estimate the placebo effects in each iteration). We say there is an effect of the treatment when the magnitude of value of the treatment effect on the treated unit is extreme relative to the permutation distribution. It’s recommended to use one-sided inference. And the permutation distribution is superior to the p-values alone (because sampling-based inference is hard under SCMs either because of undefined sampling mechanism or the sample is the population). For benchmark (permutation) distribution (e.g., uniform), see (Firpo and Possebom 2018) References "],["applications-1.html", "28.1 Applications", " 28.1 Applications 28.1.1 Example 1 by Danilo Freire # install.packages(&quot;Synth&quot;) # install.packages(&quot;gsynth&quot;) library(&quot;Synth&quot;) library(&quot;gsynth&quot;) simulate data for 10 states and 30 years. State A receives the treatment T = 20 after year 15. set.seed(1) year &lt;- rep(1:30, 10) state &lt;- rep(LETTERS[1:10], each = 30) X1 &lt;- round(rnorm(300, mean = 2, sd = 1), 2) X2 &lt;- round(rbinom(300, 1, 0.5) + rnorm(300), 2) Y &lt;- round(1 + 2 * X1 + rnorm(300), 2) df &lt;- as.data.frame(cbind(Y, X1, X2, state, year)) df$Y &lt;- as.numeric(as.character(df$Y)) df$X1 &lt;- as.numeric(as.character(df$X1)) df$X2 &lt;- as.numeric(as.character(df$X2)) df$year &lt;- as.numeric(as.character(df$year)) df$state.num &lt;- rep(1:10, each = 30) df$state &lt;- as.character(df$state) df$`T` &lt;- ifelse(df$state == &quot;A&quot; &amp; df$year &gt;= 15, 1, 0) df$Y &lt;- ifelse(df$state == &quot;A&quot; &amp; df$year &gt;= 15, df$Y + 20, df$Y) str(df) #&gt; &#39;data.frame&#39;: 300 obs. of 7 variables: #&gt; $ Y : num 2.29 4.51 2.07 8.87 4.37 1.32 8 7.49 6.98 3.72 ... #&gt; $ X1 : num 1.37 2.18 1.16 3.6 2.33 1.18 2.49 2.74 2.58 1.69 ... #&gt; $ X2 : num 1.96 0.4 -0.75 -0.56 -0.45 1.06 0.51 -2.1 0 0.54 ... #&gt; $ state : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... #&gt; $ year : num 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ state.num: int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ T : num 0 0 0 0 0 0 0 0 0 0 ... dataprep.out &lt;- dataprep( df, predictors = c(&quot;X1&quot;, &quot;X2&quot;), dependent = &quot;Y&quot;, unit.variable = &quot;state.num&quot;, time.variable = &quot;year&quot;, unit.names.variable = &quot;state&quot;, treatment.identifier = 1, controls.identifier = c(2:10), time.predictors.prior = c(1:14), time.optimize.ssr = c(1:14), time.plot = c(1:30) ) synth.out &lt;- synth(dataprep.out) #&gt; #&gt; X1, X0, Z1, Z0 all come directly from dataprep object. #&gt; #&gt; #&gt; **************** #&gt; searching for synthetic control unit #&gt; #&gt; #&gt; **************** #&gt; **************** #&gt; **************** #&gt; #&gt; MSPE (LOSS V): 9.831789 #&gt; #&gt; solution.v: #&gt; 0.3888387 0.6111613 #&gt; #&gt; solution.w: #&gt; 0.1115941 0.1832781 0.1027237 0.312091 0.06096758 0.03509706 0.05893735 0.05746256 0.07784853 print(synth.tables &lt;- synth.tab( dataprep.res = dataprep.out, synth.res = synth.out) ) #&gt; $tab.pred #&gt; Treated Synthetic Sample Mean #&gt; X1 2.028 2.028 2.017 #&gt; X2 0.513 0.513 0.394 #&gt; #&gt; $tab.v #&gt; v.weights #&gt; X1 0.389 #&gt; X2 0.611 #&gt; #&gt; $tab.w #&gt; w.weights unit.names unit.numbers #&gt; 2 0.112 B 2 #&gt; 3 0.183 C 3 #&gt; 4 0.103 D 4 #&gt; 5 0.312 E 5 #&gt; 6 0.061 F 6 #&gt; 7 0.035 G 7 #&gt; 8 0.059 H 8 #&gt; 9 0.057 I 9 #&gt; 10 0.078 J 10 #&gt; #&gt; $tab.loss #&gt; Loss W Loss V #&gt; [1,] 9.761708e-12 9.831789 path.plot(synth.res = synth.out, dataprep.res = dataprep.out, Ylab = c(&quot;Y&quot;), Xlab = c(&quot;Year&quot;), Legend = c(&quot;State A&quot;,&quot;Synthetic State A&quot;), Legend.position = c(&quot;topleft&quot;) ) abline(v = 15, lty = 2) Gaps plot: gaps.plot(synth.res = synth.out, dataprep.res = dataprep.out, Ylab = c(&quot;Gap&quot;), Xlab = c(&quot;Year&quot;), Ylim = c(-30, 30), Main = &quot;&quot; ) abline(v = 15, lty = 2) Alternatively, gsynth provides options to estimate iterative fixed effects, and handle multiple treated units at tat time. Here, we use two=way fixed effects and bootstrapped standard errors gsynth.out &lt;- gsynth( Y ~ `T` + X1 + X2, data = df, index = c(&quot;state&quot;, &quot;year&quot;), force = &quot;two-way&quot;, CV = TRUE, r = c(0, 5), se = TRUE, inference = &quot;parametric&quot;, nboots = 1000, parallel = F # TRUE ) #&gt; Cross-validating ... #&gt; r = 0; sigma2 = 1.13533; IC = 0.95632; PC = 0.96713; MSPE = 1.65502 #&gt; r = 1; sigma2 = 0.96885; IC = 1.54420; PC = 4.30644; MSPE = 1.33375 #&gt; r = 2; sigma2 = 0.81855; IC = 2.08062; PC = 6.58556; MSPE = 1.27341* #&gt; r = 3; sigma2 = 0.71670; IC = 2.61125; PC = 8.35187; MSPE = 1.79319 #&gt; r = 4; sigma2 = 0.62823; IC = 3.10156; PC = 9.59221; MSPE = 2.02301 #&gt; r = 5; sigma2 = 0.55497; IC = 3.55814; PC = 10.48406; MSPE = 2.79596 #&gt; #&gt; r* = 2 #&gt; #&gt; Simulating errors ............. Bootstrapping ... #&gt; .......... plot(gsynth.out) plot(gsynth.out, type = &quot;counterfactual&quot;) plot(gsynth.out, type = &quot;counterfactual&quot;, raw = &quot;all&quot;) # shows estimations for the control cases 28.1.2 Example 2 by Leihua Ye library(Synth) data(&quot;basque&quot;) dim(basque) #774*17 #&gt; [1] 774 17 head(basque) #&gt; regionno regionname year gdpcap sec.agriculture sec.energy sec.industry #&gt; 1 1 Spain (Espana) 1955 2.354542 NA NA NA #&gt; 2 1 Spain (Espana) 1956 2.480149 NA NA NA #&gt; 3 1 Spain (Espana) 1957 2.603613 NA NA NA #&gt; 4 1 Spain (Espana) 1958 2.637104 NA NA NA #&gt; 5 1 Spain (Espana) 1959 2.669880 NA NA NA #&gt; 6 1 Spain (Espana) 1960 2.869966 NA NA NA #&gt; sec.construction sec.services.venta sec.services.nonventa school.illit #&gt; 1 NA NA NA NA #&gt; 2 NA NA NA NA #&gt; 3 NA NA NA NA #&gt; 4 NA NA NA NA #&gt; 5 NA NA NA NA #&gt; 6 NA NA NA NA #&gt; school.prim school.med school.high school.post.high popdens invest #&gt; 1 NA NA NA NA NA NA #&gt; 2 NA NA NA NA NA NA #&gt; 3 NA NA NA NA NA NA #&gt; 4 NA NA NA NA NA NA #&gt; 5 NA NA NA NA NA NA #&gt; 6 NA NA NA NA NA NA transform data to be used in synth() dataprep.out &lt;- dataprep( foo = basque, predictors = c( &quot;school.illit&quot;, &quot;school.prim&quot;, &quot;school.med&quot;, &quot;school.high&quot;, &quot;school.post.high&quot;, &quot;invest&quot; ), predictors.op = &quot;mean&quot;, # the operator time.predictors.prior = 1964:1969, #the entire time frame from the #beginning to the end special.predictors = list( list(&quot;gdpcap&quot;, 1960:1969, &quot;mean&quot;), list(&quot;sec.agriculture&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.energy&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.industry&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.construction&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.services.venta&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.services.nonventa&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;popdens&quot;, 1969, &quot;mean&quot;) ), dependent = &quot;gdpcap&quot;, # dv unit.variable = &quot;regionno&quot;, #identifying unit numbers unit.names.variable = &quot;regionname&quot;, #identifying unit names time.variable = &quot;year&quot;, #time-periods treatment.identifier = 17, #the treated case controls.identifier = c(2:16, 18), #the control cases; all others #except number 17 time.optimize.ssr = 1960:1969, #the time-period over which to optimize time.plot = 1955:1997 ) #the entire time period before/after the treatment where \\(X_1\\) = the control case before the treatment \\(X_0\\) = the control cases after the treatment \\(Z_1\\): the treatment case before the treatment \\(Z_0\\): the treatment case after the treatment synth.out = synth(data.prep.obj = dataprep.out, method = &quot;BFGS&quot;) #&gt; #&gt; X1, X0, Z1, Z0 all come directly from dataprep object. #&gt; #&gt; #&gt; **************** #&gt; searching for synthetic control unit #&gt; #&gt; #&gt; **************** #&gt; **************** #&gt; **************** #&gt; #&gt; MSPE (LOSS V): 0.008864606 #&gt; #&gt; solution.v: #&gt; 0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 #&gt; #&gt; solution.w: #&gt; 2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07 Calculate the difference between the real basque region and the synthetic control gaps = dataprep.out$Y1plot - (dataprep.out$Y0plot %*% synth.out$solution.w) gaps[1:3,1] #&gt; 1955 1956 1957 #&gt; 0.15023473 0.09168035 0.03716475 synth.tables = synth.tab(dataprep.res = dataprep.out, synth.res = synth.out) names(synth.tables) #&gt; [1] &quot;tab.pred&quot; &quot;tab.v&quot; &quot;tab.w&quot; &quot;tab.loss&quot; synth.tables$tab.pred[1:13,] #&gt; Treated Synthetic Sample Mean #&gt; school.illit 39.888 256.337 170.786 #&gt; school.prim 1031.742 2730.104 1127.186 #&gt; school.med 90.359 223.340 76.260 #&gt; school.high 25.728 63.437 24.235 #&gt; school.post.high 13.480 36.153 13.478 #&gt; invest 24.647 21.583 21.424 #&gt; special.gdpcap.1960.1969 5.285 5.271 3.581 #&gt; special.sec.agriculture.1961.1969 6.844 6.179 21.353 #&gt; special.sec.energy.1961.1969 4.106 2.760 5.310 #&gt; special.sec.industry.1961.1969 45.082 37.636 22.425 #&gt; special.sec.construction.1961.1969 6.150 6.952 7.276 #&gt; special.sec.services.venta.1961.1969 33.754 41.104 36.528 #&gt; special.sec.services.nonventa.1961.1969 4.072 5.371 7.111 Relative importance of each unit synth.tables$tab.w[8:14, ] #&gt; w.weights unit.names unit.numbers #&gt; 9 0.000 Castilla-La Mancha 9 #&gt; 10 0.851 Cataluna 10 #&gt; 11 0.000 Comunidad Valenciana 11 #&gt; 12 0.000 Extremadura 12 #&gt; 13 0.000 Galicia 13 #&gt; 14 0.149 Madrid (Comunidad De) 14 #&gt; 15 0.000 Murcia (Region de) 15 # plot the changes before and after the treatment path.plot( synth.res = synth.out, dataprep.res = dataprep.out, Ylab = &quot;real per-capita gdp (1986 USD, thousand)&quot;, Xlab = &quot;year&quot;, Ylim = c(0, 12), Legend = c(&quot;Basque country&quot;, &quot;synthetic Basque country&quot;), Legend.position = &quot;bottomright&quot; ) gaps.plot( synth.res = synth.out, dataprep.res = dataprep.out, Ylab = &quot;gap in real per - capita GDP (1986 USD, thousand)&quot;, Xlab = &quot;year&quot;, Ylim = c(-1.5, 1.5), Main = NA ) Doubly Robust Difference-in-Differences Example from DRDID package library(DRDID) data(nsw_long) # Form the Lalonde sample with CPS comparison group eval_lalonde_cps &lt;- subset(nsw_long, nsw_long$treated == 0 | nsw_long$sample == 2) Estimate Average Treatment Effect on Treated using Improved Locally Efficient Doubly Robust DID estimator out &lt;- drdid( yname = &quot;re&quot;, tname = &quot;year&quot;, idname = &quot;id&quot;, dname = &quot;experimental&quot;, xformla = ~ age + educ + black + married + nodegree + hisp + re74, data = eval_lalonde_cps, panel = TRUE ) summary(out) #&gt; Call: #&gt; drdid(yname = &quot;re&quot;, tname = &quot;year&quot;, idname = &quot;id&quot;, dname = &quot;experimental&quot;, #&gt; xformla = ~age + educ + black + married + nodegree + hisp + #&gt; re74, data = eval_lalonde_cps, panel = TRUE) #&gt; ------------------------------------------------------------------ #&gt; Further improved locally efficient DR DID estimator for the ATT: #&gt; #&gt; ATT Std. Error t value Pr(&gt;|t|) [95% Conf. Interval] #&gt; -901.2703 393.6247 -2.2897 0.022 -1672.7747 -129.766 #&gt; ------------------------------------------------------------------ #&gt; Estimator based on panel data. #&gt; Outcome regression est. method: weighted least squares. #&gt; Propensity score est. method: inverse prob. tilting. #&gt; Analytical standard error. #&gt; ------------------------------------------------------------------ #&gt; See Sant&#39;Anna and Zhao (2020) for details. 28.1.3 Example 3 by Synth package’s authors library(Synth) data(&quot;basque&quot;) synth() requires \\(X_1\\) vector of treatment predictors \\(X_0\\) matrix of same variables for control group \\(Z_1\\) vector of outcome variable for treatment group \\(Z_0\\) matrix of outcome variable for control group use dataprep() to prepare data in the format that can be used throughout the Synth package dataprep.out &lt;- dataprep( foo = basque, predictors = c( &quot;school.illit&quot;, &quot;school.prim&quot;, &quot;school.med&quot;, &quot;school.high&quot;, &quot;school.post.high&quot;, &quot;invest&quot; ), predictors.op = &quot;mean&quot;, time.predictors.prior = 1964:1969, special.predictors = list( list(&quot;gdpcap&quot;, 1960:1969 , &quot;mean&quot;), list(&quot;sec.agriculture&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.energy&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.industry&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.construction&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.services.venta&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;sec.services.nonventa&quot;, seq(1961, 1969, 2), &quot;mean&quot;), list(&quot;popdens&quot;, 1969, &quot;mean&quot;) ), dependent = &quot;gdpcap&quot;, unit.variable = &quot;regionno&quot;, unit.names.variable = &quot;regionname&quot;, time.variable = &quot;year&quot;, treatment.identifier = 17, controls.identifier = c(2:16, 18), time.optimize.ssr = 1960:1969, time.plot = 1955:1997 ) find optimal weights that identifies the synthetic control for the treatment group synth.out &lt;- synth(data.prep.obj = dataprep.out, method = &quot;BFGS&quot;) #&gt; #&gt; X1, X0, Z1, Z0 all come directly from dataprep object. #&gt; #&gt; #&gt; **************** #&gt; searching for synthetic control unit #&gt; #&gt; #&gt; **************** #&gt; **************** #&gt; **************** #&gt; #&gt; MSPE (LOSS V): 0.008864606 #&gt; #&gt; solution.v: #&gt; 0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 #&gt; #&gt; solution.w: #&gt; 2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07 gaps &lt;- dataprep.out$Y1plot - (dataprep.out$Y0plot %*% synth.out$solution.w) gaps[1:3, 1] #&gt; 1955 1956 1957 #&gt; 0.15023473 0.09168035 0.03716475 synth.tables &lt;- synth.tab(dataprep.res = dataprep.out, synth.res = synth.out) names(synth.tables) # you can pick tables to see #&gt; [1] &quot;tab.pred&quot; &quot;tab.v&quot; &quot;tab.w&quot; &quot;tab.loss&quot; path.plot( synth.res = synth.out, dataprep.res = dataprep.out, Ylab = &quot;real per-capita GDP (1986 USD, thousand)&quot;, Xlab = &quot;year&quot;, Ylim = c(0, 12), Legend = c(&quot;Basque country&quot;, &quot;synthetic Basque country&quot;), Legend.position = &quot;bottomright&quot; ) gaps.plot( synth.res = synth.out, dataprep.res = dataprep.out, Ylab = &quot;gap in real per-capita GDP (1986 USD, thousand)&quot;, Xlab = &quot;year&quot;, Ylim = c(-1.5, 1.5), Main = NA ) You could also run placebo tests 28.1.4 Example 4 by Michael Robbins and Steven Davenport who are authors of MicroSynth with the following improvements: Standardization use.survey = TRUE and permutation ( perm = 250 and jack = TRUE ) for placebo tests Omnibus statistic (set to omnibus.var ) for multiple outcome variables incorporate multiple follow-up periods end.post Notes: Both predictors and outcome will be used to match units before intervention Outcome variable has to be time-variant Predictors are time-invariant # right now the package is not availabe for R version 4.2 library(microsynth) data(&quot;seattledmi&quot;) cov.var &lt;- c( &quot;TotalPop&quot;, &quot;BLACK&quot;, &quot;HISPANIC&quot;, &quot;Males_1521&quot;, &quot;HOUSEHOLDS&quot;, &quot;FAMILYHOUS&quot;, &quot;FEMALE_HOU&quot;, &quot;RENTER_HOU&quot;, &quot;VACANT_HOU&quot; ) match.out &lt;- c(&quot;i_felony&quot;, &quot;i_misdemea&quot;, &quot;i_drugs&quot;, &quot;any_crime&quot;) sea1 &lt;- microsynth( seattledmi, idvar = &quot;ID&quot;, timevar = &quot;time&quot;, intvar = &quot;Intervention&quot;, start.pre = 1, end.pre = 12, end.post = 16, match.out = match.out, # outcome variable will be matched on exactly match.covar = cov.var, # specify covariates will be matched on exactly result.var = match.out, # used to report results omnibus.var = match.out, # feature in the omnibus p-value test = &quot;lower&quot;, n.cores = min(parallel::detectCores(), 2) ) sea1 summary(sea1) plot_microsynth(sea1) sea2 &lt;- microsynth( seattledmi, idvar = &quot;ID&quot;, timevar = &quot;time&quot;, intvar = &quot;Intervention&quot;, start.pre = 1, end.pre = 12, end.post = c(14, 16), match.out = match.out, match.covar = cov.var, result.var = match.out, omnibus.var = match.out, test = &quot;lower&quot;, perm = 250, jack = TRUE, n.cores = min(parallel::detectCores(), 2) ) "],["augmented-synthetic-control-method.html", "28.2 Augmented Synthetic Control Method", " 28.2 Augmented Synthetic Control Method package: augsynth (Ben-Michael, Feller, and Rothstein 2021) References "],["synthetic-control-with-staggered-adoption.html", "28.3 Synthetic Control with Staggered Adoption", " 28.3 Synthetic Control with Staggered Adoption references: https://ebenmichael.github.io/assets/research/jamboree.pdf (Ben-Michael, Feller, and Rothstein 2022) package: augsynth References "],["bayesian-synthetic-control.html", "28.4 Bayesian Synthetic Control", " 28.4 Bayesian Synthetic Control S. Kim, Lee, and Gupta (2020) Pang, Liu, and Xu (2022) References "],["generalized-synthetic-control.html", "28.5 Generalized Synthetic Control", " 28.5 Generalized Synthetic Control reference: (Xu 2017) Bootstrap procedure here is biased (K. T. Li and Sonnier 2023). Hence, we need to follow K. T. Li and Sonnier (2023) in terms of SEs estimation. References "],["other-advances.html", "28.6 Other Advances", " 28.6 Other Advances L. Sun, Ben-Michael, and Feller (2023) Using Multiple Outcomes to Improve SCM Common Weights Across Outcomes: This paper proposes using a single set of synthetic control weights across multiple outcomes, rather than estimating separate weights for each outcome. Reduced Bias with Low-Rank Factor Model: By balancing a vector or an index of outcomes, this approach yields lower bias bounds under a low-rank factor model, with further improvements as the number of outcomes increases. Evidence: re-analysis of the Flint water crisis’s impact on educational outcome. References "],["event-studies.html", "Chapter 29 Event Studies", " Chapter 29 Event Studies The earliest paper that used event study was (Dolley 1933) (Campbell et al. 1998) introduced this method, which based on the efficient markets theory by (Fama 1970) Review: (McWilliams and Siegel 1997): in management (A. Sorescu, Warren, and Ertekin 2017): in marketing Previous marketing studies: Firm-initiated activities (Horsky and Swyngedouw 1987): name change (Chaney, Devinney, and Winer 1991) new product announcements (Agrawal and Kamakura 1995): celebrity endorsement (Lane and Jacobson 1995): brand extensions (Houston and Johnson 2000): joint venture (Geyskens, Gielens, and Dekimpe 2002): Internet channel (for newspapers) (Cornwell, Pruitt, and Clark 2005): sponsorship announcements (Elberse 2007): casting announcements (A. B. Sorescu, Chandy, and Prabhu 2007): M&amp;A (Sood and Tellis 2009): innovation payoff (Wiles and Danielova 2009): product placements in movies (Joshi and Hanssens 2009): movie releases (Wiles et al. 2010): Regulatory Reports of Deceptive Advertising (Boyd, Chandy, and Cunha Jr 2010): new CMO appointments (Karniouchina, Uslay, and Erenburg 2011): product placement (Wiles, Morgan, and Rego 2012): Brand Acquisition and Disposal (Kalaignanam and Bahadir 2013): corporate brand name change (Raassens, Wuyts, and Geyskens 2012): new product development outsourcing (Mazodier and Rezaee 2013): sports announcements (Borah and Tellis 2014): make, buy or ally for innovations (Homburg, Vollmayr, and Hahn 2014): channel expansions (Fang, Lee, and Yang 2015): Co-development agreements (Wu et al. 2015): horizontal collaboration in new product development (Fama et al. 1969): stock split Non-firm-initiated activities (A. B. Sorescu, Chandy, and Prabhu 2003): FDA approvals (Pandey, Shanahan, and Hansen 2005): diversity elite list (Balasubramanian, Mathur, and Thakur 2005): high-quality achievements (Tellis and Johnson 2007): quality reviews by Walter Mossberg (Fornell et al. 2006): customer satisfaction (Gielens et al. 2008): Walmart’s entry into the UK market (Boyd and Spekman 2008): indirect ties (R. S. Rao, Chandy, and Prabhu 2008): FDA approvals (Ittner, Larcker, and Taylor 2009): customer satisfaction (Tipton, Bharadwaj, and Robertson 2009): Deceptive advertising (Y. Chen, Ganesan, and Liu 2009): product recalls (Jacobson and Mizik 2009): satisfaction score release (Karniouchina, Moore, and Cooney 2009): Mad money with Jim Cramer (Wiles et al. 2010): deceptive advertising (Y. Chen, Liu, and Zhang 2012): third-party movie reviews (Xiong and Bharadwaj 2013): positive and negative news (Gao et al. 2015): product recall (Malhotra and Kubowicz Malhotra 2011): data breach (Bhagat, Bizjak, and Coles 1998): litigation Potential avenues: Ad campaigns Market entry product failure/recalls Patents Pros: Better than accounting based measures (e.g., profits) because managers can manipulate profits (Benston 1985) Easy to do Fun fact: (Dubow and Monteiro 2006) came up with a way to gauge how ‘clean’ a market is. They based their measure on how much prices seemed to move in a way that suggested insider knowledge, before the release of important regulatory announcements that could affect the stock prices. Such price shifts might suggest that insider trading was occurring. Essentially, they were watching for any unusual price changes before the day of the announcement. Events can be Internal (e.g., stock repurchase) External (e.g., macroeconomic variables) Assumptions: Efficient market theory Shareholders are the most important group among stakeholders The event sharply affects share price Expected return is calculated appropriately Steps: Event Identification: (e.g., dividends, M&amp;A, stock buyback, laws or regulation, privatization vs. nationalization, celebrity endorsements, name changes, or brand extensions etc. To see the list of events in US and international, see WRDS S&amp;P Capital IQ Key Developments). Events must affect either cash flows or on the discount rate of firms (A. Sorescu, Warren, and Ertekin 2017, 191) Estimation window: Normal return expected return (\\(T_0 \\to T_1\\)) (sometimes include days before to capture leakages). Recommendation by (Johnston 2007) is to use 250 days before the event (and 45-day between the estimation window and the event window). (Wiles, Morgan, and Rego 2012) used an 90-trading-day estimation window ending 6 days before the event (this is consistent with the finance literature). (Gielens et al. 2008) 260 to 10 days before or 300 to 46 days before (Tirunillai and Tellis 2012) estimation window of 255 days and ends 46 days before the event. Similarly, (McWilliams and Siegel 1997) and (Fornell et al. 2006) 255 days ending 46 days before the event date (A. Sorescu, Warren, and Ertekin 2017, 194) suggest 100 days before the event date Leakage: try to cover as broad news sources as possible (LexisNexis, Factiva, and RavenPack). Event window: contain the event date (\\(T_1 \\to T_2\\)) (have to argue for the event window and can’t do it empirically) One day: (Balasubramanian, Mathur, and Thakur 2005; Boyd, Chandy, and Cunha Jr 2010; Fornell et al. 2006) Two days: (Raassens, Wuyts, and Geyskens 2012; Sood and Tellis 2009) Up to 10 days: (Cornwell, Pruitt, and Clark 2005; Kalaignanam and Bahadir 2013; A. B. Sorescu, Chandy, and Prabhu 2007) Post Event window: \\(T_2 \\to T_3\\) Normal vs. Abnormal returns \\[ \\epsilon_{it}^* = \\frac{P_{it} - E(P_{it})}{P_{it-1}} = R_{it} - E(R_{it}|X_t) \\] where \\(\\epsilon_{it}^*\\) = abnormal return \\(R_{it}\\) = realized (actual) return \\(P\\) = dividend-adjusted price of the stock \\(E(R_{it}|X_t)\\) normal expected return There are several model to calculate the expected return A. Statistical Models: assumes jointly multivariate normal and iid over time (need distributional assumptions for valid finite-sample estimation) rather robust (hence, recommended) Constant Mean Return Model Market Model Adjusted Market Return Model Factor Model B. Economic Model (strong assumption regarding investor behavior) Capital Asset Pricing Model (CAPM) Arbitrage Pricing Theory (APT) References "],["other-issues.html", "29.1 Other Issues", " 29.1 Other Issues 29.1.1 Event Studies in marketing (Skiera, Bayer, and Schöler 2017) What should be the dependent variable in marketing-related event studies? Based on valuation theory, Shareholder value = the value of the operating business + non-operating asset - debt (Schulze, Skiera, and Wiesel 2012) Many marketing events only affect the operating business value, but not non-operating assets and debt Ignoring the differences in firm-specific leverage effects has dual effects: inflates the impact of observation pertaining to firms with large debt deflates those pertaining to firms with large non-operating asset. It’s recommended that marketing papers should report both \\(CAR^{OB}\\) and \\(CAR^{SHV}\\) and argue for whichever one more appropriate. Up until this paper, only two previous event studies control for financial structure: (Gielens et al. 2008) (Chaney, Devinney, and Winer 1991) Definitions: Cumulative abnormal percentage return on shareholder value (\\(CAR^{SHV}\\)) Shareholder value refers to a firm’s market capitalization = share price x # of shares. Cumulative abnormal percentage return on the value of the operating business (\\(CAR^{OB}\\)) \\(CAR^{OB} = CAR^{SHV}/\\text{leverage effect}_{before}\\) Leverage effect = Operating business value / Shareholder value (LE describes how a 1% change in operating business translates into a percentage change in shareholder value). Value of operating business = shareholder value - non-operating assets + debt Leverage effect \\(\\neq\\) leverage ratio, where leverage ratio is debt / firm size debt = long-term + short-term debt; long-term debt firm size = book value of equity; market cap; total assets; debt + equity Operating assets are those used by firm in their core business operations (e..g, property, plant, equipment, natural resources, intangible asset) Non–operating assets (redundant assets), do not play a role in a firm’s operations, but still generate some form of return (e.g., excess cash , marketable securities - commercial papers, market instruments) Marketing events usually influence the value of a firm’s operating assets (more specifically intangible assets). Then, changes in the value of the operating business can impact shareholder value. Three rare instances where marketing events can affect non-operating assets and debt (G. C. Hall, Hutchinson, and Michaelas 2004): excess pre-orderings can influence short-term debt (Berger, Ofek, and Yermack 1997) Firing CMO increase debt as the manager’s tenure is negatively associated with the firm’s debt (Bhaduri 2002) production of unique products. A marketing-related event can either influence value components of a firm’s value (= firm’s operating business, non-operating assets and its debt) only the operating business. Replication of the leverage effect \\[ \\begin{aligned} \\text{leverage effect} &amp;= \\frac{\\text{operating business}}{\\text{shareholder value}} \\\\ &amp;= \\frac{\\text{(shareholder value - non-operating assets + debt)}}{\\text{shareholder value}} \\\\ &amp;= \\frac{prcc_f \\times csho - ivst + dd1 + dltt + pstk}{prcc_f \\times csho} \\end{aligned} \\] Compustat Data Item Label Variable prcc_f Share price csho Common shares outstanding ivst short-term investments (Non-operating assets) dd1 long-term debt due in one year dltt long-term debt pstk preferred stock Since WRDS no longer maintains the S&amp;P 500 list as of the time of this writing, I can’t replicate the list used in (Skiera, Bayer, and Schöler 2017) paper. library(tidyverse) df_leverage_effect &lt;- read.csv(&quot;data/leverage_effect.csv.gz&quot;) %&gt;% # get active firms only filter(costat == &quot;A&quot;) %&gt;% # drop missing values drop_na() %&gt;% # create the leverage effect variable mutate(le = (prcc_f * csho - ivst + dd1 + dltt + pstk)/ (prcc_f * csho)) %&gt;% # get shareholder value mutate(shv = prcc_f * csho) %&gt;% # remove Infinity value for leverage effect (i.e., shareholder value = 0) filter_all(all_vars(!is.infinite(.))) %&gt;% # positive values only filter_all(all_vars(. &gt; 0)) %&gt;% # get the within coefficient of variation group_by(gvkey) %&gt;% mutate(within_var_mean_le = mean(le), within_var_sd_le = sd(le)) %&gt;% ungroup() # get the mean and standard deviation mean(df_leverage_effect$le) #&gt; [1] 150.1087 max(df_leverage_effect$le) #&gt; [1] 183629.6 hist(df_leverage_effect$le) # coefficient of variation sd(df_leverage_effect$le) / mean(df_leverage_effect$le) * 100 #&gt; [1] 2749.084 # Within-firm variation (similar to fig 3a) df_leverage_effect %&gt;% group_by(gvkey) %&gt;% slice(1) %&gt;% ungroup() %&gt;% dplyr::select(within_var_mean_le, within_var_sd_le) %&gt;% dplyr::mutate(cv = within_var_sd_le/ within_var_mean_le) %&gt;% dplyr::select(cv) %&gt;% pull() %&gt;% hist() 29.1.2 Economic significance Total wealth gain (loss) from the event \\[ \\Delta W_t = CAR_t \\times MKTVAL_0 \\] where \\(\\Delta W_t\\) = gain (loss) \\(CAR_t\\) = cumulative residuals to date \\(t\\) \\(MKTVAL_0\\) market value of the firm before the event window 29.1.3 Statistical Power increases with more firms less days in the event window (avoiding potential contamination from confounds) References "],["testing.html", "29.2 Testing", " 29.2 Testing 29.2.1 Parametric Test (S. J. Brown and Warner 1985) provide evidence that even in the presence of non-normality, the parametric tests still perform well. Since the proportion of positive and negative abnormal returns tends to be equal in the sample (of at least 5 securities). The excess returns will coverage to normality as the sample size increases. Hence, parametric test is advocated than non-parametric one. Low power to detect significance (Kothari and Warner 1997) Power = f(sample, size, the actual size of abnormal returns, the variance of abnormal returns across firms) 29.2.1.1 T-test Applying CLT \\[ \\begin{aligned} t_{CAR} &amp;= \\frac{\\bar{CAR_{it}}}{\\sigma (CAR_{it})/\\sqrt{n}} \\\\ t_{BHAR} &amp;= \\frac{\\bar{BHAR_{it}}}{\\sigma (BHAR_{it})/\\sqrt{n}} \\end{aligned} \\] Assume Abnormal returns are normally distributed Var(abnormal returns) are equal across firms No cross-correlation in abnormal returns. Hence, it will be misspecified if you suspected Heteroskedasticity Cross-sectional dependence Technically, abnormal returns could follow non-normal distribution (but because of the design of abnormal returns calculation, it typically forces the distribution to be normal) To address these concerns, Patell Standardized Residual (PSR) can sometimes help. 29.2.1.2 Patell Standardized Residual (PSR) (Patell 1976) Since market model uses observations outside the event window, abnormal returns contain prediction errors on top of the true residuals , and should be standardized: \\[ AR_{it} = \\frac{\\hat{u}_{it}}{s_i \\sqrt{C_{it}}} \\] where \\(\\hat{u}_{it}\\) = estimated residual \\(s_i\\) = standard deviation estimate of residuals (from the estimation period) \\(C_{it}\\) = a correction to account for the prediction’s increased variation outside of the estimation period (Strong 1992) \\[ C_{it} = 1 + \\frac{1}{T} + \\frac{(R_{mt} - \\bar{R}_m)^2}{\\sum_t (R_{mt} - \\bar{R}_m)^2} \\] where \\(T\\) = number of observations (from estimation period) \\(R_{mt}\\) = average rate of return of all stocks trading the the stock market at time \\(t\\) \\(\\bar{R}_m = \\frac{1}{T} \\sum_{t=1}^T R_{mt}\\) 29.2.2 Non-parametric Test No assumptions about return distribution Sign Test (assumes symmetry in returns) binom.test() Wilcoxon Signed-Rank Test (allows for non-symmetry in returns) Use wilcox.test(sample) Gen Sign Test Corrado Rank Test References "],["sample.html", "29.3 Sample", " 29.3 Sample Sample can be relative small (Wiles, Morgan, and Rego 2012) 572 acquisition announcements, 308 disposal announcements Can range from 71 (Markovitch and Golder 2008) to 3552 (Borah and Tellis 2014) 29.3.1 Confounders Avoid confounding events: earnings announcements, key executive changes, unexpected stock buybacks, changes in dividends within the two-trading day window surrounding the event, mergers and acquisitions, spin-offers, stock splits, management changes, joint ventures, unexpected dividend, IPO, debt defaults, dividend cancellations (McWilliams and Siegel 1997) According to (Fornell et al. 2006), need to control: one-day event period = day when Wall Street Journal publish ACSI announcement. 5 days before and after event to rule out other news (PR Newswires, Dow Jones, Business Wires) M&amp;A, Spin-offs, stock splits CEO or CFO changes, Layoffs, restructurings, earnings announcements, lawsuits Capital IQ - Key Developments: covers almost all important events so you don’t have to search on news. (A. Sorescu, Warren, and Ertekin 2017) examine confounding events in the short-term windows: From RavenPack, 3982 US publicly traded firms, with all the press releases (2000-2013) 3-day window around event dates The difference between a sample with full observations and a sample without confounded events is negligible (non-significant). Conclusion: excluding confounded observations may be unnecessary for short-term event studies. Biases can stem from researchers pick and choose events to exclude As time progresses, more and more events you need to exclude which can be infeasible. To further illustrate this point, let’s do a quick simulation exercise In this example, we will explore three types of events: Focal events Correlated events (i.e., events correlated with the focal events; the presence of correlated events can follow the presence of the focal event) Uncorrelated events (i.e., events with dates that might randomly coincide with the focal events, but are not correlated with them). We have the ability to control the strength of correlation between focal and correlated events in this study, as well as the number of unrelated events we wish to examine. Let’s examine the implications of including and excluding correlated and uncorrelated events on the estimates of our focal events. # Load required libraries library(dplyr) library(ggplot2) library(tidyr) library(tidyverse) # Parameters n &lt;- 100000 # Number of observations n_focal &lt;- round(n * 0.2) # Number of focal events overlap_correlated &lt;- 0.5 # Overlapping percentage between focal and correlated events # Function to compute mean and confidence interval mean_ci &lt;- function(x) { m &lt;- mean(x) ci &lt;- qt(0.975, length(x)-1) * sd(x) / sqrt(length(x)) # 95% confidence interval list(mean = m, lower = m - ci, upper = m + ci) } # Simulate data set.seed(42) data &lt;- tibble( date = seq.Date(from = as.Date(&quot;2010-01-01&quot;), by = &quot;day&quot;, length.out = n), # Date sequence focal = rep(0, n), correlated = rep(0, n), ab_ret = rnorm(n) ) # Define focal events focal_idx &lt;- sample(1:n, n_focal) data$focal[focal_idx] &lt;- 1 true_effect &lt;- 0.25 # Adjust the ab_ret for the focal events to have a mean of true_effect data$ab_ret[focal_idx] &lt;- data$ab_ret[focal_idx] - mean(data$ab_ret[focal_idx]) + true_effect # Determine the number of correlated events that overlap with focal and those that don&#39;t n_correlated_overlap &lt;- round(length(focal_idx) * overlap_correlated) n_correlated_non_overlap &lt;- n_correlated_overlap # Sample the overlapping correlated events from the focal indices correlated_idx &lt;- sample(focal_idx, size = n_correlated_overlap) # Get the remaining indices that are not part of focal remaining_idx &lt;- setdiff(1:n, focal_idx) # Check to ensure that we&#39;re not attempting to sample more than the available remaining indices if (length(remaining_idx) &lt; n_correlated_non_overlap) { stop(&quot;Not enough remaining indices for non-overlapping correlated events&quot;) } # Sample the non-overlapping correlated events from the remaining indices correlated_non_focal_idx &lt;- sample(remaining_idx, size = n_correlated_non_overlap) # Combine the two to get all correlated indices all_correlated_idx &lt;- c(correlated_idx, correlated_non_focal_idx) # Set the correlated events in the data data$correlated[all_correlated_idx] &lt;- 1 # Inflate the effect for correlated events to have a mean of correlated_non_focal_idx &lt;- setdiff(all_correlated_idx, focal_idx) # Fixing the selection of non-focal correlated events data$ab_ret[correlated_non_focal_idx] &lt;- data$ab_ret[correlated_non_focal_idx] - mean(data$ab_ret[correlated_non_focal_idx]) + 1 # Define the numbers of uncorrelated events for each scenario num_uncorrelated &lt;- c(5, 10, 20, 30, 40) # Define uncorrelated events for (num in num_uncorrelated) { for (i in 1:num) { data[paste0(&quot;uncorrelated_&quot;, i)] &lt;- 0 uncorrelated_idx &lt;- sample(1:n, round(n * 0.1)) data[uncorrelated_idx, paste0(&quot;uncorrelated_&quot;, i)] &lt;- 1 } } # Define uncorrelated columns and scenarios unc_cols &lt;- paste0(&quot;uncorrelated_&quot;, 1:num_uncorrelated) results &lt;- tibble( Scenario = c(&quot;Include Correlated&quot;, &quot;Correlated Effects&quot;, &quot;Exclude Correlated&quot;, &quot;Exclude Correlated and All Uncorrelated&quot;), MeanEffect = c( mean_ci(data$ab_ret[data$focal == 1])$mean, mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$mean, mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0])$mean, mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0 &amp; rowSums(data[, paste0(&quot;uncorrelated_&quot;, 1:num_uncorrelated)]) == 0])$mean ), LowerCI = c( mean_ci(data$ab_ret[data$focal == 1])$lower, mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$lower, mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0])$lower, mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0 &amp; rowSums(data[, paste0(&quot;uncorrelated_&quot;, 1:num_uncorrelated)]) == 0])$lower ), UpperCI = c( mean_ci(data$ab_ret[data$focal == 1])$upper, mean_ci(data$ab_ret[data$focal == 0 | data$correlated == 1])$upper, mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0])$upper, mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0 &amp; rowSums(data[, paste0(&quot;uncorrelated_&quot;, 1:num_uncorrelated)]) == 0])$upper ) ) # Add the scenarios for excluding 5, 10, 20, and 50 uncorrelated for (num in num_uncorrelated) { unc_cols &lt;- paste0(&quot;uncorrelated_&quot;, 1:num) results &lt;- results %&gt;% add_row( Scenario = paste(&quot;Exclude&quot;, num, &quot;Uncorrelated&quot;), MeanEffect = mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0 &amp; rowSums(data[, unc_cols]) == 0])$mean, LowerCI = mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0 &amp; rowSums(data[, unc_cols]) == 0])$lower, UpperCI = mean_ci(data$ab_ret[data$focal == 1 &amp; data$correlated == 0 &amp; rowSums(data[, unc_cols]) == 0])$upper ) } ggplot(results, aes( x = factor(Scenario, levels = Scenario), y = MeanEffect, ymin = LowerCI, ymax = UpperCI )) + geom_pointrange() + coord_flip() + ylab(&quot;Mean Effect&quot;) + xlab(&quot;Scenario&quot;) + ggtitle(&quot;Mean Effect of Focal Events under Different Scenarios&quot;) + geom_hline(yintercept = true_effect, linetype = &quot;dashed&quot;, color = &quot;red&quot;) As depicted in the plot, the inclusion of correlated events demonstrates minimal impact on the estimation of our focal events. Conversely, excluding these correlated events can diminish our statistical power. This is true in cases of pronounced correlation. However, the consequences of excluding unrelated events are notably more significant. It becomes evident that by omitting around 40 unrelated events from our study, we lose the ability to accurately identify the true effects of the focal events. In reality and within research, we often rely on the Key Developments database, excluding over 150 events, a practice that can substantially impair our capacity to ascertain the authentic impact of the focal events. This little experiment really drives home the point – you better have a darn good reason to exclude an event from your study (make it super convincing)! References "],["biases.html", "29.4 Biases", " 29.4 Biases Different closing time obscure estimation of the abnormal returns, check (Campbell et al. 1998) Upward bias in aggregating CAR + transaction prices (bid and ask) Cross-sectional dependence in the returns bias the standard deviation estimates downward, which inflates the test statistics when events share common dates (MacKinlay 1997). Hence, (Jaffe 1974) Calendar-time Portfolio Abnormal Returns (CTARs) should be used to correct for this bias. (Wiles, Morgan, and Rego 2012): For events confined to relatively few industries, cross-sectional dependence in the returns can bias the SD estimate downward, inflating the associated test statistics” (p. 47). To control for potential cross-sectional correlation in the abnormal returns, you can use time-series standard deviation test statistic (S. J. Brown and Warner 1980) Sample selection bias (self-selection of firms into the event treatment) similar to omitted variable bias where the omitted variable is the private info that leads a firm to take the action. See Endogenous Sample Selection for more methods to correct this bias. Use Heckman model (Acharya 1993) But hard to find an instrument that meets the exclusion requirements (and strong, because weak instruments can lead to multicollinearity in the second equation) Can estimate the private information unknown to investors (which is Mills ratio \\(\\lambda\\) itself). Testing \\(\\lambda\\) significance is to see whether private info can explain outcomes (e.g., magnitude of the CARs to the announcement). Examples: (Y. Chen, Ganesan, and Liu 2009) (Wiles, Morgan, and Rego 2012) (Fang, Lee, and Yang 2015) Counterfactual observations Propensity score matching: Finance: Doan and Iskandar-Datta (2021) (Masulis and Nahata 2011) Marketing: (Warren and Sorescu 2017) (Borah and Tellis 2014) (Cao and Sorescu 2013) Switching regression: comparison between 2 specific outcomes (also account for selection on unobservables - using instruments) (Cao and Sorescu 2013) References "],["long-run-event-studies.html", "29.5 Long-run event studies", " 29.5 Long-run event studies Usually make an assumption that the distribution of the abnormal returns to these events has a mean of 0 (A. Sorescu, Warren, and Ertekin 2017, 192). And (A. Sorescu, Warren, and Ertekin 2017) provide evidence that for all events they examine the results from samples with and without confounding events do not differ. Long-horizon event studies face challenges due to systematic errors over time and sensitivity to model choice. Two main approaches are used to measure long-term abnormal stock returns Buy and Hold Abnormal Returns (BHAR) Long-term Cumulative Abnormal Returns (LCARs) Calendar-time Portfolio Abnormal Returns (CTARs) (Jensen’s Alpha): manages cross-sectional dependence better and is less sensitive to (asset pricing) model misspecification Two types: Unexpected changes in firm specific variables (typically not announced, may not be immediately visible to all investors, impact on firm value is not straightforward): customer satisfaction scores effect on firm value (Jacobson and Mizik 2009) or unexpected changes in marketing expenditures (M. Kim and McAlister 2011) to determine mispricing. Complex consequences (investors take time to learn and incorporate info): acquisition depends on integration (A. B. Sorescu, Chandy, and Prabhu 2007) 12 - 60 months event window: (Loughran and Ritter 1995) (Brav and Gompers 1997) Example: (Dutta et al. 2018) library(crseEventStudy) # example by the package&#39;s author data(demo_returns) SAR &lt;- sar(event = demo_returns$EON, control = demo_returns$RWE, logret = FALSE) mean(SAR) #&gt; [1] 0.006870196 29.5.1 Buy and Hold Abnormal Returns (BHAR) Classic references: (Loughran and Ritter 1995) (Barber and Lyon 1997) (Lyon, Barber, and Tsai 1999) Use a portfolio of stocks that are close matches of the current firm over the same period as benchmark, and see the difference between the firm return and that of the portfolio. More technical note is that it measures returns from buying stocks in event-experiencing firms and shorting stocks in similar non-event firms within the same time. Because of high cross-sectional correlations, BHARs’ t-stat can be inflated, but its rank order is not affected (Markovitch and Golder 2008; A. B. Sorescu, Chandy, and Prabhu 2007) To construct the portfolio, use similar size book-to-market momentum Matching Procedure (Barber and Lyon 1997): Each year from July to June, all common stocks in the CRSP database are categorized into ten groups (deciles) based on their market capitalization from the previous June. Within these deciles, firms are further sorted into five groups (quintiles) based on their book-to-market ratios as of December of the previous year or earlier, considering possible delays in financial statement reporting. Benchmark portfolios are designed to exclude firms with specific events but include all firms that can be classified into the characteristic-based portfolios. Similarly, Wiles et al. (2010) uses the following matching procedure: All firms in the same two-digit SIC code with market values of 50% to 150% of the focal firms are selected From this list, the 10 firms with the most comparable book-to-market ratios are chosen to serve as the matched portfolio (the matched portfolio can have less than 10 firms). Calculations: \\[ AR_{it} = R_{it} - E(R_{it}|X_t) \\] Cumulative Abnormal Return (CAR): \\[ CAR_{it} = \\sum_{t=1}^T (R_{it} - E(R_{it})) \\] Buy-and-Hold Abnormal Return (BHAR) \\[ BHAR_{t = 1}^T = \\Pi_{t=1}^T(1 + R_{it}) - \\Pi_{t = 1}^T (1 + E(R_{it})) \\] where as CAR is the arithmetic sum, BHAR is the geometric sum. In short-term event studies, differences between CAR and BHAR are often minimal. However, in long-term studies, this difference could significantly skew results. (Barber and Lyon 1997) shows that while BHAR is usually slightly lower than annual CAR, but it dramatically surpasses CAR when annual BHAR exceeds 28%. To calculate the long-run return (\\(\\Pi_{t=1}^T (1 + E(R_{it}))\\)) of the benchmark portfolio, we can: With annual rebalance: In each period, each portfolio is re-balanced and then compound mean stock returns in a portfolio over a given period: \\[ \\Pi_{t = 1}^T (1 + E(R_{it})) = \\Pi_{t}^T (1 + \\sum_{i = s}^{n_t}w_{it} R_{it}) \\] where \\(n_t\\) is the number of firms in period \\(t\\), and \\(w_{it}\\) is (1) \\(1/n_t\\) or (2) value-weight of firm \\(i\\) in period \\(t\\). To avoid favoring recent events, in cross-sectional event studies, researchers usually treat all events equally when studying their impact on the stock market over time. This approach helps identify any abnormal changes in stock prices, especially when dealing with a series of unplanned events. Potential problems: Solution first: Form benchmark portfolios that will never change constituent firms (Mitchell and Stafford 2000), because of these problems: Newly public companies often perform worse than a balanced market index (Ritter 1991), and this, over time, might distort long-term return expectations due to the inclusion of these new companies (a phenomenon called “new listing bias” identified by Barber and Lyon (1997)). Regularly rebalancing an equal-weight portfolio can lead to overestimated long-term returns and potentially skew buy-and-hold abnormal returns (BHARs) negatively due to constant selling of winning stocks and buying of underperformers (i.e., “rebalancing bias” (Barber and Lyon 1997)). Value-weight portfolios, which favor larger market cap stocks, can be viewed as an active investment strategy that keeps buying winning stocks and selling underperformers. Over time, this approach tends to positively distort BHARs. Without annual rebalance: Compounding the returns of the securities comprising the portfolio, followed by calculating the average across all securities \\[ \\Pi_{t = s}^{T} (1 + E(R_{it})) = \\sum_{i=s}^{n_t} (w_{is} \\Pi_{t=1}^T (1 + R_{it})) \\] where \\(t\\) is the investment period, \\(R_{it}\\) is the return on security \\(i\\), \\(n_i\\) is the number of securities, \\(w_{it}\\) is either \\(1/n_s\\) or value-weight factor of security \\(i\\) at initial period \\(s\\). This portfolio’s profits come from a simple investment where all the included stocks are given equal importance, or weighted according to their market value, as they were in a specific past period (period s). This means that it doesn’t consider any stocks that were listed after this period, nor does it adjust the portfolio each month. However, one problem with this method is that the value assigned to each stock, based on its market size, needs to be corrected. This is to make sure that recent stocks don’t end up having too much influence. Fortunately, on WRDS, it will give you all types of BHAR (2x2) (equal-weighted vs. value-weighted and with annual rebalance and without annual rebalance) “MINWIN” is the smallest number of months a company trades after an event to be included in the study. “MAXWIN” is the most months that the study considers in its calculations. Companies aren’t excluded if they have less than MAXWIN months, unless they also have fewer than MINWIN months. The term “MONTH” signifies chosen months (typically 12, 24, or 36) used to work out BHAR. If monthly returns are missing during the set period, matching portfolio returns fill in the gaps. 29.5.2 Long-term Cumulative Abnormal Returns (LCARs) Formula for LCARs during the \\((1,T)\\) postevent horizon (A. B. Sorescu, Chandy, and Prabhu 2007) \\[ LCAR_{pT} = \\sum_{t = 1}^{t = T} (R_{it} - R_{pt}) \\] where \\(R_{it}\\) is the rate of return of stock \\(i\\) in month \\(t\\) \\(R_{pt}\\) is the rate of return on the counterfactual portfolio in month \\(t\\) 29.5.3 Calendar-time Portfolio Abnormal Returns (CTARs) This section follows strictly the procedure in (Wiles et al. 2010) A portfolio for every day in calendar time (including all securities which experience an event that time). For each portfolio, the securities and their returns are equally weighted For all portfolios, the average abnormal return are calculated as \\[ AAR_{Pt} = \\frac{\\sum_{i=1}^S AR_i}{S} \\] where \\(S\\) is the number of securities in portfolio \\(P\\) \\(AR_i\\) is the abnormal return for the stock \\(i\\) in the portfolio For every portfolio \\(P\\), a time series estimate of \\(\\sigma(AAR_{Pt})\\) is calculated for the preceding \\(k\\) days, assuming that the \\(AAR_{Pt}\\) are independent over time. Each portfolio’s average abnormal return is standardized \\[ SAAR_{Pt} = \\frac{AAR_{Pt}}{SD(AAR_{Pt})} \\] Average standardized residual across all portfolio’s in calendar time \\[ ASAAR = \\frac{1}{n}\\sum_{i=1}^{255} SAAR_{Pt} \\times D_t \\] where \\(D_t = 1\\) when there is at least one security in portfolio \\(t\\) \\(D_t = 0\\) when there are no security in portfolio \\(t\\) \\(n\\) is the number of days in which the portfolio have at least one security \\(n = \\sum_{i = 1}^{255}D_t\\) The cumulative average standardized average abnormal returns is \\[ CASSAR_{S_1, S_2} = \\sum_{i=S_1}^{S_2} ASAAR \\] If the ASAAR are independent over time, then standard deviation for the above estimate is \\(\\sqrt{S_2 - S_1 + 1}\\) then, the test statistics is \\[ t = \\frac{CASAAR_{S_1,S_2}}{\\sqrt{S_2 - S_1 + 1}} \\] Limitations Cannot examine individual stock difference, can only see the difference at the portfolio level. One can construct multiple portfolios (based on the metrics of interest) so that firms in the same portfolio shares that same characteristics. Then, one can compare the intercepts in each portfolio. Low power (Loughran and Ritter 2000), type II error is likely. References "],["aggregation.html", "29.6 Aggregation", " 29.6 Aggregation 29.6.1 Over Time We calculate the cumulative abnormal (CAR) for the event windows \\(H_0\\): Standardized cumulative abnormal return for stock \\(i\\) is 0 (no effect of events on stock performance) \\(H_1\\): SCAR is not 0 (there is an effect of events on stock performance) 29.6.2 Across Firms + Over Time Additional assumptions: Abnormal returns of different socks are uncorrelated (rather strong), but it’s very valid if event windows for different stocks do not overlap. If the windows for different overlap, follow (Bernard 1987) and Schipper and Smith (1983) \\(H_0\\): The mean of the abnormal returns across all firms is 0 (no effect) \\(H_1\\): The mean of the abnormal returns across all firms is different form 0 (there is an effect) Parametric (empirically either one works fine) (assume abnormal returns is normally distributed) : Aggregate the CAR of all stocks (Use this if the true abnormal variance is greater for stocks with higher variance) Aggregate the SCAR of all stocks (Use this if the true abnormal return is constant across all stocks) Non-parametric (no parametric assumptions): Sign test: Assume both the abnormal returns and CAR to be independent across stocks Assume 50% with positive abnormal returns and 50% with negative abnormal return The null will be that there is a positive abnormal return correlated with the event (if you want the alternative to be there is a negative relationship) With skewed distribution (likely in daily stock data), the size test is not trustworthy. Hence, rank test might be better Rank test Null: there is no abnormal return during the event window References "],["heterogeneity-in-the-event-effect.html", "29.7 Heterogeneity in the event effect", " 29.7 Heterogeneity in the event effect \\[ y = X \\theta + \\eta \\] where \\(y\\) = CAR \\(X\\) = Characteristics that lead to heterogeneity in the event effect (i.e., abnormal returns) (e.g., firm or event specific) \\(\\eta\\) = error term Note: In cases with selection bias (firm characteristics and investor anticipation of the event: larger firms might enjoy great positive effect of an event, and investors endogenously anticipate this effect and overvalue the stock), we have to use the White’s \\(t\\)-statistics to have the lower bounds of the true significance of the estimates. This technique should be employed even if the average CAR is not significantly different from 0, especially when the CAR variance is high (Boyd, Chandy, and Cunha Jr 2010) 29.7.1 Common variables in marketing (A. Sorescu, Warren, and Ertekin 2017) Table 4 Firm size is negatively correlated with abnormal return in finance (A. Sorescu, Warren, and Ertekin 2017), but mixed results in marketing. # of event occurrences R&amp;D expenditure Advertising expense Marketing investment (SG&amp;A) Industry concentration (HHI, # of competitors) Financial leverage Market share Market size (total sales volume within the firm’s SIC code) marketing capability Book to market value ROA Free cash flow Sales growth Firm age References "],["expected-return-calculation.html", "29.8 Expected Return Calculation", " 29.8 Expected Return Calculation 29.8.1 Statistical Models based on statistical assumptions about the behavior of returns (e..g, multivariate normality) we only need to assume stable distributions (Owen and Rabinovitch 1983) 29.8.1.1 Constant Mean Return Model The expected normal return is the mean of the real returns \\[ Ra_{it} = R_{it} - \\bar{R}_i \\] Assumption: returns revert to its mean (very questionable) The basic mean returns model generally delivers similar findings to more complex models since the variance of abnormal returns is not decreased considerably (S. J. Brown and Warner 1985) 29.8.1.2 Market Model \\[ R_{it} = \\alpha_i + \\beta R_{mt} + \\epsilon_{it} \\] where \\(R_{it}\\) = stock return \\(i\\) in period \\(t\\) \\(R_{mt}\\) = market return \\(\\epsilon_{it}\\) = zero mean (\\(E(e_{it}) = 0\\)) error term with its own variance \\(\\sigma^2\\) Notes: People typically use S&amp;P 500, CRSP value-weighed or equal-weighted index as the market portfolio. When \\(\\beta =0\\), the Market Model is the Constant Mean Return Model better fit of the market-model, the less variance in abnormal return, and the more easy to detect the event’s effect recommend generalized method of moments to be robust against auto-correlation and heteroskedasticity 29.8.1.3 Fama-French Model Please note that there is a difference between between just taking the return versus taking the excess return as the dependent variable. The correct way is to use the excess return for firm and for market (Fama and French 2010, 1917). \\(\\alpha_i\\) “is the average return left unexplained by the benchmark model” (i.e., abnormal return) 29.8.1.3.1 FF3 (Fama and French 1993) \\[ \\begin{aligned} E(R_{it}|X_t) - r_{ft} = \\alpha_i &amp;+ \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\ &amp;+ b_{2i} SML_t + b_{3i} HML_t \\end{aligned} \\] where \\(r_{ft}\\) risk-free rate (e.g., 3-month Treasury bill) \\(R_{mt}\\) is the market-rate (e.g., S&amp;P 500) SML: returns on small (size) portfolio minus returns on big portfolio HML: returns on high (B/M) portfolio minus returns on low portfolio. 29.8.1.3.2 FF4 (A. Sorescu, Warren, and Ertekin 2017, 195) suggest the use of Market Model in marketing for short-term window and Fama-French Model for the long-term window (the statistical properties of this model have not been examined the the daily setting). (Carhart 1997) \\[ \\begin{aligned} E(R_{it}|X_t) - r_{ft} = \\alpha_i &amp;+ \\beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\\\ &amp;+ b_{2i} SML_t + b_{3i} HML_t + b_{4i} UMD_t \\end{aligned} \\] where \\(UMD_t\\) is the momentum factor (difference between high and low prior return stock portfolios) in day \\(t\\). 29.8.2 Economic Model The only difference between CAPM and APT is that APT has multiple factors (including factors beyond the focal company) Economic models put limits on a statistical model that come from assumed behavior that is derived from theory. 29.8.2.1 Capital Asset Pricing Model (CAPM) \\[ E(R_i) = R_f + \\beta_i (E(R_m) - R_f) \\] where \\(E(R_i)\\) = expected firm return \\(R_f\\) = risk free rate \\(E(R_m - R_f)\\) = market risk premium \\(\\beta_i\\) = firm sensitivity 29.8.2.2 Arbitrage Pricing Theory (APT) \\[ R = R_f + \\Lambda f + \\epsilon \\] where \\(\\epsilon \\sim N(0, \\Psi)\\) \\(\\Lambda\\) = factor loadings \\(f \\sim N(\\mu, \\Omega)\\) = general factor model \\(\\mu\\) = expected risk premium vector \\(\\Omega\\) = factor covariance matrix References "],["application-14.html", "29.9 Application", " 29.9 Application Packages: eventstudies erer EventStudy AbnormalReturns Event Study Tools estudy2 PerformanceAnalytics In practice, people usually sort portfolio because they are not sure whether the FF model is specified correctly. Steps: Sort all returns in CRSP into 10 deciles based on size. In each decile, sort returns into 10 decides based on BM Get the average return of the 100 portfolios for each period (i.e., expected returns of stocks given decile - characteristics) For each stock in the event study: Compare the return of the stock to the corresponding portfolio based on size and BM. Notes: Sorting produces outcomes that are often more conservative (e.g., FF abnormal returns can be greater than those that used sorting). If the results change when we do B/M first then size or vice versa, then the results are not robust (this extends to more than just two characteristics - e.g., momentum). Examples: Forestry: (Mei and Sun 2008) M&amp;A on financial performance (forest product) (C. Sun and Liao 2011) litigation on firm values library(erer) # example by the package&#39;s author data(daEsa) hh &lt;- evReturn( y = daEsa, # dataset firm = &quot;wpp&quot;, # firm name y.date = &quot;date&quot;, # date in y index = &quot;sp500&quot;, # index est.win = 250, # estimation window wedith in days digits = 3, event.date = 19990505, # firm event dates event.win = 5 # one-side event window wdith in days (default = 3, where 3 before + 1 event date + 3 days after = 7 days) ) hh; plot(hh) #&gt; #&gt; === Regression coefficients by firm ========= #&gt; N firm event.date alpha.c alpha.e alpha.t alpha.p alpha.s beta.c beta.e #&gt; 1 1 wpp 19990505 -0.135 0.170 -0.795 0.428 0.665 0.123 #&gt; beta.t beta.p beta.s #&gt; 1 5.419 0.000 *** #&gt; #&gt; === Abnormal returns by date ================ #&gt; day Ait.wpp HNt #&gt; 1 -5 4.564 4.564 #&gt; 2 -4 0.534 5.098 #&gt; 3 -3 -1.707 3.391 #&gt; 4 -2 2.582 5.973 #&gt; 5 -1 -0.942 5.031 #&gt; 6 0 -3.247 1.784 #&gt; 7 1 -0.646 1.138 #&gt; 8 2 -2.071 -0.933 #&gt; 9 3 0.368 -0.565 #&gt; 10 4 4.141 3.576 #&gt; 11 5 0.861 4.437 #&gt; #&gt; === Average abnormal returns across firms === #&gt; name estimate error t.value p.value sig #&gt; 1 CiT.wpp 4.437 8.888 0.499 0.618 #&gt; 2 GNT 4.437 8.888 0.499 0.618 Example by Ana Julia Akaishi Padula, Pedro Albuquerque (posted on LAMFO) Example in AbnormalReturns package 29.9.1 Eventus 2 types of output: Using different estimation methods (e.g., market model to calendar-time approach) Does not include event-specific returns. Hence, no regression later to determine variables that can affect abnormal stock returns. Cross-sectional Analysis of Eventus: Event-specific abnormal returns (using monthly or data data) for cross-sectional analysis (under Cross-Sectional Analysis section) Since it has the stock-specific abnormal returns, we can do regression on CARs later. But it only gives market-adjusted model. However, according to (A. Sorescu, Warren, and Ertekin 2017), they advocate for the use of market-adjusted model for the short-term only, and reserve the FF4 for the longer-term event studies using monthly daily. 29.9.1.1 Basic Event Study Input a text file contains a firm identifier (e.g., PERMNO, CUSIP) and the event date Choose market indices: equally weighted and the value weighted index (i.e., weighted by their market capitalization). And check Fama-French and Carhart factors. Estimation options Estimation period: ESTLEN = 100 is the convention so that the estimation is not impacted by outliers. Use “autodate” options: the first trading after the event date is used if the event falls on a weekend or holiday Abnormal returns window: depends on the specific event Choose test: either parametric (including Patell Standardized Residual (PSR)) or non-parametric 29.9.1.2 Cross-sectional Analysis of Eventus Similar to the Basic Event Study, but now you can have event-specific abnormal returns. 29.9.2 Evenstudies This package does not use the Fama-French model, only the market models. This example is by the author of the package library(eventstudies) # firm and date data data(&quot;SplitDates&quot;) head(SplitDates) # stock price data data(&quot;StockPriceReturns&quot;) head(StockPriceReturns) class(StockPriceReturns) es &lt;- eventstudy( firm.returns = StockPriceReturns, event.list = SplitDates, event.window = 5, type = &quot;None&quot;, to.remap = TRUE, remap = &quot;cumsum&quot;, inference = TRUE, inference.strategy = &quot;bootstrap&quot; ) plot(es) 29.9.3 EventStudy You have to pay for the API key. (It’s $10/month). library(EventStudy) Example by the authors of the package Data Prep library(tidyquant) library(tidyverse) library(readr) library(&quot;Quandl&quot;) library(&quot;quantmod&quot;) Quandl.auth(&quot;LDqWhYXzVd2omw4zipN2&quot;) TWTR &lt;- Quandl(&quot;NSE/OIL&quot;,type =&quot;xts&quot;) candleChart(TWTR) addSMA(col=&quot;red&quot;) #Adding a Simple Moving Average addEMA() #Adding an Exponential Moving Average Reference market in Germany is DAX # Index Data # indexName &lt;- c(&quot;DAX&quot;) indexData &lt;- tq_get(&quot;^GDAXI&quot;, from = &quot;2014-05-01&quot;, to = &quot;2015-12-31&quot;) %&gt;% mutate(date = format(date, &quot;%d.%m.%Y&quot;)) %&gt;% mutate(symbol = &quot;DAX&quot;) head(indexData) Create files 01_RequestFile.csv 02_FirmData.csv 03_MarketData.csv Calculating abnormal returns # get &amp; set parameters for abnormal return Event Study # we use a garch model and csv as return # Attention: fitting a GARCH(1, 1) model is compute intensive esaParams &lt;- EventStudy::ARCApplicationInput$new() esaParams$setResultFileType(&quot;csv&quot;) esaParams$setBenchmarkModel(&quot;garch&quot;) dataFiles &lt;- c( &quot;request_file&quot; = file.path(getwd(), &quot;data&quot;, &quot;EventStudy&quot;, &quot;01_requestFile.csv&quot;), &quot;firm_data&quot; = file.path(getwd(), &quot;data&quot;, &quot;EventStudy&quot;, &quot;02_firmDataPrice.csv&quot;), &quot;market_data&quot; = file.path(getwd(), &quot;data&quot;, &quot;EventStudy&quot;, &quot;03_marketDataPrice.csv&quot;) ) # check data files, you can do it also in our R6 class EventStudy::checkFiles(dataFiles) arEventStudy &lt;- estSetup$performEventStudy(estParams = esaParams, dataFiles = dataFiles, downloadFiles = T) library(EventStudy) apiUrl &lt;- &quot;https://api.eventstudytools.com&quot; Sys.setenv(EventStudyapiKey = &quot;&quot;) # The URL is already set by default options(EventStudy.URL = apiUrl) options(EventStudy.KEY = Sys.getenv(&quot;EventStudyapiKey&quot;)) # use EventStudy estAPIKey function estAPIKey(Sys.getenv(&quot;EventStudyapiKey&quot;)) # initialize object estSetup &lt;- EventStudyAPI$new() estSetup$authentication(apiKey = Sys.getenv(&quot;EventStudyapiKey&quot;)) References "],["instrumental-variables.html", "Chapter 30 Instrumental Variables", " Chapter 30 Instrumental Variables Similar to RCT, we try to introduce randomization (random assignment to treatment) to our treatment variable by using only variation in the instrument. Logic of using an instrument: Use only exogenous variation to see the variation in treatment (try to exclude all endogenous variation in the treatment) Use only exogenous variation to see the variation in outcome (try to exclude all endogenous variation in the outcome) See the relationship between treatment and outcome in terms of residual variations that are exogenous to omitted variables. Notes: Instruments can be used to remove attenuation bias in errors-in-variables. Be careful with the F-test and standard errors when you do 2SLS by hand (you need to correct them). Repeated use of related IVs across different studies can collectively invalidate these instruments, primarily through the violation of the exclusion restriction (Gallen 2020). One needs to test for invalid instruments (Hausman-like test). Mellon (2023) shows the widespread use of weather as an instrument in social sciences (289 studies linking weather to 195 variables) demonstrates significant exclusion violations that can overturn many IV results. For [Zero-valued Outcomes], we can’t directly interpret the treatment coefficient of log-transformed outcome regression as percentage change (J. Chen and Roth 2023). We have to distinguish the treatment effect on the intensive (outcome: 10 to 11) vs. extensive margins (outcome: 0 to 1), and we can’t readily interpret the treatment coefficient of log-transformed outcome regression as percentage change. To have percentage change interpretation, we can either do: Proportional LATE: estimate \\(\\theta_{ATE\\%}\\) for those who are compliers under the instrument. To estimate proportional LATE, Regress \\(Y_i = \\beta D_i + X_i + \\epsilon_i\\) using 2SLS with an instrument on \\(D_i\\), where \\(\\beta\\) is interpreted as the LATE in levels of the control group’s mean for compliers. Get estimate of the control complier mean by regressing with same 2SLS regression (Abadie, Angrist, and Imbens 2002) where the final outcome is \\(-(D_i - 1)Y_i\\) , we refer to the new new estimated effect of \\(D_i\\) as \\(\\beta_{cc}\\) The \\(\\theta_{ATE \\%}\\) for compliers that are induced by the instrument is \\(\\hat{\\beta}/\\hat{\\beta}_{cc}\\), where it can be interpreted directly as the percentage change for compliers that are induced by the instrument under treatment as compared to under control. SE can be obtained by non-parametric bootstrap. For specific case that the instrument is binary, \\(\\theta\\) of the intensive margin for compliers can be directly obtained by Poisson IV regression (ivpoisson in Stata). Lee (2009) bounds: we can get bounds for the average treatment effect in logs for compliers who have positive outcome regardless of treatment status (i.e., intensive-margin effect). This requires a monotonicity assumption for compliers where they should still have positive outcome regardless of treatment status. Notes on First-stage: Always use the OLS regression in the first stage (regardless of the type of endogenous variables - e.g., continuous or discreet) (suggested by (J. D. Angrist and Pischke 2009). Estimates of IV can still be consistent regardless of the form of the endogenous variables (discreet vs. continuous). Alternatively, we could use “biprobit” model, but this is applicable only in cases where you have both dependent and endogenous variables to be binary. If you still want to continue and use logit or probit models for the first stage when you have binary variables, you have a “forbidden regression” (also 1, 2) (i.e., an incorrect extension of 2SLS to a nonlinear case). There are several ways to understand this problem: Identification strategy: The identification strategy in instrumental variables analysis relies on the fact that the instrumental variable affects the outcome variable only through its effect on the endogenous variable. However, when the endogenous variable is binary, the relationship between the instrumental variable and the endogenous variable is not continuous. This means that the instrumental variable can only affect the endogenous variable in discrete jumps, rather than through a continuous change. As a result, the identification of the causal effect of the endogenous variable on the outcome variable may not be possible with probit or logit regression in the first stage. Model assumptions: Both models assume that the error term has a specific distribution (normal or logistic), and that the probability of the binary outcome is a function of the linear combination of the regressors. When the endogenous variable is binary, however, the distribution of the error term is not specified, as there is no continuous relationship between the endogenous variable and the outcome variable. This means that the assumptions of the probit and logit models may not hold, and the resulting estimates may not be reliable or interpretable. Issue of weak instruments: When the instrument is weak, the variance of the inverse Mills ratio (which is used to correct for endogeneity in instrumental variables analysis) can be very large. In the case of binary endogenous variables, the inverse Mills ratio cannot be consistently estimated using probit or logit regression, and this can lead to biased and inconsistent estimates of the causal effect of the endogenous variable on the outcome variable. Problems with weak instruments (Bound, Jaeger, and Baker 1995): Weak instrumental variables can produce (finite-sample) biased and inconsistent estimates of the causal effect of an endogenous variable on an outcome variable (even in the presence of large sample size) In a finite sample, instrumental variables (IV) estimates can be biased in the same direction as ordinary least squares (OLS) estimates. Additionally, the bias of IV estimates approaches that of OLS estimates as the correlation (R2) between the instruments and the endogenous explanatory variable approaches zero. This means that when the correlation between the instruments and the endogenous variable is weak, the bias of the IV estimates can be similar to that of the OLS estimates. Weak instruments are problematic because they do not have enough variation to fully capture the variation in the endogenous variable, leading to measurement error and other sources of noise in the estimates. Using weak instruments can produce large standard errors and low t-ratio. And when the feedback (reverse causality) is strong, the bias in IV is even greater than that of OLS (C. Nelson and Startz 1988). Using lagged dependent variables as instruments for current values depends on serial correlations, typically low (C. Nelson and Startz 1988). Using multiple covariates to artificially increase the first-stage \\(R^2\\) does not solve this weak instrument problem (C. Nelson and Startz 1988). Solutions: use of multiple instruments use of instrumental variables with higher correlation use of alternative estimation methods such as limited information maximum likelihood (LIML) or two-stage least squares (2SLS) with heteroscedasticity-robust standard errors. Instrument Validity: Random assignment (Exogeneity Assumption). Any effect of the instrument on the outcome must be through the endogenous variable (Relevance Assumption). References "],["framework.html", "30.1 Framework", " 30.1 Framework \\(D_i \\sim Bern\\) Dummy Treatment \\(Y_{0i}, Y_{1i}\\) potential outcomes \\(Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i\\) observed outcome \\(Z_i \\perp Y_{0i}, Y_{1i}\\) Instrumental variables (and also correlate with \\(D_i\\)) Under constant-effects and linear (\\(Y_{1i} - Y_{0i}\\) are the same for everyone) \\[ \\begin{aligned} Y_{0i} &amp;= \\alpha + \\eta_i \\\\ Y_{1i} - Y_{0i} &amp;= \\rho \\\\ Y_i &amp;= Y_{0i} + D_i (Y_{1i} - Y_{0i}) \\\\ &amp;= \\alpha + \\eta_i + D_i \\rho \\\\ &amp;= \\alpha + \\rho D_i + \\eta_i \\end{aligned} \\] where \\(\\eta_i\\) is individual differences \\(\\rho\\) is the difference between treated outcome and untreated outcome. Here we assume they are constant for everyone However, we have a problem with OLS because \\(D_i\\) is correlated with \\(\\eta_i\\) for each unit But \\(Z_i\\) can come to the rescue, the causal estimate can be written as \\[ \\begin{aligned} \\rho &amp;= \\frac{Cov( Y_i, Z_i)}{Cov(D_i, Z_i)} \\\\ &amp;= \\frac{Cov(Y_i, Z_i) / V(Z_i) }{Cov( D_i, Z_i) / V(Z_i)} = \\frac{Reduced form}{First-stage} \\\\ &amp;= \\frac{E[Y_i |Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i | Z_i = 0 ]} \\end{aligned} \\] Under heterogeneous treatment effect (\\(Y_{1i} - Y_{0i}\\) are different for everyone) with LATE framework \\(Y_i(d,z)\\) denotes the potential outcome for unit \\(i\\) with treatment \\(D_i = d\\) and instrument \\(Z_i = z\\) Observed treatment status \\[ D_i = D_{0i} + Z_i (D_{1i} - D_{0i}) \\] where \\(D_{1i}\\) is treatment status of unit \\(i\\) when \\(z_i = 1\\) \\(D_{0i}\\) is treatment status of unit \\(i\\) when \\(z_i = 0\\) \\(D_{1i} - D_{0i}\\) is the causal effect of \\(Z_i\\) on \\(D_i\\) Assumptions Independence: The instrument is randomly assigned (i.e., independent of potential outcomes and potential treatments) \\([\\{Y_i(d,z); \\forall d, z \\}, D_{1i}, D_{0i} ] \\Pi Z_i\\) This assumption let the first-stage equation be the average causal effect of \\(Z_i\\) on \\(D_i\\) \\[ \\begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &amp;= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\\\ &amp;= E[D_{1i} - D_{0i}] \\end{aligned} \\] This assumption also is sufficient for a causal interpretation of the reduced form, where we see the effect of the instrument on the outcome. \\[ E[Y_i |Z_i = 1 ] - E[Y_i|Z_i = 0] = E[Y_i (D_{1i}, Z_i = 1) - Y_i (D_{0i} , Z_i = 0)] \\] Exclusion (i.e., existence of instruments (G. W. Imbens and Angrist 1994) The treatment \\(D_i\\) fully mediates the effect of \\(Z_i\\) on \\(Y_i\\) \\[ Y_{1i} = Y_i (1,1) = Y_i (1,0) \\\\ Y_{0i} = Y_i (0,1) = Y_i (0, 0) \\] With this assumption, the observed outcome \\(Y_i\\) can be thought of as (assume \\(Y_{1i}, Y_{0i}\\) already satisfy the independence assumption) \\[ \\begin{aligned} Y_i &amp;= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\\\ &amp;= Y_{0i} + (Y_{1i} - Y_{0i} ) D_i \\end{aligned} \\] This assumption let us go from reduced-form causal effects to treatment effects (J. D. Angrist and Imbens 1995) Monotonicity: \\(D_{1i} &gt; D_{0i} \\forall i\\) With this assumption, we have \\(E[D_{1i} - D_{0i} ] = P[D_{1i} &gt; D_{0i}]\\) This assumption lets us assume that there is a first stage, in which we examine the proportion of the population that \\(D_i\\) is driven by \\(Z_i\\) This assumption is used to solve to problem of the shifts between participation status back to non-participation status. Alternatively, one can solve the same problem by assuming constant (homogeneous) treatment effect (G. W. Imbens and Angrist 1994), but this is rather restrictive. A third solution is the assumption that there exists a value of the instrument, where the probability of participation conditional on that value is 0 J. Angrist and Imbens (1991). With these three assumptions, we have the LATE theorem (J. D. Angrist and Pischke 2009, 4.4.1) \\[ \\frac{E[Y_i | Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i |Z_i = 0]} = E[Y_{1i} - Y_{0i} | D_{1i} &gt; D_{0i}] \\] LATE assumptions allow us to go back to the types of subjects we have in Causal Inference Switchers: Compliers: \\(D_{1i} &gt; D_{0i}\\) Non-switchers: Always-takers: \\(D_{1i} = D_{0i} = 1\\) Never-takers: \\(D_{1i} = D_{0i} = 0\\) Instrumental Variables can’t say anything about non-switchers because treatment status \\(D_i\\) has no effects on them (similar to fixed effects models). When all groups are the same, we come back to the constant-effects world. Treatment effects on the treated is a weighted average of always-takers and compliers. In the special case of IV in randomized trials, we have a compliance problem (when compliance is voluntary), where those in the treated will not always take the treatment (i.e., might be selection bias). Intention-to-treat analysis is valid, but contaminated by non-compliance IV in this case (\\(Z_i\\) = random assignment to the treatment; \\(D_i\\) = whether the unit actually received/took the treatment) can solve this problem. Under certain assumptions (i.e., SUTVA, random assignment, exclusion restriction, no defiers, and monotinicity), this analysis can give causal interpreation of LATE because it’s the average causal effect for the compliers only. Without these assumptions, it’s a ratio of intention-to-treat. Without always-takers in this case, LATE = Treatment effects on the treated See proof Bloom (1984) and examples Bloom et al. (1997) and Sherman and Berk (1984) \\[ \\frac{E[Y_i |Z_i = 1] - E[Y_i |Z_i = 0]}{E[D_i |Z_i = 1]} = \\frac{\\text{Intention-to-treat effect}}{\\text{Compliance rate}} \\\\ = E[Y_{1i} - Y_{0i} |D_i = 1] \\] References "],["estimation-4.html", "30.2 Estimation", " 30.2 Estimation 30.2.1 2SLS Estimation A special case of IV-GMM Examples by authors of fixest package library(fixest) base = iris names(base) = c(&quot;y&quot;, &quot;x1&quot;, &quot;x_endo_1&quot;, &quot;x_inst_1&quot;, &quot;fe&quot;) set.seed(2) base$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5) base$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5) # est_iv = feols(y ~ x1 | x_endo_1 ~ x_inst_1 , base) est_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base) est_iv #&gt; TSLS estimation - Dep. Var.: y #&gt; Endo. : x_endo_1, x_endo_2 #&gt; Instr. : x_inst_1, x_inst_2 #&gt; Second stage: Dep. Var.: y #&gt; Observations: 150 #&gt; Standard-errors: IID #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.831380 0.411435 4.45121 1.6844e-05 *** #&gt; fit_x_endo_1 0.444982 0.022086 20.14744 &lt; 2.2e-16 *** #&gt; fit_x_endo_2 0.639916 0.307376 2.08186 3.9100e-02 * #&gt; x1 0.565095 0.084715 6.67051 4.9180e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; RMSE: 0.398842 Adj. R2: 0.761653 #&gt; F-test (1st stage), x_endo_1: stat = 903.2 , p &lt; 2.2e-16 , on 2 and 146 DoF. #&gt; F-test (1st stage), x_endo_2: stat = 3.25828, p = 0.041268, on 2 and 146 DoF. #&gt; Wu-Hausman: stat = 6.79183, p = 0.001518, on 2 and 144 DoF. Default statistics F-test first-stage (weak instrument test) Wu-Hausman endogeneity test Over-identifying restriction (Sargan) J-test fitstat( est_iv, type = c( &quot;n&quot;, &quot;ll&quot;, &quot;aic&quot;, &quot;bic&quot;, &quot;rmse&quot;, # ll means log-likelihood &quot;my&quot;, # mean dependent var &quot;g&quot;, # degrees of freedom used to compute the t-test &quot;r2&quot;, &quot;ar2&quot;, &quot;wr2&quot;, &quot;awr2&quot;, &quot;pr2&quot;, &quot;apr2&quot;, &quot;wpr2&quot;, &quot;awpr2&quot;, &quot;theta&quot;, # over-dispersion parameter in Negative Binomial models &quot;f&quot;, &quot;wf&quot;, # F-tests of nullity of the coefficients &quot;wald&quot;, # Wald test of joint nullity of the coefficients &quot;ivf&quot;, &quot;ivf1&quot;, &quot;ivf2&quot;, &quot;ivfall&quot;, &quot;ivwald&quot;, &quot;ivwald1&quot;, &quot;ivwald2&quot;, &quot;ivwaldall&quot;, &quot;cd&quot; # &quot;kpr&quot; ), cluster = &#39;fe&#39; ) #&gt; Observations: 150 #&gt; Log-Likelihood: -75.0 #&gt; AIC: 157.9 #&gt; BIC: 170.0 #&gt; RMSE: 0.398842 #&gt; Dep. Var. mean: 5.84333 #&gt; G: 3 #&gt; R2: 0.766452 #&gt; Adj. R2: 0.761653 #&gt; Within R2: NA #&gt; awr2: NA #&gt; Pseudo R2: 0.592684 #&gt; Adj. Pseudo R2: 0.576383 #&gt; Within Pseudo R2: NA #&gt; awpr2: NA #&gt; Over-dispersion: NA #&gt; F-test: stat = 1.80769, p = 0.375558, on 3 and 2 DoF. #&gt; F-test (projected): NA #&gt; Wald (joint nullity): stat = 539,363.2 , p &lt; 2.2e-16 , on 3 and 146 DoF, VCOV: Clustered (fe). #&gt; F-test (1st stage), x_endo_1: stat = 903.2 , p &lt; 2.2e-16 , on 2 and 146 DoF. #&gt; F-test (1st stage), x_endo_2: stat = 3.25828, p = 0.041268, on 2 and 146 DoF. #&gt; F-test (2nd stage): stat = 194.2 , p &lt; 2.2e-16 , on 2 and 146 DoF. #&gt; F-test (IV only): stat = 194.2 , p &lt; 2.2e-16 , on 2 and 146 DoF. #&gt; Wald (1st stage), x_endo_1 : stat = 1,482.6 , p &lt; 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe). #&gt; Wald (1st stage), x_endo_2 : stat = 2.22157, p = 0.112092, on 2 and 146 DoF, VCOV: Clustered (fe). #&gt; Wald (2nd stage): stat = 539,363.2 , p &lt; 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe). #&gt; Wald (IV only): stat = 539,363.2 , p &lt; 2.2e-16 , on 2 and 146 DoF, VCOV: Clustered (fe). #&gt; Cragg-Donald: 3.11162 To set default printing # always add second-stage Wald test setFixest_print(fitstat = ~ . + ivwald2) est_iv To see results from different stages # first-stage summary(est_iv, stage = 1) # second-stage summary(est_iv, stage = 2) # both stages etable(summary(est_iv, stage = 1:2), fitstat = ~ . + ivfall + ivwaldall.p) etable(summary(est_iv, stage = 2:1), fitstat = ~ . + ivfall + ivwaldall.p) # .p means p-value, not statistic # `all` means IV only 30.2.2 IV-GMM This is a more general framework. 2SLS Estimation is a special case of IV-GMM estimator \\[ Y = X \\beta + u, u \\sim (0, \\Omega) \\] where \\(X\\) is a matrix of endogenous variables (\\(N\\times k\\)) We will use a matrix of instruments \\(X\\) where it has \\(N \\times l\\) dimensions (where \\(l \\ge k\\)) Then, we can have a set of \\(l\\) moments: \\[ g_i (\\beta) = Z_i&#39; u_i = Z_i&#39; (Y_i - X_i \\beta) \\] where \\(i \\in (1,N)\\) Each \\(l\\) moment equation is a sample moment, which can be estimated by averaging over \\(N\\) \\[ \\bar{g}(\\beta) = \\frac{1}{N} \\sum_{i = 1}^N Z_i (Y_i - X_i \\beta) = \\frac{1}{N} Z&#39;u \\] GMM then estimate \\(\\beta\\) so that \\(\\bar{g}(\\hat{\\beta}_{GMM}) = 0\\) When \\(l = k\\) there is a unique solution to this system of equations (and equivalent to the IV estimator) \\[ \\hat{\\beta}_{IV} = (Z&#39;X)^{-1}Z&#39;Y \\] When \\(l &gt; k\\), we have a set of \\(k\\) instruments \\[ \\hat{X} = Z(Z&#39;Z)^{-1} Z&#39; X = P_ZX \\] then we can use the 2SLS estimator \\[ \\begin{aligned} \\hat{\\beta}_{2SLS} &amp;= (\\hat{X}&#39;X)^{-1} \\hat{X}&#39; Y \\\\ &amp;= (X&#39;P_Z X)^{-1}X&#39; P_Z Y \\end{aligned} \\] Differences between 2SLS and IV-GMM: In the 2SLS method, when there are more instruments available than what is actually needed for the estimation, to address this, a matrix is created that only includes the necessary instruments, which simplifies the calculation. The IV-GMM method uses all the available instruments, but applies a weighting system to prioritize the instruments that are most relevant. This approach is useful when there are more instruments than necessary, which can make the calculation more complex. The IV-GMM method uses a criterion function to weight the estimates and improve their accuracy. In short, always use IV-GMM when you have overid problems GMM estimator minimizes \\[ J (\\hat{\\beta}_{GMM} ) = N \\bar{g}(\\hat{\\beta}_{GMM})&#39; W \\bar{g} (\\hat{\\beta}_{GMM}) \\] where \\(W\\) is a symmetric weighting matrix \\(l \\times l\\) For an overid equation, solving the set of FOCs for the IV-GMM estimator, we should have \\[ \\hat{\\beta}_{GMM} = (X&#39;ZWZ&#39; X)^{-1} X&#39;ZWZ&#39;Y \\] which is identical for all \\(W\\) matrices. The optimal \\(W = S^{-1}\\) (L. P. Hansen 1982) where \\(S\\) is the covariance matrix of the moment conditions to produce the most efficient estimator: \\[ S = E[Z&#39;uu&#39;Z] = \\lim_{N \\to \\infty} N^{-1}[Z&#39; \\Omega Z] \\] With a consistent estimator of \\(S\\) from the 2SLS residuals, the feasible IV-GMM estimator can be defined as \\[ \\hat{\\beta}_{FEGMM} = (X&#39;Z \\hat{S}^{-1} Z&#39; X)^{-1} X&#39;Z \\hat{S}^{-1} Z&#39;Y \\] In cases where \\(\\Omega\\) (i.e., the vcov of the error process \\(u\\)) satisfy all classical assumptions IID \\(S = \\sigma^2_u I_N\\) The optimal weighting matrix is proportional to the identity matrix Then, IV-GMM estimator is the standard IV (or 2SLS) estimator. For IV-GMM, you also have an additional test of overid restrictions: GMM distance (also known as Hayashi C statistic) To account for clustering, one can use code provided by this blog References "],["inference-4.html", "30.3 Inference", " 30.3 Inference Under just-identified instrument variable model, we have \\[ Y = \\beta X + u \\] where \\(corr(u, Z) = 0\\) (relevant assumption) and \\(corr(Z,X) \\neq 0\\) (exogenous assumption) The t-ratio approach to construct the 95 CIs is \\[ \\hat{\\beta} \\pm 1.96 \\sqrt{\\hat{V}_N(\\hat{\\beta})} \\] But this is wrong, and has been long recognized by those who understand the “weak instruments” problem Dufour (1997) To test the null hypothesis of \\(\\beta = \\beta_0\\) (Lee et al. 2022) \\[ \\frac{(\\hat{\\beta} - \\beta_0)^2}{\\hat{V}_N(\\hat{\\beta})} = \\hat{t}^2 = \\hat{t}^2_{AR} \\times \\frac{1}{1 - \\hat{\\rho} \\frac{\\hat{t}_{AR}}{\\hat{f}} + \\frac{\\hat{t}^2_{AR}}{\\hat{f}^2}} \\] where \\(\\hat{t}_{AR}^2 \\sim \\chi^2(1)\\) (even with weak instruments) (T. W. Anderson and Rubin 1949) \\[ \\hat{t}_{AR} = \\frac{\\hat{\\pi}(\\hat{\\beta} - \\beta_0)}{\\sqrt{\\hat{V}_N (\\hat{\\pi} (\\hat{\\beta} - \\beta_0))}} \\sim N(0,1) \\] where \\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N(\\hat{\\pi})}}\\sim N\\) \\(\\hat{\\pi}\\) = 1st-stage coefficient \\(\\hat{\\rho} = COV(Zv, Zu)\\) = correlation between the 1st-stage residual and an estimate of \\(u\\) Even in large samples, \\(\\hat{t}^2 \\neq \\hat{t}^2_{AR}\\) because the right-hand term does not have a degenerate distribution. Thus, the normal t critical values wouldn’t work. The t-ratios does not match that of standard normal, but it matches the proposed density by Staiger and Stock (1997) and J. H. Stock and Yogo (2005) . The deviation between \\(\\hat{t}^2 , \\hat{t}^2_{AR}\\) depends on \\(\\pi\\) (i.e., correlation between the instrument and the endogenous variable) \\(E(F)\\) (i.e., strength of the first-stage) Magnitude of \\(|\\rho|\\) (i.e., degree of endogeneity) Hence, we can think of several scenarios: Worst case: Very weak first stage (\\(\\pi = 0\\)) and high degree of endogeneity (\\(|\\rho |= 1\\)). The interval \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) does not contain the true parameter \\(\\beta\\). A 5 percent significance test under these conditions will incorrectly reject the null hypothesis (\\(\\beta = \\beta_0\\)) 100% of the time. Best case: No endogeneity (\\(\\rho =0\\)) or very large \\(\\hat{f}\\) (very strong first-stage) The interval \\(\\hat{\\beta} \\pm 1.96 \\times SD\\) accurately contains \\(\\beta\\) at least 95% of the time. Intermediate case: The performance of the interval lies between the two extremes. Solutions: To have valid inference of \\(\\hat{\\beta} \\pm 1.96 \\times SE\\) using t-ratio (\\(\\hat{t}^2 \\approx \\hat{t}^2_{AR}\\)), we can either Assume our problem away Assume \\(E(F) &gt; 142.6\\) (Lee et al. 2022) (Not much of an assumption since we can observe first-stage F-stat empirically). Assume \\(|\\rho| &lt; 0.565\\) Lee et al. (2022), but this defeats our motivation to use IV in the first place because we think there is a strong endogeneity bias, that’s why we are trying to correct for it (circular argument). Deal with it head on AR approach (T. W. Anderson and Rubin 1949) tF Procedure (Lee et al. 2022) AK approach (J. Angrist and Kolesár 2023) Common Practices &amp; Challenges: The t-ratio test is preferred by many researchers but has its pitfalls: Known to over-reject (equivalently, under-cover confidence intervals), especially with weak instruments Dufour (1997). To address this: The first-stage F-statistic is used as an indicator of weak instruments. J. H. Stock and Yogo (2005) provided a framework to understand and correct these distortions. Misinterpretations: Common errors in application: Using a rule-of-thumb F-stat threshold of 10 instead of referring to J. H. Stock and Yogo (2005). Mislabeling intervals such as \\(\\hat{\\beta} \\pm 1.96 \\times \\hat{se}(\\hat{\\beta})\\) as 95% confidence intervals (when passed the \\(F&gt;10\\) rule of thumb). Staiger and Stock (1997) clarified that such intervals actually represent 85% confidence when using \\(F &gt; 16.38\\) from J. H. Stock and Yogo (2005) Pretesting for weak instruments might exacerbate over-rejection of the t-ratio test mentioned above (A. R. Hall, Rudebusch, and Wilcox 1996). Selective model specification (i.e., dropping certain specification) based on F-statistics also leads to significant distortions (I. Andrews, Stock, and Sun 2019). 30.3.1 AR approach Validity of Anderson-Rubin Test (notated as AR) (T. W. Anderson and Rubin 1949): Gives accurate results even under non-normal and homoskedastic errors (Staiger and Stock 1997). Maintains validity across diverse error structures (J. H. Stock and Wright 2000). Minimizes type II error among several alternative tests, in cases of: Homoskedastic errors M. J. Moreira (2009). Generalized for heteroskedastic, clustered, and autocorrelated errors (H. Moreira and Moreira 2019). library(ivDiag) # AR test (robust to weak instruments) # example by the package&#39;s authors ivDiag::AR_test( data = rueda, Y = &quot;e_vote_buying&quot;, # treatment D = &quot;lm_pob_mesa&quot;, # instruments Z = &quot;lz_pob_mesa_f&quot;, controls = c(&quot;lpopulation&quot;, &quot;lpotencial&quot;), cl = &quot;muni_code&quot;, CI = FALSE ) #&gt; $Fstat #&gt; F df1 df2 p #&gt; 50.5097 1.0000 4350.0000 0.0000 g &lt;- ivDiag::ivDiag( data = rueda, Y = &quot;e_vote_buying&quot;, D = &quot;lm_pob_mesa&quot;, Z = &quot;lz_pob_mesa_f&quot;, controls = c(&quot;lpopulation&quot;, &quot;lpotencial&quot;), cl = &quot;muni_code&quot;, cores = 4, bootstrap = FALSE ) g$AR #&gt; $Fstat #&gt; F df1 df2 p #&gt; 50.5097 1.0000 4350.0000 0.0000 #&gt; #&gt; $ci.print #&gt; [1] &quot;[-1.2545, -0.7156]&quot; #&gt; #&gt; $ci #&gt; [1] -1.2545169 -0.7155854 #&gt; #&gt; $bounded #&gt; [1] TRUE ivDiag::plot_coef(g) 30.3.2 tF Procedure Lee et al. (2022) propose a new method that is aligned better with traditional econometric training than AR, where it is called the tF procedure. It incorporates both the 1st-stage F-stat and the 2SLS \\(t\\)-value. This method is applicable to single instrumental variable (i.e., just-identified model), including Randomized trials with imperfect compliance (G. W. Imbens and Angrist 1994). Fuzzy Regression Discontinuity designs (Lee and Lemieux 2010). Fuzzy regression kink designs (Card et al. 2015). See I. Andrews, Stock, and Sun (2019) for a comparison between AR approach and tF Procedure. tF Procedure: Adjusts the t-ratio based on the first-stage F-statistic. Rather than a fixed pretesting threshold, it applies an adjustment factor to 2SLS standard errors. Adjustment factors are provided for 95% and 99% confidence levels. Advantages of the tF Procedure: Smooth Adjustment: Gives usable finite confidence intervals for smaller F statistic values. 95% confidence is applicable for \\(F &gt; 3.84\\), aligning with AR’s bounded 95% confidence intervals. Clear Confidence Levels: These levels incorporate effects of basing inference on the first-stage F. Mirrors AR or other zero distortion procedures. Robustness: Robust against common error structures (e.g., heteroskedasticity or clustering and/or autocorrelated errors). No further adjustments are necessary as long as robust variance estimators are consistently used (same robust variance estimator used for the 1st-stage as for the IV estimate). Comparison to AR: Surprisingly, with \\(F &gt; 3.84\\), AR’s expected interval length is infinite, while tF’s is finite (i.e., better). Applicability: The tF adjustment can re-evaluate published studies if the first-stage F-statistic is available. Original data access is not needed. Impacts in Applied Research: Lee et al. (2022) examined recent single-instrument specification studies from the American Economic Review (AER). Observations: For at least 25% of the studied specifications, using tF increased confidence interval lengths by: 49% (5% significance level). 136% (1% significance level). For specifications with \\(F &gt; 10\\) and \\(t &gt; 1.96\\), about 25% became statistically insignificant at the 5% level when adjusted using tF. Conclusion: tF adjustments could greatly influence inferences in research employing t-ratio inferences. Notation \\(Y = X \\beta + W \\gamma + u\\) \\(X = Z \\pi + W \\xi + \\nu\\) where \\(W\\): Additional covariates, possibly including an intercept term. \\(X\\): variable of interest \\(Z\\): instruments Key Statistics: \\(t\\)-ratio for the instrumental variable estimator: \\(\\hat{t} = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{V}_N (\\hat{\\beta})}}\\) \\(t\\)-ratio for the first-stage coefficient: \\(\\hat{f} = \\frac{\\hat{\\pi}}{\\sqrt{\\hat{V}_N (\\hat{\\pi})}}\\) \\(\\hat{F} = \\hat{f}^2\\) where \\(\\hat{\\beta}\\): Instrumental variable estimator. \\(\\hat{V}_N (\\hat{\\beta})\\): Estimated variance of \\(\\hat{\\beta}\\), possibly robust to deal with non-iid errors. \\(\\hat{t}\\): \\(t\\)-ratio under the null hypothesis. \\(\\hat{f}\\): \\(t\\)-ratio under the null hypothesis of \\(\\pi=0\\). Traditional \\(t\\) Inference: In large samples, \\(\\hat{t}^2 \\to^d t^2\\) Standard normal critical values are \\(\\pm 1.96\\) for 5% significance level testing. Distortions in Inference in the case of IV: Use of a standard normal can lead to distorted inferences even in large samples. Despite large samples, t-distribution might not be normal. But magnitude of this distortion can be quantified. J. H. Stock and Yogo (2005) provides a formula for Wald test statistics using 2SLS. \\(t^2\\) formula allows for quantification of inference distortions. In the just-identified case with one endogenous regressor \\(t^2 = f + t_{AR} + \\rho f t_{AR}\\) (J. H. Stock and Yogo 2005) \\(\\hat{f} \\to^d f\\) and \\(\\bar{f} = \\frac{\\pi}{\\sqrt{\\frac{1}{N} AV(\\hat{\\pi})}}\\) and \\(AV(\\hat{\\pi})\\) is the asymptotic variance of \\(\\hat{\\pi}\\) \\(t_{AR}\\) is a standard normal with \\(AR = t^2_{AR}\\) \\(\\rho\\) (degree of endogeneity) is the correlation of \\(Zu\\) and \\(Z \\nu\\) (when data are homoskedastic, \\(\\rho\\) is the correlation between \\(u\\) and \\(\\nu\\)) Implications of \\(t^2\\) formula: Varies rejection rates depending on \\(\\rho\\) value. \\(\\rho \\in (0,0.5]\\) (low) the t-ratio rejects at a probability below the nominal \\(0.05\\) rate \\(\\rho = 0.8\\) (high) the rejection rate can be \\(0.13\\) In short, incorrect test size when relying solely on \\(t^2\\) (based on traditional econometric understanding) To correct for this, one can Estimate the usually 2SLS standard errors Multiply the SE by the adjustment factor based on the observed first-stage \\(\\hat{F}\\) stat One can go back to the traditional hypothesis by using either the t-ratio of confidence intervals Lee et al. (2022) call this adjusted SE as “0.05 tF SE”. library(ivDiag) g &lt;- ivDiag::ivDiag( data = rueda, Y = &quot;e_vote_buying&quot;, D = &quot;lm_pob_mesa&quot;, Z = &quot;lz_pob_mesa_f&quot;, controls = c(&quot;lpopulation&quot;, &quot;lpotencial&quot;), cl = &quot;muni_code&quot;, cores = 4, bootstrap = FALSE ) g$tF #&gt; F cF Coef SE t CI2.5% CI97.5% p-value #&gt; 8598.3264 1.9600 -0.9835 0.1540 -6.3872 -1.2853 -0.6817 0.0000 # example in fixest package library(fixest) library(tidyverse) base = iris names(base) = c(&quot;y&quot;, &quot;x1&quot;, &quot;x_endo_1&quot;, &quot;x_inst_1&quot;, &quot;fe&quot;) set.seed(2) base$x_inst_2 = 0.2 * base$y + 0.2 * base$x_endo_1 + rnorm(150, sd = 0.5) base$x_endo_2 = 0.2 * base$y - 0.2 * base$x_inst_1 + rnorm(150, sd = 0.5) est_iv = feols(y ~ x1 | x_endo_1 + x_endo_2 ~ x_inst_1 + x_inst_2, base) est_iv #&gt; TSLS estimation - Dep. Var.: y #&gt; Endo. : x_endo_1, x_endo_2 #&gt; Instr. : x_inst_1, x_inst_2 #&gt; Second stage: Dep. Var.: y #&gt; Observations: 150 #&gt; Standard-errors: IID #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.831380 0.411435 4.45121 1.6844e-05 *** #&gt; fit_x_endo_1 0.444982 0.022086 20.14744 &lt; 2.2e-16 *** #&gt; fit_x_endo_2 0.639916 0.307376 2.08186 3.9100e-02 * #&gt; x1 0.565095 0.084715 6.67051 4.9180e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; RMSE: 0.398842 Adj. R2: 0.761653 #&gt; F-test (1st stage), x_endo_1: stat = 903.2 , p &lt; 2.2e-16 , on 2 and 146 DoF. #&gt; F-test (1st stage), x_endo_2: stat = 3.25828, p = 0.041268, on 2 and 146 DoF. #&gt; Wu-Hausman: stat = 6.79183, p = 0.001518, on 2 and 144 DoF. res_est_iv &lt;- est_iv$coeftable |&gt; rownames_to_column() coef_of_interest &lt;- res_est_iv[res_est_iv$rowname == &quot;fit_x_endo_1&quot;, &quot;Estimate&quot;] se_of_interest &lt;- res_est_iv[res_est_iv$rowname == &quot;fit_x_endo_1&quot;, &quot;Std. Error&quot;] fstat_1st &lt;- fitstat(est_iv, type = &quot;ivf1&quot;)[[1]]$stat # To get the correct SE based on 1st-stage F-stat (This result is similar without adjustment since F is large) # the results are the new CIS and p.value tF(coef = coef_of_interest, se = se_of_interest, Fstat = fstat_1st) |&gt; causalverse::nice_tab(5) #&gt; F cF Coef SE t CI2.5. CI97.5. p.value #&gt; 1 903.1628 1.96 0.44498 0.02209 20.14744 0.40169 0.48827 0 # We can try to see a different 1st-stage F-stat and how it changes the results tF(coef = coef_of_interest, se = se_of_interest, Fstat = 2) |&gt; causalverse::nice_tab(5) #&gt; F cF Coef SE t CI2.5. CI97.5. p.value #&gt; 1 2 18.66 0.44498 0.02209 20.14744 0.03285 0.85711 0.03432 30.3.3 AK approach (J. Angrist and Kolesár 2023) References "],["testing-assumptions.html", "30.4 Testing Assumptions", " 30.4 Testing Assumptions \\[ Y = \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon \\] where \\(X_1\\) are exogenous variables \\(X_2\\) are endogenous variables \\(Z\\) are instrumental variables If \\(Z\\) satisfies the relevance condition, it means \\(Cov(Z, X_2) \\neq 0\\) This is important because we need this to be able to estimate \\(\\beta_2\\) where \\[ \\beta_2 = \\frac{Cov(Z,Y)}{Cov(Z, X_2)} \\] If \\(Z\\) satisfies the exogeneity condition, \\(E[Z\\epsilon]=0\\), this can achieve by \\(Z\\) having no direct effect on \\(Y\\) except through \\(X_2\\) In the presence of omitted variable, \\(Z\\) is uncorrelated with this variable. If we just want to know the effect of \\(Z\\) on \\(Y\\) (reduced form) where the coefficient of \\(Z\\) is \\[ \\rho = \\frac{Cov(Y, Z)}{Var(Z)} \\] and this effect is only through \\(X_2\\) (by the exclusion restriction assumption). We can also consistently estimate the effect of \\(Z\\) on \\(X\\) (first stage) where the the coefficient of \\(X_2\\) is \\[ \\pi = \\frac{Cov(X_2, Z)}{Var(Z)} \\] and the IV estimate is \\[ \\beta_2 = \\frac{Cov(Y,Z)}{Cov(X_2, Z)} = \\frac{\\rho}{\\pi} \\] 30.4.1 Relevance Assumption Weak instruments: can explain little variation in the endogenous regressor Coefficient estimate of the endogenous variable will be inaccurate. For cases where weak instruments are unavoidable, M. J. Moreira (2003) proposes the conditional likelihood ratio test for robust inference. This test is considered approximately optimal for weak instrument scenarios (D. W. Andrews, Moreira, and Stock 2008; D. W. Andrews and Marmer 2008). Rule of thumb: Compute F-statistic in the first-stage, where it should be greater than 10. But this is discouraged now by Lee et al. (2022) use linearHypothesis() to see only instrument coefficients. First-Stage F-Test In the context of a two-stage least squares (2SLS) setup where you are estimating the equation: \\[ Y = X \\beta + \\epsilon \\] and \\(X\\) is endogenous, you typically estimate a first-stage regression of: \\[ X = Z \\pi + u \\] where Z is the instrument. The first-stage F-test evaluates the joint significance of the instruments in this first stage: \\[ F = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/ (n - k - 1)} \\] where: \\(SSR_r\\) is the sum of squared residuals from the restricted model (no instruments, just the constant). \\(SSR_{ur}\\) is the sum of squared residuals from the unrestricted model (with instruments). \\(q\\) is the number of instruments excluded from the main equation. \\(n\\) is the number of observations. \\(k\\) is the number of explanatory variables excluding the instruments. Cragg-Donald Test The Cragg-Donald statistic is essentially the same as the Wald statistic of the joint significance of the instruments in the first stage, and it’s used specifically when you have multiple endogenous regressors. It’s calculated as: \\[ CD = n \\times (R_{ur}^2 - R_r^2) \\] where: \\(R_{ur}^2\\) and \\(R_r^2\\) are the R-squared values from the unrestricted and restricted models respectively. \\(n\\) is the number of observations. For one endogenous variable, the Cragg-Donald test results should align closely with those from Stock and Yogo. The Anderson canonical correlation test, a likelihood ratio test, also works under similar conditions, contrasting with Cragg-Donald’s Wald statistic approach. Both are valid with one endogenous variable and at least one instrument. Stock-Yogo Weak IV Test The Stock-Yogo test does not directly compute a statistic like the F-test or Cragg-Donald, but rather uses pre-computed critical values to assess the strength of instruments. It often uses the eigenvalues derived from the concentration matrix: \\[ S = \\frac{1}{n} (Z&#39; X) (X&#39;Z) \\] where \\(Z\\) is the matrix of instruments and \\(X\\) is the matrix of endogenous regressors. Stock and Yogo provide critical values for different scenarios (bias, size distortion) for a given number of instruments and endogenous regressors, based on the smallest eigenvalue of \\(S\\). The test compares these eigenvalues against critical values that correspond to thresholds of permissible bias or size distortion in a 2SLS estimator. Critical Values and Test Conditions: The critical values derived by Stock and Yogo depend on the level of acceptable bias, the number of endogenous regressors, and the number of instruments. For example, with a 5% maximum acceptable bias, one endogenous variable, and three instruments, the critical value for a sufficient first stage F-statistic is 13.91. Note that this framework requires at least two overidentifying degree of freedom. Comparison Test Description Focus Usage First-Stage F-Test Evaluates the joint significance of instruments in the first stage. Predictive power of instruments for the endogenous variable. Simplest and most direct test, widely used especially with a single endogenous variable. Rule of thumb: F &lt; 10 suggests weak instruments. Cragg-Donald Test Wald statistic for joint significance of instruments. Joint strength of multiple instruments with multiple endogenous variables. More appropriate in complex IV setups with multiple endogenous variables. Compares statistic against critical values for assessing instrument strength. Stock-Yogo Weak IV Test Compares test statistic to pre-determined critical values. Minimizing size distortions and bias from weak instruments. Theoretical evaluation of instrument strength, ensuring the reliability of 2SLS estimates against specific thresholds of bias or size distortion. All the mentioned tests (Stock Yogo, Cragg-Donald, Anderson canonical correlation test) assume errors are independently and identically distributed. If this assumption is violated, the Kleinbergen-Paap test is robust against violations of the iid assumption and can be applied even with a single endogenous variable and instrument, provided the model is properly identified (Baum and Schaffer 2021). 30.4.1.1 Cragg-Donald (Cragg and Donald 1993) Similar to the first-stage F-statistic library(cragg) library(AER) # for dataaset data(&quot;WeakInstrument&quot;) cragg_donald( # control variables X = ~ 1, # endogeneous variables D = ~ x, # instrument variables Z = ~ z, data = WeakInstrument ) #&gt; Cragg-Donald test for weak instruments: #&gt; #&gt; Data: WeakInstrument #&gt; Controls: ~1 #&gt; Treatments: ~x #&gt; Instruments: ~z #&gt; #&gt; Cragg-Donald Statistic: 4.566136 #&gt; Df: 198 Large CD statistic implies that the instruments are strong, but not in our case here. But to judge it against some critical value, we have to look at Stock-Yogo 30.4.1.2 Stock-Yogo J. H. Stock and Yogo (2002) set the critical values such that the bias is less then 10% (default) \\(H_0:\\) Instruments are weak \\(H_1:\\) Instruments are not weak library(cragg) library(AER) # for dataaset data(&quot;WeakInstrument&quot;) stock_yogo_test( # control variables X = ~ 1, # endogeneous variables D = ~ x, # instrument variables Z = ~ z, size_bias = &quot;bias&quot;, data = WeakInstrument ) The CD statistic should be bigger than the set critical value to be considered strong instruments. 30.4.2 Exogeneity Assumption The local average treatment effect (LATE) is defined as: \\[ \\text{LATE} = \\frac{\\text{reduced form}}{\\text{first stage}} = \\frac{\\rho}{\\phi} \\] This implies that the reduced form (\\(\\rho\\)) is the product of the first stage (\\(\\phi\\)) and LATE: \\[ \\rho = \\phi \\times \\text{LATE} \\] Thus, if the first stage (\\(\\phi\\)) is 0, the reduced form (\\(\\rho\\)) should also be 0. # Load necessary libraries library(shiny) library(AER) # for ivreg library(ggplot2) # for visualization library(dplyr) # for data manipulation # Function to simulate the dataset simulate_iv_data &lt;- function(n, beta, phi, direct_effect) { Z &lt;- rnorm(n) epsilon_x &lt;- rnorm(n) epsilon_y &lt;- rnorm(n) X &lt;- phi * Z + epsilon_x Y &lt;- beta * X + direct_effect * Z + epsilon_y data &lt;- data.frame(Y = Y, X = X, Z = Z) return(data) } # Function to run the simulations and calculate the effects run_simulation &lt;- function(n, beta, phi, direct_effect) { # Simulate the data simulated_data &lt;- simulate_iv_data(n, beta, phi, direct_effect) # Estimate first-stage effect (phi) first_stage &lt;- lm(X ~ Z, data = simulated_data) phi &lt;- coef(first_stage)[&quot;Z&quot;] phi_ci &lt;- confint(first_stage)[&quot;Z&quot;, ] # Estimate reduced-form effect (rho) reduced_form &lt;- lm(Y ~ Z, data = simulated_data) rho &lt;- coef(reduced_form)[&quot;Z&quot;] rho_ci &lt;- confint(reduced_form)[&quot;Z&quot;, ] # Estimate LATE using IV regression iv_model &lt;- ivreg(Y ~ X | Z, data = simulated_data) iv_late &lt;- coef(iv_model)[&quot;X&quot;] iv_late_ci &lt;- confint(iv_model)[&quot;X&quot;, ] # Calculate LATE as the ratio of reduced-form and first-stage coefficients calculated_late &lt;- rho / phi calculated_late_se &lt;- sqrt( (rho_ci[2] - rho)^2 / phi^2 + (rho * (phi_ci[2] - phi) / phi^2)^2 ) calculated_late_ci &lt;- c(calculated_late - 1.96 * calculated_late_se, calculated_late + 1.96 * calculated_late_se) # Return a list of results list(phi = phi, phi_ci = phi_ci, rho = rho, rho_ci = rho_ci, direct_effect = direct_effect, direct_effect_ci = c(direct_effect, direct_effect), # Placeholder for direct effect CI iv_late = iv_late, iv_late_ci = iv_late_ci, calculated_late = calculated_late, calculated_late_ci = calculated_late_ci, true_effect = beta, true_effect_ci = c(beta, beta)) # Placeholder for true effect CI } # Define UI for the sliders ui &lt;- fluidPage( titlePanel(&quot;IV Model Simulation&quot;), sidebarLayout( sidebarPanel( sliderInput(&quot;beta&quot;, &quot;True Effect of X on Y (beta):&quot;, min = 0, max = 1.0, value = 0.5, step = 0.1), sliderInput(&quot;phi&quot;, &quot;First Stage Effect (phi):&quot;, min = 0, max = 1.0, value = 0.7, step = 0.1), sliderInput(&quot;direct_effect&quot;, &quot;Direct Effect of Z on Y:&quot;, min = -0.5, max = 0.5, value = 0, step = 0.1) ), mainPanel( plotOutput(&quot;dotPlot&quot;) ) ) ) # Define server logic to run the simulation and generate the plot server &lt;- function(input, output) { output$dotPlot &lt;- renderPlot({ # Run simulation results &lt;- run_simulation(n = 1000, beta = input$beta, phi = input$phi, direct_effect = input$direct_effect) # Prepare data for plotting plot_data &lt;- data.frame( Effect = c(&quot;First Stage (phi)&quot;, &quot;Reduced Form (rho)&quot;, &quot;Direct Effect&quot;, &quot;LATE (Ratio)&quot;, &quot;LATE (IV)&quot;, &quot;True Effect&quot;), Value = c(results$phi, results$rho, results$direct_effect, results$calculated_late, results$iv_late, results$true_effect), CI_Lower = c(results$phi_ci[1], results$rho_ci[1], results$direct_effect_ci[1], results$calculated_late_ci[1], results$iv_late_ci[1], results$true_effect_ci[1]), CI_Upper = c(results$phi_ci[2], results$rho_ci[2], results$direct_effect_ci[2], results$calculated_late_ci[2], results$iv_late_ci[2], results$true_effect_ci[2]) ) # Create dot plot with confidence intervals ggplot(plot_data, aes(x = Effect, y = Value)) + geom_point(size = 3) + geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2) + labs(title = &quot;IV Model Effects&quot;, y = &quot;Coefficient Value&quot;) + coord_cartesian(ylim = c(-1, 1)) + # Limits the y-axis to -1 to 1 but allows CI beyond theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) }) } # Run the application shinyApp(ui = ui, server = server) A statistically significant reduced form estimate without a corresponding first stage indicates an issue, suggesting an alternative channel linking instruments to outcomes or a direct effect of the IV on the outcome. No Direct Effect: When the direct effect is 0 and the first stage is 0, the reduced form is 0. Note: Extremely rare cases with multiple additional paths that perfectly cancel each other out can also produce this result, but testing for all possible paths is impractical. With Direct Effect: When there is a direct effect of the IV on the outcome, the reduced form can be significantly different from 0, even if the first stage is 0. This violates the exogeneity assumption, as the IV should only affect the outcome through the treatment variable. To test the validity of the exogeneity assumption, we can use a sanity test: Identify groups for which the effects of instruments on the treatment variable are small and not significantly different from 0. The reduced form estimate for these groups should also be 0. These “no-first-stage samples” provide evidence of whether the exogeneity assumption is violated. 30.4.2.1 Overid Tests Wald test and Hausman test for exogeneity of \\(X\\) assuming \\(Z\\) is exogenous People might prefer Wald test over Hausman test. Sargan (for 2SLS) is a simpler version of Hansen’s J test (for IV-GMM) Modified J test (i.e., Regularized jacknife IV): can handle weak instruments and small sample size (Carrasco and Doukali 2022) (also proposed a regularized F-test to test relevance assumption that is robust to heteroskedasticity). New advances: endogeneity robust inference in finite sample and sensitivity analysis of inference (Kiviet 2020) These tests that can provide evidence fo the validity of the over-identifying restrictions is not sufficient or necessary for the validity of the moment conditions (i.e., this assumption cannot be tested). (Deaton 2010; Parente and Silva 2012) The over-identifying restriction can still be valid even when the instruments are correlated with the error terms, but then in this case, what you’re estimating is no longer your parameters of interest. Rejection of the over-identifying restrictions can also be the result of parameter heterogeneity (J. D. Angrist, Graddy, and Imbens 2000) Why overid tests hold no value/info? Overidentifying restrictions are valid irrespective of the instruments’ validity Whenever instruments have the same motivation and are on the same scale, the estimated parameter of interests will be very close (Parente and Silva 2012, 316) Overidentifying restriction are invalid when each instrument is valid When the effect of your parameter of interest is heterogeneous (e.g., you have two groups with two different true effects), your first instrument can be correlated with your variable of interest only for the first group and your second interments can be correlated with your variable of interest only for the second group (i.e., each instrument is valid), and if you use each instrument, you can still identify the parameter of interest. However, if you use both of them, what you estimate is a mixture of the two groups. Hence, the overidentifying restriction will be invalid (because no single parameters can make the errors of the model orthogonal to both instruments). The result may seem confusing at first because if each subset of overidentifying restrictions is valid, the full set should also be valid. However, this interpretation is flawed because the residual’s orthogonality to the instruments depends on the chosen set of instruments, and therefore the set of restrictions tested when using two sets of instruments together is not the same as the union of the sets of restrictions tested when using each set of instruments separately (Parente and Silva 2012, 316) These tests (of overidentifying restrictions) should be used to check whether different instruments identify the same parameters of interest, not to check their validity (J. A. Hausman 1983; Parente and Silva 2012) 30.4.2.1.1 Wald Test Assuming that \\(Z\\) is exogenous (a valid instrument), we want to know whether \\(X_2\\) is exogenous 1st stage: \\[ X_2 = \\hat{\\alpha} Z + \\hat{\\epsilon} \\] 2nd stage: \\[ Y = \\delta_0 X_1 + \\delta_1 X_2 + \\delta_2 \\hat{\\epsilon} + u \\] where \\(\\hat{\\epsilon}\\) is the residuals from the 1st stage The Wald test of exogeneity assumes \\[ H_0: \\delta_2 = 0 \\\\ H_1: \\delta_2 \\neq 0 \\] If you have more than one endogenous variable with more than one instrument, \\(\\delta_2\\) is a vector of all residuals from all the first-stage equations. And the null hypothesis is that they are jointly equal 0. If you reject this hypothesis, it means that \\(X_2\\) is not endogenous. Hence, for this test, we do not want to reject the null hypothesis. If the test is not sacrificially significant, we might just don’t have enough information to reject the null. When you have a valid instrument \\(Z\\), whether \\(X_2\\) is endogenous or exogenous, your coefficient estimates of \\(X_2\\) should still be consistent. But if \\(X_2\\) is exogenous, then 2SLS will be inefficient (i.e., larger standard errors). Intuition: \\(\\hat{\\epsilon}\\) is the supposed endogenous part of \\(X_2\\), When we regress \\(Y\\) on \\(\\hat{\\epsilon}\\) and observe that its coefficient is not different from 0. It means that the exogenous part of \\(X_2\\) can explain well the impact on \\(Y\\), and there is no endogenous part. 30.4.2.1.2 Hausman’s Test Similar to Wald Test and identical to Wald Test when we have homoskedasticity (i.e., homogeneity of variances). Because of this assumption, it’s used less often than Wald Test 30.4.2.1.3 Hansen’s J (L. P. Hansen 1982) J-test (over-identifying restrictions test): test whether additional instruments are exogenous Can only be applied in cases where you have more instruments than endogenous variables \\(dim(Z) &gt; dim(X_2)\\) Assume at least one instrument within \\(Z\\) is exogenous Procedure IV-GMM: Obtain the residuals of the 2SLS estimation Regress the residuals on all instruments and exogenous variables. Test the joint hypothesis that all coefficients of the residuals across instruments are 0 (i.e., this is true when instruments are exogenous). Compute \\(J = mF\\) where \\(m\\) is the number of instruments, and \\(F\\) is your equation \\(F\\) statistic (can you use linearHypothesis() again). If your exogeneity assumption is true, then \\(J \\sim \\chi^2_{m-k}\\) where \\(k\\) is the number of endogenous variables. If you reject this hypothesis, it can be that The first sets of instruments are invalid The second sets of instruments are invalid Both sets of instruments are invalid Note: This test is only true when your residuals are homoskedastic. For a heteroskedasticity-robust \\(J\\)-statistic, see (Carrasco and Doukali 2022; H. Li et al. 2022) 30.4.2.1.4 Sargan Test (Sargan 1958) Similar to Hansen’s J, but it assumes homoskedasticity Have to be careful when sample is not collected exogenously. As such, when you have choice-based sampling design, the sampling weights have to be considered to have consistent estimates. However, even if we apply sampling weights, the tests are not suitable because the iid assumption off errors are already violated. Hence, the test is invalid in this case (Pitt 2011). If one has heteroskedasticity in its design, the Sargan test is invalid (Pitt 2011}) References "],["negative-r2.html", "30.5 Negative \\(R^2\\)", " 30.5 Negative \\(R^2\\) It’s okay to have negative \\(R^2\\) in the 2nd stage. We care more about consistent coefficient estimates. \\(R^2\\) has no statistical meaning in instrumental variable regression or 2 or 3SLS \\[ R^2 = \\frac{MSS}{TSS} \\] where MSS = model sum of squares (TSS- RSS) TSS = total sum of squares (\\(\\sum(y - \\bar{y})^2\\)) RSS = residual sum of squares (\\(\\sum (y - Xb)^2\\)) If \\(TSS &gt; RSS\\), then we have negative RSS and negative \\(R^2\\). Since the predicted values of the endogenous variables are different from the endogenous variables themselves, the error that is used to calculate RSS can be different from the error in the second stage, and RSS in the second stage can be less than TSS. For more information, see here. "],["treatment-intensity.html", "30.6 Treatment Intensity", " 30.6 Treatment Intensity Two-Stage Least Squares (TSLS) can be used to estimate the average causal effect of variable treatment intensity, and it “identifies a weighted average of per-unit treatment effects along the length of a causal response function” (J. D. Angrist and Imbens 1995, 431). For example Drug dosage Hours of exam prep on score (Powers and Swinton 1984) Cigarette smoking on birth weights (Permutt and Hebel 1989) Years of education Class size on test score (J. D. Angrist and Lavy 1999) Sibship size on earning (Lavy, Angrist, and Schlosser 2006) Social Media Adoption The average causal effect here refers to the conditional expectation of the difference in outcomes between the treated and what would have happened in the counterfactual world. Notes: We do not need a linearity assumption of the relationships between the dependent variable, treatment intensities, and instruments. Example In their original paper, J. D. Angrist and Imbens (1995) take the example of schooling effect on earnings where they have quarters of birth as the instrumental variable. For each additional year of schooling, there can be an increase in earnings, and each additional year can be heterogeneous (both in the sense that grade 9th to grade 10th is qualitatively different and one can change to a different school). \\[ Y = \\gamma_0 + \\gamma_1 X_1 + \\rho S + \\epsilon \\] where \\(S\\) is years of schooling (i.e., endogenous regressor) \\(\\rho\\) is the return to a year of schooling \\(X_1\\) is a matrix of exogenous covariates Schooling can also be related to the exogenous variable \\(X_1\\) \\[ S = \\delta_0 + X_1 \\delta_1 + X_2 \\delta_2 + \\eta \\] where \\(X_2\\) is an exogenous instrument \\(\\delta_2\\) is the coefficient of the instrument by using only the fitted value in the second, the TSLS can give a consistent estimate of the effect of schooling on earning \\[ Y = \\gamma_0 + X_1 \\gamma-1 + \\rho \\hat{S} + \\nu \\] To give \\(\\rho\\) a causal interpretation, We first have to have the SUTVA (stable unit treatment value assumption), where the potential outcomes of the same person with different years of schooling are independent. When \\(\\rho\\) has a probability limit equal to a weighted average of \\(E[Y_j - Y_{j-1}] \\forall j\\) Even though the first bullet point is not trivial, most of the time we don’t have to defend much about it in a research article, the second bullet point is the harder one to argue and only apply to certain cases. References "],["control-function.html", "30.7 Control Function", " 30.7 Control Function Also known as two-stage residual inclusion Resources: Binary outcome and binary endogenous variable application (E. Tchetgen Tchetgen 2014) In rare events: we use a logistic model in the 2nd stage In non-rare events: use risk ratio regression in the 2nd stage Application in marketing for consumer choice model (Petrin and Train 2010) Notes This approach is better suited for models with nonadditive errors (e.g., discrete choice models), or binary endogenous model, binary response variable, etc. \\[ Y = g(X) + U \\\\ X = \\pi(Z) + V \\\\ E(U |Z,V) = E(U|V) \\\\ E(V|Z) = 0 \\] Under control function approach, \\[ E(Y|Z,V) = g(X) + E(U|Z,V) \\\\ = g(X) + E(U|V) \\\\ = g(X) + h(V) \\] where \\(h(V)\\) is the control function that models the endogeneity Linear in parameters Linear Endogenous Variables: The control function function approach is identical to the usual 2SLS estimator Nonlinear Endogenous Variables: The control function is different from the 2SLS estimator Nonlinear in parameters: The CF function is superior than the 2SLS estimator 30.7.1 Simulation library(fixest) library(tidyverse) library(modelsummary) # Set the seed for reproducibility set.seed(123) n = 10000 # Generate the exogenous variable from a normal distribution exogenous &lt;- rnorm(n, mean = 5, sd = 1) # Generate the omitted variable as a function of the exogenous variable omitted &lt;- rnorm(n, mean = 2, sd = 1) # Generate the endogenous variable as a function of the omitted variable and the exogenous variable endogenous &lt;- 5 * omitted + 2 * exogenous + rnorm(n, mean = 0, sd = 1) # nonlinear endogenous variable endogenous_nonlinear &lt;- 5 * omitted^2 + 2 * exogenous + rnorm(100, mean = 0, sd = 1) unrelated &lt;- rexp(n, rate = 1) # Generate the response variable as a function of the endogenous variable and the omitted variable response &lt;- 4 + 3 * endogenous + 6 * omitted + rnorm(n, mean = 0, sd = 1) response_nonlinear &lt;- 4 + 3 * endogenous_nonlinear + 6 * omitted + rnorm(n, mean = 0, sd = 1) response_nonlinear_para &lt;- 4 + 3 * endogenous ^ 2 + 6 * omitted + rnorm(n, mean = 0, sd = 1) # Combine the variables into a data frame my_data &lt;- data.frame( exogenous, omitted, endogenous, response, unrelated, response, response_nonlinear, response_nonlinear_para ) # View the first few rows of the data frame # head(my_data) wo_omitted &lt;- feols(response ~ endogenous + sw0(unrelated), data = my_data) w_omitted &lt;- feols(response ~ endogenous + omitted + unrelated, data = my_data) # ivreg::ivreg(response ~ endogenous + unrelated | exogenous, data = my_data) iv &lt;- feols(response ~ 1 + sw0(unrelated) | endogenous ~ exogenous, data = my_data) etable( wo_omitted, w_omitted, iv, digits = 2 # vcov = list(&quot;each&quot;, &quot;iid&quot;, &quot;hetero&quot;) ) #&gt; wo_omitted.1 wo_omitted.2 w_omitted iv.1 #&gt; Dependent Var.: response response response response #&gt; #&gt; Constant -3.9*** (0.10) -4.0*** (0.10) 4.0*** (0.05) 15.7*** (0.59) #&gt; endogenous 4.0*** (0.005) 4.0*** (0.005) 3.0*** (0.004) 3.0*** (0.03) #&gt; unrelated 0.03 (0.03) 0.002 (0.010) #&gt; omitted 6.0*** (0.02) #&gt; _______________ ______________ ______________ ______________ ______________ #&gt; S.E. type IID IID IID IID #&gt; Observations 10,000 10,000 10,000 10,000 #&gt; R2 0.98566 0.98567 0.99803 0.92608 #&gt; Adj. R2 0.98566 0.98566 0.99803 0.92607 #&gt; #&gt; iv.2 #&gt; Dependent Var.: response #&gt; #&gt; Constant 15.6*** (0.59) #&gt; endogenous 3.0*** (0.03) #&gt; unrelated 0.10. (0.06) #&gt; omitted #&gt; _______________ ______________ #&gt; S.E. type IID #&gt; Observations 10,000 #&gt; R2 0.92610 #&gt; Adj. R2 0.92608 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Linear in parameter and linear in endogenous variable # manual # 2SLS first_stage = lm(endogenous ~ exogenous, data = my_data) new_data = cbind(my_data, new_endogenous = predict(first_stage, my_data)) second_stage = lm(response ~ new_endogenous, data = new_data) summary(second_stage) #&gt; #&gt; Call: #&gt; lm(formula = response ~ new_endogenous, data = new_data) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -77.683 -14.374 -0.107 14.289 78.274 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 15.6743 2.0819 7.529 5.57e-14 *** #&gt; new_endogenous 3.0142 0.1039 29.025 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 21.26 on 9998 degrees of freedom #&gt; Multiple R-squared: 0.07771, Adjusted R-squared: 0.07762 #&gt; F-statistic: 842.4 on 1 and 9998 DF, p-value: &lt; 2.2e-16 new_data_cf = cbind(my_data, residual = resid(first_stage)) second_stage_cf = lm(response ~ endogenous + residual, data = new_data_cf) summary(second_stage_cf) #&gt; #&gt; Call: #&gt; lm(formula = response ~ endogenous + residual, data = new_data_cf) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.360 -1.016 0.003 1.023 5.201 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 15.674265 0.149350 105.0 &lt;2e-16 *** #&gt; endogenous 3.014202 0.007450 404.6 &lt;2e-16 *** #&gt; residual 1.140920 0.008027 142.1 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.525 on 9997 degrees of freedom #&gt; Multiple R-squared: 0.9953, Adjusted R-squared: 0.9953 #&gt; F-statistic: 1.048e+06 on 2 and 9997 DF, p-value: &lt; 2.2e-16 modelsummary(list(second_stage, second_stage_cf))  (1)   (2) (Intercept) 15.674 15.674 (2.082) (0.149) new_endogenous 3.014 (0.104) endogenous 3.014 (0.007) residual 1.141 (0.008) Num.Obs. 10000 10000 R2 0.078 0.995 R2 Adj. 0.078 0.995 AIC 89520.9 36826.8 BIC 89542.5 36855.6 Log.Lik. −44757.438 −18409.377 F 842.424 1048263.304 RMSE 21.26 1.53 Nonlinear in endogenous variable # 2SLS first_stage = lm(endogenous_nonlinear ~ exogenous, data = my_data) new_data = cbind(my_data, new_endogenous_nonlinear = predict(first_stage, my_data)) second_stage = lm(response_nonlinear ~ new_endogenous_nonlinear, data = new_data) summary(second_stage) #&gt; #&gt; Call: #&gt; lm(formula = response_nonlinear ~ new_endogenous_nonlinear, data = new_data) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -94.43 -52.10 -15.29 36.50 446.08 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 15.3390 11.8175 1.298 0.194 #&gt; new_endogenous_nonlinear 3.0174 0.3376 8.938 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 69.51 on 9998 degrees of freedom #&gt; Multiple R-squared: 0.007927, Adjusted R-squared: 0.007828 #&gt; F-statistic: 79.89 on 1 and 9998 DF, p-value: &lt; 2.2e-16 new_data_cf = cbind(my_data, residual = resid(first_stage)) second_stage_cf = lm(response_nonlinear ~ endogenous_nonlinear + residual, data = new_data_cf) summary(second_stage_cf) #&gt; #&gt; Call: #&gt; lm(formula = response_nonlinear ~ endogenous_nonlinear + residual, #&gt; data = new_data_cf) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -17.5437 -0.8348 0.4614 1.4424 4.8154 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 15.33904 0.38459 39.88 &lt;2e-16 *** #&gt; endogenous_nonlinear 3.01737 0.01099 274.64 &lt;2e-16 *** #&gt; residual 0.24919 0.01104 22.58 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.262 on 9997 degrees of freedom #&gt; Multiple R-squared: 0.9989, Adjusted R-squared: 0.9989 #&gt; F-statistic: 4.753e+06 on 2 and 9997 DF, p-value: &lt; 2.2e-16 modelsummary(list(second_stage, second_stage_cf))  (1)   (2) (Intercept) 15.339 15.339 (11.817) (0.385) new_endogenous_nonlinear 3.017 (0.338) endogenous_nonlinear 3.017 (0.011) residual 0.249 (0.011) Num.Obs. 10000 10000 R2 0.008 0.999 R2 Adj. 0.008 0.999 AIC 113211.6 44709.6 BIC 113233.2 44738.4 Log.Lik. −56602.782 −22350.801 F 79.887 4752573.052 RMSE 69.50 2.26 Nonlinear in parameters # 2SLS first_stage = lm(endogenous ~ exogenous, data = my_data) new_data = cbind(my_data, new_endogenous = predict(first_stage, my_data)) second_stage = lm(response_nonlinear_para ~ new_endogenous, data = new_data) summary(second_stage) #&gt; #&gt; Call: #&gt; lm(formula = response_nonlinear_para ~ new_endogenous, data = new_data) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1536.5 -452.4 -80.7 368.4 3780.9 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1089.943 61.706 -17.66 &lt;2e-16 *** #&gt; new_endogenous 119.829 3.078 38.93 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 630.2 on 9998 degrees of freedom #&gt; Multiple R-squared: 0.1316, Adjusted R-squared: 0.1316 #&gt; F-statistic: 1516 on 1 and 9998 DF, p-value: &lt; 2.2e-16 new_data_cf = cbind(my_data, residual = resid(first_stage)) second_stage_cf = lm(response_nonlinear_para ~ endogenous_nonlinear + residual, data = new_data_cf) summary(second_stage_cf) #&gt; #&gt; Call: #&gt; lm(formula = response_nonlinear_para ~ endogenous_nonlinear + #&gt; residual, data = new_data_cf) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -961.00 -139.32 -16.02 135.57 1403.62 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 678.1593 9.9177 68.38 &lt;2e-16 *** #&gt; endogenous_nonlinear 17.7884 0.2759 64.46 &lt;2e-16 *** #&gt; residual 52.5016 1.1552 45.45 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 231.9 on 9997 degrees of freedom #&gt; Multiple R-squared: 0.8824, Adjusted R-squared: 0.8824 #&gt; F-statistic: 3.751e+04 on 2 and 9997 DF, p-value: &lt; 2.2e-16 modelsummary(list(second_stage, second_stage_cf))  (1)   (2) (Intercept) −1089.943 678.159 (61.706) (9.918) new_endogenous 119.829 (3.078) endogenous_nonlinear 17.788 (0.276) residual 52.502 (1.155) Num.Obs. 10000 10000 R2 0.132 0.882 R2 Adj. 0.132 0.882 AIC 157302.4 137311.3 BIC 157324.1 137340.1 Log.Lik. −78648.225 −68651.628 F 1515.642 37505.777 RMSE 630.10 231.88 References "],["new-advances.html", "30.8 New Advances", " 30.8 New Advances Combine ML and IV (Singh, Hosanagar, and Gandhi 2020) References "],["matching-methods.html", "Chapter 31 Matching Methods", " Chapter 31 Matching Methods Matching is a process that aims to close back doors - potential sources of bias - by constructing comparison groups that are similar according to a set of matching variables. This helps to ensure that any observed differences in outcomes between the treatment and comparison groups can be more confidently attributed to the treatment itself, rather than other factors that may differ between the groups. Matching and DiD can use pre-treatment outcomes to correct for selection bias. From real world data and simulation, (Chabé-Ferret 2015) found that matching generally underestimates the average causal effect and gets closer to the true effect with more number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is still consistent when implemented symmetrically (i.e., the same number of period before and after treatment). In cases where selection bias is asymmetric, the MC simulations show that Symmetric DID still performs better than Matching. Matching is useful, but not a general solution to causal problems (J. A. Smith and Todd 2005) Assumption: Observables can identify the selection into the treatment and control groups Identification: The exclusion restriction can be met conditional on the observables Motivation Effect of college quality on earnings They ultimately estimate the treatment effect on the treated of attending a top (high ACT) versus bottom (low ACT) quartile college Example Aaronson, Barrow, and Sander (2007) Do teachers qualifications (causally) affect student test scores? Step 1: \\[ Y_{ijt} = \\delta_0 + Y_{ij(t-1)} \\delta_1 + X_{it} \\delta_2 + Z_{jt} \\delta_3 + \\epsilon_{ijt} \\] There can always be another variable Any observable sorting is imperfect Step 2: \\[ Y_{ijst} = \\alpha_0 + Y_{ij(t-1)}\\alpha_1 + X_{it} \\alpha_2 + Z_{jt} \\alpha_3 + \\gamma_s + u_{isjt} \\] \\(\\delta_3 &gt;0\\) \\(\\delta_3 &gt; \\alpha_3\\) \\(\\gamma_s\\) = school fixed effect Sorting is less within school. Hence, we can introduce the school fixed effect Step 3: Find schools that look like they are putting students in class randomly (or as good as random) + we run step 2 \\[ \\begin{aligned} Y_{isjt} = Y_{isj(t-1)} \\lambda &amp;+ X_{it} \\alpha_1 +Z_{jt} \\alpha_{21} \\\\ &amp;+ (Z_{jt} \\times D_i)\\alpha_{22}+ \\gamma_5 + u_{isjt} \\end{aligned} \\] \\(D_{it}\\) is an element of \\(X_{it}\\) \\(Z_{it}\\) = teacher experience \\[ D_{it}= \\begin{cases} 1 &amp; \\text{ if high poverty} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] \\(H_0:\\) \\(\\alpha_{22} = 0\\) test for effect heterogeneity whether the effect of teacher experience (\\(Z_{jt}\\)) is different For low poverty is \\(\\alpha_{21}\\) For high poverty effect is \\(\\alpha_{21} + \\alpha_{22}\\) Matching is selection on observables and only works if you have good observables. Sufficient identification assumption under Selection on observable/ back-door criterion (based on Bernard Koch’s presentation) Strong conditional ignorability \\(Y(0),Y(1) \\perp T|X\\) No hidden confounders Overlap \\(\\forall x \\in X, t \\in \\{0, 1\\}: p (T = t | X = x&gt; 0\\) All treatments have non-zero probability of being observed SUTVA/ Consistency Treatment and outcomes of different subjects are independent Relative to OLS Matching makes the common support explicit (and changes default from “ignore” to “enforce”) Relaxes linear function form. Thus, less parametric. It also helps if you have high ratio of controls to treatments. For detail summary (Stuart 2010) Matching is defined as “any method that aims to equate (or”balance”) the distribution of covariates in the treated and control groups.” (Stuart 2010, 1) Equivalently, matching is a selection on observables identifications strategy. If you think your OLS estimate is biased, a matching estimate (almost surely) is too. Unconditionally, consider \\[ \\begin{aligned} E(Y_i^T | T) - E(Y_i^C |C) &amp;+ E(Y_i^C | T) - E(Y_i^C | T) \\\\ = E(Y_i^T - Y_i^C | T) &amp;+ [E(Y_i^C | T) - E(Y_i^C |C)] \\\\ = E(Y_i^T - Y_i^C | T) &amp;+ \\text{selection bias} \\end{aligned} \\] where \\(E(Y_i^T - Y_i^C | T)\\) is the causal inference that we want to know. Randomization eliminates the selection bias. If we don’t have randomization, then \\(E(Y_i^C | T) \\neq E(Y_i^C |C)\\) Matching tries to do selection on observables \\(E(Y_i^C | X, T) = E(Y_i^C|X, C)\\) Propensity Scores basically do \\(E(Y_i^C| P(X) , T) = E(Y_i^C | P(X), C)\\) Matching standard errors will exceed OLS standard errors The treatment should have larger predictive power than the control because you use treatment to pick control (not control to pick treatment). The average treatment effect (ATE) is \\[ \\frac{1}{N_T} \\sum_{i=1}^{N_T} (Y_i^T - \\frac{1}{N_{C_T}} \\sum_{i=1}^{N_{C_T}} Y_i^C) \\] Since there is no closed-form solution for the standard error of the average treatment effect, we have to use bootstrapping to get standard error. Professor Gary King advocates instead of using the word “matching”, we should use “pruning” (i.e., deleting observations). It is a preprocessing step where it prunes nonmatches to make control variables less important in your analysis. Without Matching Imbalance data leads to model dependence lead to a lot of researcher discretion leads to bias With Matching We have balance data which essentially erase human discretion Table @ref(tab:Gary King - International Methods Colloquium talk 2015) Balance Covariates Complete Randomization Fully Exact Observed On average Exact Unobserved On average On average Fully blocked is superior on imbalance model dependence power efficiency bias research costs robustness Matching is used when Outcomes are not available to select subjects for follow-up Outcomes are available to improve precision of the estimate (i.e., reduce bias) Hence, we can only observe one outcome of a unit (either treated or control), we can think of this problem as missing data as well. Thus, this section is closely related to Imputation (Missing Data) In observational studies, we cannot randomize the treatment effect. Subjects select their own treatments, which could introduce selection bias (i.e., systematic differences between group differences that confound the effects of response variable differences). Matching is used to reduce model dependence diagnose balance in the dataset Assumptions of matching: treatment assignment is independent of potential outcomes given the covariates \\(T \\perp (Y(0),Y(1))|X\\) known as ignorability, or ignorable, no hidden bias, or unconfounded. You typically satisfy this assumption when unobserved covariates correlated with observed covariates. But when unobserved covariates are unrelated to the observed covariates, you can use sensitivity analysis to check your result, or use “design sensitivity” (Heller, Rosenbaum, and Small 2009) positive probability of receiving treatment for all X \\(0 &lt; P(T=1|X)&lt;1 \\forall X\\) Stable Unit Treatment value Assumption (SUTVA) Outcomes of A are not affected by treatment of B. Very hard in cases where there is “spillover” effects (interactions between control and treatment). To combat, we need to reduce interactions. Generalization \\(P_t\\): treated population -&gt; \\(N_t\\): random sample from treated \\(P_c\\): control population -&gt; \\(N_c\\): random sample from control \\(\\mu_i\\) = means ; \\(\\Sigma_i\\) = variance covariance matrix of the \\(p\\) covariates in group i (\\(i = t,c\\)) \\(X_j\\) = \\(p\\) covariates of individual \\(j\\) \\(T_j\\) = treatment assignment \\(Y_j\\) = observed outcome Assume: \\(N_t &lt; N_c\\) Treatment effect is \\(\\tau(x) = R_1(x) - R_0(x)\\) where \\(R_1(x) = E(Y(1)|X)\\) \\(R_0(x) = E(Y(0)|X)\\) Assume: parallel trends hence \\(\\tau(x) = \\tau \\forall x\\) If the parallel trends are not assumed, an average effect can be estimated. Common estimands: Average effect of the treatment on the treated (ATT): effects on treatment group Average treatment effect (ATE): effect on both treatment and control Steps: Define “closeness”: decide distance measure to be used Which variables to include: Ignorability (no unobserved differences between treatment and control) Since cost of including unrelated variables is small, you should include as many as possible (unless sample size/power doesn’t allow you to because of increased variance) Do not include variables that were affected by the treatment. Note: if a matching variable (i.e., heavy drug users) is highly correlated to the outcome variable (i.e., heavy drinkers) , you will be better to exclude it in the matching set. Which distance measures: more below Matching methods Nearest neighbor matching Simple (greedy) matching: performs poorly when there is competition for controls. Optimal matching: considers global distance measure Ratio matching: to combat increase bias and reduced variation when you have k:1 matching, one can use approximations by Rubin and Thomas (1996). With or without replacement: with replacement is typically better, but one needs to account for dependent in the matched sample when doing later analysis (can use frequency weights to combat). Subclassification, Full Matching and Weighting Nearest neighbor matching assign is 0 (control) or 1 (treated), while these methods use weights between 0 and 1. Subclassification: distribution into multiple subclass (e.g., 5-10) Full matching: optimal ly minimize the average of the distances between each treated unit and each control unit within each matched set. Weighting adjustments: weighting technique uses propensity scores to estimate ATE. If the weights are extreme, the variance can be large not due to the underlying probabilities, but due to the estimation procure. To combat this, use (1) weight trimming, or (2) doubly -robust methods when propensity scores are used for weighing or matching. Inverse probability of treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\) Odds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\) Kernel weighting (e.g., in economics) averages over multiple units in the control group. Assessing Common Support common support means overlapping of the propensity score distributions in the treatment and control groups. Propensity score is used to discard control units from the common support. Alternatively, convex hull of the covariates in the multi-dimensional space. Assessing the quality of matched samples (Diagnose) Balance = similarity of the empirical distribution of the full set of covariates in the matched treated and control groups. Equivalently, treatment is unrelated to the covariates \\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) where \\(\\tilde{p}\\) is the empirical distribution. Numerical Diagnostics standardized difference in means of each covariate (most common), also known as”standardized bias”, “standardized difference in means”. standardized difference of means of the propensity score (should be &lt; 0.25) (Rubin 2001) ratio of the variances of the propensity score in the treated and control groups (should be between 0.5 and 2). (Rubin 2001) For each covariate, the ratio fo the variance of the residuals orthogonal to the propensity score in the treated and control groups. Note: can’t use hypothesis tests or p-values because of (1) in-sample property (not population), (2) conflation of changes in balance with changes in statistical power. Graphical Diagnostics QQ plots Empirical Distribution Plot Estimate the treatment effect After k:1 Need to account for weights when use matching with replacement. After Subclassification and Full Matching Weighting the subclass estimates by the number of treated units in each subclass for ATT Weighting by the overall number of individual in each subclass for ATE. Variance estimation: should incorporate uncertainties in both the matching procedure (step 3) and the estimation procedure (step 4) Notes: With missing data, use generalized boosted models, or multiple imputation (Qu and Lipkovich 2009) Violation of ignorable treatment assignment (i.e., unobservables affect treatment and outcome). control by measure pre-treatment measure of the outcome variable find the difference in outcomes between multiple control groups. If there is a significant difference, there is evidence for violation. find the range of correlations between unobservables and both treatment assignment and outcome to nullify the significant effect. Choosing between methods smallest standardized difference of mean across the largest number of covariates minimize the standardized difference of means of a few particularly prognostic covariates fest number of large standardized difference of means (&gt; 0.25) (Diamond and Sekhon 2013) automates the process In practice If ATE, ask if there is enough overlap of the treated and control groups’ propensity score to estimate ATE, if not use ATT instead If ATT, ask if there are controls across the full range of the treated group Choose matching method If ATE, use IPTW or full matching If ATT, and more controls than treated (at least 3 times), k:1 nearest neighbor without replacement If ATT, and few controls , use subclassification, full matching, and weighting by the odds Diagnostic If balance, use regression on matched samples If imbalance on few covariates, treat them with Mahalanobis If imbalance on many covariates, try k:1 matching with replacement Ways to define the distance \\(D_{ij}\\) Exact \\[ D_{ij} = \\begin{cases} 0, \\text{ if } X_i = X_j, \\\\ \\infty, \\text{ if } X_i \\neq X_j \\end{cases} \\] An advanced is Coarsened Exact Matching Mahalanobis \\[ D_{ij} = (X_i - X_j)&#39;\\Sigma^{-1} (X_i - X_j) \\] where \\(\\Sigma\\) = variance covariance matrix of X in the control group if ATT is interested polled treatment and control groups if ATE is interested Propensity score: \\[ D_{ij} = |e_i - e_j| \\] where \\(e_k\\) = the propensity score for individual k An advanced is Prognosis score (B. B. Hansen 2008), but you have to know (i.e., specify) the relationship between the covariates and outcome. Linear propensity score \\[ D_{ij} = |logit(e_i) - logit(e_j)| \\] The exact and Mahalanobis are not good in high dimensional or non normally distributed X’s cases. We can combine Mahalanobis matching with propensity score calipers (Rubin and Thomas 2000) Other advanced methods for longitudinal settings marginal structural models (Robins, Hernan, and Brumback 2000) balanced risk set matching (Y. P. Li, Propert, and Rosenbaum 2001) Most matching methods are based on (ex-post) propensity score distance metric covariates Packages cem Coarsened exact matching Matching Multivariate and propensity score matching with balance optimization MatchIt Nonparametric preprocessing for parametric causal inference. Have nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassification MatchingFrontier optimize balance and sample size (G. King, Lucas, and Nielsen 2017) optmatchoptimal matching with variable ratio, optimal and full matching PSAgraphics Propensity score graphics rbounds sensitivity analysis with matched data, examine ignorable treatment assignment assumption twang weighting and analysis of non-equivalent groups CBPS covariate balancing propensity score. Can also be used in the longitudinal setting with marginal structural models. PanelMatch based on Imai, Kim, and Wang (2018) Matching Regression Not as sensitive to the functional form of the covariates can estimate the effect of a continuous treatment Easier to asses whether it’s working Easier to explain allows a nice visualization of an evaluation estimate the effect of all the variables (not just the treatment) If you treatment is fairly rare, you may have a lot of control observations that are obviously no comparable can estimate interactions of treatment with covariates Less parametric More parametric Enforces common support (i.e., space where treatment and control have the same characteristics) However, the problem of omitted variables (i.e., those that affect both the outcome and whether observation was treated) - unobserved confounders is still present in matching methods. Difference between matching and regression following Pischke’s lecture Suppose we want to estimate the effect of treatment on the treated \\[ \\begin{aligned} \\delta_{TOT} &amp;= E[ Y_{1i} - Y_{0i} | D_i = 1 ] \\\\ &amp;= E\\{E[Y_{1i} | X_i, D_i = 1] \\\\ &amp; - E[Y_{0i}|X_i, D_i = 1]|D_i = 1\\} &amp;&amp; \\text{law of itereated expectations} \\end{aligned} \\] Under conditional independence \\[ E[Y_{0i} |X_i , D_i = 0 ] = E[Y_{0i} | X_i, D_i = 1] \\] then \\[ \\begin{aligned} \\delta_{TOT} &amp;= E \\{ E[ Y_{1i} | X_i, D_i = 1] - E[ Y_{0i}|X_i, D_i = 0 ]|D_i = 1\\} \\\\ &amp;= E\\{E[y_i | X_i, D_i = 1] - E[y_i |X_i, D_i = 0 ] | D_i = 1\\} \\\\ &amp;= E[\\delta_X |D_i = 1] \\end{aligned} \\] where \\(\\delta_X\\) is an X-specific difference in means at covariate value \\(X_i\\) When \\(X_i\\) is discrete, the matching estimand is \\[ \\delta_M = \\sum_x \\delta_x P(X_i = x |D_i = 1) \\] where \\(P(X_i = x |D_i = 1)\\) is the probability mass function for \\(X_i\\) given \\(D_i = 1\\) According to Bayes rule, \\[ P(X_i = x | D_i = 1) = \\frac{P(D_i = 1 | X_i = x) \\times P(X_i = x)}{P(D_i = 1)} \\] hence, \\[ \\begin{aligned} \\delta_M &amp;= \\frac{\\sum_x \\delta_x P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\\\ &amp;= \\sum_x \\delta_x \\frac{ P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\end{aligned} \\] On the other hand, suppose we have regression \\[ y_i = \\sum_x d_{ix} \\beta_x + \\delta_R D_i + \\epsilon_i \\] where \\(d_{ix}\\) = dummy that indicates \\(X_i = x\\) \\(\\beta_x\\) = regression-effect for \\(X_i = x\\) \\(\\delta_R\\) = regression estimand where \\[ \\begin{aligned} \\delta_R &amp;= \\frac{\\sum_x \\delta_x [P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\\\ &amp;= \\sum_x \\delta_x \\frac{[P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\end{aligned} \\] the difference between the regression and matching estimand is the weights they use to combine the covariate specific treatment effect \\(\\delta_x\\) Type uses weights which depend on interpretation makes sense because Matching \\(P(D_i = 1|X_i = x)\\) the fraction of treated observations in a covariate cell (i.e., or the mean of \\(D_i\\)) This is larger in cells with many treated observations. we want the effect of treatment on the treated Regression \\(P(D_i = 1 |X_i = x)(1 - P(D_i = 1| X_i ))\\) the variance of \\(D_i\\) in the covariate cell This weight is largest in cells where there are half treated and half untreated observations. (this is the reason why we want to treat our sample so it is balanced, before running regular regression model, as mentioned above). these cells will produce the lowest variance estimates of \\(\\delta_x\\). If all the \\(\\delta_x\\) are the same, the most efficient estimand uses the lowest variance cells most heavily. The goal of matching is to produce covariate balance (i.e., distributions of covariates in treatment and control groups are approximately similar as they would be in a successful randomized experiment). References "],["selection-on-observables.html", "31.1 Selection on Observables", " 31.1 Selection on Observables 31.1.1 MatchIt Procedure typically involves (proposed by Noah Freifer using MatchIt) planning matching checking (balance) estimating the treatment effect library(MatchIt) data(&quot;lalonde&quot;) examine treat on re78 Planning select type of effect to be estimated (e.g., mediation effect, conditional effect, marginal effect) select the target population select variables to match/balance (Austin 2011) (T. J. VanderWeele 2019) Check Initial Imbalance # No matching; constructing a pre-match matchit object m.out0 &lt;- matchit( formula(treat ~ age + educ + race + married + nodegree + re74 + re75, env = lalonde), data = data.frame(lalonde), method = NULL, # assess balance before matching distance = &quot;glm&quot; # logistic regression ) # Checking balance prior to matching summary(m.out0) Matching # 1:1 NN PS matching w/o replacement m.out1 &lt;- matchit(treat ~ age + educ, data = lalonde, method = &quot;nearest&quot;, distance = &quot;glm&quot;) m.out1 #&gt; A matchit object #&gt; - method: 1:1 nearest neighbor matching without replacement #&gt; - distance: Propensity score #&gt; - estimated with logistic regression #&gt; - number of obs.: 614 (original), 370 (matched) #&gt; - target estimand: ATT #&gt; - covariates: age, educ Check balance Sometimes you have to make trade-off between balance and sample size. # Checking balance after NN matching summary(m.out1, un = FALSE) #&gt; #&gt; Call: #&gt; matchit(formula = treat ~ age + educ, data = lalonde, method = &quot;nearest&quot;, #&gt; distance = &quot;glm&quot;) #&gt; #&gt; Summary of Balance for Matched Data: #&gt; Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean #&gt; distance 0.3080 0.3077 0.0094 0.9963 0.0033 #&gt; age 25.8162 25.8649 -0.0068 1.0300 0.0050 #&gt; educ 10.3459 10.2865 0.0296 0.5886 0.0253 #&gt; eCDF Max Std. Pair Dist. #&gt; distance 0.0432 0.0146 #&gt; age 0.0162 0.0597 #&gt; educ 0.1189 0.8146 #&gt; #&gt; Sample Sizes: #&gt; Control Treated #&gt; All 429 185 #&gt; Matched 185 185 #&gt; Unmatched 244 0 #&gt; Discarded 0 0 # examine visually plot(m.out1, type = &quot;jitter&quot;, interactive = FALSE) plot( m.out1, type = &quot;qq&quot;, interactive = FALSE, which.xs = c(&quot;age&quot;) ) Try Full Match (i.e., every treated matches with one control, and every control with one treated). # Full matching on a probit PS m.out2 &lt;- matchit(treat ~ age + educ, data = lalonde, method = &quot;full&quot;, distance = &quot;glm&quot;, link = &quot;probit&quot;) m.out2 #&gt; A matchit object #&gt; - method: Optimal full matching #&gt; - distance: Propensity score #&gt; - estimated with probit regression #&gt; - number of obs.: 614 (original), 614 (matched) #&gt; - target estimand: ATT #&gt; - covariates: age, educ Checking balance again # Checking balance after full matching summary(m.out2, un = FALSE) #&gt; #&gt; Call: #&gt; matchit(formula = treat ~ age + educ, data = lalonde, method = &quot;full&quot;, #&gt; distance = &quot;glm&quot;, link = &quot;probit&quot;) #&gt; #&gt; Summary of Balance for Matched Data: #&gt; Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean #&gt; distance 0.3082 0.3081 0.0023 0.9815 0.0028 #&gt; age 25.8162 25.8035 0.0018 0.9825 0.0062 #&gt; educ 10.3459 10.2315 0.0569 0.4390 0.0481 #&gt; eCDF Max Std. Pair Dist. #&gt; distance 0.0270 0.0382 #&gt; age 0.0249 0.1110 #&gt; educ 0.1300 0.9805 #&gt; #&gt; Sample Sizes: #&gt; Control Treated #&gt; All 429. 185 #&gt; Matched (ESS) 145.23 185 #&gt; Matched 429. 185 #&gt; Unmatched 0. 0 #&gt; Discarded 0. 0 plot(summary(m.out2)) Exact Matching # Full matching on a probit PS m.out3 &lt;- matchit( treat ~ age + educ, data = lalonde, method = &quot;exact&quot; ) m.out3 #&gt; A matchit object #&gt; - method: Exact matching #&gt; - number of obs.: 614 (original), 332 (matched) #&gt; - target estimand: ATT #&gt; - covariates: age, educ Subclassfication m.out4 &lt;- matchit( treat ~ age + educ, data = lalonde, method = &quot;subclass&quot; ) m.out4 #&gt; A matchit object #&gt; - method: Subclassification (6 subclasses) #&gt; - distance: Propensity score #&gt; - estimated with logistic regression #&gt; - number of obs.: 614 (original), 614 (matched) #&gt; - target estimand: ATT #&gt; - covariates: age, educ # Or you can use in conjunction with &quot;nearest&quot; m.out4 &lt;- matchit( treat ~ age + educ, data = lalonde, method = &quot;nearest&quot;, option = &quot;subclass&quot; ) m.out4 #&gt; A matchit object #&gt; - method: 1:1 nearest neighbor matching without replacement #&gt; - distance: Propensity score #&gt; - estimated with logistic regression #&gt; - number of obs.: 614 (original), 370 (matched) #&gt; - target estimand: ATT #&gt; - covariates: age, educ Optimal Matching m.out5 &lt;- matchit( treat ~ age + educ, data = lalonde, method = &quot;optimal&quot;, ratio = 2 ) m.out5 #&gt; A matchit object #&gt; - method: 2:1 optimal pair matching #&gt; - distance: Propensity score #&gt; - estimated with logistic regression #&gt; - number of obs.: 614 (original), 555 (matched) #&gt; - target estimand: ATT #&gt; - covariates: age, educ Genetic Matching m.out6 &lt;- matchit( treat ~ age + educ, data = lalonde, method = &quot;genetic&quot; ) m.out6 #&gt; A matchit object #&gt; - method: 1:1 genetic matching without replacement #&gt; - distance: Propensity score #&gt; - estimated with logistic regression #&gt; - number of obs.: 614 (original), 370 (matched) #&gt; - target estimand: ATT #&gt; - covariates: age, educ Estimating the Treatment Effect # get matched data m.data1 &lt;- match.data(m.out1) head(m.data1) #&gt; treat age educ race married nodegree re74 re75 re78 distance #&gt; NSW1 1 37 11 black 1 1 0 0 9930.0460 0.2536942 #&gt; NSW2 1 22 9 hispan 0 1 0 0 3595.8940 0.3245468 #&gt; NSW3 1 30 12 black 0 0 0 0 24909.4500 0.2881139 #&gt; NSW4 1 27 11 black 0 1 0 0 7506.1460 0.3016672 #&gt; NSW5 1 33 8 black 0 1 0 0 289.7899 0.2683025 #&gt; NSW6 1 22 9 black 0 1 0 0 4056.4940 0.3245468 #&gt; weights subclass #&gt; NSW1 1 1 #&gt; NSW2 1 98 #&gt; NSW3 1 109 #&gt; NSW4 1 120 #&gt; NSW5 1 131 #&gt; NSW6 1 142 library(&quot;lmtest&quot;) #coeftest library(&quot;sandwich&quot;) #vcovCL # imbalance matched dataset fit1 &lt;- lm(re78 ~ treat + age + educ , data = m.data1, weights = weights) coeftest(fit1, vcov. = vcovCL, cluster = ~subclass) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -174.902 2445.013 -0.0715 0.943012 #&gt; treat -1139.085 780.399 -1.4596 0.145253 #&gt; age 153.133 55.317 2.7683 0.005922 ** #&gt; educ 358.577 163.860 2.1883 0.029278 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 treat coefficient = estimated ATT # balance matched dataset m.data2 &lt;- match.data(m.out2) fit2 &lt;- lm(re78 ~ treat + age + educ , data = m.data2, weights = weights) coeftest(fit2, vcov. = vcovCL, cluster = ~subclass) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2151.952 3141.152 0.6851 0.49355 #&gt; treat -725.184 703.297 -1.0311 0.30289 #&gt; age 120.260 53.933 2.2298 0.02612 * #&gt; educ 175.693 241.694 0.7269 0.46755 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 When reporting, remember to mention the matching specification (method, and additional options) the distance measure (e.g., propensity score) other methods, and rationale for the final chosen method. balance statistics of the matched dataset. number of matched, unmatched, discarded estimation method for treatment effect. 31.1.2 designmatch This package includes distmatch optimal distance matching bmatch optimal bipartile matching cardmatch optimal cardinality matching profmatch optimal profile matching nmatch optimal nonbipartile matching library(designmatch) 31.1.3 MatchingFrontier As mentioned in MatchIt, you have to make trade-off (also known as bias-variance trade-off) between balance and sample size. An automated procedure to optimize this trade-off is implemented in MatchingFrontier (G. King, Lucas, and Nielsen 2017), which solves this joint optimization problem. Following MatchingFrontier guide # library(devtools) # install_github(&#39;ChristopherLucas/MatchingFrontier&#39;) library(MatchingFrontier) data(&quot;lalonde&quot;) # choose var to match on match.on &lt;- colnames(lalonde)[!(colnames(lalonde) %in% c(&#39;re78&#39;, &#39;treat&#39;))] match.on # Mahanlanobis frontier (default) mahal.frontier &lt;- makeFrontier( dataset = lalonde, treatment = &quot;treat&quot;, match.on = match.on ) mahal.frontier # L1 frontier L1.frontier &lt;- makeFrontier( dataset = lalonde, treatment = &#39;treat&#39;, match.on = match.on, QOI = &#39;SATT&#39;, metric = &#39;L1&#39;, ratio = &#39;fixed&#39; ) L1.frontier # estimate effects along the frontier # Set base form my.form &lt;- as.formula(re78 ~ treat + age + black + education + hispanic + married + nodegree + re74 + re75) # Estimate effects for the mahalanobis frontier mahal.estimates &lt;- estimateEffects( mahal.frontier, &#39;re78 ~ treat&#39;, mod.dependence.formula = my.form, continuous.vars = c(&#39;age&#39;, &#39;education&#39;, &#39;re74&#39;, &#39;re75&#39;), prop.estimated = .1, means.as.cutpoints = TRUE ) # Estimate effects for the L1 frontier L1.estimates &lt;- estimateEffects( L1.frontier, &#39;re78 ~ treat&#39;, mod.dependence.formula = my.form, continuous.vars = c(&#39;age&#39;, &#39;education&#39;, &#39;re74&#39;, &#39;re75&#39;), prop.estimated = .1, means.as.cutpoints = TRUE ) # Plot covariates means # plotPrunedMeans() # Plot estimates (deprecated) # plotEstimates( # L1.estimates, # ylim = c(-10000, 3000), # cex.lab = 1.4, # cex.axis = 1.4, # panel.first = grid(NULL, NULL, lwd = 2,) # ) # Plot estimates plotMeans(L1.frontier) # parallel plot parallelPlot( L1.frontier, N = 400, variables = c(&#39;age&#39;, &#39;re74&#39;, &#39;re75&#39;, &#39;black&#39;), treated.col = &#39;blue&#39;, control.col = &#39;gray&#39; ) # export matched dataset # take 400 units matched.data &lt;- generateDataset(L1.frontier, N = 400) 31.1.4 Propensity Scores Even though I mention the propensity scores matching method here, it is no longer recommended to use such method in research and publication (G. King and Nielsen 2019) because it increases imbalance inefficiency model dependence: small changes in the model specification lead to big changes in model results bias (Abadie and Imbens 2016)note The initial estimation of the propensity score influences the large sample distribution of the estimators. Adjustments are made to the large sample variances of these estimators for both ATE and ATT. The adjustment for the ATE estimator is either negative or zero, indicating greater efficiency when matching on an estimated propensity score versus the true score in large samples. For the ATET estimator, the sign of the adjustment depends on the data generating process. Neglecting the estimation error in the propensity score can lead to inaccurate confidence intervals for the ATT estimator, making them either too large or too small. PSM tries to accomplish complete randomization while other methods try to achieve fully blocked. Hence, you probably better off use any other methods. Propensity is “the probability of receiving the treatment given the observed covariates.” (Rosenbaum and Rubin 1985) Equivalently, it can to understood as the probability of being treated. \\[ e_i (X_i) = P(T_i = 1 | X_i) \\] Estimation using logistic regression Non parametric methods: boosted CART generalized boosted models (gbm) Steps by Gary King’s slides reduce k elements of X to scalar \\(\\pi_i \\equiv P(T_i = 1|X) = \\frac{1}{1+e^{X_i \\beta}}\\) Distance (\\(X_c, X_t\\)) = \\(|\\pi_c - \\pi_t|\\) match each treated unit to the nearest control unit control units: not reused; pruned if unused prune matches if distances &gt; caliper In the best case scenario, you randomly prune, which increases imbalance Other methods dominate because they try to match exactly hence \\(X_c = X_t \\to \\pi_c = \\pi_t\\) (exact match leads to equal propensity scores) but \\(\\pi_c = \\pi_t \\nrightarrow X_c = X_t\\) (equal propensity scores do not necessarily lead to exact match) Notes: Do not include/control for irrelevant covariates because it leads your PSM to be more random, hence more imbalance Do not include for (Bhattacharya and Vogt 2007) instrumental variable in the predictor set of a propensity score matching estimator. More generally, using variables that do not control for potential confounders, even if they are predictive of the treatment, can result in biased estimates What you left with after pruning is more important than what you start with then throw out. Diagnostics: balance of the covariates no need to concern about collinearity can’t use c-stat or stepwise because those model fit stat do not apply 31.1.4.1 Look Ahead Propensity Score Matching (Bapna, Ramaprasad, and Umyarov 2018) 31.1.5 Mahalanobis Distance Approximates fully blocked experiment Distance \\((X_c,X_t)\\) = \\(\\sqrt{(X_c - X_t)&#39;S^{-1}(X_c - X_t)}\\) where \\(S^{-1}\\) standardize the distance In application we use Euclidean distance. Prune unused control units, and prune matches if distance &gt; caliper 31.1.6 Coarsened Exact Matching Steps from Gray King’s slides International Methods Colloquium talk 2015 Temporarily coarsen \\(X\\) Apply exact matching to the coarsened \\(X, C(X)\\) sort observation into strata, each with unique values of \\(C(X)\\) prune stratum with 0 treated or 0 control units Pass on original (uncoarsened) units except those pruned Properties: Monotonic imbalance bounding (MIB) matching method maximum imbalance between the treated and control chosen ex ante meets congruence principle robust to measurement error can be implemented with multiple imputation works well for multi-category treatments Assumptions: Ignorability (i.e., no omitted variable bias) More detail in (Iacus, King, and Porro 2012) Example by package’s authors library(cem) data(LeLonde) Le &lt;- data.frame(na.omit(LeLonde)) # remove missing data # treated and control groups tr &lt;- which(Le$treated==1) ct &lt;- which(Le$treated==0) ntr &lt;- length(tr) nct &lt;- length(ct) # unadjusted, biased difference in means mean(Le$re78[tr]) - mean(Le$re78[ct]) #&gt; [1] 759.0479 # pre-treatment covariates vars &lt;- c( &quot;age&quot;, &quot;education&quot;, &quot;black&quot;, &quot;married&quot;, &quot;nodegree&quot;, &quot;re74&quot;, &quot;re75&quot;, &quot;hispanic&quot;, &quot;u74&quot;, &quot;u75&quot;, &quot;q1&quot; ) # overall imbalance statistics imbalance(group=Le$treated, data=Le[vars]) # L1 = 0.902 #&gt; #&gt; Multivariate Imbalance Measure: L1=0.902 #&gt; Percentage of local common support: LCS=5.8% #&gt; #&gt; Univariate Imbalance Measures: #&gt; #&gt; statistic type L1 min 25% 50% 75% #&gt; age -0.252373042 (diff) 5.102041e-03 0 0 0.0000 -1.0000 #&gt; education 0.153634710 (diff) 8.463851e-02 1 0 1.0000 1.0000 #&gt; black -0.010322734 (diff) 1.032273e-02 0 0 0.0000 0.0000 #&gt; married -0.009551495 (diff) 9.551495e-03 0 0 0.0000 0.0000 #&gt; nodegree -0.081217371 (diff) 8.121737e-02 0 -1 0.0000 0.0000 #&gt; re74 -18.160446880 (diff) 5.551115e-17 0 0 284.0715 806.3452 #&gt; re75 101.501761679 (diff) 5.551115e-17 0 0 485.6310 1238.4114 #&gt; hispanic -0.010144756 (diff) 1.014476e-02 0 0 0.0000 0.0000 #&gt; u74 -0.045582186 (diff) 4.558219e-02 0 0 0.0000 0.0000 #&gt; u75 -0.065555292 (diff) 6.555529e-02 0 0 0.0000 0.0000 #&gt; q1 7.494021189 (Chi2) 1.067078e-01 NA NA NA NA #&gt; max #&gt; age -6.0000 #&gt; education 1.0000 #&gt; black 0.0000 #&gt; married 0.0000 #&gt; nodegree 0.0000 #&gt; re74 -2139.0195 #&gt; re75 490.3945 #&gt; hispanic 0.0000 #&gt; u74 0.0000 #&gt; u75 0.0000 #&gt; q1 NA # drop other variables that are not pre - treatmentt matching variables todrop &lt;- c(&quot;treated&quot;, &quot;re78&quot;) imbalance(group=Le$treated, data=Le, drop=todrop) #&gt; #&gt; Multivariate Imbalance Measure: L1=0.902 #&gt; Percentage of local common support: LCS=5.8% #&gt; #&gt; Univariate Imbalance Measures: #&gt; #&gt; statistic type L1 min 25% 50% 75% #&gt; age -0.252373042 (diff) 5.102041e-03 0 0 0.0000 -1.0000 #&gt; education 0.153634710 (diff) 8.463851e-02 1 0 1.0000 1.0000 #&gt; black -0.010322734 (diff) 1.032273e-02 0 0 0.0000 0.0000 #&gt; married -0.009551495 (diff) 9.551495e-03 0 0 0.0000 0.0000 #&gt; nodegree -0.081217371 (diff) 8.121737e-02 0 -1 0.0000 0.0000 #&gt; re74 -18.160446880 (diff) 5.551115e-17 0 0 284.0715 806.3452 #&gt; re75 101.501761679 (diff) 5.551115e-17 0 0 485.6310 1238.4114 #&gt; hispanic -0.010144756 (diff) 1.014476e-02 0 0 0.0000 0.0000 #&gt; u74 -0.045582186 (diff) 4.558219e-02 0 0 0.0000 0.0000 #&gt; u75 -0.065555292 (diff) 6.555529e-02 0 0 0.0000 0.0000 #&gt; q1 7.494021189 (Chi2) 1.067078e-01 NA NA NA NA #&gt; max #&gt; age -6.0000 #&gt; education 1.0000 #&gt; black 0.0000 #&gt; married 0.0000 #&gt; nodegree 0.0000 #&gt; re74 -2139.0195 #&gt; re75 490.3945 #&gt; hispanic 0.0000 #&gt; u74 0.0000 #&gt; u75 0.0000 #&gt; q1 NA automated coarsening mat &lt;- cem( treatment = &quot;treated&quot;, data = Le, drop = &quot;re78&quot;, keep.all = TRUE ) #&gt; #&gt; Using &#39;treated&#39;=&#39;1&#39; as baseline group mat #&gt; G0 G1 #&gt; All 392 258 #&gt; Matched 95 84 #&gt; Unmatched 297 174 # mat$w coarsening by explicit user choice # categorial variables levels(Le$q1) # grouping option #&gt; [1] &quot;agree&quot; &quot;disagree&quot; &quot;neutral&quot; #&gt; [4] &quot;no opinion&quot; &quot;strongly agree&quot; &quot;strongly disagree&quot; q1.grp &lt;- list( c(&quot;strongly agree&quot;, &quot;agree&quot;), c(&quot;neutral&quot;, &quot;no opinion&quot;), c(&quot;strongly disagree&quot;, &quot;disagree&quot;) ) # if you want ordered categories # continuous variables table(Le$education) #&gt; #&gt; 3 4 5 6 7 8 9 10 11 12 13 14 15 #&gt; 1 5 4 6 12 55 106 146 173 113 19 9 1 educut &lt;- c(0, 6.5, 8.5, 12.5, 17) # use cutpoints mat1 &lt;- cem( treatment = &quot;treated&quot;, data = Le, drop = &quot;re78&quot;, cutpoints = list(education = educut), grouping = list(q1 = q1.grp) ) #&gt; #&gt; Using &#39;treated&#39;=&#39;1&#39; as baseline group mat1 #&gt; G0 G1 #&gt; All 392 258 #&gt; Matched 158 115 #&gt; Unmatched 234 143 Can also use progressive coarsening method to control the number of matches. cem can also handle some missingness. 31.1.7 Genetic Matching GM uses iterative checking process of propensity scores, which combines propensity scores and Mahalanobis distance. GenMatch (Diamond and Sekhon 2013) GM is arguably “superior” method than nearest neighbor or full matching in imbalanced data Use a genetic search algorithm to find weights for each covariate such that we have optimal balance. Implementation could use with replacement balance can be based on paired \\(t\\)-tests (dichotomous variables) Kolmogorov-Smirnov (multinomial and continuous) Packages Matching library(Matching) data(lalonde) attach(lalonde) #The covariates we want to match on X = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74) #The covariates we want to obtain balance on BalanceMat &lt;- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74, I(re74 * re75)) # #Let&#39;s call GenMatch() to find the optimal weight to give each #covariate in &#39;X&#39; so as we have achieved balance on the covariates in #&#39;BalanceMat&#39;. This is only an example so we want GenMatch to be quick #so the population size has been set to be only 16 via the &#39;pop.size&#39; #option. This is *WAY* too small for actual problems. #For details see http://sekhon.berkeley.edu/papers/MatchingJSS.pdf. # genout &lt;- GenMatch( Tr = treat, X = X, BalanceMatrix = BalanceMat, estimand = &quot;ATE&quot;, M = 1, pop.size = 16, max.generations = 10, wait.generations = 1 ) #The outcome variable Y=re78/1000 # # Now that GenMatch() has found the optimal weights, let&#39;s estimate # our causal effect of interest using those weights # mout &lt;- Match( Y = Y, Tr = treat, X = X, estimand = &quot;ATE&quot;, Weight.matrix = genout ) summary(mout) # #Let&#39;s determine if balance has actually been obtained on the variables of interest # mb &lt;- MatchBalance( treat ~ age + educ + black + hisp + married + nodegr + u74 + u75 + re75 + re74 + I(re74 * re75), match.out = mout, nboots = 500 ) 31.1.8 Entropy Balancing (Hainmueller 2012) Entropy balancing is a method for achieving covariate balance in observational studies with binary treatments. It uses a maximum entropy reweighting scheme to ensure that treatment and control groups are balanced based on sample moments. This method adjusts for inequalities in the covariate distributions, reducing dependence on the model used for estimating treatment effects. Entropy balancing improves balance across all included covariate moments and removes the need for repetitive balance checking and iterative model searching. 31.1.9 Matching for high-dimensional data One could reduce the number of dimensions using methods such as: Lasso (Gordon et al. 2019) Penalized logistic regression (Eckles and Bakshy 2021) PCA (Principal Component Analysis) Locality Preserving Projections (LPP) (S. Li et al. 2016) Random projection Autoencoders (Ramachandra 2018) Additionally, one could jointly does dimension reduction while balancing the distributions of the control and treated groups (Yao et al. 2018). 31.1.10 Matching for time series-cross-section data Examples: (Scheve and Stasavage 2012) and (Acemoglu et al. 2019) Identification strategy: Within-unit over-time variation within-time across-units variation See DID with in and out treatment condition for details of this method 31.1.11 Matching for multiple treatments In cases where you have multiple treatment groups, and you want to do matching, it’s important to have the same baseline (control) group. For more details, see (McCaffrey et al. 2013) (Lopez and Gutman 2017) (Zhao et al. 2021): also for continuous treatment If you insist on using the MatchIt package, then see this answer 31.1.12 Matching for multi-level treatments See (Yang et al. 2016) Package in R shuyang1987/multilevelMatching on Github 31.1.13 Matching for repeated treatments https://cran.r-project.org/web/packages/twang/vignettes/iptw.pdf package in R twang References "],["selection-on-unobservables.html", "31.2 Selection on Unobservables", " 31.2 Selection on Unobservables There are several ways one can deal with selection on unobservables: Rosenbaum Bounds Endogenous Sample Selection (i.e., Heckman-style correction): examine the \\(\\lambda\\) term to see whether it’s significant (sign of endogenous selection) Relative Correlation Restrictions Coefficient-stability Bounds 31.2.1 Rosenbaum Bounds Examples in marketing (Oestreicher-Singer and Zalmanson 2013): A range of 1.5 to 1.8 is important for the effect of the level of community participation of users on their willingness to pay for premium services. (M. Sun and Zhu 2013): A factor of 1.5 is essential for understanding the relationship between the launch of an ad revenue-sharing program and the popularity of content. (Manchanda, Packard, and Pattabhiramaiah 2015): A factor of 1.6 is required for the social dollar effect to be nullified. (Sudhir and Talukdar 2015): A factor of 1.9 is needed for IT adoption to impact labor productivity, and 2.2 for IT adoption to affect floor productivity. (Proserpio and Zervas 2017b): A factor of 2 is necessary for the firm’s use of management responses to influence online reputation. (S. Zhang et al. 2022): A factor of 1.55 is critical for the acquisition of verified images to drive demand for Airbnb properties. (Chae, Ha, and Schweidel 2023): A factor of 27 (not a typo) is significant in how paywall suspensions affect subsequent subscription decisions. General Matching Methods are favored for estimating treatment effects in observational data, offering advantages over regression methods because It reduces reliance on functional form assumptions. Assumes all selection-influencing covariates are observable; estimates are unbiased if no unobserved confounders are missed. Concerns arise when potentially relevant covariates are unmeasured. Rosenbaum Bounds assess the overall sensitivity of coefficient estimates to hidden bias (Rosenbaum and Rosenbaum 2002) without having knowledge (e.g., direction) of the bias. Because the unboservables that cause hidden bias have to both affect selection into treatment by a factor of \\(\\Gamma\\) and predictive of outcome, this method is also known as worst case analyses (DiPrete and Gangl 2004). Can’t provide precise bounds on estimates of treatment effects (see Relative Correlation Restrictions) Typically, we show both p-value and H-L point estimate for each level of gamma \\(\\Gamma\\) With random treatment assignment, we can use the non-parametric test (Wilcoxon signed rank test) to see if there is treatment effect. Without random treatment assignment (i.e., observational data), we cannot use this test. With Selection on Observables, we can use this test if we believe there are no unmeasured confounders. And this is where Rosenbaum (2002) can come in to talk about the believability of this notion. In layman’s terms, consider that the treatment assignment is based on a method where the odds of treatment for a unit and its control differ by a multiplier \\(\\Gamma\\) For example, \\(\\Gamma = 1\\) means that the odds of assignment are identical, indicating random treatment assignment. Another example, \\(\\Gamma = 2\\), in the same matched pair, one unit is twice as likely to receive the treatment (due to unobservables). Since we can’t know \\(\\Gamma\\) with certainty, we run sensitivity analysis to see if the results change with different values of \\(\\Gamma\\) This bias is the product of an unobservable that influences both treatment selection and outcome by a factor \\(\\Gamma\\) (omitted variable bias) In technical terms, Treatment Assignment and Probability: Consider unit \\(j\\) with a probability \\(\\pi_j\\) of receiving the treatment, and unit \\(i\\) with \\(\\pi_i\\). Ideally, after matching, if there’s no hidden bias, we’d have \\(\\pi_i = \\pi_j\\). However, observing \\(\\pi_i \\neq \\pi_j\\) raises questions about potential biases affecting our inference. This is evaluated using the odds ratio. Odds Ratio and Hidden Bias: The odds of treatment for a unit \\(j\\) is defined as \\(\\frac{\\pi_j}{1 - \\pi_j}\\). The odds ratio between two matched units \\(i\\) and \\(j\\) is constrained by \\(\\frac{1}{\\Gamma} \\le \\frac{\\pi_i / (1- \\pi_i)}{\\pi_j/ (1- \\pi_j)} \\le \\Gamma\\). If \\(\\Gamma = 1\\), it implies an absence of hidden bias. If \\(\\Gamma = 2\\), the odds of receiving treatment could differ by up to a factor of 2 between the two units. Sensitivity Analysis Using Gamma: The value of \\(\\Gamma\\) helps measure the potential departure from a bias-free study. Sensitivity analysis involves varying \\(\\Gamma\\) to examine how inferences might change with the presence of hidden biases. Incorporating Unobserved Covariates: Consider a scenario where unit \\(i\\) has observed covariates \\(x_i\\) and an unobserved covariate \\(u_i\\), that both affect the outcome. A logistic regression model could link the odds of assignment to these covariates: \\(\\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\kappa x_i + \\gamma u_i\\), where \\(\\gamma\\) represents the impact of the unobserved covariate. Steps for Sensitivity Analysis (We could create a table of different levels of \\(\\Gamma\\) to assess how the magnitude of biases can affect our evidence of the treatment effect (estimate): Select a range of values for \\(\\Gamma\\) (e.g., \\(1 \\to 2\\)). Assess how the p-value or the magnitude of the treatment effect (Hodges Jr and Lehmann 2011) (for more details, see (Hollander, Wolfe, and Chicken 2013)) changes with varying \\(\\Gamma\\) values. Employ specific randomization tests based on the type of outcome to establish bounds on inferences. report the minimum value of \\(\\Gamma\\) at which the treatment treat is nullified (i.e., become insignificant). And the literature’s rules of thumb is that if \\(\\Gamma &gt; 2\\), then we have strong evidence for our treatment effect is robust to large biases (Proserpio and Zervas 2017a) Notes: If we have treatment assignment is clustered (e.g., within school, within state) we need to adjust the bounds for clustered treatment assignment (B. B. Hansen, Rosenbaum, and Small 2014) (similar to clustered standard errors). Packages rbounds (Keele 2010) sensitivitymv (Rosenbaum 2015) Since we typically assess our estimate sensitivity to unboservables after matching, we first do some matching. library(MatchIt) library(Matching) data(&quot;lalonde&quot;) matched &lt;- MatchIt::matchit( treat ~ age + educ, data = lalonde, method = &quot;nearest&quot; ) summary(matched) #&gt; #&gt; Call: #&gt; MatchIt::matchit(formula = treat ~ age + educ, data = lalonde, #&gt; method = &quot;nearest&quot;) #&gt; #&gt; Summary of Balance for All Data: #&gt; Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean #&gt; distance 0.4203 0.4125 0.1689 1.2900 0.0431 #&gt; age 25.8162 25.0538 0.1066 1.0278 0.0254 #&gt; educ 10.3459 10.0885 0.1281 1.5513 0.0287 #&gt; eCDF Max #&gt; distance 0.1251 #&gt; age 0.0652 #&gt; educ 0.1265 #&gt; #&gt; Summary of Balance for Matched Data: #&gt; Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean #&gt; distance 0.4203 0.4179 0.0520 1.1691 0.0105 #&gt; age 25.8162 25.5081 0.0431 1.1518 0.0148 #&gt; educ 10.3459 10.2811 0.0323 1.5138 0.0224 #&gt; eCDF Max Std. Pair Dist. #&gt; distance 0.0595 0.0598 #&gt; age 0.0486 0.5628 #&gt; educ 0.0757 0.3602 #&gt; #&gt; Sample Sizes: #&gt; Control Treated #&gt; All 260 185 #&gt; Matched 185 185 #&gt; Unmatched 75 0 #&gt; Discarded 0 0 matched_data &lt;- match.data(matched) treatment_group &lt;- subset(matched_data, treat == 1) control_group &lt;- subset(matched_data, treat == 0) library(rbounds) # p-value sensitivity psens_res &lt;- psens(treatment_group$re78, control_group$re78, Gamma = 2, GammaInc = .1) psens_res #&gt; #&gt; Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value #&gt; #&gt; Unconfounded estimate .... 0.0058 #&gt; #&gt; Gamma Lower bound Upper bound #&gt; 1.0 0.0058 0.0058 #&gt; 1.1 0.0011 0.0235 #&gt; 1.2 0.0002 0.0668 #&gt; 1.3 0.0000 0.1458 #&gt; 1.4 0.0000 0.2599 #&gt; 1.5 0.0000 0.3967 #&gt; 1.6 0.0000 0.5378 #&gt; 1.7 0.0000 0.6664 #&gt; 1.8 0.0000 0.7723 #&gt; 1.9 0.0000 0.8523 #&gt; 2.0 0.0000 0.9085 #&gt; #&gt; Note: Gamma is Odds of Differential Assignment To #&gt; Treatment Due to Unobserved Factors #&gt; # Hodges-Lehmann point estimate sensitivity # median difference between treatment and control hlsens_res &lt;- hlsens(treatment_group$re78, control_group$re78, Gamma = 2, GammaInc = .1) hlsens_res #&gt; #&gt; Rosenbaum Sensitivity Test for Hodges-Lehmann Point Estimate #&gt; #&gt; Unconfounded estimate .... 1745.843 #&gt; #&gt; Gamma Lower bound Upper bound #&gt; 1.0 1745.800000 1745.8 #&gt; 1.1 1139.100000 1865.6 #&gt; 1.2 830.840000 2160.9 #&gt; 1.3 533.740000 2462.4 #&gt; 1.4 259.940000 2793.8 #&gt; 1.5 -0.056912 3059.3 #&gt; 1.6 -144.960000 3297.8 #&gt; 1.7 -380.560000 3535.7 #&gt; 1.8 -554.360000 3751.0 #&gt; 1.9 -716.360000 4012.1 #&gt; 2.0 -918.760000 4224.3 #&gt; #&gt; Note: Gamma is Odds of Differential Assignment To #&gt; Treatment Due to Unobserved Factors #&gt; For multiple control group matching library(Matching) library(MatchIt) n_ratio &lt;- 2 matched &lt;- MatchIt::matchit(treat ~ age + educ , method = &quot;nearest&quot;, ratio = n_ratio) summary(matched) matched_data &lt;- match.data(matched) mcontrol_res &lt;- rbounds::mcontrol( y = matched_data$re78, grp.id = matched_data$subclass, treat.id = matched_data$treat, group.size = n_ratio + 1, Gamma = 2.5, GammaInc = .1 ) mcontrol_res sensitivitymw is faster than sensitivitymw. But sensitivitymw can match where matched sets can have differing numbers of controls (Rosenbaum 2015). library(sensitivitymv) data(lead150) head(lead150) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] #&gt; [1,] 1.40 1.23 2.24 0.96 1.90 1.14 #&gt; [2,] 0.63 0.99 0.87 1.90 0.67 1.40 #&gt; [3,] 1.98 0.82 0.66 0.58 1.00 1.30 #&gt; [4,] 1.45 0.53 1.43 1.70 0.85 1.50 #&gt; [5,] 1.60 1.70 0.63 1.05 1.08 0.92 #&gt; [6,] 1.13 0.31 0.71 1.10 0.86 1.14 senmv(lead150,gamma=2,trim=2) #&gt; $pval #&gt; [1] 0.02665519 #&gt; #&gt; $deviate #&gt; [1] 1.932398 #&gt; #&gt; $statistic #&gt; [1] 27.97564 #&gt; #&gt; $expectation #&gt; [1] 18.0064 #&gt; #&gt; $variance #&gt; [1] 26.61524 library(sensitivitymw) senmw(lead150,gamma=2,trim=2) #&gt; $pval #&gt; [1] 0.02665519 #&gt; #&gt; $deviate #&gt; [1] 1.932398 #&gt; #&gt; $statistic #&gt; [1] 27.97564 #&gt; #&gt; $expectation #&gt; [1] 18.0064 #&gt; #&gt; $variance #&gt; [1] 26.61524 31.2.2 Relative Correlation Restrictions Examples in marketing (Manchanda, Packard, and Pattabhiramaiah 2015): 3.23 for social dollar effect to be nullified (Chae, Ha, and Schweidel 2023): 6.69 (i.e., how much stronger the selection on unobservables has to be compared to the selection on observables to negate the result) for paywall suspensions affect subsequent subscription decisions (M. Sun and Zhu 2013) General Proposed by Altonji, Elder, and Taber (2005) Generalized by Krauth (2016) Estimate bounds of the treatment effects due to unobserved selection. \\[ Y_i = X_i \\beta + C_i \\gamma + \\epsilon_i \\] where \\(\\beta\\) is the effect of interest \\(C_i\\) is the control variable Using OLS, \\(cor(X_i, \\epsilon_i) = 0\\) Under RCR analysis, we assume \\[ cor(X_i, \\epsilon_i) = \\lambda cor(X_i, C_i \\gamma) \\] where \\(\\lambda \\in (\\lambda_l, \\lambda_h)\\) Choice of \\(\\lambda\\) Strong assumption of no omitted variable bias (small If \\(\\lambda = 0\\), then \\(cor(X_i, \\epsilon_i) = 0\\) If \\(\\lambda = 1\\), then \\(cor(X_i, \\epsilon_i) = cor(X_i, C_i \\gamma)\\) We typically examine \\(\\lambda \\in (0, 1)\\) # remotes::install_github(&quot;bvkrauth/rcr/r/rcrbounds&quot;) library(rcrbounds) # rcrbounds::install_rcrpy() data(&quot;ChickWeight&quot;) rcr_res &lt;- rcrbounds::rcr(weight ~ Time | Diet, ChickWeight, rc_range = c(0, 10)) rcr_res #&gt; #&gt; Call: #&gt; rcrbounds::rcr(formula = weight ~ Time | Diet, data = ChickWeight, #&gt; rc_range = c(0, 10)) #&gt; #&gt; Coefficients: #&gt; rcInf effectInf rc0 effectL effectH #&gt; 34.676505 71.989336 34.741955 7.447713 8.750492 summary(rcr_res) #&gt; #&gt; Call: #&gt; rcrbounds::rcr(formula = weight ~ Time | Diet, data = ChickWeight, #&gt; rc_range = c(0, 10)) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; rcInf 34.676505 50.1295005 0.6917385 4.891016e-01 #&gt; effectInf 71.989336 112.5711682 0.6395007 5.224973e-01 #&gt; rc0 34.741955 58.7169195 0.5916856 5.540611e-01 #&gt; effectL 7.447713 2.4276246 3.0679014 2.155677e-03 #&gt; effectH 8.750492 0.2607671 33.5567355 7.180405e-247 #&gt; --- #&gt; conservative confidence interval: #&gt; 2.5 % 97.5 % #&gt; effect 2.689656 9.261586 # hypothesis test for the coefficient rcrbounds::effect_test(rcr_res, h0 = 0) #&gt; [1] 0.001234233 plot(rcr_res) 31.2.3 Coefficient-stability Bounds Developed by Oster (2019) Assess robustness to omitted variable bias by observing: Changes in the coefficient of interest Shifts in model \\(R^2\\) Refer Masten and Poirier (2022) for reverse sign problem. References "],["interrupted-time-series.html", "Chapter 32 Interrupted Time Series", " Chapter 32 Interrupted Time Series Regression Discontinuity in Time Control for Seasonable trends Concurrent events Pros (Penfold and Zhang 2013) control for long-term trends Cons Min of 8 data points before and 8 after an intervention Multiple events hard to distinguish Notes: For subgroup analysis (heterogeneity in effect size), see (Harper and Bruckner 2017) To interpret with control variables, see (Bottomley, Scott, and Isham 2019) Interrupted time series should be used when longitudinal data (outcome over time - observations before and after the intervention) full population was affected at one specific point in time (or can be stacked based on intervention) In each ITS framework, there can be 4 possible scenarios of outcome after an intervention No effects Immediate effect Sustained (long-term) effect (smooth) Both immediate and sustained effect \\[ Y = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 P + \\epsilon \\] where \\(Y\\) is the outcome variable \\(\\beta_0\\) is the baseline level of the outcome \\(T\\) is the time variable (e.g., days, weeks, etc.) passed from the start of the observation period \\(\\beta_1\\) is the slope of the line before the intervention \\(D\\) is the treatment variable where \\(1\\) is after the intervention and \\(0\\) is before the intervention. \\(\\beta_2\\) is the immediate effect after the intervention \\(P\\) is the time variable indicating time passed since the intervention (before the intervention, the value is set to 0) (to examine the sustained effect). \\(\\beta_3\\) is the sustained effect = difference between the slope of the line prior to the intervention and the slope of the line subsequent to the intervention Example Create a fictitious dataset where we know the true data generating process \\[ Outcome = 10 \\times time + 20 \\times treatment + 25 \\times timesincetreatment + noise \\] # number of days n = 365 # intervention at day interven = 200 # time index from 1 to 365 time = c(1:n) # treatment variable: before internvation = day 1 to 200, # after intervention = day 201 to 365 treatment = c(rep(0, interven), rep(1, n - interven)) # time since treatment timesincetreat = c(rep(0, interven), c(1:(n - interven))) # outcome outcome = 10 + 15 * time + 20 * treatment + 25 * timesincetreat + rnorm(n, mean = 0, sd = 1) df = data.frame(outcome, time, treatment, timesincetreat) head(df, 10) #&gt; outcome time treatment timesincetreat #&gt; 1 25.79832 1 0 0 #&gt; 2 42.08680 2 0 0 #&gt; 3 55.55952 3 0 0 #&gt; 4 68.54228 4 0 0 #&gt; 5 82.75827 5 0 0 #&gt; 6 100.82867 6 0 0 #&gt; 7 114.41550 7 0 0 #&gt; 8 131.06942 8 0 0 #&gt; 9 145.22532 9 0 0 #&gt; 10 161.08298 10 0 0 Visualize plot(df$time, df$outcome) # intervention date abline(v = interven, col = &quot;blue&quot;) # regression line ts &lt;- lm(outcome ~ time + treatment + timesincetreat, data = df) lines(df$time, ts$fitted.values, col = &quot;red&quot;) summary(ts) #&gt; #&gt; Call: #&gt; lm(formula = outcome ~ time + treatment + timesincetreat, data = df) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.58812 -0.67771 0.03995 0.63623 2.82507 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 9.705206 0.135820 71.46 &lt;2e-16 *** #&gt; time 15.002674 0.001172 12802.61 &lt;2e-16 *** #&gt; treatment 19.852727 0.201416 98.57 &lt;2e-16 *** #&gt; timesincetreat 24.996424 0.001954 12791.27 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.9568 on 361 degrees of freedom #&gt; Multiple R-squared: 1, Adjusted R-squared: 1 #&gt; F-statistic: 1.042e+09 on 3 and 361 DF, p-value: &lt; 2.2e-16 Interpretation Time coefficient shows before-intervention outcome trend. Positive and significant, indicating a rising trend. Every day adds 15 points. The treatment coefficient shows the immediate increase in outcome. Immediate effect is positive and significant, increasing outcome by 20 points. The time since treatment coefficient reflects a change in trend subsequent to the intervention. The sustained effect is positive and statistically significant, showing that the outcome increases by 25 points per day after the intervention. See Lee Rodgers, Beasley, and Schuelke (2014) for suggestions Plot of counterfactual # treatment prediction pred &lt;- predict(ts, df) # counterfactual dataset new_df &lt;- as.data.frame(cbind( time = time, # treatment = 0 means counterfactual treatment = rep(0, n), # time since treatment = 0 means counterfactual timesincetreat = rep(0) )) # counterfactual predictions pred_cf &lt;- predict(ts, new_df) # plot plot( outcome, col = gray(0.2, 0.2), pch = 19, xlim = c(1,365), ylim = c(0, 10000), xlab = &quot;xlab&quot;, ylab = &quot;ylab&quot; ) # regression line before treatment lines(rep(1:interven), pred[1:interven], col = &quot;blue&quot;, lwd = 3) # regression line after treatment lines(rep((interven + 1):n), pred[(interven + 1):n], col = &quot;blue&quot;, lwd = 3) # regression line after treatment (counterfactual) lines( rep(interven:n), pred_cf[(interven):n], col = &quot;yellow&quot;, lwd = 3, lty = 5 ) abline(v = interven, col = &quot;red&quot;, lty = 2) Possible threats to the validity of interrupted time series analysis (Baicker and Svoronos 2019) Delayed effects (Rodgers, John, and Coleman 2005) (may have to make assess some time after the intervention - do not assess the immediate dates). Other confounding events Linden (2017) Intervention is introduced but later withdrawn (Linden 2015) Autocorrelation (for every time series data): might cause underestimation in the standard errors (i.e., overestimating the statistical significance of the treatment effect) Regression to the mean: after a the short-term shock to the outcome, individuals can revert back to their initial states. Selection bias: only certain individuals are affected by the treatment (could use a Multiple Groups). References "],["autocorrelation.html", "32.1 Autocorrelation", " 32.1 Autocorrelation Assess autocorrelation from residual # simple regression on time simple_ts &lt;- lm(outcome ~ time, data = df) plot(resid(simple_ts)) # alternatively acf(resid(simple_ts)) This is not the best example since I created this dataset. But when residuals do have autocorrelation, you should not see any patterns (i.e., points should be randomly distributed on the plot) To formally test for autocorrelation, we can use the Durbin-Watson test lmtest::dwtest(df$outcome ~ df$time) #&gt; #&gt; Durbin-Watson test #&gt; #&gt; data: df$outcome ~ df$time #&gt; DW = 0.00037607, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true autocorrelation is greater than 0 From the p-value, we know that there is autocorrelation in the time series A solution to this problem is to use more advanced time series analysis (e.g., ARIMA - coming up in the book) to adjust for seasonality and other dependency. forecast::auto.arima(df$outcome, xreg = as.matrix(df[,-1])) #&gt; Series: df$outcome #&gt; Regression with ARIMA(3,0,2) errors #&gt; #&gt; Coefficients: #&gt; ar1 ar2 ar3 ma1 ma2 intercept time treatment #&gt; 0.1904 -0.9672 0.0925 -0.1327 0.9557 9.7122 15.0026 19.8588 #&gt; s.e. 0.0693 0.0356 0.0543 0.0467 0.0338 0.1446 0.0012 0.2141 #&gt; timesincetreat #&gt; 24.9965 #&gt; s.e. 0.0021 #&gt; #&gt; sigma^2 = 0.91: log likelihood = -496.34 #&gt; AIC=1012.67 AICc=1013.3 BIC=1051.67 "],["multiple-groups.html", "32.2 Multiple Groups", " 32.2 Multiple Groups When you suspect that you might have confounding events or selection bias, you can add a control group that did not experience the treatment (very much similar to Difference-in-differences) The model then becomes \\[ \\begin{aligned} Y = \\beta_0 &amp;+ \\beta_1 time+ \\beta_2 treatment +\\beta_3 \\times timesincetreat \\\\ &amp;+\\beta_4 group + \\beta_5 group \\times time + \\beta_6 group \\times treatment \\\\ &amp;+ \\beta_7 group \\times timesincetreat \\end{aligned} \\] where Group = 1 when the observation is under treatment and 0 under control \\(\\beta_4\\) = baseline difference between the treatment and control group \\(\\beta_5\\) = slope difference between the treatment and control group before treatment \\(\\beta_6\\) = baseline difference between the treatment and control group associated with the treatment. \\(\\beta_7\\) = difference between the sustained effect of the treatment and control group after the treatment. "],["endogeneity.html", "Chapter 33 Endogeneity", " Chapter 33 Endogeneity Refresher A general model framework \\[ \\mathbf{Y = X \\beta + \\epsilon} \\] where \\(\\mathbf{Y} = n \\times 1\\) \\(\\mathbf{X} = n \\times k\\) \\(\\beta = k \\times 1\\) \\(\\epsilon = n \\times 1\\) Then, OLS estimates of coefficients are \\[ \\begin{aligned} \\hat{\\beta}_{OLS} &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}(\\mathbf{X}&#39;\\mathbf{Y}) \\\\ &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}(\\mathbf{X}&#39;(\\mathbf{X \\beta + \\epsilon})) \\\\ &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1} (\\mathbf{X}&#39;\\mathbf{X}) \\beta + (\\mathbf{X}&#39;\\mathbf{X})^{-1} (\\mathbf{X}&#39;\\mathbf{\\epsilon}) \\\\ \\hat{\\beta}_{OLS} &amp; \\to \\beta + (\\mathbf{X}&#39;\\mathbf{X})^{-1} (\\mathbf{X}&#39;\\mathbf{\\epsilon}) \\end{aligned} \\] To have unbiased estimates, we have to get rid of the second part \\((\\mathbf{X}&#39;\\mathbf{X})^{-1} (\\mathbf{X}&#39;\\mathbf{\\epsilon})\\) There are 2 conditions to achieve unbiased estimates: \\(E(\\epsilon |X) = 0\\) (This is easy, putting an intercept can solve this issue) \\(Cov(\\mathbf{X}, \\epsilon) = 0\\) (This is the hard part) We only care about omitted variable Usually, the problem will stem Omitted Variables Bias, but we only care about omitted variable bias when Omitted variables correlate with the variables we care about (\\(X\\)). If OMV does not correlate with \\(X\\), we don’t care, and random assignment makes this correlation goes to 0) Omitted variables correlates with outcome/ dependent variable There are more types of endogeneity listed below. Types of endogeneity (See Hill et al. (2021) for a review in management): Endogenous Treatment Omitted Variables Bias Motivation Ability/talent Self-selection Feedback Effect (Simultaneity): also known as bidirectionality Reverse Causality: Subtle difference from Simultaneity: Technically, two variables affect each other sequentially, but in a big enough time frame, (e.g., monthly, or yearly), our coefficient will be biased just like simultaneity. Measurement Error Endogenous Sample Selection To deal with this problem, we have a toolbox (that has been mentioned in previous chapter 18) Using control variables in regression is a “selection on observables” identification strategy. In other words, if you believe you have an omitted variable, and you can measure it, including it in the regression model solves your problem. These uninterested variables are called control variables in your model. However, this is rarely the case (because the problem is we don’t have their measurements). Hence, we need more elaborate methods: Endogenous Treatment Endogenous Sample Selection Before we get to methods that deal with bias arises from omitted variables, we consider cases where we do have measurements of a variable, but there is measurement error (bias). References "],["endogenous-treatment.html", "33.1 Endogenous Treatment", " 33.1 Endogenous Treatment 33.1.1 Measurement Error Data error can stem from Coding errors Reporting errors Two forms of measurement error: Random (stochastic) (indeterminate error) (Classical Measurement Errors): noise or measurement errors do not show up in a consistent or predictable way. Systematic (determinate error) (Non-classical Measurement Errors): When measurement error is consistent and predictable across observations. Instrument errors (e.g., faulty scale) -&gt; calibration or adjustment Method errors (e.g., sampling errors) -&gt; better method development + study design Human errors (e.g., judgement) Usually the systematic measurement error is a bigger issue because it introduces “bias” into our estimates, while random error introduces noise into our estimates Noise -&gt; regression estimate to 0 Bias -&gt; can pull estimate to upward or downward. 33.1.1.1 Classical Measurement Errors 33.1.1.1.1 Right-hand side Right-hand side measurement error: When the measurement is in the covariates, then we have the endogeneity problem. Say you know the true model is \\[ Y_i = \\beta_0 + \\beta_1 X_i + u_i \\] But you don’t observe \\(X_i\\), but you observe \\[ \\tilde{X}_i = X_i + e_i \\] which is known as classical measurement errors where we assume \\(e_i\\) is uncorrelated with \\(X_i\\) (i.e., \\(E(X_i e_i) = 0\\)) Then, when you estimate your observed variables, you have (substitute \\(X_i\\) with \\(\\tilde{X}_i - e_i\\) ): \\[ \\begin{aligned} Y_i &amp;= \\beta_0 + \\beta_1 (\\tilde{X}_i - e_i)+ u_i \\\\ &amp;= \\beta_0 + \\beta_1 \\tilde{X}_i + u_i - \\beta_1 e_i \\\\ &amp;= \\beta_0 + \\beta_1 \\tilde{X}_i + v_i \\end{aligned} \\] In words, the measurement error in \\(X_i\\) is now a part of the error term in the regression equation \\(v_i\\). Hence, we have an endogeneity bias. Endogeneity arises when \\[ \\begin{aligned} E(\\tilde{X}_i v_i) &amp;= E((X_i + e_i )(u_i - \\beta_1 e_i)) \\\\ &amp;= -\\beta_1 Var(e_i) \\neq 0 \\end{aligned} \\] Since \\(\\tilde{X}_i\\) and \\(e_i\\) are positively correlated, then it leads to a negative bias in \\(\\hat{\\beta}_1\\) if the true \\(\\beta_1\\) is positive a positive bias if \\(\\beta_1\\) is negative In other words, measurement errors cause attenuation bias, which inter turn pushes the coefficient towards 0 As \\(Var(e_i)\\) increases or \\(\\frac{Var(e_i)}{Var(\\tilde{X})} \\to 1\\) then \\(e_i\\) is a random (noise) and \\(\\beta_1 \\to 0\\) (random variable \\(\\tilde{X}\\) should not have any relation to \\(Y_i\\)) Technical note: The size of the bias in the OLS-estimator is \\[ \\hat{\\beta}_{OLS} = \\frac{ cov(\\tilde{X}, Y)}{var(\\tilde{X})} = \\frac{cov(X + e, \\beta X + u)}{var(X + e)} \\] then \\[ plim \\hat{\\beta}_{OLS} = \\beta \\frac{\\sigma^2_X}{\\sigma^2_X + \\sigma^2_e} = \\beta \\lambda \\] where \\(\\lambda\\) is reliability or signal-to-total variance ratio or attenuation factor Reliability affect the extent to which measurement error attenuates \\(\\hat{\\beta}\\). The attenuation bias is \\[ \\hat{\\beta}_{OLS} - \\beta = -(1-\\lambda)\\beta \\] Thus, \\(\\hat{\\beta}_{OLS} &lt; \\beta\\) (unless \\(\\lambda = 1\\), in which case we don’t even have measurement error). Note: Data transformation worsen (magnify) the measurement error \\[ y= \\beta x + \\gamma x^2 + \\epsilon \\] then, the attenuation factor for \\(\\hat{\\gamma}\\) is the square of the attenuation factor for \\(\\hat{\\beta}\\) (i.e., \\(\\lambda_{\\hat{\\gamma}} = \\lambda_{\\hat{\\beta}}^2\\)) Adding covariates increases attenuation bias To fix classical measurement error problem, we can Find estimates of either \\(\\sigma^2_X, \\sigma^2_\\epsilon\\) or \\(\\lambda\\) from validation studies, or survey data. Endogenous Treatment Use instrument \\(Z\\) correlated with \\(X\\) but uncorrelated with \\(\\epsilon\\) Abandon your project 33.1.1.1.2 Left-hand side When the measurement is in the outcome variable, econometricians or causal scientists do not care because they still have an unbiased estimate of the coefficients (the zero conditional mean assumption is not violated, hence we don’t have endogeneity). However, statisticians might care because it might inflate our uncertainty in the coefficient estimates (i.e., higher standard errors). \\[ \\tilde{Y} = Y + v \\] then the model you estimate is \\[ \\tilde{Y} = \\beta X + u + v \\] Since \\(v\\) is uncorrelated with \\(X\\), then \\(\\hat{\\beta}\\) is consistently estimated by OLS If we have measurement error in \\(Y_i\\), it will pass through \\(\\beta_1\\) and go to \\(u_i\\) 33.1.1.2 Non-classical Measurement Errors Relaxing the assumption that \\(X\\) and \\(\\epsilon\\) are uncorrelated Recall the true model we have true estimate is \\[ \\hat{\\beta} = \\frac{cov(X + \\epsilon, \\beta X + u)}{var(X + \\epsilon)} \\] then without the above assumption, we have \\[ \\begin{aligned} plim \\hat{\\beta} &amp;= \\frac{\\beta (\\sigma^2_X + \\sigma_{X \\epsilon})}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}} \\\\ &amp;= (1 - \\frac{\\sigma^2_{\\epsilon} + \\sigma_{X \\epsilon}}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}}) \\beta \\\\ &amp;= (1 - b_{\\epsilon \\tilde{X}}) \\beta \\end{aligned} \\] where \\(b_{\\epsilon \\tilde{X}}\\) is the covariance between \\(\\tilde{X}\\) and \\(\\epsilon\\) (also the regression coefficient of a regression of \\(\\epsilon\\) on \\(\\tilde{X}\\)) Hence, the Classical Measurement Errors is just a special case of Non-classical Measurement Errors where \\(b_{\\epsilon \\tilde{X}} = 1 - \\lambda\\) So when \\(\\sigma_{X \\epsilon} = 0\\) (Classical Measurement Errors), increasing this covariance \\(b_{\\epsilon \\tilde{X}}\\) increases the covariance increases the attenuation factor if more than half of the variance in \\(\\tilde{X}\\) is measurement error, and decreases the attenuation factor otherwise. This is also known as mean reverting measurement error Bound, Brown, and Mathiowetz (2001) A general framework for both right-hand side and left-hand side measurement error is (Bound, Brown, and Mathiowetz 2001): consider the true model \\[ \\mathbf{Y = X \\beta + \\epsilon} \\] then \\[ \\begin{aligned} \\hat{\\beta} &amp;= \\mathbf{(\\tilde{X}&#39; \\tilde{X})^{-1}\\tilde{X} \\tilde{Y}} \\\\ &amp;= \\mathbf{(\\tilde{X}&#39; \\tilde{X})^{-1} \\tilde{X}&#39; (\\tilde{X} \\beta - U \\beta + v + \\epsilon )} \\\\ &amp;= \\mathbf{\\beta + (\\tilde{X}&#39; \\tilde{X})^{-1} \\tilde{X}&#39; (-U \\beta + v + \\epsilon)} \\\\ plim \\hat{\\beta} &amp;= \\beta + plim (\\tilde{X}&#39; \\tilde{X})^{-1} \\tilde{X}&#39; ( -U\\beta + v) \\\\ &amp;= \\beta + plim (\\tilde{X}&#39; \\tilde{X})^{-1} \\tilde{X}&#39; W \\left[ \\begin{array} {c} - \\beta \\\\ 1 \\end{array} \\right] \\end{aligned} \\] Since we collect the measurement errors in a matrix \\(W = [U|v]\\), then \\[ ( -U\\beta + v) = W \\left[ \\begin{array} {c} - \\beta \\\\ 1 \\end{array} \\right] \\] Hence, in general, biases in the coefficients \\(\\beta\\) are regression coefficients from regressing the measurement errors on the mis-measured \\(\\tilde{X}\\) Notes: Instrumental Variable can help fix this problem There can also be measurement error in dummy variables and you can still use Instrumental Variable to fix it. 33.1.1.3 Solution to Measurement Errors 33.1.1.3.1 Correlation \\[ \\begin{aligned} P(\\rho | data) &amp;= \\frac{P(data|\\rho)P(\\rho)}{P(data)} \\\\ \\text{Posterior Probability} &amp;\\propto \\text{Likelihood} \\times \\text{Prior Probability} \\end{aligned} \\] where \\(\\rho\\) is a correlation coefficient \\(P(data|\\rho)\\) is the likelihood function evaluated at \\(\\rho\\) \\(P(\\rho)\\) prior probability \\(P(data)\\) is the normalizing constant With sample correlation coefficient \\(r\\): \\[ r = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} \\] Then the posterior density approximation of \\(\\rho\\) is (Schisterman et al. 2003, 3) \\[ P(\\rho| x, y) \\propto P(\\rho) \\frac{(1- \\rho^2)^{(n-1)/2}}{(1- \\rho \\times r)^{n - (3/2)}} \\] where \\(\\rho = \\tanh \\xi\\) where \\(\\xi \\sim N(z, 1/n)\\) \\(r = \\tanh z\\) Then the posterior density follow a normal distribution where Mean \\[ \\mu_{posterior} = \\sigma^2_{posterior} \\times (n_{prior} \\times \\tanh^{-1} r_{prior}+ n_{likelihood} \\times \\tanh^{-1} r_{likelihood}) \\] variance \\[ \\sigma^2_{posterior} = \\frac{1}{n_{prior} + n_{Likelihood}} \\] To simplify the integration process, we choose prior that is \\[ P(\\rho) \\propto (1 - \\rho^2)^c \\] where \\(c\\) is the weight the prior will have in estimation (i.e., \\(c = 0\\) if no prior info, hence \\(P(\\rho) \\propto 1\\)) Example: Current study: \\(r_{xy} = 0.5, n = 200\\) Previous study: \\(r_{xy} = 0.2765, (n=50205)\\) Combining two, we have the posterior following a normal distribution with the variance of \\[ \\sigma^2_{posterior} = \\frac{1}{n_{prior} + n_{Likelihood}} = \\frac{1}{200 + 50205} = 0.0000198393 \\] Mean \\[ \\begin{aligned} \\mu_{Posterior} &amp;= \\sigma^2_{Posterior} \\times (n_{prior} \\times \\tanh^{-1} r_{prior}+ n_{likelihood} \\times \\tanh^{-1} r_{likelihood}) \\\\ &amp;= 0.0000198393 \\times (50205 \\times \\tanh^{-1} 0.2765 + 200 \\times \\tanh^{-1}0.5 )\\\\ &amp;= 0.2849415 \\end{aligned} \\] Hence, \\(Posterior \\sim N(0.691, 0.0009)\\), which means the correlation coefficient is \\(\\tanh(0.691) = 0.598\\) and 95% CI is \\[ \\mu_{posterior} \\pm 1.96 \\times \\sqrt{\\sigma^2_{Posterior}} = 0.2849415 \\pm 1.96 \\times (0.0000198393)^{1/2} = (0.2762115, 0.2936714) \\] Hence, the interval for posterior \\(\\rho\\) is \\((0.2693952, 0.2855105)\\) If future authors suspect that they have Large sampling variation Measurement error in either measures in the correlation, which attenuates the relationship between the two variables Applying this Bayesian correction can give them a better estimate of the correlation between the two. To implement this calculation in R, see below n_new &lt;- 200 r_new &lt;- 0.5 alpha &lt;- 0.05 update_correlation &lt;- function(n_new, r_new, alpha) { n_meta &lt;- 50205 r_meta &lt;- 0.2765 # Variance var_xi &lt;- 1 / (n_new + n_meta) format(var_xi, scientific = FALSE) # mean mu_xi &lt;- var_xi * (n_meta * atanh(r_meta) + n_new * (atanh(r_new))) format(mu_xi, scientific = FALSE) # confidence interval upper_xi &lt;- mu_xi + qnorm(1 - alpha / 2) * sqrt(var_xi) lower_xi &lt;- mu_xi - qnorm(1 - alpha / 2) * sqrt(var_xi) # rho mean_rho &lt;- tanh(mu_xi) upper_rho &lt;- tanh(upper_xi) lower_rho &lt;- tanh(lower_xi) # return a list return( list( &quot;mu_xi&quot; = mu_xi, &quot;var_xi&quot; = var_xi, &quot;upper_xi&quot; = upper_xi, &quot;lower_xi&quot; = lower_xi, &quot;mean_rho&quot; = mean_rho, &quot;upper_rho&quot; = upper_rho, &quot;lower_rho&quot; = lower_rho ) ) } # Old confidence interval r_new + qnorm(1 - alpha / 2) * sqrt(1/n_new) #&gt; [1] 0.6385904 r_new - qnorm(1 - alpha / 2) * sqrt(1/n_new) #&gt; [1] 0.3614096 testing = update_correlation(n_new = n_new, r_new = r_new, alpha = alpha) # Updated rho testing$mean_rho #&gt; [1] 0.2774723 # Updated confidence interval testing$upper_rho #&gt; [1] 0.2855105 testing$lower_rho #&gt; [1] 0.2693952 33.1.2 Simultaneity When independent variables (\\(X\\)’s) are jointly determined with the dependent variable \\(Y\\), typically through an equilibrium mechanism, violates the second condition for causality (i.e., temporal order). Examples: quantity and price by demand and supply, investment and productivity, sales and advertisement General Simultaneous (Structural) Equations \\[ \\begin{aligned} Y_i &amp;= \\beta_0 + \\beta_1 X_i + u_i \\\\ X_i &amp;= \\alpha_0 + \\alpha_1 Y_i + v_i \\end{aligned} \\] Hence, the solutions are \\[ \\begin{aligned} Y_i &amp;= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 v_i + u_i}{1 - \\alpha_1 \\beta_1} \\\\ X_i &amp;= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1} \\end{aligned} \\] If we run only one regression, we will have biased estimators (because of simultaneity bias): \\[ \\begin{aligned} Cov(X_i, u_i) &amp;= Cov(\\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}, u_i) \\\\ &amp;= \\frac{\\alpha_1}{1- \\alpha_1 \\beta_1} Var(u_i) \\end{aligned} \\] In an even more general model \\[ \\begin{cases} Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + u_i \\\\ X_i = \\alpha_0 + \\alpha_1 Y_i + \\alpha_2 Z_i + v_i \\end{cases} \\] where \\(X_i, Y_i\\) are endogenous variables determined within the system \\(T_i, Z_i\\) are exogenous variables Then, the reduced form of the model is \\[ \\begin{cases} \\begin{aligned} Y_i &amp;= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 \\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{u}_i \\\\ &amp;= B_0 + B_1 Z_i + B_2 T_i + \\tilde{u}_i \\end{aligned} \\\\ \\begin{aligned} X_i &amp;= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\alpha_1\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{v}_i \\\\ &amp;= A_0 + A_1 Z_i + A_2 T_i + \\tilde{v}_i \\end{aligned} \\end{cases} \\] Then, now we can get consistent estimates of the reduced form parameters And to get the original parameter estimates \\[ \\begin{aligned} \\frac{B_1}{A_1} &amp;= \\beta_1 \\\\ B_2 (1 - \\frac{B_1 A_2}{A_1B_2}) &amp;= \\beta_2 \\\\ \\frac{A_2}{B_2} &amp;= \\alpha_1 \\\\ A_1 (1 - \\frac{B_1 A_2}{A_1 B_2}) &amp;= \\alpha_2 \\end{aligned} \\] Rules for Identification Order Condition (necessary but not sufficient) \\[ K - k \\ge m - 1 \\] where \\(M\\) = number of endogenous variables in the model K = number of exogenous variables int he model \\(m\\) = number of endogenous variables in a given \\(k\\) = is the number of exogenous variables in a given equation This is actually the general framework for instrumental variables 33.1.3 Endogenous Treatment Solutions Using the OLS estimates as a reference point library(AER) library(REndo) set.seed(421) data(&quot;CASchools&quot;) school &lt;- CASchools school$stratio &lt;- with(CASchools, students / teachers) m1.ols &lt;- lm(read ~ stratio + english + lunch + grades + income + calworks + county, data = school) summary(m1.ols)$coefficients[1:7,] #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 683.45305948 9.56214469 71.4748711 3.011667e-218 #&gt; stratio -0.30035544 0.25797023 -1.1643027 2.450536e-01 #&gt; english -0.20550107 0.03765408 -5.4576041 8.871666e-08 #&gt; lunch -0.38684059 0.03700982 -10.4523759 1.427370e-22 #&gt; gradesKK-08 -1.91291321 1.35865394 -1.4079474 1.599886e-01 #&gt; income 0.71615378 0.09832843 7.2832829 1.986712e-12 #&gt; calworks -0.05273312 0.06154758 -0.8567863 3.921191e-01 33.1.3.1 Instrumental Variable A3a requires \\(\\epsilon_i\\) to be uncorrelated with \\(\\mathbf{x}_i\\) Assume A1 , A2, A5 \\[ plim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x_i&#39;x_i})]^{-1}E(\\mathbf{x_i&#39;}\\epsilon_i) \\] A3a is the weakest assumption needed for OLS to be consistent A3 fails when \\(x_{ik}\\) is correlated with \\(\\epsilon_i\\) Omitted Variables Bias: \\(\\epsilon_i\\) includes any other factors that may influence the dependent variable (linearly) Simultaneity Demand and prices are simultaneously determined. Endogenous Sample Selection we did not have iid sample Measurement Error Note Omitted Variable: an omitted variable is a variable, omitted from the model (but is in the \\(\\epsilon_i\\)) and unobserved has predictive power towards the outcome. Omitted Variable Bias: is the bias (and inconsistency when looking at large sample properties) of the OLS estimator when the omitted variable. We cam have both positive and negative selection bias (it depends on what our story is) The structural equation is used to emphasize that we are interested understanding a causal relationship \\[ y_{i1} = \\beta_0 + \\mathbf{z}_i1 \\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\] where \\(y_{it}\\) is the outcome variable (inherently correlated with \\(\\epsilon_i\\)) \\(y_{i2}\\) is the endogenous covariate (presumed to be correlated with \\(\\epsilon_i\\)) \\(\\beta_1\\) represents the causal effect of \\(y_{i2}\\) on \\(y_{i1}\\) \\(\\mathbf{z}_{i1}\\) is exogenous controls (uncorrelated with \\(\\epsilon_i\\)) (\\(E(z_{1i}&#39;\\epsilon_i) = 0\\)) OLS is an inconsistent estimator of the causal effect \\(\\beta_2\\) If there was no endogeneity \\(E(y_{i2}&#39;\\epsilon_i) = 0\\) the exogenous variation in \\(y_{i2}\\) is what identifies the causal effect If there is endogeneity Any wiggle in \\(y_{i2}\\) will shift simultaneously with \\(\\epsilon_i\\) \\[ plim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x&#39;_ix_i})]^{-1}E(\\mathbf{x&#39;_i}\\epsilon_i) \\] where \\(\\beta\\) is the causal effect \\([E(\\mathbf{x&#39;_ix_i})]^{-1}E(\\mathbf{x&#39;_i}\\epsilon_i)\\) is the endogenous effect Hence \\(\\hat{\\beta}_{OLS}\\) can be either more positive and negative than the true causal effect. Motivation for Two Stage Least Squares (2SLS) \\[ y_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\] We want to understand how movement in \\(y_{i2}\\) effects movement in \\(y_{i1}\\), but whenever we move \\(y_{i2}\\), \\(\\epsilon_i\\) also moves. Solution We need a way to move \\(y_{i2}\\) independently of \\(\\epsilon_i\\), then we can analyze the response in \\(y_{i1}\\) as a causal effect Find an instrumental variable(s) \\(z_{i2}\\) Instrument Relevance: when** \\(z_{i2}\\) moves then \\(y_{i2}\\) also moves Instrument Exogeneity: when \\(z_{i2}\\) moves then \\(\\epsilon_i\\) does not move. \\(z_{i2}\\) is the exogenous variation that identifies the causal effect \\(\\beta_2\\) Finding an Instrumental variable: Random Assignment: + Effect of class size on educational outcomes: instrument is initial random Relation’s Choice + Effect of Education on Fertility: instrument is parent’s educational level Eligibility + Trade-off between IRA and 401K retirement savings: instrument is 401k eligibility Example Return to College education is correlated with ability - endogenous Near 4year as an instrument Instrument Relevance: when near moves then education also moves Instrument Exogeneity: when near moves then \\(\\epsilon_i\\) does not move. Other potential instruments; near a 2-year college. Parent’s Education. Owning Library Card \\[ y_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\] First Stage (Reduced Form) Equation: \\[ y_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i \\] where \\(\\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2}\\) is exogenous variation \\(v_i\\) is endogenous variation This is called a reduced form equation Not interested in the causal interpretation of \\(\\pi_1\\) or \\(\\pi_2\\) A linear projection of \\(z_{i1}\\) and \\(z_{i2}\\) on \\(y_{i2}\\) (simple correlations) The projections \\(\\pi_1\\) and \\(\\pi_2\\) guarantee that \\(E(z_{i1}&#39;v_i)=0\\) and \\(E(z_{i2}&#39;v_i)=0\\) Instrumental variable \\(z_{i2}\\) Instrument Relevance: \\(\\pi_2 \\neq 0\\) Instrument Exogeneity: \\(E(\\mathbf{z_{i2}\\epsilon_i})=0\\) Moving only the exogenous part of \\(y_i2\\) is moving \\[ \\tilde{y}_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1 + z_{i2}\\pi_2} \\] two Stage Least Squares (2SLS) \\[ y_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ y_{i2}\\beta_2 + \\epsilon_i \\] \\[ y_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i} \\] Equivalently, \\[\\begin{equation} \\begin{split} y_{i1} = \\beta_0 + \\mathbf{z_{i1}}\\beta_1 + \\tilde{y}_{i2}\\beta_2 + u_i \\end{split} \\tag{33.1} \\end{equation}\\] where \\(\\tilde{y}_{i2} =\\pi_0 + \\mathbf{z_{i2}\\pi_2}\\) \\(u_i = v_i \\beta_2+ \\epsilon_i\\) The (33.1) holds for A1, A5 A2 holds if the instrument is relevant \\(\\pi_2 \\neq 0\\) + \\(y_{i1} = \\beta_0 + \\mathbf{z_{i1}\\beta_1 + (\\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2)}\\beta_2 + u_i\\) A3a holds if the instrument is exogenous \\(E(\\mathbf{z}_{i2}\\epsilon_i)=0\\) \\[ \\begin{aligned} E(\\tilde{y}_{i2}&#39;u_i) &amp;= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})(v_i\\beta_2 + \\epsilon_i)) \\\\ &amp;= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})( \\epsilon_i)) \\\\ &amp;= E(\\epsilon_i)\\pi_0 + E(\\epsilon_iz_{i1})\\pi_1 + E(\\epsilon_iz_{i2}) \\\\ &amp;=0 \\end{aligned} \\] Hence, (33.1) is consistent The 2SLS Estimator 1. Estimate the first stage using OLS \\[ y_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i} \\] and obtained estimated value \\(\\hat{y}_{i2}\\) Estimate the altered equation using OLS \\[ y_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ \\hat{y}_{i2}\\beta_2 + \\epsilon_i \\] Properties of the 2SLS Estimator Under A1, A2, A3a (for \\(z_{i1}\\)), A5 and if the instrument satisfies the following two conditions, + Instrument Relevance: \\(\\pi_2 \\neq 0\\) + Instrument Exogeneity: \\(E(\\mathbf{z}_{i2}&#39;\\epsilon_i) = 0\\) then the 2SLS estimator is consistent Can handle more than one endogenous variable and more than one instrumental variable \\[ \\begin{aligned} y_{i1} &amp;= \\beta_0 + z_{i1}\\beta_1 + y_{i2}\\beta_2 + y_{i3}\\beta_3 + \\epsilon_i \\\\ y_{i2} &amp;= \\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2 + z_{i3}\\pi_3 + z_{i4}\\pi_4 + v_{i2} \\\\ y_{i3} &amp;= \\gamma_0 + z_{i1}\\gamma_1 + z_{i2}\\gamma_2 + z_{i3}\\gamma_3 + z_{i4}\\gamma_4 + v_{i3} \\end{aligned} \\] + **IV estimator**: one endogenous variable with a single instrument + **2SLS estimator**: one endogenous variable with multiple instruments + **GMM estimator**: multiple endogenous variables with multiple instruments Standard errors produced in the second step are not correct Because we do not know \\(\\tilde{y}\\) perfectly and need to estimate it in the firs step, we are introducing additional variation We did not have this problem with FGLS because “the first stage was orthogonal to the second stage.” This is generally not true for most multi-step procedure. If A4 does not hold, need to report robust standard errors. 2SLS is less efficient than OLS and will always have larger standard errors. First, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) &gt; Var(\\epsilon_i)\\) Second, \\(\\hat{y}_{i2}\\) is generally highly collinear with \\(\\mathbf{z}_{i1}\\) The number of instruments need to be at least as many or more the number of endogenous variables. Note 2SLS can be combined with FGLS to make the estimator more efficient: You have the same first-stage, and in the second-stage, instead of using OLS, you can use FLGS with the weight matrix \\(\\hat{w}\\) Generalized Method of Moments can be more efficient than 2SLS. In the second-stage of 2SLS, you can also use MLE, but then you are making assumption on the distribution of the outcome variable, the endogenous variable, and their relationship (joint distribution). 33.1.3.1.1 Testing Assumptions Endogeneity Test: Is \\(y_{i2}\\) truly endogenous (i.e., can we just use OLS instead of 2SLS)? Exogeneity (Cannot always test and when you can it might not be informative) Relevancy (need to avoid “weak instruments”) 33.1.3.1.1.1 Endogeneity Test 2SLS is generally so inefficient that we may prefer OLS if there is not much endogeneity Biased but inefficient vs efficient but biased Want a sense of “how endogenous” \\(y_{i2}\\) is if “very” endogenous - should use 2SLS if not “very” endogenous - perhaps prefer OLS Invalid Test of Endogeneity: \\(y_{i2}\\) is endogenous if it is correlated with \\(\\epsilon_i\\), \\[ \\epsilon_i = \\gamma_0 + y_{i2}\\gamma_1 + error_i \\] where \\(\\gamma_1 \\neq 0\\) implies that there is endogeneity \\(\\epsilon_i\\) is not observed, but using the residuals \\[ e_i = \\gamma_0 + y_{i2}\\gamma_1 + error_i \\] is NOT a valid test of endogeneity + The OLS residual, e is mechanically uncorrelated with \\(y_{i2}\\) (by FOC for OLS) + In every situation, \\(\\gamma_1\\) will be essentially 0 and you will never be able to reject the null of no endogeneity Valid test of endogeneity If \\(y_{i2}\\) is not endogenous then \\(\\epsilon_i\\) and v are uncorrelated \\[ \\begin{aligned} y_{i1} &amp;= \\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\\\ y_{i2} &amp;= \\pi_0 + \\mathbf{z}_{i1}\\pi_1 + z_{i2}\\pi_2 + v_i \\end{aligned} \\] Variable Addition test: include the first stage residuals as an additional variable, \\[ y_{i1} = \\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\hat{v}_i \\theta + error_i \\] Then the usual \\(t\\)-test of significance is a valid test to evaluate the following hypothesis. note this test requires your instrument to be valid instrument. \\[ \\begin{aligned} &amp;H_0: \\theta = 0 &amp; \\text{ (not endogenous)} \\\\ &amp;H_1: \\theta \\neq 0 &amp; \\text{ (endogenous)} \\end{aligned} \\] 33.1.3.1.1.2 Exogeneity Why exogeneity matter? \\[ E(\\mathbf{z}_{i2}&#39;\\epsilon_i) = 0 \\] If A3a fails - 2SLS is also inconsistent If instrument is not exogenous, then we need to find a new one. Similar to Endogeneity Test, when there is a single instrument \\[ \\begin{aligned} e_i &amp;= \\gamma_0 + \\mathbf{z}_{i2}\\gamma_1 + error_i \\\\ H_0: \\gamma_1 &amp;= 0 \\end{aligned} \\] is NOT a valid test of endogeneity the OLS residual, e is mechanically uncorrelated with \\(z_{i2}\\): \\(\\hat{\\gamma}_1\\) will be essentially 0 and you will never be able to determine if the instrument is endogenous. Solution Testing Instrumental Exogeneity in an Over-identified Model When there is more than one exogenous instrument (per endogenous variable), we can test for instrument exogeneity. When we have multiple instruments, the model is said to be over-identified. Could estimate the same model several ways (i.e., can identify/ estimate \\(\\beta_1\\) more than one way) Idea behind the test: if the controls and instruments are truly exogenous then OLS estimation of the following regression, \\[ \\epsilon_i = \\gamma_0 + \\mathbf{z}_{i1}\\gamma_1 + \\mathbf{z}_{i2}\\gamma_2 + error_i \\] should have a very low \\(R^2\\) if the model is just identified (one instrument per endogenous variable) then the \\(R^2 = 0\\) Steps: Estimate the structural equation by 2SLS (using all available instruments) and obtain the residuals e Regress e on all controls and instruments and obtain the \\(R^2\\) Under the null hypothesis (all IV’s are uncorrelated), \\(nR^2 \\sim \\chi^2(q)\\), where q is the number of instrumental variables minus the number of endogenous variables if the model is just identified (one instrument per endogenous variable) then q = 0, and the distribution under the null collapses. low p-value means you reject the null of exogenous instruments. Hence you would like to have high p-value in this test. Pitfalls for the Overid test the overid test is essentially compiling the following information. Conditional on first instrument being exogenous is the other instrument exogenous? Conditional on the other instrument being exogenous, is the first instrument exogenous? If all instruments are endogenous than neither test will be valid really only useful if one instrument is thought to be truly exogenous (randomly assigned). even f you do reject the null, the test does not tell you which instrument is exogenous and which is endogenous. Result Implication reject the null you can be pretty sure there is an endogenous instrument, but don’t know which one. fail to reject could be either (1) they are both exogenous, (2) they are both endogenous. 33.1.3.1.1.3 Relevancy Why Relevance matter? \\[ \\pi_2 \\neq 0 \\] used to show A2 holds If \\(\\pi_2 = 0\\) (instrument is not relevant) then A2 fails - perfect multicollinearity If \\(\\pi_2\\) is close to 0 (weak instrument) then there is near perfect multicollinearity - 2SLS is highly inefficient (Large standard errors). A weak instrument will exacerbate any inconsistency due to an instrument being (even slightly) endogenous. In the simple case with no controls and a single endogenous variable and single instrumental variable, \\[ plim(\\hat{\\beta}_{2_{2SLS}}) = \\beta_2 + \\frac{E(z_{i2}\\epsilon_i)}{E(z_{i2}y_{i2})} \\] Testing Weak Instruments can use \\(t\\)-test (or \\(F\\)-test for over-identified models) in the first stage to determine if there is a weak instrument problem. J. Stock and Yogo (2005): a statistical rejection of the null hypothesis in the first stage at the 5% (or even 1%) level is not enough to insure the instrument is not weak Rule of Thumb: need a \\(F\\)-stat of at least 10 (or a \\(t\\)-stat of at least 3.2) to reject the null hypothesis that the instrument is weak. Summary of the 2SLS Estimator \\[ \\begin{aligned} y_{i1} &amp;=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\\\ y_{i2} &amp;= \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i \\end{aligned} \\] when A3a does not hold \\[ E(y_{i2}&#39;\\epsilon_i) \\neq 0 \\] Then the OLS estimator is no longer unbiased or consistent. If we have valid instruments \\(\\mathbf{z}_{i2}\\) Relevancy (need to avoid “weak instruments”): \\(\\pi_2 \\neq 0\\) Then the 2SLS estimator is consistent under A1, A2, A5a, and the above two conditions. If A4 also holds, then the usual standard errors are valid. If A4 does not hold then use the robust standard errors. \\[ \\begin{aligned} y_{i1} &amp;= \\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\\\ y_{i2} &amp;= \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i \\end{aligned} \\] When A3a does hold \\[ E(y_{i2}&#39;\\epsilon_i) = 0 \\] and we have valid instruments, then both the OLS and 2SLS estimators are consistent. The OLS estimator is always more efficient can use the variable addition test to determine if 2SLS is need (A3a does hold) or if OLS is valid (A3a does not hold) Sometimes we can test the assumption for instrument to be valid: Exogeneity : Only table when there are more instruments than endogenous variables. Relevancy (need to avoid “weak instruments”): Always testable, need the F-stat to be greater than 10 to rule out a weak instrument Application Expenditure as observed instrument m2.2sls &lt;- ivreg( read ~ stratio + english + lunch + grades + income + calworks + county | expenditure + english + lunch + grades + income + calworks + county , data = school ) summary(m2.2sls)$coefficients[1:7,] #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 700.47891593 13.58064436 51.5792106 8.950497e-171 #&gt; stratio -1.13674002 0.53533638 -2.1234126 3.438427e-02 #&gt; english -0.21396934 0.03847833 -5.5607753 5.162571e-08 #&gt; lunch -0.39384225 0.03773637 -10.4366757 1.621794e-22 #&gt; gradesKK-08 -1.89227865 1.37791820 -1.3732881 1.704966e-01 #&gt; income 0.62487986 0.11199008 5.5797785 4.668490e-08 #&gt; calworks -0.04950501 0.06244410 -0.7927892 4.284101e-01 33.1.3.1.2 Checklist Regress the dependent variable on the instrument (reduced form). Since under OLS, we have unbiased estimate, the coefficient estimate should be significant (make sure the sign makes sense) Report F-stat on the excluded instruments. F-stat &lt; 10 means you have a weak instrument (J. H. Stock, Wright, and Yogo 2002). Present \\(R^2\\) before and after including the instrument (Rossi 2014) For models with multiple instrument, present firs-t and second-stage result for each instrument separately. Overid test should be conducted (e.g., Sargan-Hansen J) Hausman test between OLS and 2SLS (don’t confuse this test for evidence that endogeneity is irrelevant - under invalid IV, the test is useless) Compare the 2SLS with the limited information ML. If they are different, you have evidence for weak instruments. 33.1.3.2 Good Instruments Exogeneity and Relevancy are necessary but not sufficient for IV to produce consistent estimates. Without theory or possible explanation, you can always create a new variable that is correlated with \\(X\\) and uncorrelated with \\(\\epsilon\\) For example, we want to estimate the effect of price on quantity (Reiss 2011, 960) \\[ \\begin{aligned} Q &amp;= \\beta_1 P + \\beta_2 X + \\epsilon \\\\ P &amp;= \\pi_1 X + \\eta \\end{aligned} \\] where \\(\\epsilon\\) and \\(\\eta\\) are jointly determined, \\(X \\perp \\epsilon, \\eta\\) Without theory, we can just create a new variable \\(Z = X + u\\) where \\(E(u) = 0; u \\perp X, \\epsilon, \\eta\\) Then, \\(Z\\) satisfied both conditions: Relevancy: \\(X\\) correlates \\(P\\) \\(\\rightarrow\\) \\(Z\\) correlates \\(P\\) Exogeneity: \\(u \\perp \\epsilon\\) (random noise) But obviously, it’s not a valid instrument (intuitively). But theoretically, relevance and exogeneity are not sufficient to identify \\(\\beta\\) because of unsatisfied rank condition for identification. Moreover, the functional form of the instrument also plays a role when choosing a good instrument. Hence, we always need to check for the robustness of our instrument. IV methods even with valid instruments can still have poor sampling properties (finite sample bias, large sampling errors) (Rossi 2014) When you have a weak instrument, it’s important to report it appropriately. This problem will be exacerbated if you have multiple instruments (Larcker and Rusticus 2010). 33.1.3.2.1 Lagged dependent variable In time series data sets, we can use lagged dependent variable as an instrument because it is not influenced by current shocks. For example, Chetty, Friedman, and Rockoff (2014) used lagged dependent variable in econ. 33.1.3.2.2 Lagged explanatory variable Common practice in applied economics: Replace suspected simultaneously determined explanatory variable with its lagged value Bellemare, Masaki, and Pepinsky (2017). This practice does not avoid simultaneity bias. Estimates using this method are still inconsistent. Hypothesis testing becomes invalid under this approach. Lagging variables changes how endogeneity bias operates, adding a “no dynamics among unobservables” assumption to the “selection on observables” assumption. Key conditions for appropriate use (Bellemare, Masaki, and Pepinsky 2017): Under unobserved confounding: No dynamics among unobservables. The lagged variable \\(X\\) is a stationary autoregressive process. Under no unobserved confounding: No reverse causality; the causal effect operates with a one-period lag (\\(X_{t-1} \\to Y\\), \\(X_t \\not\\to Y_t\\)). Reverse causality is contemporaneous, with a one-period lag effect. Reverse causality is contemporaneous; no dynamics in \\(Y\\), but dynamics exist in \\(X\\) (\\(X_{t-1} \\to X\\)). Alternative approach: Use lagged values of the endogenous variable in IV estimation. However, IV estimation is only effective if (Reed 2015): Lagged values do not belong in the estimating equation. Lagged values are sufficiently correlated with the simultaneously determined explanatory variable. Lagged IVs help mitigate endogeneity if they only violate the independence assumption. However, if lagged IVs violate both the independence assumption and exclusion restriction, they may aggravate endogeneity (Yu Wang and Bellemare 2019). 33.1.3.3 Internal instrumental variable (also known as instrument free methods). This section is based on Raluca Gui’s guide alternative to external instrumental variable approaches All approaches here assume a continuous dependent variable 33.1.3.3.1 Non-hierarchical Data (Cross-classified) \\[ Y_t = \\beta_0 + \\beta_1 P_t + \\beta_2 X_t + \\epsilon_t \\] where \\(t = 1, .., T\\) (indexes either time or cross-sectional units) \\(Y_t\\) is a \\(k \\times 1\\) response variable \\(X_t\\) is a \\(k \\times n\\) exogenous regressor \\(P_t\\) is a \\(k \\times 1\\) continuous endogenous regressor \\(\\epsilon_t\\) is a structural error term with \\(\\mu_\\epsilon =0\\) and \\(E(\\epsilon^2) = \\sigma^2\\) \\(\\beta\\) are model parameters The endogeneity problem arises from the correlation of \\(P_t\\) and \\(\\epsilon_t\\): \\[ P_t = \\gamma Z_t + v_t \\] where \\(Z_t\\) is a \\(l \\times 1\\) vector of internal instrumental variables \\(ν_t\\) is a random error with \\(\\mu_{v_t}, E(v^2) = \\sigma^2_v, E(\\epsilon v) = \\sigma_{\\epsilon v}\\) \\(Z_t\\) is assumed to be stochastic with distribution \\(G\\) \\(ν_t\\) is assumed to have density \\(h(·)\\) 33.1.3.3.1.1 Latent Instrumental Variable (Ebbes et al. 2005) assume \\(Z_t\\) (unobserved) to be uncorrelated with \\(\\epsilon_t\\), which is similar to Instrumental Variable. Hence, \\(Z_t\\) and \\(ν_t\\) can’t be identified without distributional assumptions The distributions of \\(Z_t\\) and \\(ν_t\\) need to be specified such that: endogeneity of \\(P_t\\) is corrected the distribution of \\(P_t\\) is empirically close to the integral that expresses the amount of overlap of Z as it is shifted over ν (= the convolution between \\(Z_t\\) and \\(ν_t\\)). When the density h(·) = Normal, then G cannot be normal because the parameters would not be identified (Ebbes et al. 2005) . Hence, in the LIV model the distribution of \\(Z_t\\) is discrete in the Higher Moments Method and Joint Estimation Using Copula methods, the distribution of \\(Z_t\\) is taken to be skewed. \\(Z_t\\) are assumed unobserved, discrete and exogenous, with an unknown number of groups m \\(\\gamma\\) is a vector of group means. Identification of the parameters relies on the distributional assumptions of \\(P_t\\): a non-Gaussian distribution \\(Z_t\\) discrete with \\(m \\ge 2\\) Note: If \\(Z_t\\) is continuous, the model is unidentified If \\(P_t \\sim N\\), you have inefficient estimates. m3.liv &lt;- latentIV(read ~ stratio, data = school) summary(m3.liv)$coefficients[1:7, ] #&gt; Estimate Std. Error z-score Pr(&gt;|z|) #&gt; (Intercept) 6.996014e+02 2.686186e+02 2.604441e+00 9.529597e-03 #&gt; stratio -2.272673e+00 1.367757e+01 -1.661605e-01 8.681108e-01 #&gt; pi1 -4.896363e+01 5.526907e-08 -8.859139e+08 0.000000e+00 #&gt; pi2 1.963920e+01 9.225351e-02 2.128830e+02 0.000000e+00 #&gt; theta5 6.939432e-152 3.354672e-160 2.068587e+08 0.000000e+00 #&gt; theta6 3.787512e+02 4.249457e+01 8.912932e+00 1.541524e-17 #&gt; theta7 -1.227543e+00 4.885276e+01 -2.512741e-02 9.799653e-01 it will return a coefficient very different from the other methods since there is only one endogenous variable. 33.1.3.3.1.2 Joint Estimation Using Copula assume \\(Z_t\\) (unobserved) to be uncorrelated with \\(\\epsilon_t\\), which is similar to Instrumental Variable. Hence, \\(Z_t\\) and \\(ν_t\\) can’t be identified without distributional assumptions (Park and Gupta 2012) allows joint estimation of the continuous \\(P_t\\) and \\(\\epsilon_t\\) using Gaussian copulas, where a copula is a function that maps several conditional distribution functions (CDF) into their joint CDF). The underlying idea is that using information contained in the observed data, one selects marginal distributions for \\(P_t\\) and \\(\\epsilon_t\\). Then, the copula model constructs a flexible multivariate joint distribution that allows a wide range of correlations between the two marginals. The method allows both continuous and discrete \\(P_t\\). In the special case of one continuous \\(P_t\\), estimation is based on MLE Otherwise, based on Gaussian copulas, augmented OLS estimation is used. Assumptions: skewed \\(P_t\\) the recovery of the correct parameter estimates \\(\\epsilon_t \\sim\\) normal marginal distribution. The marginal distribution of \\(P_t\\) is obtained using the Epanechnikov kernel density estimator \\[ \\hat{h}_p = \\frac{1}{T . b} \\sum_{t=1}^TK(\\frac{p - P_t}{b}) \\] where \\(P_t\\) = endogenous variables \\(K(x) = 0.75(1-x^2)I(||x||\\le 1)\\) \\(b=0.9T^{-1/5}\\times min(s, IQR/1.34)\\) IQR = interquartile range \\(s\\) = sample standard deviation \\(T\\) = n of time periods observed in the data # 1.34 comes from this diff(qnorm(c(0.25, 0.75))) #&gt; [1] 1.34898 In augmented OLS and MLE, the inference procedure occurs in two stages: (1): the empirical distribution of \\(P_t\\) is computed (2) used in it constructing the likelihood function) Hence, the standard errors would not be correct. So we use the sampling distributions (from bootstrapping) to get standard errors and the variance-covariance matrix. Since the distribution of the bootstrapped parameters is highly skewed, we report the percentile confidence intervals is preferable. set.seed(110) m4.cc &lt;- copulaCorrection( read ~ stratio + english + lunch + calworks + grades + income + county | continuous(stratio), data = school, optimx.args = list(method = c(&quot;Nelder-Mead&quot;), itnmax = 60000), num.boots = 2, verbose = FALSE ) summary(m4.cc)$coefficients[1:7,] #&gt; Point Estimate Boots SE Lower Boots CI (95%) Upper Boots CI (95%) #&gt; (Intercept) 683.06900891 2.80554212 NA NA #&gt; stratio -0.32434608 0.02075999 NA NA #&gt; english -0.21576110 0.01450666 NA NA #&gt; lunch -0.37087664 0.01902052 NA NA #&gt; calworks -0.05569058 0.02076781 NA NA #&gt; gradesKK-08 -1.92286128 0.25684614 NA NA #&gt; income 0.73595353 0.04725700 NA NA we run this model with only one endogenous continuous regressor (stratio). Sometimes, the code will not converge, in which case you can use different optimization algorithm starting values maximum number of iterations 33.1.3.3.1.3 Higher Moments Method suggested by (Lewbel 1997) to identify \\(\\epsilon_t\\) caused by measurement error. Identification is achieved by using third moments of the data, with no restrictions on the distribution of \\(\\epsilon_t\\) The following instruments can be used with 2SLS estimation to obtain consistent estimates: \\[ \\begin{aligned} q_{1t} &amp;= (G_t - \\bar{G}) \\\\ q_{2t} &amp;= (G_t - \\bar{G})(P_t - \\bar{P}) \\\\ q_{3t} &amp;= (G_t - \\bar{G})(Y_t - \\bar{Y})\\\\ q_{4t} &amp;= (Y_t - \\bar{Y})(P_t - \\bar{P}) \\\\ q_{5t} &amp;= (P_t - \\bar{P})^2 \\\\ q_{6t} &amp;= (Y_t - \\bar{Y})^2 \\\\ \\end{aligned} \\] where \\(G_t = G(X_t)\\) for any given function G that has finite third own and cross moments \\(X\\) = exogenous variable \\(q_{5t}, q_{6t}\\) can be used only when the measurement and \\(\\epsilon_t\\) are symmetrically distributed. The rest of the instruments does not require any distributional assumptions for \\(\\epsilon_t\\). Since the regressors \\(G(X) = X\\) are included as instruments, \\(G(X)\\) can’t be a linear function of X in \\(q_{1t}\\) Since this method has very strong assumptions, Higher Moments Method should only be used in case of overidentification set.seed(111) m5.hetEr &lt;- hetErrorsIV( read ~ stratio + english + lunch + calworks + income + grades + county | stratio | IIV(income, english), data = school ) summary(m5.hetEr)$coefficients[1:7,] #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 662.78791557 27.90173069 23.7543657 2.380436e-76 #&gt; stratio 0.71480686 1.31077325 0.5453322 5.858545e-01 #&gt; english -0.19522271 0.04057527 -4.8113717 2.188618e-06 #&gt; lunch -0.37834232 0.03927793 -9.6324402 9.760809e-20 #&gt; calworks -0.05665126 0.06302095 -0.8989273 3.692776e-01 #&gt; income 0.82693755 0.17236557 4.7975797 2.335271e-06 #&gt; gradesKK-08 -1.93795843 1.38723186 -1.3969968 1.632541e-01 recommend using this approach to create additional instruments to use with external ones for better efficiency. 33.1.3.3.1.4 Heteroskedastic Error Approach using means of variables that are uncorrelated with the product of heteroskedastic errors to identify structural parameters. This method can be use either when you don’t have external instruments or you want to use additional instruments to improve the efficiency of the IV estimator (Lewbel 2012) The instruments are constructed as simple functions of data Model’s assumptions: \\[ \\begin{aligned} E(X \\epsilon) &amp;= 0 \\\\ E(X v ) &amp;= 0 \\\\ cov(Z, \\epsilon v) &amp;= 0 \\\\ cov(Z, v^2) &amp;\\neq 0 \\text{ (for identification)} \\end{aligned} \\] Structural parameters are identified by 2SLS regression of Y on X and P, using X and [Z − E(Z)]ν as instruments. \\[ \\text{instrument&#39;s strength} \\propto cov((Z-\\bar{Z})v,v) \\] where \\(cov((Z-\\bar{Z})v,v)\\) is the degree of heteroskedasticity of ν with respect to Z (Lewbel 2012), which can be empirically tested. If it is zero or close to zero (i.e.,the instrument is weak), you might have imprecise estimates, with large standard errors. Under homoskedasticity, the parameters of the model are unidentified. Under heteroskedasticity related to at least some elements of X, the parameters of the model are identified. 33.1.3.3.2 Hierarchical Data Multiple independent assumptions involving various random components at different levels mean that any moderate correlation between some predictors and a random component or error term can result in a significant bias of the coefficients and of the variance components. (J.-S. Kim and Frees 2007) proposed a generalized method of moments which uses both, the between and within variations of the exogenous variables, but only assumes the within variation of the variables to be endogenous. Assumptions the errors at each level \\(\\sim iid N\\) the slope variables are exogenous the level-1 \\(\\epsilon \\perp X, P\\). If this is not the case, additional, external instruments are necessary Hierarchical Model \\[ \\begin{aligned} Y_{cst} &amp;= Z_{cst}^1 \\beta_{cs}^1 + X_{cst}^1 \\beta_1 + \\epsilon_{cst}^1 \\\\ \\beta^1_{cs} &amp;= Z_{cs}^2 \\beta_{c}^2 + X_{cst}^2 \\beta_2 + \\epsilon_{cst}^2 \\\\ \\beta^2_{c} &amp;= X^3_c \\beta_3 + \\epsilon_c^3 \\end{aligned} \\] Bias could stem from: errors at the higher two levels (\\(\\epsilon_c^3,\\epsilon_{cst}^2\\)) are correlated with some of the regressors only third level errors (\\(\\epsilon_c^3\\)) are correlated with some of the regressors (J.-S. Kim and Frees 2007) proposed When all variables are assumed exogenous, the proposed estimator equals the random effects estimator When all variables are assumed endogenous, it equals the fixed effects estimator also use omitted variable test (based on the Hausman-test (J. A. Hausman 1978) for panel data), which allows the comparison of a robust estimator and an estimator that is efficient under the null hypothesis of no omitted variables or the comparison of two robust estimators at different levels. # function &#39;cholmod_factor_ldetA&#39; not provided by package &#39;Matrix&#39; set.seed(113) school$gr08 &lt;- school$grades == &quot;KK-06&quot; m7.multilevel &lt;- multilevelIV(read ~ stratio + english + lunch + income + gr08 + calworks + (1 | county) | endo(stratio), data = school) summary(m7.multilevel)$coefficients[1:7,] Another example using simulated data level-1 regressors: \\(X_{11}, X_{12}, X_{13}, X_{14}, X_{15}\\), where \\(X_{15}\\) is correlated with the level-2 error (i.e., endogenous). level-2 regressors: \\(X_{21}, X_{22}, X_{23}, X_{24}\\) level-3 regressors: \\(X_{31}, X_{32}, X_{33}\\) We estimate a three-level model with X15 assumed endogenous. Having a three-level hierarchy, multilevelIV() returns five estimators, from the most robust to omitted variables (FE_L2), to the most efficient (REF) (i.e. lowest mean squared error). The random effects estimator (REF) is efficient assuming no omitted variables The fixed effects estimator (FE) is unbiased and asymptotically normal even in the presence of omitted variables. Because of the efficiency, the random effects estimator is preferable if you think there is no omitted. variables The robust estimator would be preferable if you think there is omitted variables. # function &#39;cholmod_factor_ldetA&#39; not provided by package &#39;Matrix&#39;’ data(dataMultilevelIV) set.seed(114) formula1 &lt;- y ~ X11 + X12 + X13 + X14 + X15 + X21 + X22 + X23 + X24 + X31 + X32 + X33 + (1 | CID) + (1 | SID) | endo(X15) m8.multilevel &lt;- multilevelIV(formula = formula1, data = dataMultilevelIV) coef(m8.multilevel) summary(m8.multilevel, &quot;REF&quot;) True \\(\\beta_{X_{15}} =-1\\). We can see that some estimators are bias because \\(X_{15}\\) is correlated with the level-two error, to which only FE_L2 and GMM_L2 are robust To select the appropriate estimator, we use the omitted variable test. In a three-level setting, we can have different estimator comparisons: Fixed effects vs. random effects estimators: Test for omitted level-two and level-three omitted effects, simultaneously, one compares FE_L2 to REF. But we will not know at which omitted variables exist. Fixed effects vs. GMM estimators: Once the existence of omitted effects is established but not sure at which level, we test for level-2 omitted effects by comparing FE_L2 vs GMM_L3. If you reject the null, the omitted variables are at level-2 The same is accomplished by testing FE_L2 vs. GMM_L2, since the latter is consistent only if there are no omitted effects at level-2. Fixed effects vs. fixed effects estimators: We can test for omitted level-2 effects, while allowing for omitted level-3 effects by comparing FE_L2 vs. FE_L3 since FE_L2 is robust against both level-2 and level-3 omitted effects while FE_L3 is only robust to level-3 omitted variables. Summary, use the omitted variable test comparing REF vs. FE_L2 first. If the null hypothesis is rejected, then there are omitted variables either at level-2 or level-3 Next, test whether there are level-2 omitted effects, since testing for omitted level three effects relies on the assumption there are no level-two omitted effects. You can use any of these pair of comparisons: FE_L2 vs. FE_L3 FE_L2 vs. GMM_L2 If no omitted variables at level-2 are found, test for omitted level-3 effects by comparing either FE_L3 vs. GMM_L3 GMM_L2 vs. GMM_L3 summary(m8.multilevel, &quot;REF&quot;) # compare REF with all the other estimators. Testing REF (the most efficient estimator) against FE_L2 (the most robust estimator), equivalently we are testing simultaneously for level-2 and level-3 omitted effects. Since the null hypothesis is rejected (p = 0.000139), there is bias in the random effects estimator. To test for level-2 omitted effects (regardless of level-3 omitted effects), we compare FE_L2 versus FE_L3 summary(m8.multilevel,&quot;FE_L2&quot;) The null hypothesis of no omitted level-2 effects is rejected (\\(p = 3.92e − 05\\)). Hence, there are omitted effects at level-two. We should use FE_L2 which is consistent with the underlying data that we generated (level-2 error correlated with \\(X_15\\), which leads to biased FE_L3 coefficients. The omitted variable test between FE_L2 and GMM_L2 should reject the null hypothesis of no omitted level-2 effects (p-value is 0). If we assume an endogenous variable as exogenous, the RE and GMM estimators will be biased because of the wrong set of internal instrumental variables. To increase our confidence, we should compare the omitted variable tests when the variable is considered endogenous vs. exogenous to get a sense whether the variable is truly endogenous. 33.1.3.4 Proxy Variables Can be in place of the omitted variable will not be able to estimate the effect of the omitted variable will be able to reduce some endogeneity caused bye the omitted variable but it can have Measurement Error. Hence, you have to be extremely careful when using proxies. Criteria for a proxy variable: The proxy is correlated with the omitted variable. Having the omitted variable in the regression will solve the problem of endogeneity The variation of the omitted variable unexplained by the proxy is uncorrelated with all independent variables, including the proxy. IQ test can be a proxy for ability in the regression between wage explained education. For the third requirement \\[ ability = \\gamma_0 + \\gamma_1 IQ + \\epsilon \\] where \\(\\epsilon\\) is uncorrelated with education and IQ test. References "],["endogenous-sample-selection.html", "33.2 Endogenous Sample Selection", " 33.2 Endogenous Sample Selection Also known as sample selection or self-selection problem or incidental truncation. The omitted variable is how people were selected into the sample Some disciplines consider nonresponse bias and selection bias as sample selection. When unobservable factors that affect who is in the sample are independent of unobservable factors that affect the outcome, the sample selection is not endogenous. Hence, the sample selection is ignorable and estimator that ignores sample selection is still consistent. when the unobservable factors that affect who is included in the sample are correlated with the unobservable factors that affect the outcome, the sample selection is endogenous and not ignorable, because estimators that ignore endogenous sample selection are not consistent (we don’t know which part of the observable outcome is related to the causal relationship and which part is due to different people were selected for the treatment and control groups). Assumptions: - The unobservables that affect the treatment selection and the outcome are jointly distributed as bivariate normal. Notes: If you don’t have strong exclusion restriction, identification is driven by the assumed non linearity in the functional form (through inverse Mills ratio). E.g., the estimate depend on the bivariate normal distribution of the error structure: With strong exclusion restriction for the covariate in the correction equation, the variation in this variable can help identify the control for selection With weak exclusion restriction, and the variable exists in both steps, it’s the assumed error structure that identifies the control for selection (J. Heckman and Navarro-Lozano 2004). In management, Wolfolds and Siegel (2019) found that papers should have valid exclusion conditions, because without these, simulations show that results using the Heckman method are less reliable than those obtained with OLS. There are differences between Heckman Sample Selection vs. Heckman-type correction Heckman Sample Selection Model Heckman-Type Corrections When Only observes one sample (treated), addressing selection bias directly. Two samples are observed (treated and untreated), known as the control function approach. Model Probit OLS (even for dummy endogenous variable) Integration of 1st stage Also include a term (called Inverse Mills ratio) besides the endogenous variable. Decompose the endogenous variable to get the part that is uncorrelated with the error terms of the outcome equation. Either use the predicted endogenous variable directly or include the residual from the first-stage equation. Advantages and Assumptions Provides a direct test for endogeneity via the coefficient of the inverse Mills ratio but requires the assumption of joint normality of errors. Does not require the assumption of joint normality, but can’t test for endogeneity directly. To deal with [Sample Selection], we can Randomization: participants are randomly selected into treatment and control. Instruments that determine the treatment status (i.e., treatment vs. control) but not the outcome (\\(Y\\)) Functional form of the selection and outcome processes: originated from (James J. Heckman 1976), later on generalize by (Amemiya 1984) We have our main model \\[ \\mathbf{y^* = xb + \\epsilon} \\] However, the pattern of missingness (i.e., censored) is related to the unobserved (latent) process: \\[ \\mathbf{z^* = w \\gamma + u} \\] and \\[ z_i = \\begin{cases} 1&amp; \\text{if } z_i^*&gt;0 \\\\ 0&amp;\\text{if } z_i^*\\le0\\\\ \\end{cases} \\] Equivalently, \\(z_i = 1\\) (\\(y_i\\) is observed) when \\[ u_i \\ge -w_i \\gamma \\] Hence, the probability of observed \\(y_i\\) is \\[ \\begin{aligned} P(u_i \\ge -w_i \\gamma) &amp;= 1 - \\Phi(-w_i \\gamma) \\\\ &amp;= \\Phi(w_i \\gamma) &amp; \\text{symmetry of the standard normal distribution} \\end{aligned} \\] We will assume the error term of the selection \\(\\mathbf{u \\sim N(0,I)}\\) \\(Var(u_i) = 1\\) for identification purposes Visually, \\(P(u_i \\ge -w_i \\gamma)\\) is the shaded area. x = seq(-3, 3, length = 200) y = dnorm(x, mean = 0, sd = 1) plot(x, y, type = &quot;l&quot;, main = bquote(&quot;Probabibility distribution of&quot; ~ u[i])) x = seq(0.3, 3, length = 100) y = dnorm(x, mean = 0, sd = 1) polygon(c(0.3, x, 3), c(0, y, 0), col = &quot;gray&quot;) text(1, 0.1, bquote(1 - Phi ~ (-w[i] ~ gamma))) arrows(-0.5, 0.1, 0.3, 0, length = .15) text(-0.5, 0.12, bquote(-w[i] ~ gamma)) legend( &quot;topright&quot;, &quot;Gray = Prob of Observed&quot;, pch = 1, title = &quot;legend&quot;, inset = .02 ) Hence in our observed model, we see \\[\\begin{equation} y_i = x_i\\beta + \\epsilon_i \\text{when $z_i=1$} \\end{equation}\\] and the joint distribution of the selection model (\\(u_i\\)), and the observed equation (\\(\\epsilon_i\\)) as \\[ \\left[ \\begin{array} {c} u \\\\ \\epsilon \\\\ \\end{array} \\right] \\sim^{iid}N \\left( \\left[ \\begin{array} {c} 0 \\\\ 0 \\\\ \\end{array} \\right], \\left[ \\begin{array} {cc} 1 &amp; \\rho \\\\ \\rho &amp; \\sigma^2_{\\epsilon} \\\\ \\end{array} \\right] \\right) \\] The relation between the observed and selection models: \\[ \\begin{aligned} E(y_i | y_i \\text{ observed}) &amp;= E(y_i| z^*&gt;0) \\\\ &amp;= E(y_i| -w_i \\gamma) \\\\ &amp;= \\mathbf{x}_i \\beta + E(\\epsilon_i | u_i &gt; -w_i \\gamma) \\\\ &amp;= \\mathbf{x}_i \\beta + \\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)} \\end{aligned} \\] where \\(\\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)}\\) is the Inverse Mills Ratio. and \\(\\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)} \\ge 0\\) A property of IMR: Its derivative is: \\(IMR&#39;(x) = -x IMR(x) - IMR(x)^2\\) Great visualization of special cases of correlation patterns among data and errors by professor Rob Hick Note: (Bareinboim and Pearl 2014) is an excellent summary of cases that we can still do causal inference in case of selection bias. I’ll try to summarize their idea here: Let \\(X\\) be an action, \\(Y\\) be an outcome, and S be a binary indicator of entry into the data pool where (\\(S = 1 =\\) in the sample, \\(S = 0 =\\) out of sample) and Q be the conditional distribution \\(Q = P(y|x)\\). Usually we want to understand , but because of \\(S\\), we only have \\(P(y, x|S = 1)\\). Hence, we’d like to recover \\(P(y|x)\\) from \\(P(y, x|S = 1)\\) If both X and Y affect S, we can’t unbiasedly estimate \\(P(y|x)\\) In the case of Omitted variable bias (\\(U\\)) and sample selection bias (\\(S\\)), you have unblocked extraneous “flow” of information between X and \\(Y\\), which causes spurious correlation for \\(X\\) and \\(Y\\). Traditionally, we would recover \\(Q\\) by parametric assumption of The data generating process (e.g., Heckman 2-step) Type of data-generating model (e..g, treatment-dependent or outcome-dependent) Selection’s probability \\(P(S = 1|P a_s)\\) with non-parametrically based causal graphical models, the authors proposed more robust way to model misspecification regardless of the type of data-generating model, and do not require selection’s probability. Hence, you can recover Q Without external data With external data Causal effects with the Selection-backdoor criterion 33.2.1 Tobit-2 also known as Heckman’s standard sample selection model Assumption: joint normality of the errors Data here is taken from Mroz (1984). We want to estimate the log(wage) for married women, with education, experience, experience squared, and a dummy variable for living in a big city. But we can only observe the wage for women who are working, which means a lot of married women in 1975 who were out of the labor force are unaccounted for. Hence, an OLS estimate of the wage equation would be bias due to sample selection. Since we have data on non-participants (i.e., those who are not working for pay), we can correct for the selection process. The Tobit-2 estimates are consistent 33.2.1.1 Example 1 library(sampleSelection) library(dplyr) # 1975 data on married women’s pay and labor-force participation # from the Panel Study of Income Dynamics (PSID) data(&quot;Mroz87&quot;) head(Mroz87) #&gt; lfp hours kids5 kids618 age educ wage repwage hushrs husage huseduc huswage #&gt; 1 1 1610 1 0 32 12 3.3540 2.65 2708 34 12 4.0288 #&gt; 2 1 1656 0 2 30 12 1.3889 2.65 2310 30 9 8.4416 #&gt; 3 1 1980 1 3 35 12 4.5455 4.04 3072 40 12 3.5807 #&gt; 4 1 456 0 3 34 12 1.0965 3.25 1920 53 10 3.5417 #&gt; 5 1 1568 1 2 31 14 4.5918 3.60 2000 32 12 10.0000 #&gt; 6 1 2032 0 0 54 12 4.7421 4.70 1040 57 11 6.7106 #&gt; faminc mtr motheduc fatheduc unem city exper nwifeinc wifecoll huscoll #&gt; 1 16310 0.7215 12 7 5.0 0 14 10.910060 FALSE FALSE #&gt; 2 21800 0.6615 7 7 11.0 1 5 19.499981 FALSE FALSE #&gt; 3 21040 0.6915 12 7 5.0 0 15 12.039910 FALSE FALSE #&gt; 4 7300 0.7815 7 7 5.0 0 6 6.799996 FALSE FALSE #&gt; 5 27300 0.6215 12 14 9.5 1 7 20.100058 TRUE FALSE #&gt; 6 19495 0.6915 14 7 7.5 1 33 9.859054 FALSE FALSE Mroz87 = Mroz87 %&gt;% mutate(kids = kids5 + kids618) library(nnet) library(ggplot2) library(reshape2) 2-stage Heckman’s model: probit equation estimates the selection process (who is in the labor force?) the results from 1st stage are used to construct a variable that captures the selection effect in the wage equation. This correction variable is called the inverse Mills ratio. # OLS: log wage regression on LF participants only ols1 = lm(log(wage) ~ educ + exper + I(exper ^ 2) + city, data = subset(Mroz87, lfp == 1)) # Heckman&#39;s Two-step estimation with LFP selection equation heck1 = heckit( selection = lfp ~ age + I(age ^ 2) + kids + huswage + educ, # the selection process, l # fp = 1 if the woman is participating in the labor force outcome = log(wage) ~ educ + exper + I(exper ^ 2) + city, data = Mroz87 ) summary(heck1$probit) #&gt; -------------------------------------------- #&gt; Probit binary choice model/Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 4 iterations #&gt; Return code 1: gradient close to zero (gradtol) #&gt; Log-Likelihood: -482.8212 #&gt; Model: Y == &#39;1&#39; in contrary to &#39;0&#39; #&gt; 753 observations (325 &#39;negative&#39; and 428 &#39;positive&#39;) and 6 free parameters (df = 747) #&gt; Estimates: #&gt; Estimate Std. error t value Pr(&gt; t) #&gt; XS(Intercept) -4.18146681 1.40241567 -2.9816 0.002867 ** #&gt; XSage 0.18608901 0.06517476 2.8552 0.004301 ** #&gt; XSI(age^2) -0.00241491 0.00075857 -3.1835 0.001455 ** #&gt; XSkids -0.14955977 0.03825079 -3.9100 9.230e-05 *** #&gt; XShuswage -0.04303635 0.01220791 -3.5253 0.000423 *** #&gt; XSeduc 0.12502818 0.02277645 5.4894 4.034e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; Significance test: #&gt; chi2(5) = 64.10407 (p=1.719042e-12) #&gt; -------------------------------------------- summary(heck1$lm) #&gt; #&gt; Call: #&gt; lm(formula = YO ~ -1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.09494 -0.30953 0.05341 0.36530 2.34770 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; XO(Intercept) -0.6143381 0.3768796 -1.630 0.10383 #&gt; XOeduc 0.1092363 0.0197062 5.543 5.24e-08 *** #&gt; XOexper 0.0419205 0.0136176 3.078 0.00222 ** #&gt; XOI(exper^2) -0.0008226 0.0004059 -2.026 0.04335 * #&gt; XOcity 0.0510492 0.0692414 0.737 0.46137 #&gt; imrData$IMR1 0.0551177 0.2111916 0.261 0.79423 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.6674 on 422 degrees of freedom #&gt; Multiple R-squared: 0.7734, Adjusted R-squared: 0.7702 #&gt; F-statistic: 240 on 6 and 422 DF, p-value: &lt; 2.2e-16 Use only variables that affect the selection process in the selection equation. Technically, the selection equation and the equation of interest could have the same set of regressors. But it is not recommended because we should only use variables (or at least one) in the selection equation that affect the selection process, but not the wage process (i.e., instruments). Here, variable kids fulfill that role: women with kids may be more likely to stay home, but working moms with kids would not have their wages change. Alternatively, # ML estimation of selection model ml1 = selection( selection = lfp ~ age + I(age ^ 2) + kids + huswage + educ, outcome = log(wage) ~ educ + exper + I(exper ^ 2) + city, data = Mroz87 ) summary(ml1) #&gt; -------------------------------------------- #&gt; Tobit 2 model (sample selection model) #&gt; Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 3 iterations #&gt; Return code 8: successive function values within relative tolerance limit (reltol) #&gt; Log-Likelihood: -914.0777 #&gt; 753 observations (325 censored and 428 observed) #&gt; 13 free parameters (df = 740) #&gt; Probit selection equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -4.1484037 1.4109302 -2.940 0.003382 ** #&gt; age 0.1842132 0.0658041 2.799 0.005253 ** #&gt; I(age^2) -0.0023925 0.0007664 -3.122 0.001868 ** #&gt; kids -0.1488158 0.0384888 -3.866 0.000120 *** #&gt; huswage -0.0434253 0.0123229 -3.524 0.000451 *** #&gt; educ 0.1255639 0.0229229 5.478 5.91e-08 *** #&gt; Outcome equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.5814781 0.3052031 -1.905 0.05714 . #&gt; educ 0.1078481 0.0172998 6.234 7.63e-10 *** #&gt; exper 0.0415752 0.0133269 3.120 0.00188 ** #&gt; I(exper^2) -0.0008125 0.0003974 -2.044 0.04129 * #&gt; city 0.0522990 0.0682652 0.766 0.44385 #&gt; Error terms: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; sigma 0.66326 0.02309 28.729 &lt;2e-16 *** #&gt; rho 0.05048 0.23169 0.218 0.828 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; -------------------------------------------- # summary(ml1$twoStep) Manual myprob &lt;- probit(lfp ~ age + I( age^2 ) + kids + huswage + educ, # x = TRUE, # iterlim = 30, data = Mroz87) summary(myprob) #&gt; -------------------------------------------- #&gt; Probit binary choice model/Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 4 iterations #&gt; Return code 1: gradient close to zero (gradtol) #&gt; Log-Likelihood: -482.8212 #&gt; Model: Y == &#39;1&#39; in contrary to &#39;0&#39; #&gt; 753 observations (325 &#39;negative&#39; and 428 &#39;positive&#39;) and 6 free parameters (df = 747) #&gt; Estimates: #&gt; Estimate Std. error t value Pr(&gt; t) #&gt; (Intercept) -4.18146681 1.40241567 -2.9816 0.002867 ** #&gt; age 0.18608901 0.06517476 2.8552 0.004301 ** #&gt; I(age^2) -0.00241491 0.00075857 -3.1835 0.001455 ** #&gt; kids -0.14955977 0.03825079 -3.9100 9.230e-05 *** #&gt; huswage -0.04303635 0.01220791 -3.5253 0.000423 *** #&gt; educ 0.12502818 0.02277645 5.4894 4.034e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; Significance test: #&gt; chi2(5) = 64.10407 (p=1.719042e-12) #&gt; -------------------------------------------- imr &lt;- invMillsRatio(myprob) Mroz87$IMR1 &lt;- imr$IMR1 manually_est &lt;- lm(log(wage) ~ educ + exper + I( exper^2 ) + city + IMR1, data = Mroz87, subset = (lfp == 1)) summary(manually_est) #&gt; #&gt; Call: #&gt; lm(formula = log(wage) ~ educ + exper + I(exper^2) + city + IMR1, #&gt; data = Mroz87, subset = (lfp == 1)) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.09494 -0.30953 0.05341 0.36530 2.34770 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.6143381 0.3768796 -1.630 0.10383 #&gt; educ 0.1092363 0.0197062 5.543 5.24e-08 *** #&gt; exper 0.0419205 0.0136176 3.078 0.00222 ** #&gt; I(exper^2) -0.0008226 0.0004059 -2.026 0.04335 * #&gt; city 0.0510492 0.0692414 0.737 0.46137 #&gt; IMR1 0.0551177 0.2111916 0.261 0.79423 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.6674 on 422 degrees of freedom #&gt; Multiple R-squared: 0.1582, Adjusted R-squared: 0.1482 #&gt; F-statistic: 15.86 on 5 and 422 DF, p-value: 2.505e-14 Similarly, probit_selection &lt;- glm(lfp ~ age + I( age^2 ) + kids + huswage + educ, data = Mroz87, family = binomial(link = &#39;probit&#39;)) # library(fixest) # probit_selection &lt;- # fixest::feglm(lfp ~ age + I( age^2 ) + kids + huswage + educ, # data = Mroz87, # family = binomial(link = &#39;probit&#39;)) probit_lp &lt;- -predict(probit_selection) inv_mills &lt;- dnorm(probit_lp) / (1 - pnorm(probit_lp)) Mroz87$inv_mills &lt;- inv_mills probit_outcome &lt;- glm( log(wage) ~ educ + exper + I(exper ^ 2) + city + inv_mills, data = Mroz87, subset = (lfp == 1) ) summary(probit_outcome) #&gt; #&gt; Call: #&gt; glm(formula = log(wage) ~ educ + exper + I(exper^2) + city + #&gt; inv_mills, data = Mroz87, subset = (lfp == 1)) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.09494 -0.30953 0.05341 0.36530 2.34770 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.6143383 0.3768798 -1.630 0.10383 #&gt; educ 0.1092363 0.0197062 5.543 5.24e-08 *** #&gt; exper 0.0419205 0.0136176 3.078 0.00222 ** #&gt; I(exper^2) -0.0008226 0.0004059 -2.026 0.04335 * #&gt; city 0.0510492 0.0692414 0.737 0.46137 #&gt; inv_mills 0.0551179 0.2111918 0.261 0.79423 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for gaussian family taken to be 0.4454809) #&gt; #&gt; Null deviance: 223.33 on 427 degrees of freedom #&gt; Residual deviance: 187.99 on 422 degrees of freedom #&gt; AIC: 876.49 #&gt; #&gt; Number of Fisher Scoring iterations: 2 library(&quot;stargazer&quot;) library(&quot;Mediana&quot;) library(&quot;plm&quot;) # function to calculate corrected SEs for regression cse = function(reg) { rob = sqrt(diag(vcovHC(reg, type = &quot;HC1&quot;))) return(rob) } # stargazer table stargazer( # ols1, heck1, ml1, # manually_est, se = list(cse(ols1), NULL, NULL), title = &quot;Married women&#39;s wage regressions&quot;, type = &quot;text&quot;, df = FALSE, digits = 4, selection.equation = T ) #&gt; #&gt; Married women&#39;s wage regressions #&gt; =================================================== #&gt; Dependent variable: #&gt; ------------------------------- #&gt; lfp #&gt; Heckman selection #&gt; selection #&gt; (1) (2) #&gt; --------------------------------------------------- #&gt; age 0.1861*** 0.1842*** #&gt; (0.0658) #&gt; #&gt; I(age2) -0.0024 -0.0024*** #&gt; (0.0008) #&gt; #&gt; kids -0.1496*** -0.1488*** #&gt; (0.0385) #&gt; #&gt; huswage -0.0430 -0.0434*** #&gt; (0.0123) #&gt; #&gt; educ 0.1250 0.1256*** #&gt; (0.0130) (0.0229) #&gt; #&gt; Constant -4.1815*** -4.1484*** #&gt; (0.2032) (1.4109) #&gt; #&gt; --------------------------------------------------- #&gt; Observations 753 753 #&gt; R2 0.1582 #&gt; Adjusted R2 0.1482 #&gt; Log Likelihood -914.0777 #&gt; rho 0.0830 0.0505 (0.2317) #&gt; Inverse Mills Ratio 0.0551 (0.2099) #&gt; =================================================== #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 stargazer( ols1, # heck1, # ml1, manually_est, se = list(cse(ols1), NULL, NULL), title = &quot;Married women&#39;s wage regressions&quot;, type = &quot;text&quot;, df = FALSE, digits = 4, selection.equation = T ) #&gt; #&gt; Married women&#39;s wage regressions #&gt; ================================================ #&gt; Dependent variable: #&gt; ---------------------------- #&gt; log(wage) #&gt; (1) (2) #&gt; ------------------------------------------------ #&gt; educ 0.1057*** 0.1092*** #&gt; (0.0130) (0.0197) #&gt; #&gt; exper 0.0411*** 0.0419*** #&gt; (0.0154) (0.0136) #&gt; #&gt; I(exper2) -0.0008* -0.0008** #&gt; (0.0004) (0.0004) #&gt; #&gt; city 0.0542 0.0510 #&gt; (0.0653) (0.0692) #&gt; #&gt; IMR1 0.0551 #&gt; (0.2112) #&gt; #&gt; Constant -0.5308*** -0.6143 #&gt; (0.2032) (0.3769) #&gt; #&gt; ------------------------------------------------ #&gt; Observations 428 428 #&gt; R2 0.1581 0.1582 #&gt; Adjusted R2 0.1501 0.1482 #&gt; Residual Std. Error 0.6667 0.6674 #&gt; F Statistic 19.8561*** 15.8635*** #&gt; ================================================ #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Rho is an estimate of the correlation of the errors between the selection and wage equations. In the lower panel, the estimated coefficient on the inverse Mills ratio is given for the Heckman model. The fact that it is not statistically different from zero is consistent with the idea that selection bias was not a serious problem in this case. If the estimated coefficient of the inverse Mills ratio in the Heckman model is not statistically different from zero, then selection bias was not a serious problem. 33.2.1.2 Example 2 This code is from R package sampleSelection set.seed(0) library(&quot;sampleSelection&quot;) library(&quot;mvtnorm&quot;) # bivariate normal disturbances eps &lt;- rmvnorm(500, c(0, 0), matrix(c(1, -0.7, -0.7, 1), 2, 2)) # uniformly distributed explanatory variable # (vectors of explanatory variables for the selection) xs &lt;- runif(500) # probit data generating process ys &lt;- xs + eps[, 1] &gt; 0 # vectors of explanatory variables for outcome equation xo &lt;- runif(500) yoX &lt;- xo + eps[, 2] # latent outcome yo &lt;- yoX * (ys &gt; 0) # observable outcome # true intercepts = 0 and our true slopes = 1 # xs and xo are independent. # Hence, exclusion restriction is fulfilled summary(selection(ys ~ xs, yo ~ xo)) #&gt; -------------------------------------------- #&gt; Tobit 2 model (sample selection model) #&gt; Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 5 iterations #&gt; Return code 1: gradient close to zero (gradtol) #&gt; Log-Likelihood: -712.3163 #&gt; 500 observations (172 censored and 328 observed) #&gt; 6 free parameters (df = 494) #&gt; Probit selection equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.2228 0.1081 -2.061 0.0399 * #&gt; xs 1.3377 0.2014 6.642 8.18e-11 *** #&gt; Outcome equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.0002265 0.1294178 -0.002 0.999 #&gt; xo 0.7299070 0.1635925 4.462 1.01e-05 *** #&gt; Error terms: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; sigma 0.9190 0.0574 16.009 &lt; 2e-16 *** #&gt; rho -0.5392 0.1521 -3.544 0.000431 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; -------------------------------------------- without the exclusion restriction, we generate yo using xs instead of xo. yoX &lt;- xs + eps[,2] yo &lt;- yoX*(ys &gt; 0) summary(selection(ys ~ xs, yo ~ xs)) #&gt; -------------------------------------------- #&gt; Tobit 2 model (sample selection model) #&gt; Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 14 iterations #&gt; Return code 8: successive function values within relative tolerance limit (reltol) #&gt; Log-Likelihood: -712.8298 #&gt; 500 observations (172 censored and 328 observed) #&gt; 6 free parameters (df = 494) #&gt; Probit selection equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.1984 0.1114 -1.781 0.0756 . #&gt; xs 1.2907 0.2085 6.191 1.25e-09 *** #&gt; Outcome equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.5499 0.5644 -0.974 0.33038 #&gt; xs 1.3987 0.4482 3.120 0.00191 ** #&gt; Error terms: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; sigma 0.85091 0.05352 15.899 &lt;2e-16 *** #&gt; rho -0.13226 0.72684 -0.182 0.856 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; -------------------------------------------- We can see that our estimates are still unbiased but standard errors are substantially larger. The exclusion restriction (i.e., independent information about the selection process) has a certain identifying power that we desire. Hence, it’s better to have different set of variable for the selection process from the interested equation. Without the exclusion restriction, we solely rely on the functional form identification. 33.2.2 Tobit-5 Also known as the switching regression model Condition: There is at least one variable in X in the selection process not included in the observed process. Used when there are separate models for participants, and non-participants. set.seed(0) vc &lt;- diag(3) vc[lower.tri(vc)] &lt;- c(0.9, 0.5, 0.1) vc[upper.tri(vc)] &lt;- vc[lower.tri(vc)] # 3 disturbance vectors by a 3-dimensional normal distribution eps &lt;- rmvnorm(500, c(0,0,0), vc) xs &lt;- runif(500) # uniformly distributed on [0, 1] ys &lt;- xs + eps[,1] &gt; 0 xo1 &lt;- runif(500) # uniformly distributed on [0, 1] yo1 &lt;- xo1 + eps[,2] xo2 &lt;- runif(500) # uniformly distributed on [0, 1] yo2 &lt;- xo2 + eps[,3] exclusion restriction is fulfilled when \\(x\\)’s are independent. # one selection equation and a list of two outcome equations summary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2))) #&gt; -------------------------------------------- #&gt; Tobit 5 model (switching regression model) #&gt; Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 11 iterations #&gt; Return code 1: gradient close to zero (gradtol) #&gt; Log-Likelihood: -895.8201 #&gt; 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE) #&gt; 10 free parameters (df = 490) #&gt; Probit selection equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.1550 0.1051 -1.474 0.141 #&gt; xs 1.1408 0.1785 6.390 3.86e-10 *** #&gt; Outcome equation 1: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.02708 0.16395 0.165 0.869 #&gt; xo1 0.83959 0.14968 5.609 3.4e-08 *** #&gt; Outcome equation 2: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.1583 0.1885 0.840 0.401 #&gt; xo2 0.8375 0.1707 4.908 1.26e-06 *** #&gt; Error terms: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; sigma1 0.93191 0.09211 10.118 &lt;2e-16 *** #&gt; sigma2 0.90697 0.04434 20.455 &lt;2e-16 *** #&gt; rho1 0.88988 0.05353 16.623 &lt;2e-16 *** #&gt; rho2 0.17695 0.33139 0.534 0.594 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; -------------------------------------------- All the estimates are close to the true values. Example of functional form misspecification set.seed(5) eps &lt;- rmvnorm(1000, rep(0, 3), vc) eps &lt;- eps^2 - 1 # subtract 1 in order to get the mean zero disturbances # interval [−1, 0] to get an asymmetric distribution over observed choices xs &lt;- runif(1000, -1, 0) ys &lt;- xs + eps[,1] &gt; 0 xo1 &lt;- runif(1000) yo1 &lt;- xo1 + eps[,2] xo2 &lt;- runif(1000) yo2 &lt;- xo2 + eps[,3] summary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2), iterlim=20)) #&gt; -------------------------------------------- #&gt; Tobit 5 model (switching regression model) #&gt; Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 4 iterations #&gt; Return code 3: Last step could not find a value above the current. #&gt; Boundary of parameter space? #&gt; Consider switching to a more robust optimisation method temporarily. #&gt; Log-Likelihood: -1665.936 #&gt; 1000 observations: 760 selection 1 (FALSE) and 240 selection 2 (TRUE) #&gt; 10 free parameters (df = 990) #&gt; Probit selection equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.53698 0.05808 -9.245 &lt; 2e-16 *** #&gt; xs 0.31268 0.09395 3.328 0.000906 *** #&gt; Outcome equation 1: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.70679 0.03573 -19.78 &lt;2e-16 *** #&gt; xo1 0.91603 0.05626 16.28 &lt;2e-16 *** #&gt; Outcome equation 2: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.1446 NaN NaN NaN #&gt; xo2 1.1196 0.5014 2.233 0.0258 * #&gt; Error terms: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; sigma1 0.67770 0.01760 38.50 &lt;2e-16 *** #&gt; sigma2 2.31432 0.07615 30.39 &lt;2e-16 *** #&gt; rho1 -0.97137 NaN NaN NaN #&gt; rho2 0.17039 NaN NaN NaN #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; -------------------------------------------- Although we still have an exclusion restriction (xo1 and xo2 are independent), we now have problems with the intercepts (i.e., they are statistically significantly different from the true values zero), and convergence problems. If we don’t have the exclusion restriction, we will have a larger variance of xs set.seed(6) xs &lt;- runif(1000, -1, 1) ys &lt;- xs + eps[,1] &gt; 0 yo1 &lt;- xs + eps[,2] yo2 &lt;- xs + eps[,3] summary(tmp &lt;- selection(ys~xs, list(yo1 ~ xs, yo2 ~ xs), iterlim=20)) #&gt; -------------------------------------------- #&gt; Tobit 5 model (switching regression model) #&gt; Maximum Likelihood estimation #&gt; Newton-Raphson maximisation, 16 iterations #&gt; Return code 8: successive function values within relative tolerance limit (reltol) #&gt; Log-Likelihood: -1936.431 #&gt; 1000 observations: 626 selection 1 (FALSE) and 374 selection 2 (TRUE) #&gt; 10 free parameters (df = 990) #&gt; Probit selection equation: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.3528 0.0424 -8.321 2.86e-16 *** #&gt; xs 0.8354 0.0756 11.050 &lt; 2e-16 *** #&gt; Outcome equation 1: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -0.55448 0.06339 -8.748 &lt;2e-16 *** #&gt; xs 0.81764 0.06048 13.519 &lt;2e-16 *** #&gt; Outcome equation 2: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.6457 0.4994 1.293 0.196 #&gt; xs 0.3520 0.3197 1.101 0.271 #&gt; Error terms: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; sigma1 0.59187 0.01853 31.935 &lt;2e-16 *** #&gt; sigma2 1.97257 0.07228 27.289 &lt;2e-16 *** #&gt; rho1 0.15568 0.15914 0.978 0.328 #&gt; rho2 -0.01541 0.23370 -0.066 0.947 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; -------------------------------------------- Usually it will not converge. Even if it does, the results may be seriously biased. Note The log-likelihood function of the models might not be globally concave. Hence, it might not converge, or converge to a local maximum. To combat this, we can use Different starting value Different maximization methods. refer to Non-linear Least Squares for suggestions. 33.2.2.0.1 Pattern-Mixture Models compared to the Heckman’s model where it assumes the value of the missing data is predetermined, pattern-mixture models assume missingness affect the distribution of variable of interest (e.g., Y) To read more, you can check NCSU, stefvanbuuren. References "],["other-biases.html", "Chapter 34 Other Biases", " Chapter 34 Other Biases In econometrics, the main objective is often to uncover causal relationships. However, coefficient estimates can be affected by various biases. Here’s a list of common biases that can affect coefficient estimates: What we’ve covered so far (see Linear Regression and Endogeneity): Omitted Variable Bias (OVB): Arises when a variable that affects the dependent variable and is correlated with an independent variable is left out of the regression. Endogeneity Bias: Occurs when an error term is correlated with an independent variable. This can be due to: Simultaneity: When the dependent variable simultaneously affects an independent variable. Omitted variables. Measurement error in the independent variable. Measurement Error: Bias introduced when variables in a model are measured with error. If the error is in an independent variable and is classical (mean zero and uncorrelated with the true value), it typically biases the coefficient towards zero. Sample Selection Bias: Arises when the sample is not randomly selected and the selection is related to the dependent variable. A classic example is the Heckman correction for labor market studies where participants self-select into the workforce. Simultaneity Bias (or Reverse Causality): Happens when the dependent variable causes changes in the independent variable, leading to a two-way causation. Multicollinearity: Not a bias in the strictest sense, but in the presence of high multicollinearity (when independent variables are highly correlated), coefficient estimates can become unstable and standard errors large. This makes it hard to determine the individual effect of predictors on the dependent variable. Specification Errors: Arise when the functional form of the model is incorrectly specified, e.g., omitting interaction terms or polynomial terms when they are needed. Autocorrelation (or Serial Correlation): Occurs in time-series data when the error terms are correlated over time. This doesn’t cause bias in the coefficient estimates of OLS, but it can make standard errors biased, leading to incorrect inference. Heteroskedasticity: Occurs when the variance of the error term is not constant across observations. Like autocorrelation, heteroskedasticity doesn’t bias the OLS estimates but can bias standard errors. In this section, we will mention other biases that you may encounter when conducting your research Introduced when data are aggregated, and analysis is conducted at this aggregate level rather than the individual level. [Survivorship Bias] (very much related to Sample Selection): Arises when the sample only includes “survivors” or those who “passed” a certain threshold. Common in finance where only funds or firms that “survive” are analyzed. Not a bias in econometric estimation per se, but relevant in the context of empirical studies. It refers to the tendency for journals to publish only significant or positive results, leading to an overrepresentation of such results in the literature. "],["aggregation-bias.html", "34.1 Aggregation Bias", " 34.1 Aggregation Bias Aggregation bias, also known as ecological fallacy, refers to the error introduced when data are aggregated and an analysis is conducted at this aggregate level, rather than at the individual level. This can be especially problematic in econometrics, where analysts are often concerned with understanding individual behavior. When the relationship between variables is different at the aggregate level than at the individual level, aggregation bias can result. The bias arises when inferences about individual behaviors are made based on aggregate data. Example: Suppose we have data on individuals’ incomes and their personal consumption. At the individual level, it’s possible that as income rises, consumption also rises. However, when we aggregate the data to, say, a neighborhood level, neighborhoods with diverse income levels might all have similar average consumption due to other unobserved factors. Step 1: Create individual level data set.seed(123) # Generate data for 1000 individuals n &lt;- 1000 income &lt;- rnorm(n, mean = 50, sd = 10) consumption &lt;- 0.5 * income + rnorm(n, mean = 0, sd = 5) # Individual level regression individual_lm &lt;- lm(consumption ~ income) summary(individual_lm) #&gt; #&gt; Call: #&gt; lm(formula = consumption ~ income) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -15.1394 -3.4572 0.0213 3.5436 16.4557 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1.99596 0.82085 -2.432 0.0152 * #&gt; income 0.54402 0.01605 33.888 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.032 on 998 degrees of freedom #&gt; Multiple R-squared: 0.535, Adjusted R-squared: 0.5346 #&gt; F-statistic: 1148 on 1 and 998 DF, p-value: &lt; 2.2e-16 This would show a significant positive relationship between income and consumption. Step 2: Aggregate data to ‘neighborhood’ level # Assume 100 neighborhoods with 10 individuals each n_neighborhoods &lt;- 100 df &lt;- data.frame(income, consumption) df$neighborhood &lt;- rep(1:n_neighborhoods, each = n / n_neighborhoods) aggregate_data &lt;- aggregate(. ~ neighborhood, data = df, FUN = mean) # Aggregate level regression aggregate_lm &lt;- lm(consumption ~ income, data = aggregate_data) summary(aggregate_lm) #&gt; #&gt; Call: #&gt; lm(formula = consumption ~ income, data = aggregate_data) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.4517 -0.9322 -0.0826 1.0556 3.5728 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -4.94338 2.60699 -1.896 0.0609 . #&gt; income 0.60278 0.05188 11.618 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.54 on 98 degrees of freedom #&gt; Multiple R-squared: 0.5794, Adjusted R-squared: 0.5751 #&gt; F-statistic: 135 on 1 and 98 DF, p-value: &lt; 2.2e-16 If aggregation bias is present, the coefficient for income in the aggregate regression might be different from the coefficient in the individual regression, even if the individual relationship is significant and strong. library(ggplot2) # Individual scatterplot p1 &lt;- ggplot(df, aes(x = income, y = consumption)) + geom_point(aes(color = neighborhood), alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;) + labs(title = &quot;Individual Level Data&quot;) + causalverse::ama_theme() # Aggregate scatterplot p2 &lt;- ggplot(aggregate_data, aes(x = income, y = consumption)) + geom_point(color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;) + labs(title = &quot;Aggregate Level Data&quot;) + causalverse::ama_theme() # print(p1) # print(p2) gridExtra::grid.arrange(grobs = list(p1, p2), ncol = 2) From these plots, you can see the relationship at the individual level, with each neighborhood being colored differently in the first plot. The second plot shows the aggregate data, where each point now represents a whole neighborhood. Direction of Bias: The direction of the aggregation bias isn’t predetermined. It depends on the underlying relationship and the data distribution. In some cases, aggregation might attenuate (reduce) a relationship, while in other cases, it might exaggerate it. Relation to Other Biases: Aggregation bias is closely related to several other biases in econometrics: Specification bias: If you don’t properly account for the hierarchical structure of your data (like individuals nested within neighborhoods), your model might be mis-specified, leading to biased estimates. Measurement Error: Aggregation can introduce or amplify measurement errors. For instance, if you aggregate noisy measures, the aggregate might not accurately represent any underlying signal. Omitted Variable Bias (see Endogeneity): When you aggregate data, you lose information. If the loss of this information results in omitting important predictors that are correlated with both the independent and dependent variables, it can introduce omitted variable bias. 34.1.1 Simpson’s Paradox Simpson’s Paradox, also known as the Yule-Simpson effect, is a phenomenon in probability and statistics where a trend that appears in different groups of data disappears or reverses when the groups are combined. It’s a striking example of how aggregated data can sometimes provide a misleading representation of the actual situation. Illustration of Simpson’s Paradox: Consider a hypothetical scenario involving two hospitals: Hospital A and Hospital B. We want to analyze the success rates of treatments at both hospitals. When we break the data down by the severity of the cases (i.e., minor cases vs. major cases): Hospital A: Minor cases: 95% success rate Major cases: 80% success rate Hospital B: Minor cases: 90% success rate Major cases: 85% success rate From this breakdown, Hospital A appears to be better in treating both minor and major cases since it has a higher success rate in both categories. However, let’s consider the overall success rates without considering case severity: Hospital A: 83% overall success rate Hospital B: 86% overall success rate Suddenly, Hospital B seems better overall. This surprising reversal happens because the two hospitals might handle very different proportions of minor and major cases. For example, if Hospital A treats many more major cases (which have lower success rates) than Hospital B, it can drag down its overall success rate. Causes: Simpson’s Paradox can arise due to: A lurking or confounding variable that wasn’t initially considered (in our example, the severity of the medical cases). Different group sizes, where one group might be much larger than the other, influencing the aggregate results. Implications: Simpson’s Paradox highlights the dangers of interpreting aggregated data without considering potential underlying sub-group structures. It underscores the importance of disaggregating data and being aware of the context in which it’s analyzed. Relation to Aggregation Bias In the most extreme case, aggregation bias can reverse the coefficient sign of the relationship of interest (i.e., Simpson’s Paradox). Example: Suppose we are studying the effect of a new study technique on student grades. We have two groups of students: those who used the new technique (treatment = 1) and those who did not (treatment = 0). We want to see if using the new study technique is related to higher grades. Let’s assume grades are influenced by the starting ability of the students. Perhaps in our sample, many high-ability students didn’t use the new technique (because they felt they didn’t need it), while many low-ability students did. Here’s a setup: High-ability students tend to have high grades regardless of the technique. The new technique has a positive effect on grades, but this is masked by the fact that many low-ability students use it. set.seed(123) # Generate data for 1000 students n &lt;- 1000 # 500 students are of high ability, 500 of low ability ability &lt;- c(rep(&quot;high&quot;, 500), rep(&quot;low&quot;, 500)) # High ability students are less likely to use the technique treatment &lt;- ifelse(ability == &quot;high&quot;, rbinom(500, 1, 0.2), rbinom(500, 1, 0.8)) # Grades are influenced by ability and treatment (new technique), # but the treatment has opposite effects based on ability. grades &lt;- ifelse( ability == &quot;high&quot;, rnorm(500, mean = 85, sd = 5) + treatment * -3, rnorm(500, mean = 60, sd = 5) + treatment * 5 ) df &lt;- data.frame(ability, treatment, grades) # Regression without considering ability overall_lm &lt;- lm(grades ~ factor(treatment), data = df) summary(overall_lm) #&gt; #&gt; Call: #&gt; lm(formula = grades ~ factor(treatment), data = df) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -33.490 -4.729 0.986 6.368 25.607 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 80.0133 0.4373 183.0 &lt;2e-16 *** #&gt; factor(treatment)1 -11.7461 0.6248 -18.8 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 9.877 on 998 degrees of freedom #&gt; Multiple R-squared: 0.2615, Adjusted R-squared: 0.2608 #&gt; F-statistic: 353.5 on 1 and 998 DF, p-value: &lt; 2.2e-16 # Regression within ability groups high_ability_lm &lt;- lm(grades ~ factor(treatment), data = df[df$ability == &quot;high&quot;,]) low_ability_lm &lt;- lm(grades ~ factor(treatment), data = df[df$ability == &quot;low&quot;,]) summary(high_ability_lm) #&gt; #&gt; Call: #&gt; lm(formula = grades ~ factor(treatment), data = df[df$ability == #&gt; &quot;high&quot;, ]) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.2156 -3.4813 0.1186 3.4952 13.2919 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 85.1667 0.2504 340.088 &lt; 2e-16 *** #&gt; factor(treatment)1 -3.9489 0.5776 -6.837 2.37e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.046 on 498 degrees of freedom #&gt; Multiple R-squared: 0.08581, Adjusted R-squared: 0.08398 #&gt; F-statistic: 46.75 on 1 and 498 DF, p-value: 2.373e-11 summary(low_ability_lm) #&gt; #&gt; Call: #&gt; lm(formula = grades ~ factor(treatment), data = df[df$ability == #&gt; &quot;low&quot;, ]) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -13.3717 -3.5413 0.1097 3.3531 17.0568 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 59.8950 0.4871 122.956 &lt;2e-16 *** #&gt; factor(treatment)1 5.2979 0.5474 9.679 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.968 on 498 degrees of freedom #&gt; Multiple R-squared: 0.1583, Adjusted R-squared: 0.1566 #&gt; F-statistic: 93.68 on 1 and 498 DF, p-value: &lt; 2.2e-16 From this simulation: The overall_lm might show that the new study technique is associated with lower grades (negative coefficient), because many of the high-ability students (who naturally have high grades) did not use it. The high_ability_lm will likely show that high-ability students who used the technique had slightly lower grades than high-ability students who didn’t. The low_ability_lm will likely show that low-ability students who used the technique had much higher grades than low-ability students who didn’t. This is a classic example of Simpson’s Paradox: within each ability group, the technique appears beneficial, but when data is aggregated, the effect seems negative because of the distribution of the technique across ability groups. library(ggplot2) # Scatterplot for overall data p1 &lt;- ggplot(df, aes( x = factor(treatment), y = grades, color = ability )) + geom_jitter(width = 0.2, height = 0) + geom_boxplot(alpha = 0.6, outlier.shape = NA) + labs(title = &quot;Overall Effect of Study Technique on Grades&quot;, x = &quot;Treatment (0 = No Technique, 1 = New Technique)&quot;, y = &quot;Grades&quot;) # Scatterplot for high-ability students p2 &lt;- ggplot(df[df$ability == &quot;high&quot;, ], aes( x = factor(treatment), y = grades, color = ability )) + geom_jitter(width = 0.2, height = 0) + geom_boxplot(alpha = 0.6, outlier.shape = NA) + labs(title = &quot;Effect of Study Technique on Grades (High Ability)&quot;, x = &quot;Treatment (0 = No Technique, 1 = New Technique)&quot;, y = &quot;Grades&quot;) # Scatterplot for low-ability students p3 &lt;- ggplot(df[df$ability == &quot;low&quot;, ], aes( x = factor(treatment), y = grades, color = ability )) + geom_jitter(width = 0.2, height = 0) + geom_boxplot(alpha = 0.6, outlier.shape = NA) + labs(title = &quot;Effect of Study Technique on Grades (Low Ability)&quot;, x = &quot;Treatment (0 = No Technique, 1 = New Technique)&quot;, y = &quot;Grades&quot;) # print(p1) # print(p2) # print(p3) gridExtra::grid.arrange(grobs = list(p1, p2, p3), ncol = 1) "],["contamination-bias.html", "34.2 Contamination Bias", " 34.2 Contamination Bias Goldsmith-Pinkham, Hull, and Kolesár (2022) show regressions with multiple treatments and flexible controls often fail to estimate convex averages of heterogeneous treatment effects, resulting in contamination by non-convex averages of other treatments’ effects. 3 estimation methods to avoid this bias and find significant contamination bias in observational studies, with experimental studies showing less due to lower variability in propensity scores. References "],["survivorship-bias.html", "34.3 Survivorship Bias", " 34.3 Survivorship Bias Survivorship bias refers to the logical error of concentrating on the entities that have made it past some selection process and overlooking those that didn’t, typically because of a lack of visibility. This can skew results and lead to overly optimistic conclusions. Example: If you were to analyze the success of companies based only on the ones that are still in business today, you’d miss out on the insights from all those that failed. This would give you a distorted view of what makes a successful company, as you wouldn’t account for all those that had those same attributes but didn’t succeed. Relation to Other Biases: Sample Selection Bias: Survivorship bias is a specific form of sample selection bias. While survivorship bias focuses on entities that “survive”, sample selection bias broadly deals with any non-random sample. Confirmation Bias: Survivorship bias can reinforce confirmation bias. By only looking at the “winners”, we might confirm our existing beliefs about what leads to success, ignoring evidence to the contrary from those that didn’t survive. set.seed(42) # Generating data for 100 companies n &lt;- 100 # Randomly generate earnings; assume true average earnings is 50 earnings &lt;- rnorm(n, mean = 50, sd = 10) # Threshold for bankruptcy threshold &lt;- 40 # Only companies with earnings above the threshold &quot;survive&quot; survivor_earnings &lt;- earnings[earnings &gt; threshold] # Average earnings for all companies vs. survivors true_avg &lt;- mean(earnings) survivor_avg &lt;- mean(survivor_earnings) true_avg #&gt; [1] 50.32515 survivor_avg #&gt; [1] 53.3898 Using a histogram to visualize the distribution of earnings, highlighting the “survivors”. library(ggplot2) df &lt;- data.frame(earnings) p &lt;- ggplot(df, aes(x = earnings)) + geom_histogram( binwidth = 2, fill = &quot;grey&quot;, color = &quot;black&quot;, alpha = 0.7 ) + geom_vline(aes(xintercept = true_avg, color = &quot;True Avg&quot;), linetype = &quot;dashed&quot;, size = 1) + geom_vline( aes(xintercept = survivor_avg, color = &quot;Survivor Avg&quot;), linetype = &quot;dashed&quot;, size = 1 ) + scale_color_manual(values = c(&quot;True Avg&quot; = &quot;blue&quot;, &quot;Survivor Avg&quot; = &quot;red&quot;), name = &quot;Average Type&quot;) + labs(title = &quot;Distribution of Company Earnings&quot;, x = &quot;Earnings&quot;, y = &quot;Number of Companies&quot;) + causalverse::ama_theme() print(p) In the plot, the “True Avg” might be lower than the “Survivor Avg”, indicating that by only looking at the survivors, we overestimate the average earnings. Remedies: Awareness: Recognizing the potential for survivorship bias is the first step. Inclusive Data Collection: Wherever possible, try to include data from entities that didn’t “survive” in your sample. Statistical Techniques: In cases where the missing data is inherent, methods like Heckman’s two-step procedure can be used to correct for sample selection bias. External Data Sources: Sometimes, complementary datasets can provide insights into the missing “non-survivors”. Sensitivity Analysis: Test how sensitive your results are to assumptions about the non-survivors. "],["publication-bias.html", "34.4 Publication Bias", " 34.4 Publication Bias Publication bias occurs when the results of studies influence the likelihood of their being published. Typically, studies with significant, positive, or sensational results are more likely to be published than those with non-significant or negative results. This can skew the perceived effectiveness or results when researchers conduct meta-analyses or literature reviews, leading them to draw inaccurate conclusions. Example: Imagine pharmaceutical research. If 10 studies are done on a new drug, and only 2 show a positive effect while 8 show no effect, but only the 2 positive studies get published, a later review of the literature might erroneously conclude the drug is effective. Relation to Other Biases: Selection Bias: Publication bias is a form of selection bias, where the selection (publication in this case) isn’t random but based on the results of the study. Confirmation Bias: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might only find and cite studies that confirm their beliefs, overlooking the unpublished studies that might contradict them. Let’s simulate an experiment on a new treatment. We’ll assume that the treatment has no effect, but due to random variation, some studies will show significant positive or negative effects. set.seed(42) # Number of studies n &lt;- 100 # Assuming no real effect (effect size = 0) true_effect &lt;- 0 # Random variation in results results &lt;- rnorm(n, mean = true_effect, sd = 1) # Only &quot;significant&quot; results get published # (arbitrarily defining significant as abs(effect) &gt; 1.5) published_results &lt;- results[abs(results) &gt; 1.5] # Average effect for all studies vs. published studies true_avg_effect &lt;- mean(results) published_avg_effect &lt;- mean(published_results) true_avg_effect #&gt; [1] 0.03251482 published_avg_effect #&gt; [1] -0.3819601 Using a histogram to visualize the distribution of study results, highlighting the “published” studies. library(ggplot2) df &lt;- data.frame(results) p &lt;- ggplot(df, aes(x = results)) + geom_histogram( binwidth = 0.2, fill = &quot;grey&quot;, color = &quot;black&quot;, alpha = 0.7 ) + geom_vline( aes(xintercept = true_avg_effect, color = &quot;True Avg Effect&quot;), linetype = &quot;dashed&quot;, size = 1 ) + geom_vline( aes(xintercept = published_avg_effect, color = &quot;Published Avg Effect&quot;), linetype = &quot;dashed&quot;, size = 1 ) + scale_color_manual( values = c( &quot;True Avg Effect&quot; = &quot;blue&quot;, &quot;Published Avg Effect&quot; = &quot;red&quot; ), name = &quot;Effect Type&quot; ) + labs(title = &quot;Distribution of Study Results&quot;, x = &quot;Effect Size&quot;, y = &quot;Number of Studies&quot;) + causalverse::ama_theme() print(p) The plot might show that the “True Avg Effect” is around zero, while the “Published Avg Effect” is likely higher or lower, depending on which studies happen to have significant results in the simulation. Remedies: Awareness: Understand and accept that publication bias exists, especially when conducting literature reviews or meta-analyses. Study Registries: Encourage the use of study registries where researchers register their studies before they start. This way, one can see all initiated studies, not just the published ones. Publish All Results: Journals and researchers should make an effort to publish negative or null results. Some journals, known as “null result journals”, specialize in this. Funnel Plots and Egger’s Test: In meta-analyses, these are methods to visually and statistically detect publication bias. Use of Preprints: Promote the use of preprint servers where researchers can upload studies before they’re peer-reviewed, ensuring that results are available regardless of eventual publication status. p-curve analysis: addresses publication bias and p-hacking by analyzing the distribution of p-values below 0.05 in research studies. It posits that a right-skewed distribution of these p-values indicates a true effect, whereas a left-skewed distribution suggests p-hacking and no true underlying effect. The method includes a “half-curve” test to counteract extensive p-hacking Simonsohn, Simmons, and Nelson (2015). References "],["controls.html", "Chapter 35 Controls", " Chapter 35 Controls This section follows (Cinelli, Forney, and Pearl 2022) and code library(dagitty) library(ggdag) Traditional literature usually considers adding additional control variables is harmless to analysis. More specifically, this problem is most prevalent in the review process. Reviewers only ask authors to add more variables to “control” for such variable, which can be asked with only limited rationale. Rarely ever you will see a reviewer asks an author to remove some variables to see the behavior of the variable of interest (This is also related to Coefficient stability). However, adding more controls is only good in limited cases. References "],["bad-controls.html", "35.1 Bad Controls", " 35.1 Bad Controls 35.1.1 M-bias Traditional textbooks (G. W. Imbens and Rubin 2015; J. D. Angrist and Pischke 2009) consider \\(Z\\) as a good control because it’s a pre-treatment variable, where it correlates with the treatment and the outcome. This is most prevalent in Matching Methods, where we are recommended to include all “pre-treatment” variables. However, it is a bad control because it opens the back-door path \\(Z \\leftarrow U_1 \\to Z \\leftarrow U_2 \\to Y\\) # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; u1-&gt;x; u1-&gt;z; u2-&gt;z; u2-&gt;y}&quot;) # set u as latent latents(model) &lt;- c(&quot;u1&quot;, &quot;u2&quot;) ## coordinates for plotting coordinates(model) &lt;- list(x = c( x = 1, u1 = 1, z = 2, u2 = 3, y = 3 ), y = c( x = 1, u1 = 2, z = 1.5, u2 = 2, y = 1 )) ## ggplot ggdag(model) + theme_dag() Even though \\(Z\\) can correlate with both \\(X\\) and \\(Y\\) very well, it’s not a confounder. Controlling for \\(Z\\) can bias the \\(X \\to Y\\) estimate, because it opens the colliding path \\(X \\leftarrow U_1 \\rightarrow Z \\leftarrow U_2 \\leftarrow Y\\) n &lt;- 1e4 u1 &lt;- rnorm(n) u2 &lt;- rnorm(n) z &lt;- u1 + u2 + rnorm(n) x &lt;- u1 + rnorm(n) causal_coef &lt;- 2 y &lt;- causal_coef * x - 4*u2 + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.1: Model 1Model 2 (Intercept)-0.03&nbsp;&nbsp;&nbsp;&nbsp;-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.04)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp; x2.00 ***2.82 *** (0.03)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1.61 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.32&nbsp;&nbsp;&nbsp;&nbsp;0.58&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Another worse variation is # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; u1-&gt;x; u1-&gt;z; u2-&gt;z; u2-&gt;y; z-&gt;y}&quot;) # set u as latent latents(model) &lt;- c(&quot;u1&quot;, &quot;u2&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, u1=1, z=2, u2=3, y=3), y = c(x=1, u1=2, z=1.5, u2=2, y=1)) ## ggplot ggdag(model) + theme_dag() You can’t do much in this case. If you don’t control for \\(Z\\), then you have an open back-door path \\(X \\leftarrow U_1 \\to Z \\to Y\\), and the unadjusted estimate is biased If you control for \\(Z\\), then you open backdoor path \\(X \\leftarrow U_1 \\to Z \\leftarrow U_2 \\to Y\\), and the adjusted estimate is also biased Hence, we cannot identify the causal effect in this case. We can do sensitivity analyses to examine (Cinelli et al. 2019; Cinelli and Hazlett 2020) the plausible bounds on the strength of the direct effect of \\(Z \\to Y\\) the strength of the effects of the latent variables 35.1.2 Bias Amplification # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;x; u-&gt;y; z-&gt;x}&quot;) # set u as latent latents(model) &lt;- c(&quot;u&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(z=1, x=2, u=3, y=4), y = c(z=1, x=1, u=2, y=1)) ## ggplot ggdag(model) + theme_dag() Controlling for Z amplifies the omitted variable bias n &lt;- 1e4 z &lt;- rnorm(n) u &lt;- rnorm(n) x &lt;- 2*z + u + rnorm(n) y &lt;- x + 2*u + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.2: Model 1Model 2 (Intercept)0.01&nbsp;&nbsp;&nbsp;&nbsp;0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; x1.33 ***2.00 *** (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.01 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.71&nbsp;&nbsp;&nbsp;&nbsp;0.80&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. 35.1.3 Overcontrol bias Sometimes, this is similar to controlling for variables that are proxy of the dependent variable. # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;z; z-&gt;y}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, y=3), y = c(x=1, z=1, y=1)) ## ggplot ggdag(model) + theme_dag() If X is a proxy for Z (i.e., a mediator between Z and Y), controlling for Z is bad n &lt;- 1e4 x &lt;- rnorm(n) z &lt;- x + rnorm(n) y &lt;- z + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.3: Model 1Model 2 (Intercept)-0.02&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x1.00 ***-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.01 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.33&nbsp;&nbsp;&nbsp;&nbsp;0.66&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Now you see that \\(Z\\) is significant, which is technically true, but we are interested in the causal coefficient of \\(X\\) on \\(Y\\). Another setting for overcontrol bias is # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;m; m-&gt;z; m-&gt;y}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, m=2, z=2, y=3), y = c(x=2, m=2, z=1, y=2)) ## ggplot ggdag(model) + theme_dag() n &lt;- 1e4 x &lt;- rnorm(n) m &lt;- x + rnorm(n) z &lt;- m + rnorm(n) y &lt;- m + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.4: Model 1Model 2 (Intercept)0.01&nbsp;&nbsp;&nbsp;&nbsp;-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x0.99 ***0.48 *** (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.51 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.32&nbsp;&nbsp;&nbsp;&nbsp;0.50&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Another setting for this bias is # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;z; z-&gt;y; u-&gt;z; u-&gt;y}&quot;) # set u as latent latents(model) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, u=3, y=4), y = c(x=1, z=1, u=2, y=1)) ## ggplot ggdag(model) + theme_dag() set.seed(1) n &lt;- 1e4 x &lt;- rnorm(n) u &lt;- rnorm(n) z &lt;- x + u + rnorm(n) y &lt;- z + u + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.5: Model 1Model 2 (Intercept)-0.01&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x1.01 ***-0.47 *** (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.48 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.15&nbsp;&nbsp;&nbsp;&nbsp;0.78&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. The total effect of \\(X\\) on \\(Y\\) is not biased (i.e., \\(1.01 \\approx 1.48 - 0.47\\)). Controlling for Z will fail to identify the direct effect of \\(X\\) on \\(Y\\) and opens the biasing path \\(X \\rightarrow Z \\leftarrow U \\rightarrow Y\\) 35.1.4 Selection Bias Also known as “collider stratification bias” rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; x-&gt;z; u-&gt;z;u-&gt;y}&quot;) # set u as latent latents(model) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, u=2, y=3), y = c(x=3, z=2, u=4, y=3)) ## ggplot ggdag(model) + theme_dag() Adjusting \\(Z\\) opens the colliding path \\(X \\to Z \\leftarrow U \\to Y\\) n &lt;- 1e4 x &lt;- rnorm(n) u &lt;- rnorm(n) z &lt;- x + u + rnorm(n) y &lt;- x + 2*u + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.6: Model 1Model 2 (Intercept)-0.01&nbsp;&nbsp;&nbsp;&nbsp;0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; x0.97 ***-0.03&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.00 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.16&nbsp;&nbsp;&nbsp;&nbsp;0.49&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Another setting is rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; x-&gt;z; y-&gt;z}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, y=3), y = c(x=2, z=1, y=2)) ## ggplot ggdag(model) + theme_dag() Controlling \\(Z\\) opens the colliding path \\(X \\to Z \\leftarrow Y\\) n &lt;- 1e4 x &lt;- rnorm(n) y &lt;- x + rnorm(n) z &lt;- x + y + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.7: Model 1Model 2 (Intercept)0.00&nbsp;&nbsp;&nbsp;&nbsp;0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x1.03 ***-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.51 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.51&nbsp;&nbsp;&nbsp;&nbsp;0.76&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. 35.1.5 Case-control Bias rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; y-&gt;z}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, y=3), y = c(x=2, z=1, y=2)) ## ggplot ggdag(model) + theme_dag() Controlling \\(Z\\) opens a virtual collider (a descendant of a collider). However, if \\(X\\) truly has no causal effect on \\(Y\\). Then, controlling for \\(Z\\) is valid for testing whether the effect of \\(X\\) on \\(Y\\) is 0 because X is d-separated from \\(Y\\) regardless of adjusting for \\(Z\\) n &lt;- 1e4 x &lt;- rnorm(n) y &lt;- x + rnorm(n) z &lt;- y + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.8: Model 1Model 2 (Intercept)-0.00&nbsp;&nbsp;&nbsp;&nbsp;-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x1.00 ***0.50 *** (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.50 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.50&nbsp;&nbsp;&nbsp;&nbsp;0.75&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. References "],["good-controls.html", "35.2 Good Controls", " 35.2 Good Controls 35.2.1 Omitted Variable Bias Correction This is when \\(Z\\) can block all back-door paths. rm(list = ls()) model &lt;- dagitty(&quot;dag{x-&gt;y; z-&gt;x; z-&gt;y}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, y=3, z=2), y = c(x=1, y=1, z=2)) ## ggplot ggdag(model) + theme_dag() Unadjusted estimate is biased adjusting for \\(Z\\) blocks the backdoor path n &lt;- 1e4 z &lt;- rnorm(n) causal_coef = 2 beta2 = 3 x &lt;- z + rnorm(n) y &lt;- causal_coef * x + beta2 * z + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.9: Model 1Model 2 (Intercept)0.01&nbsp;&nbsp;&nbsp;&nbsp;-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x3.51 ***2.00 *** (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.02 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.82&nbsp;&nbsp;&nbsp;&nbsp;0.97&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. # cleans workspace rm(list = ls()) # Draw DAG # specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; z-&gt;x; u-&gt;y}&quot;) # set u as latent latents(model) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, u=3, y = 4), y = c(x=1, y=1, z=2, u = 3)) ## ggplot ggdag(model) + theme_dag() Unadjusted estimate is biased adjusting for \\(Z\\) blocks the backdoor door path due to \\(U\\) n &lt;- 1e4 u &lt;- rnorm(n) z &lt;- u + rnorm(n) causal_coef = 2 x &lt;- z + rnorm(n) y &lt;- causal_coef * x + u + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.10: Model 1Model 2 (Intercept)0.03 *&nbsp;&nbsp;0.03 *&nbsp;&nbsp; (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x2.34 ***2.01 *** (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.49 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.91&nbsp;&nbsp;&nbsp;&nbsp;0.92&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Even though \\(Z\\) is significant, we cannot give it a causal interpretation. # cleans workspace rm(list = ls()) # Draw DAG # specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; u-&gt;x; z-&gt;y}&quot;) # set u as latent latents(model) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=3, u=2, y = 4), y = c(x=1, y=1, z=2, u = 3)) ## ggplot ggdag(model) + theme_dag() n &lt;- 1e4 u &lt;- rnorm(n) z &lt;- u + rnorm(n) x &lt;- u + rnorm(n) causal_coef &lt;- 2 y &lt;- causal_coef * x + z + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 17.1: Model 1Model 2 (Intercept)-0.03&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x2.51 ***2.01 *** (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.01 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.84&nbsp;&nbsp;&nbsp;&nbsp;0.93&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Even though \\(Z\\) is significant, we cannot give it a causal interpretation. Summary # cleans workspace rm(list = ls()) # Model 1 model1 &lt;- dagitty(&quot;dag{x-&gt;y; z-&gt;x; z-&gt;y}&quot;) ## coordinates for plotting coordinates(model1) &lt;- list( x = c(x=1, y=3, z=2), y = c(x=1, y=1, z=2)) # Model 2 # specify edges model2 &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; z-&gt;x; u-&gt;y}&quot;) # set u as latent latents(model2) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model2) &lt;- list( x = c(x=1, z=2, u=3, y = 4), y = c(x=1, y=1, z=2, u = 3)) # Model 3 # specify edges model3 &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; u-&gt;x; z-&gt;y}&quot;) # set u as latent latents(model3) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model3) &lt;- list( x = c(x=1, z=3, u=2, y = 4), y = c(x=1, y=1, z=2, u = 3)) par(mfrow=c(1,3)) ## ggplot ggdag(model1) + theme_dag() ## ggplot ggdag(model2) + theme_dag() ## ggplot ggdag(model3) + theme_dag() 35.2.2 Omitted Variable Bias in Mediation Correction Common causes of \\(X\\) and any mediator (between \\(X\\) and \\(Y\\)) confound the effect of \\(X\\) on \\(Y\\) # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; z-&gt;x; x-&gt;m; z-&gt;m; m-&gt;y}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, m=3, y=4), y = c(x=1, z=2, m=1, y=1)) ## ggplot ggdag(model) + theme_dag() \\(Z\\) is a confounder of both the mediator \\(M\\) and \\(X\\) n &lt;- 1e4 z &lt;- rnorm(n) x &lt;- z + rnorm(n) causal_coef &lt;- 2 m &lt;- causal_coef * x + z + rnorm(n) y &lt;- m + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.11: Model 1Model 2 (Intercept)-0.02&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x2.49 ***1.97 *** (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.02 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.83&nbsp;&nbsp;&nbsp;&nbsp;0.86&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; z-&gt;x; x-&gt;m; u-&gt;m; m-&gt;y}&quot;) # set u as latent latents(model) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, u=3, m=4, y=5), y = c(x=1, z=2, u=3, m=1, y=1)) ## ggplot ggdag(model) + theme_dag() n &lt;- 1e4 u &lt;- rnorm(n) z &lt;- u + rnorm(n) x &lt;- z + rnorm(n) causal_coef &lt;- 2 m &lt;- causal_coef * x + u + rnorm(n) y &lt;- m + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 17.2: Model 1Model 2 (Intercept)-0.01&nbsp;&nbsp;&nbsp;&nbsp;-0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; x2.31 ***2.00 *** (0.01)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.49 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.86&nbsp;&nbsp;&nbsp;&nbsp;0.86&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; z-&gt;m; x-&gt;m; u-&gt;x; m-&gt;y}&quot;) # set u as latent latents(model) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=3, u=2, m=4, y=5), y = c(x=1, z=2, u=3, m=1, y=1)) ## ggplot ggdag(model) + theme_dag() n &lt;- 1e4 u &lt;- rnorm(n) z &lt;- u + rnorm(n) x &lt;- u + rnorm(n) causal_coef &lt;- 2 m &lt;- causal_coef * x + z + rnorm(n) y &lt;- m + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.12: Model 1Model 2 (Intercept)0.01&nbsp;&nbsp;&nbsp;&nbsp;-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x2.50 ***1.99 *** (0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.02 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.78&nbsp;&nbsp;&nbsp;&nbsp;0.87&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Summary # model 4 model4 &lt;- dagitty(&quot;dag{x-&gt;y; z-&gt;x; x-&gt;m; z-&gt;m; m-&gt;y}&quot;) ## coordinates for plotting coordinates(model4) &lt;- list( x = c(x=1, z=2, m=3, y=4), y = c(x=1, z=2, m=1, y=1)) # model 5 model5 &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; z-&gt;x; x-&gt;m; u-&gt;m; m-&gt;y}&quot;) # set u as latent latents(model5) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model5) &lt;- list( x = c(x=1, z=2, u=3, m=4, y=5), y = c(x=1, z=2, u=3, m=1, y=1)) # model 6 model6 &lt;- dagitty(&quot;dag{x-&gt;y; u-&gt;z; z-&gt;m; x-&gt;m; u-&gt;x; m-&gt;y}&quot;) # set u as latent latents(model6) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model6) &lt;- list( x = c(x=1, z=3, u=2, m=4, y=5), y = c(x=1, z=2, u=3, m=1, y=1)) par(mfrow=c(1,3)) ## ggplot ggdag(model4) + theme_dag() ## ggplot ggdag(model5) + theme_dag() ## ggplot ggdag(model6) + theme_dag() "],["neutral-controls.html", "35.3 Neutral Controls", " 35.3 Neutral Controls 35.3.1 Good Predictive Controls Good for precision # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; z-&gt;y}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, y=2), y = c(x=1, z=2, y=1)) ## ggplot ggdag(model) + theme_dag() Controlling for \\(Z\\) does not help or hurt identification, but it can increase precision (i.e., reducing SE) n &lt;- 1e4 z &lt;- rnorm(n) x &lt;- rnorm(n) y &lt;- x + 2 * z + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.13: Model 1Model 2 (Intercept)0.01&nbsp;&nbsp;&nbsp;&nbsp;0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; x1.00 ***1.01 *** (0.02)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.00 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.17&nbsp;&nbsp;&nbsp;&nbsp;0.83&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Similar coefficients, but smaller SE when controlling for \\(Z\\) Another variation is # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; x-&gt;m; z-&gt;m; m-&gt;y}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, m=2, y=3), y = c(x=1, z=2, m=1, y=1)) ## ggplot ggdag(model) + theme_dag() n &lt;- 1e4 z &lt;- rnorm(n) x &lt;- rnorm(n) m &lt;- 2 * z + rnorm(n) y &lt;- x + 2 * m + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.14: Model 1Model 2 (Intercept)-0.00&nbsp;&nbsp;&nbsp;&nbsp;-0.00&nbsp;&nbsp;&nbsp;&nbsp; (0.05)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; x0.97 ***0.99 *** (0.05)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.02 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.04&nbsp;&nbsp;&nbsp;&nbsp;0.77&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Controlling for \\(Z\\) can reduce SE 35.3.2 Good Selection Bias # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; x-&gt;z; z-&gt;w; u-&gt;w;u-&gt;y}&quot;) # set u as latent latents(model) &lt;- &quot;u&quot; ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, w=3, u=3, y=5), y = c(x=3, z=2, w=1, u=4, y=3)) ## ggplot ggdag(model) + theme_dag() Unadjusted estimate is unbiased Controlling for Z can increase SE Controlling for Z while having on W can help identify X n &lt;- 1e4 x &lt;- rnorm(n) u &lt;- rnorm(n) z &lt;- x + rnorm(n) w &lt;- z + u + rnorm(n) y &lt;- x - 2*u + rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + w), lm(y ~ x + z + w)) Table 35.15: Model 1Model 2Model 3 (Intercept)0.01&nbsp;&nbsp;&nbsp;&nbsp;0.01&nbsp;&nbsp;&nbsp;&nbsp;0.03&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; x0.99 ***1.65 ***0.99 *** (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.67 ***-1.01 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp;(0.01)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.02 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.16&nbsp;&nbsp;&nbsp;&nbsp;0.39&nbsp;&nbsp;&nbsp;&nbsp;0.50&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. 35.3.3 Bad Predictive Controls # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; z-&gt;x}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=1, y=2), y = c(x=1, z=2, y=1)) ## ggplot ggdag(model) + theme_dag() n &lt;- 1e4 z &lt;- rnorm(n) x &lt;- 2 * z + rnorm(n) y &lt;- x + 2 * rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.16: Model 1Model 2 (Intercept)-0.02&nbsp;&nbsp;&nbsp;&nbsp;-0.02&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; x0.99 ***1.00 *** (0.01)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.00&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.04)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.55&nbsp;&nbsp;&nbsp;&nbsp;0.55&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Similar coefficients, but greater SE when controlling for \\(Z\\) Another variation is rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; x-&gt;z}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=1, y=2), y = c(x=1, z=2, y=1)) ## ggplot ggdag(model) + theme_dag() set.seed(1) n &lt;- 1e4 x &lt;- rnorm(n) z &lt;- 2 * x + rnorm(n) y &lt;- x + 2 * rnorm(n) jtools::export_summs(lm(y ~ x), lm(y ~ x + z)) Table 35.17: Model 1Model 2 (Intercept)0.02&nbsp;&nbsp;&nbsp;&nbsp;0.02&nbsp;&nbsp;&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; x1.00 ***0.99 *** (0.02)&nbsp;&nbsp;&nbsp;(0.05)&nbsp;&nbsp;&nbsp; z&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.00&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp; N10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.20&nbsp;&nbsp;&nbsp;&nbsp;0.20&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Worse SE when controlling for \\(Z\\) (\\(0.02 &lt; 0.05\\)) 35.3.4 Bad Selection Bias # cleans workspace rm(list = ls()) # DAG ## specify edges model &lt;- dagitty(&quot;dag{x-&gt;y; x-&gt;z}&quot;) ## coordinates for plotting coordinates(model) &lt;- list( x = c(x=1, z=2, y=2), y = c(x=1, z=2, y=1)) ## ggplot ggdag(model) + theme_dag() Not all post-treatment variables are bad. Controlling for \\(Z\\) is neutral, but it might hurt the precision of the causal effect. "],["choosing-controls.html", "35.4 Choosing Controls", " 35.4 Choosing Controls library(pcalg) library(dagitty) library(causaleffect) By providing a causal diagram, deciding the appropriateness of controls are automated. Fusion DAGitty Guide on how to choose confounders: T. J. VanderWeele (2019) In cases where it’s hard to determine the plausibility of controls, we might need to further analysis. sensemakr provides such tools. library(sensemakr) In simple cases, we can follow the simple rules of thumb provided by Steinmetz and Block (2022) (p. 614, Fig 2) References "],["mediation.html", "Chapter 36 Mediation ", " Chapter 36 Mediation "],["traditional-approach.html", "36.1 Traditional Approach", " 36.1 Traditional Approach Baron and Kenny (1986) is outdated because of step 1, but we could still see the original idea. 3 regressions Step 1: \\(X \\to Y\\) Step 2: \\(X \\to M\\) Step 3: \\(X + M \\to Y\\) where \\(X\\) = independent (causal) variable \\(Y\\) = dependent (outcome) variable \\(M\\) = mediating variable Note: Originally, the first path from \\(X \\to Y\\) suggested by (Baron and Kenny 1986) needs to be significant. But there are cases in which you could have indirect of \\(X\\) on \\(Y\\) without significant direct effect of \\(X\\) on \\(Y\\) (e.g., when the effect is absorbed into M, or there are two counteracting effects \\(M_1, M_2\\) that cancel out each other effect). Unmediated model where \\(c\\) is the total effect where \\(c&#39;\\) = direct effect (effect of \\(X\\) on \\(Y\\) after accounting for the indirect path) \\(ab\\) = indirect effect Hence, \\[ \\begin{aligned} \\text{total effect} &amp;= \\text{direct effect} + \\text{indirect effect} \\\\ c &amp;= c&#39; + ab \\end{aligned} \\] However, this simple equation does not only hold in cases of Models with latent variables Logistic models (only approximately). Hence, you can only calculate \\(c\\) as the total effect of \\(c&#39; + ab\\) Multi-level models (Bauer, Preacher, and Gil 2006) To measure mediation (i.e., indirect effect), \\(1 - \\frac{c&#39;}{c}\\) highly unstable (D. P. MacKinnon, Warsi, and Dwyer 1995), especially in cases that \\(c\\) is small (not re* recommended) Product method: \\(a\\times b\\) Difference method: \\(c- c&#39;\\) For linear models, we have the following assumptions: No unmeasured confound between \\(X-Y\\), \\(X-M\\) and \\(M-Y\\) relationships. \\(X \\not\\rightarrow C\\) where \\(C\\) is a confounder between \\(M-Y\\) relationship Reliability: No errors in measurement of \\(M\\) (also known as reliability assumption) (can consider errors-in-variables models) Mathematically, \\[ Y = b_0 + b_1 X + \\epsilon \\] \\(b_1\\) does not need to be significant. We examine the effect of \\(X\\) on \\(M\\). This step requires that there is a significant effect of \\(X\\) on \\(M\\) to continue with the analysis Mathematically, \\[ M = b_0 + b_2 X + \\epsilon \\] where \\(b_2\\) needs to be significant. In this step, we want to the effect of \\(M\\) on \\(Y\\) “absorbs” most of the direct effect of \\(X\\) on \\(Y\\) (or at least makes the effect smaller). Mathematically, \\[ Y = b_0 + b_4 X + b_3 M + \\epsilon \\] \\(b_4\\) needs to be either smaller or insignificant. The effect of \\(X\\) on \\(Y\\) then, \\(M\\) … mediates between \\(X\\) and \\(Y\\) completely disappear (\\(b_4\\) insignificant) Fully (i.e., full mediation) partially disappear (\\(b_4 &lt; b_1\\) in step 1) Partially (i.e., partial mediation) Examine the mediation effect (i.e., whether it is significant) Sobel Test (Sobel 1982) Joint Significance Test Bootstrapping Shrout and Bolger (2002) (preferable) Notes: Proximal mediation (\\(a &gt; b\\)) can lead to multicollinearity and reduce statistical power, whereas distal mediation (\\(b &gt; a\\)) is preferred for maximizing test power. The ideal balance for maximizing power in mediation analysis involves slightly distal mediators (i.e., path \\(b\\) is somewhat larger than path \\(a\\)) (Hoyle 1999). Tests for direct effects (c and c’) have lower power compared to the indirect effect (ab), making it possible for ab to be significant while c is not, even in cases where there seems to be complete mediation but no statistical evidence of a direct cause-effect relationship between X and Y without considering M (Kenny and Judd 2014). The testing of \\(ab\\) offers a power advantage over \\(c’\\) because it effectively combines two tests. However, claims of complete mediation based solely on the non-significance of \\(c’\\) should be approached with caution, emphasizing the need for sufficient sample size and power, especially in assessing partial mediation. Or one should never make complete mediation claim (Hayes and Scharkow 2013) 36.1.1 Assumptions 36.1.1.1 Direction Quick fix but not convincing: Measure \\(X\\) before \\(M\\) and \\(Y\\) to prevent \\(M\\) or \\(Y\\) causing \\(X\\); measure \\(M\\) before \\(Y\\) to avoid \\(Y\\) causing \\(M\\). \\(Y\\) may cause \\(M\\) in a feedback model. Assuming \\(c&#39; =0\\) (full mediation) allows for estimating models with reciprocal causal effects between \\(M\\) and \\(Y\\) via IV estimation. E. R. Smith (1982) proposes treating both \\(M\\) and \\(Y\\) as outcomes with potential to mediate each other, requiring distinct instrumental variables for each that do not affect the other. 36.1.1.2 Interaction When M interact with X to affect Y, M is both a mediator and a mediator (Baron and Kenny 1986). Interaction between \\(XM\\) should always be estimated. For the interpretation of this interaction, see (T. VanderWeele 2015) 36.1.1.3 Reliability When mediator contains measurement errors, \\(b, c&#39;\\) are biased. Possible fix: mediator = latent variable (but loss of power) (Ledgerwood and Shrout 2011) \\(b\\) is attenuated (closer to 0) \\(c&#39;\\) is overestimated when \\(ab &gt;0\\) underestiamted when \\(ab&lt;0\\) When treatment contains measurement errors, \\(a,b\\) are biased \\(a\\) is attenuated \\(b\\) is overestimated when \\(ac&#39;&gt;0\\) underestimated when \\(ac&#39; &lt;0\\) When outcome contains measurement errors, If unstandardized, no bias If standardized, attenuation bias 36.1.1.4 Confounding Omitted variable bias can happen to any pair of relationships To deal with this problem, one can either use Design Strategies Statistical Strategies 36.1.1.4.1 Design Strategies Randomization of treatment variable. If possible, also mediator Control for the confounder (but still only for measureable observables) 36.1.1.4.2 Statistical Strategies Instrumental variable on treatment Specifically for confounder affecting the \\(M-Y\\) pair, front-door adjustment is possible when there is a variable that completely mediates the effect of the mediator on the outcome and is unaffected by the confounder. Weighting methods (e.g., inverse propensity) See Heiss for R code Need strong ignorability assumption (i.e.., all confounders are included and measured without error (Westfall and Yarkoni 2016)). Not fixable, but can be examined with robustness checks. 36.1.2 Indirect Effect Tests 36.1.2.1 Sobel Test developed by Sobel (1982) also known as the delta method not recommend because it assumes the indirect effect \\(b\\) has a normal distribution when it’s not (D. P. MacKinnon, Warsi, and Dwyer 1995). Mediation can occur even if direct and indirect effects oppose each other, termed “inconsistent mediation” (D. P. MacKinnon, Fairchild, and Fritz 2007). This is when the mediator acts as a suppressor variable. Standard Error \\[ \\sqrt{\\hat{b}^2 s_{\\hat{a}} + \\hat{a}^2 s_{b}^2} \\] The test of the indirect effect is \\[ z = \\frac{\\hat{ab}}{\\sqrt{\\hat{b}^2 s_{\\hat{a}} + \\hat{a}^2 s_{b}^2}} \\] Disadvantages Assume \\(a\\) and \\(b\\) are independent. Assume \\(ab\\) is normally distributed. Does not work well for small sample sizes. Power of the test is low and the test is conservative as compared to Bootstrapping. 36.1.2.2 Joint Significance Test Effective for determining if the indirect effect is nonzero (by testing whether \\(a\\) and \\(b\\) are both statistically significant), assumes \\(a \\perp b\\). It’s recommended to use it with other tests and has similar performance to a Bootstrapping test (Hayes and Scharkow 2013). The test’s accuracy can be affected by heteroscedasticity (Fossum and Montoya 2023) but not by non-normality. Although helpful in computing power for the test of the indirect effect, it doesn’t provide a confidence interval for the effect. 36.1.2.3 Bootstrapping First used by Bollen and Stine (1990) It allows for the calculation of confidence intervals, p-values, etc. It does not require \\(a \\perp b\\) and corrects for bias in the bootstrapped distribution. It can handle non-normality (in the sampling distribution of the indirect effect), complex models, and small samples. Concerns exist about the bias-corrected bootstrapping being too liberal (Fritz, Taylor, and MacKinnon 2012). Hence, current recommendations favor percentile bootstrap without bias correction for better Type I error rates (Tibbe and Montoya 2022). A special case of bootstrapping is a proposed by where you don’t need access to raw data to generate resampling, you only need \\(a, b, var(a), var(b), cov(a,b)\\) (which can be taken from lots of primary studies) result &lt;- causalverse::med_ind( a = 0.5, b = 0.7, var_a = 0.04, var_b = 0.05, cov_ab = 0.01 ) result$plot 36.1.2.3.1 With Instrument library(DiagrammeR) grViz(&quot; digraph { graph [] node [shape = plaintext] X [label = &#39;Treatment&#39;] Y [label = &#39;Outcome&#39;] edge [minlen = 2] X-&gt;Y { rank = same; X; Y } } &quot;) grViz(&quot; digraph { graph [] node [shape = plaintext] X [label =&#39;Treatment&#39;, shape = box] Y [label =&#39;Outcome&#39;, shape = box] M [label =&#39;Mediator&#39;, shape = box] IV [label =&#39;Instrument&#39;, shape = box] edge [minlen = 2] IV-&gt;X X-&gt;M M-&gt;Y X-&gt;Y { rank = same; X; Y; M } } &quot;) library(mediation) data(&quot;boundsdata&quot;) library(fixest) # Total Effect out1 &lt;- feols(out ~ ttt, data = boundsdata) # Indirect Effect out2 &lt;- feols(med ~ ttt, data = boundsdata) # Direct and Indirect Effect out3 &lt;- feols(out ~ med + ttt, data = boundsdata) # Proportion Test # To what extent is the effect of the treatment mediated by the mediator? coef(out2)[&#39;ttt&#39;] * coef(out3)[&#39;med&#39;] / coef(out1)[&#39;ttt&#39;] * 100 #&gt; ttt #&gt; 68.63609 # Sobel Test bda::mediation.test(boundsdata$med, boundsdata$ttt, boundsdata$out) |&gt; tibble::rownames_to_column() |&gt; causalverse::nice_tab(2) #&gt; rowname Sobel Aroian Goodman #&gt; 1 z.value 4.05 4.03 4.07 #&gt; 2 p.value 0.00 0.00 0.00 # Mediation Analysis using boot library(boot) set.seed(1) mediation_fn &lt;- function(data, i){ # sample the dataset df &lt;- data[i,] a_path &lt;- feols(med ~ ttt, data = df) a &lt;- coef(a_path)[&#39;ttt&#39;] b_path &lt;- feols(out ~ med + ttt, data = df) b &lt;- coef(b_path)[&#39;med&#39;] cp &lt;- coef(b_path)[&#39;ttt&#39;] # indirect effect ind_ef &lt;- a*b total_ef &lt;- a*b + cp return(c(ind_ef, total_ef)) } boot_med &lt;- boot(boundsdata, mediation_fn, R = 100, parallel = &quot;multicore&quot;, ncpus = 2) boot_med #&gt; #&gt; ORDINARY NONPARAMETRIC BOOTSTRAP #&gt; #&gt; #&gt; Call: #&gt; boot(data = boundsdata, statistic = mediation_fn, R = 100, parallel = &quot;multicore&quot;, #&gt; ncpus = 2) #&gt; #&gt; #&gt; Bootstrap Statistics : #&gt; original bias std. error #&gt; t1* 0.04112035 0.0006346725 0.009539903 #&gt; t2* 0.05991068 -0.0004462572 0.029556611 summary(boot_med) |&gt; causalverse::nice_tab() #&gt; R original bootBias bootSE bootMed #&gt; 1 100 0.04 0 0.01 0.04 #&gt; 2 100 0.06 0 0.03 0.06 # confidence intervals (percentile is always recommended) boot.ci(boot_med, type = c(&quot;norm&quot;, &quot;perc&quot;)) #&gt; BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS #&gt; Based on 100 bootstrap replicates #&gt; #&gt; CALL : #&gt; boot.ci(boot.out = boot_med, type = c(&quot;norm&quot;, &quot;perc&quot;)) #&gt; #&gt; Intervals : #&gt; Level Normal Percentile #&gt; 95% ( 0.0218, 0.0592 ) ( 0.0249, 0.0623 ) #&gt; Calculations and Intervals on Original Scale #&gt; Some percentile intervals may be unstable # point estimates (Indirect, and Total Effects) colMeans(boot_med$t) #&gt; [1] 0.04175502 0.05946442 Alternatively, one can use the robmed package library(robmed) Power test or use app library(pwr2ppl) # indirect path ab power medjs( # X on M (path a) rx1m1 = .3, # correlation between X and Y (path c&#39;) rx1y = .1, # correlation between M and Y (path b) rym1 = .3, # sample size n = 100, alpha = 0.05, # number of mediators mvars = 1, # should use 10000 rep = 1000 ) 36.1.3 Multiple Mediation The most general package to handle multiple cases is manymome See vignette for an example library(manymome) 36.1.3.1 Multiple Mediators Notes Vignette Package library(mma) 36.1.3.1.0.1 Lavaan # Load required packages library(MASS) # for mvrnorm library(lavaan) # Function to generate synthetic data with correctly correlated errors for mediators generate_data &lt;- function(n = 10000, a1 = 0.5, a2 = -0.35, b1 = 0.7, b2 = 0.48, corr = TRUE, correlation_value = 0.7) { set.seed(12345) X &lt;- rnorm(n) # Generate correlated errors using a multivariate normal distribution if (corr) { Sigma &lt;- matrix(c(1, correlation_value, correlation_value, 1), nrow = 2) # Higher covariance matrix for errors errors &lt;- mvrnorm(n, mu = c(0, 0), Sigma = Sigma) # Generate correlated errors } else { errors &lt;- mvrnorm(n, mu = c(0, 0), Sigma = diag(2)) # Independent errors } M1 &lt;- a1 * X + errors[, 1] M2 &lt;- a2 * X + errors[, 2] Y &lt;- b1 * M1 + b2 * M2 + rnorm(n) # Y depends on M1 and M2 data.frame(X = X, M1 = M1, M2 = M2, Y = Y) } # Ground truth for comparison ground_truth &lt;- data.frame(Parameter = c(&quot;b1&quot;, &quot;b2&quot;), GroundTruth = c(0.7, 0.48)) # Function to extract relevant estimates, standard errors, and model fit extract_estimates_b1_b2 &lt;- function(fit) { estimates &lt;- parameterEstimates(fit) estimates &lt;- estimates[estimates$lhs == &quot;Y&quot; &amp; estimates$rhs %in% c(&quot;M1&quot;, &quot;M2&quot;), c(&quot;rhs&quot;, &quot;est&quot;, &quot;se&quot;)] estimates$Parameter &lt;- ifelse(estimates$rhs == &quot;M1&quot;, &quot;b1&quot;, &quot;b2&quot;) estimates &lt;- estimates[, c(&quot;Parameter&quot;, &quot;est&quot;, &quot;se&quot;)] fit_stats &lt;- fitMeasures(fit, c(&quot;aic&quot;, &quot;bic&quot;, &quot;rmsea&quot;, &quot;chisq&quot;)) return(list(estimates = estimates, fit_stats = fit_stats)) } # Case 1: Correlated errors for mediators (modeled correctly) Data_corr &lt;- generate_data(n = 10000, corr = TRUE, correlation_value = 0.7) model_corr &lt;- &#39; Y ~ b1 * M1 + b2 * M2 + c * X M1 ~ a1 * X M2 ~ a2 * X M1 ~~ M2 # Correlated mediators (errors) &#39; fit_corr &lt;- sem(model = model_corr, data = Data_corr) results_corr &lt;- extract_estimates_b1_b2(fit_corr) # Case 2: Uncorrelated errors for mediators (modeled correctly) Data_uncorr &lt;- generate_data(n = 10000, corr = FALSE) model_uncorr &lt;- &#39; Y ~ b1 * M1 + b2 * M2 + c * X M1 ~ a1 * X M2 ~ a2 * X &#39; fit_uncorr &lt;- sem(model = model_uncorr, data = Data_uncorr) results_uncorr &lt;- extract_estimates_b1_b2(fit_uncorr) # Case 3: Correlated errors, but not modeled as correlated fit_corr_incorrect &lt;- sem(model = model_uncorr, data = Data_corr) results_corr_incorrect &lt;- extract_estimates_b1_b2(fit_corr_incorrect) # Case 4: Uncorrelated errors, but modeled as correlated fit_uncorr_incorrect &lt;- sem(model = model_corr, data = Data_uncorr) results_uncorr_incorrect &lt;- extract_estimates_b1_b2(fit_uncorr_incorrect) # Combine all estimates for comparison estimates_combined &lt;- list( &quot;Correlated (Correct)&quot; = results_corr$estimates, &quot;Uncorrelated (Correct)&quot; = results_uncorr$estimates, &quot;Correlated (Incorrect)&quot; = results_corr_incorrect$estimates, &quot;Uncorrelated (Incorrect)&quot; = results_uncorr_incorrect$estimates ) # Combine all into a single table comparison_table &lt;- do.call(rbind, lapply(names(estimates_combined), function(case) { df &lt;- estimates_combined[[case]] df$Case &lt;- case df })) # Merge with ground truth for final comparison comparison_table &lt;- merge(comparison_table, ground_truth, by = &quot;Parameter&quot;) # Display the comparison table comparison_table #&gt; Parameter est se Case GroundTruth #&gt; 1 b1 0.7002984 0.013870433 Correlated (Correct) 0.70 #&gt; 2 b1 0.6973612 0.009859426 Uncorrelated (Correct) 0.70 #&gt; 3 b1 0.7002984 0.010010367 Correlated (Incorrect) 0.70 #&gt; 4 b1 0.6973612 0.009859634 Uncorrelated (Incorrect) 0.70 #&gt; 5 b2 0.4871118 0.013805615 Correlated (Correct) 0.48 #&gt; 6 b2 0.4868318 0.010009908 Uncorrelated (Correct) 0.48 #&gt; 7 b2 0.4871118 0.009963588 Correlated (Incorrect) 0.48 #&gt; 8 b2 0.4868318 0.010010119 Uncorrelated (Incorrect) 0.48 # Display model fit statistics for each case fit_stats_combined &lt;- list( &quot;Correlated (Correct)&quot; = results_corr$fit_stats, &quot;Uncorrelated (Correct)&quot; = results_uncorr$fit_stats, &quot;Correlated (Incorrect)&quot; = results_corr_incorrect$fit_stats, &quot;Uncorrelated (Incorrect)&quot; = results_uncorr_incorrect$fit_stats ) fit_stats_combined #&gt; $`Correlated (Correct)` #&gt; aic bic rmsea chisq #&gt; 77932.45 77997.34 0.00 0.00 #&gt; #&gt; $`Uncorrelated (Correct)` #&gt; aic bic rmsea chisq #&gt; 84664.312 84721.995 0.000 0.421 #&gt; #&gt; $`Correlated (Incorrect)` #&gt; aic bic rmsea chisq #&gt; 84453.208 84510.891 0.808 6522.762 #&gt; #&gt; $`Uncorrelated (Incorrect)` #&gt; aic bic rmsea chisq #&gt; 84665.89 84730.78 0.00 0.00 36.1.3.2 Multiple Treatments (Hayes and Preacher 2014) Code in Process References "],["causal-inference-approach.html", "36.2 Causal Inference Approach", " 36.2 Causal Inference Approach 36.2.1 Example 1 from Virginia’s library myData &lt;- read.csv(&#39;http://static.lib.virginia.edu/statlab/materials/data/mediationData.csv&#39;) # Step 1 (no longer necessary) model.0 &lt;- lm(Y ~ X, myData) summary(model.0) #&gt; #&gt; Call: #&gt; lm(formula = Y ~ X, data = myData) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -5.0262 -1.2340 -0.3282 1.5583 5.1622 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.8572 0.6932 4.122 7.88e-05 *** #&gt; X 0.3961 0.1112 3.564 0.000567 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.929 on 98 degrees of freedom #&gt; Multiple R-squared: 0.1147, Adjusted R-squared: 0.1057 #&gt; F-statistic: 12.7 on 1 and 98 DF, p-value: 0.0005671 # Step 2 model.M &lt;- lm(M ~ X, myData) summary(model.M) #&gt; #&gt; Call: #&gt; lm(formula = M ~ X, data = myData) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.3046 -0.8656 0.1344 1.1344 4.6954 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.49952 0.58920 2.545 0.0125 * #&gt; X 0.56102 0.09448 5.938 4.39e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.639 on 98 degrees of freedom #&gt; Multiple R-squared: 0.2646, Adjusted R-squared: 0.2571 #&gt; F-statistic: 35.26 on 1 and 98 DF, p-value: 4.391e-08 # Step 3 model.Y &lt;- lm(Y ~ X + M, myData) summary(model.Y) #&gt; #&gt; Call: #&gt; lm(formula = Y ~ X + M, data = myData) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.7631 -1.2393 0.0308 1.0832 4.0055 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.9043 0.6055 3.145 0.0022 ** #&gt; X 0.0396 0.1096 0.361 0.7187 #&gt; M 0.6355 0.1005 6.321 7.92e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.631 on 97 degrees of freedom #&gt; Multiple R-squared: 0.373, Adjusted R-squared: 0.3601 #&gt; F-statistic: 28.85 on 2 and 97 DF, p-value: 1.471e-10 # Step 4 (boostrapping) library(mediation) results &lt;- mediate( model.M, model.Y, treat = &#39;X&#39;, mediator = &#39;M&#39;, boot = TRUE, sims = 500 ) summary(results) #&gt; #&gt; Causal Mediation Analysis #&gt; #&gt; Nonparametric Bootstrap Confidence Intervals with the Percentile Method #&gt; #&gt; Estimate 95% CI Lower 95% CI Upper p-value #&gt; ACME 0.3565 0.2119 0.51 &lt;2e-16 *** #&gt; ADE 0.0396 -0.1750 0.28 0.760 #&gt; Total Effect 0.3961 0.1743 0.64 0.004 ** #&gt; Prop. Mediated 0.9000 0.5042 1.94 0.004 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Sample Size Used: 100 #&gt; #&gt; #&gt; Simulations: 500 Total Effect = 0.3961 = \\(b_1\\) (step 1) = total effect of \\(X\\) on \\(Y\\) without \\(M\\) Direct Effect = ADE = 0.0396 = \\(b_4\\) (step 3) = direct effect of \\(X\\) on \\(Y\\) accounting for the indirect effect of \\(M\\) ACME = Average Causal Mediation Effects = \\(b_1 - b_4\\) = 0.3961 - 0.0396 = 0.3565 = \\(b_2 \\times b_3\\) = 0.56102 * 0.6355 = 0.3565 Using mediation package suggested by Imai, Keele, and Yamamoto (2010). More details of the package can be found here 2 types of Inference in this package: Model-based inference: Assumptions: Treatment is randomized (could use matching methods to achieve this). Sequential Ignorability: conditional on covariates, there is other confounders that affect the relationship between (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard to argue in observational data. This assumption is for the identification of ACME (i.e., average causal mediation effects). Design-based inference Notations: we stay consistent with package instruction \\(M_i(t)\\) = mediator \\(T_i\\) = treatment status \\((0,1)\\) \\(Y_i(t,m)\\) = outcome where \\(t\\) = treatment, and \\(m\\) = mediating variables. \\(X_i\\) = vector of observed pre-treatment confounders Treatment effect (per unit \\(i\\)) = \\(\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\\) which has 2 effects Causal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\) Direct effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\) summing up to the treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\) More on sequential ignorability \\[ \\{ Y_i (t&#39;, m) , M_i (t) \\} \\perp T_i |X_i = x \\] \\[ Y_i(t&#39;,m) \\perp M_i(t) | T_i = t, X_i = x \\] where \\(0 &lt; P(T_i = t | X_i = x)\\) \\(0 &lt; P(M_i = m | T_i = t , X_i =x)\\) First condition is the standard strong ignorability condition where treatment assignment is random conditional on pre-treatment confounders. Second condition is stronger where the mediators is also random given the observed treatment and pre-treatment confounders. This condition is satisfied only when there is no unobserved pre-treatment confounders, and post-treatment confounders, and multiple mediators that are correlated. My understanding is that until the moment I write this note, there is no way to test the sequential ignorability assumption. Hence, researchers can only do sensitivity analysis to argue for their result. References "],["model-based-causal-mediation-analysis.html", "36.3 Model-based causal mediation analysis", " 36.3 Model-based causal mediation analysis Other resources: here Fit 2 models mediator model: conditional distribution of the mediators \\(M_i | T_i, X_i\\) Outcome model: conditional distribution of \\(Y_i | T_i, M_i, X_i\\) mediation can accommodate almost all types of model for both mediator model and outcome model except Censored mediator model. The update here is that estimation of ACME does not rely on product or difference of coefficients (see 36.2.1 , which requires very strict assumption: (1) linear regression models of mediator and outcome, (2) \\(T_i\\) and \\(M_i\\) effects are additive and no interaction library(mediation) set.seed(2014) data(&quot;framing&quot;, package = &quot;mediation&quot;) med.fit &lt;- lm(emo ~ treat + age + educ + gender + income, data = framing) out.fit &lt;- glm( cong_mesg ~ emo + treat + age + educ + gender + income, data = framing, family = binomial(&quot;probit&quot;) ) # Quasi-Bayesian Monte Carlo med.out &lt;- mediate( med.fit, out.fit, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, robustSE = TRUE, sims = 100 # should be 10000 in practice ) summary(med.out) #&gt; #&gt; Causal Mediation Analysis #&gt; #&gt; Quasi-Bayesian Confidence Intervals #&gt; #&gt; Estimate 95% CI Lower 95% CI Upper p-value #&gt; ACME (control) 0.0791 0.0351 0.15 &lt;2e-16 *** #&gt; ACME (treated) 0.0804 0.0367 0.16 &lt;2e-16 *** #&gt; ADE (control) 0.0206 -0.0976 0.12 0.70 #&gt; ADE (treated) 0.0218 -0.1053 0.12 0.70 #&gt; Total Effect 0.1009 -0.0497 0.23 0.14 #&gt; Prop. Mediated (control) 0.6946 -6.3109 3.68 0.14 #&gt; Prop. Mediated (treated) 0.7118 -5.7936 3.50 0.14 #&gt; ACME (average) 0.0798 0.0359 0.15 &lt;2e-16 *** #&gt; ADE (average) 0.0212 -0.1014 0.12 0.70 #&gt; Prop. Mediated (average) 0.7032 -6.0523 3.59 0.14 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Sample Size Used: 265 #&gt; #&gt; #&gt; Simulations: 100 Nonparametric bootstrap version med.out &lt;- mediate( med.fit, out.fit, boot = TRUE, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, sims = 100, # should be 10000 in practice boot.ci.type = &quot;bca&quot; # bias-corrected and accelerated intervals ) summary(med.out) #&gt; #&gt; Causal Mediation Analysis #&gt; #&gt; Nonparametric Bootstrap Confidence Intervals with the BCa Method #&gt; #&gt; Estimate 95% CI Lower 95% CI Upper p-value #&gt; ACME (control) 0.0848 0.0424 0.14 &lt;2e-16 *** #&gt; ACME (treated) 0.0858 0.0410 0.14 &lt;2e-16 *** #&gt; ADE (control) 0.0117 -0.0726 0.13 0.58 #&gt; ADE (treated) 0.0127 -0.0784 0.14 0.58 #&gt; Total Effect 0.0975 0.0122 0.25 0.06 . #&gt; Prop. Mediated (control) 0.8698 1.7460 151.20 0.06 . #&gt; Prop. Mediated (treated) 0.8804 1.6879 138.91 0.06 . #&gt; ACME (average) 0.0853 0.0434 0.14 &lt;2e-16 *** #&gt; ADE (average) 0.0122 -0.0756 0.14 0.58 #&gt; Prop. Mediated (average) 0.8751 1.7170 145.05 0.06 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Sample Size Used: 265 #&gt; #&gt; #&gt; Simulations: 100 If theoretically understanding suggests that there is treatment and mediator interaction med.fit &lt;- lm(emo ~ treat + age + educ + gender + income, data = framing) out.fit &lt;- glm( cong_mesg ~ emo * treat + age + educ + gender + income, data = framing, family = binomial(&quot;probit&quot;) ) med.out &lt;- mediate( med.fit, out.fit, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, robustSE = TRUE, sims = 100 ) summary(med.out) #&gt; #&gt; Causal Mediation Analysis #&gt; #&gt; Quasi-Bayesian Confidence Intervals #&gt; #&gt; Estimate 95% CI Lower 95% CI Upper p-value #&gt; ACME (control) 0.07417 0.02401 0.14 &lt;2e-16 *** #&gt; ACME (treated) 0.09496 0.02702 0.16 &lt;2e-16 *** #&gt; ADE (control) -0.01353 -0.11855 0.11 0.76 #&gt; ADE (treated) 0.00726 -0.11007 0.11 0.90 #&gt; Total Effect 0.08143 -0.05646 0.19 0.26 #&gt; Prop. Mediated (control) 0.64510 -14.31243 3.13 0.26 #&gt; Prop. Mediated (treated) 0.98006 -17.83202 4.01 0.26 #&gt; ACME (average) 0.08457 0.02738 0.15 &lt;2e-16 *** #&gt; ADE (average) -0.00314 -0.11457 0.12 1.00 #&gt; Prop. Mediated (average) 0.81258 -16.07223 3.55 0.26 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Sample Size Used: 265 #&gt; #&gt; #&gt; Simulations: 100 test.TMint(med.out, conf.level = .95) # test treatment-mediator interaction effect #&gt; #&gt; Test of ACME(1) - ACME(0) = 0 #&gt; #&gt; data: estimates from med.out #&gt; ACME(1) - ACME(0) = 0.020796, p-value = 0.3 #&gt; alternative hypothesis: true ACME(1) - ACME(0) is not equal to 0 #&gt; 95 percent confidence interval: #&gt; -0.01757310 0.07110837 plot(med.out) mediation can be used in conjunction with any of your imputation packages. And it can also handle mediated moderation or non-binary treatment variables, or multi-level data Sensitivity Analysis for sequential ignorability test for unobserved pre-treatment covariates \\(\\rho\\) = correlation between the residuals of the mediator and outcome regressions. If \\(\\rho\\) is significant, we have evidence for violation of sequential ignorability (i.e., there is unobserved pre-treatment confounders). med.fit &lt;- lm(emo ~ treat + age + educ + gender + income, data = framing) out.fit &lt;- glm( cong_mesg ~ emo + treat + age + educ + gender + income, data = framing, family = binomial(&quot;probit&quot;) ) med.out &lt;- mediate( med.fit, out.fit, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, robustSE = TRUE, sims = 100 ) sens.out &lt;- medsens(med.out, rho.by = 0.1, # \\rho varies from -0.9 to 0.9 by 0.1 effect.type = &quot;indirect&quot;, # sensitivity on ACME # effect.type = &quot;direct&quot;, # sensitivity on ADE # effect.type = &quot;both&quot;, # sensitivity on ACME and ADE sims = 100) summary(sens.out) #&gt; #&gt; Mediation Sensitivity Analysis: Average Mediation Effect #&gt; #&gt; Sensitivity Region: ACME for Control Group #&gt; #&gt; Rho ACME(control) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~ #&gt; [1,] 0.3 0.0062 -0.0073 0.0188 0.09 0.0493 #&gt; [2,] 0.4 -0.0084 -0.0238 0.0017 0.16 0.0877 #&gt; #&gt; Rho at which ACME for Control Group = 0: 0.3 #&gt; R^2_M*R^2_Y* at which ACME for Control Group = 0: 0.09 #&gt; R^2_M~R^2_Y~ at which ACME for Control Group = 0: 0.0493 #&gt; #&gt; #&gt; Sensitivity Region: ACME for Treatment Group #&gt; #&gt; Rho ACME(treated) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~ #&gt; [1,] 0.3 0.0071 -0.0092 0.0213 0.09 0.0493 #&gt; [2,] 0.4 -0.0101 -0.0295 0.0023 0.16 0.0877 #&gt; #&gt; Rho at which ACME for Treatment Group = 0: 0.3 #&gt; R^2_M*R^2_Y* at which ACME for Treatment Group = 0: 0.09 #&gt; R^2_M~R^2_Y~ at which ACME for Treatment Group = 0: 0.0493 plot(sens.out, sens.par = &quot;rho&quot;, main = &quot;Anxiety&quot;, ylim = c(-0.2, 0.2)) ACME confidence intervals contains 0 when \\(\\rho \\in (0.3,0.4)\\) Alternatively, using \\(R^2\\) interpretation, we need to specify the direction of confounder that affects the mediator and outcome variables in plot using sign.prod = \"positive\" (i.e., same direction) or sign.prod = \"negative\" (i.e., opposite direction). plot(sens.out, sens.par = &quot;R2&quot;, r.type = &quot;total&quot;, sign.prod = &quot;positive&quot;) "],["directed-acyclic-graph.html", "Chapter 37 Directed Acyclic Graph", " Chapter 37 Directed Acyclic Graph Native R: dagitty ggdag dagR r-causal: by Center for Causal Discovery. Also available in Python Publication-ready (with R and Latex): shinyDAG Standalone program: DAG program by Sven Knuppel "],["basic-notations.html", "37.1 Basic Notations", " 37.1 Basic Notations Basic building blocks of DAG Mediators (chains): \\(X \\to Z \\to Y\\) controlling for Z blocks (closes) the causal impact of \\(X \\to Y\\) Common causes (forks): \\(X \\leftarrow Z \\to Y\\) Z (i.e., confounder) is a common cause in which it induces a non-causal association between \\(X\\) and \\(Y\\). Controlling for \\(Z\\) should close this association. \\(Z\\) d-separates \\(X\\) from \\(Y\\) when it blocks (closes) all paths from \\(X\\) to \\(Y\\) (i.e., \\(X \\perp Y |Z\\)). This applies to both common causes and mediators. Common effects (colliders): \\(X \\to Z \\leftarrow Y\\) Not controlling for \\(Z\\) does not induce an association between \\(X\\) and \\(Y\\) Controlling for \\(Z\\) induces a non-causal association between \\(X\\) and \\(Y\\) Notes: A descendant of a variable behavior similarly to that variable (e.g., a descendant of \\(Z\\) can behave like \\(Z\\) and partially control for \\(Z\\)) Rule of thumb for multiple Controls: o have Causal inference \\(X \\to Y\\), we must Close all backdoor path between \\(X\\) and \\(Y\\) (to eliminate spurious correlation) Do not close any causal path between \\(X\\) and \\(Y\\) (any mediators). "],["report.html", "Chapter 38 Report", " Chapter 38 Report Structure Exploratory analysis plots preliminary results interesting structure/features in the data outliers Model Assumptions Why this model/ How is this model the best one? Consideration: interactions, collinearity, dependence Model Fit How well does it fit? Are the model assumptions met? Residual analysis Inference/ Prediction Are there different way to support your inference? Conclusion Recommendation Limitation of the analysis How to correct those in the future This chapter is based on the jtools package. More information can be found here. "],["one-summary-table.html", "38.1 One summary table", " 38.1 One summary table Packages for reporting: Summary Statistics Table: qwraps2 vtable gtsummary apaTables stargazer Regression Table gtsummary sjPlot,sjmisc, sjlabelled stargazer: recommended (Example) modelsummary library(jtools) data(movies) fit &lt;- lm(metascore ~ budget + us_gross + year, data = movies) summ(fit) Observations 831 (10 missing obs. deleted) Dependent variable metascore Type OLS linear regression F(3,827) 26.23 R² 0.09 Adj. R² 0.08 Est. S.E. t val. p (Intercept) 52.06 139.67 0.37 0.71 budget -0.00 0.00 -5.89 0.00 us_gross 0.00 0.00 7.61 0.00 year 0.01 0.07 0.08 0.94 Standard errors: OLS summ( fit, scale = TRUE, vifs = TRUE, part.corr = TRUE, confint = TRUE, pvals = FALSE ) # notice that scale here is TRUE Observations 831 (10 missing obs. deleted) Dependent variable metascore Type OLS linear regression F(3,827) 26.23 R² 0.09 Adj. R² 0.08 Est. 2.5% 97.5% t val. VIF partial.r part.r (Intercept) 63.01 61.91 64.11 112.23 NA NA NA budget -3.78 -5.05 -2.52 -5.89 1.31 -0.20 -0.20 us_gross 5.28 3.92 6.64 7.61 1.52 0.26 0.25 year 0.05 -1.18 1.28 0.08 1.24 0.00 0.00 Standard errors: OLS; Continuous predictors are mean-centered and scaled by 1 s.d. The outcome variable remains in its original units. #obtain clsuter-robust SE data(&quot;PetersenCL&quot;, package = &quot;sandwich&quot;) fit2 &lt;- lm(y ~ x, data = PetersenCL) summ(fit2, robust = &quot;HC3&quot;, cluster = &quot;firm&quot;) Observations 5000 Dependent variable y Type OLS linear regression F(1,4998) 1310.74 R² 0.21 Adj. R² 0.21 Est. S.E. t val. p (Intercept) 0.03 0.07 0.44 0.66 x 1.03 0.05 20.36 0.00 Standard errors: Cluster-robust, type = HC3 Model to Equation # install.packages(&quot;equatiomatic&quot;) # not available for R 4.2 fit &lt;- lm(metascore ~ budget + us_gross + year, data = movies) # show the theoretical model equatiomatic::extract_eq(fit) # display the actual coefficients equatiomatic::extract_eq(fit, use_coefs = TRUE) "],["model-comparison.html", "38.2 Model Comparison", " 38.2 Model Comparison fit &lt;- lm(metascore ~ log(budget), data = movies) fit_b &lt;- lm(metascore ~ log(budget) + log(us_gross), data = movies) fit_c &lt;- lm(metascore ~ log(budget) + log(us_gross) + runtime, data = movies) coef_names &lt;- c(&quot;Budget&quot; = &quot;log(budget)&quot;, &quot;US Gross&quot; = &quot;log(us_gross)&quot;, &quot;Runtime (Hours)&quot; = &quot;runtime&quot;, &quot;Constant&quot; = &quot;(Intercept)&quot;) export_summs(fit, fit_b, fit_c, robust = &quot;HC3&quot;, coefs = coef_names) Table 35.1: Model 1Model 2Model 3 Budget-2.43 ***-5.16 ***-6.70 *** (0.44)&nbsp;&nbsp;&nbsp;(0.62)&nbsp;&nbsp;&nbsp;(0.67)&nbsp;&nbsp;&nbsp; US Gross&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.96 ***3.85 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.51)&nbsp;&nbsp;&nbsp;(0.48)&nbsp;&nbsp;&nbsp; Runtime (Hours)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14.29 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1.63)&nbsp;&nbsp;&nbsp; Constant105.29 ***81.84 ***83.35 *** (7.65)&nbsp;&nbsp;&nbsp;(8.66)&nbsp;&nbsp;&nbsp;(8.82)&nbsp;&nbsp;&nbsp; N831&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;831&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;831&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.03&nbsp;&nbsp;&nbsp;&nbsp;0.09&nbsp;&nbsp;&nbsp;&nbsp;0.17&nbsp;&nbsp;&nbsp;&nbsp; Standard errors are heteroskedasticity robust. *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Another package is modelsummary library(modelsummary) lm_mod &lt;- lm(mpg ~ wt + hp + cyl, mtcars) msummary(lm_mod, vcov = c(&quot;iid&quot;,&quot;robust&quot;,&quot;HC4&quot;))  (1)   (2)   (3) (Intercept) 38.752 38.752 38.752 (1.787) (2.286) (2.177) wt −3.167 −3.167 −3.167 (0.741) (0.833) (0.819) hp −0.018 −0.018 −0.018 (0.012) (0.010) (0.013) cyl −0.942 −0.942 −0.942 (0.551) (0.573) (0.572) Num.Obs. 32 32 32 R2 0.843 0.843 0.843 R2 Adj. 0.826 0.826 0.826 AIC 155.5 155.5 155.5 BIC 162.8 162.8 162.8 Log.Lik. −72.738 −72.738 −72.738 F 50.171 31.065 32.623 RMSE 2.35 2.35 2.35 Std.Errors IID HC3 HC4 modelplot(lm_mod, vcov = c(&quot;iid&quot;,&quot;robust&quot;,&quot;HC4&quot;)) Another package is stargazer library(&quot;stargazer&quot;) stargazer(attitude) #&gt; #&gt; % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com #&gt; % Date and time: Thu, Aug 29, 2024 - 4:10:22 PM #&gt; \\begin{table}[!htbp] \\centering #&gt; \\caption{} #&gt; \\label{} #&gt; \\begin{tabular}{@{\\extracolsep{5pt}}lccccc} #&gt; \\\\[-1.8ex]\\hline #&gt; \\hline \\\\[-1.8ex] #&gt; Statistic &amp; \\multicolumn{1}{c}{N} &amp; \\multicolumn{1}{c}{Mean} &amp; \\multicolumn{1}{c}{St. Dev.} &amp; \\multicolumn{1}{c}{Min} &amp; \\multicolumn{1}{c}{Max} \\\\ #&gt; \\hline \\\\[-1.8ex] #&gt; rating &amp; 30 &amp; 64.633 &amp; 12.173 &amp; 40 &amp; 85 \\\\ #&gt; complaints &amp; 30 &amp; 66.600 &amp; 13.315 &amp; 37 &amp; 90 \\\\ #&gt; privileges &amp; 30 &amp; 53.133 &amp; 12.235 &amp; 30 &amp; 83 \\\\ #&gt; learning &amp; 30 &amp; 56.367 &amp; 11.737 &amp; 34 &amp; 75 \\\\ #&gt; raises &amp; 30 &amp; 64.633 &amp; 10.397 &amp; 43 &amp; 88 \\\\ #&gt; critical &amp; 30 &amp; 74.767 &amp; 9.895 &amp; 49 &amp; 92 \\\\ #&gt; advance &amp; 30 &amp; 42.933 &amp; 10.289 &amp; 25 &amp; 72 \\\\ #&gt; \\hline \\\\[-1.8ex] #&gt; \\end{tabular} #&gt; \\end{table} ## 2 OLS models linear.1 &lt;- lm(rating ~ complaints + privileges + learning + raises + critical, data = attitude) linear.2 &lt;- lm(rating ~ complaints + privileges + learning, data = attitude) ## create an indicator dependent variable, and run a probit model attitude$high.rating &lt;- (attitude$rating &gt; 70) probit.model &lt;- glm( high.rating ~ learning + critical + advance, data = attitude, family = binomial(link = &quot;probit&quot;) ) stargazer(linear.1, linear.2, probit.model, title = &quot;Results&quot;, align = TRUE) #&gt; #&gt; % Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com #&gt; % Date and time: Thu, Aug 29, 2024 - 4:10:22 PM #&gt; % Requires LaTeX packages: dcolumn #&gt; \\begin{table}[!htbp] \\centering #&gt; \\caption{Results} #&gt; \\label{} #&gt; \\begin{tabular}{@{\\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } #&gt; \\\\[-1.8ex]\\hline #&gt; \\hline \\\\[-1.8ex] #&gt; &amp; \\multicolumn{3}{c}{\\textit{Dependent variable:}} \\\\ #&gt; \\cline{2-4} #&gt; \\\\[-1.8ex] &amp; \\multicolumn{2}{c}{rating} &amp; \\multicolumn{1}{c}{high.rating} \\\\ #&gt; \\\\[-1.8ex] &amp; \\multicolumn{2}{c}{\\textit{OLS}} &amp; \\multicolumn{1}{c}{\\textit{probit}} \\\\ #&gt; \\\\[-1.8ex] &amp; \\multicolumn{1}{c}{(1)} &amp; \\multicolumn{1}{c}{(2)} &amp; \\multicolumn{1}{c}{(3)}\\\\ #&gt; \\hline \\\\[-1.8ex] #&gt; complaints &amp; 0.692^{***} &amp; 0.682^{***} &amp; \\\\ #&gt; &amp; (0.149) &amp; (0.129) &amp; \\\\ #&gt; &amp; &amp; &amp; \\\\ #&gt; privileges &amp; -0.104 &amp; -0.103 &amp; \\\\ #&gt; &amp; (0.135) &amp; (0.129) &amp; \\\\ #&gt; &amp; &amp; &amp; \\\\ #&gt; learning &amp; 0.249 &amp; 0.238^{*} &amp; 0.164^{***} \\\\ #&gt; &amp; (0.160) &amp; (0.139) &amp; (0.053) \\\\ #&gt; &amp; &amp; &amp; \\\\ #&gt; raises &amp; -0.033 &amp; &amp; \\\\ #&gt; &amp; (0.202) &amp; &amp; \\\\ #&gt; &amp; &amp; &amp; \\\\ #&gt; critical &amp; 0.015 &amp; &amp; -0.001 \\\\ #&gt; &amp; (0.147) &amp; &amp; (0.044) \\\\ #&gt; &amp; &amp; &amp; \\\\ #&gt; advance &amp; &amp; &amp; -0.062 \\\\ #&gt; &amp; &amp; &amp; (0.042) \\\\ #&gt; &amp; &amp; &amp; \\\\ #&gt; Constant &amp; 11.011 &amp; 11.258 &amp; -7.476^{**} \\\\ #&gt; &amp; (11.704) &amp; (7.318) &amp; (3.570) \\\\ #&gt; &amp; &amp; &amp; \\\\ #&gt; \\hline \\\\[-1.8ex] #&gt; Observations &amp; \\multicolumn{1}{c}{30} &amp; \\multicolumn{1}{c}{30} &amp; \\multicolumn{1}{c}{30} \\\\ #&gt; R$^{2}$ &amp; \\multicolumn{1}{c}{0.715} &amp; \\multicolumn{1}{c}{0.715} &amp; \\\\ #&gt; Adjusted R$^{2}$ &amp; \\multicolumn{1}{c}{0.656} &amp; \\multicolumn{1}{c}{0.682} &amp; \\\\ #&gt; Log Likelihood &amp; &amp; &amp; \\multicolumn{1}{c}{-9.087} \\\\ #&gt; Akaike Inf. Crit. &amp; &amp; &amp; \\multicolumn{1}{c}{26.175} \\\\ #&gt; Residual Std. Error &amp; \\multicolumn{1}{c}{7.139 (df = 24)} &amp; \\multicolumn{1}{c}{6.863 (df = 26)} &amp; \\\\ #&gt; F Statistic &amp; \\multicolumn{1}{c}{12.063$^{***}$ (df = 5; 24)} &amp; \\multicolumn{1}{c}{21.743$^{***}$ (df = 3; 26)} &amp; \\\\ #&gt; \\hline #&gt; \\hline \\\\[-1.8ex] #&gt; \\textit{Note:} &amp; \\multicolumn{3}{r}{$^{*}$p$&lt;$0.1; $^{**}$p$&lt;$0.05; $^{***}$p$&lt;$0.01} \\\\ #&gt; \\end{tabular} #&gt; \\end{table} # Latex stargazer( linear.1, linear.2, probit.model, title = &quot;Regression Results&quot;, align = TRUE, dep.var.labels = c(&quot;Overall Rating&quot;, &quot;High Rating&quot;), covariate.labels = c( &quot;Handling of Complaints&quot;, &quot;No Special Privileges&quot;, &quot;Opportunity to Learn&quot;, &quot;Performance-Based Raises&quot;, &quot;Too Critical&quot;, &quot;Advancement&quot; ), omit.stat = c(&quot;LL&quot;, &quot;ser&quot;, &quot;f&quot;), no.space = TRUE ) # ASCII text output stargazer( linear.1, linear.2, type = &quot;text&quot;, title = &quot;Regression Results&quot;, dep.var.labels = c(&quot;Overall Rating&quot;, &quot;High Rating&quot;), covariate.labels = c( &quot;Handling of Complaints&quot;, &quot;No Special Privileges&quot;, &quot;Opportunity to Learn&quot;, &quot;Performance-Based Raises&quot;, &quot;Too Critical&quot;, &quot;Advancement&quot; ), omit.stat = c(&quot;LL&quot;, &quot;ser&quot;, &quot;f&quot;), ci = TRUE, ci.level = 0.90, single.row = TRUE ) #&gt; #&gt; Regression Results #&gt; ======================================================================== #&gt; Dependent variable: #&gt; ----------------------------------------------- #&gt; Overall Rating #&gt; (1) (2) #&gt; ------------------------------------------------------------------------ #&gt; Handling of Complaints 0.692*** (0.447, 0.937) 0.682*** (0.470, 0.894) #&gt; No Special Privileges -0.104 (-0.325, 0.118) -0.103 (-0.316, 0.109) #&gt; Opportunity to Learn 0.249 (-0.013, 0.512) 0.238* (0.009, 0.467) #&gt; Performance-Based Raises -0.033 (-0.366, 0.299) #&gt; Too Critical 0.015 (-0.227, 0.258) #&gt; Advancement 11.011 (-8.240, 30.262) 11.258 (-0.779, 23.296) #&gt; ------------------------------------------------------------------------ #&gt; Observations 30 30 #&gt; R2 0.715 0.715 #&gt; Adjusted R2 0.656 0.682 #&gt; ======================================================================== #&gt; Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 stargazer( linear.1, linear.2, probit.model, title = &quot;Regression Results&quot;, align = TRUE, dep.var.labels = c(&quot;Overall Rating&quot;, &quot;High Rating&quot;), covariate.labels = c( &quot;Handling of Complaints&quot;, &quot;No Special Privileges&quot;, &quot;Opportunity to Learn&quot;, &quot;Performance-Based Raises&quot;, &quot;Too Critical&quot;, &quot;Advancement&quot; ), omit.stat = c(&quot;LL&quot;, &quot;ser&quot;, &quot;f&quot;), no.space = TRUE ) Correlation Table correlation.matrix &lt;- cor(attitude[, c(&quot;rating&quot;, &quot;complaints&quot;, &quot;privileges&quot;)]) stargazer(correlation.matrix, title = &quot;Correlation Matrix&quot;) "],["changes-in-an-estimate.html", "38.3 Changes in an estimate", " 38.3 Changes in an estimate coef_names &lt;- coef_names[1:3] # Dropping intercept for plots plot_summs(fit, fit_b, fit_c, robust = &quot;HC3&quot;, coefs = coef_names) plot_summs( fit_c, robust = &quot;HC3&quot;, coefs = coef_names, plot.distributions = TRUE ) "],["standard-errors-3.html", "38.4 Standard Errors", " 38.4 Standard Errors sandwich vignette Type Applicable Usage Reference const Assume constant variances HC HC0 vcovCL Heterogeneity White’s estimator All other heterogeneity SE methods are derivatives of this. No small sample bias adjustment (White 1980) HC1 vcovCL Uses a degrees of freedom-based correction When the number of clusters is small, HC2 and HC3 are better (Cameron, Gelbach, and Miller 2008) (J. G. MacKinnon and White 1985) HC2 vcovCL Better with the linear model, but still applicable for Generalized Linear Models Needs a hat (weighted) matrix HC3 vcovCL Better with the linear model, but still applicable for Generalized Linear Models Needs a hat (weighted) matrix HC4 vcovHC (Cribari-Neto 2004) HC4m vcovHC (Cribari-Neto, Souza, and Vasconcellos 2007) HC5 vcovHC (Cribari-Neto and Silva 2011) data(cars) model &lt;- lm(speed ~ dist, data = cars) summary(model) #&gt; #&gt; Call: #&gt; lm(formula = speed ~ dist, data = cars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.5293 -2.1550 0.3615 2.4377 6.4179 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.28391 0.87438 9.474 1.44e-12 *** #&gt; dist 0.16557 0.01749 9.464 1.49e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3.156 on 48 degrees of freedom #&gt; Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 #&gt; F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 lmtest::coeftest(model, vcov. = sandwich::vcovHC(model, type = &quot;HC1&quot;)) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.283906 0.891860 9.2883 2.682e-12 *** #&gt; dist 0.165568 0.019402 8.5335 3.482e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 References "],["coefficient-uncertainty-and-distribution.html", "38.5 Coefficient Uncertainty and Distribution", " 38.5 Coefficient Uncertainty and Distribution The ggdist allows us to visualize uncertainty under both frequentist and Bayesian frameworks library(ggdist) "],["descriptive-tables.html", "38.6 Descriptive Tables", " 38.6 Descriptive Tables Export APA theme data(&quot;mtcars&quot;) library(flextable) theme_apa(flextable(mtcars[1:5,1:5])) Export to Latex print(xtable::xtable(mtcars, type = &quot;latex&quot;), file = file.path(getwd(), &quot;output&quot;, &quot;mtcars_xtable.tex&quot;)) # American Economic Review style stargazer::stargazer( mtcars, title = &quot;Testing&quot;, style = &quot;aer&quot;, out = file.path(getwd(), &quot;output&quot;, &quot;mtcars_stargazer.tex&quot;) ) # other styles include # Administrative Science Quarterly # Quarterly Journal of Economics However, the above codes do not play well with notes. Hence, I create my own custom code that follows the AMA guidelines ama_tbl &lt;- function(data, caption, label, note, output_path) { library(tidyverse) library(xtable) # Function to determine column alignment get_column_alignment &lt;- function(data) { # Start with the alignment for the header row alignment &lt;- c(&quot;l&quot;, &quot;l&quot;) # Check each column for (col in seq_len(ncol(data))[-1]) { if (is.numeric(data[[col]])) { alignment &lt;- c(alignment, &quot;r&quot;) # Right alignment for numbers } else { alignment &lt;- c(alignment, &quot;c&quot;) # Center alignment for other data } } return(alignment) } data %&gt;% # bold + left align first column rename_with(~paste(&quot;\\\\multicolumn{1}{l}{\\\\textbf{&quot;, ., &quot;}}&quot;), 1) %&gt;% # bold + center align all other columns `colnames&lt;-`(ifelse(colnames(.) != colnames(.)[1], paste(&quot;\\\\multicolumn{1}{c}{\\\\textbf{&quot;, colnames(.), &quot;}}&quot;), colnames(.))) %&gt;% xtable(caption = caption, label = label, align = get_column_alignment(data), auto = TRUE) %&gt;% print( include.rownames = FALSE, caption.placement = &quot;top&quot;, hline.after=c(-1, 0), # p{0.9\\linewidth} sets the width of the column to 90% of the line width, and the @{} removes any extra padding around the cell. add.to.row = list(pos = list(nrow(data)), # Add at the bottom of the table command = c(paste0(&quot;\\\\hline \\n \\\\multicolumn{&quot;,ncol(data), &quot;}{l} {&quot;, &quot;\\n \\\\begin{tabular}{@{}p{0.9\\\\linewidth}@{}} \\n&quot;,&quot;Note: &quot;, note, &quot;\\n \\\\end{tabular} } \\n&quot;))), # Add your note here # make sure your heading is untouched (because you manually change it above) sanitize.colnames.function = identity, # place a the top of the page table.placement = &quot;h&quot;, file = output_path ) } ama_tbl( mtcars, caption = &quot;This is caption&quot;, label = &quot;tab:this_is_label&quot;, note = &quot;this is note&quot;, output_path = file.path(getwd(), &quot;output&quot;, &quot;mtcars_custom_ama.tex&quot;) ) "],["visualizations-and-plots.html", "38.7 Visualizations and Plots", " 38.7 Visualizations and Plots You can customize your plots based on your preferred journals. Here, I am creating a custom setting for the American Marketing Association. American-Marketing-Association-ready theme for plots library(ggplot2) # check available fonts # windowsFonts() # for Times New Roman # names(windowsFonts()[windowsFonts()==&quot;TT Times New Roman&quot;]) # Making a theme amatheme = theme_bw(base_size = 14, base_family = &quot;serif&quot;) + # This is Time New Roman theme( # remove major gridlines panel.grid.major = element_blank(), # remove minor gridlines panel.grid.minor = element_blank(), # remove panel border panel.border = element_blank(), line = element_line(), # change font text = element_text(), # if you want to remove legend title # legend.title = element_blank(), legend.title = element_text(size = rel(0.6), face = &quot;bold&quot;), # change font size of legend legend.text = element_text(size = rel(0.6)), legend.background = element_rect(color = &quot;black&quot;), # legend.margin = margin(t = 5, l = 5, r = 5, b = 5), # legend.key = element_rect(color = NA, fill = NA), # change font size of main title plot.title = element_text( size = rel(1.2), face = &quot;bold&quot;, hjust = 0.5, margin = margin(b = 15) ), plot.margin = unit(c(1, 1, 1, 1), &quot;cm&quot;), # add black line along axes axis.line = element_line(colour = &quot;black&quot;, linewidth = .8), axis.ticks = element_line(), # axis title axis.title.x = element_text(size = rel(1.2), face = &quot;bold&quot;), axis.title.y = element_text(size = rel(1.2), face = &quot;bold&quot;), # axis text size axis.text.y = element_text(size = rel(1)), axis.text.x = element_text(size = rel(1)) ) Example library(tidyverse) library(ggsci) data(&quot;mtcars&quot;) yourplot &lt;- mtcars %&gt;% select(mpg, cyl, gear) %&gt;% ggplot(., aes(x = mpg, y = cyl, fill = gear)) + geom_point() + labs(title=&quot;Some Plot&quot;) yourplot + amatheme + # choose different color theme scale_color_npg() yourplot + amatheme + scale_color_continuous() Other pre-specified themes library(ggthemes) # Stata theme yourplot + theme_stata() # The economist theme yourplot + theme_economist() yourplot + theme_economist_white() # Wall street journal theme yourplot + theme_wsj() # APA theme yourplot + jtools::theme_apa( legend.font.size = 24, x.font.size = 20, y.font.size = 20 ) "],["exploratory-data-analysis.html", "Chapter 39 Exploratory Data Analysis", " Chapter 39 Exploratory Data Analysis # load to get txhousing data library(ggplot2) Data Report Feature Engineering Missing Data # install.packages(&quot;DataExplorer&quot;) library(DataExplorer) # creat a html file that contain all reports create_report(txhousing) introduce() # see basic info dummify() # create binary columns from discrete variables split_columns() # split data into discrete and continuous parts plot_correlation() # heatmap for discrete var plot_intro() plot_missing() # plot missing value profile_missing() # profile missing values plot_prcomp() # plot PCA Error Identification # install.packages(&quot;dataReporter&quot;) library(dataReporter) makeDataReport() # detailed report like DataExplorer Summary statistics library(skimr) skim() # give only few quick summary stat, not as detailed as the other two packages Not so code-y process Quick and dirty way to look at your data # install.packages(&quot;rpivotTable&quot;) library(rpivotTable) # give set up just like Excel table data %&gt;% rpivotTable::rpivotTable() Code generation and wrangling Shiny-app based Tableu style # install.packages(&quot;esquisse&quot;) library(esquisse) esquisse::esquisser() Customized your daily/automatic report # install.packages(&quot;chronicle&quot;) library(chronicle) # install.packages(&quot;dlookr&quot;) # install.packages(&quot;descriptr&quot;) "],["sensitivity-analysis-robustness-check.html", "Chapter 40 Sensitivity Analysis/ Robustness Check ", " Chapter 40 Sensitivity Analysis/ Robustness Check "],["specification-curve.html", "40.1 Specification curve", " 40.1 Specification curve also known as Specification robustness graph or coefficient stability plot Resources In Stata or speccurve (Simonsohn, Simmons, and Nelson 2020) 40.1.1 starbility Recommend Installation devtools::install_github(&#39;https://github.com/AakaashRao/starbility&#39;) library(starbility) Example by the package’s author library(tidyverse) library(starbility) library(lfe) data(&quot;diamonds&quot;) set.seed(43) indices = sample(1:nrow(diamonds), replace = F, size = round(nrow(diamonds) / 20)) diamonds = diamonds[indices, ] Plot different combinations of controls # If you want to make the diamond dimensions as base control base_controls = c( &#39;Diamond dimensions&#39; = &#39;x + y + z&#39; # include all variables under 1 dimension ) perm_controls = c( &#39;Depth&#39; = &#39;depth&#39;, &#39;Table width&#39; = &#39;table&#39; ) nonperm_fe_controls = c( &#39;Clarity FE (granular)&#39; = &#39;clarity&#39;, &#39;Clarity FE (binary)&#39; = &#39;high_clarity&#39; ) # Adding fixed effects nonperm_fe_controls = c( &#39;Clarity FE (granular)&#39; = &#39;clarity&#39;, &#39;Clarity FE (binary)&#39; = &#39;high_clarity&#39; ) # Adding instrumental variables instruments = &#39;x+y+z&#39; # clustering and weights diamonds$sample_weights = runif(n = nrow(diamonds)) # robust standard errors starb_felm_custom = function(spec, data, rhs, ...) { spec = as.formula(spec) model = lfe::felm(spec, data=data) %&gt;% broom::tidy() row = which(model$term==rhs) coef = model[row, &#39;estimate&#39;] %&gt;% as.numeric() se = model[row, &#39;std.error&#39;] %&gt;% as.numeric() p = model[row, &#39;p.value&#39;] %&gt;% as.numeric() # 99% confidence interval z = qnorm(0.995) # one-tailed test return(c(coef, p/2, coef+z*se, coef-z*se)) } plots = stability_plot( data = diamonds, lhs = &#39;price&#39;, rhs = &#39;carat&#39;, error_geom = &#39;ribbon&#39;, # make the plot more aesthetics # error_geom = &#39;none&#39;, # if you don&#39;t want ribbon (i.e., error bar) model = starb_felm_custom, cluster = &#39;cut&#39;, weights = &#39;sample_weights&#39;, # iv = instruments, perm = perm_controls, base = base_controls, # perm_fe = perm_fe_controls, # if you want to include fixed effects sequentially (not all combinations) # (e.g., you want to test country or state fixed effect, not both ) # nonperm_fe = nonperm_fe_controls, # fe_always = F, # if you want to have a model without any Fixed Effects # sort &quot;asc&quot;, &quot;desc&quot;, or by fixed effects: &quot;asc-by-fe&quot; or &quot;desc-by-fe&quot; sort = &quot;asc-by-fe&quot;, # if you have less variables and want more aesthetics # control_geom = &#39;circle&#39;, # point_size = 2, # control_spacing = 0.3, # error_alpha = 0.2, # change alpha of the error geom # point_size = 1.5, # change the size of the coefficient points # control_text_size = 10, # change the size of the control labels # coef_ylim = c(-5000, 35000), # change the endpoints of the y-axis # trip_top = 3, # change the spacing between the two panels rel_height = 0.6 ) plots # add comments # replacement_coef_panel = plots[[1]] + # scale_y_reverse() + # theme(panel.grid.minor = element_blank()) + # geom_vline(xintercept = 41, # linetype = &#39;dashed&#39;, # alpha = 0.4) + # annotate( # geom = &#39;label&#39;, # x = 52, # y = 30000, # label = &#39;What a great\\nspecification!&#39;, # alpha = 0.75 # ) # # combine_plots(replacement_coef_panel, # plots[[2]], # rel_height = 0.6) Note: \\(p &lt; 0.01\\): red \\(p &lt; 0.05\\): green \\(p &lt; 0.1\\): blue \\(p &gt; 0.1\\): black More Advanced Stuff # Step 1: Control Grid diamonds$high_clarity = diamonds$clarity %in% c(&#39;VS1&#39;,&#39;VVS2&#39;,&#39;VVS1&#39;,&#39;IF&#39;) base_controls = c( &#39;Diamond dimensions&#39; = &#39;x + y + z&#39; ) perm_controls = c( &#39;Depth&#39; = &#39;depth&#39;, &#39;Table width&#39; = &#39;table&#39; ) perm_fe_controls = c( &#39;Cut FE&#39; = &#39;cut&#39;, &#39;Color FE&#39; = &#39;color&#39; ) nonperm_fe_controls = c( &#39;Clarity FE (granular)&#39; = &#39;clarity&#39;, &#39;Clarity FE (binary)&#39; = &#39;high_clarity&#39; ) grid1 = stability_plot(data = diamonds, lhs = &#39;price&#39;, rhs = &#39;carat&#39;, perm = perm_controls, base = base_controls, perm_fe = perm_fe_controls, nonperm_fe = nonperm_fe_controls, run_to=2) knitr::kable(grid1 %&gt;% head(10)) Diamond dimensions Depth Table width Cut FE Color FE np_fe 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 # Step 2: Get model expression grid2 = stability_plot(grid = grid1, data=diamonds, lhs=&#39;price&#39;, rhs=&#39;carat&#39;, perm=perm_controls, base=base_controls, run_from=2, run_to=3) knitr::kable(grid2 %&gt;% head(10)) Diamond dimensions Depth Table width np_fe expr 1 0 0 0 price~carat+x+y+z|0|0|0 1 1 0 0 price~carat+x+y+z+depth|0|0|0 1 0 1 0 price~carat+x+y+z+table|0|0|0 1 1 1 0 price~carat+x+y+z+depth+table|0|0|0 1 0 0 0 price~carat+x+y+z|0|0|0 1 1 0 0 price~carat+x+y+z+depth|0|0|0 1 0 1 0 price~carat+x+y+z+table|0|0|0 1 1 1 0 price~carat+x+y+z+depth+table|0|0|0 1 0 0 0 price~carat+x+y+z|0|0|0 1 1 0 0 price~carat+x+y+z+depth|0|0|0 # Step 3: Estimate models grid3 = stability_plot(grid = grid2, data=diamonds, lhs=&#39;price&#39;, rhs=&#39;carat&#39;, perm=perm_controls, base=base_controls, run_from=3, run_to=4) knitr::kable(grid3 %&gt;% head(10)) Diamond dimensions Depth Table width np_fe expr coef p error_high error_low 1 0 0 0 price~carat+x+y+z|0|0|0 10461.86 p&lt;0.01 11031.84 9891.876 1 1 0 0 price~carat+x+y+z+depth|0|0|0 10808.25 p&lt;0.01 11388.81 10227.683 1 0 1 0 price~carat+x+y+z+table|0|0|0 10423.42 p&lt;0.01 10992.00 9854.849 1 1 1 0 price~carat+x+y+z+depth+table|0|0|0 10851.31 p&lt;0.01 11428.58 10274.037 1 0 0 0 price~carat+x+y+z|0|0|0 10461.86 p&lt;0.01 11031.84 9891.876 1 1 0 0 price~carat+x+y+z+depth|0|0|0 10808.25 p&lt;0.01 11388.81 10227.683 1 0 1 0 price~carat+x+y+z+table|0|0|0 10423.42 p&lt;0.01 10992.00 9854.849 1 1 1 0 price~carat+x+y+z+depth+table|0|0|0 10851.31 p&lt;0.01 11428.58 10274.037 1 0 0 0 price~carat+x+y+z|0|0|0 10461.86 p&lt;0.01 11031.84 9891.876 1 1 0 0 price~carat+x+y+z+depth|0|0|0 10808.25 p&lt;0.01 11388.81 10227.683 # Step 4: Get dataframe to draw dfs = stability_plot(grid = grid3, data=diamonds, lhs=&#39;price&#39;, rhs=&#39;carat&#39;, perm=perm_controls, base=base_controls, run_from=4, run_to=5) coef_grid = dfs[[1]] control_grid = dfs[[2]] knitr::kable(coef_grid %&gt;% head(10)) Diamond dimensions Depth Table width np_fe expr coef p error_high error_low model 1 0 0 0 price~carat+x+y+z|0|0|0 10461.86 p&lt;0.01 11031.84 9891.876 1 1 1 0 0 price~carat+x+y+z+depth|0|0|0 10808.25 p&lt;0.01 11388.81 10227.683 2 1 0 1 0 price~carat+x+y+z+table|0|0|0 10423.42 p&lt;0.01 10992.00 9854.849 3 1 1 1 0 price~carat+x+y+z+depth+table|0|0|0 10851.31 p&lt;0.01 11428.58 10274.037 4 1 0 0 0 price~carat+x+y+z|0|0|0 10461.86 p&lt;0.01 11031.84 9891.876 5 1 1 0 0 price~carat+x+y+z+depth|0|0|0 10808.25 p&lt;0.01 11388.81 10227.683 6 1 0 1 0 price~carat+x+y+z+table|0|0|0 10423.42 p&lt;0.01 10992.00 9854.849 7 1 1 1 0 price~carat+x+y+z+depth+table|0|0|0 10851.31 p&lt;0.01 11428.58 10274.037 8 1 0 0 0 price~carat+x+y+z|0|0|0 10461.86 p&lt;0.01 11031.84 9891.876 9 1 1 0 0 price~carat+x+y+z+depth|0|0|0 10808.25 p&lt;0.01 11388.81 10227.683 10 # Step 5: plot the sensitivity graph panels = stability_plot(data = diamonds, lhs=&#39;price&#39;, rhs=&#39;carat&#39;, coef_grid = coef_grid, control_grid = control_grid, run_from=5, run_to=6) stability_plot(data = diamonds, lhs=&#39;price&#39;, rhs=&#39;carat&#39;, coef_panel = panels[[1]], control_panel = panels[[2]], run_from = 6, run_to = 7) In step 2, we can modify to use other function (e.g., glm) diamonds$above_med_price = as.numeric(diamonds$price &gt; median(diamonds$price)) base_controls = c(&#39;Diamond dimensions&#39; = &#39;x + y + z&#39;) perm_controls = c(&#39;Depth&#39; = &#39;depth&#39;, &#39;Table width&#39; = &#39;table&#39;, &#39;Clarity&#39; = &#39;clarity&#39;) lhs_var = &#39;above_med_price&#39; rhs_var = &#39;carat&#39; grid1 = stability_plot( data = diamonds, lhs = lhs_var, rhs = rhs_var, perm = perm_controls, base = base_controls, fe_always = F, run_to = 2 ) # Create control part of formula base_perm = c(base_controls, perm_controls) grid1$expr = apply(grid1[, 1:length(base_perm)], 1, function(x) paste(base_perm[names(base_perm)[which(x == 1)]], collapse = &#39;+&#39;)) # Complete formula with LHS and RHS variables grid1$expr = paste(lhs_var, &#39;~&#39;, rhs_var, &#39;+&#39;, grid1$expr, sep = &#39;&#39;) knitr::kable(grid1 %&gt;% head(10)) Diamond dimensions Depth Table width Clarity np_fe expr 1 0 0 0 above_med_price~carat+x + y + z 1 1 0 0 above_med_price~carat+x + y + z+depth 1 0 1 0 above_med_price~carat+x + y + z+table 1 1 1 0 above_med_price~carat+x + y + z+depth+table 1 0 0 1 above_med_price~carat+x + y + z+clarity 1 1 0 1 above_med_price~carat+x + y + z+depth+clarity 1 0 1 1 above_med_price~carat+x + y + z+table+clarity 1 1 1 1 above_med_price~carat+x + y + z+depth+table+clarity # customer function for the logit model starb_logit = function(spec, data, rhs, ...) { spec = as.formula(spec) model = glm(spec, data=data, family=&#39;binomial&#39;, weights=data$weight) %&gt;% broom::tidy() row = which(model$term==rhs) coef = model[row, &#39;estimate&#39;] %&gt;% as.numeric() se = model[row, &#39;std.error&#39;] %&gt;% as.numeric() p = model[row, &#39;p.value&#39;] %&gt;% as.numeric() return(c(coef, p, coef+1.96*se, coef-1.96*se)) } stability_plot(grid = grid1, data = diamonds, lhs = lhs_var, rhs = rhs_var, model = starb_logit, perm = perm_controls, base = base_controls, fe_always = F, run_from=3) For getting other specification (e.g., different CI) library(margins) starb_logit_enhanced = function(spec, data, rhs, ...) { # Unpack ... l = list(...) get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F spec = as.formula(spec) if (get_mfx) { model = glm(spec, data=data, family=&#39;binomial&#39;, weights=data$weight) %&gt;% margins() %&gt;% summary row = which(model$factor==rhs) coef = model[row, &#39;AME&#39;] %&gt;% as.numeric() se = model[row, &#39;SE&#39;] %&gt;% as.numeric() p = model[row, &#39;p&#39;] %&gt;% as.numeric() } else { model = glm(spec, data=data, family=&#39;binomial&#39;, weights=data$weight) %&gt;% broom::tidy() row = which(model$term==rhs) coef = model[row, &#39;estimate&#39;] %&gt;% as.numeric() se = model[row, &#39;std.error&#39;] %&gt;% as.numeric() p = model[row, &#39;p.value&#39;] %&gt;% as.numeric() } z = qnorm(0.995) return(c(coef, p, coef+z*se, coef-z*se)) } stability_plot(grid = grid1, data = diamonds, lhs = lhs_var, rhs = rhs_var, model = starb_logit_enhanced, get_mfx = T, perm = perm_controls, base = base_controls, fe_always = F, run_from = 3) To get your customized plot dfs = stability_plot(grid = grid1, data = diamonds, lhs = lhs_var, rhs = rhs_var, model = starb_logit_enhanced, get_mfx = T, perm = perm_controls, base = base_controls, fe_always = F, run_from = 3, run_to = 5) coef_grid_logit = dfs[[1]] control_grid_logit = dfs[[2]] min_space = 0.5 coef_plot = ggplot2::ggplot(coef_grid_logit, aes( x = model, y = coef, shape = p, group = p )) + geom_linerange(aes(ymin = error_low, ymax = error_high), alpha = 0.75) + geom_point(size = 5, aes(col = p, fill = p), alpha = 1) + viridis::scale_color_viridis(discrete = TRUE, option = &quot;D&quot;) + scale_shape_manual(values = c(15, 17, 18, 19)) + theme_classic() + geom_hline(yintercept = 0, linetype = &#39;dotted&#39;) + ggtitle(&#39;A custom coefficient stability plot!&#39;) + labs(subtitle = &quot;Error bars represent 99% confidence intervals&quot;) + theme( axis.text.x = element_blank(), axis.title = element_blank(), axis.ticks.x = element_blank() ) + coord_cartesian(xlim = c(1 - min_space, max(coef_grid_logit$model) + min_space), ylim = c(-0.1, 1.6)) + guides(fill = F, shape = F, col = F) control_plot = ggplot(control_grid_logit) + geom_point(aes(x = model, y = y, fill=value), shape=23, size=4) + scale_fill_manual(values=c(&#39;#FFFFFF&#39;, &#39;#000000&#39;)) + guides(fill=F) + scale_y_continuous(breaks = unique(control_grid_logit$y), labels = unique(control_grid_logit$key), limits=c(min(control_grid_logit$y)-1, max(control_grid_logit$y)+1)) + scale_x_continuous(breaks=c(1:max(control_grid_logit$model))) + coord_cartesian(xlim=c(1-min_space, max(control_grid_logit$model)+min_space)) + theme_classic() + theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), axis.title = element_blank(), axis.text.y = element_text(size=10), axis.ticks = element_blank(), axis.line = element_blank()) cowplot::plot_grid(coef_plot, control_plot, rel_heights=c(1,0.5), align=&#39;v&#39;, ncol=1, axis=&#39;b&#39;) To get different model specification (e.g., probit vs. logit) starb_probit = function(spec, data, rhs, ...) { # Unpack ... l = list(...) get_mfx = ifelse(is.null(l$get_mfx), F, T) # Set a default to F spec = as.formula(spec) if (get_mfx) { model = glm( spec, data = data, family = binomial(link = &#39;probit&#39;), weights = data$weight ) %&gt;% margins() %&gt;% summary row = which(model$factor == rhs) coef = model[row, &#39;AME&#39;] %&gt;% as.numeric() se = model[row, &#39;SE&#39;] %&gt;% as.numeric() p = model[row, &#39;p&#39;] %&gt;% as.numeric() } else { model = glm( spec, data = data, family = binomial(link = &#39;probit&#39;), weights = data$weight ) %&gt;% broom::tidy() row = which(model$term == rhs) coef = model[row, &#39;estimate&#39;] %&gt;% as.numeric() se = model[row, &#39;std.error&#39;] %&gt;% as.numeric() p = model[row, &#39;p.value&#39;] %&gt;% as.numeric() } z = qnorm(0.995) return(c(coef, p, coef + z * se, coef - z * se)) } probit_dfs = stability_plot( grid = grid1, data = diamonds, lhs = lhs_var, rhs = rhs_var, model = starb_probit, get_mfx = T, perm = perm_controls, base = base_controls, fe_always = F, run_from = 3, run_to = 5 ) # We&#39;ll put the probit DFs on the left, #so we need to adjust the model numbers accordingly # so the probit and logit DFs don&#39;t plot on top of one another! coef_grid_probit = probit_dfs[[1]] %&gt;% mutate(model = model + max(coef_grid_logit$model)) control_grid_probit = probit_dfs[[2]] %&gt;% mutate(model = model + max(control_grid_logit$model)) coef_grid = bind_rows(coef_grid_logit, coef_grid_probit) control_grid = bind_rows(control_grid_logit, control_grid_probit) panels = stability_plot( coef_grid = coef_grid, control_grid = control_grid, data = diamonds, lhs = lhs_var, rhs = rhs_var, perm = perm_controls, base = base_controls, fe_always = F, run_from = 5, run_to = 6 ) coef_plot = panels[[1]] + geom_vline(xintercept = 8.5, linetype = &#39;dashed&#39;, alpha = 0.8) + annotate( geom = &#39;label&#39;, x = 4.25, y = 1.8, label = &#39;Logit models&#39;, size = 6, fill = &#39;#D3D3D3&#39;, alpha = 0.7 ) + annotate( geom = &#39;label&#39;, x = 12.75, y = 1.8, label = &#39;Probit models&#39;, size = 6, fill = &#39;#D3D3D3&#39;, alpha = 0.7 ) + coord_cartesian(ylim = c(-0.5, 1.9)) control_plot = panels[[2]] + geom_vline(xintercept = 8.5, linetype = &#39;dashed&#39;, alpha = 0.8) cowplot::plot_grid( coef_plot, control_plot, rel_heights = c(1, 0.5), align = &#39;v&#39;, ncol = 1, axis = &#39;b&#39; ) 40.1.2 rdfanalysis Not recommend Installation devtools::install_github(&quot;joachim-gassen/rdfanalysis&quot;) Example by the package’s author library(rdfanalysis) load(url(&quot;https://joachim-gassen.github.io/data/rdf_ests.RData&quot;)) plot_rdf_spec_curve(ests, &quot;est&quot;, &quot;lb&quot;, &quot;ub&quot;) Shiny app for readers to explore design &lt;- define_design(steps = c(&quot;read_data&quot;, &quot;select_idvs&quot;, &quot;treat_extreme_obs&quot;, &quot;specify_model&quot;, &quot;est_model&quot;), rel_dir = &quot;vignettes/case_study_code&quot;) shiny_rdf_spec_curve(ests, list(&quot;est&quot;, &quot;lb&quot;, &quot;ub&quot;), design, &quot;vignettes/case_study_code&quot;, &quot;https://joachim-gassen.github.io/data/wb_new.csv&quot;) References "],["coefficient-stability.html", "40.2 Coefficient stability", " 40.2 Coefficient stability (Oster 2019) Coefficient stability can be evident against omitted variable bias. But coefficient stability alone can be misleading, but combing with \\(R^2\\) movement, it can become informative. Packages mplot: graphical Model stability and Variable Selection robomit: Robustness checks for omitted variable bias (implementation of library(robomit) # estimate beta o_beta( y = &quot;mpg&quot;, # dependent variable x = &quot;wt&quot;, # independent treatment variable con = &quot;hp + qsec&quot;, # related control variables delta = 1, # delta R2max = 0.9, # maximum R-square type = &quot;lm&quot;, # model type data = mtcars # dataset ) #&gt; # A tibble: 10 × 2 #&gt; Name Value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 beta* -2.00 #&gt; 2 (beta*-beta controlled)^2 5.56 #&gt; 3 Alternative Solution 1 -7.01 #&gt; 4 (beta[AS1]-beta controlled)^2 7.05 #&gt; 5 Uncontrolled Coefficient -5.34 #&gt; 6 Controlled Coefficient -4.36 #&gt; 7 Uncontrolled R-square 0.753 #&gt; 8 Controlled R-square 0.835 #&gt; 9 Max R-square 0.9 #&gt; 10 delta 1 References "],["omitted-variable-bias-quantification.html", "40.3 Omitted Variable Bias Quantification", " 40.3 Omitted Variable Bias Quantification To quantify the bias needed to change the substantive conclusion from a causal inference study. library(konfound) pkonfound( est_eff = 5, std_err = 2, n_obs = 1000, n_covariates = 5 ) #&gt; Robustness of Inference to Replacement (RIR): #&gt; To invalidate an inference, 21.506 % of the estimate would have to be due to bias. #&gt; This is based on a threshold of 3.925 for statistical significance (alpha = 0.05). #&gt; #&gt; To invalidate an inference, 215 observations would have to be replaced with cases #&gt; for which the effect is 0 (RIR = 215). #&gt; #&gt; See Frank et al. (2013) for a description of the method. #&gt; #&gt; Citation: Frank, K.A., Maroulis, S., Duong, M., and Kelcey, B. (2013). #&gt; What would it take to change an inference? #&gt; Using Rubin&#39;s causal model to interpret the #&gt; robustness of causal inferences. #&gt; Education, Evaluation and #&gt; Policy Analysis, 35 437-460. pkonfound( est_eff = 5, std_err = 2, n_obs = 1000, n_covariates = 5, to_return = &quot;thresh_plot&quot; ) pkonfound( est_eff = 5, std_err = 2, n_obs = 1000, n_covariates = 5, to_return = &quot;corr_plot&quot; ) "],["replication-and-synthetic-data.html", "Chapter 41 Replication and Synthetic Data", " Chapter 41 Replication and Synthetic Data Access to comprehensive data is pivotal for replication, especially in the realm of social sciences. Yet, often the data are inaccessible, making replication a challenge (G. King 1995). This chapter dives into the nuances of replication, the exceptions to its norms, and the significance of synthetic data. References "],["the-replication-standard.html", "41.1 The Replication Standard", " 41.1 The Replication Standard Replicability in research ensures: Credibility and comprehension of empirical studies. Continuity and progression in the discipline. Enhanced readership and academic citations. For a research to be replicable, the “replication standard” is vital: it entails providing all requisite information for replication by third parties. While quantitative research can, to some extent, offer clear data, qualitative studies pose complexities due to data depth. 41.1.1 Solutions for Empirical Replication Role of Individual Authors: Authors need to vouch for the replication standard for enhancing their work’s credibility. Archives like the Inter-University Consortium for Political and Social Research (ICPSR) serve as depositories for replication datasets. Creation of a Replication Data Set: A public data set, consisting of both original and relevant complementary data, can serve replication purposes. Professional Data Archives: Organizations like ICPSR provide solutions to data storage and accessibility problems. Educational Implications: Replication can be an excellent educational tool, and many programs now emphasize its importance. 41.1.2 Free Data Repositories Zenodo: Hosted by CERN, it provides a place for researchers to deposit datasets. It’s not subject-specific, so it caters to various disciplines. figshare: Allows researchers to upload, share, and cite their datasets. Dryad: Primarily for datasets associated with published articles in the biological and medical sciences. OpenICPSR: A public-facing version of the Inter-University Consortium for Political and Social Research (ICPSR) where researchers can deposit data without any cost. Harvard Dataverse: Hosted by Harvard University, this is an open-source repository software application dedicated to archiving, sharing, and citing research data. Mendeley Data: A multidisciplinary, free-to-use open access data repository where researchers can upload and share their datasets. Open Science Framework (OSF): Offers both a platform for conducting research and a place to deposit datasets. PubMed Central: Specific to life sciences, but it’s an open repository for journal articles, preprints, and datasets. Registry of Research Data Repositories (re3data): While not a repository itself, it provides a global registry of research data repositories from various academic disciplines. SocArXiv: An open archive for the social sciences. EarthArXiv: A preprints archive for earth science. Protein Data Bank (PDB): For 3D structures of large biological molecules. Gene Expression Omnibus (GEO): A public functional genomics data repository. The Language Archive (TLA): Dedicated to data on languages worldwide, especially endangered languages. B2SHARE: A platform for storing and sharing research data sets in various disciplines, especially from European research projects. 41.1.3 Exceptions to Replication Some exceptions to the replication standard are: Confidentiality: Sometimes data, even fragmented, is too sensitive to share. Proprietary Data: Data sets owned by entities might restrict dissemination, but usually, parts of such data can still be shared. Rights of First Publication: Embargos might be set, but the essential data used in a study should be accessible. "],["synthetic-data-an-overview.html", "41.2 Synthetic Data: An Overview", " 41.2 Synthetic Data: An Overview Synthetic data, modeling real data while ensuring anonymity, is becoming pivotal in research. While promising, it has its own complexities and should be approached with caution. 41.2.1 Benefits Privacy preservation. Data fairness and augmentation. Acceleration in research. 41.2.2 Concerns Misconceptions about inherent privacy. Challenges with data outliers. Models relying solely on synthetic data can pose risks. 41.2.3 Further Insights on Synthetic Data Synthetic data bridges the model-centric and data-centric perspectives, making it an essential tool in modern research. Analogously, it’s like viewing the Mona Lisa’s replica, with the real painting stored securely. Future projects, such as utilizing the R’s diamonds dataset for synthetic data generation, hold promise in demonstrating the vast potentials of this technology. For a deeper dive into synthetic data and its applications, refer to (Jordon et al. 2022). References "],["application-15.html", "41.3 Application", " 41.3 Application The easiest way to create synthetic data is to use the synthpop package. Alternatively, you can do it manually library(synthpop) library(tidyverse) library(performance) # library(effectsize) # library(see) # library(patchwork) # library(knitr) # library(kableExtra) head(iris) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa synthpop::codebook.syn(iris) #&gt; $tab #&gt; variable class nmiss perctmiss ndistinct #&gt; 1 Sepal.Length numeric 0 0 35 #&gt; 2 Sepal.Width numeric 0 0 23 #&gt; 3 Petal.Length numeric 0 0 43 #&gt; 4 Petal.Width numeric 0 0 22 #&gt; 5 Species factor 0 0 3 #&gt; details #&gt; 1 Range: 4.3 - 7.9 #&gt; 2 Range: 2 - 4.4 #&gt; 3 Range: 1 - 6.9 #&gt; 4 Range: 0.1 - 2.5 #&gt; 5 &#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39; #&gt; #&gt; $labs #&gt; NULL syn_df &lt;- syn(iris, seed = 3) #&gt; #&gt; Synthesis #&gt; ----------- #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species # check for replciated uniques replicated.uniques(syn_df, iris) #&gt; $replications #&gt; [1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE #&gt; [13] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE #&gt; [25] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE #&gt; [73] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE #&gt; [85] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE #&gt; [97] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE #&gt; [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; [133] FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [145] FALSE FALSE FALSE FALSE FALSE FALSE #&gt; #&gt; $no.uniques #&gt; [1] 148 #&gt; #&gt; $no.replications #&gt; [1] 17 #&gt; #&gt; $per.replications #&gt; [1] 11.33333 # remove replicated uniques and adds a FAKE_DATA label # (in case a person can see his or own data in # the replicated data by chance) syn_df_sdc &lt;- sdc(syn_df, iris, label = &quot;FAKE_DATA&quot;, rm.replicated.uniques = T) #&gt; no. of replicated uniques: 17 iris |&gt; GGally::ggpairs() syn_df$syn |&gt; GGally::ggpairs() lm_ori &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = iris) # performance::check_model(lm_ori) summary(lm_ori) #&gt; #&gt; Call: #&gt; lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = iris) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.96159 -0.23489 0.00077 0.21453 0.78557 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.24914 0.24797 9.07 7.04e-16 *** #&gt; Sepal.Width 0.59552 0.06933 8.59 1.16e-14 *** #&gt; Petal.Length 0.47192 0.01712 27.57 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3333 on 147 degrees of freedom #&gt; Multiple R-squared: 0.8402, Adjusted R-squared: 0.838 #&gt; F-statistic: 386.4 on 2 and 147 DF, p-value: &lt; 2.2e-16 lm_syn &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df$syn) # performance::check_model(lm_syn) summary(lm_syn) #&gt; #&gt; Call: #&gt; lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = syn_df$syn) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.79165 -0.22790 -0.01448 0.15893 1.13360 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.96449 0.24538 12.081 &lt; 2e-16 *** #&gt; Sepal.Width 0.39214 0.06816 5.754 4.9e-08 *** #&gt; Petal.Length 0.45267 0.01743 25.974 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3658 on 147 degrees of freedom #&gt; Multiple R-squared: 0.8246, Adjusted R-squared: 0.8222 #&gt; F-statistic: 345.6 on 2 and 147 DF, p-value: &lt; 2.2e-16 Open data can be assessed for its utility in two distinct ways: General Utility: This refers to the broad resemblances within the dataset, allowing for preliminary data exploration. Specific Utility: This focuses on the comparability of models derived from synthetic and original datasets, emphasizing analytical reproducibility. For General utility compare(syn_df, iris) Specific utility # just like regular lm, but for synthetic data lm_syn &lt;- lm.synds(Sepal.Length ~ Sepal.Width + Petal.Length , data = syn_df) compare(lm_syn, iris) #&gt; #&gt; Call used to fit models to the data: #&gt; lm.synds(formula = Sepal.Length ~ Sepal.Width + Petal.Length, #&gt; data = syn_df) #&gt; #&gt; Differences between results based on synthetic and observed data: #&gt; Synthetic Observed Diff Std. coef diff CI overlap #&gt; (Intercept) 2.9644900 2.2491402 0.71534988 2.884829 0.2640608 #&gt; Sepal.Width 0.3921429 0.5955247 -0.20338187 -2.933611 0.2516161 #&gt; Petal.Length 0.4526695 0.4719200 -0.01925058 -1.124602 0.7131064 #&gt; #&gt; Measures for one synthesis and 3 coefficients #&gt; Mean confidence interval overlap: 0.4095944 #&gt; Mean absolute std. coef diff: 2.314347 #&gt; #&gt; Mahalanobis distance ratio for lack-of-fit (target 1.0): 3.08 #&gt; Lack-of-fit test: 9.23442; p-value 0.0263 for test that synthesis model is #&gt; compatible with a chi-squared test with 3 degrees of freedom. #&gt; #&gt; Confidence interval plot: # summary(lm_syn) You basically want your lack-of-fit test to be non-significant. "],["appendix.html", "A Appendix ", " A Appendix "],["git.html", "A.1 Git", " A.1 Git Cheat Sheet Cheat Sheet in different languages Learn Git Interactive Cheat Sheet Ultimate Guide of Git and GitHub for R user Setting up Git: git config with --global option to configure user name, email, editor, etc. Creating a repository: git init to initialize a repo. Git stores all of its repo data in the .git directory. Tracking changes: git status shows the status of the repo File are stored in the project’s working directory (which users see) The staging area (where the next commit is being built) local repo is where commits are permanently recorded git add put files in the staging area git commit saves the staged content as a new commit in the local repo. git commit -m \"your own message\" to give a messages for the purpose of your commit. History git diff shows differences between commits git checkout recovers old version of fields git checkout HEAD to go to the last commit git checkout &lt;unique ID of your commit&gt; to go to such commit Ignoring .gitignore file tells Git what files to ignore cat . gitignore *.dat results/ ignore files ending with “dat” and folder “results”. Remotes in GitHub A local git repo can be connected to one or more remote repos. Use the HTTPS protocol to connect to remote repos git push copies changes from a local repo to a remote repo git pull copies changes from a remote repo to a local repo Collaborating git clone copies remote repo to create a local repo with a remote called origin automatically set up Branching git check - b &lt;new-branch-name git checkout master to switch to master branch. Conflicts occur when 2 or more people change the same lines of the same file the version control system does not allow to overwrite each other’s changes blindly, but highlights conflicts so that they can be resolved. Licensing People who incorporate General Public License (GPL’d) software into their won software must make their software also open under the GPL license; most other open licenses do not require this. The Creative Commons family of licenses allow people to mix and match requirements and restrictions on attribution, creation of derivative works, further sharing and commercialization. Citation: Add a CITATION file to a repo to explain how you want others to cite your work. Hosting Rules regarding intellectual property and storage of sensitive info apply no matter where code and data are hosted. "],["short-cut.html", "A.2 Short-cut", " A.2 Short-cut These are shortcuts that you probably you remember when working with R. Even though it might take a bit of time to learn and use them as your second nature, but they will save you a lot of time. Just like learning another language, the more you speak and practice it, the more comfortable you are speaking it. function short-cut navigate folders in console \" \" + tab pull up short-cut cheat sheet ctrl + shift + k go to file/function (everything in your project) ctrl + . search everything cmd + shift + f navigate between tabs Crtl + shift + . type function faster snip + shift + tab type faster use tab for fuzzy match cmd + up ctrl + . Sometimes you can’t stage a folder because it’s too large. In such case, use Terminal pane in Rstudio then type git add -A to stage all changes then commit and push like usual. "],["function-short-cut.html", "A.3 Function short-cut", " A.3 Function short-cut apply one function to your data to create a new variable: mutate(mod=map(data,function)) instead of using i in 1:length(object): for (i in seq_along(object)) apply multiple function: map_dbl apply multiple function to multiple variables:map2 autoplot(data) plot times series data mod_tidy = linear(reg) %&gt;% set_engine('lm') %&gt;% fit(price ~ ., data=data) fit lm model. It could also fit other models (stan, spark, glmnet, keras) Sometimes, data-masking will not be able to recognize whether you’re calling from environment or data variables. To bypass this, we use .data$variable or .env$variable. For example data %&gt;% mutate(x=.env$variable/.data$variable Problems with data-masking: Unexpected masking by data-var: Use .data and .env to disambiguate Data-var cant get through: Tunnel data-var with {{}} + Subset .data with [[]] Passing Data-variables through arguments library(&quot;dplyr&quot;) mean_by &lt;- function(data,by,var){ data %&gt;% group_by({{{by}}}) %&gt;% summarise(&quot;{{var}}&quot;:=mean({{var}})) # new name for each var will be created by tunnel data-var inside strings } mean_by &lt;- function(data,by,var){ data %&gt;% group_by({{{by}}}) %&gt;% summarise(&quot;{var}&quot;:=mean({{var}})) # use single {} to glue the string, but hard to reuse code in functions } Trouble with selection: library(&quot;purrr&quot;) name &lt;- c(&quot;mass&quot;,&quot;height&quot;) starwars %&gt;% select(name) # Data-var. Here you are referring to variable named &quot;name&quot; starwars %&gt;% select(all_of((name))) # use all_of() to disambiguate when averages &lt;- function(data,vars){ # take character vectors with all_of() data %&gt;% select(all_of(vars)) %&gt;% map_dbl(mean,na.rm=TRUE) } x = c(&quot;Sepal.Length&quot;,&quot;Petal.Length&quot;) iris %&gt;% averages(x) # Another way averages &lt;- function(data,vars){ # Tunnel selectiosn with {{}} data %&gt;% select({{vars}}) %&gt;% map_dbl(mean,na.rm=TRUE) } x = c(&quot;Sepal.Length&quot;,&quot;Petal.Length&quot;) iris %&gt;% averages(x) "],["citation.html", "A.4 Citation", " A.4 Citation include a citation by [@Farjam_2015] cite packages used in this session package=ls(sessionInfo()$loadedOnly) for (i in package){print(toBibtex(citation(i)))} package=ls(sessionInfo()$loadedOnly) for (i in package){ print(toBibtex(citation(i))) } "],["install-all-necessary-packageslibaries-on-your-local-machine.html", "A.5 Install all necessary packages/libaries on your local machine", " A.5 Install all necessary packages/libaries on your local machine Get a list of packages you need to install from this book (or your local device) installed &lt;- as.data.frame(installed.packages()) head(installed) #&gt; Package LibPath Version Priority #&gt; abind abind C:/Program Files/R/R-4.2.3/library 1.4-5 &lt;NA&gt; #&gt; ade4 ade4 C:/Program Files/R/R-4.2.3/library 1.7-22 &lt;NA&gt; #&gt; admisc admisc C:/Program Files/R/R-4.2.3/library 0.33 &lt;NA&gt; #&gt; AER AER C:/Program Files/R/R-4.2.3/library 1.2-10 &lt;NA&gt; #&gt; afex afex C:/Program Files/R/R-4.2.3/library 1.3-0 &lt;NA&gt; #&gt; agridat agridat C:/Program Files/R/R-4.2.3/library 1.21 &lt;NA&gt; #&gt; Depends #&gt; abind R (&gt;= 1.5.0) #&gt; ade4 R (&gt;= 2.10) #&gt; admisc R (&gt;= 3.5.0) #&gt; AER R (&gt;= 3.0.0), car (&gt;= 2.0-19), lmtest, sandwich (&gt;= 2.4-0),\\nsurvival (&gt;= 2.37-5), zoo #&gt; afex R (&gt;= 3.5.0), lme4 (&gt;= 1.1-8) #&gt; agridat &lt;NA&gt; #&gt; Imports #&gt; abind methods, utils #&gt; ade4 graphics, grDevices, methods, stats, utils, MASS, pixmap, sp,\\nRcpp #&gt; admisc methods #&gt; AER stats, Formula (&gt;= 0.2-0) #&gt; afex pbkrtest (&gt;= 0.4-1), lmerTest (&gt;= 3.0-0), car, reshape2,\\nstats, methods, utils #&gt; agridat &lt;NA&gt; #&gt; LinkingTo #&gt; abind &lt;NA&gt; #&gt; ade4 Rcpp, RcppArmadillo #&gt; admisc &lt;NA&gt; #&gt; AER &lt;NA&gt; #&gt; afex &lt;NA&gt; #&gt; agridat &lt;NA&gt; #&gt; Suggests #&gt; abind &lt;NA&gt; #&gt; ade4 ade4TkGUI, adegraphics, adephylo, ape, CircStats, deldir,\\nlattice, spdep, splancs, waveslim, progress, foreach, parallel,\\ndoParallel, iterators #&gt; admisc QCA (&gt;= 3.7) #&gt; AER boot, dynlm, effects, fGarch, forecast, foreign, ineq,\\nKernSmooth, lattice, longmemo, MASS, mlogit, nlme, nnet, np,\\nplm, pscl, quantreg, rgl, ROCR, rugarch, sampleSelection,\\nscatterplot3d, strucchange, systemfit (&gt;= 1.1-20), truncreg,\\ntseries, urca, vars #&gt; afex emmeans (&gt;= 1.4), coin, xtable, parallel, plyr, optimx,\\nnloptr, knitr, rmarkdown, R.rsp, lattice, latticeExtra,\\nmultcomp, testthat, mlmRev, dplyr, tidyr, dfoptim, Matrix,\\npsychTools, ggplot2, MEMSS, effects, carData, ggbeeswarm, nlme,\\ncowplot, jtools, ggpubr, ggpol, MASS, glmmTMB, brms, rstanarm,\\nstatmod, performance (&gt;= 0.7.2), see (&gt;= 0.6.4), ez,\\nggResidpanel, grid, vdiffr #&gt; agridat AER, agricolae, betareg, broom, car, coin, corrgram, desplot,\\ndplyr, effects, equivalence, emmeans, FrF2, gam, gge, ggplot2,\\ngnm, gstat, HH, knitr, lattice, latticeExtra, lme4, lucid,\\nmapproj, maps, MASS, MCMCglmm, metafor, mgcv, NADA, nlme,\\nnullabor, ordinal, pbkrtest, pls, pscl, reshape2, rgdal,\\nrmarkdown, qicharts, qtl, sp, SpATS, survival, vcd, testthat #&gt; Enhances License License_is_FOSS License_restricts_use OS_type #&gt; abind &lt;NA&gt; LGPL (&gt;= 2) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; ade4 &lt;NA&gt; GPL (&gt;= 2) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; admisc &lt;NA&gt; GPL (&gt;= 3) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; AER &lt;NA&gt; GPL-2 | GPL-3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; afex &lt;NA&gt; GPL (&gt;= 2) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; agridat &lt;NA&gt; CC BY-SA 4.0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; MD5sum NeedsCompilation Built #&gt; abind &lt;NA&gt; no 4.2.0 #&gt; ade4 &lt;NA&gt; yes 4.2.3 #&gt; admisc &lt;NA&gt; yes 4.2.3 #&gt; AER &lt;NA&gt; no 4.2.3 #&gt; afex &lt;NA&gt; no 4.2.3 #&gt; agridat &lt;NA&gt; no 4.2.3 write.csv(installed, file.path(getwd(),&#39;installed.csv&#39;)) After having the installed.csv file on your new or local machine, you can just install the list of packages # import the list of packages installed &lt;- read.csv(&#39;installed.csv&#39;) # get the list of packages that you have on your device baseR &lt;- as.data.frame(installed.packages()) # install only those that you don&#39;t have install.packages(setdiff(installed, baseR)) "],["bookdown-cheat-sheet.html", "B Bookdown cheat sheet", " B Bookdown cheat sheet # to see non-scientific notation a result format(12e-17, scientific = FALSE) #&gt; [1] &quot;0.00000000000000012&quot; "],["operation.html", "B.1 Operation", " B.1 Operation R commands to do derivatives of a defined function Taking derivatives in R involves using the expression, D, and eval functions. You wrap the function you want to take the derivative of in expression(), then use D, then eval as follows. simple example #define a function f=expression(sqrt(x)) #take the first derivative df.dx=D(f,&#39;x&#39;) df.dx #&gt; 0.5 * x^-0.5 #take the second derivative d2f.dx2=D(D(f,&#39;x&#39;),&#39;x&#39;) d2f.dx2 #&gt; 0.5 * (-0.5 * x^-1.5) Evaluate The first argument passed to eval is the expression you want to evaluate the second is a list containing the values of all quantities that are not defined elsewhere. #evaluate the function at a given x eval(f,list(x=3)) #&gt; [1] 1.732051 #evaluate the first derivative at a given x eval(df.dx,list(x=3)) #&gt; [1] 0.2886751 #evaluate the second derivative at a given x eval(d2f.dx2,list(x=3)) #&gt; [1] -0.04811252 "],["math-expression-syntax.html", "B.2 Math Expression/ Syntax", " B.2 Math Expression/ Syntax Full list Aligning equations \\begin{aligned} a &amp; = b \\\\ X &amp;\\sim {Norm}(10, 3) \\\\ 5 &amp; \\le 10 \\end{aligned} \\[ \\begin{aligned} a &amp; = b \\\\ X &amp;\\sim {Norm}(10, 3) \\\\ 5 &amp; \\le 10 \\end{aligned} \\] Cross-reference equation \\begin{equation} a = b (\\#eq:test) \\end{equation} \\[\\begin{equation} a = b \\tag{B.1} \\end{equation}\\] to refer in a sentence (B.1) (\\@ref(eq:test)) Math Syntax Notation $\\pm$ \\(\\pm\\) $\\ge$ \\(\\ge\\) $\\le$ \\(\\le\\) $\\neq$ \\(\\neq\\) $\\equiv$ \\(\\equiv\\) $^\\circ$ \\(^\\circ\\) $\\times$ \\(\\times\\) $\\cdot$ \\(\\cdot\\) $\\leq$ \\(\\leq\\) $\\geq$ \\(\\geq\\) \\propto \\(\\propto\\) $\\subset$ \\(\\subset\\) $\\subseteq$ \\(\\subseteq\\) $\\leftarrow$ \\(\\leftarrow\\) $\\rightarrow$ \\(\\rightarrow\\) $\\Leftarrow$ \\(\\Leftarrow\\) $\\Rightarrow$ \\(\\Rightarrow\\) $\\approx$ \\(\\approx\\) $\\mathbb{R}$ \\(\\mathbb{R}\\) $\\sum_{n=1}^{10} n^2$ \\(\\sum_{n=1}^{10} n^2\\) $$\\sum_{n=1}^{10} n^2$$ \\[\\sum_{n=1}^{10} n^2\\] $x^{n}$ \\(x^{n}\\) $x_{n}$ \\(x_{n}\\) $\\overline{x}$ \\(\\overline{x}\\) $\\hat{x}$ \\(\\hat{x}\\) $\\tilde{x}$ \\(\\tilde{x}\\) \\check{} \\(\\check{}\\) \\underset{\\gamma}{\\operatorname{argmin}} \\(\\underset{\\gamma}{\\operatorname{argmin}}\\) $\\frac{a}{b}$ \\(\\frac{a}{b}\\) $\\frac{a}{b}$ \\(\\frac{a}{b}\\) $\\displaystyle \\frac{a}{b}$ \\(\\displaystyle \\frac{a}{b}\\) $\\binom{n}{k}$ \\(\\binom{n}{k}\\) $x_{1} + x_{2} + \\cdots + x_{n}$ \\(x_{1} + x_{2} + \\cdots + x_{n}\\) $x_{1}, x_{2}, \\dots, x_{n}$ \\(x_{1}, x_{2}, \\dots, x_{n}\\) \\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle$ \\(\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle\\) $x \\in A$ \\(x \\in A\\) $|A|$ \\(|A|\\) $x \\in A$ \\(x \\in A\\) $x \\subset B$ \\(x \\subset B\\) $x \\subseteq B$ \\(x \\subseteq B\\) $A \\cup B$ \\(A \\cup B\\) $A \\cap B$ \\(A \\cap B\\) $X \\sim Binom(n, \\pi)$ \\(X \\sim Binom(n, \\pi)\\) $\\mathrm{P}(X \\le x) = \\text{pbinom}(x, n, \\pi)$ \\(\\mathrm{P}(X \\le x) = \\text{pbinom}(x, n, \\pi)\\) $P(A \\mid B)$ \\(P(A \\mid B)\\) $\\mathrm{P}(A \\mid B)$ \\(\\mathrm{P}(A \\mid B)\\) $\\{1, 2, 3\\}$ \\(\\{1, 2, 3\\}\\) $\\sin(x)$ \\(\\sin(x)\\) $\\log(x)$ \\(\\log(x)\\) $\\int_{a}^{b}$ \\(\\int_{a}^{b}\\) $\\left(\\int_{a}^{b} f(x) \\; dx\\right)$ \\(\\left(\\int_{a}^{b} f(x) \\; dx\\right)\\) $\\left[\\int_{\\-infty}^{\\infty} f(x) \\; dx\\right]$ \\(\\left[\\int_{-\\infty}^{\\infty} f(x) \\; dx\\right]\\) $\\left. F(x) \\right|_{a}^{b}$ \\(\\left. F(x) \\right|_{a}^{b}\\) $\\sum_{x = a}^{b} f(x)$ \\(\\sum_{x = a}^{b} f(x)\\) $\\prod_{x = a}^{b} f(x)$ \\(\\prod_{x = a}^{b} f(x)\\) $\\lim_{x \\to \\infty} f(x)$ \\(\\lim_{x \\to \\infty} f(x)\\) $\\displaystyle \\lim_{x \\to \\infty} f(x)$ \\(\\displaystyle \\lim_{x \\to \\infty} f(x)\\) Greek Letters $\\alpha A$ \\(\\alpha A\\) $\\beta B$ \\(\\beta B\\) $\\gamma \\Gamma$ \\(\\gamma \\Gamma\\) $\\delta \\Delta$ \\(\\delta \\Delta\\) $\\epsilon \\varepsilon E$ \\(\\epsilon \\varepsilon E\\) $\\zeta Z \\sigma $ \\(\\zeta Z \\sigma\\) $\\eta H$ \\(\\eta H\\) $\\theta \\vartheta \\Theta$ \\(\\theta \\vartheta \\Theta\\) $\\iota I$ \\(\\iota I\\) $\\kappa K$ \\(\\kappa K\\) $\\lambda \\Lambda$ \\(\\lambda \\Lambda\\) $\\mu M$ \\(\\mu M\\) $\\nu N$ \\(\\nu N\\) $\\xi\\Xi$ \\(\\xi\\Xi\\) $o O$ \\(o O\\) $\\pi \\Pi$ \\(\\pi \\Pi\\) $\\rho\\varrho P$ \\(\\rho\\varrho P\\) $\\sigma \\Sigma$ \\(\\sigma \\Sigma\\) $\\tau T$ \\(\\tau T\\) $\\upsilon \\Upsilon$ \\(\\upsilon \\Upsilon\\) $\\phi \\varphi \\Phi$ \\(\\phi \\varphi \\Phi\\) $\\chi X$ \\(\\chi X\\) $\\psi \\Psi$ \\(\\psi \\Psi\\) $\\omega \\Omega$ \\(\\omega \\Omega\\) $\\cdot$ \\(\\cdot\\) $\\cdots$ \\(\\cdots\\) $\\ddots$ \\(\\ddots\\) $\\ldots$ \\(\\ldots\\) Limit P(\\lim_{n\\to \\infty}\\bar{X}_n =\\mu) =1 \\[ P(\\lim_{n\\to \\infty}\\bar{X}_n =\\mu) =1 \\] Matrices $$\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} $$ \\[ \\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} \\] $$\\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] $$ \\[ \\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] \\] Aligning Equations Aligning Equations with Comments \\begin{aligned} 3+x &amp;=4 &amp;&amp; \\text{(Solve for} x \\text{.)}\\\\ x &amp;=4-3 &amp;&amp; \\text{(Subtract 3 from both sides.)}\\\\ x &amp;=1 &amp;&amp; \\text{(Yielding the solution.)} \\end{aligned} \\[ \\begin{aligned} 3+x &amp;=4 &amp; &amp;\\text{(Solve for} x \\text{.)} \\\\ x &amp;=4-3 &amp;&amp; \\text{(Subtract 3 from both sides.)} \\\\ x &amp;=1 &amp;&amp; \\text{(Yielding the solution.)} \\end{aligned} \\] B.2.1 Statistics Notation $$ f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y} $$ \\[ f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y} \\] \\begin{cases} \\frac{1}{b-a}&amp;\\text{for $x\\in[a,b]$}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases} \\[ \\begin{cases} \\frac{1}{b-a} &amp; \\text{for } x\\in[a,b]\\\\ 0 &amp; \\text{otherwise}\\\\ \\end{cases} \\] "],["table.html", "B.3 Table", " B.3 Table +---------------+---------------+--------------------+ | Fruit | Price | Advantages | +===============+===============+====================+ | *Bananas* | $1.34 | - built-in wrapper | | | | - bright color | +---------------+---------------+--------------------+ | Oranges | $2.10 | - cures scurvy | | | | - **tasty** | +---------------+---------------+--------------------+ Fruit Price Advantages Bananas $1.34 built-in wrapper bright color Oranges $2.10 cures scurvy tasty (\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y} \\((\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}\\) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
