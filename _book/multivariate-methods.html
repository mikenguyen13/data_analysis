<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 22 Multivariate Methods | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="\(y_1,...,y_p\) are possibly correlated random variables with means \(\mu_1,...,\mu_p\) \[ \mathbf{y} = \left( \begin{array} {c} y_1 \\ . \\ y_p \\ \end{array} \right) \] \[ E(\mathbf{y}) = \left(...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Chapter 22 Multivariate Methods | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/multivariate-methods.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="\(y_1,...,y_p\) are possibly correlated random variables with means \(\mu_1,...,\mu_p\) \[ \mathbf{y} = \left( \begin{array} {c} y_1 \\ . \\ y_p \\ \end{array} \right) \] \[ E(\mathbf{y}) = \left(...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 22 Multivariate Methods | A Guide on Data Analysis">
<meta name="twitter:description" content="\(y_1,...,y_p\) are possibly correlated random variables with means \(\mu_1,...,\mu_p\) \[ \mathbf{y} = \left( \begin{array} {c} y_1 \\ . \\ y_p \\ \end{array} \right) \] \[ E(\mathbf{y}) = \left(...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-stat.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="model-specification.html"><span class="header-section-number">10</span> Model Specification</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">11</span> Imputation (Missing Data)</a></li>
<li><a class="" href="data.html"><span class="header-section-number">12</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">13</span> Variable Transformation</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">14</span> Hypothesis Testing</a></li>
<li><a class="" href="marginal-effects.html"><span class="header-section-number">15</span> Marginal Effects</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">16</span> Prediction and Estimation</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">17</span> Moderation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">18</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="experimental-design.html"><span class="header-section-number">19</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">20</span> Sampling</a></li>
<li><a class="" href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></li>
<li><a class="active" href="multivariate-methods.html"><span class="header-section-number">22</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></li>
<li><a class="" href="regression-discontinuity.html"><span class="header-section-number">24</span> Regression Discontinuity</a></li>
<li><a class="" href="synthetic-difference-in-differences.html"><span class="header-section-number">25</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="difference-in-differences.html"><span class="header-section-number">26</span> Difference-in-differences</a></li>
<li><a class="" href="synthetic-control.html"><span class="header-section-number">27</span> Synthetic Control</a></li>
<li><a class="" href="event-studies.html"><span class="header-section-number">28</span> Event Studies</a></li>
<li><a class="" href="matching-methods.html"><span class="header-section-number">29</span> Matching Methods</a></li>
<li><a class="" href="interrupted-time-series.html"><span class="header-section-number">30</span> Interrupted Time Series</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">31</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">32</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">33</span> Controls</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="mediation.html"><span class="header-section-number">34</span> Mediation</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">35</span> Directed Acyclic Graph</a></li>
<li><a class="" href="report.html"><span class="header-section-number">36</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">37</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">38</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">39</span> Replication and Synthetic Data</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multivariate-methods" class="section level1" number="22">
<h1>
<span class="header-section-number">22</span> Multivariate Methods<a class="anchor" aria-label="anchor" href="#multivariate-methods"><i class="fas fa-link"></i></a>
</h1>
<p><span class="math inline">\(y_1,...,y_p\)</span> are possibly correlated random variables with means <span class="math inline">\(\mu_1,...,\mu_p\)</span></p>
<p><span class="math display">\[
\mathbf{y} =
\left(
\begin{array}
{c}
y_1 \\
. \\
y_p \\
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
E(\mathbf{y}) =
\left(
\begin{array}
{c}
\mu_1 \\
. \\
\mu_p \\
\end{array}
\right)
\]</span></p>
<p>Let <span class="math inline">\(\sigma_{ij} = cov(y_i, y_j)\)</span> for <span class="math inline">\(i,j = 1,…,p\)</span></p>
<p><span class="math display">\[
\mathbf{\Sigma} = (\sigma_{ij}) =
\left(
\begin{array}
{cccc}
\sigma_{11} &amp; \sigma_{22} &amp; ... &amp;  \sigma_{1p} \\
\sigma_{21} &amp; \sigma_{22} &amp; ... &amp; \sigma_{2p} \\
. &amp; . &amp; . &amp; . \\
\sigma_{p1} &amp; \sigma_{p2} &amp; ... &amp; \sigma_{pp}
\end{array}
\right)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\Sigma}\)</span> (symmetric) is the variance-covariance or dispersion matrix</p>
<p>Let <span class="math inline">\(\mathbf{u}_{p \times 1}\)</span> and <span class="math inline">\(\mathbf{v}_{q \times 1}\)</span> be random vectors with means <span class="math inline">\(\mu_u\)</span> and <span class="math inline">\(\mu_v\)</span> . Then</p>
<p><span class="math display">\[
\mathbf{\Sigma}_{uv} = cov(\mathbf{u,v}) = E[(\mathbf{u} - \mu_u)(\mathbf{v} - \mu_v)']
\]</span></p>
<p>in which <span class="math inline">\(\mathbf{\Sigma}_{uv} \neq \mathbf{\Sigma}_{vu}\)</span> and <span class="math inline">\(\mathbf{\Sigma}_{uv} = \mathbf{\Sigma}_{vu}'\)</span></p>
<p><br><strong>Properties of Covariance Matrices</strong></p>
<ol style="list-style-type: decimal">
<li>Symmetric <span class="math inline">\(\mathbf{\Sigma}' = \mathbf{\Sigma}\)</span>
</li>
<li>Non-negative definite <span class="math inline">\(\mathbf{a'\Sigma a} \ge 0\)</span> for any <span class="math inline">\(\mathbf{a} \in R^p\)</span>, which is equivalent to eigenvalues of <span class="math inline">\(\mathbf{\Sigma}\)</span>, <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p \ge 0\)</span>
</li>
<li>
<span class="math inline">\(|\mathbf{\Sigma}| = \lambda_1 \lambda_2 ... \lambda_p \ge 0\)</span> (<strong>generalized variance</strong>) (the bigger this number is, the more variation there is</li>
<li>
<span class="math inline">\(trace(\mathbf{\Sigma}) = tr(\mathbf{\Sigma}) = \lambda_1 + ... + \lambda_p = \sigma_{11} + ... + \sigma_{pp} =\)</span> sum of variance (<strong>total variance</strong>)</li>
</ol>
<p>Note:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{\Sigma}\)</span> is typically required to be positive definite, which means all eigenvalues are positive, and <span class="math inline">\(\mathbf{\Sigma}\)</span> has an inverse <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span> such that <span class="math inline">\(\mathbf{\Sigma}^{-1}\mathbf{\Sigma} = \mathbf{I}_{p \times p} = \mathbf{\Sigma \Sigma}^{-1}\)</span>
</li>
</ul>
<p><strong>Correlation Matrices</strong></p>
<p><span class="math display">\[
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}}
\]</span></p>
<p><span class="math display">\[
\mathbf{R} =
\left(
\begin{array}
{cccc}
\rho_{11} &amp; \rho_{12} &amp; ... &amp; \rho_{1p} \\
\rho_{21} &amp; \rho_{22} &amp; ... &amp; \rho_{2p} \\
. &amp; . &amp; . &amp;. \\
\rho_{p1} &amp; \rho_{p2} &amp; ... &amp; \rho_{pp} \\
\end{array}
\right)
\]</span></p>
<p>where <span class="math inline">\(\rho_{ij}\)</span> is the correlation, and <span class="math inline">\(\rho_{ii} = 1\)</span> for all i</p>
<p>Alternatively,</p>
<p><span class="math display">\[
\mathbf{R} = [diag(\mathbf{\Sigma})]^{-1/2}\mathbf{\Sigma}[diag(\mathbf{\Sigma})]^{-1/2}
\]</span></p>
<p>where <span class="math inline">\(diag(\mathbf{\Sigma})\)</span> is the matrix which has the <span class="math inline">\(\sigma_{ii}\)</span>’s on the diagonal and 0’s elsewhere</p>
<p>and <span class="math inline">\(\mathbf{A}^{1/2}\)</span> (the square root of a symmetric matrix) is a symmetric matrix such as <span class="math inline">\(\mathbf{A} = \mathbf{A}^{1/2}\mathbf{A}^{1/2}\)</span></p>
<p><strong>Equalities</strong></p>
<p>Let</p>
<ul>
<li><p><span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> be random vectors with means <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_y\)</span> and variance -variance matrices <span class="math inline">\(\mathbf{\Sigma}_x\)</span> and <span class="math inline">\(\mathbf{\Sigma}_y\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> be matrices of constants and <span class="math inline">\(\mathbf{c}\)</span> and <span class="math inline">\(\mathbf{d}\)</span> be vectors of constants</p></li>
</ul>
<p>Then</p>
<ul>
<li><p><span class="math inline">\(E(\mathbf{Ay + c} ) = \mathbf{A} \mu_y + c\)</span></p></li>
<li><p><span class="math inline">\(var(\mathbf{Ay + c}) = \mathbf{A} var(\mathbf{y})\mathbf{A}' = \mathbf{A \Sigma_y A}'\)</span></p></li>
<li><p><span class="math inline">\(cov(\mathbf{Ay + c, By+ d}) = \mathbf{A\Sigma_y B}'\)</span></p></li>
<li><p><span class="math inline">\(E(\mathbf{Ay + Bx + c}) = \mathbf{A \mu_y + B \mu_x + c}\)</span></p></li>
<li><p><span class="math inline">\(var(\mathbf{Ay + Bx + c}) = \mathbf{A \Sigma_y A' + B \Sigma_x B' + A \Sigma_{yx}B' + B\Sigma'_{yx}A'}\)</span></p></li>
</ul>
<p><strong>Multivariate Normal Distribution</strong></p>
<p>Let <span class="math inline">\(\mathbf{y}\)</span> be a multivariate normal (MVN) random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\mathbf{\Sigma}\)</span>. Then the density of <span class="math inline">\(\mathbf{y}\)</span> is</p>
<p><span class="math display">\[
f(\mathbf{y}) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2} \mathbf{(y-\mu)'\Sigma^{-1}(y-\mu)} )
\]</span></p>
<p><span class="math inline">\(\mathbf{y} \sim N_p(\mu, \mathbf{\Sigma})\)</span></p>
<div id="properties-of-mvn" class="section level3" number="22.0.1">
<h3>
<span class="header-section-number">22.0.1</span> Properties of MVN<a class="anchor" aria-label="anchor" href="#properties-of-mvn"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Let <span class="math inline">\(\mathbf{A}_{r \times p}\)</span> be a fixed matrix. Then <span class="math inline">\(\mathbf{Ay} \sim N_r (\mathbf{A \mu, A \Sigma A'})\)</span> . <span class="math inline">\(r \le p\)</span> and all rows of <span class="math inline">\(\mathbf{A}\)</span> must be linearly independent to guarantee that <span class="math inline">\(\mathbf{A \Sigma A}'\)</span> is non-singular.</p></li>
<li><p>Let <span class="math inline">\(\mathbf{G}\)</span> be a matrix such that <span class="math inline">\(\mathbf{\Sigma}^{-1} = \mathbf{GG}'\)</span>. Then <span class="math inline">\(\mathbf{G'y} \sim N_p(\mathbf{G' \mu, I})\)</span> and <span class="math inline">\(\mathbf{G'(y-\mu)} \sim N_p (0,\mathbf{I})\)</span></p></li>
<li><p>Any fixed linear combination of <span class="math inline">\(y_1,...,y_p\)</span> (say <span class="math inline">\(\mathbf{c'y}\)</span>) follows <span class="math inline">\(\mathbf{c'y} \sim N_1 (\mathbf{c' \mu, c' \Sigma c})\)</span></p></li>
<li>
<p>Define a partition, <span class="math inline">\([\mathbf{y}'_1,\mathbf{y}_2']'\)</span> where</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y}_1\)</span> is <span class="math inline">\(p_1 \times 1\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{y}_2\)</span> is <span class="math inline">\(p_2 \times 1\)</span>,</p></li>
<li><p><span class="math inline">\(p_1 + p_2 = p\)</span></p></li>
<li><p><span class="math inline">\(p_1,p_2 \ge 1\)</span> Then</p></li>
</ul>
</li>
</ul>
<p><span class="math display">\[
\left(
\begin{array}
{c}
\mathbf{y}_1 \\
\mathbf{y}_2 \\
\end{array}
\right)
\sim
N
\left(
\left(
\begin{array}
{c}
\mu_1 \\
\mu_2 \\
\end{array}
\right),
\left(
\begin{array}
{cc}
\mathbf{\Sigma}_{11} &amp; \mathbf{\Sigma}_{12} \\
\mathbf{\Sigma}_{21} &amp; \mathbf{\Sigma}_{22}\\
\end{array}
\right)
\right)
\]</span></p>
<ul>
<li><p>The marginal distributions of <span class="math inline">\(\mathbf{y}_1\)</span> and <span class="math inline">\(\mathbf{y}_2\)</span> are <span class="math inline">\(\mathbf{y}_1 \sim N_{p1}(\mathbf{\mu_1, \Sigma_{11}})\)</span> and <span class="math inline">\(\mathbf{y}_2 \sim N_{p2}(\mathbf{\mu_2, \Sigma_{22}})\)</span></p></li>
<li><p>Individual components <span class="math inline">\(y_1,...,y_p\)</span> are all normally distributed <span class="math inline">\(y_i \sim N_1(\mu_i, \sigma_{ii})\)</span></p></li>
<li>
<p>The conditional distribution of <span class="math inline">\(\mathbf{y}_1\)</span> and <span class="math inline">\(\mathbf{y}_2\)</span> is normal</p>
<ul>
<li>
<p><span class="math inline">\(\mathbf{y}_1 | \mathbf{y}_2 \sim N_{p1}(\mathbf{\mu_1 + \Sigma_{12} \Sigma_{22}^{-1}(y_2 - \mu_2),\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \sigma_{21}})\)</span></p>
<ul>
<li>In this formula, we see if we know (have info about) <span class="math inline">\(\mathbf{y}_2\)</span>, we can re-weight <span class="math inline">\(\mathbf{y}_1\)</span> ’s mean, and the variance is reduced because we know more about <span class="math inline">\(\mathbf{y}_1\)</span> because we know <span class="math inline">\(\mathbf{y}_2\)</span>
</li>
</ul>
</li>
<li><p>which is analogous to <span class="math inline">\(\mathbf{y}_2 | \mathbf{y}_1\)</span>. And <span class="math inline">\(\mathbf{y}_1\)</span> and <span class="math inline">\(\mathbf{y}_2\)</span> are independently distrusted only if <span class="math inline">\(\mathbf{\Sigma}_{12} = 0\)</span></p></li>
</ul>
</li>
<li><p>If <span class="math inline">\(\mathbf{y} \sim N(\mathbf{\mu, \Sigma})\)</span> and <span class="math inline">\(\mathbf{\Sigma}\)</span> is positive definite, then <span class="math inline">\(\mathbf{(y-\mu)' \Sigma^{-1} (y - \mu)} \sim \chi^2_{(p)}\)</span></p></li>
<li><p>If <span class="math inline">\(\mathbf{y}_i\)</span> are independent <span class="math inline">\(N_p (\mathbf{\mu}_i , \mathbf{\Sigma}_i)\)</span> random variables, then for fixed matrices <span class="math inline">\(\mathbf{A}_{i(m \times p)}\)</span>, <span class="math inline">\(\sum_{i=1}^k \mathbf{A}_i \mathbf{y}_i \sim N_m (\sum_{i=1}^{k} \mathbf{A}_i \mathbf{\mu}_i, \sum_{i=1}^k \mathbf{A}_i \mathbf{\Sigma}_i \mathbf{A}_i)\)</span></p></li>
</ul>
<p><strong>Multiple Regression</strong></p>
<p><span class="math display">\[
\left(
\begin{array}
{c}
Y \\
\mathbf{x}
\end{array}
\right)
\sim
N_{p+1}
\left(
\left[
\begin{array}
{c}
\mu_y \\
\mathbf{\mu}_x
\end{array}
\right]
,
\left[
\begin{array}
{cc}
\sigma^2_Y &amp; \mathbf{\Sigma}_{yx} \\
\mathbf{\Sigma}_{yx} &amp; \mathbf{\Sigma}_{xx}
\end{array}
\right]
\right)
\]</span></p>
<p>The conditional distribution of Y given x follows a univariate normal distribution with</p>
<p><span class="math display">\[
\begin{aligned}
E(Y| \mathbf{x}) &amp;= \mu_y + \mathbf{\Sigma}_{yx} \Sigma_{xx}^{-1} (\mathbf{x}- \mu_x) \\
&amp;= \mu_y - \Sigma_{yx} \Sigma_{xx}^{-1}\mu_x + \Sigma_{yx} \Sigma_{xx}^{-1}\mathbf{x} \\
&amp;= \beta_0 + \mathbf{\beta'x}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\beta = (\beta_1,...,\beta_p)' = \mathbf{\Sigma}_{xx}^{-1} \mathbf{\Sigma}_{yx}'\)</span> (e.g., analogous to <span class="math inline">\(\mathbf{(x'x)^{-1}x'y}\)</span> but not the same if we consider <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\mathbf{x}_i\)</span>, <span class="math inline">\(i = 1,..,n\)</span> and use the empirical covariance formula: <span class="math inline">\(var(Y|\mathbf{x}) = \sigma^2_Y - \mathbf{\Sigma_{yx}\Sigma^{-1}_{xx} \Sigma'_{yx}}\)</span>)</p>
<p><strong>Samples from Multivariate Normal Populations</strong></p>
<p>A random sample of size n, <span class="math inline">\(\mathbf{y}_1,.., \mathbf{y}_n\)</span> from <span class="math inline">\(N_p (\mathbf{\mu}, \mathbf{\Sigma})\)</span>. Then</p>
<ul>
<li><p>Since <span class="math inline">\(\mathbf{y}_1,..., \mathbf{y}_n\)</span> are iid, their sample mean, <span class="math inline">\(\bar{\mathbf{y}} = \sum_{i=1}^n \mathbf{y}_i/n \sim N_p (\mathbf{\mu}, \mathbf{\Sigma}/n)\)</span>. that is, <span class="math inline">\(\bar{\mathbf{y}}\)</span> is an unbiased estimator of <span class="math inline">\(\mathbf{\mu}\)</span></p></li>
<li>
<p>The <span class="math inline">\(p \times p\)</span> sample variance-covariance matrix, <span class="math inline">\(\mathbf{S}\)</span> is <span class="math inline">\(\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n (\mathbf{y}_i - \bar{\mathbf{y}})(\mathbf{y}_i - \bar{\mathbf{y}})' = \frac{1}{n-1} (\sum_{i=1}^n \mathbf{y}_i \mathbf{y}_i' - n \bar{\mathbf{y}}\bar{\mathbf{y}}')\)</span></p>
<ul>
<li>where <span class="math inline">\(\mathbf{S}\)</span> is symmetric, unbiased estimator of <span class="math inline">\(\mathbf{\Sigma}\)</span> and has <span class="math inline">\(p(p+1)/2\)</span> random variables.</li>
</ul>
</li>
<li><p><span class="math inline">\((n-1)\mathbf{S} \sim W_p (n-1, \mathbf{\Sigma})\)</span> is a Wishart distribution with n-1 degrees of freedom and expectation <span class="math inline">\((n-1) \mathbf{\Sigma}\)</span>. The Wishart distribution is a multivariate extension of the Chi-squared distribution.</p></li>
<li><p><span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{S}\)</span> are independent</p></li>
<li><p><span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{S}\)</span> are sufficient statistics. (All of the info in the data about <span class="math inline">\(\mathbf{\mu}\)</span> and <span class="math inline">\(\mathbf{\Sigma}\)</span> is contained in <span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{S}\)</span> , regardless of sample size).</p></li>
</ul>
<p><strong>Large Sample Properties</strong></p>
<p><span class="math inline">\(\mathbf{y}_1,..., \mathbf{y}_n\)</span> are a random sample from some population with mean <span class="math inline">\(\mathbf{\mu}\)</span> and variance-covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<ul>
<li><p><span class="math inline">\(\bar{\mathbf{y}}\)</span> is a consistent estimator for <span class="math inline">\(\mu\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{S}\)</span> is a consistent estimator for <span class="math inline">\(\mathbf{\Sigma}\)</span></p></li>
<li><p><strong>Multivariate Central Limit Theorem</strong>: Similar to the univariate case, <span class="math inline">\(\sqrt{n}(\bar{\mathbf{y}} - \mu) \dot{\sim} N_p (\mathbf{0,\Sigma})\)</span> where n is large relative to p (<span class="math inline">\(n \ge 25p\)</span>), which is equivalent to <span class="math inline">\(\bar{\mathbf{y}} \dot{\sim} N_p (\mu, \mathbf{\Sigma}/n)\)</span></p></li>
<li><p><strong>Wald’s Theorem</strong>: <span class="math inline">\(n(\bar{\mathbf{y}} - \mu)' \mathbf{S}^{-1} (\bar{\mathbf{y}} - \mu)\)</span> when n is large relative to p.</p></li>
</ul>
<p>Maximum Likelihood Estimation for MVN</p>
<p>Suppose iid <span class="math inline">\(\mathbf{y}_1 ,... \mathbf{y}_n \sim N_p (\mu, \mathbf{\Sigma})\)</span>, the likelihood function for the data is</p>
<p><span class="math display">\[
\begin{aligned}
L(\mu, \mathbf{\Sigma}) &amp;= \prod_{j=1}^n (\frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2}(\mathbf{y}_j -\mu)'\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)) \\
&amp;= \frac{1}{(2\pi)^{np/2}|\mathbf{\Sigma}|^{n/2}} \exp(-\frac{1}{2} \sum_{j=1}^n(\mathbf{y}_j -\mu)'\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)
\end{aligned}
\]</span></p>
<p>Then, the MLEs are</p>
<p><span class="math display">\[
\hat{\mu} = \bar{\mathbf{y}}
\]</span></p>
<p><span class="math display">\[
\hat{\mathbf{\Sigma}} = \frac{n-1}{n} \mathbf{S}
\]</span></p>
<p>using derivatives of the log of the likelihood function with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<p><strong>Properties of MLEs</strong></p>
<ul>
<li><p>Invariance: If <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then the MLE of <span class="math inline">\(h(\theta)\)</span> is <span class="math inline">\(h(\hat{\theta})\)</span> for any function h(.)</p></li>
<li><p>Consistency: MLEs are consistent estimators, but they are usually biased</p></li>
<li><p>Efficiency: MLEs are efficient estimators (no other estimator has a smaller variance for large samples)</p></li>
<li>
<p>Asymptotic normality: Suppose that <span class="math inline">\(\hat{\theta}_n\)</span> is the MLE for <span class="math inline">\(\theta\)</span> based upon n independent observations. Then <span class="math inline">\(\hat{\theta}_n \dot{\sim} N(\theta, \mathbf{H}^{-1})\)</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{H}\)</span> is the Fisher Information Matrix, which contains the expected values of the second partial derivatives fo the log-likelihood function. the (i,j)th element of <span class="math inline">\(\mathbf{H}\)</span> is <span class="math inline">\(-E(\frac{\partial^2 l(\mathbf{\theta})}{\partial \theta_i \partial \theta_j})\)</span></p></li>
<li><p>we can estimate <span class="math inline">\(\mathbf{H}\)</span> by finding the form determined above, and evaluate it at <span class="math inline">\(\theta = \hat{\theta}_n\)</span></p></li>
</ul>
</li>
<li>
<p>Likelihood ratio testing: for some null hypothesis, <span class="math inline">\(H_0\)</span> we can form a likelihood ratio test</p>
<ul>
<li><p>The statistic is: <span class="math inline">\(\Lambda = \frac{\max_{H_0}l(\mathbf{\mu}, \mathbf{\Sigma|Y})}{\max l(\mu, \mathbf{\Sigma | Y})}\)</span></p></li>
<li><p>For large n, <span class="math inline">\(-2 \log \Lambda \sim \chi^2_{(v)}\)</span> where v is the number of parameters in the unrestricted space minus the number of parameters under <span class="math inline">\(H_0\)</span></p></li>
</ul>
</li>
</ul>
<p><strong>Test of Multivariate Normality</strong></p>
<ul>
<li>
<p>Check univariate normality for each trait (X) separately</p>
<ul>
<li><p>Can check <span class="math display">\[Normality Assessment\]</span></p></li>
<li><p>The good thing is that if any of the univariate trait is not normal, then the joint distribution is not normal (see again [m]). If a joint multivariate distribution is normal, then the marginal distribution has to be normal.</p></li>
<li><p>However, marginal normality of all traits does not imply joint MVN</p></li>
<li><p>Easily rule out multivariate normality, but not easy to prove it</p></li>
</ul>
</li>
<li>
<p>Mardia’s tests for multivariate normality</p>
<ul>
<li><p>Multivariate skewness is<span class="math display">\[
\beta_{1,p} = E[(\mathbf{y}- \mathbf{\mu})' \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^3
\]</span></p></li>
<li><p>where <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are independent, but have the same distribution (note: <span class="math inline">\(\beta\)</span> here is not regression coefficient)</p></li>
<li><p>Multivariate kurtosis is defined as</p></li>
<li><p><span class="math display">\[
\beta_{2,p} - E[(\mathbf{y}- \mathbf{\mu})' \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^2
\]</span></p></li>
<li><p>For the MVN distribution, we have <span class="math inline">\(\beta_{1,p} = 0\)</span> and <span class="math inline">\(\beta_{2,p} = p(p+2)\)</span></p></li>
<li>
<p>For a sample of size n, we can estimate</p>
<p><span class="math display">\[
\hat{\beta}_{1,p} = \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n g^2_{ij}
\]</span></p>
<p><span class="math display">\[
\hat{\beta}_{2,p} = \frac{1}{n} \sum_{i=1}^n g^2_{ii}
\]</span></p>
<ul>
<li>where <span class="math inline">\(g_{ij} = (\mathbf{y}_i - \bar{\mathbf{y}})' \mathbf{S}^{-1} (\mathbf{y}_j - \bar{\mathbf{y}})\)</span>. Note: <span class="math inline">\(g_{ii} = d^2_i\)</span> where <span class="math inline">\(d^2_i\)</span> is the Mahalanobis distance</li>
</ul>
</li>
<li>
<p><span class="citation">(<a href="references.html#ref-mardia1970measures">Mardia 1970</a>)</span> shows for large n</p>
<p><span class="math display">\[
\kappa_1 = \frac{n \hat{\beta}_{1,p}}{6} \dot{\sim} \chi^2_{p(p+1)(p+2)/6}
\]</span></p>
<p><span class="math display">\[
\kappa_2 = \frac{\hat{\beta}_{2,p} - p(p+2)}{\sqrt{8p(p+2)/n}} \sim N(0,1)
\]</span></p>
<ul>
<li><p>Hence, we can use <span class="math inline">\(\kappa_1\)</span> and <span class="math inline">\(\kappa_2\)</span> to test the null hypothesis of MVN.</p></li>
<li><p>When the data are non-normal, normal theory tests on the mean are sensitive to <span class="math inline">\(\beta_{1,p}\)</span> , while tests on the covariance are sensitive to <span class="math inline">\(\beta_{2,p}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Alternatively, Doornik-Hansen test for multivariate normality <span class="citation">(<a href="references.html#ref-doornik2008omnibus">Doornik and Hansen 2008</a>)</span></p></li>
<li>
<p>Chi-square Q-Q plot</p>
<ul>
<li><p>Let <span class="math inline">\(\mathbf{y}_i, i = 1,...,n\)</span> be a random sample sample from <span class="math inline">\(N_p(\mathbf{\mu}, \mathbf{\Sigma})\)</span></p></li>
<li><p>Then <span class="math inline">\(\mathbf{z}_i = \mathbf{\Sigma}^{-1/2}(\mathbf{y}_i - \mathbf{\mu}), i = 1,...,n\)</span> are iid <span class="math inline">\(N_p (\mathbf{0}, \mathbf{I})\)</span>. Thus, <span class="math inline">\(d_i^2 = \mathbf{z}_i' \mathbf{z}_i \sim \chi^2_p , i = 1,...,n\)</span></p></li>
<li><p>plot the ordered <span class="math inline">\(d_i^2\)</span> values against the qualities of the <span class="math inline">\(\chi^2_p\)</span> distribution. When normality holds, the plot should approximately resemble a straight lien passing through the origin at a 45 degree</p></li>
<li><p>it requires large sample size (i.e., sensitive to sample size). Even if we generate data from a MVN, the tail of the Chi-square Q-Q plot can still be out of line.</p></li>
</ul>
</li>
<li>
<p>If the data are not normal, we can</p>
<ul>
<li><p>ignore it</p></li>
<li><p>use nonparametric methods</p></li>
<li><p>use models based upon an approximate distribution (e.g., GLMM)</p></li>
<li><p>try performing a transformation</p></li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb367"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://friendly.github.io/heplots/">heplots</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">ICSNP</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">MVN</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">trees</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/trees.dat"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">trees</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Nitrogen"</span>,<span class="st">"Phosphorous"</span>,<span class="st">"Potassium"</span>,<span class="st">"Ash"</span>,<span class="st">"Height"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">trees</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':    26 obs. of  5 variables:</span></span>
<span><span class="co">#&gt;  $ Nitrogen   : num  2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ...</span></span>
<span><span class="co">#&gt;  $ Phosphorous: num  0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ...</span></span>
<span><span class="co">#&gt;  $ Potassium  : num  1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ...</span></span>
<span><span class="co">#&gt;  $ Ash        : num  1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ...</span></span>
<span><span class="co">#&gt;  $ Height     : int  351 249 171 373 321 191 225 291 284 213 ...</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">trees</span><span class="op">)</span></span>
<span><span class="co">#&gt;     Nitrogen      Phosphorous       Potassium           Ash        </span></span>
<span><span class="co">#&gt;  Min.   :1.130   Min.   :0.1570   Min.   :0.3800   Min.   :0.4500  </span></span>
<span><span class="co">#&gt;  1st Qu.:1.532   1st Qu.:0.1963   1st Qu.:0.6050   1st Qu.:0.6375  </span></span>
<span><span class="co">#&gt;  Median :1.855   Median :0.2250   Median :0.7150   Median :0.9300  </span></span>
<span><span class="co">#&gt;  Mean   :1.896   Mean   :0.2506   Mean   :0.7619   Mean   :0.8873  </span></span>
<span><span class="co">#&gt;  3rd Qu.:2.160   3rd Qu.:0.2975   3rd Qu.:0.8975   3rd Qu.:0.9825  </span></span>
<span><span class="co">#&gt;  Max.   :2.880   Max.   :0.4170   Max.   :1.3500   Max.   :1.7900  </span></span>
<span><span class="co">#&gt;      Height     </span></span>
<span><span class="co">#&gt;  Min.   : 65.0  </span></span>
<span><span class="co">#&gt;  1st Qu.:122.5  </span></span>
<span><span class="co">#&gt;  Median :181.0  </span></span>
<span><span class="co">#&gt;  Mean   :196.6  </span></span>
<span><span class="co">#&gt;  3rd Qu.:276.0  </span></span>
<span><span class="co">#&gt;  Max.   :373.0</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">trees</span>, method <span class="op">=</span> <span class="st">"pearson"</span><span class="op">)</span> <span class="co"># correlation matrix</span></span>
<span><span class="co">#&gt;              Nitrogen Phosphorous Potassium       Ash    Height</span></span>
<span><span class="co">#&gt; Nitrogen    1.0000000   0.6023902 0.5462456 0.6509771 0.8181641</span></span>
<span><span class="co">#&gt; Phosphorous 0.6023902   1.0000000 0.7037469 0.6707871 0.7739656</span></span>
<span><span class="co">#&gt; Potassium   0.5462456   0.7037469 1.0000000 0.6710548 0.7915683</span></span>
<span><span class="co">#&gt; Ash         0.6509771   0.6707871 0.6710548 1.0000000 0.7676771</span></span>
<span><span class="co">#&gt; Height      0.8181641   0.7739656 0.7915683 0.7676771 1.0000000</span></span>
<span></span>
<span><span class="co"># qq-plot </span></span>
<span><span class="va">gg</span> <span class="op">&lt;-</span> <span class="va">trees</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span>, names_to <span class="op">=</span> <span class="st">"Var"</span>, values_to <span class="op">=</span> <span class="st">"Value"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>sample <span class="op">=</span> <span class="va">Value</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">geom_qq_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="st">"Var"</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span>
<span><span class="va">gg</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb368"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Univariate normality</span></span>
<span><span class="va">sw_tests</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">trees</span>, MARGIN <span class="op">=</span> <span class="fl">2</span>, FUN <span class="op">=</span> <span class="va">shapiro.test</span><span class="op">)</span></span>
<span><span class="va">sw_tests</span></span>
<span><span class="co">#&gt; $Nitrogen</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Shapiro-Wilk normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  newX[, i]</span></span>
<span><span class="co">#&gt; W = 0.96829, p-value = 0.5794</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Phosphorous</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Shapiro-Wilk normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  newX[, i]</span></span>
<span><span class="co">#&gt; W = 0.93644, p-value = 0.1104</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Potassium</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Shapiro-Wilk normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  newX[, i]</span></span>
<span><span class="co">#&gt; W = 0.95709, p-value = 0.3375</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Ash</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Shapiro-Wilk normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  newX[, i]</span></span>
<span><span class="co">#&gt; W = 0.92071, p-value = 0.04671</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Height</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Shapiro-Wilk normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  newX[, i]</span></span>
<span><span class="co">#&gt; W = 0.94107, p-value = 0.1424</span></span>
<span><span class="co"># Kolmogorov-Smirnov test </span></span>
<span><span class="va">ks_tests</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span><span class="va">trees</span>, <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/ks.test.html">ks.test</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">.x</span><span class="op">)</span>,<span class="st">"pnorm"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">ks_tests</span></span>
<span><span class="co">#&gt; $Nitrogen</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Asymptotic one-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  scale(.x)</span></span>
<span><span class="co">#&gt; D = 0.12182, p-value = 0.8351</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Phosphorous</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Asymptotic one-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  scale(.x)</span></span>
<span><span class="co">#&gt; D = 0.17627, p-value = 0.3944</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Potassium</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Asymptotic one-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  scale(.x)</span></span>
<span><span class="co">#&gt; D = 0.10542, p-value = 0.9348</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Ash</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Asymptotic one-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  scale(.x)</span></span>
<span><span class="co">#&gt; D = 0.14503, p-value = 0.6449</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $Height</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Asymptotic one-sample Kolmogorov-Smirnov test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  scale(.x)</span></span>
<span><span class="co">#&gt; D = 0.1107, p-value = 0.9076</span></span>
<span><span class="co">#&gt; alternative hypothesis: two-sided</span></span>
<span></span>
<span><span class="co"># Mardia's test, need large sample size for power</span></span>
<span><span class="va">mardia_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/MVN/man/mvn.html">mvn</a></span><span class="op">(</span></span>
<span>        <span class="va">trees</span>,</span>
<span>        mvnTest <span class="op">=</span> <span class="st">"mardia"</span>,</span>
<span>        covariance <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>        multivariatePlot <span class="op">=</span> <span class="st">"qq"</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="va">mardia_test</span><span class="op">$</span><span class="va">multivariateNormality</span></span>
<span><span class="co">#&gt;              Test         Statistic            p value Result</span></span>
<span><span class="co">#&gt; 1 Mardia Skewness  29.7248528871795   0.72054426745778    YES</span></span>
<span><span class="co">#&gt; 2 Mardia Kurtosis -1.67743173185383 0.0934580886477281    YES</span></span>
<span><span class="co">#&gt; 3             MVN              &lt;NA&gt;               &lt;NA&gt;    YES</span></span>
<span></span>
<span><span class="co"># Doornik-Hansen's test </span></span>
<span><span class="va">dh_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/MVN/man/mvn.html">mvn</a></span><span class="op">(</span></span>
<span>        <span class="va">trees</span>,</span>
<span>        mvnTest <span class="op">=</span> <span class="st">"dh"</span>,</span>
<span>        covariance <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>        multivariatePlot <span class="op">=</span> <span class="st">"qq"</span></span>
<span>    <span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-1-2.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb369"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dh_test</span><span class="op">$</span><span class="va">multivariateNormality</span></span>
<span><span class="co">#&gt;             Test        E df      p value MVN</span></span>
<span><span class="co">#&gt; 1 Doornik-Hansen 161.9446 10 1.285352e-29  NO</span></span>
<span></span>
<span><span class="co"># Henze-Zirkler's test </span></span>
<span><span class="va">hz_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/MVN/man/mvn.html">mvn</a></span><span class="op">(</span></span>
<span>        <span class="va">trees</span>,</span>
<span>        mvnTest <span class="op">=</span> <span class="st">"hz"</span>,</span>
<span>        covariance <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>        multivariatePlot <span class="op">=</span> <span class="st">"qq"</span></span>
<span>    <span class="op">)</span></span>
<span><span class="va">hz_test</span><span class="op">$</span><span class="va">multivariateNormality</span></span>
<span><span class="co">#&gt;            Test        HZ   p value MVN</span></span>
<span><span class="co">#&gt; 1 Henze-Zirkler 0.7591525 0.6398905 YES</span></span>
<span><span class="co"># The last column indicates whether dataset follows a multivariate normality or not (i.e, YES or NO) at significance level 0.05.</span></span>
<span></span>
<span><span class="co"># Royston's test</span></span>
<span><span class="co"># can only apply for 3 &lt; obs &lt; 5000 (because of Shapiro-Wilk's test)</span></span>
<span><span class="va">royston_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/MVN/man/mvn.html">mvn</a></span><span class="op">(</span></span>
<span>        <span class="va">trees</span>,</span>
<span>        mvnTest <span class="op">=</span> <span class="st">"royston"</span>,</span>
<span>        covariance <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>        multivariatePlot <span class="op">=</span> <span class="st">"qq"</span></span>
<span>    <span class="op">)</span></span>
<span><span class="va">royston_test</span><span class="op">$</span><span class="va">multivariateNormality</span></span>
<span><span class="co">#&gt;      Test        H    p value MVN</span></span>
<span><span class="co">#&gt; 1 Royston 9.064631 0.08199215 YES</span></span>
<span></span>
<span></span>
<span><span class="co"># E-statistic</span></span>
<span><span class="va">estat_test</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/MVN/man/mvn.html">mvn</a></span><span class="op">(</span></span>
<span>        <span class="va">trees</span>,</span>
<span>        mvnTest <span class="op">=</span> <span class="st">"energy"</span>,</span>
<span>        covariance <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>        multivariatePlot <span class="op">=</span> <span class="st">"qq"</span></span>
<span>    <span class="op">)</span></span>
<span><span class="va">estat_test</span><span class="op">$</span><span class="va">multivariateNormality</span></span>
<span><span class="co">#&gt;          Test Statistic p value MVN</span></span>
<span><span class="co">#&gt; 1 E-statistic  1.091101   0.532 YES</span></span></code></pre></div>
</div>
<div id="mean-vector-inference" class="section level3" number="22.0.2">
<h3>
<span class="header-section-number">22.0.2</span> Mean Vector Inference<a class="anchor" aria-label="anchor" href="#mean-vector-inference"><i class="fas fa-link"></i></a>
</h3>
<p>In the univariate normal distribution, we test <span class="math inline">\(H_0: \mu =\mu_0\)</span> by using</p>
<p><span class="math display">\[
T = \frac{\bar{y}- \mu_0}{s/\sqrt{n}} \sim t_{n-1}
\]</span></p>
<p>under the null hypothesis. And reject the null if <span class="math inline">\(|T|\)</span> is large relative to <span class="math inline">\(t_{(1-\alpha/2,n-1)}\)</span> because it means that seeing a value as large as what we observed is rare if the null is true</p>
<p>Equivalently,</p>
<p><span class="math display">\[
T^2 = \frac{(\bar{y}- \mu_0)^2}{s^2/n} = n(\bar{y}- \mu_0)(s^2)^{-1}(\bar{y}- \mu_0) \sim f_{(1,n-1)}
\]</span></p>
<div id="natural-multivariate-generalization" class="section level4" number="22.0.2.1">
<h4>
<span class="header-section-number">22.0.2.1</span> <strong>Natural Multivariate Generalization</strong><a class="anchor" aria-label="anchor" href="#natural-multivariate-generalization"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \mathbf{\mu} = \mathbf{\mu}_0 \\
&amp;H_a: \mathbf{\mu} \neq \mathbf{\mu}_0
\end{aligned}
\]</span></p>
<p>Define <strong>Hotelling’s</strong> <span class="math inline">\(T^2\)</span> by</p>
<p><span class="math display">\[
T^2 = n(\bar{\mathbf{y}} - \mathbf{\mu}_0)'\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0)
\]</span></p>
<p>which can be viewed as a generalized distance between <span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{\mu}_0\)</span></p>
<p>Under the assumption of normality,</p>
<p><span class="math display">\[
F = \frac{n-p}{(n-1)p} T^2 \sim f_{(p,n-p)}
\]</span></p>
<p>and reject the null hypothesis when <span class="math inline">\(F &gt; f_{(1-\alpha, p, n-p)}\)</span></p>
<ul>
<li>
<p>The <span class="math inline">\(T^2\)</span> test is invariant to changes in measurement units.</p>
<ul>
<li>If <span class="math inline">\(\mathbf{z = Cy + d}\)</span> where <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(\mathbf{d}\)</span> do not depend on <span class="math inline">\(\mathbf{y}\)</span>, then <span class="math inline">\(T^2(\mathbf{z}) - T^2(\mathbf{y})\)</span>
</li>
</ul>
</li>
<li><p>The <span class="math inline">\(T^2\)</span> test can be derived as a <strong>likelihood ratio</strong> test of <span class="math inline">\(H_0: \mu = \mu_0\)</span></p></li>
</ul>
</div>
<div id="confidence-intervals" class="section level4" number="22.0.2.2">
<h4>
<span class="header-section-number">22.0.2.2</span> Confidence Intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"><i class="fas fa-link"></i></a>
</h4>
<div id="confidence-region" class="section level5" number="22.0.2.2.1">
<h5>
<span class="header-section-number">22.0.2.2.1</span> Confidence Region<a class="anchor" aria-label="anchor" href="#confidence-region"><i class="fas fa-link"></i></a>
</h5>
<p>An “exact” <span class="math inline">\(100(1-\alpha)\%\)</span> confidence region for <span class="math inline">\(\mathbf{\mu}\)</span> is the set of all vectors, <span class="math inline">\(\mathbf{v}\)</span>, which are “close enough” to the observed mean vector, <span class="math inline">\(\bar{\mathbf{y}}\)</span> to satisfy</p>
<p><span class="math display">\[
n(\bar{\mathbf{y}} - \mathbf{\mu}_0)'\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0) \le \frac{(n-1)p}{n-p} f_{(1-\alpha, p, n-p)}
\]</span></p>
<ul>
<li>
<span class="math inline">\(\mathbf{v}\)</span> are just the mean vectors that are not rejected by the <span class="math inline">\(T^2\)</span> test when <span class="math inline">\(\mathbf{\bar{y}}\)</span> is observed.</li>
</ul>
<p>In case that you have 2 parameters, the confidence region is a “hyper-ellipsoid”.</p>
<p>In this region, it consists of all <span class="math inline">\(\mathbf{\mu}_0\)</span> vectors for which the <span class="math inline">\(T^2\)</span> test would not reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span></p>
<p>Even though the confidence region better assesses the joint knowledge concerning plausible values of <span class="math inline">\(\mathbf{\mu}\)</span> , people typically include confidence statement about the individual component means. We’d like all of the separate confidence statements to hold <strong>simultaneously</strong> with a specified high probability. Simultaneous confidence intervals: intervals <strong>against</strong> any statement being incorrect</p>
<div id="simultaneous-confidence-statements" class="section level6" number="22.0.2.2.1.1">
<h6>
<span class="header-section-number">22.0.2.2.1.1</span> Simultaneous Confidence Statements<a class="anchor" aria-label="anchor" href="#simultaneous-confidence-statements"><i class="fas fa-link"></i></a>
</h6>
<ul>
<li>Intervals based on a rectangular confidence region by projecting the previous region onto the coordinate axes:</li>
</ul>
<p><span class="math display">\[
\bar{y}_{i} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{s_{ii}}{n}}
\]</span></p>
<p>for all <span class="math inline">\(i = 1,..,p\)</span></p>
<p>which implied confidence region is conservative; it has at least <span class="math inline">\(100(1- \alpha)\%\)</span></p>
<p>Generally, simultaneous <span class="math inline">\(100(1-\alpha) \%\)</span> confidence intervals for all linear combinations , <span class="math inline">\(\mathbf{a}\)</span> of the elements of the mean vector are given by</p>
<p><span class="math display">\[
\mathbf{a'\bar{y}} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{\mathbf{a'Sa}}{n}}
\]</span></p>
<ul>
<li><p>works for any arbitrary linear combination <span class="math inline">\(\mathbf{a'\mu} = a_1 \mu_1 + ... + a_p \mu_p\)</span>, which is a projection onto the axis in the direction of <span class="math inline">\(\mathbf{a}\)</span></p></li>
<li><p>These intervals have the property that the probability that at least one such interval does not contain the appropriate <span class="math inline">\(\mathbf{a' \mu}\)</span> is no more than <span class="math inline">\(\alpha\)</span></p></li>
<li><p>These types of intervals can be used for “data snooping” (like <span class="math display">\[Scheffe\]</span>)</p></li>
</ul>
</div>
<div id="one-mu-at-a-time" class="section level6" number="22.0.2.2.1.2">
<h6>
<span class="header-section-number">22.0.2.2.1.2</span> One <span class="math inline">\(\mu\)</span> at a time<a class="anchor" aria-label="anchor" href="#one-mu-at-a-time"><i class="fas fa-link"></i></a>
</h6>
<ul>
<li>One at a time confidence intervals:</li>
</ul>
<p><span class="math display">\[
\bar{y}_i \pm t_{(1 - \alpha/2, n-1} \sqrt{\frac{s_{ii}}{n}}
\]</span></p>
<ul>
<li><p>Each of these intervals has a probability of <span class="math inline">\(1-\alpha\)</span> of covering the appropriate <span class="math inline">\(\mu_i\)</span></p></li>
<li><p>But they ignore the covariance structure of the <span class="math inline">\(p\)</span> variables</p></li>
<li><p>If we only care about <span class="math inline">\(k\)</span> simultaneous intervals, we can use “one at a time” method with the <span class="math display">\[Bonferroni\]</span> correction.</p></li>
<li><p>This method gets more conservative as the number of intervals <span class="math inline">\(k\)</span> increases.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="general-hypothesis-testing" class="section level3" number="22.0.3">
<h3>
<span class="header-section-number">22.0.3</span> General Hypothesis Testing<a class="anchor" aria-label="anchor" href="#general-hypothesis-testing"><i class="fas fa-link"></i></a>
</h3>
<div id="one-sample-tests" class="section level4" number="22.0.3.1">
<h4>
<span class="header-section-number">22.0.3.1</span> One-sample Tests<a class="anchor" aria-label="anchor" href="#one-sample-tests"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[
H_0: \mathbf{C \mu= 0}
\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(c \times p\)</span> matrix of rank c where <span class="math inline">\(c \le p\)</span>
</li>
</ul>
<p>We can test this hypothesis using the following statistic</p>
<p><span class="math display">\[
F = \frac{n - c}{(n-1)c} T^2
\]</span></p>
<p>where <span class="math inline">\(T^2 = n(\mathbf{C\bar{y}})' (\mathbf{CSC'})^{-1} (\mathbf{C\bar{y}})\)</span></p>
<p>Example:</p>
<p><span class="math display">\[
H_0: \mu_1 = \mu_2 = ... = \mu_p
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\begin{aligned}
\mu_1 - \mu_2 &amp;= 0 \\
&amp;\vdots \\
\mu_{p-1} - \mu_p &amp;= 0
\end{aligned}
\]</span></p>
<p>a total of <span class="math inline">\(p-1\)</span> tests. Hence, we have <span class="math inline">\(\mathbf{C}\)</span> as the <span class="math inline">\(p - 1 \times p\)</span> matrix</p>
<p><span class="math display">\[
\mathbf{C} =
\left(
\begin{array}
{ccccc}
1 &amp; -1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; -1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; 1 &amp; -1
\end{array}
\right)
\]</span></p>
<p>number of rows = <span class="math inline">\(c = p -1\)</span></p>
<p>Equivalently, we can also compare all of the other means to the first mean. Then, we test <span class="math inline">\(\mu_1 - \mu_2 = 0, \mu_1 - \mu_3 = 0,..., \mu_1 - \mu_p = 0\)</span>, the <span class="math inline">\((p-1) \times p\)</span> matrix <span class="math inline">\(\mathbf{C}\)</span> is</p>
<p><span class="math display">\[
\mathbf{C} =
\left(
\begin{array}
{ccccc}
-1 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
-1 &amp; 0 &amp; 1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-1 &amp; 0 &amp; \ldots &amp; 0 &amp; 1
\end{array}
\right)
\]</span></p>
<p>The value of <span class="math inline">\(T^2\)</span> is invariant to these equivalent choices of <span class="math inline">\(\mathbf{C}\)</span></p>
<p>This is often used for <strong>repeated measures designs</strong>, where each subject receives each treatment once over successive periods of time (all treatments are administered to each unit).</p>
<p>Example:</p>
<p>Let <span class="math inline">\(y_{ij}\)</span> be the response from subject i at time j for <span class="math inline">\(i = 1,..,n, j = 1,...,T\)</span>. In this case, <span class="math inline">\(\mathbf{y}_i = (y_{i1}, ..., y_{iT})', i = 1,...,n\)</span> are a random sample from <span class="math inline">\(N_T (\mathbf{\mu}, \mathbf{\Sigma})\)</span></p>
<p>Let <span class="math inline">\(n=8\)</span> subjects, <span class="math inline">\(T = 6\)</span>. We are interested in <span class="math inline">\(\mu_1, .., \mu_6\)</span></p>
<p><span class="math display">\[
H_0: \mu_1 = \mu_2 = ... = \mu_6
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\begin{aligned}
\mu_1 - \mu_2 &amp;= 0 \\
\mu_2 - \mu_3 &amp;= 0 \\
&amp;... \\
\mu_5  - \mu_6 &amp;= 0
\end{aligned}
\]</span></p>
<p>We can test orthogonal polynomials for 4 equally spaced time points. To test for example the null hypothesis that quadratic and cubic effects are jointly equal to 0, we would define <span class="math inline">\(\mathbf{C}\)</span></p>
<p><span class="math display">\[
\mathbf{C} =
\left(
\begin{array}
{cccc}
1 &amp; -1 &amp; -1 &amp; 1 \\
-1 &amp; 3 &amp; -3 &amp; 1
\end{array}
\right)
\]</span></p>
</div>
<div id="two-sample-tests" class="section level4" number="22.0.3.2">
<h4>
<span class="header-section-number">22.0.3.2</span> Two-Sample Tests<a class="anchor" aria-label="anchor" href="#two-sample-tests"><i class="fas fa-link"></i></a>
</h4>
<p>Consider the analogous two sample multivariate tests.</p>
<p>Example: we have data on two independent random samples, one sample from each of two populations</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y}_{1i} &amp;\sim N_p (\mathbf{\mu_1, \Sigma}) \\
\mathbf{y}_{2j} &amp;\sim N_p (\mathbf{\mu_2, \Sigma})
\end{aligned}
\]</span></p>
<p>We <strong>assume</strong></p>
<ul>
<li><p>normality</p></li>
<li><p>equal variance-covariance matrices</p></li>
<li><p>independent random samples</p></li>
</ul>
<p>We can summarize our data using the <strong>sufficient statistics</strong> <span class="math inline">\(\mathbf{\bar{y}}_1, \mathbf{S}_1, \mathbf{\bar{y}}_2, \mathbf{S}_2\)</span> with respective sample sizes, <span class="math inline">\(n_1,n_2\)</span></p>
<p>Since we assume that <span class="math inline">\(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}\)</span>, compute a pooled estimate of the variance-covariance matrix on <span class="math inline">\(n_1 + n_2 - 2\)</span> df</p>
<p><span class="math display">\[
\mathbf{S} = \frac{(n_1 - 1)\mathbf{S}_1 + (n_2-1) \mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \mathbf{\mu}_1 = \mathbf{\mu}_2 \\
&amp;H_a: \mathbf{\mu}_1 \neq \mathbf{\mu}_2
\end{aligned}
\]</span></p>
<p>At least one element of the mean vectors is different</p>
<p>We use</p>
<ul>
<li><p><span class="math inline">\(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2\)</span> to estimate <span class="math inline">\(\mu_1 - \mu_2\)</span></p></li>
<li>
<p><span class="math inline">\(\mathbf{S}\)</span> to estimate <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<p>Note: because we assume the two populations are independent, there is no covariance</p>
<p><span class="math inline">\(cov(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) = var(\mathbf{\bar{y}}_1) + var(\mathbf{\bar{y}}_2) = \frac{\mathbf{\Sigma_1}}{n_1} + \frac{\mathbf{\Sigma_2}}{n_2} = \mathbf{\Sigma}(\frac{1}{n_1} + \frac{1}{n_2})\)</span></p>
</li>
</ul>
<p>Reject <span class="math inline">\(H_0\)</span> if</p>
<p><span class="math display">\[
\begin{aligned}
T^2 &amp;= (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'\{ \mathbf{S} (\frac{1}{n_1} + \frac{1}{n_2})\}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
&amp;= \frac{n_1 n_2}{n_1 +n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'\{ \mathbf{S} \}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
&amp; \ge \frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \alpha,n_1 + n_2 - p -1)}
\end{aligned}
\]</span></p>
<p>or equivalently, if</p>
<p><span class="math display">\[
F = \frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \ge f_{(1- \alpha, p , n_1 + n_2 -p -1)}
\]</span></p>
<p>A <span class="math inline">\(100(1-\alpha) \%\)</span> confidence region for <span class="math inline">\(\mu_1 - \mu_2\)</span> consists of all vector <span class="math inline">\(\delta\)</span> which satisfy</p>
<p><span class="math display">\[
\frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta})' \mathbf{S}^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta}) \le \frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\alpha, p , n_1 + n_2 - p -1)}
\]</span></p>
<p>The simultaneous confidence intervals for all linear combinations of <span class="math inline">\(\mu_1 - \mu_2\)</span> have the form</p>
<p><span class="math display">\[
\mathbf{a'}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \pm \sqrt{\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\alpha, p, n_1 + n_2 -p -1)} \times \sqrt{\mathbf{a'Sa}(\frac{1}{n_1} + \frac{1}{n_2})}
\]</span></p>
<p>Bonferroni intervals, for k combinations</p>
<p><span class="math display">\[
(\bar{y}_{1i} - \bar{y}_{2i}) \pm t_{(1-\alpha/2k, n_1 + n_2 - 2)}\sqrt{(\frac{1}{n_1}  + \frac{1}{n_2})s_{ii}}
\]</span></p>
</div>
<div id="model-assumptions" class="section level4" number="22.0.3.3">
<h4>
<span class="header-section-number">22.0.3.3</span> Model Assumptions<a class="anchor" aria-label="anchor" href="#model-assumptions"><i class="fas fa-link"></i></a>
</h4>
<p>If model assumption are not met</p>
<ul>
<li>
<p>Unequal Covariance Matrices</p>
<ul>
<li><p>If <span class="math inline">\(n_1 = n_2\)</span> (large samples) there is little effect on the Type I error rate and power fo the two sample test</p></li>
<li><p>If <span class="math inline">\(n_1 &gt; n_2\)</span> and the eigenvalues of <span class="math inline">\(\mathbf{\Sigma}_1 \mathbf{\Sigma}^{-1}_2\)</span> are less than 1, the Type I error level is inflated</p></li>
<li><p>If <span class="math inline">\(n_1 &gt; n_2\)</span> and some eigenvalues of <span class="math inline">\(\mathbf{\Sigma}_1 \mathbf{\Sigma}_2^{-1}\)</span> are greater than 1, the Type I error rate is too small, leading to a reduction in power</p></li>
</ul>
</li>
<li>
<p>Sample Not Normal</p>
<ul>
<li><p>Type I error level of the two sample <span class="math inline">\(T^2\)</span> test isn’t much affect by moderate departures from normality if the two populations being sampled have similar distributions</p></li>
<li><p>One sample <span class="math inline">\(T^2\)</span> test is much more sensitive to lack of normality, especially when the distribution is skewed.</p></li>
<li><p>Intuitively, you can think that in one sample your distribution will be sensitive, but the distribution of the difference between two similar distributions will not be as sensitive.</p></li>
<li>
<p>Solutions:</p>
<ul>
<li><p>Transform to make the data more normal</p></li>
<li>
<p>Large large samples, use the <span class="math inline">\(\chi^2\)</span> (Wald) test, in which populations don’t need to be normal, or equal sample sizes, or equal variance-covariance matrices</p>
<ul>
<li>
<span class="math inline">\(H_0: \mu_1 - \mu_2 =0\)</span> use <span class="math inline">\((\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'( \frac{1}{n_1} \mathbf{S}_1 + \frac{1}{n_2}\mathbf{S}_2)^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \dot{\sim} \chi^2_{(p)}\)</span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div id="equal-covariance-matrices-tests" class="section level5" number="22.0.3.3.1">
<h5>
<span class="header-section-number">22.0.3.3.1</span> Equal Covariance Matrices Tests<a class="anchor" aria-label="anchor" href="#equal-covariance-matrices-tests"><i class="fas fa-link"></i></a>
</h5>
<p>With independent random samples from k populations of <span class="math inline">\(p\)</span>-dimensional vectors. We compute the sample covariance matrix for each, <span class="math inline">\(\mathbf{S}_i\)</span>, where <span class="math inline">\(i = 1,...,k\)</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \ldots = \mathbf{\Sigma}_k = \mathbf{\Sigma} \\
&amp;H_a: \text{at least 2 are different}
\end{aligned}
\]</span></p>
<p>Assume <span class="math inline">\(H_0\)</span> is true, we would use a pooled estimate of the common covariance matrix, <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<p><span class="math display">\[
\mathbf{S} = \frac{\sum_{i=1}^k (n_i -1)\mathbf{S}_i}{\sum_{i=1}^k (n_i - 1)}
\]</span></p>
<p>with <span class="math inline">\(\sum_{i=1}^k (n_i -1)\)</span></p>
<div id="bartletts-test" class="section level6" number="22.0.3.3.1.1">
<h6>
<span class="header-section-number">22.0.3.3.1.1</span> Bartlett’s Test<a class="anchor" aria-label="anchor" href="#bartletts-test"><i class="fas fa-link"></i></a>
</h6>
<p>(a modification of the likelihood ratio test). Define</p>
<p><span class="math display">\[
N = \sum_{i=1}^k n_i
\]</span></p>
<p>and (note: <span class="math inline">\(| |\)</span> are determinants here, not absolute value)</p>
<p><span class="math display">\[
M = (N - k) \log|\mathbf{S}| - \sum_{i=1}^k (n_i - 1)  \log|\mathbf{S}_i|
\]</span></p>
<p><span class="math display">\[
C^{-1} = 1 - \frac{2p^2 + 3p - 1}{6(p+1)(k-1)} \{\sum_{i=1}^k (\frac{1}{n_i - 1}) - \frac{1}{N-k} \}
\]</span></p>
<ul>
<li><p>Reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(MC^{-1} &gt; \chi^2_{1- \alpha, (k-1)p(p+1)/2}\)</span></p></li>
<li><p>If not all samples are from normal populations, <span class="math inline">\(MC^{-1}\)</span> has a distribution which is often shifted to the right of the nominal <span class="math inline">\(\chi^2\)</span> distribution, which means <span class="math inline">\(H_0\)</span> is often rejected even when it is true (the Type I error level is inflated). Hence, it is better to test individual normality first, or then multivariate normality before you do Bartlett’s test.</p></li>
</ul>
</div>
</div>
</div>
<div id="two-sample-repeated-measurements" class="section level4" number="22.0.3.4">
<h4>
<span class="header-section-number">22.0.3.4</span> Two-Sample Repeated Measurements<a class="anchor" aria-label="anchor" href="#two-sample-repeated-measurements"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p>Define <span class="math inline">\(\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\)</span> to be the observations from the i-th subject in the h-th group for times 1 through T</p></li>
<li><p>Assume that <span class="math inline">\(\mathbf{y}_{11}, ..., \mathbf{y}_{1n_1}\)</span> are iid <span class="math inline">\(N_t(\mathbf{\mu}_1, \mathbf{\Sigma})\)</span> and that <span class="math inline">\(\mathbf{y}_{21},...,\mathbf{y}_{2n_2}\)</span> are iid <span class="math inline">\(N_t(\mathbf{\mu}_2, \mathbf{\Sigma})\)</span></p></li>
<li><p><span class="math inline">\(H_0: \mathbf{C}(\mathbf{\mu}_1 - \mathbf{\mu}_2) = \mathbf{0}_c\)</span> where <span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(c \times t\)</span> matrix of rank <span class="math inline">\(c\)</span> where <span class="math inline">\(c \le t\)</span></p></li>
<li><p>The test statistic has the form</p></li>
</ul>
<p><span class="math display">\[
T^2 = \frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)' \mathbf{C}'(\mathbf{CSC}')^{-1}\mathbf{C} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{S}\)</span> is the pooled covariance estimate. Then,</p>
<p><span class="math display">\[
F = \frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \sim f_{(c, n_1 + n_2 - c-1)}
\]</span></p>
<p>when <span class="math inline">\(H_0\)</span> is true</p>
<p>If the null hypothesis <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span> is rejected. A weaker hypothesis is that the profiles for the two groups are parallel.</p>
<p><span class="math display">\[
\begin{aligned}
\mu_{11} - \mu_{21} &amp;= \mu_{12} - \mu_{22} \\
&amp;\vdots \\
\mu_{1t-1} - \mu_{2t-1} &amp;= \mu_{1t} - \mu_{2t}
\end{aligned}
\]</span></p>
<p>The null hypothesis matrix term is then</p>
<p><span class="math inline">\(H_0: \mathbf{C}(\mu_1 - \mu_2) = \mathbf{0}_c\)</span> , where <span class="math inline">\(c = t - 1\)</span> and</p>
<p><span class="math display">\[
\mathbf{C} =
\left(
\begin{array}
{ccccc}
1 &amp; -1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; -1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \ldots &amp; -1
\end{array}
\right)_{(t-1) \times t}
\]</span></p>
<div class="sourceCode" id="cb370"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># One-sample Hotelling's T^2 test</span></span>
<span><span class="co">#  Create data frame</span></span>
<span><span class="va">plants</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    y1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2.11</span>, <span class="fl">2.36</span>, <span class="fl">2.13</span>, <span class="fl">2.78</span>, <span class="fl">2.17</span><span class="op">)</span>,</span>
<span>    y2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10.1</span>, <span class="fl">35.0</span>, <span class="fl">2.0</span>, <span class="fl">6.0</span>, <span class="fl">2.0</span><span class="op">)</span>,</span>
<span>    y3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3.4</span>, <span class="fl">4.1</span>, <span class="fl">1.9</span>, <span class="fl">3.8</span>, <span class="fl">1.7</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Center the data with </span></span>
<span><span class="co"># the hypothesized means and make a matrix</span></span>
<span><span class="va">plants_ctr</span> <span class="op">&lt;-</span> <span class="va">plants</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/transmute.html">transmute</a></span><span class="op">(</span>y1_ctr <span class="op">=</span> <span class="va">y1</span> <span class="op">-</span> <span class="fl">2.85</span>,</span>
<span>              y2_ctr <span class="op">=</span> <span class="va">y2</span> <span class="op">-</span> <span class="fl">15.0</span>,</span>
<span>              y3_ctr <span class="op">=</span> <span class="va">y3</span> <span class="op">-</span> <span class="fl">6.0</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Use anova.mlm to calculate Wilks' lambda</span></span>
<span><span class="va">onesamp_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">plants_ctr</span> <span class="op">~</span> <span class="fl">1</span><span class="op">)</span>, test <span class="op">=</span> <span class="st">"Wilks"</span><span class="op">)</span></span>
<span><span class="va">onesamp_fit</span></span>
<span><span class="co">#&gt; Analysis of Variance Table</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;             Df    Wilks approx F num Df den Df  Pr(&gt;F)  </span></span>
<span><span class="co">#&gt; (Intercept)  1 0.054219   11.629      3      2 0.08022 .</span></span>
<span><span class="co">#&gt; Residuals    4                                          </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>can’t reject the null of hypothesized vector of means</p>
<div class="sourceCode" id="cb371"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Paired-Sample Hotelling's T^2 test</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">ICSNP</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#  Create data frame</span></span>
<span><span class="va">waste</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    case <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">11</span>,</span>
<span>    com_y1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">6</span>, <span class="fl">6</span>, <span class="fl">18</span>, <span class="fl">8</span>, <span class="fl">11</span>, <span class="fl">34</span>, <span class="fl">28</span>, <span class="fl">71</span>, <span class="fl">43</span>, <span class="fl">33</span>, <span class="fl">20</span><span class="op">)</span>,</span>
<span>    com_y2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">27</span>, <span class="fl">23</span>, <span class="fl">64</span>, <span class="fl">44</span>, <span class="fl">30</span>, <span class="fl">75</span>, <span class="fl">26</span>, <span class="fl">124</span>, <span class="fl">54</span>, <span class="fl">30</span>, <span class="fl">14</span><span class="op">)</span>,</span>
<span>    state_y1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">25</span>, <span class="fl">28</span>, <span class="fl">36</span>, <span class="fl">35</span>, <span class="fl">15</span>, <span class="fl">44</span>, <span class="fl">42</span>, <span class="fl">54</span>, <span class="fl">34</span>, <span class="fl">29</span>, <span class="fl">39</span><span class="op">)</span>,</span>
<span>    state_y2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">15</span>, <span class="fl">13</span>, <span class="fl">22</span>, <span class="fl">29</span>, <span class="fl">31</span>, <span class="fl">64</span>, <span class="fl">30</span>, <span class="fl">64</span>, <span class="fl">56</span>, <span class="fl">20</span>, <span class="fl">21</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate the difference between commercial and state labs</span></span>
<span><span class="va">waste_diff</span> <span class="op">&lt;-</span> <span class="va">waste</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/transmute.html">transmute</a></span><span class="op">(</span>y1_diff <span class="op">=</span> <span class="va">com_y1</span> <span class="op">-</span> <span class="va">state_y1</span>,</span>
<span>              y2_diff <span class="op">=</span> <span class="va">com_y2</span> <span class="op">-</span> <span class="va">state_y2</span><span class="op">)</span></span>
<span><span class="co"># Run the test</span></span>
<span><span class="va">paired_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ICSNP/man/HotellingsT.html">HotellingsT2</a></span><span class="op">(</span><span class="va">waste_diff</span><span class="op">)</span></span>
<span><span class="co"># value T.2 in the output corresponds to </span></span>
<span><span class="co"># the approximate F-value in the output from anova.mlm</span></span>
<span><span class="va">paired_fit</span> </span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Hotelling's one sample T2-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  waste_diff</span></span>
<span><span class="co">#&gt; T.2 = 6.1377, df1 = 2, df2 = 9, p-value = 0.02083</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location is not equal to c(0,0)</span></span></code></pre></div>
<p>reject the null that the two labs’ measurements are equal</p>
<div class="sourceCode" id="cb372"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Independent-Sample Hotelling's T^2 test with Bartlett's test</span></span>
<span></span>
<span><span class="co"># Read in data</span></span>
<span><span class="va">steel</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/steel.dat"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">steel</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Temp"</span>, <span class="st">"Yield"</span>, <span class="st">"Strength"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">steel</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':    12 obs. of  3 variables:</span></span>
<span><span class="co">#&gt;  $ Temp    : int  1 1 1 1 1 2 2 2 2 2 ...</span></span>
<span><span class="co">#&gt;  $ Yield   : int  33 36 35 38 40 35 36 38 39 41 ...</span></span>
<span><span class="co">#&gt;  $ Strength: int  60 61 64 63 65 57 59 59 61 63 ...</span></span>
<span></span>
<span><span class="co"># Plot the data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">steel</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Yield</span>, y <span class="op">=</span> <span class="va">Strength</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html">geom_text</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">Temp</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_segment.html">geom_segment</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>        x <span class="op">=</span> <span class="fl">33</span>,</span>
<span>        y <span class="op">=</span> <span class="fl">57.5</span>,</span>
<span>        xend <span class="op">=</span> <span class="fl">42</span>,</span>
<span>        yend <span class="op">=</span> <span class="fl">65</span></span>
<span>    <span class="op">)</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-4-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb373"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span></span>
<span><span class="co"># Bartlett's test for equality of covariance matrices</span></span>
<span><span class="co"># same thing as Box's M test in the multivariate setting</span></span>
<span><span class="va">bart_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://friendly.github.io/heplots/reference/boxM.html">boxM</a></span><span class="op">(</span><span class="va">steel</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="va">steel</span><span class="op">$</span><span class="va">Temp</span><span class="op">)</span></span>
<span><span class="va">bart_test</span> <span class="co"># fail to reject the null of equal covariances </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Box's M-test for Homogeneity of Covariance Matrices</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  steel[, -1]</span></span>
<span><span class="co">#&gt; Chi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442</span></span>
<span></span>
<span><span class="co"># anova.mlm</span></span>
<span><span class="va">twosamp_fit</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">Yield</span>, <span class="va">Strength</span><span class="op">)</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">Temp</span><span class="op">)</span>, </span>
<span>             data <span class="op">=</span> <span class="va">steel</span><span class="op">)</span>, </span>
<span>          test <span class="op">=</span> <span class="st">"Wilks"</span><span class="op">)</span></span>
<span><span class="va">twosamp_fit</span></span>
<span><span class="co">#&gt; Analysis of Variance Table</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;              Df    Wilks approx F num Df den Df    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; (Intercept)   1 0.001177   3818.1      2      9 6.589e-14 ***</span></span>
<span><span class="co">#&gt; factor(Temp)  1 0.294883     10.8      2      9  0.004106 ** </span></span>
<span><span class="co">#&gt; Residuals    10                                              </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span></span>
<span><span class="co"># ICSNP package</span></span>
<span><span class="va">twosamp_fit2</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/ICSNP/man/HotellingsT.html">HotellingsT2</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">steel</span><span class="op">$</span><span class="va">Yield</span>, <span class="va">steel</span><span class="op">$</span><span class="va">Strength</span><span class="op">)</span> <span class="op">~</span> </span>
<span>                     <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">steel</span><span class="op">$</span><span class="va">Temp</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">twosamp_fit2</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Hotelling's two sample T2-test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  cbind(steel$Yield, steel$Strength) by factor(steel$Temp)</span></span>
<span><span class="co">#&gt; T.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106</span></span>
<span><span class="co">#&gt; alternative hypothesis: true location difference is not equal to c(0,0)</span></span></code></pre></div>
<p>reject null. Hence, there is a difference in the means of the bivariate normal distributions</p>
</div>
</div>
<div id="manova" class="section level2" number="22.1">
<h2>
<span class="header-section-number">22.1</span> MANOVA<a class="anchor" aria-label="anchor" href="#manova"><i class="fas fa-link"></i></a>
</h2>
<p>Multivariate Analysis of Variance</p>
<p>One-way MANOVA</p>
<p>Compare treatment means for h different populations</p>
<p>Population 1: <span class="math inline">\(\mathbf{y}_{11}, \mathbf{y}_{12}, \dots, \mathbf{y}_{1n_1} \sim idd N_p (\mathbf{\mu}_1, \mathbf{\Sigma})\)</span></p>
<p><span class="math inline">\(\vdots\)</span></p>
<p>Population h: <span class="math inline">\(\mathbf{y}_{h1}, \mathbf{y}_{h2}, \dots, \mathbf{y}_{hn_h} \sim idd N_p (\mathbf{\mu}_h, \mathbf{\Sigma})\)</span></p>
<p><strong>Assumptions</strong></p>
<ol style="list-style-type: decimal">
<li>Independent random samples from <span class="math inline">\(h\)</span> different populations</li>
<li>Common covariance matrices</li>
<li>Each population is multivariate <strong>normal</strong>
</li>
</ol>
<p>Calculate the summary statistics <span class="math inline">\(\mathbf{\bar{y}}_i, \mathbf{S}\)</span> and the pooled estimate of the covariance matrix <span class="math inline">\(\mathbf{S}\)</span></p>
<p>Similar to the univariate one-way ANVOA, we can use the effects model formulation <span class="math inline">\(\mathbf{\mu}_i = \mathbf{\mu} + \mathbf{\tau}_i\)</span>, where</p>
<ul>
<li><p><span class="math inline">\(\mathbf{\mu}_i\)</span> is the population mean for population i</p></li>
<li><p><span class="math inline">\(\mathbf{\mu}\)</span> is the overall mean effect</p></li>
<li><p><span class="math inline">\(\mathbf{\tau}_i\)</span> is the treatment effect of the i-th treatment.</p></li>
</ul>
<p>For the one-way model: <span class="math inline">\(\mathbf{y}_{ij} = \mu + \tau_i + \epsilon_{ij}\)</span> for <span class="math inline">\(i = 1,..,h; j = 1,..., n_i\)</span> and <span class="math inline">\(\epsilon_{ij} \sim N_p(\mathbf{0, \Sigma})\)</span></p>
<p>However, the above model is over-parameterized (i.e., infinite number of ways to define <span class="math inline">\(\mathbf{\mu}\)</span> and the <span class="math inline">\(\mathbf{\tau}_i\)</span>’s such that they add up to <span class="math inline">\(\mu_i\)</span>. Thus we can constrain by having</p>
<p><span class="math display">\[
\sum_{i=1}^h n_i \tau_i = 0
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\mathbf{\tau}_h = 0
\]</span></p>
<p>The observational equivalent of the effects model is</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y}_{ij} &amp;= \mathbf{\bar{y}} + (\mathbf{\bar{y}}_i - \mathbf{\bar{y}}) + (\mathbf{y}_{ij} - \mathbf{\bar{y}}_i) \\
&amp;= \text{overall sample mean} + \text{treatement effect} + \text{residual} \text{ (under univariate ANOVA)}
\end{aligned}
\]</span></p>
<p>After manipulation</p>
<p><span class="math display">\[
\sum_{i = 1}^h \sum_{j = 1}^{n_i} (\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})(\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})' = \sum_{i = 1}^h n_i (\mathbf{\bar{y}}_i - \mathbf{\bar{y}})(\mathbf{\bar{y}}_i - \mathbf{\bar{y}})' + \sum_{i=1}^h \sum_{j = 1}^{n_i} (\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})(\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}}_i)'
\]</span></p>
<p>LHS = Total corrected sums of squares and cross products (SSCP) matrix</p>
<p>RHS =</p>
<ul>
<li><p>1st term = Treatment (or between subjects) sum of squares and cross product matrix (denoted H;B)</p></li>
<li><p>2nd term = residual (or within subject) SSCP matrix denoted (E;W)</p></li>
</ul>
<p>Note:</p>
<p><span class="math display">\[
\mathbf{E} = (n_1 - 1)\mathbf{S}_1  + ... + (n_h -1) \mathbf{S}_h = (n-h) \mathbf{S}
\]</span></p>
<p>MANOVA table</p>
<div class="inline-table"><table class="table table-sm">
<caption>MONOVA table</caption>
<thead><tr class="header">
<th>Source</th>
<th>SSCP</th>
<th>df</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Treatment</td>
<td><span class="math inline">\(\mathbf{H}\)</span></td>
<td><span class="math inline">\(h -1\)</span></td>
</tr>
<tr class="even">
<td>Residual (error)</td>
<td><span class="math inline">\(\mathbf{E}\)</span></td>
<td><span class="math inline">\(\sum_{i= 1}^h n_i - h\)</span></td>
</tr>
<tr class="odd">
<td>Total Corrected</td>
<td><span class="math inline">\(\mathbf{H + E}\)</span></td>
<td><span class="math inline">\(\sum_{i=1}^h n_i -1\)</span></td>
</tr>
</tbody>
</table></div>
<p><span class="math display">\[
H_0: \tau_1 = \tau_2 = \dots = \tau_h = \mathbf{0}
\]</span></p>
<p>We consider the relative “sizes” of <span class="math inline">\(\mathbf{E}\)</span> and <span class="math inline">\(\mathbf{H+E}\)</span></p>
<p>Wilk’s Lambda</p>
<p>Define Wilk’s Lambda</p>
<p><span class="math display">\[
\Lambda^* = \frac{|\mathbf{E}|}{|\mathbf{H+E}|}
\]</span></p>
<p>Properties:</p>
<ol style="list-style-type: decimal">
<li><p>Wilk’s Lambda is equivalent to the F-statistic in the univariate case</p></li>
<li><p>The exact distribution of <span class="math inline">\(\Lambda^*\)</span> can be determined for especial cases.</p></li>
<li><p>For large sample sizes, reject <span class="math inline">\(H_0\)</span> if</p></li>
</ol>
<p><span class="math display">\[
-(\sum_{i=1}^h n_i - 1 - \frac{p+h}{2}) \log(\Lambda^*) &gt; \chi^2_{(1-\alpha, p(h-1))}
\]</span></p>
<div id="testing-general-hypotheses" class="section level3" number="22.1.1">
<h3>
<span class="header-section-number">22.1.1</span> Testing General Hypotheses<a class="anchor" aria-label="anchor" href="#testing-general-hypotheses"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p><span class="math inline">\(h\)</span> different treatments</p></li>
<li><p>with the i-th treatment</p></li>
<li><p>applied to <span class="math inline">\(n_i\)</span> subjects that</p></li>
<li><p>are observed for <span class="math inline">\(p\)</span> repeated measures.</p></li>
</ul>
<p>Consider this a <span class="math inline">\(p\)</span> dimensional obs on a random sample from each of <span class="math inline">\(h\)</span> different treatment populations.</p>
<p><span class="math display">\[
\mathbf{y}_{ij} = \mathbf{\mu} + \mathbf{\tau}_i + \mathbf{\epsilon}_{ij}
\]</span></p>
<p>for <span class="math inline">\(i = 1,..,h\)</span> and <span class="math inline">\(j = 1,..,n_i\)</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{XB} + \mathbf{\epsilon}
\]</span></p>
<p>where <span class="math inline">\(n = \sum_{i = 1}^h n_i\)</span> and with restriction <span class="math inline">\(\mathbf{\tau}_h = 0\)</span></p>
<p><span class="math display">\[
\mathbf{Y}_{(n \times p)} =
\left[
\begin{array}
{c}
\mathbf{y}_{11}' \\
\vdots \\
\mathbf{y}_{1n_1}' \\
\vdots \\
\mathbf{y}_{hn_h}'
\end{array}
\right],
\mathbf{B}_{(h \times p)} =
\left[
\begin{array}
{c}
\mathbf{\mu}' \\
\mathbf{\tau}_1' \\
\vdots \\
\mathbf{\tau}_{h-1}'
\end{array}
\right],
\mathbf{\epsilon}_{(n \times p)} =
\left[
\begin{array}
{c}
\epsilon_{11}' \\
\vdots \\
\epsilon_{1n_1}' \\
\vdots \\
\epsilon_{hn_h}'
\end{array}
\right]
\]</span></p>
<p><span class="math display">\[
\mathbf{X}_{(n \times h)} =
\left[
\begin{array}
{ccccc}
1 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\
1 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\
1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0
\end{array}
\right]
\]</span></p>
<p>Estimation</p>
<p><span class="math display">\[
\mathbf{\hat{B}} = (\mathbf{X'X})^{-1} \mathbf{X'Y}
\]</span></p>
<p>Rows of <span class="math inline">\(\mathbf{Y}\)</span> are independent (i.e., <span class="math inline">\(var(\mathbf{Y}) = \mathbf{I}_n \otimes \mathbf{\Sigma}\)</span> , an <span class="math inline">\(np \times np\)</span> matrix, where <span class="math inline">\(\otimes\)</span> is the Kronecker product).</p>
<p><span class="math display">\[
\begin{aligned}
&amp;H_0: \mathbf{LBM} = 0 \\
&amp;H_a: \mathbf{LBM} \neq 0
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\mathbf{L}\)</span> is a <span class="math inline">\(g \times h\)</span> matrix of full row rank (<span class="math inline">\(g \le h\)</span>) = comparisons across groups</p></li>
<li><p><span class="math inline">\(\mathbf{M}\)</span> is a <span class="math inline">\(p \times u\)</span> matrix of full column rank (<span class="math inline">\(u \le p\)</span>) = comparisons across traits</p></li>
</ul>
<p>The general treatment corrected sums of squares and cross product is</p>
<p><span class="math display">\[
\mathbf{H} = \mathbf{M'Y'X(X'X)^{-1}L'[L(X'X)^{-1}L']^{-1}L(X'X)^{-1}X'YM}
\]</span></p>
<p>or for the null hypothesis <span class="math inline">\(H_0: \mathbf{LBM} = \mathbf{D}\)</span></p>
<p><span class="math display">\[
\mathbf{H} = (\mathbf{\hat{LBM}} - \mathbf{D})'[\mathbf{X(X'X)^{-1}L}]^{-1}(\mathbf{\hat{LBM}} - \mathbf{D})
\]</span></p>
<p>The general matrix of residual sums of squares and cross product</p>
<p><span class="math display">\[
\mathbf{E} = \mathbf{M'Y'[I-X(X'X)^{-1}X']YM} = \mathbf{M'[Y'Y - \hat{B}'(X'X)^{-1}\hat{B}]M}
\]</span></p>
<p>We can compute the following statistic eigenvalues of <span class="math inline">\(\mathbf{HE}^{-1}\)</span></p>
<ul>
<li><p>Wilk’s Criterion: <span class="math inline">\(\Lambda^* = \frac{|\mathbf{E}|}{|\mathbf{H} + \mathbf{E}|}\)</span> . The df depend on the rank of <span class="math inline">\(\mathbf{L}, \mathbf{M}, \mathbf{X}\)</span></p></li>
<li><p>Lawley-Hotelling Trace: <span class="math inline">\(U = tr(\mathbf{HE}^{-1})\)</span></p></li>
<li><p>Pillai Trace: <span class="math inline">\(V = tr(\mathbf{H}(\mathbf{H}+ \mathbf{E}^{-1})\)</span></p></li>
<li><p>Roy’s Maximum Root: largest eigenvalue of <span class="math inline">\(\mathbf{HE}^{-1}\)</span></p></li>
</ul>
<p>If <span class="math inline">\(H_0\)</span> is true and n is large, <span class="math inline">\(-(n-1- \frac{p+h}{2})\ln \Lambda^* \sim \chi^2_{p(h-1)}\)</span>. Some special values of p and h can give exact F-dist under <span class="math inline">\(H_0\)</span></p>
<div class="sourceCode" id="cb374"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># One-way MANOVA</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rvlenth/emmeans">emmeans</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">profileR</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Read in the data</span></span>
<span><span class="va">gpagmat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/gpagmat.dat"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Change the variable names</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">gpagmat</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"y1"</span>, <span class="st">"y2"</span>, <span class="st">"admit"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Check the structure</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">gpagmat</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':    85 obs. of  3 variables:</span></span>
<span><span class="co">#&gt;  $ y1   : num  2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ...</span></span>
<span><span class="co">#&gt;  $ y2   : int  596 473 482 527 505 693 626 663 447 588 ...</span></span>
<span><span class="co">#&gt;  $ admit: int  1 1 1 1 1 1 1 1 1 1 ...</span></span>
<span></span>
<span></span>
<span><span class="co">## Plot the data</span></span>
<span><span class="va">gg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">gpagmat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">y1</span>, y <span class="op">=</span> <span class="va">y2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html">geom_text</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">admit</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="va">admit</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_colour_discrete.html">scale_color_discrete</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Admission"</span>,</span>
<span>                         labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Admit"</span>, <span class="st">"Do not admit"</span>, <span class="st">"Borderline"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_continuous</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"GPA"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_y_continuous</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"GMAT"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Fit one-way MANOVA</span></span>
<span><span class="va">oneway_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/manova.html">manova</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">y1</span>, <span class="va">y2</span><span class="op">)</span> <span class="op">~</span> <span class="va">admit</span>, data <span class="op">=</span> <span class="va">gpagmat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">oneway_fit</span>, test <span class="op">=</span> <span class="st">"Wilks"</span><span class="op">)</span></span>
<span><span class="co">#&gt;           Df  Wilks approx F num Df den Df    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; admit      1 0.6126   25.927      2     82 1.881e-09 ***</span></span>
<span><span class="co">#&gt; Residuals 83                                            </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>reject the null of equal multivariate mean vectors between the three admmission groups</p>
<div class="sourceCode" id="cb375"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Repeated Measures MANOVA</span></span>
<span></span>
<span></span>
<span><span class="co">## Create data frame</span></span>
<span><span class="va">stress</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>    subject <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">8</span>,</span>
<span>    begin <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">5</span>, <span class="fl">6</span>, <span class="fl">1</span>, <span class="fl">5</span>, <span class="fl">1</span>, <span class="fl">5</span><span class="op">)</span>,</span>
<span>    middle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">7</span>, <span class="fl">4</span>, <span class="fl">7</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>    final <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">6</span>, <span class="fl">7</span>, <span class="fl">4</span>, <span class="fl">7</span>, <span class="fl">6</span>, <span class="fl">7</span>, <span class="fl">3</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<ul>
<li>If independent = time with 3 levels -&gt; univariate ANOVA (require sphericity assumption (i.e., the variances for all differences are equal))</li>
<li>If each level of independent time as a separate variable -&gt; MANOVA (does not require sphericity assumption)</li>
</ul>
<div class="sourceCode" id="cb376"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## MANOVA</span></span>
<span><span class="va">stress_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">begin</span>, <span class="va">middle</span>, <span class="va">final</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">stress</span><span class="op">)</span></span>
<span><span class="va">idata</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>time <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"begin"</span>, <span class="st">"middle"</span>, <span class="st">"final"</span><span class="op">)</span>,</span>
<span>        levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"begin"</span>, <span class="st">"middle"</span>, <span class="st">"final"</span><span class="op">)</span></span>
<span>    <span class="op">)</span><span class="op">)</span></span>
<span><span class="va">repeat_fit</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html">Anova</a></span><span class="op">(</span></span>
<span>        <span class="va">stress_mod</span>,</span>
<span>        idata <span class="op">=</span> <span class="va">idata</span>,</span>
<span>        idesign <span class="op">=</span> <span class="op">~</span> <span class="va">time</span>,</span>
<span>        icontrasts <span class="op">=</span> <span class="st">"contr.poly"</span></span>
<span>    <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">repeat_fit</span><span class="op">)</span> </span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Type III Repeated Measures MANOVA Tests:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ------------------------------------------</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Term: (Intercept) </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Response transformation matrix:</span></span>
<span><span class="co">#&gt;        (Intercept)</span></span>
<span><span class="co">#&gt; begin            1</span></span>
<span><span class="co">#&gt; middle           1</span></span>
<span><span class="co">#&gt; final            1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;             (Intercept)</span></span>
<span><span class="co">#&gt; (Intercept)        1352</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: (Intercept)</span></span>
<span><span class="co">#&gt;                  Df test stat approx F num Df den Df     Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; Pillai            1  0.896552 60.66667      1      7 0.00010808 ***</span></span>
<span><span class="co">#&gt; Wilks             1  0.103448 60.66667      1      7 0.00010808 ***</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  1  8.666667 60.66667      1      7 0.00010808 ***</span></span>
<span><span class="co">#&gt; Roy               1  8.666667 60.66667      1      7 0.00010808 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ------------------------------------------</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Term: time </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Response transformation matrix:</span></span>
<span><span class="co">#&gt;               time.L     time.Q</span></span>
<span><span class="co">#&gt; begin  -7.071068e-01  0.4082483</span></span>
<span><span class="co">#&gt; middle -7.850462e-17 -0.8164966</span></span>
<span><span class="co">#&gt; final   7.071068e-01  0.4082483</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;           time.L   time.Q</span></span>
<span><span class="co">#&gt; time.L 18.062500 6.747781</span></span>
<span><span class="co">#&gt; time.Q  6.747781 2.520833</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: time</span></span>
<span><span class="co">#&gt;                  Df test stat approx F num Df den Df   Pr(&gt;F)  </span></span>
<span><span class="co">#&gt; Pillai            1 0.7080717 7.276498      2      6 0.024879 *</span></span>
<span><span class="co">#&gt; Wilks             1 0.2919283 7.276498      2      6 0.024879 *</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  1 2.4254992 7.276498      2      6 0.024879 *</span></span>
<span><span class="co">#&gt; Roy               1 2.4254992 7.276498      2      6 0.024879 *</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Univariate Type III Repeated-Measures ANOVA Assuming Sphericity</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;             Sum Sq num Df Error SS den Df F value    Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; (Intercept) 450.67      1    52.00      7 60.6667 0.0001081 ***</span></span>
<span><span class="co">#&gt; time         20.58      2    24.75     14  5.8215 0.0144578 *  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mauchly Tests for Sphericity</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      Test statistic p-value</span></span>
<span><span class="co">#&gt; time         0.7085 0.35565</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Greenhouse-Geisser and Huynh-Feldt Corrections</span></span>
<span><span class="co">#&gt;  for Departure from Sphericity</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;       GG eps Pr(&gt;F[GG])  </span></span>
<span><span class="co">#&gt; time 0.77429    0.02439 *</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;         HF eps Pr(&gt;F[HF])</span></span>
<span><span class="co">#&gt; time 0.9528433 0.01611634</span></span></code></pre></div>
<p>can’t reject the null hypothesis of sphericity, hence univariate ANOVA is also appropriate.We also see linear significant time effect, but no quadratic time effect</p>
<div class="sourceCode" id="cb377"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Polynomial contrasts</span></span>
<span><span class="co"># What is the reference for the marginal means?</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/emmeans/man/ref_grid.html">ref_grid</a></span><span class="op">(</span><span class="va">stress_mod</span>, mult.name <span class="op">=</span> <span class="st">"time"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'emmGrid' object with variables:</span></span>
<span><span class="co">#&gt;     1 = 1</span></span>
<span><span class="co">#&gt;     time = multivariate response levels: begin, middle, final</span></span>
<span></span>
<span><span class="co"># marginal means for the levels of time</span></span>
<span><span class="va">contr_means</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/emmeans/man/emmeans.html">emmeans</a></span><span class="op">(</span><span class="va">stress_mod</span>, <span class="op">~</span> <span class="va">time</span>, mult.name <span class="op">=</span> <span class="st">"time"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/emmeans/man/contrast.html">contrast</a></span><span class="op">(</span><span class="va">contr_means</span>, method <span class="op">=</span> <span class="st">"poly"</span><span class="op">)</span></span>
<span><span class="co">#&gt;  contrast  estimate    SE df t.ratio p.value</span></span>
<span><span class="co">#&gt;  linear        2.12 0.766  7   2.773  0.0276</span></span>
<span><span class="co">#&gt;  quadratic     1.38 0.944  7   1.457  0.1885</span></span></code></pre></div>
<div class="sourceCode" id="cb378"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># MANOVA</span></span>
<span></span>
<span></span>
<span><span class="co">## Read in Data</span></span>
<span><span class="va">heart</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/heart.dat"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">heart</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"drug"</span>, <span class="st">"y1"</span>, <span class="st">"y2"</span>, <span class="st">"y3"</span>, <span class="st">"y4"</span><span class="op">)</span></span>
<span><span class="co">## Create a subject ID nested within drug</span></span>
<span><span class="va">heart</span> <span class="op">&lt;-</span> <span class="va">heart</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">drug</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>subject <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/row_number.html">row_number</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">ungroup</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">heart</span><span class="op">)</span></span>
<span><span class="co">#&gt; tibble [24 × 6] (S3: tbl_df/tbl/data.frame)</span></span>
<span><span class="co">#&gt;  $ drug   : chr [1:24] "ax23" "ax23" "ax23" "ax23" ...</span></span>
<span><span class="co">#&gt;  $ y1     : int [1:24] 72 78 71 72 66 74 62 69 85 82 ...</span></span>
<span><span class="co">#&gt;  $ y2     : int [1:24] 86 83 82 83 79 83 73 75 86 86 ...</span></span>
<span><span class="co">#&gt;  $ y3     : int [1:24] 81 88 81 83 77 84 78 76 83 80 ...</span></span>
<span><span class="co">#&gt;  $ y4     : int [1:24] 77 82 75 69 66 77 70 70 80 84 ...</span></span>
<span><span class="co">#&gt;  $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ...</span></span>
<span></span>
<span><span class="co">## Create means summary for profile plot,</span></span>
<span><span class="co"># pivot longer for plotting with ggplot</span></span>
<span><span class="va">heart_means</span> <span class="op">&lt;-</span> <span class="va">heart</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">drug</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise_all.html">summarize_at</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/vars.html">vars</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/starts_with.html">starts_with</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span><span class="op">)</span>, <span class="va">mean</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">ungroup</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="op">-</span><span class="va">drug</span>, names_to <span class="op">=</span> <span class="st">"time"</span>, values_to <span class="op">=</span> <span class="st">"mean"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>time <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">time</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">gg_profile</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">heart_means</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">time</span>, y <span class="op">=</span> <span class="va">mean</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>col <span class="op">=</span> <span class="va">drug</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>col <span class="op">=</span> <span class="va">drug</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"Profile Plot"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_y_continuous</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Response"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_discrete.html">scale_x_discrete</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Time"</span><span class="op">)</span></span>
<span><span class="va">gg_profile</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb379"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Fit model</span></span>
<span><span class="va">heart_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">y1</span>, <span class="va">y2</span>, <span class="va">y3</span>, <span class="va">y4</span><span class="op">)</span> <span class="op">~</span> <span class="va">drug</span>, data <span class="op">=</span> <span class="va">heart</span><span class="op">)</span></span>
<span><span class="va">man_fit</span> <span class="op">&lt;-</span> <span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html">Anova</a></span><span class="op">(</span><span class="va">heart_mod</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">man_fit</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Type II MANOVA Tests:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for error:</span></span>
<span><span class="co">#&gt;        y1      y2      y3     y4</span></span>
<span><span class="co">#&gt; y1 641.00 601.750 535.250 426.00</span></span>
<span><span class="co">#&gt; y2 601.75 823.875 615.500 534.25</span></span>
<span><span class="co">#&gt; y3 535.25 615.500 655.875 555.25</span></span>
<span><span class="co">#&gt; y4 426.00 534.250 555.250 674.50</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ------------------------------------------</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Term: drug </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;        y1       y2       y3    y4</span></span>
<span><span class="co">#&gt; y1 567.00 335.2500  42.7500 387.0</span></span>
<span><span class="co">#&gt; y2 335.25 569.0833 404.5417 367.5</span></span>
<span><span class="co">#&gt; y3  42.75 404.5417 391.0833 171.0</span></span>
<span><span class="co">#&gt; y4 387.00 367.5000 171.0000 316.0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: drug</span></span>
<span><span class="co">#&gt;                  Df test stat  approx F num Df den Df     Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; Pillai            2  1.283456  8.508082      8     38 1.5010e-06 ***</span></span>
<span><span class="co">#&gt; Wilks             2  0.079007 11.509581      8     36 6.3081e-08 ***</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  2  7.069384 15.022441      8     34 3.9048e-09 ***</span></span>
<span><span class="co">#&gt; Roy               2  6.346509 30.145916      4     19 5.4493e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>reject the null hypothesis of no difference in means between treatments</p>
<div class="sourceCode" id="cb380"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Contrasts</span></span>
<span><span class="va">heart</span><span class="op">$</span><span class="va">drug</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">heart</span><span class="op">$</span><span class="va">drug</span><span class="op">)</span></span>
<span><span class="va">L</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">2</span>,</span>
<span>              <span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span>,<span class="op">-</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">3</span>, byrow <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">L</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"bww9:ctrl"</span>, <span class="st">"ax23:rest"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">L</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unique.html">unique</a></span><span class="op">(</span><span class="va">heart</span><span class="op">$</span><span class="va">drug</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">heart</span><span class="op">$</span><span class="va">drug</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">L</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">heart</span><span class="op">$</span><span class="va">drug</span><span class="op">)</span></span>
<span><span class="co">#&gt;      bww9:ctrl ax23:rest</span></span>
<span><span class="co">#&gt; ax23         0         2</span></span>
<span><span class="co">#&gt; bww9         1        -1</span></span>
<span><span class="co">#&gt; ctrl        -1        -1</span></span>
<span></span>
<span><span class="co"># do not set contrast L if you do further analysis (e.g., Anova, lm)</span></span>
<span><span class="co"># do M matrix instead</span></span>
<span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span>,</span>
<span>              <span class="fl">0</span>, <span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>,</span>
<span>              <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">## update model to test contrasts</span></span>
<span><span class="va">heart_mod2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">heart_mod</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">heart_mod2</span><span class="op">)</span></span>
<span><span class="co">#&gt;                  y1         y2        y3    y4</span></span>
<span><span class="co">#&gt; (Intercept)   75.00 78.9583333 77.041667 74.75</span></span>
<span><span class="co">#&gt; drugbww9:ctrl  4.50  5.8125000  3.562500  4.25</span></span>
<span><span class="co">#&gt; drugax23:rest -2.25  0.7708333  1.979167 -0.75</span></span>
<span></span>
<span><span class="co"># Hypothesis test for bww9 vs control after transformation M</span></span>
<span><span class="co"># same as linearHypothesis(heart_mod, hypothesis.matrix = c(0,1,-1), P = M)</span></span>
<span><span class="va">bww9vctrl</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">heart_mod2</span>,</span>
<span>                     hypothesis.matrix <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>                     P <span class="op">=</span> <span class="va">M</span><span class="op">)</span></span>
<span><span class="va">bww9vctrl</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Response transformation matrix:</span></span>
<span><span class="co">#&gt;    [,1] [,2] [,3]</span></span>
<span><span class="co">#&gt; y1    1    0    0</span></span>
<span><span class="co">#&gt; y2   -1    1    0</span></span>
<span><span class="co">#&gt; y3    0   -1    1</span></span>
<span><span class="co">#&gt; y4    0    0   -1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;          [,1]   [,2]     [,3]</span></span>
<span><span class="co">#&gt; [1,]  27.5625 -47.25  14.4375</span></span>
<span><span class="co">#&gt; [2,] -47.2500  81.00 -24.7500</span></span>
<span><span class="co">#&gt; [3,]  14.4375 -24.75   7.5625</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for error:</span></span>
<span><span class="co">#&gt;          [,1]     [,2]    [,3]</span></span>
<span><span class="co">#&gt; [1,]  261.375 -141.875  28.000</span></span>
<span><span class="co">#&gt; [2,] -141.875  248.750 -19.375</span></span>
<span><span class="co">#&gt; [3,]   28.000  -19.375 219.875</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: </span></span>
<span><span class="co">#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)</span></span>
<span><span class="co">#&gt; Pillai            1 0.2564306 2.184141      3     19 0.1233</span></span>
<span><span class="co">#&gt; Wilks             1 0.7435694 2.184141      3     19 0.1233</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233</span></span>
<span><span class="co">#&gt; Roy               1 0.3448644 2.184141      3     19 0.1233</span></span>
<span></span>
<span><span class="va">bww9vctrl</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">heart_mod</span>,</span>
<span>                     hypothesis.matrix <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span>,</span>
<span>                     P <span class="op">=</span> <span class="va">M</span><span class="op">)</span></span>
<span><span class="va">bww9vctrl</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Response transformation matrix:</span></span>
<span><span class="co">#&gt;    [,1] [,2] [,3]</span></span>
<span><span class="co">#&gt; y1    1    0    0</span></span>
<span><span class="co">#&gt; y2   -1    1    0</span></span>
<span><span class="co">#&gt; y3    0   -1    1</span></span>
<span><span class="co">#&gt; y4    0    0   -1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;          [,1]   [,2]     [,3]</span></span>
<span><span class="co">#&gt; [1,]  27.5625 -47.25  14.4375</span></span>
<span><span class="co">#&gt; [2,] -47.2500  81.00 -24.7500</span></span>
<span><span class="co">#&gt; [3,]  14.4375 -24.75   7.5625</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for error:</span></span>
<span><span class="co">#&gt;          [,1]     [,2]    [,3]</span></span>
<span><span class="co">#&gt; [1,]  261.375 -141.875  28.000</span></span>
<span><span class="co">#&gt; [2,] -141.875  248.750 -19.375</span></span>
<span><span class="co">#&gt; [3,]   28.000  -19.375 219.875</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: </span></span>
<span><span class="co">#&gt;                  Df test stat approx F num Df den Df Pr(&gt;F)</span></span>
<span><span class="co">#&gt; Pillai            1 0.2564306 2.184141      3     19 0.1233</span></span>
<span><span class="co">#&gt; Wilks             1 0.7435694 2.184141      3     19 0.1233</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233</span></span>
<span><span class="co">#&gt; Roy               1 0.3448644 2.184141      3     19 0.1233</span></span></code></pre></div>
<p>there is no significant difference in means between the control and <code>bww9</code> drug</p>
<div class="sourceCode" id="cb381"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Hypothesis test for ax23 vs rest after transformation M</span></span>
<span><span class="va">axx23vrest</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">heart_mod2</span>,</span>
<span>                     hypothesis.matrix <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>                     P <span class="op">=</span> <span class="va">M</span><span class="op">)</span></span>
<span><span class="va">axx23vrest</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Response transformation matrix:</span></span>
<span><span class="co">#&gt;    [,1] [,2] [,3]</span></span>
<span><span class="co">#&gt; y1    1    0    0</span></span>
<span><span class="co">#&gt; y2   -1    1    0</span></span>
<span><span class="co">#&gt; y3    0   -1    1</span></span>
<span><span class="co">#&gt; y4    0    0   -1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;           [,1]       [,2]      [,3]</span></span>
<span><span class="co">#&gt; [1,]  438.0208  175.20833 -395.7292</span></span>
<span><span class="co">#&gt; [2,]  175.2083   70.08333 -158.2917</span></span>
<span><span class="co">#&gt; [3,] -395.7292 -158.29167  357.5208</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for error:</span></span>
<span><span class="co">#&gt;          [,1]     [,2]    [,3]</span></span>
<span><span class="co">#&gt; [1,]  261.375 -141.875  28.000</span></span>
<span><span class="co">#&gt; [2,] -141.875  248.750 -19.375</span></span>
<span><span class="co">#&gt; [3,]   28.000  -19.375 219.875</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: </span></span>
<span><span class="co">#&gt;                  Df test stat approx F num Df den Df     Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; Pillai            1  0.855364 37.45483      3     19 3.5484e-08 ***</span></span>
<span><span class="co">#&gt; Wilks             1  0.144636 37.45483      3     19 3.5484e-08 ***</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  1  5.913921 37.45483      3     19 3.5484e-08 ***</span></span>
<span><span class="co">#&gt; Roy               1  5.913921 37.45483      3     19 3.5484e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span></span>
<span><span class="va">axx23vrest</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">heart_mod</span>,</span>
<span>                     hypothesis.matrix <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>                     P <span class="op">=</span> <span class="va">M</span><span class="op">)</span></span>
<span><span class="va">axx23vrest</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Response transformation matrix:</span></span>
<span><span class="co">#&gt;    [,1] [,2] [,3]</span></span>
<span><span class="co">#&gt; y1    1    0    0</span></span>
<span><span class="co">#&gt; y2   -1    1    0</span></span>
<span><span class="co">#&gt; y3    0   -1    1</span></span>
<span><span class="co">#&gt; y4    0    0   -1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;           [,1]       [,2]      [,3]</span></span>
<span><span class="co">#&gt; [1,]  402.5208  127.41667 -390.9375</span></span>
<span><span class="co">#&gt; [2,]  127.4167   40.33333 -123.7500</span></span>
<span><span class="co">#&gt; [3,] -390.9375 -123.75000  379.6875</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for error:</span></span>
<span><span class="co">#&gt;          [,1]     [,2]    [,3]</span></span>
<span><span class="co">#&gt; [1,]  261.375 -141.875  28.000</span></span>
<span><span class="co">#&gt; [2,] -141.875  248.750 -19.375</span></span>
<span><span class="co">#&gt; [3,]   28.000  -19.375 219.875</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: </span></span>
<span><span class="co">#&gt;                  Df test stat approx F num Df den Df     Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; Pillai            1  0.842450 33.86563      3     19 7.9422e-08 ***</span></span>
<span><span class="co">#&gt; Wilks             1  0.157550 33.86563      3     19 7.9422e-08 ***</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  1  5.347205 33.86563      3     19 7.9422e-08 ***</span></span>
<span><span class="co">#&gt; Roy               1  5.347205 33.86563      3     19 7.9422e-08 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>there is a significant difference in means between ax23 drug treatment and the rest of the treatments</p>
</div>
<div id="profile-analysis" class="section level3" number="22.1.2">
<h3>
<span class="header-section-number">22.1.2</span> Profile Analysis<a class="anchor" aria-label="anchor" href="#profile-analysis"><i class="fas fa-link"></i></a>
</h3>
<p>Examine similarities between the treatment effects (between subjects), which is useful for longitudinal analysis. Null is that all treatments have the same average effect.</p>
<p><span class="math display">\[
H_0: \mu_1 = \mu_2 = \dots = \mu_h
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
H_0: \tau_1 = \tau_2 = \dots = \tau_h
\]</span></p>
<p>The exact nature of the similarities and differences between the treatments can be examined under this analysis.</p>
<p>Sequential steps in profile analysis:</p>
<ol style="list-style-type: decimal">
<li>Are the profiles <strong>parallel</strong>? (i.e., is there no interaction between treatment and time)</li>
<li>Are the profiles <strong>coincidental</strong>? (i.e., are the profiles identical?)</li>
<li>Are the profiles <strong>horizontal</strong>? (i.e., are there no differences between any time points?)</li>
</ol>
<p>If we reject the null hypothesis that the profiles are parallel, we can test</p>
<ul>
<li><p>Are there differences among groups within some subset of the total time points?</p></li>
<li><p>Are there differences among time points in a particular group (or groups)?</p></li>
<li><p>Are there differences within some subset of the total time points in a particular group (or groups)?</p></li>
</ul>
<p>Example</p>
<ul>
<li><p>4 times (p = 4)</p></li>
<li><p>3 treatments (h=3)</p></li>
</ul>
<div id="parallel-profile" class="section level4" number="22.1.2.1">
<h4>
<span class="header-section-number">22.1.2.1</span> Parallel Profile<a class="anchor" aria-label="anchor" href="#parallel-profile"><i class="fas fa-link"></i></a>
</h4>
<p>Are the profiles for each population identical expect for a mean shift?</p>
<p><span class="math display">\[
\begin{aligned}
H_0: \mu_{11} - \mu_{21} - \mu_{12} - \mu_{22} = &amp;\dots = \mu_{1t} - \mu_{2t} \\
\mu_{11} - \mu_{31} - \mu_{12} - \mu_{32} = &amp;\dots = \mu_{1t} - \mu_{3t} \\
&amp;\dots
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(h-1\)</span> equations</p>
<p>Equivalently,</p>
<p><span class="math display">\[
H_0: \mathbf{LBM = 0}
\]</span></p>
<p><span class="math display">\[
\mathbf{LBM} =
\left[
\begin{array}
{ccc}
1 &amp; -1 &amp; 0 \\
1 &amp; 0 &amp; -1
\end{array}
\right]
\left[
\begin{array}
{ccc}
\mu_{11} &amp; \dots &amp; \mu_{14} \\
\mu_{21} &amp; \dots &amp; \mu_{24} \\
\mu_{31} &amp; \dots &amp; \mu_{34}
\end{array}
\right]
\left[
\begin{array}
{ccc}
1 &amp; 1 &amp; 1 \\
-1 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; -1
\end{array}
\right]
=
\mathbf{0}
\]</span></p>
<p>where this is the cell means parameterization of <span class="math inline">\(\mathbf{B}\)</span></p>
<p>The multiplication of the first 2 matrices <span class="math inline">\(\mathbf{LB}\)</span> is</p>
<p><span class="math display">\[
\left[
\begin{array}
{cccc}
\mu_{11} - \mu_{21} &amp; \mu_{12} - \mu_{22} &amp; \mu_{13} - \mu_{23} &amp; \mu_{14} - \mu_{24}\\
\mu_{11} - \mu_{31} &amp; \mu_{12} - \mu_{32} &amp; \mu_{13} - \mu_{33} &amp; \mu_{14} - \mu_{34}
\end{array}
\right]
\]</span></p>
<p>which is the differences in treatment means at the same time</p>
<p>Multiplying by <span class="math inline">\(\mathbf{M}\)</span>, we get the comparison across time</p>
<p><span class="math display">\[
\left[
\begin{array}
{ccc}
(\mu_{11} - \mu_{21}) - (\mu_{12} - \mu_{22}) &amp; (\mu_{11} - \mu_{21}) -(\mu_{13} - \mu_{23}) &amp; (\mu_{11} - \mu_{21}) - (\mu_{14} - \mu_{24}) \\
(\mu_{11} - \mu_{31}) - (\mu_{12} - \mu_{32}) &amp; (\mu_{11} - \mu_{31}) - (\mu_{13} - \mu_{33}) &amp; (\mu_{11} - \mu_{31}) -(\mu_{14} - \mu_{34})
\end{array}
\right]
\]</span></p>
<p>Alternatively, we can also use the effects parameterization</p>
<p><span class="math display">\[
\mathbf{LBM} =
\left[
\begin{array}
{cccc}
0 &amp; 1 &amp; -1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; -1
\end{array}
\right]
\left[
\begin{array}
{c}
\mu' \\
\tau'_1 \\
\tau_2' \\
\tau_3'
\end{array}
\right]
\left[
\begin{array}
{ccc}
1 &amp; 1 &amp; 1 \\
-1 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; -1
\end{array}
\right]
= \mathbf{0}
\]</span></p>
<p>In both parameterizations, <span class="math inline">\(rank(\mathbf{L}) = h-1\)</span> and <span class="math inline">\(rank(\mathbf{M}) = p-1\)</span></p>
<p>We could also choose <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\mathbf{M}\)</span> in other forms</p>
<p><span class="math display">\[
\mathbf{L} = \left[
\begin{array}
{cccc}
0 &amp; 1 &amp; 0 &amp; -1 \\
0 &amp; 0 &amp; 1 &amp; -1
\end{array}
\right]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbf{M} = \left[
\begin{array}
{ccc}
1 &amp; 0 &amp; 0 \\
-1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; -1
\end{array}
\right]
\]</span></p>
<p>and still obtain the same result.</p>
</div>
<div id="coincidental-profiles" class="section level4" number="22.1.2.2">
<h4>
<span class="header-section-number">22.1.2.2</span> Coincidental Profiles<a class="anchor" aria-label="anchor" href="#coincidental-profiles"><i class="fas fa-link"></i></a>
</h4>
<p>After we have evidence that the profiles are parallel (i.e., fail to reject the parallel profile test), we can ask whether they are identical?</p>
<p>Given profiles are <strong>parallel</strong>, then if the sums of the components of <span class="math inline">\(\mu_i\)</span> are identical for all the treatments, then the profiles are <strong>identical</strong>.</p>
<p><span class="math display">\[
H_0: \mathbf{1'}_p \mu_1 = \mathbf{1'}_p \mu_2 = \dots = \mathbf{1'}_p \mu_h
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
H_0: \mathbf{LBM} = \mathbf{0}
\]</span></p>
<p>where for the cell means parameterization</p>
<p><span class="math display">\[
\mathbf{L} =
\left[
\begin{array}
{ccc}
1 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; -1
\end{array}
\right]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbf{M} =
\left[
\begin{array}
{cccc}
1 &amp; 1 &amp; 1 &amp; 1
\end{array}
\right]'
\]</span></p>
<p>multiplication yields</p>
<p><span class="math display">\[
\left[
\begin{array}
{c}
(\mu_{11} + \mu_{12} + \mu_{13} + \mu_{14}) - (\mu_{31} + \mu_{32} + \mu_{33} + \mu_{34}) \\
(\mu_{21} + \mu_{22} + \mu_{23} + \mu_{24}) - (\mu_{31} + \mu_{32} + \mu_{33} + \mu_{34})
\end{array}
\right]
=
\left[
\begin{array}
{c}
0 \\
0
\end{array}
\right]
\]</span></p>
<p>Different choices of <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\mathbf{M}\)</span> can yield the same result</p>
</div>
<div id="horizontal-profiles" class="section level4" number="22.1.2.3">
<h4>
<span class="header-section-number">22.1.2.3</span> Horizontal Profiles<a class="anchor" aria-label="anchor" href="#horizontal-profiles"><i class="fas fa-link"></i></a>
</h4>
<p>Given that we can’t reject the null hypothesis that all <span class="math inline">\(h\)</span> profiles are the same, we can ask whether all of the elements of the common profile equal? (i.e., horizontal)</p>
<p><span class="math display">\[
H_0: \mathbf{LBM} = \mathbf{0}
\]</span></p>
<p><span class="math display">\[
\mathbf{L} =
\left[
\begin{array}
{ccc}
1 &amp; 0 &amp; 0
\end{array}
\right]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbf{M} = \left[
\begin{array}
{ccc}
1 &amp; 0 &amp; 0 \\
-1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; -1
\end{array}
\right]
\]</span></p>
<p>hence,</p>
<p><span class="math display">\[
\left[
\begin{array}
{ccc}
(\mu_{11} - \mu_{12}) &amp; (\mu_{12} - \mu_{13}) &amp; (\mu_{13} + \mu_{14})
\end{array}
\right]
=
\left[
\begin{array}
{ccc}
0 &amp; 0 &amp; 0
\end{array}
\right]
\]</span></p>
<p>Note:</p>
<ul>
<li>If we fail to reject all 3 hypotheses, then we fail to reject the null hypotheses of both no difference between treatments and no differences between traits.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>Test</th>
<th>Equivalent test for</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Parallel profile</td>
<td>Interaction</td>
</tr>
<tr class="even">
<td>Coincidental profile</td>
<td>main effect of between-subjects factor</td>
</tr>
<tr class="odd">
<td>Horizontal profile</td>
<td>main effect of repeated measures factor</td>
</tr>
</tbody>
</table></div>
<div class="sourceCode" id="cb382"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">profile_fit</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/profileR/man/pbg.html">pbg</a></span><span class="op">(</span></span>
<span>        data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">heart</span><span class="op">[</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span>,</span>
<span>        group <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">heart</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span>,</span>
<span>        original.names <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>        profile.plot <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>    <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">profile_fit</span><span class="op">)</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, </span></span>
<span><span class="co">#&gt;     1]), original.names = TRUE, profile.plot = FALSE)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Hypothesis Tests:</span></span>
<span><span class="co">#&gt; $`Ho: Profiles are parallel`</span></span>
<span><span class="co">#&gt;   Multivariate.Test Statistic  Approx.F num.df den.df      p.value</span></span>
<span><span class="co">#&gt; 1             Wilks 0.1102861 12.737599      6     38 7.891497e-08</span></span>
<span><span class="co">#&gt; 2            Pillai 1.0891707  7.972007      6     40 1.092397e-05</span></span>
<span><span class="co">#&gt; 3  Hotelling-Lawley 6.2587852 18.776356      6     36 9.258571e-10</span></span>
<span><span class="co">#&gt; 4               Roy 5.9550887 39.700592      3     20 1.302458e-08</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $`Ho: Profiles have equal levels`</span></span>
<span><span class="co">#&gt;             Df Sum Sq Mean Sq F value  Pr(&gt;F)   </span></span>
<span><span class="co">#&gt; group        2  328.7  164.35   5.918 0.00915 **</span></span>
<span><span class="co">#&gt; Residuals   21  583.2   27.77                   </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $`Ho: Profiles are flat`</span></span>
<span><span class="co">#&gt;          F df1 df2      p-value</span></span>
<span><span class="co">#&gt; 1 14.30928   3  19 4.096803e-05</span></span>
<span><span class="co"># reject null hypothesis of parallel profiles</span></span>
<span><span class="co"># reject the null hypothesis of coincidental profiles</span></span>
<span><span class="co"># reject the null hypothesis that the profiles are flat</span></span></code></pre></div>
</div>
</div>
<div id="summary-6" class="section level3" number="22.1.3">
<h3>
<span class="header-section-number">22.1.3</span> Summary<a class="anchor" aria-label="anchor" href="#summary-6"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="images/MANOVA_summary.PNG" title="MANOVA summary" style="display: block; margin: 1em auto" width="600" height="400"></div>
</div>
</div>
<div id="principal-components" class="section level2" number="22.2">
<h2>
<span class="header-section-number">22.2</span> Principal Components<a class="anchor" aria-label="anchor" href="#principal-components"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Unsupervised learning</li>
<li>find important features</li>
<li>reduce the dimensions of the data set</li>
<li>“decorrelate” multivariate vectors that have dependence.</li>
<li>uses eigenvector/eigvenvalue decomposition of covariance (correlation) matrices.</li>
</ul>
<p>According to the “spectral decomposition theorem”, if <span class="math inline">\(\mathbf{\Sigma}_{p \times p}\)</span> i s a positive semi-definite, symmetric, real matrix, then there exists an orthogonal matrix <span class="math inline">\(\mathbf{A}\)</span> such that <span class="math inline">\(\mathbf{A'\Sigma A} = \Lambda\)</span> where <span class="math inline">\(\Lambda\)</span> is a diagonal matrix containing the eigenvalues <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<p><span class="math display">\[
\mathbf{\Lambda} =
\left(
\begin{array}
{cccc}
\lambda_1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; \lambda_2 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; \lambda_p
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{A} =
\left(
\begin{array}
{cccc}
\mathbf{a}_1 &amp; \mathbf{a}_2 &amp; \ldots &amp; \mathbf{a}_p
\end{array}
\right)
\]</span></p>
<p>the i-th column of <span class="math inline">\(\mathbf{A}\)</span> , <span class="math inline">\(\mathbf{a}_i\)</span>, is the i-th <span class="math inline">\(p \times 1\)</span> eigenvector of <span class="math inline">\(\mathbf{\Sigma}\)</span> that corresponds to the eigenvalue, <span class="math inline">\(\lambda_i\)</span> , where <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p\)</span> . Alternatively, express in matrix decomposition:</p>
<p><span class="math display">\[
\mathbf{\Sigma} = \mathbf{A \Lambda A}'
\]</span></p>
<p><span class="math display">\[
\mathbf{\Sigma} = \mathbf{A}
\left(
\begin{array}
{cccc}
\lambda_1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; \lambda_2 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots&amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; \lambda_p
\end{array}
\right)
\mathbf{A}'
= \sum_{i=1}^p \lambda_i \mathbf{a}_i \mathbf{a}_i'
\]</span></p>
<p>where the outer product <span class="math inline">\(\mathbf{a}_i \mathbf{a}_i'\)</span> is a <span class="math inline">\(p \times p\)</span> matrix of rank 1.</p>
<p>For example,</p>
<p><span class="math inline">\(\mathbf{x} \sim N_2(\mathbf{\mu}, \mathbf{\Sigma})\)</span></p>
<p><span class="math display">\[
\mathbf{\mu} =
\left(
\begin{array}
{c}
5 \\
12
\end{array}
\right);
\mathbf{\Sigma} =
\left(
\begin{array}
{cc}
4 &amp; 1 \\
1 &amp; 2
\end{array}
\right)
\]</span></p>
<div class="sourceCode" id="cb383"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="va">mu</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">12</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Sigma</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span>, byrow <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="va">sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>, mu <span class="op">=</span> <span class="va">mu</span>, Sigma <span class="op">=</span> <span class="va">Sigma</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">sim</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">sim</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-14-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Here,</p>
<p><span class="math display">\[
\mathbf{A} =
\left(
\begin{array}
{cc}
0.9239 &amp; -0.3827 \\
0.3827 &amp; 0.9239 \\
\end{array}
\right)
\]</span></p>
<p>Columns of <span class="math inline">\(\mathbf{A}\)</span> are the eigenvectors for the decomposition</p>
<p>Under matrix multiplication (<span class="math inline">\(\mathbf{A'\Sigma A}\)</span> or <span class="math inline">\(\mathbf{A'A}\)</span> ), the off-diagonal elements equal to 0</p>
<p>Multiplying data by this matrix (i.e., projecting the data onto the orthogonal axes); the distribution of the resulting data (i.e., “scores”) is</p>
<p><span class="math display">\[
N_2 (\mathbf{A'\mu,A'\Sigma A}) = N_2 (\mathbf{A'\mu, \Lambda})
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{A'x} \sim N
\left[
\left(
\begin{array}
{c}
9.2119 \\
9.1733
\end{array}
\right),
\left(
\begin{array}
{cc}
4.4144 &amp; 0 \\
0 &amp; 1.5859
\end{array}
\right)
\right]
\]</span></p>
<div class="sourceCode" id="cb384"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">A_matrix</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.9239</span>, <span class="op">-</span><span class="fl">0.3827</span>, <span class="fl">0.3827</span>, <span class="fl">0.9239</span><span class="op">)</span>,</span>
<span>                  nrow <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                  byrow <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">A_matrix</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">A_matrix</span></span>
<span><span class="co">#&gt;          [,1]     [,2]</span></span>
<span><span class="co">#&gt; [1,] 1.000051 0.000000</span></span>
<span><span class="co">#&gt; [2,] 0.000000 1.000051</span></span>
<span></span>
<span><span class="va">sim1</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/mvrnorm.html">mvrnorm</a></span><span class="op">(</span></span>
<span>        n <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>        mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">A_matrix</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">mu</span>,</span>
<span>        Sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">A_matrix</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">Sigma</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">A_matrix</span></span>
<span>    <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">sim1</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, <span class="va">sim1</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-15-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>No more dependence in the data structure, plot</p>
<p>Notes:</p>
<ul>
<li><p>The i-th eigenvalue is the variance of a linear combination of the elements of <span class="math inline">\(\mathbf{x}\)</span> ; <span class="math inline">\(var(y_i) = var(\mathbf{a'_i x}) = \lambda_i\)</span></p></li>
<li><p>The values on the transformed set of axes (i.e., the <span class="math inline">\(y_i\)</span>’s) are called the scores. These are the orthogonal projections of the data onto the “new principal component axes</p></li>
<li><p>Variances of <span class="math inline">\(y_1\)</span> are greater than those for any other possible projection</p></li>
</ul>
<p>Covariance matrix decomposition and projection onto orthogonal axes = PCA</p>
<div id="population-principal-components" class="section level3" number="22.2.1">
<h3>
<span class="header-section-number">22.2.1</span> Population Principal Components<a class="anchor" aria-label="anchor" href="#population-principal-components"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(p \times 1\)</span> vectors <span class="math inline">\(\mathbf{x}_1, \dots , \mathbf{x}_n\)</span> which are iid with <span class="math inline">\(var(\mathbf{x}_i) = \mathbf{\Sigma}\)</span></p>
<ul>
<li><p>The first PC is the linear combination <span class="math inline">\(y_1 = \mathbf{a}_1' \mathbf{x} = a_{11}x_1 + \dots + a_{1p}x_p\)</span> with <span class="math inline">\(\mathbf{a}_1' \mathbf{a}_1 = 1\)</span> such that <span class="math inline">\(var(y_1)\)</span> is the maximum of all linear combinations of <span class="math inline">\(\mathbf{x}\)</span> which have unit length</p></li>
<li><p>The second PC is the linear combination <span class="math inline">\(y_1 = \mathbf{a}_2' \mathbf{x} = a_{21}x_1 + \dots + a_{2p}x_p\)</span> with <span class="math inline">\(\mathbf{a}_2' \mathbf{a}_2 = 1\)</span> such that <span class="math inline">\(var(y_1)\)</span> is the maximum of all linear combinations of <span class="math inline">\(\mathbf{x}\)</span> which have unit length and uncorrelated with <span class="math inline">\(y_1\)</span> (i.e., <span class="math inline">\(cov(\mathbf{a}_1' \mathbf{x}, \mathbf{a}'_2 \mathbf{x}) =0\)</span></p></li>
<li><p>continues for all <span class="math inline">\(y_i\)</span> to <span class="math inline">\(y_p\)</span></p></li>
</ul>
<p><span class="math inline">\(\mathbf{a}_i\)</span>’s are those that make up the matrix <span class="math inline">\(\mathbf{A}\)</span> in the symmetric decomposition <span class="math inline">\(\mathbf{A'\Sigma A} = \mathbf{\Lambda}\)</span> , where <span class="math inline">\(var(y_1) = \lambda_1, \dots , var(y_p) = \lambda_p\)</span> And the total variance of <span class="math inline">\(\mathbf{x}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
var(x_1) + \dots + var(x_p) &amp;= tr(\Sigma) = \lambda_1 + \dots + \lambda_p \\
&amp;= var(y_1) + \dots + var(y_p)
\end{aligned}
\]</span></p>
<p>Data Reduction</p>
<p>To reduce the dimension of data from p (original) to k dimensions without much “loss of information”, we can use properties of the population principal components</p>
<ul>
<li><p>Suppose <span class="math inline">\(\mathbf{\Sigma} \approx \sum_{i=1}^k \lambda_i \mathbf{a}_i \mathbf{a}_i'\)</span> . Even thought the true variance-covariance matrix has rank <span class="math inline">\(p\)</span> , it can be be well approximate by a matrix of rank k (k &lt;p)</p></li>
<li><p>New “traits” are linear combinations of the measured traits. We can attempt to make meaningful interpretation fo the combinations (with orthogonality constraints).</p></li>
<li><p>The proportion of the total variance accounted for by the j-th principal component is</p></li>
</ul>
<p><span class="math display">\[
\frac{var(y_j)}{\sum_{i=1}^p var(y_i)} = \frac{\lambda_j}{\sum_{i=1}^p \lambda_i}
\]</span></p>
<ul>
<li><p>The proportion of the total variation accounted for by the first k principal components is <span class="math inline">\(\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}\)</span></p></li>
<li><p>Above example , we have <span class="math inline">\(4.4144/(4+2) = .735\)</span> of the total variability can be explained by the first principal component</p></li>
</ul>
</div>
<div id="sample-principal-components" class="section level3" number="22.2.2">
<h3>
<span class="header-section-number">22.2.2</span> Sample Principal Components<a class="anchor" aria-label="anchor" href="#sample-principal-components"><i class="fas fa-link"></i></a>
</h3>
<p>Since <span class="math inline">\(\mathbf{\Sigma}\)</span> is unknown, we use</p>
<p><span class="math display">\[
\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})'
\]</span></p>
<p>Let <span class="math inline">\(\hat{\lambda}_1 \ge \hat{\lambda}_2 \ge \dots \ge \hat{\lambda}_p \ge 0\)</span> be the eigenvalues of <span class="math inline">\(\mathbf{S}\)</span> and <span class="math inline">\(\hat{\mathbf{a}}_1, \hat{\mathbf{a}}_2, \dots, \hat{\mathbf{a}}_p\)</span> denote the eigenvectors of <span class="math inline">\(\mathbf{S}\)</span></p>
<p>Then, the i-th sample principal component score (or principal component or score) is</p>
<p><span class="math display">\[
\hat{y}_{ij} = \sum_{k=1}^p \hat{a}_{ik}x_{kj} = \hat{\mathbf{a}}_i'\mathbf{x}_j
\]</span></p>
<p><strong>Properties of Sample Principal Components</strong></p>
<ul>
<li><p>The estimated variance of <span class="math inline">\(y_i = \hat{\mathbf{a}}_i'\mathbf{x}_j\)</span> is <span class="math inline">\(\hat{\lambda}_i\)</span></p></li>
<li><p>The sample covariance between <span class="math inline">\(\hat{y}_i\)</span> and <span class="math inline">\(\hat{y}_{i'}\)</span> is 0 when <span class="math inline">\(i \neq i'\)</span></p></li>
<li><p>The proportion of the total sample variance accounted for by the i-th sample principal component is <span class="math inline">\(\frac{\hat{\lambda}_i}{\sum_{k=1}^p \hat{\lambda}_k}\)</span></p></li>
<li><p>The estimated correlation between the <span class="math inline">\(i\)</span>-th principal component score and the <span class="math inline">\(l\)</span>-th attribute of <span class="math inline">\(\mathbf{x}\)</span> is</p></li>
</ul>
<p><span class="math display">\[
r_{x_l , \hat{y}_i} = \frac{\hat{a}_{il}\sqrt{\lambda_i}}{\sqrt{s_{ll}}}
\]</span></p>
<ul>
<li><p>The correlation coefficient is typically used to interpret the components (i.e., if this correlation is high then it suggests that the l-th original trait is important in the i-th principle component). According to <span class="citation">R. A. Johnson, Wichern, et al. (<a href="references.html#ref-johnson2002applied">2002</a>)</span>, pp.433-434, <span class="math inline">\(r_{x_l, \hat{y}_i}\)</span> only measures the univariate contribution of an individual X to a component Y without taking into account the presence of the other X’s. Hence, some prefer <span class="math inline">\(\hat{a}_{il}\)</span> coefficient to interpret the principal component.</p></li>
<li><p><span class="math inline">\(r_{x_l, \hat{y}_i} ; \hat{a}_{il}\)</span> are referred to as “loadings”</p></li>
</ul>
<p>To use k principal components, we must calculate the scores for each data vector in the sample</p>
<p><span class="math display">\[
\mathbf{y}_j =
\left(
\begin{array}
{c}
y_{1j} \\
y_{2j} \\
\vdots \\
y_{kj}
\end{array}
\right) =
\left(
\begin{array}
{c}
\hat{\mathbf{a}}_1' \mathbf{x}_j \\
\hat{\mathbf{a}}_2' \mathbf{x}_j \\
\vdots \\
\hat{\mathbf{a}}_k' \mathbf{x}_j
\end{array}
\right) =
\left(
\begin{array}
{c}
\hat{\mathbf{a}}_1' \\
\hat{\mathbf{a}}_2' \\
\vdots \\
\hat{\mathbf{a}}_k'
\end{array}
\right) \mathbf{x}_j
\]</span></p>
<p>Issues:</p>
<ul>
<li><p>Large sample theory exists for eigenvalues and eigenvectors of sample covariance matrices if inference is necessary. But we do not do inference with PCA, we only use it as exploratory or descriptive analysis.</p></li>
<li>
<p>PC is not invariant to changes in scale (Exception: if all trait are rescaled by multiplying by the same constant, such as feet to inches).</p>
<ul>
<li><p>PCA based on the correlation matrix <span class="math inline">\(\mathbf{R}\)</span> is different than that based on the covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span></p></li>
<li><p>PCA for the correlation matrix is just rescaling each trait to have unit variance</p></li>
<li><p>Transform <span class="math inline">\(\mathbf{x}\)</span> to <span class="math inline">\(\mathbf{z}\)</span> where <span class="math inline">\(z_{ij} = (x_{ij} - \bar{x}_i)/\sqrt{s_{ii}}\)</span> where the denominator affects the PCA</p></li>
<li><p>After transformation, <span class="math inline">\(cov(\mathbf{z}) = \mathbf{R}\)</span></p></li>
<li><p>PCA on <span class="math inline">\(\mathbf{R}\)</span> is calculated in the same way as that on <span class="math inline">\(\mathbf{S}\)</span> (where <span class="math inline">\(\hat{\lambda}{}_1 + \dots + \hat{\lambda}{}_p = p\)</span> )</p></li>
<li>
<p>The use of <span class="math inline">\(\mathbf{R}, \mathbf{S}\)</span> depends on the purpose of PCA.</p>
<ul>
<li>If the scale of the observations if different, covariance matrix is more preferable. but if they are dramatically different, analysis can still be dominated by the large variance traits.</li>
</ul>
</li>
<li>
<p>How many PCs to use can be guided by</p>
<ul>
<li><p>Scree Graphs: plot the eigenvalues against their indices. Look for the “elbow” where the steep decline in the graph suddenly flattens out; or big gaps.</p></li>
<li><p>minimum Percent of total variation (e.g., choose enough components to have 50% or 90%). can be used for interpretations.</p></li>
<li><p>Kaiser’s rule: use only those PC with eigenvalues larger than 1 (applied to PCA on the correlation matrix) - ad hoc</p></li>
<li><p>Compare to the eigenvalue scree plot of data to the scree plot when the data are randomized.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div id="application-10" class="section level3" number="22.2.3">
<h3>
<span class="header-section-number">22.2.3</span> Application<a class="anchor" aria-label="anchor" href="#application-10"><i class="fas fa-link"></i></a>
</h3>
<p>PCA on the covariance matrix is usually not preferred due to the fact that PCA is not invariant to changes in scale. Hence, PCA on the correlation matrix is more preferred</p>
<p>This also addresses the problem of multicollinearity</p>
<p>The eigvenvectors may differ by a multiplication of -1 for different implementation, but same interpretation.</p>
<div class="sourceCode" id="cb385"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="co">## Read in and check data</span></span>
<span><span class="va">stock</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/stock.dat"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">stock</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"allied"</span>, <span class="st">"dupont"</span>, <span class="st">"carbide"</span>, <span class="st">"exxon"</span>, <span class="st">"texaco"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">stock</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':    100 obs. of  5 variables:</span></span>
<span><span class="co">#&gt;  $ allied : num  0 0.027 0.1228 0.057 0.0637 ...</span></span>
<span><span class="co">#&gt;  $ dupont : num  0 -0.04485 0.06077 0.02995 -0.00379 ...</span></span>
<span><span class="co">#&gt;  $ carbide: num  0 -0.00303 0.08815 0.06681 -0.03979 ...</span></span>
<span><span class="co">#&gt;  $ exxon  : num  0.0395 -0.0145 0.0862 0.0135 -0.0186 ...</span></span>
<span><span class="co">#&gt;  $ texaco : num  0 0.0435 0.0781 0.0195 -0.0242 ...</span></span>
<span></span>
<span><span class="co">## Covariance matrix of data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">stock</span><span class="op">)</span></span>
<span><span class="co">#&gt;               allied       dupont      carbide        exxon       texaco</span></span>
<span><span class="co">#&gt; allied  0.0016299269 0.0008166676 0.0008100713 0.0004422405 0.0005139715</span></span>
<span><span class="co">#&gt; dupont  0.0008166676 0.0012293759 0.0008276330 0.0003868550 0.0003109431</span></span>
<span><span class="co">#&gt; carbide 0.0008100713 0.0008276330 0.0015560763 0.0004872816 0.0004624767</span></span>
<span><span class="co">#&gt; exxon   0.0004422405 0.0003868550 0.0004872816 0.0008023323 0.0004084734</span></span>
<span><span class="co">#&gt; texaco  0.0005139715 0.0003109431 0.0004624767 0.0004084734 0.0007587370</span></span>
<span></span>
<span><span class="co">## Correlation matrix of data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">stock</span><span class="op">)</span></span>
<span><span class="co">#&gt;            allied    dupont   carbide     exxon    texaco</span></span>
<span><span class="co">#&gt; allied  1.0000000 0.5769244 0.5086555 0.3867206 0.4621781</span></span>
<span><span class="co">#&gt; dupont  0.5769244 1.0000000 0.5983841 0.3895191 0.3219534</span></span>
<span><span class="co">#&gt; carbide 0.5086555 0.5983841 1.0000000 0.4361014 0.4256266</span></span>
<span><span class="co">#&gt; exxon   0.3867206 0.3895191 0.4361014 1.0000000 0.5235293</span></span>
<span><span class="co">#&gt; texaco  0.4621781 0.3219534 0.4256266 0.5235293 1.0000000</span></span>
<span></span>
<span><span class="co"># cov(scale(stock)) # give the same result</span></span>
<span></span>
<span><span class="co">## PCA with covariance</span></span>
<span><span class="va">cov_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">stock</span><span class="op">)</span> </span>
<span><span class="co"># uses singular value decomposition for calculation and an N -1 divisor</span></span>
<span><span class="co"># alternatively, princomp can do PCA via spectral decomposition, </span></span>
<span><span class="co"># but it has worse numerical accuracy</span></span>
<span></span>
<span><span class="co"># eigen values</span></span>
<span><span class="va">cov_results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>eigen_values <span class="op">=</span> <span class="va">cov_pca</span><span class="op">$</span><span class="va">sdev</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">cov_results</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>proportion <span class="op">=</span> <span class="va">eigen_values</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eigen_values</span><span class="op">)</span>,</span>
<span>           cumulative <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">proportion</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="co">#&gt;   eigen_values proportion cumulative</span></span>
<span><span class="co">#&gt; 1 0.0035953867 0.60159252  0.6015925</span></span>
<span><span class="co">#&gt; 2 0.0007921798 0.13255027  0.7341428</span></span>
<span><span class="co">#&gt; 3 0.0007364426 0.12322412  0.8573669</span></span>
<span><span class="co">#&gt; 4 0.0005086686 0.08511218  0.9424791</span></span>
<span><span class="co">#&gt; 5 0.0003437707 0.05752091  1.0000000</span></span>
<span><span class="co"># first 2 PCs account for 73% variance in the data</span></span>
<span></span>
<span><span class="co"># eigen vectors</span></span>
<span><span class="va">cov_pca</span><span class="op">$</span><span class="va">rotation</span> <span class="co"># prcomp calls rotation</span></span>
<span><span class="co">#&gt;               PC1         PC2        PC3         PC4         PC5</span></span>
<span><span class="co">#&gt; allied  0.5605914  0.73884565 -0.1260222  0.28373183 -0.20846832</span></span>
<span><span class="co">#&gt; dupont  0.4698673 -0.09286987 -0.4675066 -0.68793190  0.28069055</span></span>
<span><span class="co">#&gt; carbide 0.5473322 -0.65401929 -0.1140581  0.50045312 -0.09603973</span></span>
<span><span class="co">#&gt; exxon   0.2908932 -0.11267353  0.6099196 -0.43808002 -0.58203935</span></span>
<span><span class="co">#&gt; texaco  0.2842017  0.07103332  0.6168831  0.06227778  0.72784638</span></span>
<span><span class="co"># princomp calls loadings.</span></span>
<span></span>
<span><span class="co"># first PC = overall average</span></span>
<span><span class="co"># second PC compares Allied to Carbide</span></span>
<span></span>
<span><span class="co">## PCA with correlation</span></span>
<span><span class="co">#same as scale(stock) %&gt;% prcomp</span></span>
<span><span class="va">cor_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">stock</span>, scale <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span></span>
<span></span>
<span></span>
<span><span class="co"># eigen values</span></span>
<span><span class="va">cor_results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>eigen_values <span class="op">=</span> <span class="va">cor_pca</span><span class="op">$</span><span class="va">sdev</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">cor_results</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>proportion <span class="op">=</span> <span class="va">eigen_values</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eigen_values</span><span class="op">)</span>,</span>
<span>           cumulative <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">proportion</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;   eigen_values proportion cumulative</span></span>
<span><span class="co">#&gt; 1    2.8564869 0.57129738  0.5712974</span></span>
<span><span class="co">#&gt; 2    0.8091185 0.16182370  0.7331211</span></span>
<span><span class="co">#&gt; 3    0.5400440 0.10800880  0.8411299</span></span>
<span><span class="co">#&gt; 4    0.4513468 0.09026936  0.9313992</span></span>
<span><span class="co">#&gt; 5    0.3430038 0.06860076  1.0000000</span></span>
<span></span>
<span><span class="co"># first egiven values corresponds to less variance </span></span>
<span><span class="co"># than PCA based on the covariance matrix</span></span>
<span></span>
<span><span class="co"># eigen vectors</span></span>
<span><span class="va">cor_pca</span><span class="op">$</span><span class="va">rotation</span></span>
<span><span class="co">#&gt;               PC1        PC2        PC3        PC4        PC5</span></span>
<span><span class="co">#&gt; allied  0.4635405 -0.2408499  0.6133570 -0.3813727  0.4532876</span></span>
<span><span class="co">#&gt; dupont  0.4570764 -0.5090997 -0.1778996 -0.2113068 -0.6749814</span></span>
<span><span class="co">#&gt; carbide 0.4699804 -0.2605774 -0.3370355  0.6640985  0.3957247</span></span>
<span><span class="co">#&gt; exxon   0.4216770  0.5252647 -0.5390181 -0.4728036  0.1794482</span></span>
<span><span class="co">#&gt; texaco  0.4213291  0.5822416  0.4336029  0.3812273 -0.3874672</span></span>
<span><span class="co"># interpretation of PC2 is different from above: </span></span>
<span><span class="co"># it is a comparison of Allied, Dupont and Carbid to Exxon and Texaco </span></span></code></pre></div>
<p>Covid Example</p>
<p>To reduce collinearity problem in this dataset, we can use principal components as regressors.</p>
<div class="sourceCode" id="cb386"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/load.html">load</a></span><span class="op">(</span><span class="st">'images/MOcovid.RData'</span><span class="op">)</span></span>
<span><span class="va">covidpca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">ndat</span><span class="op">[</span>,<span class="op">-</span><span class="fl">1</span><span class="op">]</span>,scale <span class="op">=</span> <span class="cn">T</span>,center <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span></span>
<span><span class="va">covidpca</span><span class="op">$</span><span class="va">rotation</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="co">#&gt;                                                          PC1         PC2</span></span>
<span><span class="co">#&gt; X..Population.in.Rural.Areas                      0.32865838  0.05090955</span></span>
<span><span class="co">#&gt; Area..sq..miles.                                  0.12014444 -0.28579183</span></span>
<span><span class="co">#&gt; Population.density..sq..miles.                   -0.29670124  0.28312922</span></span>
<span><span class="co">#&gt; Literacy.rate                                    -0.12517700 -0.08999542</span></span>
<span><span class="co">#&gt; Families                                         -0.25856941  0.16485752</span></span>
<span><span class="co">#&gt; Area.of.farm.land..sq..miles.                     0.02101106 -0.31070363</span></span>
<span><span class="co">#&gt; Number.of.farms                                  -0.03814582 -0.44809679</span></span>
<span><span class="co">#&gt; Average.value.of.all.property.per.farm..dollars. -0.05410709  0.14404306</span></span>
<span><span class="co">#&gt; Estimation.of.rurality..                         -0.19040210  0.12089501</span></span>
<span><span class="co">#&gt; Male..                                            0.02182394 -0.09568768</span></span>
<span><span class="co">#&gt; Number.of.Physcians.per.100.000                  -0.31451606  0.13598026</span></span>
<span><span class="co">#&gt; average.age                                       0.29414708  0.35593459</span></span>
<span><span class="co">#&gt; X0.4.age.proportion                              -0.11431336 -0.23574057</span></span>
<span><span class="co">#&gt; X20.44.age.proportion                            -0.32802128 -0.22718550</span></span>
<span><span class="co">#&gt; X65.and.over.age.proportion                       0.30585033  0.32201626</span></span>
<span><span class="co">#&gt; prop..White..nonHisp                              0.35627561 -0.14142646</span></span>
<span><span class="co">#&gt; prop..Hispanic                                   -0.16655381 -0.15105342</span></span>
<span><span class="co">#&gt; prop..Black                                      -0.33333359  0.24405802</span></span>
<span></span>
<span></span>
<span><span class="co"># Variability of each principal component: pr.var</span></span>
<span><span class="va">pr.var</span> <span class="op">&lt;-</span> <span class="va">covidpca</span><span class="op">$</span><span class="va">sdev</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="co"># Variance explained by each principal component: pve</span></span>
<span><span class="va">pve</span> <span class="op">&lt;-</span> <span class="va">pr.var</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pr.var</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="va">pve</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Principal Component"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Proportion of Variance Explained"</span>,</span>
<span>    ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>    type <span class="op">=</span> <span class="st">"b"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb387"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">pve</span><span class="op">)</span>,</span>
<span>    xlab <span class="op">=</span> <span class="st">"Principal Component"</span>,</span>
<span>    ylab <span class="op">=</span> <span class="st">"Cumulative Proportion of Variance Explained"</span>,</span>
<span>    ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>    type <span class="op">=</span> <span class="st">"b"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-17-2.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb388"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># the first six principe account for around 80% of the variance. </span></span>
<span></span>
<span></span>
<span><span class="co">#using base lm function for PC regression</span></span>
<span><span class="va">pcadat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">covidpca</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">pcadat</span><span class="op">$</span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">ndat</span><span class="op">$</span><span class="va">Y</span></span>
<span><span class="va">pcr.man</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">pcadat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">pcr.man</span><span class="op">$</span><span class="va">residuals</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.03453371</span></span>
<span></span>
<span><span class="co">#comparison to lm w/o prin comps</span></span>
<span><span class="va">lm.fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">ndat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">lm.fit</span><span class="op">$</span><span class="va">residuals</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.02335128</span></span></code></pre></div>
<p>MSE for the PC-based model is larger than regular regression, because models with a large degree of collinearity can still perform well.</p>
<p><code>pcr</code> function in <code>pls</code> can be used for fitting PC regression (it will select the optimal number of components in the model).</p>
</div>
</div>
<div id="factor-analysis" class="section level2" number="22.3">
<h2>
<span class="header-section-number">22.3</span> Factor Analysis<a class="anchor" aria-label="anchor" href="#factor-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>Purpose</p>
<ul>
<li><p>Using a few linear combinations of underlying unobservable (latent) traits, we try to describe the covariance relationship among a large number of measured traits</p></li>
<li><p>Similar to <a href="multivariate-methods.html#principal-components">PCA</a>, but factor analysis is <strong>model based</strong></p></li>
</ul>
<p>More details can be found on <a href="https://online.stat.psu.edu/stat505/book/export/html/691">PSU stat</a> or <a href="http://users.stat.umn.edu/~helwig/notes/factanal-Notes.pdf">UMN stat</a></p>
<p>Let <span class="math inline">\(\mathbf{y}\)</span> be the set of <span class="math inline">\(p\)</span> measured variables</p>
<p><span class="math inline">\(E(\mathbf{y}) = \mathbf{\mu}\)</span></p>
<p><span class="math inline">\(var(\mathbf{y}) = \mathbf{\Sigma}\)</span></p>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} - \mathbf{\mu} &amp;= \mathbf{Lf} + \epsilon \\
&amp;=
\left(
\begin{array}
{c}
l_{11}f_1 + l_{12}f_2 + \dots + l_{tm}f_m \\
\vdots \\
l_{p1}f_1 + l_{p2}f_2 + \dots + l_{pm} f_m
\end{array}
\right)
+
\left(
\begin{array}
{c}
\epsilon_1 \\
\vdots \\
\epsilon_p
\end{array}
\right)
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y} - \mathbf{\mu}\)</span> = the p centered measurements</p></li>
<li><p><span class="math inline">\(\mathbf{L}\)</span> = <span class="math inline">\(p \times m\)</span> matrix of factor loadings</p></li>
<li><p><span class="math inline">\(\mathbf{f}\)</span> = unobserved common factors for the population</p></li>
<li><p><span class="math inline">\(\mathbf{\epsilon}\)</span> = random errors (i.e., variation that is not accounted for by the common factors).</p></li>
</ul>
<p>We want <span class="math inline">\(m\)</span> (the number of factors) to be much smaller than <span class="math inline">\(p\)</span> (the number of measured attributes)</p>
<p><strong>Restrictions on the model</strong></p>
<ul>
<li><p><span class="math inline">\(E(\epsilon) = \mathbf{0}\)</span></p></li>
<li><p><span class="math inline">\(var(\epsilon) = \Psi_{p \times p} = diag( \psi_1, \dots, \psi_p)\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\epsilon}, \mathbf{f}\)</span> are independent</p></li>
<li><p>Additional assumption could be <span class="math inline">\(E(\mathbf{f}) = \mathbf{0}, var(\mathbf{f}) = \mathbf{I}_{m \times m}\)</span> (known as the orthogonal factor model) , which imposes the following covariance structure on <span class="math inline">\(\mathbf{y}\)</span></p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
var(\mathbf{y}) = \mathbf{\Sigma} &amp;=  var(\mathbf{Lf} + \mathbf{\epsilon}) \\
&amp;= var(\mathbf{Lf}) + var(\epsilon) \\
&amp;= \mathbf{L} var(\mathbf{f}) \mathbf{L}' + \mathbf{\Psi} \\
&amp;= \mathbf{LIL}' + \mathbf{\Psi} \\
&amp;= \mathbf{LL}' + \mathbf{\Psi}
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{\Psi}\)</span> is diagonal, the off-diagonal elements of <span class="math inline">\(\mathbf{LL}'\)</span> are <span class="math inline">\(\sigma_{ij}\)</span>, the co variances in <span class="math inline">\(\mathbf{\Sigma}\)</span>, which means <span class="math inline">\(cov(y_i, y_j) = \sum_{k=1}^m l_{ik}l_{jk}\)</span> and the covariance of <span class="math inline">\(\mathbf{y}\)</span> is completely determined by the m factors ( <span class="math inline">\(m &lt;&lt;p\)</span>)</p>
<p><span class="math inline">\(var(y_i) = \sum_{k=1}^m l_{ik}^2 + \psi_i\)</span> where <span class="math inline">\(\psi_i\)</span> is the <strong>specific variance</strong> and the summation term is the i-th <strong>communality</strong> (i.e., portion of the variance of the i-th variable contributed by the <span class="math inline">\(m\)</span> common factors (<span class="math inline">\(h_i^2 = \sum_{k=1}^m l_{ik}^2\)</span>)</p>
<p>The factor model is only uniquely determined up to an orthogonal transformation of the factors.</p>
<p>Let <span class="math inline">\(\mathbf{T}_{m \times m}\)</span> be an orthogonal matrix <span class="math inline">\(\mathbf{TT}' = \mathbf{T'T} = \mathbf{I}\)</span> then</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} - \mathbf{\mu} &amp;= \mathbf{Lf} + \epsilon \\
&amp;= \mathbf{LTT'f} + \epsilon \\
&amp;= \mathbf{L}^*(\mathbf{T'f}) + \epsilon &amp; \text{where } \mathbf{L}^* = \mathbf{LT}
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{\Sigma} &amp;= \mathbf{LL}' + \mathbf{\Psi} \\
&amp;= \mathbf{LTT'L} + \mathbf{\Psi} \\
&amp;= (\mathbf{L}^*)(\mathbf{L}^*)' + \mathbf{\Psi}
\end{aligned}
\]</span></p>
<p>Hence, any orthogonal transformation of the factors is an equally good description of the correlations among the observed traits.</p>
<p>Let <span class="math inline">\(\mathbf{y} = \mathbf{Cx}\)</span> , where <span class="math inline">\(\mathbf{C}\)</span> is any diagonal matrix, then <span class="math inline">\(\mathbf{L}_y = \mathbf{CL}_x\)</span> and <span class="math inline">\(\mathbf{\Psi}_y = \mathbf{C\Psi}_x\mathbf{C}\)</span></p>
<p>Hence, we can see that factor analysis is also invariant to changes in scale</p>
<div id="methods-of-estimation" class="section level3" number="22.3.1">
<h3>
<span class="header-section-number">22.3.1</span> Methods of Estimation<a class="anchor" aria-label="anchor" href="#methods-of-estimation"><i class="fas fa-link"></i></a>
</h3>
<p>To estimate <span class="math inline">\(\mathbf{L}\)</span></p>
<ol style="list-style-type: decimal">
<li><a href="multivariate-methods.html#principal-component-method">Principal Component Method</a></li>
<li><a href="multivariate-methods.html#principal-factor-method">Principal Factor Method</a></li>
<li><a href="multivariate-methods.html#maximum-likelihood-method-factor-analysis">22.3.1.3</a></li>
</ol>
<div id="principal-component-method" class="section level4" number="22.3.1.1">
<h4>
<span class="header-section-number">22.3.1.1</span> Principal Component Method<a class="anchor" aria-label="anchor" href="#principal-component-method"><i class="fas fa-link"></i></a>
</h4>
<p>Spectral decomposition</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{\Sigma} &amp;= \lambda_1 \mathbf{a}_1 \mathbf{a}_1' + \dots + \lambda_p \mathbf{a}_p \mathbf{a}_p' \\
&amp;= \mathbf{A\Lambda A}' \\
&amp;= \sum_{k=1}^m \lambda+k \mathbf{a}_k \mathbf{a}_k' + \sum_{k= m+1}^p \lambda_k \mathbf{a}_k \mathbf{a}_k' \\
&amp;= \sum_{k=1}^m l_k l_k' + \sum_{k=m+1}^p \lambda_k \mathbf{a}_k \mathbf{a}_k'
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(l_k = \mathbf{a}_k \sqrt{\lambda_k}\)</span> and the second term is not diagonal in general.</p>
<p>Assume</p>
<p><span class="math display">\[
\psi_i = \sigma_{ii} - \sum_{k=1}^m l_{ik}^2 = \sigma_{ii} -  \sum_{k=1}^m \lambda_i a_{ik}^2
\]</span></p>
<p>then</p>
<p><span class="math display">\[
\mathbf{\Sigma} \approx \mathbf{LL}' + \mathbf{\Psi}
\]</span></p>
<p>To estimate <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\Psi\)</span> , we use the expected eigenvalues and eigenvectors from <span class="math inline">\(\mathbf{S}\)</span> or <span class="math inline">\(\mathbf{R}\)</span></p>
<ul>
<li><p>The estimated factor loadings don’t change as the number of actors increases</p></li>
<li><p>The diagonal elements of <span class="math inline">\(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}\)</span> are equal to the diagonal elements of <span class="math inline">\(\mathbf{S}\)</span> and <span class="math inline">\(\mathbf{R}\)</span>, but the covariances may not be exactly reproduced</p></li>
<li><p>We select <span class="math inline">\(m\)</span> so that the off-diagonal elements close to the values in <span class="math inline">\(\mathbf{S}\)</span> (or to make the off-diagonal elements of <span class="math inline">\(\mathbf{S} - \hat{\mathbf{L}} \hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}\)</span> small)</p></li>
</ul>
</div>
<div id="principal-factor-method" class="section level4" number="22.3.1.2">
<h4>
<span class="header-section-number">22.3.1.2</span> Principal Factor Method<a class="anchor" aria-label="anchor" href="#principal-factor-method"><i class="fas fa-link"></i></a>
</h4>
<p>Consider modeling the correlation matrix, <span class="math inline">\(\mathbf{R} = \mathbf{L} \mathbf{L}' + \mathbf{\Psi}\)</span> . Then</p>
<p><span class="math display">\[
\mathbf{L} \mathbf{L}' = \mathbf{R} - \mathbf{\Psi} =
\left(
\begin{array}
{cccc}
h_1^2 &amp; r_{12} &amp; \dots &amp; r_{1p} \\
r_{21} &amp; h_2^2 &amp; \dots &amp; r_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
r_{p1} &amp; r_{p2} &amp; \dots &amp; h_p^2
\end{array}
\right)
\]</span></p>
<p>where <span class="math inline">\(h_i^2 = 1- \psi_i\)</span> (the communality)</p>
<p>Suppose that initial estimates are available for the communalities, <span class="math inline">\((h_1^*)^2,(h_2^*)^2, \dots , (h_p^*)^2\)</span>, then we can regress each trait on all the others, and then use the <span class="math inline">\(r^2\)</span> as <span class="math inline">\(h^2\)</span></p>
<p>The estimate of <span class="math inline">\(\mathbf{R} - \mathbf{\Psi}\)</span> at step k is</p>
<p><span class="math display">\[
(\mathbf{R} - \mathbf{\Psi})_k =
\left(
\begin{array}
{cccc}
(h_1^*)^2 &amp; r_{12} &amp; \dots &amp; r_{1p} \\
r_{21} &amp; (h_2^*)^2 &amp; \dots &amp; r_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
r_{p1} &amp; r_{p2} &amp; \dots &amp; (h_p^*)^2
\end{array}
\right) =
\mathbf{L}_k^*(\mathbf{L}_k^*)'
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathbf{L}_k^* = (\sqrt{\hat{\lambda}_1^*\hat{\mathbf{a}}_1^* , \dots \hat{\lambda}_m^*\hat{\mathbf{a}}_m^*})
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\hat{\psi}_{i,k}^* = 1 - \sum_{j=1}^m \hat{\lambda}_i^* (\hat{a}_{ij}^*)^2
\]</span></p>
<p>we used the spectral decomposition on the estimated matrix <span class="math inline">\((\mathbf{R}- \mathbf{\Psi})\)</span> to calculate the <span class="math inline">\(\hat{\lambda}_i^* s\)</span> and the <span class="math inline">\(\mathbf{\hat{a}}_i^* s\)</span></p>
<p>After updating the values of <span class="math inline">\((\hat{h}_i^*)^2 = 1 - \hat{\psi}_{i,k}^*\)</span> we will use them to form a new <span class="math inline">\(\mathbf{L}_{k+1}^*\)</span> via another spectral decomposition. Repeat the process</p>
<p>Notes:</p>
<ul>
<li><p>The matrix <span class="math inline">\((\mathbf{R} - \mathbf{\Psi})_k\)</span> is not necessarily positive definite</p></li>
<li><p>The principal component method is similar to principal factor if one considers the initial communalities are <span class="math inline">\(h^2 = 1\)</span></p></li>
<li>
<p>if <span class="math inline">\(m\)</span> is too large, some communalities may become larger than 1, causing the iterations to terminate. To combat, we can</p>
<ul>
<li><p>fix any communality that is greater than 1 at 1 and then continues.</p></li>
<li><p>continue iterations regardless of the size of the communalities. However, results can be outside fo the parameter space.</p></li>
</ul>
</li>
</ul>
</div>
<div id="maximum-likelihood-method-factor-analysis" class="section level4" number="22.3.1.3">
<h4>
<span class="header-section-number">22.3.1.3</span> Maximum Likelihood Method<a class="anchor" aria-label="anchor" href="#maximum-likelihood-method-factor-analysis"><i class="fas fa-link"></i></a>
</h4>
<p>Since we need the likelihood function, we make the additional (critical) assumption that</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y}_j \sim N(\mathbf{\mu},\mathbf{\Sigma})\)</span> for <span class="math inline">\(j = 1,..,n\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{f} \sim N(\mathbf{0}, \mathbf{I})\)</span></p></li>
<li><p><span class="math inline">\(\epsilon_j \sim N(\mathbf{0}, \mathbf{\Psi})\)</span></p></li>
</ul>
<p>and restriction</p>
<ul>
<li>
<span class="math inline">\(\mathbf{L}' \mathbf{\Psi}^{-1}\mathbf{L} = \mathbf{\Delta}\)</span> where <span class="math inline">\(\mathbf{\Delta}\)</span> is a diagonal matrix. (since the factor loading matrix is not unique, we need this restriction).</li>
</ul>
<p>Notes:</p>
<ul>
<li><p>Finding MLE can be computationally expensive</p></li>
<li><p>we typically use other methods for exploratory data analysis</p></li>
<li><p>Likelihood ratio tests could be used for testing hypotheses in this framework (i.e., Confirmatory Factor Analysis)</p></li>
</ul>
</div>
</div>
<div id="factor-rotation" class="section level3" number="22.3.2">
<h3>
<span class="header-section-number">22.3.2</span> Factor Rotation<a class="anchor" aria-label="anchor" href="#factor-rotation"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(\mathbf{T}_{m \times m}\)</span> is an orthogonal matrix that has the property that</p>
<p><span class="math display">\[
\hat{\mathbf{L}} \hat{\mathbf{L}}' + \hat{\mathbf{\Psi}} = \hat{\mathbf{L}}^*(\hat{\mathbf{L}}^*)' + \hat{\mathbf{\Psi}}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{L}^* = \mathbf{LT}\)</span></p>
<p>This means that estimated specific variances and communalities are not altered by the orthogonal transformation.</p>
<p>Since there are an infinite number of choices for <span class="math inline">\(\mathbf{T}\)</span>, some selection criterion is necessary</p>
<p>For example, we can find the orthogonal transformation that maximizes the objective function</p>
<p><span class="math display">\[
\sum_{j = 1}^m [\frac{1}{p}\sum_{i=1}^p (\frac{l_{ij}^{*2}}{h_i})^2 - \{\frac{\gamma}{p} \sum_{i=1}^p (\frac{l_{ij}^{*2}}{h_i})^2 \}^2]
\]</span></p>
<p>where <span class="math inline">\(\frac{l_{ij}^{*2}}{h_i}\)</span> are “scaled loadings”, which gives variables with small communalities more influence.</p>
<p>Different choices of <span class="math inline">\(\gamma\)</span> in the objective function correspond to different orthogonal rotation found in the literature;</p>
<ol style="list-style-type: decimal">
<li><p>Varimax <span class="math inline">\(\gamma = 1\)</span> (rotate the factors so that each of the <span class="math inline">\(p\)</span> variables should have a high loading on only one factor, but this is not always possible).</p></li>
<li><p>Quartimax <span class="math inline">\(\gamma = 0\)</span></p></li>
<li><p>Equimax <span class="math inline">\(\gamma = m/2\)</span></p></li>
<li><p>Parsimax <span class="math inline">\(\gamma = \frac{p(m-1)}{p+m-2}\)</span></p></li>
<li><p>Promax: non-orthogonal or olique transformations</p></li>
<li><p>Harris-Kaiser (HK): non-orthogonal or oblique transformations</p></li>
</ol>
</div>
<div id="estimation-of-factor-scores" class="section level3" number="22.3.3">
<h3>
<span class="header-section-number">22.3.3</span> Estimation of Factor Scores<a class="anchor" aria-label="anchor" href="#estimation-of-factor-scores"><i class="fas fa-link"></i></a>
</h3>
<p>Recall</p>
<p><span class="math display">\[
(\mathbf{y}_j - \mathbf{\mu}) = \mathbf{L}_{p \times m}\mathbf{f}_j + \epsilon_j
\]</span></p>
<p>If the factor model is correct then</p>
<p><span class="math display">\[
var(\epsilon_j) = \mathbf{\Psi} = diag (\psi_1, \dots , \psi_p)
\]</span></p>
<p>Thus we could consider using weighted least squares to estimate <span class="math inline">\(\mathbf{f}_j\)</span> , the vector of factor scores for the j-th sampled unit by</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\mathbf{f}} &amp;= (\mathbf{L}'\mathbf{\Psi}^{-1} \mathbf{L})^{-1} \mathbf{L}' \mathbf{\Psi}^{-1}(\mathbf{y}_j - \mathbf{\mu}) \\
&amp; \approx (\mathbf{L}'\mathbf{\Psi}^{-1} \mathbf{L})^{-1} \mathbf{L}' \mathbf{\Psi}^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\end{aligned}
\]</span></p>
<div id="the-regression-method" class="section level4" number="22.3.3.1">
<h4>
<span class="header-section-number">22.3.3.1</span> The Regression Method<a class="anchor" aria-label="anchor" href="#the-regression-method"><i class="fas fa-link"></i></a>
</h4>
<p>Alternatively, we can use the regression method to estimate the factor scores</p>
<p>Consider the joint distribution of <span class="math inline">\((\mathbf{y}_j - \mathbf{\mu})\)</span> and <span class="math inline">\(\mathbf{f}_j\)</span> assuming multivariate normality, as in the maximum likelihood approach. then,</p>
<p><span class="math display">\[
\left(
\begin{array}
{c}
\mathbf{y}_j - \mathbf{\mu} \\
\mathbf{f}_j
\end{array}
\right) \sim
N_{p + m}
\left(
\left[
\begin{array}
{cc}
\mathbf{LL}' + \mathbf{\Psi} &amp; \mathbf{L} \\
\mathbf{L}' &amp; \mathbf{I}_{m\times m}
\end{array}
\right]
\right)
\]</span></p>
<p>when the <span class="math inline">\(m\)</span> factor model is correct</p>
<p>Hence,</p>
<p><span class="math display">\[
E(\mathbf{f}_j | \mathbf{y}_j - \mathbf{\mu}) = \mathbf{L}' (\mathbf{LL}' + \mathbf{\Psi})^{-1}(\mathbf{y}_j - \mathbf{\mu})
\]</span></p>
<p>notice that <span class="math inline">\(\mathbf{L}' (\mathbf{LL}' + \mathbf{\Psi})^{-1}\)</span> is an <span class="math inline">\(m \times p\)</span> matrix of regression coefficients</p>
<p>Then, we use the estimated conditional mean vector to estimate the factor scores</p>
<p><span class="math display">\[
\mathbf{\hat{f}}_j = \mathbf{\hat{L}}'(\mathbf{\hat{L}}\mathbf{\hat{L}}' + \mathbf{\hat{\Psi}})^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\]</span></p>
<p>Alternatively, we could reduce the effect of possible incorrect determination fo the number of factors <span class="math inline">\(m\)</span> by using <span class="math inline">\(\mathbf{S}\)</span> as a substitute for <span class="math inline">\(\mathbf{\hat{L}}\mathbf{\hat{L}}' + \mathbf{\hat{\Psi}}\)</span> then</p>
<p><span class="math display">\[
\mathbf{\hat{f}}_j = \mathbf{\hat{L}}'\mathbf{S}^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\]</span></p>
<p>where <span class="math inline">\(j = 1,\dots,n\)</span></p>
</div>
</div>
<div id="model-diagnostic" class="section level3" number="22.3.4">
<h3>
<span class="header-section-number">22.3.4</span> Model Diagnostic<a class="anchor" aria-label="anchor" href="#model-diagnostic"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>Plots</p></li>
<li><p>Check for outliers (recall that <span class="math inline">\(\mathbf{f}_j \sim iid N(\mathbf{0}, \mathbf{I}_{m \times m})\)</span>)</p></li>
<li><p>Check for multivariate normality assumption</p></li>
<li><p>Use univariate tests for normality to check the factor scores</p></li>
<li><p><strong>Confirmatory Factor Analysis</strong>: formal testing of hypotheses about loadings, use MLE and full/reduced model testing paradigm and measures of model fit</p></li>
</ul>
</div>
<div id="application-11" class="section level3" number="22.3.5">
<h3>
<span class="header-section-number">22.3.5</span> Application<a class="anchor" aria-label="anchor" href="#application-11"><i class="fas fa-link"></i></a>
</h3>
<p>In the <code>psych</code> package,</p>
<ul>
<li><p>h2 = the communalities</p></li>
<li><p>u2 = the uniqueness</p></li>
<li><p>com = the complexity</p></li>
</ul>
<div class="sourceCode" id="cb389"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://personality-project.org/r/psych/">psych</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="co">## Load the data from the psych package</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Harman.5</span><span class="op">)</span></span>
<span><span class="va">Harman.5</span></span>
<span><span class="co">#&gt;         population schooling employment professional housevalue</span></span>
<span><span class="co">#&gt; Tract1        5700      12.8       2500          270      25000</span></span>
<span><span class="co">#&gt; Tract2        1000      10.9        600           10      10000</span></span>
<span><span class="co">#&gt; Tract3        3400       8.8       1000           10       9000</span></span>
<span><span class="co">#&gt; Tract4        3800      13.6       1700          140      25000</span></span>
<span><span class="co">#&gt; Tract5        4000      12.8       1600          140      25000</span></span>
<span><span class="co">#&gt; Tract6        8200       8.3       2600           60      12000</span></span>
<span><span class="co">#&gt; Tract7        1200      11.4        400           10      16000</span></span>
<span><span class="co">#&gt; Tract8        9100      11.5       3300           60      14000</span></span>
<span><span class="co">#&gt; Tract9        9900      12.5       3400          180      18000</span></span>
<span><span class="co">#&gt; Tract10       9600      13.7       3600          390      25000</span></span>
<span><span class="co">#&gt; Tract11       9600       9.6       3300           80      12000</span></span>
<span><span class="co">#&gt; Tract12       9400      11.4       4000          100      13000</span></span>
<span></span>
<span><span class="co"># Correlation matrix</span></span>
<span><span class="va">cor_mat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Harman.5</span><span class="op">)</span></span>
<span><span class="va">cor_mat</span></span>
<span><span class="co">#&gt;              population  schooling employment professional housevalue</span></span>
<span><span class="co">#&gt; population   1.00000000 0.00975059  0.9724483    0.4388708 0.02241157</span></span>
<span><span class="co">#&gt; schooling    0.00975059 1.00000000  0.1542838    0.6914082 0.86307009</span></span>
<span><span class="co">#&gt; employment   0.97244826 0.15428378  1.0000000    0.5147184 0.12192599</span></span>
<span><span class="co">#&gt; professional 0.43887083 0.69140824  0.5147184    1.0000000 0.77765425</span></span>
<span><span class="co">#&gt; housevalue   0.02241157 0.86307009  0.1219260    0.7776543 1.00000000</span></span>
<span></span>
<span><span class="co">## Principal Component Method with Correlation</span></span>
<span><span class="va">cor_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">Harman.5</span>, scale <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span>
<span><span class="co"># eigen values</span></span>
<span><span class="va">cor_results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>eigen_values <span class="op">=</span> <span class="va">cor_pca</span><span class="op">$</span><span class="va">sdev</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">cor_results</span> <span class="op">&lt;-</span> <span class="va">cor_results</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span></span>
<span>        proportion <span class="op">=</span> <span class="va">eigen_values</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">eigen_values</span><span class="op">)</span>,</span>
<span>        cumulative <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">proportion</span><span class="op">)</span>,</span>
<span>        number <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/row_number.html">row_number</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="va">cor_results</span></span>
<span><span class="co">#&gt;   eigen_values  proportion cumulative number</span></span>
<span><span class="co">#&gt; 1   2.87331359 0.574662719  0.5746627      1</span></span>
<span><span class="co">#&gt; 2   1.79666009 0.359332019  0.9339947      2</span></span>
<span><span class="co">#&gt; 3   0.21483689 0.042967377  0.9769621      3</span></span>
<span><span class="co">#&gt; 4   0.09993405 0.019986811  0.9969489      4</span></span>
<span><span class="co">#&gt; 5   0.01525537 0.003051075  1.0000000      5</span></span>
<span></span>
<span><span class="co"># Scree plot of Eigenvalues</span></span>
<span><span class="va">scree_gg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">cor_results</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">number</span>, y <span class="op">=</span> <span class="va">eigen_values</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html">geom_text</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">number</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_continuous</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Number"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_y_continuous</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Eigenvalue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">scree_gg</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-18-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb390"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/screeplot.html">screeplot</a></span><span class="op">(</span><span class="va">cor_pca</span>, type <span class="op">=</span> <span class="st">'lines'</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-18-2.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb391"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co">## Keep 2 factors based on scree plot and eigenvalues</span></span>
<span><span class="va">factor_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/principal.html">principal</a></span><span class="op">(</span><span class="va">Harman.5</span>, nfactors <span class="op">=</span> <span class="fl">2</span>, rotate <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></span>
<span><span class="va">factor_pca</span></span>
<span><span class="co">#&gt; Principal Components Analysis</span></span>
<span><span class="co">#&gt; Call: principal(r = Harman.5, nfactors = 2, rotate = "none")</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;               PC1   PC2   h2    u2 com</span></span>
<span><span class="co">#&gt; population   0.58  0.81 0.99 0.012 1.8</span></span>
<span><span class="co">#&gt; schooling    0.77 -0.54 0.89 0.115 1.8</span></span>
<span><span class="co">#&gt; employment   0.67  0.73 0.98 0.021 2.0</span></span>
<span><span class="co">#&gt; professional 0.93 -0.10 0.88 0.120 1.0</span></span>
<span><span class="co">#&gt; housevalue   0.79 -0.56 0.94 0.062 1.8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;                        PC1  PC2</span></span>
<span><span class="co">#&gt; SS loadings           2.87 1.80</span></span>
<span><span class="co">#&gt; Proportion Var        0.57 0.36</span></span>
<span><span class="co">#&gt; Cumulative Var        0.57 0.93</span></span>
<span><span class="co">#&gt; Proportion Explained  0.62 0.38</span></span>
<span><span class="co">#&gt; Cumulative Proportion 0.62 1.00</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mean item complexity =  1.7</span></span>
<span><span class="co">#&gt; Test of the hypothesis that 2 components are sufficient.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The root mean square of the residuals (RMSR) is  0.03 </span></span>
<span><span class="co">#&gt;  with the empirical chi square  0.29  with prob &lt;  0.59 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fit based upon off diagonal values = 1</span></span>
<span></span>
<span><span class="co"># factor 1 = overall socioeconomic health</span></span>
<span><span class="co"># factor 2 = contrast of the population and employment against school and house value</span></span>
<span></span>
<span></span>
<span><span class="co">## Ssquared multiple correlation (SMC) prior, no rotation</span></span>
<span><span class="va">factor_pca_smc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span></span>
<span>    <span class="va">Harman.5</span>,</span>
<span>    nfactors <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    fm <span class="op">=</span> <span class="st">"pa"</span>,</span>
<span>    rotate <span class="op">=</span> <span class="st">"none"</span>,</span>
<span>    SMC <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">factor_pca_smc</span></span>
<span><span class="co">#&gt; Factor Analysis using method =  pa</span></span>
<span><span class="co">#&gt; Call: fa(r = Harman.5, nfactors = 2, rotate = "none", SMC = TRUE, fm = "pa")</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;               PA1   PA2   h2      u2 com</span></span>
<span><span class="co">#&gt; population   0.62  0.78 1.00 -0.0027 1.9</span></span>
<span><span class="co">#&gt; schooling    0.70 -0.53 0.77  0.2277 1.9</span></span>
<span><span class="co">#&gt; employment   0.70  0.68 0.96  0.0413 2.0</span></span>
<span><span class="co">#&gt; professional 0.88 -0.15 0.80  0.2017 1.1</span></span>
<span><span class="co">#&gt; housevalue   0.78 -0.60 0.96  0.0361 1.9</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;                        PA1  PA2</span></span>
<span><span class="co">#&gt; SS loadings           2.76 1.74</span></span>
<span><span class="co">#&gt; Proportion Var        0.55 0.35</span></span>
<span><span class="co">#&gt; Cumulative Var        0.55 0.90</span></span>
<span><span class="co">#&gt; Proportion Explained  0.61 0.39</span></span>
<span><span class="co">#&gt; Cumulative Proportion 0.61 1.00</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mean item complexity =  1.7</span></span>
<span><span class="co">#&gt; Test of the hypothesis that 2 factors are sufficient.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; df null model =  10  with the objective function =  6.38 with Chi Square =  54.25</span></span>
<span><span class="co">#&gt; df of  the model are 1  and the objective function was  0.34 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The root mean square of the residuals (RMSR) is  0.01 </span></span>
<span><span class="co">#&gt; The df corrected root mean square of the residuals is  0.03 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The harmonic n.obs is  12 with the empirical chi square  0.02  with prob &lt;  0.88 </span></span>
<span><span class="co">#&gt; The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob &lt;  0.12 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Tucker Lewis Index of factoring reliability =  0.596</span></span>
<span><span class="co">#&gt; RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967</span></span>
<span><span class="co">#&gt; BIC =  -0.04</span></span>
<span><span class="co">#&gt; Fit based upon off diagonal values = 1</span></span>
<span></span>
<span><span class="co">## SMC prior, Promax rotation</span></span>
<span><span class="va">factor_pca_smc_pro</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span></span>
<span>    <span class="va">Harman.5</span>,</span>
<span>    nfactors <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    fm <span class="op">=</span> <span class="st">"pa"</span>,</span>
<span>    rotate <span class="op">=</span> <span class="st">"Promax"</span>,</span>
<span>    SMC <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">factor_pca_smc_pro</span></span>
<span><span class="co">#&gt; Factor Analysis using method =  pa</span></span>
<span><span class="co">#&gt; Call: fa(r = Harman.5, nfactors = 2, rotate = "Promax", SMC = TRUE, </span></span>
<span><span class="co">#&gt;     fm = "pa")</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;                PA1   PA2   h2      u2 com</span></span>
<span><span class="co">#&gt; population   -0.11  1.02 1.00 -0.0027 1.0</span></span>
<span><span class="co">#&gt; schooling     0.90 -0.11 0.77  0.2277 1.0</span></span>
<span><span class="co">#&gt; employment    0.02  0.97 0.96  0.0413 1.0</span></span>
<span><span class="co">#&gt; professional  0.75  0.33 0.80  0.2017 1.4</span></span>
<span><span class="co">#&gt; housevalue    1.01 -0.14 0.96  0.0361 1.0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;                        PA1  PA2</span></span>
<span><span class="co">#&gt; SS loadings           2.38 2.11</span></span>
<span><span class="co">#&gt; Proportion Var        0.48 0.42</span></span>
<span><span class="co">#&gt; Cumulative Var        0.48 0.90</span></span>
<span><span class="co">#&gt; Proportion Explained  0.53 0.47</span></span>
<span><span class="co">#&gt; Cumulative Proportion 0.53 1.00</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  With factor correlations of </span></span>
<span><span class="co">#&gt;      PA1  PA2</span></span>
<span><span class="co">#&gt; PA1 1.00 0.25</span></span>
<span><span class="co">#&gt; PA2 0.25 1.00</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mean item complexity =  1.1</span></span>
<span><span class="co">#&gt; Test of the hypothesis that 2 factors are sufficient.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; df null model =  10  with the objective function =  6.38 with Chi Square =  54.25</span></span>
<span><span class="co">#&gt; df of  the model are 1  and the objective function was  0.34 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The root mean square of the residuals (RMSR) is  0.01 </span></span>
<span><span class="co">#&gt; The df corrected root mean square of the residuals is  0.03 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The harmonic n.obs is  12 with the empirical chi square  0.02  with prob &lt;  0.88 </span></span>
<span><span class="co">#&gt; The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob &lt;  0.12 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Tucker Lewis Index of factoring reliability =  0.596</span></span>
<span><span class="co">#&gt; RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967</span></span>
<span><span class="co">#&gt; BIC =  -0.04</span></span>
<span><span class="co">#&gt; Fit based upon off diagonal values = 1</span></span>
<span></span>
<span><span class="co">## SMC prior, varimax rotation</span></span>
<span><span class="va">factor_pca_smc_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span></span>
<span>    <span class="va">Harman.5</span>,</span>
<span>    nfactors <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    fm <span class="op">=</span> <span class="st">"pa"</span>,</span>
<span>    rotate <span class="op">=</span> <span class="st">"varimax"</span>,</span>
<span>    SMC <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">## Make a data frame of the loadings for ggplot2</span></span>
<span><span class="va">factors_df</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_rows.html">bind_rows</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>            y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">factor_pca_smc</span><span class="op">$</span><span class="va">loadings</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/class.html">unclass</a></span><span class="op">(</span><span class="va">factor_pca_smc</span><span class="op">$</span><span class="va">loadings</span><span class="op">)</span></span>
<span>        <span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>            y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">factor_pca_smc_pro</span><span class="op">$</span><span class="va">loadings</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/class.html">unclass</a></span><span class="op">(</span><span class="va">factor_pca_smc_pro</span><span class="op">$</span><span class="va">loadings</span><span class="op">)</span></span>
<span>        <span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>            y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">factor_pca_smc_var</span><span class="op">$</span><span class="va">loadings</span><span class="op">)</span>,</span>
<span>            <span class="fu"><a href="https://rdrr.io/r/base/class.html">unclass</a></span><span class="op">(</span><span class="va">factor_pca_smc_var</span><span class="op">$</span><span class="va">loadings</span><span class="op">)</span></span>
<span>        <span class="op">)</span>,</span>
<span>        .id <span class="op">=</span> <span class="st">"Rotation"</span></span>
<span>    <span class="op">)</span></span>
<span><span class="va">flag_gg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">factors_df</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span></span>
<span>        x <span class="op">=</span> <span class="va">PA2</span>,</span>
<span>        y <span class="op">=</span> <span class="va">PA1</span>,</span>
<span>        col <span class="op">=</span> <span class="va">y</span>,</span>
<span>        shape <span class="op">=</span> <span class="va">y</span></span>
<span>    <span class="op">)</span>, size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_continuous</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Factor 2"</span>, limits <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.1</span>, <span class="fl">1.1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_y_continuous</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"Factor1"</span>, limits <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.1</span>, <span class="fl">1.1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="st">"Rotation"</span>, labeller <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labeller.html">labeller</a></span><span class="op">(</span>Rotation <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>        <span class="st">"1"</span> <span class="op">=</span> <span class="st">"Original"</span>, <span class="st">"2"</span> <span class="op">=</span> <span class="st">"Promax"</span>, <span class="st">"3"</span> <span class="op">=</span> <span class="st">"Varimax"</span></span>
<span>    <span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_fixed.html">coord_fixed</a></span><span class="op">(</span>ratio <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="co"># make aspect ratio of each facet 1</span></span>
<span></span>
<span><span class="va">flag_gg</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-18-3.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb392"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># promax and varimax did a good job to assign trait to a particular factor</span></span>
<span></span>
<span><span class="va">factor_mle_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span></span>
<span>    <span class="va">Harman.5</span>,</span>
<span>    nfactors <span class="op">=</span> <span class="fl">1</span>,</span>
<span>    fm <span class="op">=</span> <span class="st">"mle"</span>,</span>
<span>    rotate <span class="op">=</span> <span class="st">"none"</span>,</span>
<span>    SMC <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">factor_mle_1</span></span>
<span><span class="co">#&gt; Factor Analysis using method =  ml</span></span>
<span><span class="co">#&gt; Call: fa(r = Harman.5, nfactors = 1, rotate = "none", SMC = TRUE, fm = "mle")</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;               ML1    h2     u2 com</span></span>
<span><span class="co">#&gt; population   0.97 0.950 0.0503   1</span></span>
<span><span class="co">#&gt; schooling    0.14 0.021 0.9791   1</span></span>
<span><span class="co">#&gt; employment   1.00 0.995 0.0049   1</span></span>
<span><span class="co">#&gt; professional 0.51 0.261 0.7388   1</span></span>
<span><span class="co">#&gt; housevalue   0.12 0.014 0.9864   1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;                 ML1</span></span>
<span><span class="co">#&gt; SS loadings    2.24</span></span>
<span><span class="co">#&gt; Proportion Var 0.45</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mean item complexity =  1</span></span>
<span><span class="co">#&gt; Test of the hypothesis that 1 factor is sufficient.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; df null model =  10  with the objective function =  6.38 with Chi Square =  54.25</span></span>
<span><span class="co">#&gt; df of  the model are 5  and the objective function was  3.14 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The root mean square of the residuals (RMSR) is  0.41 </span></span>
<span><span class="co">#&gt; The df corrected root mean square of the residuals is  0.57 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The harmonic n.obs is  12 with the empirical chi square  39.41  with prob &lt;  2e-07 </span></span>
<span><span class="co">#&gt; The total n.obs was  12  with Likelihood Chi Square =  24.56  with prob &lt;  0.00017 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Tucker Lewis Index of factoring reliability =  0.022</span></span>
<span><span class="co">#&gt; RMSEA index =  0.564  and the 90 % confidence intervals are  0.374 0.841</span></span>
<span><span class="co">#&gt; BIC =  12.14</span></span>
<span><span class="co">#&gt; Fit based upon off diagonal values = 0.5</span></span>
<span><span class="co">#&gt; Measures of factor score adequacy             </span></span>
<span><span class="co">#&gt;                                                    ML1</span></span>
<span><span class="co">#&gt; Correlation of (regression) scores with factors   1.00</span></span>
<span><span class="co">#&gt; Multiple R square of scores with factors          1.00</span></span>
<span><span class="co">#&gt; Minimum correlation of possible factor scores     0.99</span></span>
<span></span>
<span><span class="va">factor_mle_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span></span>
<span>    <span class="va">Harman.5</span>,</span>
<span>    nfactors <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    fm <span class="op">=</span> <span class="st">"mle"</span>,</span>
<span>    rotate <span class="op">=</span> <span class="st">"none"</span>,</span>
<span>    SMC <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">factor_mle_2</span></span>
<span><span class="co">#&gt; Factor Analysis using method =  ml</span></span>
<span><span class="co">#&gt; Call: fa(r = Harman.5, nfactors = 2, rotate = "none", SMC = TRUE, fm = "mle")</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;                ML2  ML1   h2    u2 com</span></span>
<span><span class="co">#&gt; population   -0.03 1.00 1.00 0.005 1.0</span></span>
<span><span class="co">#&gt; schooling     0.90 0.04 0.81 0.193 1.0</span></span>
<span><span class="co">#&gt; employment    0.09 0.98 0.96 0.036 1.0</span></span>
<span><span class="co">#&gt; professional  0.78 0.46 0.81 0.185 1.6</span></span>
<span><span class="co">#&gt; housevalue    0.96 0.05 0.93 0.074 1.0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;                        ML2  ML1</span></span>
<span><span class="co">#&gt; SS loadings           2.34 2.16</span></span>
<span><span class="co">#&gt; Proportion Var        0.47 0.43</span></span>
<span><span class="co">#&gt; Cumulative Var        0.47 0.90</span></span>
<span><span class="co">#&gt; Proportion Explained  0.52 0.48</span></span>
<span><span class="co">#&gt; Cumulative Proportion 0.52 1.00</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mean item complexity =  1.1</span></span>
<span><span class="co">#&gt; Test of the hypothesis that 2 factors are sufficient.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; df null model =  10  with the objective function =  6.38 with Chi Square =  54.25</span></span>
<span><span class="co">#&gt; df of  the model are 1  and the objective function was  0.31 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The root mean square of the residuals (RMSR) is  0.01 </span></span>
<span><span class="co">#&gt; The df corrected root mean square of the residuals is  0.05 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The harmonic n.obs is  12 with the empirical chi square  0.05  with prob &lt;  0.82 </span></span>
<span><span class="co">#&gt; The total n.obs was  12  with Likelihood Chi Square =  2.22  with prob &lt;  0.14 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Tucker Lewis Index of factoring reliability =  0.658</span></span>
<span><span class="co">#&gt; RMSEA index =  0.307  and the 90 % confidence intervals are  0 0.945</span></span>
<span><span class="co">#&gt; BIC =  -0.26</span></span>
<span><span class="co">#&gt; Fit based upon off diagonal values = 1</span></span>
<span><span class="co">#&gt; Measures of factor score adequacy             </span></span>
<span><span class="co">#&gt;                                                    ML2  ML1</span></span>
<span><span class="co">#&gt; Correlation of (regression) scores with factors   0.98 1.00</span></span>
<span><span class="co">#&gt; Multiple R square of scores with factors          0.95 1.00</span></span>
<span><span class="co">#&gt; Minimum correlation of possible factor scores     0.91 0.99</span></span>
<span></span>
<span><span class="va">factor_mle_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span></span>
<span>    <span class="va">Harman.5</span>,</span>
<span>    nfactors <span class="op">=</span> <span class="fl">3</span>,</span>
<span>    fm <span class="op">=</span> <span class="st">"mle"</span>,</span>
<span>    rotate <span class="op">=</span> <span class="st">"none"</span>,</span>
<span>    SMC <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">factor_mle_3</span></span>
<span><span class="co">#&gt; Factor Analysis using method =  ml</span></span>
<span><span class="co">#&gt; Call: fa(r = Harman.5, nfactors = 3, rotate = "none", SMC = TRUE, fm = "mle")</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;                ML2  ML1   ML3   h2     u2 com</span></span>
<span><span class="co">#&gt; population   -0.12 0.98 -0.11 0.98 0.0162 1.1</span></span>
<span><span class="co">#&gt; schooling     0.89 0.15  0.29 0.90 0.0991 1.3</span></span>
<span><span class="co">#&gt; employment    0.00 1.00  0.04 0.99 0.0052 1.0</span></span>
<span><span class="co">#&gt; professional  0.72 0.52 -0.10 0.80 0.1971 1.9</span></span>
<span><span class="co">#&gt; housevalue    0.97 0.13 -0.09 0.97 0.0285 1.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;                        ML2  ML1  ML3</span></span>
<span><span class="co">#&gt; SS loadings           2.28 2.26 0.11</span></span>
<span><span class="co">#&gt; Proportion Var        0.46 0.45 0.02</span></span>
<span><span class="co">#&gt; Cumulative Var        0.46 0.91 0.93</span></span>
<span><span class="co">#&gt; Proportion Explained  0.49 0.49 0.02</span></span>
<span><span class="co">#&gt; Cumulative Proportion 0.49 0.98 1.00</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mean item complexity =  1.2</span></span>
<span><span class="co">#&gt; Test of the hypothesis that 3 factors are sufficient.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; df null model =  10  with the objective function =  6.38 with Chi Square =  54.25</span></span>
<span><span class="co">#&gt; df of  the model are -2  and the objective function was  0 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The root mean square of the residuals (RMSR) is  0 </span></span>
<span><span class="co">#&gt; The df corrected root mean square of the residuals is  NA </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The harmonic n.obs is  12 with the empirical chi square  0  with prob &lt;  NA </span></span>
<span><span class="co">#&gt; The total n.obs was  12  with Likelihood Chi Square =  0  with prob &lt;  NA </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Tucker Lewis Index of factoring reliability =  1.318</span></span>
<span><span class="co">#&gt; Fit based upon off diagonal values = 1</span></span>
<span><span class="co">#&gt; Measures of factor score adequacy             </span></span>
<span><span class="co">#&gt;                                                    ML2  ML1  ML3</span></span>
<span><span class="co">#&gt; Correlation of (regression) scores with factors   0.99 1.00 0.82</span></span>
<span><span class="co">#&gt; Multiple R square of scores with factors          0.98 1.00 0.68</span></span>
<span><span class="co">#&gt; Minimum correlation of possible factor scores     0.96 0.99 0.36</span></span></code></pre></div>
<p>The output info for the null hypothesis of no common factors is in the statement “The degrees of freedom for the null model ..”</p>
<p>The output info for the null hypothesis that number of factors is sufficient is in the statement “The total number of observations was …”</p>
<p>One factor is not enough, two is sufficient, and not enough data for 3 factors (df of -2 and NA for p-value). Hence, we should use 2-factor model.</p>
</div>
</div>
<div id="discriminant-analysis" class="section level2" number="22.4">
<h2>
<span class="header-section-number">22.4</span> Discriminant Analysis<a class="anchor" aria-label="anchor" href="#discriminant-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>Suppose we have two or more different populations from which observations could come from. Discriminant analysis seeks to determine which of the possible population an observation comes from while making as few mistakes as possible</p>
<ul>
<li>
<p>This is an alternative to logistic approaches with the following advantages:</p>
<ul>
<li><p>when there is clear separation between classes, the parameter estimates for the logic regression model can be <strong>surprisingly</strong> unstable, while discriminant approaches do not suffer</p></li>
<li><p>If X is normal in each of the classes and the sample size is small, then discriminant approaches can be more accurate</p></li>
</ul>
</li>
</ul>
<p>Notation</p>
<p>Similar to MANOVA, let <span class="math inline">\(\mathbf{y}_{j1},\mathbf{y}_{j2},\dots, \mathbf{y}_{in_j} \sim iid f_j (\mathbf{y})\)</span> for <span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>Let <span class="math inline">\(f_j(\mathbf{y})\)</span> be the density function for population j . Note that each vector <span class="math inline">\(\mathbf{y}\)</span> contain measurements on all <span class="math inline">\(p\)</span> traits</p>
<ol style="list-style-type: decimal">
<li>Assume that each observation is from one of <span class="math inline">\(h\)</span> possible populations.</li>
<li>We want to form a discriminant rule that will allocate an observation <span class="math inline">\(\mathbf{y}\)</span> to population j when <span class="math inline">\(\mathbf{y}\)</span> is in fact from this population</li>
</ol>
<div id="known-populations" class="section level3" number="22.4.1">
<h3>
<span class="header-section-number">22.4.1</span> Known Populations<a class="anchor" aria-label="anchor" href="#known-populations"><i class="fas fa-link"></i></a>
</h3>
<p>The maximum likelihood discriminant rule for assigning an observation <span class="math inline">\(\mathbf{y}\)</span> to one of the <span class="math inline">\(h\)</span> populations allocates <span class="math inline">\(\mathbf{y}\)</span> to the population that gives the largest likelihood to <span class="math inline">\(\mathbf{y}\)</span></p>
<p>Consider the likelihood for a single observation <span class="math inline">\(\mathbf{y}\)</span>, which has the form <span class="math inline">\(f_j (\mathbf{y})\)</span> where j is the true population.</p>
<p>Since <span class="math inline">\(j\)</span> is unknown, to make the likelihood as large as possible, we should choose the value j which causes <span class="math inline">\(f_j (\mathbf{y})\)</span> to be as large as possible</p>
<p>Consider a simple univariate example. Suppose we have data from one of two binomial populations.</p>
<ul>
<li><p>The first population has <span class="math inline">\(n= 10\)</span> trials with success probability <span class="math inline">\(p = .5\)</span></p></li>
<li><p>The second population has <span class="math inline">\(n= 10\)</span> trials with success probability <span class="math inline">\(p = .7\)</span></p></li>
<li><p>to which population would we assign an observation of <span class="math inline">\(y = 7\)</span></p></li>
<li>
<p>Note:</p>
<ul>
<li><p><span class="math inline">\(f(y = 7|n = 10, p = .5) = .117\)</span></p></li>
<li><p><span class="math inline">\(f(y = 7|n = 10, p = .7) = .267\)</span> where <span class="math inline">\(f(.)\)</span> is the binomial likelihood.</p></li>
<li><p>Hence, we choose the second population</p></li>
</ul>
</li>
</ul>
<p>Another example</p>
<p>We have 2 populations, where</p>
<ul>
<li><p>First population: <span class="math inline">\(N(\mu_1, \sigma^2_1)\)</span></p></li>
<li><p>Second population: <span class="math inline">\(N(\mu_2, \sigma^2_2)\)</span></p></li>
</ul>
<p>The likelihood for a single observation is</p>
<p><span class="math display">\[
f_j (y) = (2\pi \sigma^2_j)^{-1/2} \exp\{ -\frac{1}{2}(\frac{y - \mu_j}{\sigma_j})^2\}
\]</span></p>
<p>Consider a likelihood ratio rule</p>
<p><span class="math display">\[
\begin{aligned}
\Lambda &amp;= \frac{\text{likelihood of y from pop 1}}{\text{likelihood of y from pop 2}} \\
&amp;= \frac{f_1(y)}{f_2(y)} \\
&amp;= \frac{\sigma_2}{\sigma_1} \exp\{-\frac{1}{2}[(\frac{y - \mu_1}{\sigma_1})^2- (\frac{y - \mu_2}{\sigma_2})^2] \}
\end{aligned}
\]</span></p>
<p>Hence, we classify into</p>
<ul>
<li><p>pop 1 if <span class="math inline">\(\Lambda &gt;1\)</span></p></li>
<li><p>pop 2 if <span class="math inline">\(\Lambda &lt;1\)</span></p></li>
<li><p>for ties, flip a coin</p></li>
</ul>
<p>Another way to think:</p>
<p>we classify into population 1 if the “standardized distance” of y from <span class="math inline">\(\mu_1\)</span> is less than the “standardized distance” of y from <span class="math inline">\(\mu_2\)</span> which is referred to as a <strong>quadratic discriminant rule</strong>.</p>
<p>(Significant simplification occurs in th special case where <span class="math inline">\(\sigma_1 = \sigma_2 = \sigma^2\)</span>)</p>
<p>Thus, we classify into population 1 if</p>
<p><span class="math display">\[
(y - \mu_2)^2 &gt; (y - \mu_1)^2
\]</span></p>
<p>or</p>
<p><span class="math display">\[
|y- \mu_2| &gt; |y - \mu_1|
\]</span></p>
<p>and</p>
<p><span class="math display">\[
-2 \log (\Lambda) = -2y  \frac{(\mu_1 - \mu_2)}{\sigma^2} + \frac{(\mu_1^2 - \mu_2^2)}{\sigma^2} = \beta y + \alpha
\]</span></p>
<p>Thus, we classify into population 1 if this is less than 0.</p>
<p>Discriminant classification rule is linear in y in this case.</p>
<div id="multivariate-expansion" class="section level4" number="22.4.1.1">
<h4>
<span class="header-section-number">22.4.1.1</span> Multivariate Expansion<a class="anchor" aria-label="anchor" href="#multivariate-expansion"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose that there are 2 populations</p>
<ul>
<li><p><span class="math inline">\(N_p(\mathbf{\mu}_1, \mathbf{\Sigma}_1)\)</span></p></li>
<li><p><span class="math inline">\(N_p(\mathbf{\mu}_2, \mathbf{\Sigma}_2)\)</span></p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
-2 \log(\frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})}) &amp;= \log|\mathbf{\Sigma}_1| + (\mathbf{x} - \mathbf{\mu}_1)' \mathbf{\Sigma}^{-1}_1 (\mathbf{x} - \mathbf{\mu}_1) \\
&amp;- [\log|\mathbf{\Sigma}_2|+ (\mathbf{x} - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1}_2 (\mathbf{x} - \mathbf{\mu}_2) ]
\end{aligned}
\]</span></p>
<p>Again, we classify into population 1 if this is less than 0, otherwise, population 2. And like the univariate case with non-equal variances, this is a quadratic discriminant rule.</p>
<p>And if the covariance matrices are equal: <span class="math inline">\(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}_1\)</span> classify into population 1 if</p>
<p><span class="math display">\[
(\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2) \ge 0
\]</span></p>
<p>This linear discriminant rule is also referred to as <strong>Fisher’s linear discriminant function</strong></p>
<p>By <strong>assuming the covariance matrices are equal, we assume that the shape and orientation fo the two populations must be the same (which can be a strong restriction)</strong></p>
<p>In other words, for each variable, it can have different mean but the same variance.</p>
<ul>
<li>Note: LDA Bayes decision boundary is linear. Hence, quadratic decision boundary might lead to better classification. Moreover, the assumption of same variance/covariance matrix across all classes for Gaussian densities imposes the linear rule, if we allow the predictors in each class to follow MVN distribution with class-specific mean vectors and variance/covariance matrices, then it is <strong>Quadratic Discriminant Analysis.</strong> But then, you will have more parameters to estimate (which gives more flexibility than LDA) at the cost of more variance (bias -variance tradeoff).</li>
</ul>
<p>When <span class="math inline">\(\mathbf{\mu}_1, \mathbf{\mu}_2, \mathbf{\Sigma}\)</span> are known, the probability of misclassification can be determined:</p>
<p><span class="math display">\[
\begin{aligned}
P(2|1) &amp;= P(\text{calssify into pop 2| x is from pop 1}) \\
&amp;= P((\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} \mathbf{x} \le \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)|\mathbf{x} \sim N(\mu_1, \mathbf{\Sigma}) \\
&amp;= \Phi(-\frac{1}{2} \delta)
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\delta^2 = (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)\)</span></p></li>
<li><p><span class="math inline">\(\Phi\)</span> is the standard normal CDF</p></li>
</ul>
<p>Suppose there are <span class="math inline">\(h\)</span> possible populations, which are distributed as <span class="math inline">\(N_p (\mathbf{\mu}_p, \mathbf{\Sigma})\)</span>. Then, the maximum likelihood (linear) discriminant rule allocates <span class="math inline">\(\mathbf{y}\)</span> to population j where j minimizes the squared Mahalanobis distance</p>
<p><span class="math display">\[
(\mathbf{y} - \mathbf{\mu}_j)' \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{\mu}_j)
\]</span></p>
</div>
<div id="bayes-discriminant-rules" class="section level4" number="22.4.1.2">
<h4>
<span class="header-section-number">22.4.1.2</span> Bayes Discriminant Rules<a class="anchor" aria-label="anchor" href="#bayes-discriminant-rules"><i class="fas fa-link"></i></a>
</h4>
<p>If we know that population j has prior probabilities <span class="math inline">\(\pi_j\)</span> (assume <span class="math inline">\(\pi_j &gt;0\)</span>) we can form the Bayes discriminant rule.</p>
<p>This rule allocates an observation <span class="math inline">\(\mathbf{y}\)</span> to the population for which <span class="math inline">\(\pi_j f_j (\mathbf{y})\)</span> is maximized.</p>
<p>Note:</p>
<ul>
<li>
<strong>Maximum likelihood discriminant rule</strong> is a special case of the <strong>Bayes discriminant rule</strong>, where it sets all the <span class="math inline">\(\pi_j = 1/h\)</span>
</li>
</ul>
<p>Optimal Properties of Bayes Discriminant Rules</p>
<ul>
<li><p>let <span class="math inline">\(p_{ii}\)</span> be the probability of correctly assigning an observation from population i</p></li>
<li><p>then one rule (with probabilities <span class="math inline">\(p_{ii}\)</span> ) is as good as another rule (with probabilities <span class="math inline">\(p_{ii}'\)</span> ) if <span class="math inline">\(p_{ii} \ge p_{ii}'\)</span> for all <span class="math inline">\(i = 1,\dots, h\)</span></p></li>
<li><p>The first rule is better than the alternative if <span class="math inline">\(p_{ii} &gt; p_{ii}'\)</span> for at least one i.</p></li>
<li><p>A rule for which there is no better alternative is called admissible</p></li>
<li><p>Bayes Discriminant Rules are admissible</p></li>
<li><p>If we utilized prior probabilities, then we can form the posterior probability of a correct allocation, <span class="math inline">\(\sum_{i=1}^h \pi_i p_{ii}\)</span></p></li>
<li><p>Bayes Discriminant Rules have the largest possible posterior probability of correct allocation with respect to the prior</p></li>
<li><p>These properties show that <strong>Bayes Discriminant rule is our best approach</strong>.</p></li>
</ul>
<p>Unequal Cost</p>
<ul>
<li>
<p>We want to consider the cost misallocation</p>
<ul>
<li>Define <span class="math inline">\(c_{ij}\)</span> to be the cost associated with allocation a member of population j to population i.</li>
</ul>
</li>
<li>
<p>Assume that</p>
<ul>
<li><p><span class="math inline">\(c_{ij} &gt;0\)</span> for all <span class="math inline">\(i \neq j\)</span></p></li>
<li><p><span class="math inline">\(c_{ij} = 0\)</span> if <span class="math inline">\(i = j\)</span></p></li>
</ul>
</li>
<li><p>We could determine the expected amount of loss for an observation allocated to population i as <span class="math inline">\(\sum_j c_{ij} p_{ij}\)</span> where the <span class="math inline">\(p_{ij}s\)</span> are the probabilities of allocating an observation from population j into population i</p></li>
<li><p>We want to minimize the amount of loss expected for our rule. Using a Bayes Discrimination, allocate <span class="math inline">\(\mathbf{y}\)</span> to the population j which minimizes <span class="math inline">\(\sum_{k \neq j} c_{ij} \pi_k f_k(\mathbf{y})\)</span></p></li>
<li><p>We could assign equal probabilities to each group and get a maximum likelihood type rule. here, we would allocate <span class="math inline">\(\mathbf{y}\)</span> to population j which minimizes <span class="math inline">\(\sum_{k \neq j}c_{jk} f_k(\mathbf{y})\)</span></p></li>
</ul>
<p><strong>Example</strong>:</p>
<p>Two binomial populations, each of size 10, with probabilities <span class="math inline">\(p_1 = .5\)</span> and <span class="math inline">\(p_2 = .7\)</span></p>
<p>And the probability of being in the first population is .9</p>
<p>However, suppose the cost of inappropriately allocating into the first population is 1 and the cost of incorrectly allocating into the second population is 5.</p>
<p>In this case, we pick population 1 over population 2</p>
<p>In general, we consider two regions, <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> associated with population 1 and 2:</p>
<p><span class="math display">\[
R_1: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} \ge \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]</span></p>
<p><span class="math display">\[
R_2: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} &lt; \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]</span></p>
<p>where <span class="math inline">\(c_{12}\)</span> is the cost of assigning a member of population 2 to population 1.</p>
</div>
<div id="discrimination-under-estimation" class="section level4" number="22.4.1.3">
<h4>
<span class="header-section-number">22.4.1.3</span> Discrimination Under Estimation<a class="anchor" aria-label="anchor" href="#discrimination-under-estimation"><i class="fas fa-link"></i></a>
</h4>
<p>Suppose we know the form of the distributions for populations of interests, but we still have to estimate the parameters.</p>
<p>Example:</p>
<p>we know the distributions are multivariate normal, but we have to estimate the means and variances</p>
<p>The maximum likelihood discriminant rule allocates an observation <span class="math inline">\(\mathbf{y}\)</span> to population j when j maximizes the function</p>
<p><span class="math display">\[
f_j (\mathbf{y} |\hat{\theta})
\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}\)</span> are the maximum likelihood estimates of the unknown parameters</p>
<p>For instance, we have 2 multivariate normal populations with distinct means, but common variance covariance matrix</p>
<p>MLEs for <span class="math inline">\(\mathbf{\mu}_1\)</span> and <span class="math inline">\(\mathbf{\mu}_2\)</span> are <span class="math inline">\(\mathbf{\bar{y}}_1\)</span> and <span class="math inline">\(\mathbf{\bar{y}}_2\)</span>and common <span class="math inline">\(\mathbf{\Sigma}\)</span> is <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>Thus, an estimated discriminant rule could be formed by substituting these sample values for the population values</p>
</div>
<div id="native-bayes" class="section level4" number="22.4.1.4">
<h4>
<span class="header-section-number">22.4.1.4</span> Native Bayes<a class="anchor" aria-label="anchor" href="#native-bayes"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p>The challenge with classification using Bayes’ is that we don’t know the (true) densities, <span class="math inline">\(f_k, k = 1, \dots, K\)</span>, while LDA and QDA make <strong>strong multivariate normality assumptions</strong> to deal with this.</p></li>
<li><p>Naive Bayes makes only one assumption: <strong>within the k-th class, the p predictors are independent (i.e,, for</strong> <span class="math inline">\(k = 1,\dots, K\)</span></p></li>
</ul>
<p><span class="math display">\[
f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)
\]</span></p>
<p>where <span class="math inline">\(f_{kj}\)</span> is the density function of the j-th predictor among observation in the k-th class.</p>
<p>This assumption allows the use of joint distribution without the need to account for dependence between observations. However, this (native) assumption can be unrealistic, but still works well in cases where the number of sample (n) is not large relative to the number of features (p).</p>
<p>With this assumption, we have</p>
<p><span class="math display">\[
P(Y=k|X=x) = \frac{\pi_k \times f_{k1}(x_1) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1)\times \dots f_{lp}(x_p)}
\]</span></p>
<p>we only need to estimate the one-dimensional density function <span class="math inline">\(f_{kj}\)</span> with either of these approaches:</p>
<ul>
<li><p>When <span class="math inline">\(X_j\)</span> is quantitative, assume it has a univariate normal distribution (with independence): <span class="math inline">\(X_j | Y = k \sim N(\mu_{jk}, \sigma^2_{jk})\)</span> which is more restrictive than QDA because it assumes predictors are independent (e.g., a diagonal covariance matrix)</p></li>
<li><p>When <span class="math inline">\(X_j\)</span> is quantitative, use a kernel density estimator <a href="multivariate-methods.html#kernel-methods">Kernel Methods</a> ; which is a smoothed histogram</p></li>
<li><p>When <span class="math inline">\(X_j\)</span> is qualitative, we count the promotion of training observations for the j-th predictor corresponding to each class.</p></li>
</ul>
</div>
<div id="comparison-of-classification-methods" class="section level4" number="22.4.1.5">
<h4>
<span class="header-section-number">22.4.1.5</span> Comparison of Classification Methods<a class="anchor" aria-label="anchor" href="#comparison-of-classification-methods"><i class="fas fa-link"></i></a>
</h4>
<p>Assuming we have K classes and K is the baseline from (James , Witten, Hastie, and Tibshirani book)</p>
<p>Comparing the log odds relative to the K class</p>
<div id="logistic-regression-2" class="section level5" number="22.4.1.5.1">
<h5>
<span class="header-section-number">22.4.1.5.1</span> Logistic Regression<a class="anchor" aria-label="anchor" href="#logistic-regression-2"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
\log(\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \beta_{k0} + \sum_{j=1}^p \beta_{kj}x_j
\]</span></p>
</div>
<div id="lda" class="section level5" number="22.4.1.5.2">
<h5>
<span class="header-section-number">22.4.1.5.2</span> LDA<a class="anchor" aria-label="anchor" href="#lda"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
\log(\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \sum_{j=1}^p b_{kj} x_j
\]</span></p>
<p>where <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_{kj}\)</span> are functions of <span class="math inline">\(\pi_k, \pi_K, \mu_k , \mu_K, \mathbf{\Sigma}\)</span></p>
<p>Similar to logistic regression, LDA assumes the log odds is linear in <span class="math inline">\(x\)</span></p>
<p>Even though they look like having the same form, the parameters in logistic regression are estimated by MLE, where as LDA linear parameters are specified by the prior and normal distributions</p>
<p>We expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and logistic regression to perform better when it does not</p>
</div>
<div id="qda" class="section level5" number="22.4.1.5.3">
<h5>
<span class="header-section-number">22.4.1.5.3</span> QDA<a class="anchor" aria-label="anchor" href="#qda"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
\log(\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \sum_{j=1}^{p}b_{kj}x_{j} + \sum_{j=1}^p \sum_{l=1}^p c_{kjl}x_j x_l
\]</span></p>
<p>where <span class="math inline">\(a_k, b_{kj}, c_{kjl}\)</span> are functions <span class="math inline">\(\pi_k , \pi_K, \mu_k, \mu_K ,\mathbf{\Sigma}_k, \mathbf{\Sigma}_K\)</span></p>
</div>
<div id="naive-bayes" class="section level5" number="22.4.1.5.4">
<h5>
<span class="header-section-number">22.4.1.5.4</span> Naive Bayes<a class="anchor" aria-label="anchor" href="#naive-bayes"><i class="fas fa-link"></i></a>
</h5>
<p><span class="math display">\[
\log (\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \sum_{j=1}^p g_{kj} (x_j)
\]</span></p>
<p>where <span class="math inline">\(a_k = \log (\pi_k / \pi_K)\)</span> and <span class="math inline">\(g_{kj}(x_j) = \log(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\)</span> which is the form of generalized additive model</p>
</div>
<div id="summary-7" class="section level5" number="22.4.1.5.5">
<h5>
<span class="header-section-number">22.4.1.5.5</span> Summary<a class="anchor" aria-label="anchor" href="#summary-7"><i class="fas fa-link"></i></a>
</h5>
<ul>
<li><p>LDA is a special case of QDA</p></li>
<li><p>LDA is robust when it comes to high dimensions</p></li>
<li><p>Any classifier with a linear decision boundary is a special case of naive Bayes with <span class="math inline">\(g_{kj}(x_j) = b_{kj} x_j\)</span>, which means LDA is a special case of naive Bayes. LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes assumes independence of the features.</p></li>
<li><p>Naive bayes is also a special case of LDA with <span class="math inline">\(\mathbf{\Sigma}\)</span> restricted to a diagonal matrix with diagonals, <span class="math inline">\(\sigma^2\)</span> (another notation <span class="math inline">\(diag (\mathbf{\Sigma})\)</span> ) assuming <span class="math inline">\(f_{kj}(x_j) = N(\mu_{kj}, \sigma^2_j)\)</span></p></li>
<li><p>QDA and naive Bayes are not special case of each other. In principal,e naive Bayes can produce a more flexible fit by the choice of <span class="math inline">\(g_{kj}(x_j)\)</span> , but it’s restricted to only purely additive fit, but QDA includes multiplicative terms of the form <span class="math inline">\(c_{kjl}x_j x_l\)</span></p></li>
<li><p>None of these methods uniformly dominates the others: the choice of method depends on the true distribution of the predictors in each of the K classes, n and p (i.e., related to the bias-variance tradeoff).</p></li>
</ul>
<p>Compare to the non-parametric method (KNN)</p>
<ul>
<li><p>KNN would outperform both LDA and logistic regression when the decision boundary is highly nonlinear, but can’t say which predictors are most important, and requires many observations</p></li>
<li><p>KNN is also limited in high-dimensions due to the curse of dimensionality</p></li>
<li><p>Since QDA is a special type of nonlinear decision boundary (quadratic), it can be considered as a compromise between the linear methods and KNN classification. QDA can have fewer training observations than KNN but not as flexible.</p></li>
</ul>
<p>From simulation:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="61%">
<col width="38%">
</colgroup>
<thead><tr class="header">
<th>True decision boundary</th>
<th>Best performance</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Linear</td>
<td>LDA + Logistic regression</td>
</tr>
<tr class="even">
<td>Moderately nonlinear</td>
<td>QDA + Naive Bayes</td>
</tr>
<tr class="odd">
<td>Highly nonlinear (many training, p is not large)</td>
<td>KNN</td>
</tr>
</tbody>
</table></div>
<ul>
<li>like linear regression, we can also introduce flexibility by including transformed features <span class="math inline">\(\sqrt{X}, X^2, X^3\)</span>
</li>
</ul>
</div>
</div>
</div>
<div id="probabilities-of-misclassification" class="section level3" number="22.4.2">
<h3>
<span class="header-section-number">22.4.2</span> Probabilities of Misclassification<a class="anchor" aria-label="anchor" href="#probabilities-of-misclassification"><i class="fas fa-link"></i></a>
</h3>
<p>When the distribution are exactly known, we can determine the misclassification probabilities exactly. however, when we need to estimate the population parameters, we have to estimate the probability of misclassification</p>
<ul>
<li>
<p>Naive method</p>
<ul>
<li><p>Plugging the parameters estimates into the form for the misclassification probabilities results to derive at the estimates of the misclassification probability.</p></li>
<li><p>But this will tend to be optimistic when the number of samples in one or more populations is small.</p></li>
</ul>
</li>
<li>
<p>Resubstitution method</p>
<ul>
<li><p>Use the proportion of the samples from population i that would be allocated to another population as an estimate of the misclassification probability</p></li>
<li><p>But also optimistic when the number of samples is small</p></li>
</ul>
</li>
<li>
<p>Jack-knife estimates:</p>
<ul>
<li><p>The above two methods use observation to estimate both parameters and also misclassification probabilities based upon the discriminant rule</p></li>
<li><p>Alternatively, we determine the discriminant rule based upon all of the data except the k-th observation from the j-th population</p></li>
<li><p>then, determine if the k-th observation would be misclassified under this rule</p></li>
<li><p>perform this process for all <span class="math inline">\(n_j\)</span> observation in population j . An estimate fo the misclassification probability would be the fraction of <span class="math inline">\(n_j\)</span> observations which were misclassified</p></li>
<li><p>repeat the process for other <span class="math inline">\(i \neq j\)</span> populations</p></li>
<li><p>This method is more reliable than the others, but also computationally intensive</p></li>
</ul>
</li>
<li><p>Cross-Validation</p></li>
</ul>
<p><strong>Summary</strong></p>
<p>Consider the group-specific densities <span class="math inline">\(f_j (\mathbf{x})\)</span> for multivariate vector <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Assume equal misclassifications costs, the Bayes classification probability of <span class="math inline">\(\mathbf{x}\)</span> belonging to the j-th population is</p>
<p><span class="math display">\[
p(j |\mathbf{x}) = \frac{\pi_j f_j (\mathbf{x})}{\sum_{k=1}^h \pi_k f_k (\mathbf{x})}
\]</span></p>
<p><span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>where there are <span class="math inline">\(h\)</span> possible groups.</p>
<p>We then classify into the group for which this probability of membership is largest</p>
<p>Alternatively, we can write this in terms of a <strong>generalized squared distance</strong> formation</p>
<p><span class="math display">\[
D_j^2 (\mathbf{x}) = d_j^2 (\mathbf{x})+ g_1(j) + g_2 (j)
\]</span></p>
<p>where</p>
<ul>
<li>
<p><span class="math inline">\(d_j^2(\mathbf{x}) = (\mathbf{x} - \mathbf{\mu}_j)' \mathbf{V}_j^{-1} (\mathbf{x} - \mathbf{\mu}_j)\)</span> is the squared Mahalanobis distance from <span class="math inline">\(\mathbf{x}\)</span> to the centroid of group j, and</p>
<ul>
<li><p><span class="math inline">\(\mathbf{V}_j = \mathbf{S}_j\)</span> if the within group covariance matrices are not equal</p></li>
<li><p><span class="math inline">\(\mathbf{V}_j = \mathbf{S}_p\)</span> if a pooled covariance estimate is appropriate</p></li>
</ul>
</li>
</ul>
<p>and</p>
<p><span class="math display">\[
g_1(j) =
\begin{cases}
\ln |\mathbf{S}_j| &amp; \text{within group covariances are not equal} \\
0 &amp; \text{pooled covariance}
\end{cases}
\]</span></p>
<p><span class="math display">\[
g_2(j) =
\begin{cases}
-2 \ln \pi_j &amp; \text{prior probabilities are not equal} \\
0 &amp; \text{prior probabilities are equal}
\end{cases}
\]</span></p>
<p>then, the posterior probability of belonging to group j is</p>
<p><span class="math display">\[
p(j| \mathbf{x})  = \frac{\exp(-.5 D_j^2(\mathbf{x}))}{\sum_{k=1}^h \exp(-.5 D^2_k (\mathbf{x}))}
\]</span></p>
<p>where <span class="math inline">\(j = 1,\dots , h\)</span></p>
<p>and <span class="math inline">\(\mathbf{x}\)</span> is classified into group j if <span class="math inline">\(p(j | \mathbf{x})\)</span> is largest for <span class="math inline">\(j = 1,\dots,h\)</span> (or, <span class="math inline">\(D_j^2(\mathbf{x})\)</span> is smallest).</p>
<div id="assessing-classification-performance" class="section level4" number="22.4.2.1">
<h4>
<span class="header-section-number">22.4.2.1</span> Assessing Classification Performance<a class="anchor" aria-label="anchor" href="#assessing-classification-performance"><i class="fas fa-link"></i></a>
</h4>
<p>For binary classification, confusion matrix</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="17%">
<col width="24%">
<col width="23%">
<col width="23%">
<col width="10%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Predicted class</th>
<th></th>
<th></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td>- or Null</td>
<td>+ or Null</td>
<td>Total</td>
</tr>
<tr class="even">
<td>True Class</td>
<td>- or Null</td>
<td>True Neg (TN)</td>
<td>False Pos (FP)</td>
<td>N</td>
</tr>
<tr class="odd">
<td></td>
<td>+ or Null</td>
<td>False Neg (FN)</td>
<td>True Pos (TP)</td>
<td>P</td>
</tr>
<tr class="even">
<td></td>
<td>Total</td>
<td>N*</td>
<td>P*</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>and table 4.6 from <span class="citation">(<a href="references.html#ref-james2013">James et al. 2013</a>)</span></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="25%">
<col width="25%">
<col width="49%">
</colgroup>
<thead><tr class="header">
<th>Name</th>
<th>Definition</th>
<th>Synonyms</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>False Pos rate</td>
<td>FP/N</td>
<td>Type I error, 1 0 Specificity</td>
</tr>
<tr class="even">
<td>True Pos. rate</td>
<td>TP/P</td>
<td>1 - Type II error, power, sensitivity, recall</td>
</tr>
<tr class="odd">
<td>Pos Pred. value</td>
<td>TP/P*</td>
<td>Precision, 1 - false discovery promotion</td>
</tr>
<tr class="even">
<td>Neg. Pred. value</td>
<td>TN/N*</td>
<td></td>
</tr>
</tbody>
</table></div>
<p>ROC curve (receiver Operating Characteristics) is a graphical comparison between <strong>sensitivity</strong> (true positive) and <strong>specificity</strong> ( = 1 - false positive)</p>
<p>y-axis = true positive rate</p>
<p>x-axis = false positive rate</p>
<p>as we change the threshold rate for classifying an observation as from 0 to 1</p>
<p>AUC (area under the ROC) ideally would equal to 1, a bad classifier would have AUC = 0.5 (pure chance)</p>
</div>
</div>
<div id="unknown-populations-nonparametric-discrimination" class="section level3" number="22.4.3">
<h3>
<span class="header-section-number">22.4.3</span> Unknown Populations/ Nonparametric Discrimination<a class="anchor" aria-label="anchor" href="#unknown-populations-nonparametric-discrimination"><i class="fas fa-link"></i></a>
</h3>
<p>When your multivariate data are not Gaussian, or known distributional form at all, we can use the following methods</p>
<div id="kernel-methods" class="section level4" number="22.4.3.1">
<h4>
<span class="header-section-number">22.4.3.1</span> Kernel Methods<a class="anchor" aria-label="anchor" href="#kernel-methods"><i class="fas fa-link"></i></a>
</h4>
<p>We approximate <span class="math inline">\(f_j (\mathbf{x})\)</span> by a kernel density estimate</p>
<p><span class="math display">\[
\hat{f}_j(\mathbf{x}) = \frac{1}{n_j} \sum_{i = 1}^{n_j} K_j (\mathbf{x} - \mathbf{x}_i)
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(K_j (.)\)</span> is a kernel function satisfying <span class="math inline">\(\int K_j(\mathbf{z})d\mathbf{z} =1\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{x}_i\)</span> , <span class="math inline">\(i = 1,\dots , n_j\)</span> is a random sample from the j-th population.</p></li>
</ul>
<p>Thus, after finding <span class="math inline">\(\hat{f}_j (\mathbf{x})\)</span> for each of the <span class="math inline">\(h\)</span> populations, the posterior probability of group membership is</p>
<p><span class="math display">\[
p(j |\mathbf{x}) = \frac{\pi_j \hat{f}_j (\mathbf{x})}{\sum_{k-1}^h \pi_k \hat{f}_k (\mathbf{x})}
\]</span></p>
<p>where <span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>There are different choices for the kernel function:</p>
<ul>
<li><p>Uniform</p></li>
<li><p>Normal</p></li>
<li><p>Epanechnikov</p></li>
<li><p>Biweight</p></li>
<li><p>Triweight</p></li>
</ul>
<p>We these kernels, we have to pick the “radius” (or variance, width, window width, bandwidth) of the kernel, which is a smoothing parameter (the larger the radius, the more smooth the kernel estimate of the density).</p>
<p>To select the smoothness parameter, we can use the following method</p>
<p>If we believe the populations were close to multivariate normal, then</p>
<p><span class="math display">\[
R = (\frac{4/(2p+1)}{n_j})^{1/(p+1}
\]</span></p>
<p>But since we do not know for sure, we might choose several different values and select one that vies the best out of sample or cross-validation discrimination.</p>
<p>Moreover, you also have to decide whether to use different kernel smoothness for different populations, which is similar to the individual and pooled covariances in the classical methodology.</p>
</div>
<div id="nearest-neighbor-methods" class="section level4" number="22.4.3.2">
<h4>
<span class="header-section-number">22.4.3.2</span> Nearest Neighbor Methods<a class="anchor" aria-label="anchor" href="#nearest-neighbor-methods"><i class="fas fa-link"></i></a>
</h4>
<p>The nearest neighbor (also known as k-nearest neighbor) method performs the classification of a new observation vector based on the group membership of its nearest neighbors. In practice, we find</p>
<p><span class="math display">\[
d_{ij}^2 (\mathbf{x}, \mathbf{x}_i) = (\mathbf{x}, \mathbf{x}_i) V_j^{-1}(\mathbf{x}, \mathbf{x}_i)
\]</span></p>
<p>which is the distance between the vector <span class="math inline">\(\mathbf{x}\)</span> and the <span class="math inline">\(i\)</span>-th observation in group <span class="math inline">\(j\)</span></p>
<p>We consider different choices for <span class="math inline">\(\mathbf{V}_j\)</span></p>
<p>For example,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{V}_j &amp;= \mathbf{S}_p \\
\mathbf{V}_j &amp;= \mathbf{S}_j \\
\mathbf{V}_j &amp;= \mathbf{I} \\
\mathbf{V}_j &amp;= diag (\mathbf{S}_p)
\end{aligned}
\]</span></p>
<p>We find the <span class="math inline">\(k\)</span> observations that are closest to <span class="math inline">\(\mathbf{x}\)</span> (where users pick <span class="math inline">\(k\)</span>). Then we classify into the most common population, weighted by the prior.</p>
</div>
<div id="modern-discriminant-methods" class="section level4" number="22.4.3.3">
<h4>
<span class="header-section-number">22.4.3.3</span> Modern Discriminant Methods<a class="anchor" aria-label="anchor" href="#modern-discriminant-methods"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Note</strong>:</p>
<p>Logistic regression (with or without random effects) is a flexible model-based procedure for classification between two populations.</p>
<p>The extension of logistic regression to the multi-group setting is polychotomous logistic regression (or, mulinomial regression).</p>
<p>The machine learning and pattern recognition are growing with strong focus on nonlinear discriminant analysis methods such as:</p>
<ul>
<li><p>radial basis function networks</p></li>
<li><p>support vector machines</p></li>
<li><p>multiplayer perceptrons (neural networks)</p></li>
</ul>
<p>The general framework</p>
<p><span class="math display">\[
g_j (\mathbf{x}) = \sum_{l = 1}^m w_{jl}\phi_l (\mathbf{x}; \mathbf{\theta}_l) + w_{j0}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(j = 1,\dots, h\)</span></p></li>
<li><p><span class="math inline">\(m\)</span> nonlinear basis functions <span class="math inline">\(\phi_l\)</span>, each of which has <span class="math inline">\(n_m\)</span> parameters given by <span class="math inline">\(\theta_l = \{ \theta_{lk}: k = 1, \dots , n_m \}\)</span></p></li>
</ul>
<p>We assign <span class="math inline">\(\mathbf{x}\)</span> to the <span class="math inline">\(j\)</span>-th population if <span class="math inline">\(g_j(\mathbf{x})\)</span> is the maximum for all <span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>Development usually focuses on the choice and estimation of the basis functions, <span class="math inline">\(\phi_l\)</span> and the estimation of the weights <span class="math inline">\(w_{jl}\)</span></p>
<p>More details can be found <span class="citation">(<a href="references.html#ref-webb2011statistical">Webb, Copsey, and Cawley 2011</a>)</span></p>
</div>
</div>
<div id="application-12" class="section level3" number="22.4.4">
<h3>
<span class="header-section-number">22.4.4</span> Application<a class="anchor" aria-label="anchor" href="#application-12"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb393"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">class</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://statistik.tu-dortmund.de">klaR</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Read in the data</span></span>
<span><span class="va">crops</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/crops.txt"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">crops</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"crop"</span>, <span class="st">"y1"</span>, <span class="st">"y2"</span>, <span class="st">"y3"</span>, <span class="st">"y4"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">crops</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':    36 obs. of  5 variables:</span></span>
<span><span class="co">#&gt;  $ crop: chr  "Corn" "Corn" "Corn" "Corn" ...</span></span>
<span><span class="co">#&gt;  $ y1  : int  16 15 16 18 15 15 12 20 24 21 ...</span></span>
<span><span class="co">#&gt;  $ y2  : int  27 23 27 20 15 32 15 23 24 25 ...</span></span>
<span><span class="co">#&gt;  $ y3  : int  31 30 27 25 31 32 16 23 25 23 ...</span></span>
<span><span class="co">#&gt;  $ y4  : int  33 30 26 23 32 15 73 25 32 24 ...</span></span>
<span></span>
<span></span>
<span><span class="co">## Read in test data</span></span>
<span><span class="va">crops_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/crops_test.txt"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">crops_test</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"crop"</span>, <span class="st">"y1"</span>, <span class="st">"y2"</span>, <span class="st">"y3"</span>, <span class="st">"y4"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">crops_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':    5 obs. of  5 variables:</span></span>
<span><span class="co">#&gt;  $ crop: chr  "Corn" "Soybeans" "Cotton" "Sugarbeets" ...</span></span>
<span><span class="co">#&gt;  $ y1  : int  16 21 29 54 32</span></span>
<span><span class="co">#&gt;  $ y2  : int  27 25 24 23 32</span></span>
<span><span class="co">#&gt;  $ y3  : int  31 23 26 21 62</span></span>
<span><span class="co">#&gt;  $ y4  : int  33 24 28 54 16</span></span></code></pre></div>
<div id="lda-1" class="section level4" number="22.4.4.1">
<h4>
<span class="header-section-number">22.4.4.1</span> LDA<a class="anchor" aria-label="anchor" href="#lda-1"><i class="fas fa-link"></i></a>
</h4>
<p>Default prior is proportional to sample size and <code>lda</code> and <code>qda</code> do not fit a constant or intercept term</p>
<div class="sourceCode" id="cb394"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Linear discriminant analysis</span></span>
<span><span class="va">lda_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">crop</span> <span class="op">~</span> <span class="va">y1</span> <span class="op">+</span> <span class="va">y2</span> <span class="op">+</span> <span class="va">y3</span> <span class="op">+</span> <span class="va">y4</span>,</span>
<span>               data <span class="op">=</span> <span class="va">crops</span><span class="op">)</span></span>
<span><span class="va">lda_mod</span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lda(crop ~ y1 + y2 + y3 + y4, data = crops)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Prior probabilities of groups:</span></span>
<span><span class="co">#&gt;     Clover       Corn     Cotton   Soybeans Sugarbeets </span></span>
<span><span class="co">#&gt;  0.3055556  0.1944444  0.1666667  0.1666667  0.1666667 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Group means:</span></span>
<span><span class="co">#&gt;                  y1       y2       y3       y4</span></span>
<span><span class="co">#&gt; Clover     46.36364 32.63636 34.18182 36.63636</span></span>
<span><span class="co">#&gt; Corn       15.28571 22.71429 27.42857 33.14286</span></span>
<span><span class="co">#&gt; Cotton     34.50000 32.66667 35.00000 39.16667</span></span>
<span><span class="co">#&gt; Soybeans   21.00000 27.00000 23.50000 29.66667</span></span>
<span><span class="co">#&gt; Sugarbeets 31.00000 32.16667 20.00000 40.50000</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients of linear discriminants:</span></span>
<span><span class="co">#&gt;              LD1          LD2         LD3          LD4</span></span>
<span><span class="co">#&gt; y1 -6.147360e-02  0.009215431 -0.02987075 -0.014680566</span></span>
<span><span class="co">#&gt; y2 -2.548964e-02  0.042838972  0.04631489  0.054842132</span></span>
<span><span class="co">#&gt; y3  1.642126e-02 -0.079471595  0.01971222  0.008938745</span></span>
<span><span class="co">#&gt; y4  5.143616e-05 -0.013917423  0.05381787 -0.025717667</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Proportion of trace:</span></span>
<span><span class="co">#&gt;    LD1    LD2    LD3    LD4 </span></span>
<span><span class="co">#&gt; 0.7364 0.1985 0.0576 0.0075</span></span>
<span></span>
<span><span class="co">## Look at accuracy on the training data</span></span>
<span><span class="va">lda_fitted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lda_mod</span>,newdata <span class="op">=</span> <span class="va">crops</span><span class="op">)</span></span>
<span><span class="co"># Contingency table</span></span>
<span><span class="va">lda_table</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">crops</span><span class="op">$</span><span class="va">crop</span>, fitted <span class="op">=</span> <span class="va">lda_fitted</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="va">lda_table</span></span>
<span><span class="co">#&gt;             fitted</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          6    0      3        0          2</span></span>
<span><span class="co">#&gt;   Corn            0    6      0        1          0</span></span>
<span><span class="co">#&gt;   Cotton          3    0      1        2          0</span></span>
<span><span class="co">#&gt;   Soybeans        0    1      1        3          1</span></span>
<span><span class="co">#&gt;   Sugarbeets      1    1      0        2          2</span></span>
<span><span class="co"># accuracy of 0.5 is just random (not good)</span></span>
<span></span>
<span><span class="co">## Posterior probabilities of membership</span></span>
<span><span class="va">crops_post</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind.data.frame</a></span><span class="op">(</span><span class="va">crops</span>,</span>
<span>                               crop_pred <span class="op">=</span> <span class="va">lda_fitted</span><span class="op">$</span><span class="va">class</span>,</span>
<span>                               <span class="va">lda_fitted</span><span class="op">$</span><span class="va">posterior</span><span class="op">)</span></span>
<span><span class="va">crops_post</span> <span class="op">&lt;-</span> <span class="va">crops_post</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>missed <span class="op">=</span> <span class="va">crop</span> <span class="op">!=</span> <span class="va">crop_pred</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">crops_post</span><span class="op">)</span></span>
<span><span class="co">#&gt;   crop y1 y2 y3 y4 crop_pred     Clover      Corn    Cotton  Soybeans</span></span>
<span><span class="co">#&gt; 1 Corn 16 27 31 33      Corn 0.08935164 0.4054296 0.1763189 0.2391845</span></span>
<span><span class="co">#&gt; 2 Corn 15 23 30 30      Corn 0.07690181 0.4558027 0.1420920 0.2530101</span></span>
<span><span class="co">#&gt; 3 Corn 16 27 27 26      Corn 0.09817815 0.3422454 0.1365315 0.3073105</span></span>
<span><span class="co">#&gt; 4 Corn 18 20 25 23      Corn 0.10521511 0.3633673 0.1078076 0.3281477</span></span>
<span><span class="co">#&gt; 5 Corn 15 15 31 32      Corn 0.05879921 0.5753907 0.1173332 0.2086696</span></span>
<span><span class="co">#&gt; 6 Corn 15 32 32 15  Soybeans 0.09723648 0.3278382 0.1318370 0.3419924</span></span>
<span><span class="co">#&gt;   Sugarbeets missed</span></span>
<span><span class="co">#&gt; 1 0.08971545  FALSE</span></span>
<span><span class="co">#&gt; 2 0.07219340  FALSE</span></span>
<span><span class="co">#&gt; 3 0.11573442  FALSE</span></span>
<span><span class="co">#&gt; 4 0.09546233  FALSE</span></span>
<span><span class="co">#&gt; 5 0.03980738  FALSE</span></span>
<span><span class="co">#&gt; 6 0.10109590   TRUE</span></span>
<span><span class="co"># posterior shows that posterior of corn membership is much higher than the prior</span></span>
<span></span>
<span><span class="co">## LOOCV</span></span>
<span><span class="co"># leave-one-out cross validation for linear discriminant analysis</span></span>
<span><span class="co"># cannot run the predict function using the object with CV = TRUE </span></span>
<span><span class="co"># because it returns the within sample predictions</span></span>
<span><span class="va">lda_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">crop</span> <span class="op">~</span> <span class="va">y1</span> <span class="op">+</span> <span class="va">y2</span> <span class="op">+</span> <span class="va">y3</span> <span class="op">+</span> <span class="va">y4</span>,</span>
<span>              data <span class="op">=</span> <span class="va">crops</span>, CV <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># Contingency table</span></span>
<span><span class="va">lda_table_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">crops</span><span class="op">$</span><span class="va">crop</span>, fitted <span class="op">=</span> <span class="va">lda_cv</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="va">lda_table_cv</span></span>
<span><span class="co">#&gt;             fitted</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          4    3      1        0          3</span></span>
<span><span class="co">#&gt;   Corn            0    4      1        2          0</span></span>
<span><span class="co">#&gt;   Cotton          3    0      0        2          1</span></span>
<span><span class="co">#&gt;   Soybeans        0    1      1        3          1</span></span>
<span><span class="co">#&gt;   Sugarbeets      2    1      0        2          1</span></span>
<span></span>
<span><span class="co">## Predict the test data</span></span>
<span><span class="va">lda_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lda_mod</span>, newdata <span class="op">=</span> <span class="va">crops_test</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Make a contingency table with truth and most likely class</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth<span class="op">=</span><span class="va">crops_test</span><span class="op">$</span><span class="va">crop</span>, predict<span class="op">=</span><span class="va">lda_pred</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt;             predict</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          0    0      1        0          0</span></span>
<span><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span><span class="co">#&gt;   Cotton          0    0      0        1          0</span></span>
<span><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span><span class="co">#&gt;   Sugarbeets      1    0      0        0          0</span></span></code></pre></div>
<p>LDA didn’t do well on both within sample and out-of-sample data.</p>
</div>
<div id="qda-1" class="section level4" number="22.4.4.2">
<h4>
<span class="header-section-number">22.4.4.2</span> QDA<a class="anchor" aria-label="anchor" href="#qda-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb395"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Quadratic discriminant analysis</span></span>
<span><span class="va">qda_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">crop</span> <span class="op">~</span> <span class="va">y1</span> <span class="op">+</span> <span class="va">y2</span> <span class="op">+</span> <span class="va">y3</span> <span class="op">+</span> <span class="va">y4</span>,</span>
<span>               data <span class="op">=</span> <span class="va">crops</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Look at accuracy on the training data</span></span>
<span><span class="va">qda_fitted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">qda_mod</span>, newdata <span class="op">=</span> <span class="va">crops</span><span class="op">)</span></span>
<span><span class="co"># Contingency table</span></span>
<span><span class="va">qda_table</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">crops</span><span class="op">$</span><span class="va">crop</span>, fitted <span class="op">=</span> <span class="va">qda_fitted</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="va">qda_table</span></span>
<span><span class="co">#&gt;             fitted</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          9    0      0        0          2</span></span>
<span><span class="co">#&gt;   Corn            0    7      0        0          0</span></span>
<span><span class="co">#&gt;   Cotton          0    0      6        0          0</span></span>
<span><span class="co">#&gt;   Soybeans        0    0      0        6          0</span></span>
<span><span class="co">#&gt;   Sugarbeets      0    0      1        1          4</span></span>
<span></span>
<span><span class="co">## LOOCV</span></span>
<span><span class="va">qda_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">crop</span> <span class="op">~</span> <span class="va">y1</span> <span class="op">+</span> <span class="va">y2</span> <span class="op">+</span> <span class="va">y3</span> <span class="op">+</span> <span class="va">y4</span>,</span>
<span>              data <span class="op">=</span> <span class="va">crops</span>, CV <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># Contingency table</span></span>
<span><span class="va">qda_table_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">crops</span><span class="op">$</span><span class="va">crop</span>, fitted <span class="op">=</span> <span class="va">qda_cv</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="va">qda_table_cv</span></span>
<span><span class="co">#&gt;             fitted</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          9    0      0        0          2</span></span>
<span><span class="co">#&gt;   Corn            3    2      0        0          2</span></span>
<span><span class="co">#&gt;   Cotton          3    0      2        0          1</span></span>
<span><span class="co">#&gt;   Soybeans        3    0      0        2          1</span></span>
<span><span class="co">#&gt;   Sugarbeets      3    0      1        1          1</span></span>
<span></span>
<span><span class="co">## Predict the test data</span></span>
<span><span class="va">qda_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">qda_mod</span>, newdata <span class="op">=</span> <span class="va">crops_test</span><span class="op">)</span></span>
<span><span class="co">## Make a contingency table with truth and most likely class</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">crops_test</span><span class="op">$</span><span class="va">crop</span>, predict <span class="op">=</span> <span class="va">qda_pred</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt;             predict</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          1    0      0        0          0</span></span>
<span><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span><span class="co">#&gt;   Cotton          0    0      1        0          0</span></span>
<span><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span><span class="co">#&gt;   Sugarbeets      0    0      0        0          1</span></span></code></pre></div>
</div>
<div id="knn-1" class="section level4" number="22.4.4.3">
<h4>
<span class="header-section-number">22.4.4.3</span> KNN<a class="anchor" aria-label="anchor" href="#knn-1"><i class="fas fa-link"></i></a>
</h4>
<p><code>knn</code> uses design matrices of the features.</p>
<div class="sourceCode" id="cb396"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Design matrices</span></span>
<span><span class="va">X_train</span> <span class="op">&lt;-</span> <span class="va">crops</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">crop</span><span class="op">)</span></span>
<span><span class="va">X_test</span> <span class="op">&lt;-</span> <span class="va">crops_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>    <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">crop</span><span class="op">)</span></span>
<span><span class="va">Y_train</span> <span class="op">&lt;-</span> <span class="va">crops</span><span class="op">$</span><span class="va">crop</span></span>
<span><span class="va">Y_test</span> <span class="op">&lt;-</span> <span class="va">crops_test</span><span class="op">$</span><span class="va">crop</span></span>
<span></span>
<span><span class="co">## Nearest neighbors with 2 neighbors</span></span>
<span><span class="va">knn_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">X_train</span>, <span class="va">X_train</span>, <span class="va">Y_train</span>, k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">Y_train</span>, fitted <span class="op">=</span> <span class="va">knn_2</span><span class="op">)</span></span>
<span><span class="co">#&gt;             fitted</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          7    0      2        1          1</span></span>
<span><span class="co">#&gt;   Corn            0    7      0        0          0</span></span>
<span><span class="co">#&gt;   Cotton          0    0      4        0          2</span></span>
<span><span class="co">#&gt;   Soybeans        0    0      0        4          2</span></span>
<span><span class="co">#&gt;   Sugarbeets      1    0      2        0          3</span></span>
<span></span>
<span><span class="co">## Accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_train</span><span class="op">==</span><span class="va">knn_2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.6944444</span></span>
<span></span>
<span><span class="co">## Performance on test data</span></span>
<span><span class="va">knn_2_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">X_train</span>, <span class="va">X_test</span>, <span class="va">Y_train</span>, k <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">Y_test</span>, predict <span class="op">=</span> <span class="va">knn_2_test</span><span class="op">)</span></span>
<span><span class="co">#&gt;             predict</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          1    0      0        0          0</span></span>
<span><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span><span class="co">#&gt;   Cotton          0    0      0        0          1</span></span>
<span><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span><span class="co">#&gt;   Sugarbeets      0    0      0        0          1</span></span>
<span></span>
<span><span class="co">## Accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_test</span><span class="op">==</span><span class="va">knn_2_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.8</span></span>
<span></span>
<span><span class="co">## Nearest neighbors with 3 neighbors</span></span>
<span><span class="va">knn_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">X_train</span>, <span class="va">X_train</span>, <span class="va">Y_train</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">Y_train</span>, fitted <span class="op">=</span> <span class="va">knn_3</span><span class="op">)</span></span>
<span><span class="co">#&gt;             fitted</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          8    0      1        1          1</span></span>
<span><span class="co">#&gt;   Corn            0    4      1        2          0</span></span>
<span><span class="co">#&gt;   Cotton          1    1      3        0          1</span></span>
<span><span class="co">#&gt;   Soybeans        0    1      1        4          0</span></span>
<span><span class="co">#&gt;   Sugarbeets      0    0      0        2          4</span></span>
<span></span>
<span><span class="co">## Accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_train</span><span class="op">==</span><span class="va">knn_3</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.6388889</span></span>
<span></span>
<span><span class="co">## Performance on test data</span></span>
<span><span class="va">knn_3_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span><span class="va">X_train</span>, <span class="va">X_test</span>, <span class="va">Y_train</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">Y_test</span>, predict <span class="op">=</span> <span class="va">knn_3_test</span><span class="op">)</span></span>
<span><span class="co">#&gt;             predict</span></span>
<span><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span><span class="co">#&gt;   Clover          1    0      0        0          0</span></span>
<span><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span><span class="co">#&gt;   Cotton          0    0      1        0          0</span></span>
<span><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span><span class="co">#&gt;   Sugarbeets      0    0      0        0          1</span></span>
<span></span>
<span><span class="co">## Accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_test</span><span class="op">==</span><span class="va">knn_3_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1</span></span></code></pre></div>
</div>
<div id="stepwise" class="section level4" number="22.4.4.4">
<h4>
<span class="header-section-number">22.4.4.4</span> Stepwise<a class="anchor" aria-label="anchor" href="#stepwise"><i class="fas fa-link"></i></a>
</h4>
<p>Stepwise discriminant analysis using the <code>stepclass</code> in function in the <code>klaR</code> package.</p>
<div class="sourceCode" id="cb397"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">step</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/klaR/man/stepclass.html">stepclass</a></span><span class="op">(</span></span>
<span>    <span class="va">crop</span> <span class="op">~</span> <span class="va">y1</span> <span class="op">+</span> <span class="va">y2</span> <span class="op">+</span> <span class="va">y3</span> <span class="op">+</span> <span class="va">y4</span>,</span>
<span>    data <span class="op">=</span> <span class="va">crops</span>,</span>
<span>    method <span class="op">=</span> <span class="st">"qda"</span>,</span>
<span>    improvement <span class="op">=</span> <span class="fl">0.15</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; correctness rate: 0.45;  in: "y1";  variables (1): y1 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  hr.elapsed min.elapsed sec.elapsed </span></span>
<span><span class="co">#&gt;        0.00        0.00        0.16</span></span>
<span></span>
<span><span class="va">step</span><span class="op">$</span><span class="va">process</span></span>
<span><span class="co">#&gt;    step var varname result.pm</span></span>
<span><span class="co">#&gt; 0 start   0      --      0.00</span></span>
<span><span class="co">#&gt; 1    in   1      y1      0.45</span></span>
<span></span>
<span><span class="va">step</span><span class="op">$</span><span class="va">performance.measure</span></span>
<span><span class="co">#&gt; [1] "correctness rate"</span></span></code></pre></div>
<p>Iris Data</p>
<div class="sourceCode" id="cb398"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">'iris'</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">samp</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample.int</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span>, size <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">floor</a></span><span class="op">(</span><span class="fl">0.70</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">iris</span><span class="op">)</span><span class="op">)</span>, replace <span class="op">=</span> <span class="cn">F</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train.iris</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">samp</span>,<span class="op">]</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate_all.html">mutate_if</a></span><span class="op">(</span><span class="va">is.numeric</span>,<span class="va">scale</span><span class="op">)</span></span>
<span><span class="va">test.iris</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="op">-</span><span class="va">samp</span>,<span class="op">]</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate_all.html">mutate_if</a></span><span class="op">(</span><span class="va">is.numeric</span>,<span class="va">scale</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="va">iris.model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">Species</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">train.iris</span><span class="op">)</span></span>
<span><span class="co">#pred</span></span>
<span><span class="va">pred.lda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">iris.model</span>, <span class="va">test.iris</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">test.iris</span><span class="op">$</span><span class="va">Species</span>, prediction <span class="op">=</span> <span class="va">pred.lda</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt;             prediction</span></span>
<span><span class="co">#&gt; truth        setosa versicolor virginica</span></span>
<span><span class="co">#&gt;   setosa         15          0         0</span></span>
<span><span class="co">#&gt;   versicolor      0         17         0</span></span>
<span><span class="co">#&gt;   virginica       0          0        13</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">iris.model</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-24-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb399"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="va">iris.model.qda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">Species</span><span class="op">~</span><span class="va">.</span>,data<span class="op">=</span><span class="va">train.iris</span><span class="op">)</span></span>
<span><span class="co">#pred</span></span>
<span><span class="va">pred.qda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">iris.model.qda</span>,<span class="va">test.iris</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth<span class="op">=</span><span class="va">test.iris</span><span class="op">$</span><span class="va">Species</span>,prediction<span class="op">=</span><span class="va">pred.qda</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt;             prediction</span></span>
<span><span class="co">#&gt; truth        setosa versicolor virginica</span></span>
<span><span class="co">#&gt;   setosa         15          0         0</span></span>
<span><span class="co">#&gt;   versicolor      0         16         1</span></span>
<span><span class="co">#&gt;   virginica       0          0        13</span></span></code></pre></div>
</div>
<div id="pca-with-discriminant-analysis" class="section level4" number="22.4.4.5">
<h4>
<span class="header-section-number">22.4.4.5</span> PCA with Discriminant Analysis<a class="anchor" aria-label="anchor" href="#pca-with-discriminant-analysis"><i class="fas fa-link"></i></a>
</h4>
<p>we can use both PCA for dimension reduction in discriminant analysis</p>
<div class="sourceCode" id="cb400"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">zeros</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/mnist0_train_b.txt"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">nines</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"images/mnist9_train_b.txt"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/cbind.html">rbind</a></span><span class="op">(</span><span class="va">zeros</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>, <span class="op">]</span>, <span class="va">nines</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="va">train</span> <span class="op">/</span> <span class="fl">255</span> <span class="co">#divide by 255 per notes (so ranges from 0 to 1)</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span> <span class="co">#each column is an observation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/image.html">image</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">train</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span>, nrow <span class="op">=</span> <span class="fl">28</span><span class="op">)</span>, main <span class="op">=</span> <span class="st">'Example image, unrotated'</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="22-multivariate_files/figure-html/unnamed-chunk-25-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb401"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://amices.org/mice/reference/cbind.html">rbind</a></span><span class="op">(</span><span class="va">zeros</span><span class="op">[</span><span class="fl">2501</span><span class="op">:</span><span class="fl">3000</span>, <span class="op">]</span>, <span class="va">nines</span><span class="op">[</span><span class="fl">2501</span><span class="op">:</span><span class="fl">3000</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="va">test</span> <span class="op">/</span> <span class="fl">255</span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="va">y.train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1000</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">9</span>, <span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">y.test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">500</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">9</span>, <span class="fl">500</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="va">pc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train.large</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">y.train</span>, <span class="va">pc</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">large</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">y.train</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">train.large</span><span class="op">)</span></span>
<span><span class="co">#the test data set needs to be constucted w/ the same 10 princomps</span></span>
<span><span class="va">test.large</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://amices.org/mice/reference/cbind.html">cbind</a></span><span class="op">(</span><span class="va">y.test</span>, <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">pc</span>, <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">test</span><span class="op">)</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">pred.lda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">large</span>, <span class="va">test.large</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">test.large</span><span class="op">$</span><span class="va">y.test</span>, prediction <span class="op">=</span> <span class="va">pred.lda</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt;      prediction</span></span>
<span><span class="co">#&gt; truth   0   9</span></span>
<span><span class="co">#&gt;     0 491   9</span></span>
<span><span class="co">#&gt;     9   5 495</span></span>
<span></span>
<span><span class="va">large.qda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">y.train</span><span class="op">~</span><span class="va">.</span>,data<span class="op">=</span><span class="va">train.large</span><span class="op">)</span></span>
<span><span class="co">#prediction</span></span>
<span><span class="va">pred.qda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">large.qda</span>,<span class="va">test.large</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>truth<span class="op">=</span><span class="va">test.large</span><span class="op">$</span><span class="va">y.test</span>,prediction<span class="op">=</span><span class="va">pred.qda</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt;      prediction</span></span>
<span><span class="co">#&gt; truth   0   9</span></span>
<span><span class="co">#&gt;     0 493   7</span></span>
<span><span class="co">#&gt;     9   3 497</span></span></code></pre></div>

</div>
</div>
</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="analysis-of-variance-anova.html"><span class="header-section-number">21</span> Analysis of Variance (ANOVA)</a></div>
<div class="next"><a href="quasi-experimental.html"><span class="header-section-number">23</span> Quasi-experimental</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li>
<a class="nav-link" href="#multivariate-methods"><span class="header-section-number">22</span> Multivariate Methods</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#properties-of-mvn"><span class="header-section-number">22.0.1</span> Properties of MVN</a></li>
<li><a class="nav-link" href="#mean-vector-inference"><span class="header-section-number">22.0.2</span> Mean Vector Inference</a></li>
<li><a class="nav-link" href="#general-hypothesis-testing"><span class="header-section-number">22.0.3</span> General Hypothesis Testing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#manova"><span class="header-section-number">22.1</span> MANOVA</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#testing-general-hypotheses"><span class="header-section-number">22.1.1</span> Testing General Hypotheses</a></li>
<li><a class="nav-link" href="#profile-analysis"><span class="header-section-number">22.1.2</span> Profile Analysis</a></li>
<li><a class="nav-link" href="#summary-6"><span class="header-section-number">22.1.3</span> Summary</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#principal-components"><span class="header-section-number">22.2</span> Principal Components</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#population-principal-components"><span class="header-section-number">22.2.1</span> Population Principal Components</a></li>
<li><a class="nav-link" href="#sample-principal-components"><span class="header-section-number">22.2.2</span> Sample Principal Components</a></li>
<li><a class="nav-link" href="#application-10"><span class="header-section-number">22.2.3</span> Application</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#factor-analysis"><span class="header-section-number">22.3</span> Factor Analysis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#methods-of-estimation"><span class="header-section-number">22.3.1</span> Methods of Estimation</a></li>
<li><a class="nav-link" href="#factor-rotation"><span class="header-section-number">22.3.2</span> Factor Rotation</a></li>
<li><a class="nav-link" href="#estimation-of-factor-scores"><span class="header-section-number">22.3.3</span> Estimation of Factor Scores</a></li>
<li><a class="nav-link" href="#model-diagnostic"><span class="header-section-number">22.3.4</span> Model Diagnostic</a></li>
<li><a class="nav-link" href="#application-11"><span class="header-section-number">22.3.5</span> Application</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#discriminant-analysis"><span class="header-section-number">22.4</span> Discriminant Analysis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#known-populations"><span class="header-section-number">22.4.1</span> Known Populations</a></li>
<li><a class="nav-link" href="#probabilities-of-misclassification"><span class="header-section-number">22.4.2</span> Probabilities of Misclassification</a></li>
<li><a class="nav-link" href="#unknown-populations-nonparametric-discrimination"><span class="header-section-number">22.4.3</span> Unknown Populations/ Nonparametric Discrimination</a></li>
<li><a class="nav-link" href="#application-12"><span class="header-section-number">22.4.4</span> Application</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/22-multivariate.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/22-multivariate.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2024-02-08.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
