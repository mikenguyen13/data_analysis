<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 16 Hypothesis Testing | A Guide on Data Analysis</title>
<meta name="author" content="Mike Nguyen">
<meta name="description" content="Hypothesis testing is one of the cornerstones of statistical inference, used widely across disciplines such as economics, finance, psychology, and more. Researchers employ hypothesis testing to...">
<meta name="generator" content="bookdown 0.42 with bs4_book()">
<meta property="og:title" content="Chapter 16 Hypothesis Testing | A Guide on Data Analysis">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/mike/data_analysis/hypothesis-testing.html">
<meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<meta property="og:description" content="Hypothesis testing is one of the cornerstones of statistical inference, used widely across disciplines such as economics, finance, psychology, and more. Researchers employ hypothesis testing to...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 16 Hypothesis Testing | A Guide on Data Analysis">
<meta name="twitter:description" content="Hypothesis testing is one of the cornerstones of statistical inference, used widely across disciplines such as economics, finance, psychology, and more. Researchers employ hypothesis testing to...">
<meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){window.dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-DMNX2X65HQ');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Guide on Data Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="prerequisites.html"><span class="header-section-number">2</span> Prerequisites</a></li>
<li class="book-part">I. BASIC</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">3</span> Descriptive Statistics</a></li>
<li><a class="" href="basic-statistical-inference.html"><span class="header-section-number">4</span> Basic Statistical Inference</a></li>
<li class="book-part">II. REGRESSION</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">5</span> Linear Regression</a></li>
<li><a class="" href="non-linear-regression.html"><span class="header-section-number">6</span> Non-Linear Regression</a></li>
<li><a class="" href="generalized-linear-models.html"><span class="header-section-number">7</span> Generalized Linear Models</a></li>
<li><a class="" href="sec-linear-mixed-models.html"><span class="header-section-number">8</span> Linear Mixed Models</a></li>
<li><a class="" href="sec-nonlinear-and-generalized-linear-mixed-models.html"><span class="header-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a></li>
<li><a class="" href="sec-nonparametric-regression.html"><span class="header-section-number">10</span> Nonparametric Regression</a></li>
<li class="book-part">III. RAMIFICATIONS</li>
<li><a class="" href="data.html"><span class="header-section-number">11</span> Data</a></li>
<li><a class="" href="variable-transformation.html"><span class="header-section-number">12</span> Variable Transformation</a></li>
<li><a class="" href="imputation-missing-data.html"><span class="header-section-number">13</span> Imputation (Missing Data)</a></li>
<li><a class="" href="model-specification-tests.html"><span class="header-section-number">14</span> Model Specification Tests</a></li>
<li><a class="" href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></li>
<li><a class="active" href="hypothesis-testing.html"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li><a class="" href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></li>
<li><a class="" href="moderation.html"><span class="header-section-number">18</span> Moderation</a></li>
<li><a class="" href="mediation.html"><span class="header-section-number">19</span> Mediation</a></li>
<li><a class="" href="prediction-and-estimation.html"><span class="header-section-number">20</span> Prediction and Estimation</a></li>
<li class="book-part">IV. CAUSAL INFERENCE</li>
<li><a class="" href="sec-causal-inference.html"><span class="header-section-number">21</span> Causal Inference</a></li>
<li class="book-part">A. EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-experimental-design.html"><span class="header-section-number">22</span> Experimental Design</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">23</span> Sampling</a></li>
<li><a class="" href="sec-analysis-of-variance-anova.html"><span class="header-section-number">24</span> Analysis of Variance</a></li>
<li><a class="" href="sec-multivariate-methods.html"><span class="header-section-number">25</span> Multivariate Methods</a></li>
<li class="book-part">B. QUASI-EXPERIMENTAL DESIGN</li>
<li><a class="" href="sec-quasi-experimental.html"><span class="header-section-number">26</span> Quasi-Experimental Methods</a></li>
<li><a class="" href="sec-regression-discontinuity.html"><span class="header-section-number">27</span> Regression Discontinuity</a></li>
<li><a class="" href="temporal-discontinuity-designs.html"><span class="header-section-number">28</span> Temporal Discontinuity Designs</a></li>
<li><a class="" href="sec-synthetic-difference-in-differences.html"><span class="header-section-number">29</span> Synthetic Difference-in-Differences</a></li>
<li><a class="" href="sec-difference-in-differences.html"><span class="header-section-number">30</span> Difference-in-Differences</a></li>
<li><a class="" href="sec-changes-in-changes.html"><span class="header-section-number">31</span> Changes-in-Changes</a></li>
<li><a class="" href="sec-synthetic-control.html"><span class="header-section-number">32</span> Synthetic Control</a></li>
<li><a class="" href="sec-event-studies.html"><span class="header-section-number">33</span> Event Studies</a></li>
<li><a class="" href="sec-instrumental-variables.html"><span class="header-section-number">34</span> Instrumental Variables</a></li>
<li><a class="" href="sec-matching-methods.html"><span class="header-section-number">35</span> Matching Methods</a></li>
<li class="book-part">C. OTHER CONCERNS</li>
<li><a class="" href="endogeneity.html"><span class="header-section-number">36</span> Endogeneity</a></li>
<li><a class="" href="other-biases.html"><span class="header-section-number">37</span> Other Biases</a></li>
<li><a class="" href="controls.html"><span class="header-section-number">38</span> Controls</a></li>
<li><a class="" href="directed-acyclic-graph.html"><span class="header-section-number">39</span> Directed Acyclic Graph</a></li>
<li class="book-part">V. MISCELLANEOUS</li>
<li><a class="" href="report.html"><span class="header-section-number">40</span> Report</a></li>
<li><a class="" href="exploratory-data-analysis.html"><span class="header-section-number">41</span> Exploratory Data Analysis</a></li>
<li><a class="" href="sensitivity-analysis-robustness-check.html"><span class="header-section-number">42</span> Sensitivity Analysis/ Robustness Check</a></li>
<li><a class="" href="replication-and-synthetic-data.html"><span class="header-section-number">43</span> Replication and Synthetic Data</a></li>
<li><a class="" href="high-performance-computing.html"><span class="header-section-number">44</span> High-Performance Computing</a></li>
<li class="book-part">APPENDIX</li>
<li><a class="" href="appendix.html"><span class="header-section-number">A</span> Appendix</a></li>
<li><a class="" href="bookdown-cheat-sheet.html"><span class="header-section-number">B</span> Bookdown cheat sheet</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mikenguyen13/data_analysis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="hypothesis-testing" class="section level1" number="16">
<h1>
<span class="header-section-number">16</span> Hypothesis Testing<a class="anchor" aria-label="anchor" href="#hypothesis-testing"><i class="fas fa-link"></i></a>
</h1>
<p>Hypothesis testing is one of the cornerstones of statistical inference, used widely across disciplines such as economics, finance, psychology, and more. Researchers employ hypothesis testing to draw conclusions about population parameters based on sample data. Central to this process is the concept of the <em>p-value</em>, which helps quantify how unlikely the observed data (or more extreme data) would be if the null hypothesis were true.</p>
<p>However, as data collection has become easier and cheaper—especially in the age of big data—there is a growing awareness that <em>large sample sizes (large</em> <span class="math inline">\(n\)</span>) can inflate the likelihood of finding statistically significant, but practically negligible, effects. Moreover, this can lead to “p-value hacking,” where researchers run numerous tests or adopt flexible analytical approaches until they find a (sometimes minuscule) effect that achieves a conventional significance level (often <span class="math inline">\(p &lt; .05\)</span>).</p>
<div id="sec-null-hypothesis-significance-testing" class="section level2" number="16.1">
<h2>
<span class="header-section-number">16.1</span> Null Hypothesis Significance Testing<a class="anchor" aria-label="anchor" href="#sec-null-hypothesis-significance-testing"><i class="fas fa-link"></i></a>
</h2>
<p>Null Hypothesis Significance Testing (NHST) is the foundation of statistical inference. It provides a structured approach to evaluating whether observed data provides sufficient evidence to reject a null hypothesis (<span class="math inline">\(H_0\)</span>) in favor of an alternative hypothesis (<span class="math inline">\(H_a\)</span>).</p>
<p>NHST follows these key steps:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Define Hypotheses</strong>
<ul>
<li>The null hypothesis (<span class="math inline">\(H_0\)</span>) represents the default assumption (e.g., no effect, no difference).</li>
<li>The alternative hypothesis (<span class="math inline">\(H_a\)</span>) represents the competing claim (e.g., a nonzero effect, a relationship between variables).</li>
</ul>
</li>
<li>
<strong>Select a Test Statistic</strong>
<ul>
<li>The test statistic (e.g., <span class="math inline">\(T\)</span>, <span class="math inline">\(W\)</span>, <span class="math inline">\(F\)</span>) quantifies evidence against <span class="math inline">\(H_0\)</span>.</li>
<li>It follows a known distribution under <span class="math inline">\(H_0\)</span> (e.g., normal, chi-square, F-distribution).</li>
</ul>
</li>
<li>
<strong>Decision Rule &amp; p-value</strong>
<ul>
<li>If the test statistic <strong>exceeds a critical value</strong> or the <strong>p-value is below</strong> <span class="math inline">\(\alpha\)</span>, we reject <span class="math inline">\(H_0\)</span>.</li>
<li>Otherwise, we fail to reject <span class="math inline">\(H_0\)</span>, meaning the evidence is insufficient to rule it out.</li>
</ul>
</li>
</ol>
<hr>
<div id="error-types-in-hypothesis-testing" class="section level3" number="16.1.1">
<h3>
<span class="header-section-number">16.1.1</span> Error Types in Hypothesis Testing<a class="anchor" aria-label="anchor" href="#error-types-in-hypothesis-testing"><i class="fas fa-link"></i></a>
</h3>
<p>In hypothesis testing, we may <strong>incorrectly</strong> reject or fail to reject the null hypothesis, leading to two types of errors:</p>
<ul>
<li>
<strong>Type I Error (False Positive)</strong>:
<ul>
<li>
<strong>Rejecting</strong> <span class="math inline">\(H_0\)</span> when it is actually true.</li>
<li>Example: Concluding an effect exists when it does not.</li>
</ul>
</li>
<li>
<strong>Type II Error (False Negative)</strong>:
<ul>
<li>
<strong>Failing to reject</strong> <span class="math inline">\(H_0\)</span> when it is actually false.</li>
<li>Example: Missing a real effect because the test lacked power.</li>
</ul>
</li>
</ul>
<p>The <strong>power of a test</strong> is the probability of correctly rejecting <span class="math inline">\(H_0\)</span> when it is false:</p>
<p><span class="math display">\[
\text{Power} = 1 - P(\text{Type II Error})
\]</span></p>
<p>A <strong>higher power</strong> (typically <span class="math inline">\(\geq 0.8\)</span>) reduces Type II errors and increases the likelihood of detecting true effects.</p>
<hr>
</div>
<div id="hypothesis-testing-framework-1" class="section level3" number="16.1.2">
<h3>
<span class="header-section-number">16.1.2</span> Hypothesis Testing Framework<a class="anchor" aria-label="anchor" href="#hypothesis-testing-framework-1"><i class="fas fa-link"></i></a>
</h3>
<p>Hypothesis tests can be <strong>two-sided</strong> or <strong>one-sided</strong>, depending on the research question.</p>
<div id="two-sided-test" class="section level4" number="16.1.2.1">
<h4>
<span class="header-section-number">16.1.2.1</span> Two-Sided Test<a class="anchor" aria-label="anchor" href="#two-sided-test"><i class="fas fa-link"></i></a>
</h4>
<p>In a two-sided test, we examine whether a parameter is significantly different from a hypothesized value (usually zero):</p>
<p><span class="math display">\[ \begin{aligned} &amp;H_0: \beta_j = 0 \\ &amp;H_1: \beta_j \neq 0  \end{aligned} \]</span></p>
<p>Under the null hypothesis, and assuming standard ordinary least squares assumptions (A1-A3a, A5), the asymptotic distribution of the OLS estimator is:</p>
<p><span class="math display">\[ \sqrt{n} \hat{\beta_j} \sim N(0, \text{Avar}(\sqrt{n} \hat{\beta}_j)) \]</span></p>
<p>where <span class="math inline">\(\text{Avar}(\cdot)\)</span> denotes the asymptotic variance.</p>
</div>
<div id="one-sided-test" class="section level4" number="16.1.2.2">
<h4>
<span class="header-section-number">16.1.2.2</span> One-Sided Test<a class="anchor" aria-label="anchor" href="#one-sided-test"><i class="fas fa-link"></i></a>
</h4>
<p>For a one-sided hypothesis test, the null hypothesis includes a range of values, and we test against a directional alternative:</p>
<p><span class="math display">\[ \begin{aligned} &amp;H_0: \beta_j \geq 0 \\ &amp;H_1: \beta_j &lt; 0 \end{aligned} \]</span></p>
<p>The “hardest” null value to reject is <span class="math inline">\(\beta_j = 0\)</span>. Under this specific null, the estimator follows the same asymptotic distribution:</p>
<p><span class="math display">\[ \sqrt{n} \hat{\beta_j} \sim N(0, \text{Avar}(\sqrt{n} \hat{\beta}_j)) \]</span></p>
<hr>
</div>
</div>
<div id="interpreting-hypothesis-testing-results" class="section level3" number="16.1.3">
<h3>
<span class="header-section-number">16.1.3</span> Interpreting Hypothesis Testing Results<a class="anchor" aria-label="anchor" href="#interpreting-hypothesis-testing-results"><i class="fas fa-link"></i></a>
</h3>
<p>When conducting hypothesis tests, it is essential to distinguish between <strong>population parameters</strong> and <strong>sample estimates</strong>:</p>
<ul>
<li>Hypotheses are always written in terms of the population parameter (<span class="math inline">\(\beta\)</span>), not the sample estimate (<span class="math inline">\(\hat{\beta}\)</span>).</li>
<li>Some disciplines use different notations:
<ul>
<li>
<span class="math inline">\(\beta\)</span>: Standardized coefficient (useful for comparing relative effects, scale-free).</li>
<li>
<span class="math inline">\(\mathbf{b}\)</span>: Unstandardized coefficient (more interpretable in practical applications, e.g., policy decisions).</li>
</ul>
</li>
</ul>
<p>The relationship between these coefficients is:</p>
<p><span class="math display">\[
\beta_j = \mathbf{b}_j \frac{s_{x_j}}{s_y}
\]</span></p>
<p>where <span class="math inline">\(s_{x_j}\)</span> and <span class="math inline">\(s_y\)</span> are the standard deviations of the independent and dependent variables.</p>
<hr>
</div>
<div id="understanding-p-values" class="section level3" number="16.1.4">
<h3>
<span class="header-section-number">16.1.4</span> Understanding p-Values<a class="anchor" aria-label="anchor" href="#understanding-p-values"><i class="fas fa-link"></i></a>
</h3>
<p>The p-value is the probability, under the assumption that <span class="math inline">\(H_0\)</span> is true, of observing a test statistic at least as extreme as the one computed from the sample data. Formally,</p>
<p><span class="math display">\[
p\text{-value} = P(\text{Test Statistic} \geq \text{observed value} \mid H_0 \ \text{is true})
\]</span></p>
<p><strong>Interpretation</strong></p>
<ul>
<li><p>A small p-value indicates that <em>if</em> <span class="math inline">\(H_0\)</span> were true, seeing the observed data (or something more extreme) would be unlikely.</p></li>
<li><p>By convention, if <span class="math inline">\(p &lt; \alpha\)</span> (often 0.05), the result is deemed “statistically significant,” and we reject <span class="math inline">\(H_0\)</span>.</p></li>
<li><p><strong>Important Caveat</strong>: “Statistically significant” is <em>not</em> the same as “practically significant” or “economically significant.” A difference can be statistically significant yet trivial in magnitude, with negligible real-world implications.</p></li>
</ul>
<p><strong>Misconceptions</strong></p>
<ul>
<li><p>The p-value is <em>not</em> the probability that <span class="math inline">\(H_0\)</span> is true or false.</p></li>
<li><p>A p-value above <span class="math inline">\(0.05\)</span> does <em>not</em> prove that there is “no effect.” It simply suggests that the data do not provide sufficient evidence (at the chosen significance level) to reject <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>A p-value below 0.05 does <em>not</em> prove that an effect is “real” or large. It indicates that the data are unusual enough under <span class="math inline">\(H_0\)</span> that we decide to reject <span class="math inline">\(H_0\)</span>, given our chosen threshold.</p></li>
</ul>
<hr>
</div>
<div id="the-role-of-sample-size" class="section level3" number="16.1.5">
<h3>
<span class="header-section-number">16.1.5</span> The Role of Sample Size<a class="anchor" aria-label="anchor" href="#the-role-of-sample-size"><i class="fas fa-link"></i></a>
</h3>
<p>A critical factor influencing the outcome of hypothesis tests is <em>sample size</em> (<span class="math inline">\(n\)</span>).</p>
<p>Increasing Power with Large <span class="math inline">\(n\)</span></p>
<ul>
<li><p><strong>Statistical Power</strong>: The probability of correctly rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is false. Large sample sizes increase statistical power, making it easier to detect even tiny deviations from <span class="math inline">\(H_0\)</span>.</p></li>
<li><p><strong>Implication</strong>: If the true effect size in the population is very small (e.g., a 0.2% difference in average returns between two trading strategies), a study with a large enough <span class="math inline">\(n\)</span> might still find it statistically significant (p-value &lt; 0.05).</p></li>
</ul>
<p>Tendency Toward Over-Sensitivity</p>
<p>As <span class="math inline">\(n\)</span> grows, the standard errors decrease. Thus, even minuscule differences from the null hypothesis become less likely to be attributed to random chance, yielding low p-values. This can lead to findings that are <em>statistically</em> significant but have <em>negligible</em> real-world impact.</p>
<ul>
<li>
<strong>Example</strong>: Suppose an economist is testing if a policy intervention changes employment rates by 0.1%. With a small sample size, the test might not detect this difference. But with a massive dataset, the same 0.1% difference might yield a <span class="math inline">\(p\)</span>-value &lt; 0.05, even though a 0.1% change may not be economically meaningful.</li>
</ul>
<hr>
</div>
<div id="p-value-hacking" class="section level3" number="16.1.6">
<h3>
<span class="header-section-number">16.1.6</span> p-Value Hacking<a class="anchor" aria-label="anchor" href="#p-value-hacking"><i class="fas fa-link"></i></a>
</h3>
<p><strong>p-Hacking</strong> refers to the process of manipulating data analysis until a statistically significant result (<span class="math inline">\(p\)</span>-value &lt; 0.05) is achieved. This can include:</p>
<ul>
<li><p>Running multiple tests on the same dataset and only reporting those that yield significance.</p></li>
<li><p>Stopping data collection once a significant p-value is reached.</p></li>
<li><p>Trying various model specifications (e.g., adding or removing control variables) until one finds a significant effect.</p></li>
<li><p>Selectively reporting outcomes (publication bias).</p></li>
</ul>
<p>With large datasets, the “search space” for potential analyses grows exponentially. If researchers test many hypotheses or sift through a wide range of variables and subgroups, they can almost always find a “significant” result by chance alone.</p>
<ul>
<li>
<strong>Multiple Comparison Problem</strong>: When multiple tests are conducted, the chance of finding at least one “significant” result purely by coincidence increases. For instance, with 20 independent tests at <span class="math inline">\(\alpha = .05\)</span>, there is a 64% chance (<span class="math inline">\(1 - 0.95^{20}\)</span>) of incorrectly rejecting at least one null hypothesis.</li>
</ul>
<hr>
</div>
<div id="practical-vs.-statistical-significance" class="section level3" number="16.1.7">
<h3>
<span class="header-section-number">16.1.7</span> Practical vs. Statistical Significance<a class="anchor" aria-label="anchor" href="#practical-vs.-statistical-significance"><i class="fas fa-link"></i></a>
</h3>
<p>In economics and finance, it is crucial to distinguish between results that are <em>statistically significant</em> and those that are <em>economically meaningful</em>. Economic or financial significance asks: <em>Does this effect have tangible importance to policymakers, businesses, or investors?</em></p>
<ul>
<li>A result might show that a new trading algorithm yields returns that are statistically different from zero, but if that difference is 0.0001% on average, it might not be profitable after accounting for transaction fees, taxes, or other frictions—hence lacking <em>economic significance</em>.</li>
</ul>
<hr>
</div>
<div id="mitigating-the-misuse-of-p-values" class="section level3" number="16.1.8">
<h3>
<span class="header-section-number">16.1.8</span> Mitigating the Misuse of p-Values<a class="anchor" aria-label="anchor" href="#mitigating-the-misuse-of-p-values"><i class="fas fa-link"></i></a>
</h3>
<div id="pre-registration-and-replication" class="section level4" number="16.1.8.1">
<h4>
<span class="header-section-number">16.1.8.1</span> Pre-Registration and Replication<a class="anchor" aria-label="anchor" href="#pre-registration-and-replication"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Pre-Registration</strong>: Researchers specify hypotheses and analytical methods before seeing the data, reducing the temptation to p-hack.</p></li>
<li><p><strong>Replication</strong>: Independent replication studies help confirm whether a result is robust or merely a fluke.</p></li>
</ul>
</div>
<div id="using-alternatives-to-or-supplements-for-p-values" class="section level4" number="16.1.8.2">
<h4>
<span class="header-section-number">16.1.8.2</span> Using Alternatives to (or Supplements for) p-Values<a class="anchor" aria-label="anchor" href="#using-alternatives-to-or-supplements-for-p-values"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Bayesian Methods</strong>: Provide posterior probabilities that incorporate prior information, often giving a more nuanced understanding of uncertainty.</p></li>
<li><p><strong>Effect Size &amp; Confidence Intervals</strong>: Shift the focus from “Is it significant?” to “How large is the effect, and what is its plausible range?”</p></li>
<li><p><a href="hypothesis-testing.html#sec-two-one-sided-tests-equivalence-testing">Equivalence Testing</a>: Sometimes the goal is to show the effect is <em>not</em> larger than a certain threshold. Equivalence tests can be used to conclude “no clinically (or economically) significant difference.”</p></li>
</ul>
</div>
<div id="adjusting-for-multiple-comparisons" class="section level4" number="16.1.8.3">
<h4>
<span class="header-section-number">16.1.8.3</span> Adjusting for Multiple Comparisons<a class="anchor" aria-label="anchor" href="#adjusting-for-multiple-comparisons"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>Bonferroni Correction</strong>: Requires using a more stringent significance threshold when multiple tests are performed (e.g., <span class="math inline">\(\alpha/m\)</span> for <span class="math inline">\(m\)</span> tests).</p></li>
<li><p><a href="hypothesis-testing.html#sec-false-discovery-rate">False Discovery Rate</a> Control: Allows a more flexible approach, controlling the expected proportion of false positives among significant findings.</p></li>
</ul>
</div>
<div id="emphasizing-relevance-over-statistical-stars" class="section level4" number="16.1.8.4">
<h4>
<span class="header-section-number">16.1.8.4</span> Emphasizing Relevance Over Statistical “Stars”<a class="anchor" aria-label="anchor" href="#emphasizing-relevance-over-statistical-stars"><i class="fas fa-link"></i></a>
</h4>
<p>Encourage journals, reviewers, and academic circles to stress the <em>magnitude of effects</em> and <em>robustness checks</em> over whether the result crosses a conventional p-value threshold (like 0.05).</p>
<hr>
<p>There are three commonly used methods for hypothesis testing:</p>
<ol style="list-style-type: decimal">
<li><p><a href="hypothesis-testing.html#sec-likelihood-ratio-test">Likelihood Ratio Test</a>: Compares the likelihood under the null and alternative models. Often used for nested models.</p></li>
<li><p><a href="hypothesis-testing.html#sec-wald-test">Wald Test</a>: Assesses whether an estimated parameter is significantly different from a hypothesized value. Requires only one maximization (under the full model).</p></li>
<li><p><a href="hypothesis-testing.html#lagrange-multiplier-score">Lagrange Multiplier (Score) Test</a>: Evaluates the slope of the likelihood function at the null hypothesis value. Performs well in small to moderate samples.</p></li>
</ol>
<hr>
</div>
</div>
<div id="sec-wald-test" class="section level3" number="16.1.9">
<h3>
<span class="header-section-number">16.1.9</span> Wald Test<a class="anchor" aria-label="anchor" href="#sec-wald-test"><i class="fas fa-link"></i></a>
</h3>
<p>The Wald test assesses whether estimated parameters are significantly different from hypothesized values, based on the asymptotic distribution of the estimator.</p>
<p>The general form of the Wald statistic is:</p>
<p><span class="math display">\[
\begin{aligned}
W &amp;= (\hat{\theta}-\theta_0)'[cov(\hat{\theta})]^{-1}(\hat{\theta}-\theta_0) \\
W &amp;\sim \chi_q^2
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(cov(\hat{\theta})\)</span> is given by the inverse Fisher Information matrix evaluated at <span class="math inline">\(\hat{\theta}\)</span>,</p></li>
<li><p><span class="math inline">\(q\)</span> is the rank of <span class="math inline">\(cov(\hat{\theta})\)</span>, which corresponds to the number of non-redundant parameters in <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
<p>The Wald statistic can also be expressed in different ways:</p>
<ol style="list-style-type: decimal">
<li>Quadratic form of the test statistic:</li>
</ol>
<p><span class="math display">\[
t_W=\frac{(\hat{\theta}-\theta_0)^2}{I(\theta_0)^{-1}} \sim \chi^2_{(v)}
\]</span></p>
<p>where <span class="math inline">\(v\)</span> is the degree of freedom.</p>
<ol start="2" style="list-style-type: decimal">
<li>Standardized Wald test statistic:</li>
</ol>
<p><span class="math display">\[
s_W= \frac{\hat{\theta}-\theta_0}{\sqrt{I(\hat{\theta})^{-1}}} \sim Z
\]</span></p>
<p>This represents how far the sample estimate is from the hypothesized population parameter.</p>
<p>Significance Level and Confidence Level</p>
<ul>
<li>The significance level (<span class="math inline">\(\alpha\)</span>) is the probability threshold at which we reject the null hypothesis.</li>
<li>The confidence level (<span class="math inline">\(1-\alpha\)</span>) determines the range within which the population parameter is expected to fall with a given probability.</li>
</ul>
<p>To standardize the estimator and null value, we define the test statistic for the OLS estimator:</p>
<p><span class="math display">\[
T = \frac{\sqrt{n}(\hat{\beta}_j-\beta_{j0})}{\sqrt{n}SE(\hat{\beta_j})} \sim^a N(0,1)
\]</span></p>
<p>Equivalently:</p>
<p><span class="math display">\[
T = \frac{(\hat{\beta}_j-\beta_{j0})}{SE(\hat{\beta_j})} \sim^a N(0,1)
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(T\)</span> is the test statistic (a function of the data and null hypothesis),</p></li>
<li><p><span class="math inline">\(t\)</span> is the observed realization of <span class="math inline">\(T\)</span>.</p></li>
</ul>
<hr>
<div id="evaluating-the-test-statistic" class="section level4" number="16.1.9.1">
<h4>
<span class="header-section-number">16.1.9.1</span> Evaluating the Test Statistic<a class="anchor" aria-label="anchor" href="#evaluating-the-test-statistic"><i class="fas fa-link"></i></a>
</h4>
<p>There are three equivalent methods for evaluating hypothesis tests:</p>
<ol style="list-style-type: decimal">
<li>Critical Value Method</li>
</ol>
<p>For a given significance level <span class="math inline">\(\alpha\)</span>, determine the critical value (<span class="math inline">\(c\)</span>):</p>
<ul>
<li>One-sided test: <span class="math inline">\(H_0: \beta_j \geq \beta_{j0}\)</span>
</li>
</ul>
<p><span class="math display">\[
P(T &lt; c | H_0) = \alpha
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(t &lt; c\)</span>.</p>
<ul>
<li>One-sided test: <span class="math inline">\(H_0: \beta_j \leq \beta_{j0}\)</span>
</li>
</ul>
<p><span class="math display">\[
P(T &gt; c | H_0) = \alpha
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(t &gt; c\)</span>.</p>
<ul>
<li>Two-sided test: <span class="math inline">\(H_0: \beta_j \neq \beta_{j0}\)</span>
</li>
</ul>
<p><span class="math display">\[
P(|T| &gt; c | H_0) = \alpha
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(|t| &gt; c\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>p-value Method</li>
</ol>
<p>The p-value is the probability of observing a test statistic as extreme as the one obtained, given that the null hypothesis is true.</p>
<ul>
<li>One-sided test: <span class="math inline">\(H_0: \beta_j \geq \beta_{j0}\)</span>
</li>
</ul>
<p><span class="math display">\[
\text{p-value} = P(T &lt; t | H_0)
\]</span></p>
<ul>
<li>One-sided test: <span class="math inline">\(H_0: \beta_j \leq \beta_{j0}\)</span>
</li>
</ul>
<p><span class="math display">\[
\text{p-value} = P(T &gt; t | H_0)
\]</span></p>
<ul>
<li>Two-sided test: <span class="math inline">\(H_0: \beta_j \neq \beta_{j0}\)</span>
</li>
</ul>
<p><span class="math display">\[
\text{p-value} = P(|T| &gt; |t| | H_0)
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\text{p-value} &lt; \alpha\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Confidence Interval Method</li>
</ol>
<p>Using the critical value associated with a given significance level, construct a confidence interval:</p>
<p><span class="math display">\[
CI(\hat{\beta}_j)_{\alpha} = \left[\hat{\beta}_j - c \times SE(\hat{\beta}_j), \hat{\beta}_j + c \times SE(\hat{\beta}_j)\right]
\]</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> if the hypothesized value falls outside the confidence interval.</p>
<hr>
<ul>
<li><p>We are not testing whether the true population value is close to the estimate. Instead, we are testing:<br><em>Given a fixed true population value of the parameter, how likely is it that we observed this estimate?</em></p></li>
<li><p>This can be interpreted as:<br><em>We believe with</em> <span class="math inline">\((1-\alpha)\times 100 \%\)</span> probability that the confidence interval captures the true parameter value.</p></li>
</ul>
<hr>
<p>Finite Sample Properties</p>
<p>Under stronger assumptions (A1-A6), we can consider finite sample properties:</p>
<p><span class="math display">\[
T = \frac{\hat{\beta}_j-\beta_{j0}}{SE(\hat{\beta}_j)} \sim T(n-k)
\]</span></p>
<p>where:</p>
<ul>
<li>
<p>The derivation of this distribution depends strongly on:</p>
<ul>
<li><p>A4 (Homoskedasticity)</p></li>
<li><p>A5 (Data Generation via Random Sampling)</p></li>
</ul>
</li>
<li>
<p>The <span class="math inline">\(T\)</span>-statistic follows a Student’s t-distribution because:</p>
<ul>
<li>The numerator is normally distributed.</li>
</ul>
</li>
<li><p>The denominator follows a <span class="math inline">\(\chi^2\)</span> distribution.</p></li>
<li><p>Critical values and p-values will be computed using the Student’s t-distribution instead of the standard normal distribution.</p></li>
<li><p>As <span class="math inline">\(n \to \infty\)</span>, the <span class="math inline">\(T(n-k)\)</span> distribution converges to a standard normal distribution.</p></li>
</ul>
<hr>
<p>Rule of Thumb</p>
<ul>
<li>If <span class="math inline">\(n-k &gt; 120\)</span>:
<ul>
<li>The t-distribution critical values and p-values closely approximate those from the standard normal distribution.</li>
</ul>
</li>
<li>If <span class="math inline">\(n-k &lt; 120\)</span>:
<ul>
<li>If (A1-A6) hold, the t-test is an exact finite-sample test.</li>
<li>If (A1-A3a, A5) hold, the t-distribution is asymptotically normal.
<ul>
<li>Using the t-distribution for critical values is a valid asymptotic test.</li>
<li>The discrepancy in critical values disappears as <span class="math inline">\(n \to \infty\)</span>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
</div>
<div id="multiple-hypothesis-testing" class="section level4" number="16.1.9.2">
<h4>
<span class="header-section-number">16.1.9.2</span> Multiple Hypothesis Testing<a class="anchor" aria-label="anchor" href="#multiple-hypothesis-testing"><i class="fas fa-link"></i></a>
</h4>
<p>We often need to test multiple parameters simultaneously:</p>
<ul>
<li>Example 1: <span class="math inline">\(H_0: \beta_1 = 0\)</span> and <span class="math inline">\(\beta_2 = 0\)</span>
</li>
<li>Example 2: <span class="math inline">\(H_0: \beta_1 = 1\)</span> and <span class="math inline">\(\beta_2 = 0\)</span>
</li>
</ul>
<p>Performing separate hypothesis tests on individual parameters does not answer the question of joint significance.<br>
We need a test that accounts for joint distributions rather than evaluating two marginal distributions separately.</p>
<p>Consider the multiple regression model:</p>
<p><span class="math display">\[
y = \beta_0 + x_1 \beta_1 + x_2 \beta_2 + x_3 \beta_3 + \epsilon
\]</span></p>
<p>The null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> and <span class="math inline">\(\beta_2 = 0\)</span> can be rewritten in matrix form as:</p>
<p><span class="math display">\[
H_0: \mathbf{R} \beta - \mathbf{q} = 0
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{R}\)</span> is an <span class="math inline">\(m \times k\)</span> matrix, where:
<ul>
<li>
<span class="math inline">\(m\)</span> = number of restrictions.</li>
<li>
<span class="math inline">\(k\)</span> = number of parameters.</li>
</ul>
</li>
<li>
<span class="math inline">\(\mathbf{q}\)</span> is a <span class="math inline">\(k \times 1\)</span> vector that contains the null hypothesis values.</li>
</ul>
<p>For the example <span class="math inline">\(H_0: \beta_1 = 0\)</span> and <span class="math inline">\(\beta_2 = 0\)</span>, we define:</p>
<p><span class="math display">\[
\mathbf{R} =
\begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
\mathbf{q} =
\begin{bmatrix}
0 \\
0
\end{bmatrix}
\]</span></p>
<p>For the OLS estimator under multiple hypotheses, we use the F-statistic:</p>
<p><span class="math display">\[
F = \frac{(\mathbf{R\hat{\beta} - q})' \hat{\Sigma}^{-1} (\mathbf{R\hat{\beta} - q})}{m} \sim^a F(m, n-k)
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\hat{\Sigma}^{-1}\)</span> is the estimator for the asymptotic variance-covariance matrix.</p></li>
<li><p><span class="math inline">\(m\)</span> is the number of restrictions.</p></li>
<li><p><span class="math inline">\(n-k\)</span> is the residual degrees of freedom.</p></li>
</ul>
<p>Assumptions for Variance Estimation</p>
<ul>
<li>If A4 (Homoskedasticity) holds:
<ul>
<li>Both the homoskedastic and heteroskedastic variance estimators are valid.</li>
</ul>
</li>
<li>If A4 does not hold:
<ul>
<li>Only the heteroskedastic variance estimator remains valid.</li>
</ul>
</li>
</ul>
<hr>
<p>Relationship Between F and t-Tests</p>
<ul>
<li>When <span class="math inline">\(m = 1\)</span> (only one restriction), the F-statistic is simply the squared t-statistic:</li>
</ul>
<p><span class="math display">\[
F = t^2
\]</span></p>
<ul>
<li>Since the F-distribution is strictly positive, it is one-sided by definition.</li>
</ul>
<hr>
</div>
<div id="linear-combination-testing" class="section level4" number="16.1.9.3">
<h4>
<span class="header-section-number">16.1.9.3</span> Linear Combination Testing<a class="anchor" aria-label="anchor" href="#linear-combination-testing"><i class="fas fa-link"></i></a>
</h4>
<p>When testing multiple parameters simultaneously, we often assess linear combinations of parameters rather than testing them individually.</p>
<p>For example, consider the following hypotheses:</p>
<p><span class="math display">\[
\begin{aligned}
H_0 &amp;: \beta_1 - \beta_2 = 0 \\
H_0 &amp;: \beta_1 - \beta_2 &gt; 0 \\
H_0 &amp;: \beta_1 - 2\beta_2 = 0
\end{aligned}
\]</span></p>
<p>Each of these represents a single restriction on a function of the parameters.</p>
<hr>
<p>The null hypothesis:</p>
<p><span class="math display">\[
H_0: \beta_1 - \beta_2 = 0
\]</span></p>
<p>can be rewritten in matrix form as:</p>
<p><span class="math display">\[
H_0: \mathbf{R} \beta - \mathbf{q} = 0
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{R} =
\begin{bmatrix}
0 &amp; 1 &amp; -1 &amp; 0 &amp; 0
\end{bmatrix}, \quad
\mathbf{q} =
\begin{bmatrix}
0
\end{bmatrix}
\]</span></p>
<p>Interpretation:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{R}\)</span> is a <span class="math inline">\(1 \times k\)</span> matrix that selects the relevant parameters for the hypothesis.</li>
<li>
<span class="math inline">\(\mathbf{q}\)</span> is a <span class="math inline">\(k \times 1\)</span> vector containing the hypothesized values of the linear combination.</li>
<li>This formulation allows us to use a generalized <a href="hypothesis-testing.html#sec-wald-test">Wald test</a> to assess whether the constraint holds.</li>
</ul>
<p>The Wald test statistic for a linear hypothesis:</p>
<p><span class="math display">\[
W = \frac{(\mathbf{R} \hat{\beta} - \mathbf{q})' \left( \mathbf{R} \hat{\Sigma} \mathbf{R}' \right)^{-1} (\mathbf{R} \hat{\beta} - \mathbf{q})}{s^2 q}
\sim F_{q, n-k}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\hat{\beta}\)</span> is the vector of estimated coefficients.</p></li>
<li><p><span class="math inline">\(\hat{\Sigma}\)</span> is the variance-covariance matrix of <span class="math inline">\(\hat{\beta}\)</span>.</p></li>
<li><p><span class="math inline">\(s^2\)</span> is the estimated error variance.</p></li>
<li><p><span class="math inline">\(q\)</span> is the number of restrictions.</p></li>
<li><p>The test follows an F-distribution with degrees of freedom <span class="math inline">\((q, n-k)\)</span>.</p></li>
</ul>
<hr>
<div class="sourceCode" id="cb562"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a multiple regression model</span></span>
<span><span class="va">mod.duncan</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span>, data<span class="op">=</span><span class="va">Duncan</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Test whether income and education coefficients are equal</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/linearHypothesis.html">linearHypothesis</a></span><span class="op">(</span><span class="va">mod.duncan</span>, <span class="st">"1*income - 1*education = 0"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Linear hypothesis test:</span></span>
<span><span class="co">#&gt; income - education = 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 1: restricted model</span></span>
<span><span class="co">#&gt; Model 2: prestige ~ income + education</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)</span></span>
<span><span class="co">#&gt; 1     43 7518.9                           </span></span>
<span><span class="co">#&gt; 2     42 7506.7  1    12.195 0.0682 0.7952</span></span></code></pre></div>
<ul>
<li><p>This tests whether <span class="math inline">\(\beta_1 = \beta_2\)</span> (i.e., whether income and education have the same effect on prestige).</p></li>
<li><p>If the p-value is low, we reject the null hypothesis and conclude that income and education contribute differently to prestige.</p></li>
</ul>
</div>
<div id="estimating-the-difference-between-two-coefficients" class="section level4" number="16.1.9.4">
<h4>
<span class="header-section-number">16.1.9.4</span> Estimating the Difference Between Two Coefficients<a class="anchor" aria-label="anchor" href="#estimating-the-difference-between-two-coefficients"><i class="fas fa-link"></i></a>
</h4>
<p>In some cases, we may be interested in comparing two regression coefficients directly rather than evaluating them separately. For example, we might want to test:</p>
<p><span class="math display">\[
H_0: \beta_1 = \beta_2
\]</span></p>
<p>which is equivalent to testing whether their difference is zero:</p>
<p><span class="math display">\[
H_0: \beta_1 - \beta_2 = 0
\]</span></p>
<p>Alternatively, we can directly estimate the difference between two regression coefficients.</p>
<div class="sourceCode" id="cb563"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">difftest_lm</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">model</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Compute coefficient difference</span></span>
<span>    <span class="va">diffest</span> <span class="op">&lt;-</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="va">x1</span>, <span class="st">"Estimate"</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="va">x2</span>, <span class="st">"Estimate"</span><span class="op">]</span></span>
<span>    </span>
<span>    <span class="co"># Compute variance of the difference</span></span>
<span>    <span class="va">vardiff</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="va">x1</span>, <span class="st">"Std. Error"</span><span class="op">]</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">+</span></span>
<span>                    <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="va">x2</span>, <span class="st">"Std. Error"</span><span class="op">]</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span><span class="op">[</span><span class="va">x1</span>, <span class="va">x2</span><span class="op">]</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Compute standard error of the difference</span></span>
<span>    <span class="va">diffse</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">vardiff</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Compute t-statistic</span></span>
<span>    <span class="va">tdiff</span> <span class="op">&lt;-</span> <span class="va">diffest</span> <span class="op">/</span> <span class="va">diffse</span></span>
<span>    </span>
<span>    <span class="co"># Compute p-value (two-sided test)</span></span>
<span>    <span class="va">ptdiff</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">tdiff</span><span class="op">)</span>, <span class="va">model</span><span class="op">$</span><span class="va">df.residual</span><span class="op">)</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Compute confidence interval</span></span>
<span>    <span class="va">upr</span> <span class="op">&lt;-</span> <span class="va">diffest</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.975</span>, df <span class="op">=</span> <span class="va">model</span><span class="op">$</span><span class="va">df.residual</span><span class="op">)</span> <span class="op">*</span> <span class="va">diffse</span></span>
<span>    <span class="va">lwr</span> <span class="op">&lt;-</span> <span class="va">diffest</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.975</span>, df <span class="op">=</span> <span class="va">model</span><span class="op">$</span><span class="va">df.residual</span><span class="op">)</span> <span class="op">*</span> <span class="va">diffse</span></span>
<span>    </span>
<span>    <span class="co"># Return results as a named list</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/pkg/gsubfn/man/list.html">list</a></span><span class="op">(</span></span>
<span>            estimate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">diffest</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>            t_stat <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">tdiff</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>            p_value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">ptdiff</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>            lower_CI <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lwr</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>            upper_CI <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">upr</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>            df <span class="op">=</span> <span class="va">model</span><span class="op">$</span><span class="va">df.residual</span></span>
<span>        <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>We demonstrate this function using the Duncan dataset from the {car} package:</p>
<div class="sourceCode" id="cb564"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load Duncan dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Duncan</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear regression model</span></span>
<span><span class="va">mod.duncan</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span>, data <span class="op">=</span> <span class="va">Duncan</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare the effects of income and education</span></span>
<span><span class="fu">difftest_lm</span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="va">mod.duncan</span><span class="op">)</span></span>
<span><span class="co">#&gt; $estimate</span></span>
<span><span class="co">#&gt; [1] 0.05</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $t_stat</span></span>
<span><span class="co">#&gt; [1] 0.26</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $p_value</span></span>
<span><span class="co">#&gt; [1] 0.7952</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $lower_CI</span></span>
<span><span class="co">#&gt; [1] -0.36</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $upper_CI</span></span>
<span><span class="co">#&gt; [1] 0.46</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $df</span></span>
<span><span class="co">#&gt; [1] 42</span></span></code></pre></div>
</div>
<div id="nonlinear-hypothesis-testing" class="section level4" number="16.1.9.5">
<h4>
<span class="header-section-number">16.1.9.5</span> Nonlinear Hypothesis Testing<a class="anchor" aria-label="anchor" href="#nonlinear-hypothesis-testing"><i class="fas fa-link"></i></a>
</h4>
<p>In many applications, we may need to test nonlinear restrictions on parameters. These can be expressed as a set of <span class="math inline">\(q\)</span> nonlinear functions:</p>
<p><span class="math display">\[
\mathbf{h}(\theta) = \{ h_1 (\theta), ..., h_q (\theta)\}'
\]</span></p>
<p>where each <span class="math inline">\(h_j(\theta)\)</span> is a nonlinear function of the parameter vector <span class="math inline">\(\theta\)</span>.</p>
<hr>
<p>To approximate nonlinear restrictions, we use the Jacobian matrix, denoted as <span class="math inline">\(\mathbf{H}(\theta)\)</span>, which contains the first-order partial derivatives of <span class="math inline">\(\mathbf{h}(\theta)\)</span> with respect to the parameters:</p>
<p><span class="math display">\[
\mathbf{H}_{q \times p}(\theta) =
\begin{bmatrix}
\frac{\partial h_1(\theta)}{\partial \theta_1} &amp; \dots &amp; \frac{\partial h_1(\theta)}{\partial \theta_p} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial h_q(\theta)}{\partial \theta_1} &amp; \dots &amp; \frac{\partial h_q(\theta)}{\partial \theta_p}
\end{bmatrix}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(q\)</span> is the number of nonlinear restrictions,</p></li>
<li><p><span class="math inline">\(p\)</span> is the number of estimated parameters.</p></li>
</ul>
<p>The Jacobian matrix linearizes the nonlinear restrictions and allows for an approximation of the hypothesis test using a Wald statistic.</p>
<hr>
<p>We test the null hypothesis:</p>
<p><span class="math display">\[
H_0: \mathbf{h} (\theta) = 0
\]</span></p>
<p>against the two-sided alternative using the Wald statistic:</p>
<p><span class="math display">\[
W = \frac{\mathbf{h(\hat{\theta})}' \left\{ \mathbf{H}(\hat{\theta}) \left[ \mathbf{F}(\hat{\theta})' \mathbf{F}(\hat{\theta}) \right]^{-1} \mathbf{H}(\hat{\theta})' \right\}^{-1} \mathbf{h}(\hat{\theta})}{s^2 q} \sim F_{q, n-p}
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\hat{\theta}\)</span> is the estimated parameter vector,</p></li>
<li><p><span class="math inline">\(\mathbf{H}(\hat{\theta})\)</span> is the Jacobian matrix evaluated at <span class="math inline">\(\hat{\theta}\)</span>,</p></li>
<li><p><span class="math inline">\(\mathbf{F}(\hat{\theta})\)</span> is the <a href="generalized-linear-models.html#fisher-information-matrix">Fisher Information Matrix</a>,</p></li>
<li><p><span class="math inline">\(s^2\)</span> is the estimated error variance,</p></li>
<li><p><span class="math inline">\(q\)</span> is the number of restrictions,</p></li>
<li><p><span class="math inline">\(n\)</span> is the sample size,</p></li>
<li><p><span class="math inline">\(p\)</span> is the number of parameters.</p></li>
</ul>
<p>The test statistic follows an F-distribution with degrees of freedom <span class="math inline">\((q, n - p)\)</span>.</p>
<hr>
<div class="sourceCode" id="cb565"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">nlWaldTest</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load example data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Duncan</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a multiple regression model</span></span>
<span><span class="va">mod.duncan</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span>, data <span class="op">=</span> <span class="va">Duncan</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define a nonlinear hypothesis: income squared equals education</span></span>
<span><span class="va">nl_hypothesis</span> <span class="op">&lt;-</span> <span class="st">"b[2]^2 - b[3] = 0"</span></span>
<span></span>
<span><span class="co"># Conduct the nonlinear Wald test</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/nlWaldTest/man/nlWaldtest.html">nlWaldtest</a></span><span class="op">(</span><span class="va">mod.duncan</span>, texts <span class="op">=</span> <span class="va">nl_hypothesis</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Wald Chi-square test of a restriction on model parameters</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  mod.duncan</span></span>
<span><span class="co">#&gt; Chisq = 0.69385, df = 1, p-value = 0.4049</span></span></code></pre></div>
<ul>
<li><p>If the Wald statistic is large, we reject <span class="math inline">\(H_0\)</span> and conclude that the nonlinear restriction does not hold.</p></li>
<li><p>The p-value provides the probability of observing such an extreme test statistic under the null hypothesis.</p></li>
<li><p>The F-distribution accounts for the fact that multiple nonlinear restrictions are being tested.</p></li>
</ul>
</div>
</div>
<div id="sec-likelihood-ratio-test" class="section level3" number="16.1.10">
<h3>
<span class="header-section-number">16.1.10</span> Likelihood Ratio Test<a class="anchor" aria-label="anchor" href="#sec-likelihood-ratio-test"><i class="fas fa-link"></i></a>
</h3>
<p>The <a href="hypothesis-testing.html#sec-likelihood-ratio-test">Likelihood Ratio Test</a> (LRT) is a general method for comparing two nested models:</p>
<ul>
<li>The reduced model under the null hypothesis (<span class="math inline">\(H_0\)</span>), which imposes constraints on parameters.</li>
<li>The full model, which allows more flexibility under the alternative hypothesis (<span class="math inline">\(H_a\)</span>).</li>
</ul>
<p>The test evaluates how much more likely the data is under the full model compared to the restricted model.</p>
<hr>
<p>The likelihood ratio test statistic is given by:</p>
<p><span class="math display">\[
t_{LR} = 2[l(\hat{\theta}) - l(\theta_0)] \sim \chi^2_v
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(l(\hat{\theta})\)</span> is the log-likelihood evaluated at the estimated parameter <span class="math inline">\(\hat{\theta}\)</span> (from the full model),</p></li>
<li><p><span class="math inline">\(l(\theta_0)\)</span> is the log-likelihood evaluated at the hypothesized parameter <span class="math inline">\(\theta_0\)</span> (from the reduced model),</p></li>
<li><p><span class="math inline">\(v\)</span> is the degrees of freedom (the difference in the number of parameters between the full and reduced models).</p></li>
</ul>
<p>This test compares the height of the log-likelihood of the sample estimate versus the hypothesized population parameter.</p>
<hr>
<p>This test also considers the ratio of two maximized likelihoods:</p>
<p><span class="math display">\[
\begin{aligned}
L_r &amp;= \text{maximized likelihood under } H_0 \text{ (reduced model)} \\
L_f &amp;= \text{maximized likelihood under } H_0 \cup H_a \text{ (full model)}
\end{aligned}
\]</span></p>
<p>Then, the likelihood ratio is defined as:</p>
<p><span class="math display">\[
\Lambda = \frac{L_r}{L_f}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(\Lambda\)</span> cannot exceed 1, because <span class="math inline">\(L_f\)</span> (the likelihood of the full model) is always at least as large as <span class="math inline">\(L_r\)</span>.</li>
</ul>
<p>The likelihood ratio test statistic is then:</p>
<p><span class="math display">\[
\begin{aligned}
-2 \ln(\Lambda) &amp;= -2 \ln \left( \frac{L_r}{L_f} \right) = -2 (l_r - l_f) \\
\lim_{n \to \infty}(-2 \ln(\Lambda)) &amp;\sim \chi^2_v
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>
<span class="math inline">\(v\)</span> is the difference in the number of parameters between the full and reduced models.</li>
</ul>
<p>If the likelihood ratio is small (i.e., <span class="math inline">\(L_r\)</span> is much smaller than <span class="math inline">\(L_f\)</span>), then:</p>
<ul>
<li>The test statistic exceeds the critical value from the <span class="math inline">\(\chi^2_v\)</span> distribution.</li>
<li>We reject the reduced model and accept the full model at the <span class="math inline">\(\alpha \times 100\%\)</span> significance level.</li>
</ul>
<hr>
<div class="sourceCode" id="cb566"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load example dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a full model with two predictors</span></span>
<span><span class="va">full_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span> <span class="op">+</span> <span class="va">wt</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a reduced model with only one predictor</span></span>
<span><span class="va">reduced_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span>, data <span class="op">=</span> <span class="va">mtcars</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform the likelihood ratio test</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/lrtest.html">lrtest</a></span><span class="op">(</span><span class="va">reduced_model</span>, <span class="va">full_model</span><span class="op">)</span></span>
<span><span class="co">#&gt; Likelihood ratio test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 1: mpg ~ hp</span></span>
<span><span class="co">#&gt; Model 2: mpg ~ hp + wt</span></span>
<span><span class="co">#&gt;   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    </span></span>
<span><span class="co">#&gt; 1   3 -87.619                         </span></span>
<span><span class="co">#&gt; 2   4 -74.326  1 26.586   2.52e-07 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<ul>
<li><p>If the p-value is small, the reduced model is significantly worse, and we reject <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>A large test statistic indicates that removing a predictor leads to a substantial drop in model fit.</p></li>
</ul>
</div>
<div id="lagrange-multiplier-score" class="section level3" number="16.1.11">
<h3>
<span class="header-section-number">16.1.11</span> Lagrange Multiplier (Score) Test<a class="anchor" aria-label="anchor" href="#lagrange-multiplier-score"><i class="fas fa-link"></i></a>
</h3>
<p>The Lagrange Multiplier (LM) Test, also known as the Score Test, evaluates whether a restricted model (under <span class="math inline">\(H_0\)</span>) significantly underperforms compared to an unrestricted model (under <span class="math inline">\(H_a\)</span>) without estimating the full model.</p>
<p>Unlike the <a href="hypothesis-testing.html#sec-likelihood-ratio-test">Likelihood Ratio Test</a>, which requires estimating both models, the LM test only requires estimation under the restricted model (<span class="math inline">\(H_0\)</span>).</p>
<p>The LM test statistic is based on the first derivative (score function) of the log-likelihood function, evaluated at the parameter estimate under the null hypothesis (<span class="math inline">\(\theta_0\)</span>):</p>
<p><span class="math display">\[
t_S = \frac{S(\theta_0)^2}{I(\theta_0)} \sim \chi^2_v
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(S(\theta_0) = \frac{\partial l(\theta)}{\partial \theta} \bigg|_{\theta=\theta_0}\)</span> is the score function, i.e., the first derivative of the log-likelihood function evaluated at <span class="math inline">\(\theta_0\)</span>.</p></li>
<li><p><span class="math inline">\(I(\theta_0)\)</span> is the <a href="generalized-linear-models.html#fisher-information-matrix">Fisher Information Matrix</a>, which quantifies the curvature (second derivative) of the log-likelihood.</p></li>
<li><p><span class="math inline">\(v\)</span> is the degrees of freedom, equal to the number of constraints imposed by <span class="math inline">\(H_0\)</span>.</p></li>
</ul>
<p>This test compares:</p>
<ul>
<li><p>The slope of the log-likelihood function at <span class="math inline">\(\theta_0\)</span> (which should be flat under <span class="math inline">\(H_0\)</span>).</p></li>
<li><p>The curvature of the log-likelihood function (captured by <span class="math inline">\(I(\theta_0)\)</span>).</p></li>
</ul>
<hr>
<p>Interpretation of the LM Test</p>
<ul>
<li>If <span class="math inline">\(t_S\)</span> is large, the slope of the log-likelihood function at <span class="math inline">\(\theta_0\)</span> is steep, indicating that the model fit improves significantly when moving away from <span class="math inline">\(\theta_0\)</span>.</li>
<li>If <span class="math inline">\(t_S\)</span> is small, the log-likelihood function remains nearly flat at <span class="math inline">\(\theta_0\)</span>, meaning that the additional parameters in the unrestricted model do not substantially improve the fit.</li>
</ul>
<p>If the score function <span class="math inline">\(S(\theta_0)\)</span> is significantly different from zero, then we reject <span class="math inline">\(H_0\)</span> because it suggests that the likelihood function is increasing, implying a better model fit when moving away from <span class="math inline">\(\theta_0\)</span>.</p>
<hr>
<div class="sourceCode" id="cb567"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">lmtest</span><span class="op">)</span>  <span class="co"># For the Lagrange Multiplier test</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span>     <span class="co"># For example data</span></span>
<span></span>
<span><span class="co"># Load example data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Prestige</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit a linear regression model</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span>, data <span class="op">=</span> <span class="va">Prestige</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform the Lagrange Multiplier test for heteroscedasticity</span></span>
<span><span class="co"># Using the Breusch-Pagan test (a type of LM test)</span></span>
<span><span class="va">lm_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/bptest.html">bptest</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Print the results</span></span>
<span><span class="fu"><a href="https://www.math.uzh.ch/pages/spam/reference/print.html">print</a></span><span class="op">(</span><span class="va">lm_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  studentized Breusch-Pagan test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  model</span></span>
<span><span class="co">#&gt; BP = 4.1838, df = 2, p-value = 0.1235</span></span></code></pre></div>
<ul>
<li><p><code>bptest</code>: This function from the <code>lmtest</code> package performs the Breusch-Pagan test, which is a Lagrange Multiplier test for heteroscedasticity.</p></li>
<li><p>Null Hypothesis: The null hypothesis is that the variance of the residuals is constant (homoscedasticity).</p></li>
<li><p>Alternative Hypothesis: The alternative hypothesis is that the variance of the residuals is not constant (heteroscedasticity).</p></li>
</ul>
<hr>
</div>
<div id="comparing-hypothesis-tests" class="section level3" number="16.1.12">
<h3>
<span class="header-section-number">16.1.12</span> Comparing Hypothesis Tests<a class="anchor" aria-label="anchor" href="#comparing-hypothesis-tests"><i class="fas fa-link"></i></a>
</h3>
<p>A visual comparison of hypothesis tests is shown below:</p>
<div class="sourceCode" id="cb568"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load required libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate data for a normal likelihood function</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, <span class="fl">3</span>, length.out <span class="op">=</span> <span class="fl">200</span><span class="op">)</span>  <span class="co"># Theta values</span></span>
<span></span>
<span><span class="co"># Likelihood function with theta_hat = 1</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">theta</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  </span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">theta</span>, <span class="va">likelihood</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define key points</span></span>
<span><span class="va">theta_0</span>   <span class="op">&lt;-</span> <span class="fl">0</span>  <span class="co"># Null hypothesis value</span></span>
<span><span class="va">theta_hat</span> <span class="op">&lt;-</span> <span class="fl">1</span>  <span class="co"># Estimated parameter (full model)</span></span>
<span><span class="va">likelihood_0</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">theta_0</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Likelihood at theta_0</span></span>
<span><span class="va">likelihood_hat</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">theta_hat</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Likelihood at theta_hat</span></span>
<span></span>
<span><span class="co"># Plot likelihood function</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="va">likelihood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"blue"</span>, linewidth <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span>  <span class="co"># Likelihood curve</span></span>
<span>    </span>
<span>    <span class="co"># Vertical lines for theta_0 and theta_hat</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span></span>
<span>        xintercept <span class="op">=</span> <span class="va">theta_0</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"black"</span>,</span>
<span>        linewidth <span class="op">=</span> <span class="fl">1</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span></span>
<span>        xintercept <span class="op">=</span> <span class="va">theta_hat</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dashed"</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        linewidth <span class="op">=</span> <span class="fl">1</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    </span>
<span>    <span class="co"># Labels for theta_0 and theta_hat</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"text"</span>,</span>
<span>        x <span class="op">=</span> <span class="va">theta_0</span> <span class="op">-</span> <span class="fl">0.1</span>,</span>
<span>        y <span class="op">=</span> <span class="op">-</span><span class="fl">0.02</span>,</span>
<span>        label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"black"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">5</span>,</span>
<span>        fontface <span class="op">=</span> <span class="st">"bold"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"text"</span>,</span>
<span>        x <span class="op">=</span> <span class="va">theta_hat</span> <span class="op">+</span> <span class="fl">0.1</span>,</span>
<span>        y <span class="op">=</span> <span class="op">-</span><span class="fl">0.02</span>,</span>
<span>        label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hat</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"red"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">5</span>,</span>
<span>        fontface <span class="op">=</span> <span class="st">"bold"</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    </span>
<span>    <span class="co"># LRT: Compare heights of likelihood at theta_0 and theta_hat</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"segment"</span>,</span>
<span>        x <span class="op">=</span> <span class="va">theta_0</span>,</span>
<span>        xend <span class="op">=</span> <span class="va">theta_0</span>,</span>
<span>        y <span class="op">=</span> <span class="va">likelihood_0</span>,</span>
<span>        yend <span class="op">=</span> <span class="va">likelihood_hat</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>        linewidth <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        arrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grid/arrow.html">arrow</a></span><span class="op">(</span>length <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grid/unit.html">unit</a></span><span class="op">(</span><span class="fl">0.15</span>, <span class="st">"inches"</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"text"</span>,</span>
<span>        x <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>,</span>
<span>        y <span class="op">=</span> <span class="op">(</span><span class="va">likelihood_0</span> <span class="op">+</span> <span class="va">likelihood_hat</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span> <span class="op">+</span> <span class="fl">0.02</span>,</span>
<span>        label <span class="op">=</span> <span class="st">"LRT: Height"</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>        hjust <span class="op">=</span> <span class="fl">0</span>,</span>
<span>        fontface <span class="op">=</span> <span class="st">"bold"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">5</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="co"># Add horizontal lines at both ends of LRT height comparison</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"segment"</span>,</span>
<span>        x <span class="op">=</span> <span class="op">-</span><span class="fl">2.5</span>,</span>
<span>        xend <span class="op">=</span> <span class="fl">2.5</span>,</span>
<span>        y <span class="op">=</span> <span class="va">likelihood_0</span>,</span>
<span>        yend <span class="op">=</span> <span class="va">likelihood_0</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dotted"</span>,</span>
<span>        linewidth <span class="op">=</span> <span class="fl">1</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"segment"</span>,</span>
<span>        x <span class="op">=</span> <span class="op">-</span><span class="fl">2.5</span>,</span>
<span>        xend <span class="op">=</span> <span class="fl">2.5</span>,</span>
<span>        y <span class="op">=</span> <span class="va">likelihood_hat</span>,</span>
<span>        yend <span class="op">=</span> <span class="va">likelihood_hat</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"purple"</span>,</span>
<span>        linetype <span class="op">=</span> <span class="st">"dotted"</span>,</span>
<span>        linewidth <span class="op">=</span> <span class="fl">1</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    </span>
<span>    <span class="co"># Wald Test: Distance between theta_0 and theta_hat</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"segment"</span>,</span>
<span>        x <span class="op">=</span> <span class="va">theta_0</span>,</span>
<span>        xend <span class="op">=</span> <span class="va">theta_hat</span>,</span>
<span>        y <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span>        yend <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>        linewidth <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        arrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grid/arrow.html">arrow</a></span><span class="op">(</span>length <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grid/unit.html">unit</a></span><span class="op">(</span><span class="fl">0.15</span>, <span class="st">"inches"</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"text"</span>,</span>
<span>        x <span class="op">=</span> <span class="op">(</span><span class="va">theta_0</span> <span class="op">+</span> <span class="va">theta_hat</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span>,</span>
<span>        y <span class="op">=</span> <span class="fl">0.07</span>,</span>
<span>        label <span class="op">=</span> <span class="st">"Wald: Distance"</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"green"</span>,</span>
<span>        hjust <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>        fontface <span class="op">=</span> <span class="st">"bold"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">5</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    </span>
<span>    <span class="co"># LM Test: Slope at theta_0</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"segment"</span>,</span>
<span>        x <span class="op">=</span> <span class="va">theta_0</span> <span class="op">-</span> <span class="fl">0.2</span>,</span>
<span>        xend <span class="op">=</span> <span class="va">theta_0</span> <span class="op">+</span> <span class="fl">0.2</span>,</span>
<span>        y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">theta_0</span> <span class="op">-</span> <span class="fl">0.2</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>        yend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">theta_0</span> <span class="op">+</span> <span class="fl">0.2</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"orange"</span>,</span>
<span>        linewidth <span class="op">=</span> <span class="fl">1.2</span>,</span>
<span>        arrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grid/arrow.html">arrow</a></span><span class="op">(</span>length <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grid/unit.html">unit</a></span><span class="op">(</span><span class="fl">0.15</span>, <span class="st">"inches"</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/annotate.html">annotate</a></span><span class="op">(</span></span>
<span>        <span class="st">"text"</span>,</span>
<span>        x <span class="op">=</span> <span class="op">-</span><span class="fl">1.5</span>,</span>
<span>        y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, mean <span class="op">=</span> <span class="fl">1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span> <span class="fl">.2</span>,</span>
<span>        label <span class="op">=</span> <span class="st">"LM: Slope"</span>,</span>
<span>        color <span class="op">=</span> <span class="st">"orange"</span>,</span>
<span>        hjust <span class="op">=</span> <span class="fl">0</span>,</span>
<span>        fontface <span class="op">=</span> <span class="st">"bold"</span>,</span>
<span>        size <span class="op">=</span> <span class="fl">5</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>    </span>
<span>    <span class="co"># Titles and themes</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Comparison of Hypothesis Tests"</span>,</span>
<span>         x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Likelihood"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span></span>
<span>        plot.title <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">16</span>, face <span class="op">=</span> <span class="st">"bold"</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>        axis.title <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">14</span><span class="op">)</span>,</span>
<span>        axis.text <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">12</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="16-hypothesis_files/figure-html/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="images/nested_tests.jpg" alt=""></div>
<p><em>Figure adapted from <span class="citation">(<a href="references.html#ref-fox1997applied">Fox 1997</a>)</span>.</em></p>
<p>Each test approaches hypothesis evaluation differently:</p>
<ol style="list-style-type: decimal">
<li>
<a href="hypothesis-testing.html#sec-likelihood-ratio-test">Likelihood Ratio Test</a>: Compares the <strong>heights</strong> of the log-likelihood at <span class="math inline">\(\hat{\theta}\)</span> (full model) vs. <span class="math inline">\(\theta_0\)</span> (restricted model).</li>
<li>
<a href="hypothesis-testing.html#sec-wald-test">Wald Test</a>: Measures the <strong>distance</strong> between <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta_0\)</span>.</li>
<li>
<a href="hypothesis-testing.html#lagrange-multiplier-score">Lagrange Multiplier Test</a>: Examines the <strong>slope</strong> of the log-likelihood at <span class="math inline">\(\theta_0\)</span> to check if movement towards <span class="math inline">\(\hat{\theta}\)</span> significantly improves fit.</li>
</ol>
<p>The <a href="hypothesis-testing.html#sec-likelihood-ratio-test">Likelihood Ratio Test</a> and <a href="hypothesis-testing.html#lagrange-multiplier-score">Lagrange Multiplier Test</a> perform well in small to moderate samples, while the Wald Test is computationally simpler as it only requires one model estimation.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="24%">
<col width="27%">
<col width="24%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th>Test</th>
<th>Key Idea</th>
<th>Computation</th>
<th>Best Use Case</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="hypothesis-testing.html#sec-likelihood-ratio-test">Likelihood Ratio Test</a></td>
<td>Compares log-likelihoods of full vs. restricted models</td>
<td>Estimates both models</td>
<td>When both models can be estimated</td>
</tr>
<tr class="even">
<td><a href="hypothesis-testing.html#sec-wald-test">Wald Test</a></td>
<td>Checks if parameters significantly differ from <span class="math inline">\(H_0\)</span>
</td>
<td>Estimates only the full model</td>
<td>When the full model is available</td>
</tr>
<tr class="odd">
<td><a href="hypothesis-testing.html#lagrange-multiplier-score">Lagrange Multiplier Test</a></td>
<td>Tests if the score function suggests moving away from <span class="math inline">\(H_0\)</span>
</td>
<td>Estimates only the restricted model</td>
<td>When the full model is difficult to estimate</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="sec-two-one-sided-tests-equivalence-testing" class="section level2" number="16.2">
<h2>
<span class="header-section-number">16.2</span> Two One-Sided Tests Equivalence Testing<a class="anchor" aria-label="anchor" href="#sec-two-one-sided-tests-equivalence-testing"><i class="fas fa-link"></i></a>
</h2>
<p>The Two One-Sided Tests (TOST) procedure is a method used in equivalence testing to determine whether a population effect size falls within a range of practical equivalence.</p>
<p>Unlike traditional null hypothesis significance testing (NHST), which focuses on detecting differences, TOST tests for similarity by checking whether an effect is small enough to be practically insignificant.</p>
<div id="when-to-use-tost" class="section level3" number="16.2.1">
<h3>
<span class="header-section-number">16.2.1</span> When to Use TOST?<a class="anchor" aria-label="anchor" href="#when-to-use-tost"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>Bioequivalence Testing
<ul>
<li>Example: Determining whether a generic drug is equivalent to a brand-name drug in terms of effectiveness.</li>
</ul>
</li>
<li>Non-Inferiority Testing
<ul>
<li>Example: Assessing whether a new teaching method is not worse than a traditional method by a meaningful margin.</li>
</ul>
</li>
<li>Equivalence in Business &amp; Finance
<ul>
<li>Example: Comparing the performance of two financial models to determine if they produce practically the same results.</li>
</ul>
</li>
<li>Psychological &amp; Behavioral Research
<ul>
<li>Example: Determining whether a new intervention is equally effective as an existing one.</li>
</ul>
</li>
</ol>
<hr>
<p>In traditional hypothesis testing, we assess:</p>
<p><span class="math display">\[
H_0: \theta = \theta_0 \quad vs. \quad H_a: \theta \neq \theta_0
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is a population parameter (e.g., mean difference, regression coefficient, or effect size).</p>
<p>However, in equivalence testing, we are interested in whether <span class="math inline">\(\theta\)</span> falls within a predefined equivalence margin (<span class="math inline">\(-\Delta, \Delta\)</span>).</p>
<p>This leads to the TOST procedure, where we conduct two one-sided tests:</p>
<p>1st One-Sided Test:</p>
<p><span class="math display">\[
H_0: \theta \leq -\Delta \quad vs. \quad H_a: \theta &gt; -\Delta
\]</span></p>
<p>2nd One-Sided Test:</p>
<p><span class="math display">\[
H_0: \theta \geq \Delta \quad vs. \quad H_a: \theta &lt; \Delta
\]</span></p>
<p>If both null hypotheses are rejected, then we conclude equivalence (i.e., <span class="math inline">\(\theta\)</span> is within the equivalence range).</p>
<hr>
</div>
<div id="interpretation-of-the-tost-procedure" class="section level3" number="16.2.2">
<h3>
<span class="header-section-number">16.2.2</span> Interpretation of the TOST Procedure<a class="anchor" aria-label="anchor" href="#interpretation-of-the-tost-procedure"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>If the p-value for both one-sided tests is less than <span class="math inline">\(\alpha\)</span>, then we conclude that the effect size falls within the equivalence bounds.</li>
<li>If one or both p-values are greater than <span class="math inline">\(\alpha\)</span>, we fail to reject the null hypothesis and cannot claim equivalence.</li>
<li>The TOST procedure provides stronger evidence of similarity than traditional NHST, which only assesses whether an effect is <em>statistically different from zero</em> rather than <em>practically insignificant</em>.</li>
</ul>
<hr>
</div>
<div id="relationship-to-confidence-intervals" class="section level3" number="16.2.3">
<h3>
<span class="header-section-number">16.2.3</span> Relationship to Confidence Intervals<a class="anchor" aria-label="anchor" href="#relationship-to-confidence-intervals"><i class="fas fa-link"></i></a>
</h3>
<p>Another way to interpret TOST is through confidence intervals (CIs):</p>
<ul>
<li>If the entire <span class="math inline">\((1 - 2\alpha) \times 100\%\)</span> confidence interval lies within <span class="math inline">\([-\Delta, \Delta]\)</span>, we conclude equivalence.</li>
<li>If the confidence interval extends beyond the equivalence range, we fail to establish equivalence.</li>
</ul>
<p>This relationship ensures that TOST is consistent with CI-based inference.</p>
<hr>
</div>
<div id="example-1-testing-the-equivalence-of-two-means" class="section level3" number="16.2.4">
<h3>
<span class="header-section-number">16.2.4</span> Example 1: Testing the Equivalence of Two Means<a class="anchor" aria-label="anchor" href="#example-1-testing-the-equivalence-of-two-means"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have two groups and want to test whether their mean difference is practically insignificant within a range of <span class="math inline">\([-0.5, 0.5]\)</span>.</p>
<div class="sourceCode" id="cb569"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://aaroncaldwell.us/TOSTERpkg/">TOSTER</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated data: Two groups with similar means</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">group1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">30</span>, mean <span class="op">=</span> <span class="fl">5</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">group2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">30</span>, mean <span class="op">=</span> <span class="fl">5.1</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform TOST equivalence test</span></span>
<span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/TOSTtwo.html">TOSTtwo</a></span><span class="op">(</span></span>
<span>    m1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">group1</span><span class="op">)</span>,</span>
<span>    sd1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">group1</span><span class="op">)</span>,</span>
<span>    n1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">group1</span><span class="op">)</span>,</span>
<span>    m2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">group2</span><span class="op">)</span>,</span>
<span>    sd2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">group2</span><span class="op">)</span>,</span>
<span>    n2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">group2</span><span class="op">)</span>,</span>
<span>    low_eqbound <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>,</span>
<span>    high_eqbound <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="16-hypothesis_files/figure-html/unnamed-chunk-8-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; TOST results:
#&gt; t-value lower bound: 0.553   p-value lower bound: 0.291
#&gt; t-value upper bound: -3.32   p-value upper bound: 0.0008
#&gt; degrees of freedom : 56.56
#&gt; 
#&gt; Equivalence bounds (Cohen's d):
#&gt; low eqbound: -0.5 
#&gt; high eqbound: 0.5
#&gt; 
#&gt; Equivalence bounds (raw scores):
#&gt; low eqbound: -0.4555 
#&gt; high eqbound: 0.4555
#&gt; 
#&gt; TOST confidence interval:
#&gt; lower bound 90% CI: -0.719
#&gt; upper bound 90% CI:  0.068
#&gt; 
#&gt; NHST confidence interval:
#&gt; lower bound 95% CI: -0.797
#&gt; upper bound 95% CI:  0.146
#&gt; 
#&gt; Equivalence Test Result:
#&gt; The equivalence test was non-significant, t(56.56) = 0.553, p = 0.291, given equivalence bounds of -0.456 and 0.456 (on a raw scale) and an alpha of 0.05.
#&gt; 
#&gt; Null Hypothesis Test Result:
#&gt; The null hypothesis test was non-significant, t(56.56) = -1.384, p = 0.172, given an alpha of 0.05.</code></pre>
<ul>
<li><p>If both p-values are less than 0.05, we conclude that the groups are equivalent within the given range.</p></li>
<li><p>The confidence interval helps visualize whether the effect size falls entirely within <span class="math inline">\([-0.5, 0.5]\)</span>.</p></li>
</ul>
<div id="example-2-tost-for-correlation-equivalence" class="section level4" number="16.2.4.1">
<h4>
<span class="header-section-number">16.2.4.1</span> Example 2: TOST for Correlation Equivalence<a class="anchor" aria-label="anchor" href="#example-2-tost-for-correlation-equivalence"><i class="fas fa-link"></i></a>
</h4>
<p>We can also use TOST to test whether a correlation coefficient is effectively zero.</p>
<div class="sourceCode" id="cb571"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulated correlation data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">*</span> <span class="fl">0.02</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">50</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Very weak correlation</span></span>
<span></span>
<span><span class="co"># TOST for correlation</span></span>
<span><span class="fu"><a href="https://aaroncaldwell.us/TOSTERpkg/reference/TOSTr.html">TOSTr</a></span><span class="op">(</span></span>
<span>    n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>,</span>
<span>    r <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span>,</span>
<span>    low_eqbound_r <span class="op">=</span> <span class="op">-</span><span class="fl">0.1</span>,</span>
<span>    high_eqbound_r <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="16-hypothesis_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;"></div>
<pre><code>#&gt; TOST results:
#&gt; p-value lower bound: 0.280
#&gt; p-value upper bound: 0.214
#&gt; 
#&gt; Equivalence bounds (r):
#&gt; low eqbound: -0.1 
#&gt; high eqbound: 0.1
#&gt; 
#&gt; TOST confidence interval:
#&gt; lower bound 90% CI: -0.25
#&gt; upper bound 90% CI:  0.221
#&gt; 
#&gt; NHST confidence interval:
#&gt; lower bound 95% CI: -0.293
#&gt; upper bound 95% CI:  0.264
#&gt; 
#&gt; Equivalence Test Result:
#&gt; The equivalence test was non-significant, p = 0.280, given equivalence bounds of -0.100 and 0.100 and an alpha of 0.05.
#&gt; 
#&gt; Null Hypothesis Test Result:
#&gt; The null hypothesis test was non-significant, p = 0.915, given an alpha of 0.05.</code></pre>
<ul>
<li><p>This tests whether the correlation is within <span class="math inline">\([-0.1, 0.1]\)</span>, meaning “practically zero”.</p></li>
<li><p>If both p-values are significant, we conclude that the correlation is effectively negligible.</p></li>
</ul>
</div>
</div>
<div id="advantages-of-tost-equivalence-testing" class="section level3" number="16.2.5">
<h3>
<span class="header-section-number">16.2.5</span> Advantages of TOST Equivalence Testing<a class="anchor" aria-label="anchor" href="#advantages-of-tost-equivalence-testing"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<p>Avoids Misinterpretation of Non-Significance</p>
<ul>
<li><p>Traditional NHST failing to reject <span class="math inline">\(H_0\)</span> does not imply equivalence.</p></li>
<li><p>TOST explicitly tests for equivalence, preventing misinterpretation.</p></li>
</ul>
</li>
<li>
<p>Aligned with Confidence Intervals</p>
<ul>
<li>TOST conclusions align with confidence interval-based reasoning.</li>
</ul>
</li>
<li>
<p>Applicable to Various Statistical Tests</p>
<ul>
<li>Can be used for means, correlations, regression coefficients, and more.</li>
</ul>
</li>
<li>
<p>Commonly Used in Regulatory &amp; Clinical Studies</p>
<ul>
<li>Required for bioequivalence trials by organizations like the FDA <span class="citation">(<a href="references.html#ref-schuirmann1987comparison">Schuirmann 1987</a>)</span>.</li>
</ul>
</li>
</ol>
</div>
<div id="when-not-to-use-tost" class="section level3" number="16.2.6">
<h3>
<span class="header-section-number">16.2.6</span> When <em>Not</em> to Use TOST<a class="anchor" aria-label="anchor" href="#when-not-to-use-tost"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>If your research question is about detecting a difference rather than establishing equivalence.</p></li>
<li><p>If the equivalence bounds are too wide to be meaningful in practice.</p></li>
<li><p>If the sample size is too small, making it difficult to detect equivalence reliably.</p></li>
</ul>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="23%">
<col width="36%">
<col width="39%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Traditional <a href="hypothesis-testing.html#sec-null-hypothesis-significance-testing">NHST</a>
</th>
<th><a href="hypothesis-testing.html#sec-two-one-sided-tests-equivalence-testing">TOST Equivalence Testing</a></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Null Hypothesis</td>
<td>
<span class="math inline">\(H_0\)</span>: No effect (<span class="math inline">\(\theta = 0\)</span>)</td>
<td>
<span class="math inline">\(H_0\)</span>: Effect is outside equivalence bounds</td>
</tr>
<tr class="even">
<td>Alternative Hypothesis</td>
<td>
<span class="math inline">\(H_a\)</span>: There is an effect (<span class="math inline">\(\theta \neq 0\)</span>)</td>
<td>
<span class="math inline">\(H_a\)</span>: Effect is within equivalence bounds</td>
</tr>
<tr class="odd">
<td>Goal</td>
<td>Detect difference</td>
<td>Establish similarity</td>
</tr>
<tr class="even">
<td>p-value Interpretation</td>
<td>Small <span class="math inline">\(p\)</span> means evidence for an effect</td>
<td>Small <span class="math inline">\(p\)</span> means evidence for equivalence</td>
</tr>
</tbody>
</table></div>
</div>
</div>
<div id="sec-false-discovery-rate" class="section level2" number="16.3">
<h2>
<span class="header-section-number">16.3</span> False Discovery Rate<a class="anchor" aria-label="anchor" href="#sec-false-discovery-rate"><i class="fas fa-link"></i></a>
</h2>
<p>When conducting multiple hypothesis tests simultaneously, we increase the probability of false positives (Type I errors). Traditional correction methods like Bonferroni correction are too conservative, reducing statistical power.</p>
<p>The False Discovery Rate (FDR), introduced by <span class="citation">Benjamini and Hochberg (<a href="references.html#ref-benjamini1995controlling">1995</a>)</span>, is a more flexible and powerful approach that controls the proportion of false discoveries (incorrect rejections of the null hypothesis) while maintaining a reasonable chance of detecting true effects.</p>
<hr>
<p>Suppose we perform <span class="math inline">\(m\)</span> independent hypothesis tests, each with a significance level <span class="math inline">\(\alpha\)</span>. The probability of making at least one Type I error (false positive) is:</p>
<p><span class="math display">\[
P(\text{at least one false positive}) = 1 - (1 - \alpha)^m
\]</span></p>
<p>For example, with <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(m = 20\)</span> tests:</p>
<p><span class="math display">\[
P(\text{at least one false positive}) = 1 - (0.95)^{20} \approx 0.64
\]</span></p>
<p>Thus, if we do not adjust for multiple testing, we are highly likely to reject at least one true null hypothesis just by chance.</p>
<hr>
<p>Family-Wise Error Rate vs. False Discovery Rate</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th>Approach</th>
<th>Controls</th>
<th>Method</th>
<th>Pros</th>
<th>Cons</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Bonferroni Correction</td>
<td>FWER (Probability of <span class="math inline">\(\ge 1\)</span> false positive)</td>
<td>Adjusts <span class="math inline">\(\alpha\)</span>: <span class="math inline">\(\alpha/m\)</span>
</td>
<td>Very conservative, reduces false positives</td>
<td>Low power, increases false negatives</td>
</tr>
<tr class="even">
<td>False Discovery Rate</td>
<td>Expected proportion of false discoveries</td>
<td>Adjusts <span class="math inline">\(p\)</span>-values dynamically</td>
<td>Higher power, fewer false negatives</td>
<td>Some false positives allowed</td>
</tr>
</tbody>
</table></div>
<p>Why FDR?</p>
<ul>
<li>FWER control (Bonferroni, Holm) is too strict, reducing true discoveries.</li>
<li>FDR control allows a small fraction of false positives while keeping most discoveries valid.</li>
</ul>
<hr>
<p>Let:</p>
<ul>
<li><p><span class="math inline">\(m\)</span> = total number of hypotheses tested</p></li>
<li><p><span class="math inline">\(V\)</span> = number of false discoveries (Type I errors)</p></li>
<li><p><span class="math inline">\(R\)</span> = total number of rejected null hypotheses</p></li>
</ul>
<p>Then, the <a href="hypothesis-testing.html#sec-false-discovery-rate">False Discovery Rate</a> is:</p>
<p><span class="math display">\[
\text{FDR} = E\left[\frac{V}{\max(R,1)}\right]
\]</span></p>
<ul>
<li>If no null hypotheses are rejected (<span class="math inline">\(R = 0\)</span>), we define FDR = 0.</li>
<li>Unlike FWER, which controls the probability of any false positives, FDR controls the expected proportion of false positives.</li>
</ul>
<hr>
<div id="sec-benjamini-hochberg-procedure" class="section level3" number="16.3.1">
<h3>
<span class="header-section-number">16.3.1</span> Benjamini-Hochberg Procedure<a class="anchor" aria-label="anchor" href="#sec-benjamini-hochberg-procedure"><i class="fas fa-link"></i></a>
</h3>
<p>The Benjamini-Hochberg (BH) procedure is the most widely used FDR-controlling method <span class="citation">(<a href="references.html#ref-benjamini1995controlling">Benjamini and Hochberg 1995</a>)</span>. It works as follows:</p>
<p>Step-by-Step Algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Perform <span class="math inline">\(m\)</span> hypothesis tests and obtain <span class="math inline">\(p\)</span>-values: <span class="math inline">\(p_1, p_2, ..., p_m\)</span>.</p></li>
<li><p>Rank the <span class="math inline">\(p\)</span>-values in ascending order: <span class="math inline">\(p_{(1)} \leq p_{(2)} \leq ... \leq p_{(m)}\)</span>.</p></li>
<li>
<p>Calculate the Benjamini-Hochberg critical value for each test:</p>
<p><span class="math display">\[
p_{(i)} \leq \frac{i}{m} \alpha
\]</span></p>
</li>
<li><p>Find the largest <span class="math inline">\(i\)</span> where <span class="math inline">\(p_{(i)} \leq \frac{i}{m} \alpha\)</span>.</p></li>
<li><p>Reject all hypotheses with <span class="math inline">\(p \leq p_{(i)}\)</span>.</p></li>
</ol>
<p>Interpretation:</p>
<ul>
<li>This ensures that the expected proportion of false discoveries is controlled at level <span class="math inline">\(\alpha\)</span>.</li>
<li>Unlike Bonferroni, it does not require independence of tests, making it more powerful.</li>
</ul>
<hr>
<div id="example-1-fdr-correction-on-simulated-data" class="section level4" number="16.3.1.1">
<h4>
<span class="header-section-number">16.3.1.1</span> Example 1: FDR Correction on Simulated Data<a class="anchor" aria-label="anchor" href="#example-1-fdr-correction-on-simulated-data"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb573"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate 20 random p-values</span></span>
<span><span class="va">p_values</span> <span class="op">&lt;-</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">0</span>, <span class="fl">0.1</span><span class="op">)</span>  <span class="co"># Simulating p-values from multiple tests</span></span>
<span></span>
<span><span class="co"># Apply FDR correction (Benjamini-Hochberg)</span></span>
<span><span class="va">adjusted_p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"BH"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare raw and adjusted p-values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Raw_p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_values</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>           Adjusted_p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">adjusted_p</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt;   Raw_p Adjusted_p</span></span>
<span><span class="co">#&gt; 1 0.029      0.095</span></span>
<span><span class="co">#&gt; 2 0.079      0.096</span></span>
<span><span class="co">#&gt; 3 0.041      0.095</span></span>
<span><span class="co">#&gt; 4 0.088      0.096</span></span>
<span><span class="co">#&gt; 5 0.094      0.096</span></span>
<span><span class="co">#&gt; 6 0.005      0.046</span></span></code></pre></div>
<ul>
<li><p>Adjusted p-values control the expected proportion of false discoveries.</p></li>
<li><p>If an adjusted p-value is below <span class="math inline">\(\alpha\)</span>, we reject the null hypothesis.</p></li>
</ul>
</div>
<div id="example-2-fdr-correction-in-gene-expression-analysis" class="section level4" number="16.3.1.2">
<h4>
<span class="header-section-number">16.3.1.2</span> Example 2: FDR Correction in Gene Expression Analysis<a class="anchor" aria-label="anchor" href="#example-2-fdr-correction-in-gene-expression-analysis"><i class="fas fa-link"></i></a>
</h4>
<p>FDR is widely used in genomics, where thousands of genes are tested for differential expression.</p>
<div class="sourceCode" id="cb574"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">multtest</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated gene expression study with 1000 genes</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">p_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">0</span>, <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply different multiple testing corrections</span></span>
<span></span>
<span><span class="co"># Bonferroni (very strict)</span></span>
<span><span class="va">p_bonf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"bonferroni"</span><span class="op">)</span>  </span>
<span></span>
<span><span class="co"># Holm's method</span></span>
<span><span class="va">p_holm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"holm"</span><span class="op">)</span>        </span>
<span></span>
<span><span class="co"># Benjamini-Hochberg (FDR)</span></span>
<span><span class="va">p_fdr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"BH"</span><span class="op">)</span>           </span>
<span></span>
<span><span class="co"># Compare significance rates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_bonf</span> <span class="op">&lt;</span> <span class="fl">0.05</span><span class="op">)</span>  <span class="co"># Strictest correction</span></span>
<span><span class="co">#&gt; [1] 3</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_holm</span> <span class="op">&lt;</span> <span class="fl">0.05</span><span class="op">)</span>  <span class="co"># Moderately strict</span></span>
<span><span class="co">#&gt; [1] 3</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_fdr</span> <span class="op">&lt;</span> <span class="fl">0.05</span><span class="op">)</span>   <span class="co"># Most discoveries, controlled FDR</span></span>
<span><span class="co">#&gt; [1] 5</span></span></code></pre></div>
<ul>
<li><p>Bonferroni results in few discoveries (low power).</p></li>
<li><p><a href="hypothesis-testing.html#sec-benjamini-hochberg-procedure">Benjamini-Hochberg</a> allows more discoveries, while controlling the proportion of false positives.</p></li>
</ul>
<p><strong>Use FDR when:</strong></p>
<ul>
<li><p>You perform many hypothesis tests (e.g., genomics, finance, A/B testing).</p></li>
<li><p>You want to balance false positives and false negatives.</p></li>
<li><p>Bonferroni is too strict, leading to low power.</p></li>
</ul>
<p><strong>Do not use FDR if:</strong></p>
<ul>
<li><p>Strict control of any false positives is required (e.g., drug approval studies).</p></li>
<li><p>There are only a few tests, where Bonferroni is appropriate.</p></li>
</ul>
<hr>
</div>
</div>
<div id="sec-benjamini-yekutieli-procedure" class="section level3" number="16.3.2">
<h3>
<span class="header-section-number">16.3.2</span> Benjamini-Yekutieli Procedure<a class="anchor" aria-label="anchor" href="#sec-benjamini-yekutieli-procedure"><i class="fas fa-link"></i></a>
</h3>
<p>The Benjamini-Yekutieli (BY) method modifies the Benjamini-Hochberg (BH) procedure to account for correlated test statistics <span class="citation">(<a href="references.html#ref-benjamini2001control">Benjamini and Yekutieli 2001</a>)</span>.</p>
<p>The key adjustment is a larger critical value for significance, making it more conservative than BH.</p>
<p>Similar to BH, the BY procedure ranks <span class="math inline">\(m\)</span> p-values in ascending order:</p>
<p><span class="math display">\[
p_{(1)} \leq p_{(2)} \leq \dots \leq p_{(m)}
\]</span></p>
<p>Instead of using <span class="math inline">\(\alpha \frac{i}{m}\)</span> as in BH, BY introduces a <strong>correction factor</strong>:</p>
<p><span class="math display">\[
p_{(i)} \leq \frac{i}{m C(m)} \alpha
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
C(m) = \sum_{j=1}^{m} \frac{1}{j} \approx \ln(m) + 0.577
\]</span></p>
<ul>
<li>
<span class="math inline">\(C(m)\)</span> is the harmonic series correction (ensures control under dependence).</li>
<li>This makes the BY threshold larger than BH, reducing false positives.</li>
<li>Recommended when tests are positively correlated (e.g., in fMRI, finance).</li>
</ul>
<hr>
<p>We can apply BY correction using the built-in <code><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust()</a></code> function.</p>
<div class="sourceCode" id="cb575"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate 50 random p-values</span></span>
<span><span class="va">p_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">0</span>, <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply BY correction</span></span>
<span><span class="va">p_by</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"BY"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare raw and adjusted p-values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Raw_p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_values</span>, <span class="fl">3</span><span class="op">)</span>, Adjusted_BY <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_by</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;    Raw_p Adjusted_BY</span></span>
<span><span class="co">#&gt; 1  0.029       0.430</span></span>
<span><span class="co">#&gt; 2  0.079       0.442</span></span>
<span><span class="co">#&gt; 3  0.041       0.430</span></span>
<span><span class="co">#&gt; 4  0.088       0.442</span></span>
<span><span class="co">#&gt; 5  0.094       0.442</span></span>
<span><span class="co">#&gt; 6  0.005       0.342</span></span>
<span><span class="co">#&gt; 7  0.053       0.442</span></span>
<span><span class="co">#&gt; 8  0.089       0.442</span></span>
<span><span class="co">#&gt; 9  0.055       0.442</span></span>
<span><span class="co">#&gt; 10 0.046       0.430</span></span>
<span><span class="co">#&gt; 11 0.096       0.442</span></span>
<span><span class="co">#&gt; 12 0.045       0.430</span></span>
<span><span class="co">#&gt; 13 0.068       0.442</span></span>
<span><span class="co">#&gt; 14 0.057       0.442</span></span>
<span><span class="co">#&gt; 15 0.010       0.429</span></span>
<span><span class="co">#&gt; 16 0.090       0.442</span></span>
<span><span class="co">#&gt; 17 0.025       0.430</span></span>
<span><span class="co">#&gt; 18 0.004       0.342</span></span>
<span><span class="co">#&gt; 19 0.033       0.430</span></span>
<span><span class="co">#&gt; 20 0.095       0.442</span></span>
<span><span class="co">#&gt; 21 0.089       0.442</span></span>
<span><span class="co">#&gt; 22 0.069       0.442</span></span>
<span><span class="co">#&gt; 23 0.064       0.442</span></span>
<span><span class="co">#&gt; 24 0.099       0.447</span></span>
<span><span class="co">#&gt; 25 0.066       0.442</span></span>
<span><span class="co">#&gt; 26 0.071       0.442</span></span>
<span><span class="co">#&gt; 27 0.054       0.442</span></span>
<span><span class="co">#&gt; 28 0.059       0.442</span></span>
<span><span class="co">#&gt; 29 0.029       0.430</span></span>
<span><span class="co">#&gt; 30 0.015       0.429</span></span>
<span><span class="co">#&gt; 31 0.096       0.442</span></span>
<span><span class="co">#&gt; 32 0.090       0.442</span></span>
<span><span class="co">#&gt; 33 0.069       0.442</span></span>
<span><span class="co">#&gt; 34 0.080       0.442</span></span>
<span><span class="co">#&gt; 35 0.002       0.342</span></span>
<span><span class="co">#&gt; 36 0.048       0.430</span></span>
<span><span class="co">#&gt; 37 0.076       0.442</span></span>
<span><span class="co">#&gt; 38 0.022       0.430</span></span>
<span><span class="co">#&gt; 39 0.032       0.430</span></span>
<span><span class="co">#&gt; 40 0.023       0.430</span></span>
<span><span class="co">#&gt; 41 0.014       0.429</span></span>
<span><span class="co">#&gt; 42 0.041       0.430</span></span>
<span><span class="co">#&gt; 43 0.041       0.430</span></span>
<span><span class="co">#&gt; 44 0.037       0.430</span></span>
<span><span class="co">#&gt; 45 0.015       0.429</span></span>
<span><span class="co">#&gt; 46 0.014       0.429</span></span>
<span><span class="co">#&gt; 47 0.023       0.430</span></span>
<span><span class="co">#&gt; 48 0.047       0.430</span></span>
<span><span class="co">#&gt; 49 0.027       0.430</span></span>
<span><span class="co">#&gt; 50 0.086       0.442</span></span></code></pre></div>
<p>Comparison: BH vs. BY</p>
<div class="sourceCode" id="cb576"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p_bh</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"BH"</span><span class="op">)</span>  <span class="co"># BH correction</span></span>
<span></span>
<span><span class="co"># Visualize differences</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>Raw_p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_values</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>           BH_Adjusted <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_bh</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>           BY_Adjusted <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_by</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt;   Raw_p BH_Adjusted BY_Adjusted</span></span>
<span><span class="co">#&gt; 1 0.029       0.096       0.430</span></span>
<span><span class="co">#&gt; 2 0.079       0.098       0.442</span></span>
<span><span class="co">#&gt; 3 0.041       0.096       0.430</span></span>
<span><span class="co">#&gt; 4 0.088       0.098       0.442</span></span>
<span><span class="co">#&gt; 5 0.094       0.098       0.442</span></span>
<span><span class="co">#&gt; 6 0.005       0.076       0.342</span></span></code></pre></div>
<ul>
<li><p>Benjamini-Yekutieli is more conservative than Benjamini-Hochberg.</p></li>
<li><p>If BH identifies significant results but BY does not, the tests are likely correlated.</p></li>
</ul>
<hr>
</div>
<div id="sec-storeys-q-value-approach" class="section level3" number="16.3.3">
<h3>
<span class="header-section-number">16.3.3</span> Storey’s q-value Approach<a class="anchor" aria-label="anchor" href="#sec-storeys-q-value-approach"><i class="fas fa-link"></i></a>
</h3>
<p>Storey’s q-value method directly estimates <a href="hypothesis-testing.html#sec-false-discovery-rate">False Discovery Rate</a> rather than adjusting individual <span class="math inline">\(p\)</span>-values <span class="citation">(<a href="references.html#ref-benjamini2001control">Benjamini and Yekutieli 2001</a>)</span>.</p>
<p>Unlike BH/BY, it does not assume a fixed threshold (<span class="math inline">\(\alpha\)</span>) but estimates the FDR dynamically from the data.</p>
<p>Define:</p>
<ul>
<li><p><span class="math inline">\(m_0\)</span>: Number of <strong>true null hypotheses</strong>.</p></li>
<li><p><span class="math inline">\(\pi_0\)</span>: Estimated <strong>proportion of true nulls</strong> in the dataset.</p></li>
</ul>
<p>Storey’s q-value adjusts <span class="math inline">\(p\)</span>-values based on: <span class="math display">\[
q(p) = \frac{\pi_0 \cdot m \cdot p}{\sum_{i=1}^{m} 1_{p_i \leq p}}
\]</span> where:</p>
<ul>
<li><p><span class="math inline">\(\pi_0\)</span> is estimated from the distribution of large p-values.</p></li>
<li><p>Unlike BH/BY, Storey’s method dynamically estimates the null proportion.</p></li>
</ul>
<hr>
<div class="sourceCode" id="cb577"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># devtools::install_github("jdstorey/qvalue")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/StoreyLab/qvalue">qvalue</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulated data: 1000 hypothesis tests</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">p_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="fl">0</span>, <span class="fl">0.1</span><span class="op">)</span>  </span>
<span></span>
<span><span class="co"># Compute q-values</span></span>
<span><span class="va">qvals</span> <span class="op">&lt;-</span> <span class="fu">qvalue_truncp</span><span class="op">(</span><span class="va">p_values</span><span class="op">)</span><span class="op">$</span><span class="va">qvalues</span></span>
<span></span>
<span><span class="co"># Summary of q-values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">qvals</span><span class="op">)</span></span>
<span><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">#&gt;  0.3126  0.9689  0.9771  0.9684  0.9847  1.0000</span></span></code></pre></div>
<p>Comparison: BH vs. BY vs. Storey</p>
<div class="sourceCode" id="cb578"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Apply multiple corrections</span></span>
<span><span class="va">p_bh</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"BH"</span><span class="op">)</span>   <span class="co"># Benjamini-Hochberg</span></span>
<span><span class="va">p_by</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/p.adjust.html">p.adjust</a></span><span class="op">(</span><span class="va">p_values</span>, method <span class="op">=</span> <span class="st">"BY"</span><span class="op">)</span>   <span class="co"># Benjamini-Yekutieli</span></span>
<span><span class="va">q_vals</span> <span class="op">&lt;-</span> <span class="fu">qvalue_truncp</span><span class="op">(</span><span class="va">p_values</span><span class="op">)</span><span class="op">$</span><span class="va">qvalues</span>   <span class="co"># Storey's q-value</span></span>
<span></span>
<span><span class="co"># Compare significance rates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  Raw_p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_values</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  BH <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_bh</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  BY <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_by</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  q_value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">q_vals</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;    Raw_p    BH    BY q_value</span></span>
<span><span class="co">#&gt; 1  0.029 0.097 0.725   0.969</span></span>
<span><span class="co">#&gt; 2  0.079 0.098 0.737   0.985</span></span>
<span><span class="co">#&gt; 3  0.041 0.097 0.725   0.969</span></span>
<span><span class="co">#&gt; 4  0.088 0.099 0.740   0.989</span></span>
<span><span class="co">#&gt; 5  0.094 0.100 0.746   0.998</span></span>
<span><span class="co">#&gt; 6  0.005 0.094 0.700   0.936</span></span>
<span><span class="co">#&gt; 7  0.053 0.098 0.737   0.985</span></span>
<span><span class="co">#&gt; 8  0.089 0.099 0.740   0.989</span></span>
<span><span class="co">#&gt; 9  0.055 0.098 0.737   0.985</span></span>
<span><span class="co">#&gt; 10 0.046 0.098 0.731   0.977</span></span></code></pre></div>
</div>
<div id="summary-false-discovery-rate-methods" class="section level3" number="16.3.4">
<h3>
<span class="header-section-number">16.3.4</span> Summary: False Discovery Rate Methods<a class="anchor" aria-label="anchor" href="#summary-false-discovery-rate-methods"><i class="fas fa-link"></i></a>
</h3>
<p>FDR control methods balance Type I and Type II errors, making them more powerful than conservative Family-Wise Error Rate (FWER) methods like Bonferroni.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="24%">
<col width="24%">
<col width="24%">
<col width="26%">
</colgroup>
<thead><tr class="header">
<th>Method</th>
<th>Type</th>
<th>Strength</th>
<th>Best for</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="hypothesis-testing.html#sec-benjamini-hochberg-procedure">Benjamini-Hochberg</a></td>
<td>Adjusted <span class="math inline">\(p\)</span>-values</td>
<td>Most powerful</td>
<td>Independent tests (e.g., surveys, psychology)</td>
</tr>
<tr class="even">
<td><a href="hypothesis-testing.html#sec-benjamini-yekutieli-procedure">Benjamini-Yekutieli</a></td>
<td>Adjusted <span class="math inline">\(p\)</span>-values</td>
<td>More conservative</td>
<td>Correlated tests (e.g., fMRI, finance)</td>
</tr>
<tr class="odd">
<td><a href="hypothesis-testing.html#sec-storeys-q-value-approach">Storey’s q-value</a></td>
<td>Direct FDR estimation</td>
<td>Most flexible</td>
<td>Large-scale studies (e.g., genomics, proteomics)</td>
</tr>
<tr class="even">
<td>Bonferroni</td>
<td>Family-Wise Error Rate (FWER)</td>
<td>Very conservative</td>
<td>Small number of tests, strict control</td>
</tr>
<tr class="odd">
<td>Holm’s Method</td>
<td>FWER (less strict than Bonferroni)</td>
<td>Moderate</td>
<td>Moderately strict correction</td>
</tr>
</tbody>
</table></div>
<hr>
</div>
</div>
<div id="comparison-of-testing-frameworks" class="section level2" number="16.4">
<h2>
<span class="header-section-number">16.4</span> Comparison of Testing Frameworks<a class="anchor" aria-label="anchor" href="#comparison-of-testing-frameworks"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="24%">
<col width="26%">
<col width="24%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th>Framework</th>
<th>Key Concept</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="hypothesis-testing.html#sec-null-hypothesis-significance-testing">NHST</a></td>
<td>Tests if an effect is statistically significant (p-value based)</td>
<td>Simple, widely used</td>
<td>Over-reliance on p-values, arbitrary thresholds (e.g., 0.05)</td>
</tr>
<tr class="even">
<td><a href="hypothesis-testing.html#sec-two-one-sided-tests-equivalence-testing">TOST</a></td>
<td>Tests whether two means are sufficiently close (equivalence)</td>
<td>Useful in equivalence testing</td>
<td>Requires pre-specified equivalence margin</td>
</tr>
<tr class="odd">
<td>Bayesian Testing</td>
<td>Uses posterior probabilities and Bayes Factor</td>
<td>Incorporates prior knowledge, intuitive</td>
<td>Requires prior distribution, computationally expensive</td>
</tr>
<tr class="even">
<td>Decision-Theoretic</td>
<td>Minimizes expected loss based on cost functions</td>
<td>Practical for decision-making</td>
<td>Needs subjective cost assignment</td>
</tr>
<tr class="odd">
<td><a href="hypothesis-testing.html#sec-false-discovery-rate">False Discovery Rate</a></td>
<td>Controls proportion of false positives in multiple tests</td>
<td>Useful in high-dimensional data</td>
<td>Can still lead to false discoveries</td>
</tr>
</tbody>
</table></div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="variable-selection.html"><span class="header-section-number">15</span> Variable Selection</a></div>
<div class="next"><a href="sec-marginal-effects.html"><span class="header-section-number">17</span> Marginal Effects</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#hypothesis-testing"><span class="header-section-number">16</span> Hypothesis Testing</a></li>
<li>
<a class="nav-link" href="#sec-null-hypothesis-significance-testing"><span class="header-section-number">16.1</span> Null Hypothesis Significance Testing</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#error-types-in-hypothesis-testing"><span class="header-section-number">16.1.1</span> Error Types in Hypothesis Testing</a></li>
<li><a class="nav-link" href="#hypothesis-testing-framework-1"><span class="header-section-number">16.1.2</span> Hypothesis Testing Framework</a></li>
<li><a class="nav-link" href="#interpreting-hypothesis-testing-results"><span class="header-section-number">16.1.3</span> Interpreting Hypothesis Testing Results</a></li>
<li><a class="nav-link" href="#understanding-p-values"><span class="header-section-number">16.1.4</span> Understanding p-Values</a></li>
<li><a class="nav-link" href="#the-role-of-sample-size"><span class="header-section-number">16.1.5</span> The Role of Sample Size</a></li>
<li><a class="nav-link" href="#p-value-hacking"><span class="header-section-number">16.1.6</span> p-Value Hacking</a></li>
<li><a class="nav-link" href="#practical-vs.-statistical-significance"><span class="header-section-number">16.1.7</span> Practical vs. Statistical Significance</a></li>
<li><a class="nav-link" href="#mitigating-the-misuse-of-p-values"><span class="header-section-number">16.1.8</span> Mitigating the Misuse of p-Values</a></li>
<li><a class="nav-link" href="#sec-wald-test"><span class="header-section-number">16.1.9</span> Wald Test</a></li>
<li><a class="nav-link" href="#sec-likelihood-ratio-test"><span class="header-section-number">16.1.10</span> Likelihood Ratio Test</a></li>
<li><a class="nav-link" href="#lagrange-multiplier-score"><span class="header-section-number">16.1.11</span> Lagrange Multiplier (Score) Test</a></li>
<li><a class="nav-link" href="#comparing-hypothesis-tests"><span class="header-section-number">16.1.12</span> Comparing Hypothesis Tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-two-one-sided-tests-equivalence-testing"><span class="header-section-number">16.2</span> Two One-Sided Tests Equivalence Testing</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#when-to-use-tost"><span class="header-section-number">16.2.1</span> When to Use TOST?</a></li>
<li><a class="nav-link" href="#interpretation-of-the-tost-procedure"><span class="header-section-number">16.2.2</span> Interpretation of the TOST Procedure</a></li>
<li><a class="nav-link" href="#relationship-to-confidence-intervals"><span class="header-section-number">16.2.3</span> Relationship to Confidence Intervals</a></li>
<li><a class="nav-link" href="#example-1-testing-the-equivalence-of-two-means"><span class="header-section-number">16.2.4</span> Example 1: Testing the Equivalence of Two Means</a></li>
<li><a class="nav-link" href="#advantages-of-tost-equivalence-testing"><span class="header-section-number">16.2.5</span> Advantages of TOST Equivalence Testing</a></li>
<li><a class="nav-link" href="#when-not-to-use-tost"><span class="header-section-number">16.2.6</span> When Not to Use TOST</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#sec-false-discovery-rate"><span class="header-section-number">16.3</span> False Discovery Rate</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sec-benjamini-hochberg-procedure"><span class="header-section-number">16.3.1</span> Benjamini-Hochberg Procedure</a></li>
<li><a class="nav-link" href="#sec-benjamini-yekutieli-procedure"><span class="header-section-number">16.3.2</span> Benjamini-Yekutieli Procedure</a></li>
<li><a class="nav-link" href="#sec-storeys-q-value-approach"><span class="header-section-number">16.3.3</span> Storey’s q-value Approach</a></li>
<li><a class="nav-link" href="#summary-false-discovery-rate-methods"><span class="header-section-number">16.3.4</span> Summary: False Discovery Rate Methods</a></li>
</ul>
</li>
<li><a class="nav-link" href="#comparison-of-testing-frameworks"><span class="header-section-number">16.4</span> Comparison of Testing Frameworks</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mikenguyen13/data_analysis/blob/main/16-hypothesis.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mikenguyen13/data_analysis/edit/main/16-hypothesis.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Guide on Data Analysis</strong>" was written by Mike Nguyen. It was last built on 2025-04-07.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
